---
title: 2111.03186v1 EditGAN  High-Precision Semantic Image Editing
date: 2021-11-04
---

# [EditGAN: High-Precision Semantic Image Editing](http://arxiv.org/abs/2111.03186v1)

authors: Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, Sanja Fidler


## What, Why and How

[1]: https://arxiv.org/abs/2111.03186 "[2111.03186] EditGAN: High-Precision Semantic Image Editing - arXiv.org"
[2]: https://arxiv.org/pdf/2111.03186v1.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2111.03186v1 "[2111.03186v1] EditGAN: High-Precision Semantic Image Editing"

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes EditGAN, a novel method for high quality, high precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car.
- **Why**: The paper aims to overcome the limitations of existing GAN based image editing methods that often require large scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images.
- **How**: EditGAN builds on a GAN framework that jointly models images and their semantic segmentations, requiring only a handful of labeled examples, making it a scalable tool for editing. Specifically, it embeds an image into the GAN latent space and performs conditional latent code optimization according to the segmentation edit, which effectively also modifies the image. To amortize optimization, it finds editing vectors in latent space that realize the edits. The framework allows it to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. It experimentally shows that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality. It can also easily combine multiple edits and perform plausible edits beyond EditGAN training data. It demonstrates EditGAN on a wide variety of image types and quantitatively outperforms several previous editing methods on standard editing benchmark tasks.


## Main Contributions

[1]: https://arxiv.org/abs/2111.03186 "[2111.03186] EditGAN: High-Precision Semantic Image Editing - arXiv.org"
[2]: https://arxiv.org/pdf/2111.03186v1.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2111.03186v1 "[2111.03186v1] EditGAN: High-Precision Semantic Image Editing"

According to the paper[^1^][1], the main contributions are:

- **A novel method for high quality, high precision semantic image editing**, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car.
- **A scalable tool for editing that requires only a handful of labeled examples**, building on a GAN framework that jointly models images and their semantic segmentations.
- **A conditional latent code optimization technique that effectively modifies the image according to the segmentation edit**, embedding an image into the GAN latent space and performing gradient-based optimization on the latent code.
- **A method to amortize optimization by finding editing vectors in latent space that realize the edits**, allowing it to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates.
- **Extensive experiments and evaluations on a wide variety of image types**, showing that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality. It can also easily combine multiple edits and perform plausible edits beyond EditGAN training data. It quantitatively outperforms several previous editing methods on standard editing benchmark tasks.

## Method Summary

[1]: https://arxiv.org/abs/2111.03186 "[2111.03186] EditGAN: High-Precision Semantic Image Editing - arXiv.org"
[2]: https://arxiv.org/pdf/2111.03186v1.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2111.03186v1 "[2111.03186v1] EditGAN: High-Precision Semantic Image Editing"

Here is a summary of the method section of the paper[^1^][1]:

- **Overview**: The paper presents EditGAN, a method for high quality, high precision semantic image editing, that allows users to edit images by modifying their highly detailed part segmentation masks. EditGAN consists of three main components: a GAN framework that jointly models images and their semantic segmentations, a conditional latent code optimization technique that effectively modifies the image according to the segmentation edit, and a method to amortize optimization by finding editing vectors in latent space that realize the edits.
- **GAN Framework**: The paper builds on a GAN framework that jointly models images and their semantic segmentations, requiring only a handful of labeled examples, making it a scalable tool for editing. The GAN framework consists of a generator G that takes a latent code z and outputs an image x and its segmentation mask m, and a discriminator D that takes an image x and its mask m and outputs a score indicating whether they are real or fake. The GAN framework is trained on a large-scale unlabeled dataset with an additional small-scale labeled dataset using self-supervised learning and semi-supervised learning objectives. The paper also introduces a novel mask consistency loss that encourages the generator to produce consistent masks across different latent codes for the same image.
- **Conditional Latent Code Optimization**: The paper proposes a conditional latent code optimization technique that effectively modifies the image according to the segmentation edit. Given an input image x and its mask m, the paper first embeds them into the GAN latent space by finding the latent code z that minimizes the reconstruction error between x and G(z). Then, given a user-specified segmentation edit m', the paper performs gradient-based optimization on z to find z' that minimizes the mask alignment error between m' and G(z'). The resulting image x' = G(z') is then the edited image that reflects the user's edit while preserving the image quality.
- **Editing Vectors**: The paper introduces a method to amortize optimization by finding editing vectors in latent space that realize the edits. The idea is to learn a set of editing vectors e_i that correspond to different types of edits, such as changing color, shape, or position of an object. Given an input image x and its mask m, the paper first embeds them into the latent space as z. Then, given an editing vector e_i, the paper applies it to z as z' = z + e_i and obtains the edited image x' = G(z'). The paper learns the editing vectors by sampling pairs of images from the labeled dataset that have different masks for the same object category and optimizing them to minimize the mask alignment error between them. The paper shows that the learned editing vectors can be directly applied on other images at interactive rates and can also be combined to perform multiple edits at once.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator G and the discriminator D
G = Generator()
D = Discriminator()

# Train the GAN framework on a large-scale unlabeled dataset and a small-scale labeled dataset
for epoch in range(num_epochs):
  # Sample a batch of images and masks from the labeled dataset
  x_l, m_l = sample_labeled_batch()
  # Sample a batch of images from the unlabeled dataset
  x_u = sample_unlabeled_batch()
  # Generate fake images and masks from random latent codes
  z_f = sample_random_latent_codes()
  x_f, m_f = G(z_f)
  # Compute the self-supervised loss for the unlabeled images
  L_self = self_supervised_loss(x_u, G, D)
  # Compute the semi-supervised loss for the labeled images
  L_semi = semi_supervised_loss(x_l, m_l, G, D)
  # Compute the mask consistency loss for the fake images and masks
  L_mask = mask_consistency_loss(x_f, m_f, G)
  # Compute the adversarial loss for the generator and the discriminator
  L_G_adv = adversarial_loss(x_f, m_f, D)
  L_D_adv = adversarial_loss(x_l, m_l, D) + adversarial_loss(x_u, None, D) + adversarial_loss(x_f, m_f, D)
  # Update the generator and the discriminator parameters
  update_parameters(G, L_G_adv + L_mask + lambda * L_self)
  update_parameters(D, L_D_adv + lambda * L_self)

# Define a set of editing vectors e_i for different types of edits
e = [e_1, e_2, ..., e_n]

# Learn the editing vectors by sampling pairs of images from the labeled dataset that have different masks for the same object category
for epoch in range(num_epochs):
  # Sample a batch of pairs of images and masks from the labeled dataset
  x_1, m_1, x_2, m_2 = sample_mask_pairs()
  # Embed the first image and mask into the latent space
  z_1 = embed(x_1, m_1, G)
  # Embed the second image and mask into the latent space
  z_2 = embed(x_2, m_2, G)
  # Compute the editing vector as the difference between the latent codes
  e_i = z_2 - z_1
  # Apply the editing vector to the first latent code and generate a new image and mask
  z' = z_1 + e_i
  x', m' = G(z')
  # Compute the mask alignment error between the generated mask and the second mask
  L_align = mask_alignment_loss(m', m_2)
  # Update the editing vector to minimize the mask alignment error
  update_parameters(e_i, L_align)

# Perform semantic image editing on a new image x and its mask m by modifying its part segmentation mask as m'
# Embed x and m into the latent space as z
z = embed(x, m, G)
# Perform conditional latent code optimization to find z' that minimizes the mask alignment error between m' and G(z')
z' = optimize(z, m', G)
# Generate the edited image x' as G(z')
x' = G(z')

# Alternatively, perform semantic image editing on a new image x and its mask m by applying one or more editing vectors e_i to z
# Embed x and m into the latent space as z
z = embed(x, m, G)
# Apply one or more editing vectors e_i to z as z' = z + e_i (or a linear combination of e_i)
z' = z + e_i # or z' = z + alpha * e_i + beta * e_j + ...
# Generate the edited image x' as G(z')
x' = G(z')
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
num_epochs = 100 # number of training epochs
batch_size = 32 # batch size for training
lr = 0.0002 # learning rate for optimization
beta1 = 0.5 # beta1 parameter for Adam optimizer
beta2 = 0.999 # beta2 parameter for Adam optimizer
lambda = 0.1 # weight for the self-supervised loss
nz = 128 # dimension of the latent code z
nc = 3 # number of channels in the image x
nm = 10 # number of channels in the mask m (one for each object category)
ngf = 64 # number of filters in the generator
ndf = 64 # number of filters in the discriminator

# Define the generator G as a U-Net with skip connections and residual blocks
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the encoder part of the U-Net
    self.encoder = nn.Sequential(
      # input is z, going into a convolution
      nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
      nn.BatchNorm2d(ngf * 8),
      nn.ReLU(True),
      # state size. (ngf*8) x 4 x 4
      nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf * 8),
      nn.ReLU(True),
      # state size. (ngf*8) x 8 x 8
      ResBlock(ngf * 8), # add a residual block
      nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf * 4),
      nn.ReLU(True),
      # state size. (ngf*4) x 16 x 16
      ResBlock(ngf * 4), # add a residual block
      nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf * 2),
      nn.ReLU(True),
      # state size. (ngf*2) x 32 x 32
      ResBlock(ngf * 2), # add a residual block
      nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf),
      nn.ReLU(True),
      # state size. (ngf) x 64 x 64
    )
    # Define the decoder part of the U-Net with skip connections
    self.decoder_x = nn.Sequential(
      # input is (ngf) x 64 x 64 from the encoder and (nc) x 64 x64 from the skip connection
      ResBlock(ngf + nc), # add a residual block with skip connection from image x
      nn.ConvTranspose2d(ngf + nc, ngf // 2, 4, 2, 1, bias=False),
      nn.BatchNorm2d(ngf // 2),
      nn.ReLU(True),
      # state size. (ngf//2) x 128 x128 
      ResBlock(ngf //2), # add a residual block 
      nn.ConvTranspose2d(ngf //2 , nc ,3 ,1 ,1 ,bias=False ),
      nn.Tanh()
      # output is an image x' with (nc) x128 x128 
    )
    self.decoder_m = nn.Sequential(
      # input is (ngf) x64 x64 from the encoder and (nm) x64 x64 from the skip connection 
      ResBlock(ngf + nm), # add a residual block with skip connection from mask m 
      nn.ConvTranspose2d(ngf + nm , ngf //2 ,4 ,2 ,1 ,bias=False ),
      nn.BatchNorm2d(ngf //2 ),
      nn.ReLU(True ),
      # state size. (ngf//2) x128 x128 
      ResBlock(ngf //2 ), # add a residual block 
      nn.ConvTranspose2d(ngf //2 , nm ,3 ,1 ,1 ,bias=False ),
      nn.Softmax(dim=1)
      # output is a mask m' with (nm) x128 x128 
    )

  def forward(self, z):
    # Encode the latent code z into a feature map
    e = self.encoder(z)
    # Decode the feature map into an image x' and a mask m'
    x = self.decoder_x(torch.cat([e, x], dim=1)) # concatenate the feature map and the image x along the channel dimension
    m = self.decoder_m(torch.cat([e, m], dim=1)) # concatenate the feature map and the mask m along the channel dimension
    return x, m

# Define the discriminator D as a PatchGAN with spectral normalization
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Define the main part of the discriminator
    self.main = nn.Sequential(
      # input is (nc + nm) x 128 x 128, concatenating image x and mask m along the channel dimension
      nn.utils.spectral_norm(nn.Conv2d(nc + nm, ndf, 4, 2, 1, bias=False)),
      nn.LeakyReLU(0.2, inplace=True),
      # state size. (ndf) x 64 x 64
      nn.utils.spectral_norm(nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)),
      nn.BatchNorm2d(ndf * 2),
      nn.LeakyReLU(0.2, inplace=True),
      # state size. (ndf*2) x 32 x 32
      nn.utils.spectral_norm(nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)),
      nn.BatchNorm2d(ndf * 4),
      nn.LeakyReLU(0.2, inplace=True),
      # state size. (ndf*4) x 16 x 16
      nn.utils.spectral_norm(nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)),
      nn.BatchNorm2d(ndf * 8),
      nn.LeakyReLU(0.2, inplace=True),
      # state size. (ndf*8) x 8 x 8
      nn.utils.spectral_norm(nn.Conv2d(ndf * 8, ndf * 8, 4, 2, 1, bias=False)),
      nn.BatchNorm2d(ndf * 8),
      nn.LeakyReLU(0.2, inplace=True),
      # state size. (ndf*8) x 4 x 4
    )
    # Define the final part of the discriminator that outputs a score
    self.final = nn.Sequential(
      # input is (ndf*8) x4 x4 
      nn.utils.spectral_norm(nn.Conv2d(ndf *8 ,1 ,4 ,1 ,0 ,bias=False )),
      nn.Sigmoid()
      # output is a score between 0 and 1 
    )

  def forward(self, x, m):
    # Concatenate the image x and the mask m along the channel dimension
    xm = torch.cat([x,m], dim=1)
    # Pass xm through the main part of the discriminator
    f = self.main(xm)
    # Pass f through the final part of the discriminator to get a score
    s = self.final(f)
    return s

# Define a function to embed an image x and its mask m into the latent space by finding z that minimizes the reconstruction error
def embed(x,m,G):
   # Initialize z as a random latent code 
   z = torch.randn(batch_size,nz ,1 ,1 ).to(device )
   # Define an optimizer for z 
   optimizer_z = optim.Adam([z], lr=lr,beta1=beta1,beta2=beta2 )
   for i in range(num_iterations ):
     # Generate an image x' and a mask m' from z 
     x',m' = G(z)
     # Compute the reconstruction loss as the sum of L1 losses between x and x' and between m and m'
     L_rec = torch.nn.L1Loss()(x,x') + torch.nn.L1Loss()(m,m')
     # Update z to minimize the reconstruction loss 
     optimizer_z.zero_grad()
     L_rec.backward()
     optimizer_z.step()
   return z

# Define a function to perform conditional latent code optimization to find z' that minimizes the mask alignment error between m' and G(z')
def optimize(z,m',G):
   # Define an optimizer for z 
   optimizer_z = optim.Adam([z], lr