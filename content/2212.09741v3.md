---
title: 2212.09741v3 One Embedder, Any Task  Instruction-Finetuned Text Embeddings
date: 2022-12-10
---

# [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](http://arxiv.org/abs/2212.09741v3)

authors: Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, Tao Yu


## What, Why and How

[1]: https://arxiv.org/pdf/2212.09741v3.pdf "One Embedder, Any Task: Instruction-Finetuned Text Embeddings - arXiv.org"
[2]: https://arxiv.org/abs/2212.09741 "One Embedder, Any Task: Instruction-Finetuned Text Embeddings"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.09741v3 "[2212.09741v3] One Embedder, Any Task: Instruction-Finetuned Text ..."

Here is a summary of the paper:

- **What**: The paper introduces INSTRUCTOR, a new method for computing text embeddings given task instructions. Text embeddings are vector representations of text inputs that can be used for various downstream tasks.
- **Why**: The paper aims to address the problem of existing text embeddings that are specialized for specific tasks or domains and have degraded performance when applied to new ones. The paper proposes to use task instructions as a way to tailor the embeddings to different use cases without any further training.
- **How**: The paper first annotates instructions for 330 diverse tasks and trains INSTRUCTOR on this multitask mixture with a contrastive loss. The paper then evaluates INSTRUCTOR on 70 embedding evaluation tasks, ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. The paper shows that INSTRUCTOR achieves state-of-the-art performance on these tasks, with an average improvement of 3.4% compared to the previous best results. The paper also analyzes the robustness and effectiveness of INSTRUCTOR and its instruction finetuning mechanism.

This summary is based on the abstract and introduction of the paper[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2212.09741v3.pdf "One Embedder, Any Task: Instruction-Finetuned Text Embeddings - arXiv.org"
[2]: https://arxiv.org/abs/2212.09741 "One Embedder, Any Task: Instruction-Finetuned Text Embeddings"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.09741v3 "[2212.09741v3] One Embedder, Any Task: Instruction-Finetuned Text ..."

According to the paper[^1^][1], the main contributions are:

- **A new method for computing text embeddings given task instructions**: The paper proposes INSTRUCTOR, a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training, by using instructions as a way to condition the embeddings on the use case.
- **A large-scale dataset of task instructions**: The paper annotates instructions for 330 diverse tasks and releases them as a new resource for the research community. The paper also provides guidelines and best practices for writing task instructions.
- **An extensive evaluation of INSTRUCTOR on 70 embedding evaluation tasks**: The paper evaluates INSTRUCTOR on a wide range of tasks, including classification, information retrieval, semantic textual similarity, and text generation evaluation. The paper shows that INSTRUCTOR outperforms previous state-of-the-art models on these tasks, with an average improvement of 3.4%.
- **An analysis of the robustness and effectiveness of INSTRUCTOR and its instruction finetuning mechanism**: The paper analyzes how INSTRUCTOR handles changes in instructions, how instruction finetuning improves the performance of INSTRUCTOR on diverse datasets, and how INSTRUCTOR compares with other methods that use task descriptions or prompts.

## Method Summary

[1]: https://arxiv.org/pdf/2212.09741v3.pdf "One Embedder, Any Task: Instruction-Finetuned Text Embeddings - arXiv.org"
[2]: https://arxiv.org/abs/2212.09741 "One Embedder, Any Task: Instruction-Finetuned Text Embeddings"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.09741v3 "[2212.09741v3] One Embedder, Any Task: Instruction-Finetuned Text ..."

Here is a summary of the method section of the paper:

- **INSTRUCTOR model**: The paper describes the architecture and training of INSTRUCTOR, a text embedder that takes both a text input and a task instruction as inputs and outputs a text embedding. The model consists of two components: an instruction encoder and a text encoder. The instruction encoder is a pretrained language model that encodes the task instruction into a vector. The text encoder is also a pretrained language model that encodes the text input into a vector. The final text embedding is obtained by concatenating the instruction vector and the text vector and passing them through a linear projection layer.
- **Instruction annotation**: The paper annotates instructions for 330 diverse tasks, covering various domains, genres, and modalities. The paper follows a three-step procedure to collect the instructions: (1) selecting tasks from existing datasets or creating new ones; (2) writing instructions for each task following some guidelines; and (3) validating and revising the instructions through crowdsourcing.
- **Multitask contrastive learning**: The paper trains INSTRUCTOR on a multitask mixture of 330 tasks with a contrastive loss. The paper uses hard negative mining to sample negative examples for each task. The paper also applies data augmentation techniques such as backtranslation and paraphrasing to increase the diversity of the training data.
- **Instruction finetuning**: The paper introduces a novel technique called instruction finetuning, which allows INSTRUCTOR to adapt to new tasks or domains by fine-tuning only the instruction encoder with a small amount of labeled data. The paper shows that instruction finetuning can significantly improve the performance of INSTRUCTOR on unseen tasks or domains.

This summary is based on sections 3 and 4 of the paper[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define INSTRUCTOR model
class INSTRUCTOR(nn.Module):
  def __init__(self):
    # Initialize instruction encoder and text encoder as pretrained language models
    self.instruction_encoder = PretrainedLM()
    self.text_encoder = PretrainedLM()
    # Initialize linear projection layer
    self.projection = nn.Linear(2 * hidden_size, embedding_size)

  def forward(self, instruction, text):
    # Encode instruction and text into vectors
    instruction_vector = self.instruction_encoder(instruction)
    text_vector = self.text_encoder(text)
    # Concatenate instruction vector and text vector
    concat_vector = torch.cat([instruction_vector, text_vector], dim=-1)
    # Project concat vector into text embedding
    text_embedding = self.projection(concat_vector)
    return text_embedding

# Annotate instructions for 330 tasks
tasks = [] # List of tasks
instructions = [] # List of instructions
for task in tasks:
  # Write instruction for task following guidelines
  instruction = write_instruction(task)
  # Validate and revise instruction through crowdsourcing
  instruction = validate_and_revise(instruction)
  # Append instruction to list
  instructions.append(instruction)

# Train INSTRUCTOR on multitask mixture with contrastive loss
optimizer = Adam(INSTRUCTOR.parameters()) # Define optimizer
for epoch in range(num_epochs):
  for task in tasks:
    # Sample positive and negative examples for task
    positives, negatives = sample_examples(task)
    # Apply data augmentation techniques to positives and negatives
    positives, negatives = augment_data(positives, negatives)
    # Get instruction for task
    instruction = instructions[task]
    # Compute text embeddings for positives and negatives using INSTRUCTOR
    positive_embeddings = INSTRUCTOR(instruction, positives)
    negative_embeddings = INSTRUCTOR(instruction, negatives)
    # Compute contrastive loss using cosine similarity
    loss = contrastive_loss(positive_embeddings, negative_embeddings)
    # Update INSTRUCTOR parameters using optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Fine-tune instruction encoder for new tasks or domains with labeled data
new_tasks = [] # List of new tasks or domains
new_data = [] # List of labeled data for new tasks or domains
for new_task in new_tasks:
  # Freeze text encoder parameters
  INSTRUCTOR.text_encoder.requires_grad_(False)
  # Get labeled data for new task or domain
  data = new_data[new_task]
  for epoch in range(num_epochs):
    for batch in data:
      # Get instruction, text, and label from batch
      instruction, text, label = batch
      # Compute text embedding using INSTRUCTOR
      text_embedding = INSTRUCTOR(instruction, text)
      # Compute task-specific loss using label and text embedding
      loss = task_loss(label, text_embedding)
      # Update instruction encoder parameters using optimizer
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers
import numpy as np
import random

# Define INSTRUCTOR model
class INSTRUCTOR(nn.Module):
  def __init__(self, pretrained_model_name, embedding_size):
    super().__init__()
    # Initialize instruction encoder and text encoder as pretrained language models
    self.instruction_encoder = transformers.AutoModel.from_pretrained(pretrained_model_name)
    self.text_encoder = transformers.AutoModel.from_pretrained(pretrained_model_name)
    # Get hidden size from language model config
    hidden_size = self.instruction_encoder.config.hidden_size
    # Initialize linear projection layer
    self.projection = nn.Linear(2 * hidden_size, embedding_size)

  def forward(self, instruction, text):
    # Encode instruction and text into vectors using the last hidden states of the language models
    instruction_vector = self.instruction_encoder(instruction)[0][:, 0]
    text_vector = self.text_encoder(text)[0][:, 0]
    # Concatenate instruction vector and text vector along the last dimension
    concat_vector = torch.cat([instruction_vector, text_vector], dim=-1)
    # Project concat vector into text embedding using linear layer
    text_embedding = self.projection(concat_vector)
    return text_embedding

# Annotate instructions for 330 tasks
tasks = [] # List of tasks
instructions = [] # List of instructions
for task in tasks:
  # Write instruction for task following guidelines
  instruction = write_instruction(task)
  # Validate and revise instruction through crowdsourcing
  instruction = validate_and_revise(instruction)
  # Append instruction to list
  instructions.append(instruction)

# Train INSTRUCTOR on multitask mixture with contrastive loss
# Define hyperparameters
pretrained_model_name = "bert-base-uncased" # Name of the pretrained language model to use
embedding_size = 768 # Size of the text embedding vector
num_epochs = 10 # Number of epochs to train INSTRUCTOR
batch_size = 32 # Batch size for training INSTRUCTOR
temperature = 0.05 # Temperature parameter for contrastive loss
margin = 0.2 # Margin parameter for contrastive loss
num_negatives = 5 # Number of negative examples to sample for each positive example
num_augments = 2 # Number of augmented examples to generate for each original example

# Initialize INSTRUCTOR model with pretrained model name and embedding size
INSTRUCTOR = INSTRUCTOR(pretrained_model_name, embedding_size)
# Move INSTRUCTOR model to device (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
INSTRUCTOR.to(device)
# Define optimizer as Adam with default parameters
optimizer = torch.optim.Adam(INSTRUCTOR.parameters())
# Define tokenizer as the tokenizer corresponding to the pretrained model name
tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)

for epoch in range(num_epochs):
  for task in tasks:
    # Sample positive examples for task from the corresponding dataset
    positives = sample_examples(task, batch_size)
    # Apply data augmentation techniques such as backtranslation and paraphrasing to positives using online services or models
    positives = augment_data(positives, num_augments)
    # Sample negative examples for task from other tasks or datasets using hard negative mining
    negatives = sample_negatives(task, batch_size, num_negatives)
    # Apply data augmentation techniques to negatives 
    negatives = augment_data(negatives, num_augments)
    # Get instruction for task from the list of instructions
    instruction = instructions[task]
    # Tokenize instruction, positives, and negatives using tokenizer
    instruction_tokens = tokenizer(instruction, return_tensors="pt", padding=True, truncation=True).to(device)
    positive_tokens = tokenizer(positives, return_tensors="pt", padding=True, truncation=True).to(device)
    negative_tokens = tokenizer(negatives, return_tensors="pt", padding=True, truncation=True).to(device)
    # Compute text embeddings for positives and negatives using INSTRUCTOR model
    positive_embeddings = INSTRUCTOR(instruction_tokens, positive_tokens) # Shape: (batch_size * num_augments, embedding_size)
    negative_embeddings = INSTRUCTOR(instruction_tokens, negative_tokens) # Shape: (batch_size * num_augments * num_negatives, embedding_size)
    # Compute cosine similarity between positive embeddings and negative embeddings using torch.matmul and F.normalize 
    similarity_matrix = torch.matmul(F.normalize(positive_embeddings), F.normalize(negative_embeddings).T) / temperature # Shape: (batch_size * num_augments, batch_size * num_augments * num_negatives)
    # Compute contrastive loss using similarity matrix, margin, and F.relu
    # The loss is the average of the max-margin losses for each positive embedding and its corresponding negative embeddings
    loss = torch.mean(torch.sum(F.relu(margin - similarity_matrix), dim=-1)) / num_negatives
    # Update INSTRUCTOR parameters using optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Fine-tune instruction encoder for new tasks or domains with labeled data
new_tasks = [] # List of new tasks or domains
new_data = [] # List of labeled data for new tasks or domains
for new_task in new_tasks:
  # Freeze text encoder parameters by setting requires_grad to False
  for param in INSTRUCTOR.text_encoder.parameters():
    param.requires_grad = False
  # Get labeled data for new task or domain as a PyTorch DataLoader
  data_loader = new_data[new_task]
  for epoch in range(num_epochs):
    for batch in data_loader:
      # Get instruction, text, and label from batch
      instruction, text, label = batch
      # Move instruction, text, and label to device
      instruction = instruction.to(device)
      text = text.to(device)
      label = label.to(device)
      # Compute text embedding using INSTRUCTOR model
      text_embedding = INSTRUCTOR(instruction, text)
      # Compute task-specific loss using label and text embedding
      # The loss function depends on the type of the task (e.g., cross entropy for classification, cosine similarity for retrieval, etc.)
      loss = task_loss(label, text_embedding)
      # Update instruction encoder parameters using optimizer
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
```