---
title: 2306.13651v2 Bring Your Own Data! Self-Supervised Evaluation for Large Language Models
date: 2023-06-14
---

# [Bring Your Own Data! Self-Supervised Evaluation for Large Language Models](http://arxiv.org/abs/2306.13651v2)

authors: Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein


## What, Why and How

[1]: https://arxiv.org/pdf/2306.13651v2.pdf "Bring Your Own Data! Self-Supervised Evaluation of Large ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.13651 "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"
[3]: https://arxiv.org/pdf/2306.13650 "arXiv:2306.13650v1 [hep-th] 23 Jun 2023"

Here is a summary of the paper:

```
What: The paper proposes a framework for self-supervised evaluation of large language models (LLMs) by analyzing their sensitivity or invariance to transformations on the input text.

Why: The paper argues that current evaluations of LLMs rely on small, domain-specific datasets with human-curated labels, which are time-consuming, expensive, and prone to leakage and bias. The paper claims that self-supervised evaluation can directly monitor LLM behavior on realistic data collected in the wild or streamed during live model deployment.

How: The paper demonstrates self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. The paper compares the self-supervised evaluations with human-supervised evaluations on similar benchmarks and finds strong correlations. The paper also provides code and examples for implementing self-supervised evaluation.
```


## Main Contributions

[1]: https://arxiv.org/pdf/2306.13651v2.pdf "Bring Your Own Data! Self-Supervised Evaluation of Large ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.13651 "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"
[3]: https://arxiv.org/pdf/2306.13650 "arXiv:2306.13650v1 [hep-th] 23 Jun 2023"

Some of the contributions of this paper are:

- Introducing a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text.
- Demonstrating self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors.
- Finding strong correlations between self-supervised and human-supervised evaluations on similar benchmarks.
- Providing code and examples for implementing self-supervised evaluation.

## Method Summary

[1]: https://arxiv.org/pdf/2306.13651v2.pdf "Bring Your Own Data! Self-Supervised Evaluation of Large ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.13651 "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"
[3]: https://arxiv.org/pdf/2306.13650 "arXiv:2306.13650v1 [hep-th] 23 Jun 2023"

Here is a summary of the method section of the paper:

```
The method section of the paper consists of four subsections: Data Collection, Perturbation Generation, Evaluation Metrics, and Implementation Details.

- Data Collection: The paper describes how to collect data from various sources, such as Wikipedia, Reddit, Twitter, and Common Crawl, to create corpora for different domains and tasks. The paper also discusses how to filter and preprocess the data to ensure quality and diversity.

- Perturbation Generation: The paper explains how to generate perturbations on the input text using different techniques, such as negation, paraphrasing, spelling errors, and word replacement. The paper also provides examples of perturbations for each task and domain.

- Evaluation Metrics: The paper defines different evaluation metrics to measure the sensitivity or invariance of LLMs to perturbations on the input text. The paper uses perplexity, probability distributions, and text outputs as evaluation metrics and compares them across pairs of original and perturbed texts.

- Implementation Details: The paper provides details on how to implement the self-supervised evaluation framework using PyTorch and HuggingFace Transformers. The paper also reports the hyperparameters and computational resources used for each experiment.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2306.13651v2.pdf "Bring Your Own Data! Self-Supervised Evaluation of Large ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.13651 "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"
[3]: https://arxiv.org/pdf/2306.13650 "arXiv:2306.13650v1 [hep-th] 23 Jun 2023"

Here is a high-level pseudo code for the paper:

```
# Define the task and domain of interest
task = "closed-book knowledge"
domain = "Wikipedia"

# Collect data from the domain source
corpus = collect_data(domain)

# Generate perturbations on the input text
perturbations = generate_perturbations(corpus, task)

# Create pairs of original and perturbed texts
pairs = create_pairs(corpus, perturbations)

# Load a pre-trained LLM
model = load_model("GPT-3")

# Define an evaluation metric
metric = "perplexity"

# Evaluate the LLM on the pairs using the metric
results = evaluate(model, pairs, metric)

# Analyze the results and compare with human-supervised evaluations
analyze(results)
compare(results, human_evaluations)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2306.13651v2.pdf "Bring Your Own Data! Self-Supervised Evaluation of Large ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.13651 "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"
[3]: https://arxiv.org/pdf/2306.13650 "arXiv:2306.13650v1 [hep-th] 23 Jun 2023"

Here is a detailed pseudo code to implement the paper:

```
# Import libraries
import torch
import transformers
import nltk
import textattack

# Define the task and domain of interest
task = "closed-book knowledge"
domain = "Wikipedia"

# Define the perturbation technique
perturbation = "negation"

# Define the evaluation metric
metric = "perplexity"

# Define the LLM to evaluate
model_name = "GPT-3"

# Load the LLM and the tokenizer
model = transformers.AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

# Collect data from the domain source
corpus = []
for article in wikipedia.random_articles(domain):
  corpus.append(article.text)

# Filter and preprocess the data
corpus = filter_by_length(corpus, min_length=10, max_length=100)
corpus = preprocess(corpus, lower_case=True, remove_punctuation=True, remove_stopwords=True)

# Generate perturbations on the input text
perturbations = []
for text in corpus:
  perturbed_text = textattack.transformations.WordSwapNegate(text, perturbation)
  perturbations.append(perturbed_text)

# Create pairs of original and perturbed texts
pairs = []
for i in range(len(corpus)):
  pair = (corpus[i], perturbations[i])
  pairs.append(pair)

# Evaluate the LLM on the pairs using the metric
results = []
for pair in pairs:
  original_text, perturbed_text = pair
  original_tokens = tokenizer.encode(original_text, return_tensors="pt")
  perturbed_tokens = tokenizer.encode(perturbed_text, return_tensors="pt")
  original_perplexity = torch.exp(model(original_tokens, labels=original_tokens).loss).item()
  perturbed_perplexity = torch.exp(model(perturbed_tokens, labels=perturbed_tokens).loss).item()
  result = (original_perplexity, perturbed_perplexity)
  results.append(result)

# Analyze the results and compare with human-supervised evaluations
mean_original_perplexity = mean([result[0] for result in results])
mean_perturbed_perplexity = mean([result[1] for result in results])
print(f"Mean original perplexity: {mean_original_perplexity}")
print(f"Mean perturbed perplexity: {mean_perturbed_perplexity}")
human_evaluations = load_human_evaluations(task, domain, metric)
correlation = compute_correlation(results, human_evaluations)
print(f"Correlation with human-supervised evaluations: {correlation}")
```