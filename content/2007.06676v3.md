---
title: 2007.06676v3 UnRectDepthNet  Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models
date: 2020-07-07
---

# [UnRectDepthNet: Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models](http://arxiv.org/abs/2007.06676v3)

authors: Varun Ravi Kumar, Senthil Yogamani, Markus Bach, Christian Witt, Stefan Milz, Patrick Mader


## What, Why and How

[1]: https://arxiv.org/abs/2007.06676v3 "[2007.06676v3] UnRectDepthNet: Self-Supervised Monocular Depth ..."
[2]: https://arxiv.org/pdf/2007.06676v3.pdf "UnRectDepthNet: Self-Supervised Monocular Depth Estimation ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2007.06676v3 "[2007.06676v3] UnRectDepthNet: Self-Supervised Monocular Depth ..."

Here is a summary of the paper:

- **What**: The paper proposes a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos[^1^][1].
- **Why**: The paper aims to overcome the limitations of rectification, which is a common preprocessing step for multi-view depth estimation. Rectification simplifies the depth estimation problem, but it also introduces side effects such as reduced field of view, resampling distortion, and sensitivity to calibration errors[^1^][1]. These effects are especially pronounced for cameras with significant distortion, such as wide-angle fisheye cameras[^1^][1].
- **How**: The paper leverages a novel projection and unprojection function that can handle different camera distortion models, such as barrel and fisheye[^1^][1]. The paper also introduces a scale-aware loss function that can handle the scale ambiguity in monocular depth estimation[^1^][1]. The paper evaluates the proposed method on the unrectified KITTI dataset with barrel distortion and on a wide-angle fisheye dataset with 190° horizontal field of view[^1^][1]. The paper shows that the proposed method achieves comparable or better results than state-of-the-art methods that use rectified images[^1^][1].

## Main Contributions

The paper claims the following contributions:

- A generic framework for self-supervised monocular depth estimation that can handle different camera distortion models without rectification.
- A scale-aware loss function that can estimate the absolute scale of depth and distance from monocular videos.
- A comprehensive evaluation of the proposed method on unrectified KITTI dataset with barrel distortion and on a wide-angle fisheye dataset with 190° horizontal field of view.
- State-of-the-art results on the rectified KITTI dataset that improve upon the previous work FisheyeDistanceNet.

## Method Summary

The method section of the paper consists of four subsections:

- **Projection and Unprojection Function**: The paper defines a generic projection function that maps a 3D point in the camera coordinate system to a 2D point in the image plane, and an unprojection function that does the inverse. The paper shows how these functions can be adapted to different camera distortion models, such as barrel and fisheye, by using the camera intrinsic parameters and distortion coefficients. The paper also shows how these functions can be implemented efficiently using differentiable tensor operations.
- **Depth Network**: The paper uses a modified version of the Monodepth2 network [3] as the depth network, which takes a single image as input and outputs a disparity map. The paper modifies the network to output a scale factor that represents the ratio between the predicted depth and the true depth. The paper also modifies the network to use the unprojection function instead of a fixed inverse camera matrix to handle different camera distortion models.
- **Pose Network**: The paper uses a modified version of the PoseCNN network [3] as the pose network, which takes a pair of images as input and outputs the relative pose between them. The paper modifies the network to use the projection function instead of a fixed camera matrix to handle different camera distortion models.
- **Loss Function**: The paper defines a scale-aware loss function that consists of three terms: a photometric consistency term, a smoothness term, and a scale consistency term. The photometric consistency term measures the pixel-wise similarity between the warped source image and the target image using the predicted depth, pose, and projection function. The smoothness term encourages the predicted depth to be locally smooth. The scale consistency term enforces the predicted scale factor to be consistent across different image pairs. The paper also introduces a mask to exclude moving objects and occluded regions from the loss computation.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the projection and unprojection functions for a given camera distortion model
def project(point_3d, camera_params, distortion_coeffs):
  # Apply the camera intrinsic matrix to the 3D point
  point_2d = camera_params * point_3d
  # Apply the distortion model to the 2D point
  point_2d = distort(point_2d, distortion_coeffs)
  # Return the projected 2D point
  return point_2d

def unproject(point_2d, camera_params, distortion_coeffs):
  # Apply the inverse distortion model to the 2D point
  point_2d = undistort(point_2d, distortion_coeffs)
  # Apply the inverse camera intrinsic matrix to the 2D point
  point_3d = camera_params^-1 * point_2d
  # Return the unprojected 3D point
  return point_3d

# Define the depth network that takes an image and outputs a disparity map and a scale factor
def depth_network(image):
  # Use a modified Monodepth2 network to encode and decode the image
  features = encoder(image)
  disparity, scale = decoder(features)
  # Return the disparity map and the scale factor
  return disparity, scale

# Define the pose network that takes a pair of images and outputs the relative pose between them
def pose_network(image_1, image_2):
  # Use a modified PoseCNN network to encode and concatenate the images
  features_1 = encoder(image_1)
  features_2 = encoder(image_2)
  features = concatenate(features_1, features_2)
  # Use a fully connected layer to output the pose parameters
  pose = fc_layer(features)
  # Return the pose parameters
  return pose

# Define the loss function that consists of three terms: photometric consistency, smoothness, and scale consistency
def loss_function(target_image, source_image, disparity, scale, pose, camera_params, distortion_coeffs):
  # Compute the depth from the disparity and the scale factor
  depth = scale / disparity
  # Unproject the target image pixels to the camera coordinate system using the depth and the unprojection function
  target_points = unproject(target_image * depth, camera_params, distortion_coeffs)
  # Transform the target points to the source coordinate system using the pose parameters
  source_points = transform(target_points, pose)
  # Project the source points to the source image plane using the projection function
  source_pixels = project(source_points, camera_params, distortion_coeffs)
  # Warp the source image to the target image plane using the source pixels
  warped_source_image = warp(source_image, source_pixels)
  # Compute a mask to exclude moving objects and occluded regions from the loss computation
  mask = compute_mask(target_image, source_image, warped_source_image)
  # Compute the photometric consistency term as the pixel-wise similarity between the warped source image and the target image
  photometric_loss = similarity(warped_source_image * mask, target_image * mask)
  # Compute the smoothness term as the gradient magnitude of the disparity map
  smoothness_loss = gradient(disparity)
  # Compute the scale consistency term as the variance of the scale factor across different image pairs
  scale_consistency_loss = variance(scale)
  # Combine the three terms with different weights to get the total loss
  total_loss = photometric_loss + smoothness_weight * smoothness_loss + scale_consistency_weight * scale_consistency_loss
  # Return the total loss
  return total_loss

# Train and test the proposed method on unrectified monocular videos with different camera distortion models

# Initialize the depth network and the pose network with random weights
depth_network = DepthNetwork()
pose_network = PoseNetwork()

# Load a dataset of unrectified monocular videos with different camera distortion models and corresponding parameters and coefficients
dataset = Dataset()

# Loop over epochs
for epoch in epochs:
  
    # Loop over batches of image sequences from the dataset
    for batch in dataset:

        # Get a target image and a source image from each sequence in the batch
        target_image, source_image = get_images(batch)

        # Get the camera parameters and distortion coefficients for each sequence in the batch
        camera_params, distortion_coeffs = get_params(batch)

        # Forward pass: feed the images to the depth network and the pose network and get their outputs
        disparity, scale = depth_network(target_image)
        pose = pose_network(target_image, source_image)

        # Compute the loss function using the outputs and other inputs 
        loss = loss_function(target_image, source_image, disparity, scale, pose, camera_params, distortion_coeffs)

        # Backward pass: update the network weights using the loss and an optimizer
        optimizer.update(loss, depth_network, pose_network)

    # Evaluate the performance of the method on a validation set and save the best model
    performance = evaluate(depth_network, pose_network, validation_set)
    save_best_model(depth_network, pose_network, performance)

# Load the best model and test it on a test set
depth_network, pose_network = load_best_model()
test(depth_network, pose_network, test_set)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import cv2

# Define the projection and unprojection functions for a given camera distortion model
def project(point_3d, camera_params, distortion_coeffs):
  # Apply the camera intrinsic matrix to the 3D point
  # point_3d: (B, 3, H, W) tensor of 3D points in the camera coordinate system
  # camera_params: (B, 3, 3) tensor of camera intrinsic parameters
  # point_2d: (B, 2, H, W) tensor of 2D points in the normalized image plane
  point_2d = torch.matmul(camera_params, point_3d)

  # Apply the distortion model to the 2D point
  # distortion_coeffs: (B, N) tensor of distortion coefficients, where N depends on the distortion model
  # distorted_point_2d: (B, 2, H, W) tensor of distorted 2D points in the normalized image plane

  # For barrel distortion model (N = 5), use the following formula:
  # r = sqrt(x^2 + y^2)
  # x' = x * (1 + k1 * r^2 + k2 * r^4 + k3 * r^6) + p1 * (r^2 + 2 * x^2) + p2 * (r^4 + 4 * x^2)
  # y' = y * (1 + k1 * r^2 + k2 * r^4 + k3 * r^6) + p1 * (r^2 + 2 * y^2) + p2 * (r^4 + 4 * y^2)
  
  x = point_2d[:,0,:,:]
  y = point_2d[:,1,:,:]
  r = torch.sqrt(x**2 + y**2)
  
  k1 = distortion_coeffs[:,0].unsqueeze(1).unsqueeze(1)
  k2 = distortion_coeffs[:,1].unsqueeze(1).unsqueeze(1)
  k3 = distortion_coeffs[:,4].unsqueeze(1).unsqueeze(1)
  
  p1 = distortion_coeffs[:,2].unsqueeze(1).unsqueeze(1)
  p2 = distortion_coeffs[:,3].unsqueeze(1).unsqueeze(1)

  x_prime = x * (1 + k1 * r**2 + k2 * r**4 + k3 * r**6) + p1 * (r**2 + 2 * x**2) + p2 * (r**4 + 4 * x**2)
  
  y_prime = y * (1 + k1 * r**2 + k2 * r**4 + k3 * r**6) + p1 * (r**2 + 2 * y**2) + p2 * (r**4 + 4 * y**2)

  
  distorted_point_2d = torch.stack([x_prime, y_prime], dim=1)

  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
  




  
  
  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


 


  
 
  




  
  
  




  
  
  




  
  
  




  
  
  




  
  
  




  
  
  




  
  
  




  
  
  




  
  
  




  
  
  

# For fisheye distortion model (N = 4), use the following formula:
# theta = atan(sqrt(x^2 + y^2))
# theta_d = theta * (1 + k1*theta^2 + k2*theta^4 + k3*theta^6 + k4*theta^8)
# x' = theta_d / sqrt(x^2 + y^2) * x
# y' = theta_d / sqrt(x^2 + y^2) * y

x = point_2d[:,0,:,:]
y = point_2d[:,1,:,:]
theta = torch.atan(torch.sqrt(x**2 + y**2))

k1 = distortion_coeffs[:,0].unsqueeze(1).unsqueeze(1)
k2 = distortion_coeffs[:,1].unsqueeze(1).unsqueeze(1)
k3 = distortion_coeffs[:,2].unsqueeze(1).unsqueeze(1)
k4 = distortion_coeffs[:,3].unsqueeze(1).unsqueeze(1)

theta_d = theta * (1 + k1*theta**2 + k2*theta**4 + k3*theta**6 + k4*theta**8)

x_prime = theta_d / torch.sqrt(x**2 + y**2) * x
y_prime = theta_d / torch.sqrt(x**2 + y**2) * y

distorted_point_2d = torch.stack([x_prime, y_prime], dim=1)

  # Return the projected 2D point
  return distorted_point_2d

def unproject(point_2d, camera_params, distortion_coeffs):
  # Apply the inverse distortion model to the 2D point
  # point_2d: (B, 2, H, W) tensor of distorted 2D points in the normalized image plane
  # distortion_coeffs: (B, N) tensor of distortion coefficients, where N depends on the distortion model
  # undistorted_point_2d: (B, 2, H, W) tensor of undistorted 2D points in the normalized image plane

  # For barrel distortion model (N = 5), use an iterative method to solve for the undistorted point:
  # x' = x * (1 + k1 * r'^2 + k2 * r'^4 + k3 * r'^6) + p1 * (r'^2 + 2 * x'^2) + p2 * (r'^4 + 4 * x'^2)
  # y' = y * (1 + k1 * r'^2 + k2 * r'^4 + k3 * r'^6) + p1 * (r'^2 + 2 * y'^2) + p2 * (r'^4 + 4 * y'^2)
  # where r' = sqrt(x'^2 + y'^2)
  # Initialize x' and y' with x and y
  # Repeat until convergence:
  #   Compute r' from x' and y'
  #   Update x' and y' using the above equations

  
  x_prime = point_2d[:,0,:,:]
  y_prime = point_2d[:,1,:,:]

  
  k1 = distortion_coeffs[:,0].unsqueeze(1).unsqueeze(1)
  k2 = distortion_coeffs[:,1].unsqueeze(1).unsqueeze(1)
  k3 = distortion_coeffs[:,4].unsqueeze(1).unsqueeze(1)
  
  p1 = distortion_coeffs[:,2].unsqueeze(1).unsqueeze(1)
  p2 = distortion_coeffs[:,3].unsqueeze(1).unsqueeze(1)

  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  
 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
  




  
  
  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


  


 


  
 
  




  
  
  




  
  
  

# Define a convergence threshold and a maximum number of iterations
threshold = 0.00001
max_iter = 10

# Loop until convergence or maximum iterations
for i in range(max_iter):
    # Compute r' from x' and y'
    r_prime = torch.sqrt(x_prime**2 + y_prime**2)

    # Update x' and y' using the above equations
    x_prime_new = point_2d[:,0,:,:] - (x_prime * (k1 * r_prime**2 + k2 * r_prime**4 + k3 * r_prime**6) + p1 * (r_prime**2 + 2 * x_prime**2) + p2 * (r_prime**4 + 4 * x_prime**2))
    y_prime_new = point_2d[:,1,:,:] - (y_prime * (k1 * r_prime**2 + k2 * r_prime**4 + k3 * r_prime**6) + p1 * (r_prime**2 + 2 * y_prime**2) + p2 * (r_prime**4 + 4 * y_prime**2))

    # Compute the error between the new and old values
    error = torch.max(torch.abs(x_prime_new - x_prime), torch.abs(y_prime_new - y_prime))

    # Check if the error is below the threshold
    if error < threshold:
        break

    # Otherwise, update x' and y' with the new values