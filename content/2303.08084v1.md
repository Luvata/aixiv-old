---
title: 2303.08084v1 Editing Implicit Assumptions in Text-to-Image Diffusion Models
date: 2023-03-09
---

# [Editing Implicit Assumptions in Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.08084v1)

authors: Hadas Orgad, Bahjat Kawar, Yonatan Belinkov


## What, Why and How

[1]: https://arxiv.org/abs/2303.08084 "Editing Implicit Assumptions in Text-to-Image Diffusion Models"
[2]: https://arxiv.org/pdf/2303.08084.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2303.08084 "Editing Implicit Assumptions in Text-to-Image Diffusion Models"

Here is a summary of the paper:

- **What**: The paper proposes a method to edit implicit assumptions in text-to-image diffusion models, such as roses are red or dogs are big, without requiring explicit user input or costly re-training. The method is called TIME (Text-to-Image Model Editing).
- **Why**: The paper argues that implicit assumptions can be outdated, incorrect, or reflective of social biases present in the training data, and that there is a need to control them for better image generation and diversity.
- **How**: The paper introduces a pair of inputs: a source prompt and a destination prompt. The source prompt is an under-specified text that triggers an implicit assumption in the model, such as "a pack of roses". The destination prompt is a text that describes the same setting, but with a specified desired attribute, such as "a pack of blue roses". The paper then updates the cross-attention layers in the pre-trained diffusion model, such that the source prompt is projected close to the destination prompt. The paper evaluates the method on a new dataset called TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. The paper shows that the method is successful in editing implicit assumptions, generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.


## Main Contributions

According to the paper, the main contributions are:

- A novel method for editing implicit assumptions in text-to-image diffusion models, called TIME, that requires only a pair of source and destination prompts as input and modifies a small fraction of the model's parameters in a fast and efficient way.
- A new dataset for evaluating model editing approaches, called TIMED, containing 147 source and destination prompt pairs from various domains, such as animals, flowers, food, and people.
- An extensive experimental evaluation of TIME on TIMED using Stable Diffusion , showing that TIME can successfully edit implicit assumptions, generalize well for related prompts unseen during editing, and impose minimal effect on unrelated generations.


## Method Summary

Here is a summary of the method section of the paper:

- The paper uses a text-to-image diffusion model as the base model, which consists of a text encoder, an image encoder, and a decoder. The text encoder is a pre-trained language model that encodes the textual prompt into a sequence of embeddings. The image encoder and decoder are convolutional neural networks that encode and decode the image into a sequence of latent variables. The decoder also has cross-attention layers that attend to the text embeddings and assign visual meaning to textual tokens.
- The paper introduces TIME, a method for editing implicit assumptions in the base model. TIME receives a pair of inputs: a source prompt and a destination prompt. The source prompt is an under-specified text that triggers an implicit assumption in the model, such as "a pack of roses". The destination prompt is a text that describes the same setting, but with a specified desired attribute, such as "a pack of blue roses".
- TIME then updates the projection matrices in the cross-attention layers of the base model, such that the source prompt is projected close to the destination prompt. This is done by minimizing a distance function between the projected embeddings of the source and destination prompts, using gradient descent. The distance function is based on cosine similarity and KL divergence.
- TIME only modifies 2.2% of the model's parameters, which are the projection matrices in the cross-attention layers. The rest of the parameters remain unchanged. This makes TIME fast and efficient, as it can edit the model in under one second.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a source prompt and a destination prompt
# Output: an edited text-to-image diffusion model

# Load the base text-to-image diffusion model
model = load_model()

# Encode the source and destination prompts using the text encoder
source_embeddings = model.text_encoder(source_prompt)
destination_embeddings = model.text_encoder(destination_prompt)

# Initialize the projection matrices in the cross-attention layers
projection_matrices = initialize_projection_matrices()

# Define the distance function between the projected embeddings
def distance_function(source_embeddings, destination_embeddings, projection_matrices):
  # Project the source and destination embeddings using the projection matrices
  projected_source_embeddings = project(source_embeddings, projection_matrices)
  projected_destination_embeddings = project(destination_embeddings, projection_matrices)
  # Compute the cosine similarity and KL divergence between the projected embeddings
  cosine_similarity = compute_cosine_similarity(projected_source_embeddings, projected_destination_embeddings)
  kl_divergence = compute_kl_divergence(projected_source_embeddings, projected_destination_embeddings)
  # Return a weighted combination of the cosine similarity and KL divergence
  return alpha * cosine_similarity + beta * kl_divergence

# Minimize the distance function using gradient descent
for i in range(num_iterations):
  # Compute the gradient of the distance function with respect to the projection matrices
  gradient = compute_gradient(distance_function, projection_matrices)
  # Update the projection matrices using the gradient and a learning rate
  projection_matrices = projection_matrices - learning_rate * gradient

# Replace the projection matrices in the cross-attention layers of the base model with the updated ones
model.cross_attention.projection_matrices = projection_matrices

# Return the edited model
return model
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import transformers # for text encoder
import stable_diffusion # for image encoder and decoder
import torch.nn.functional as F # for distance functions

# Input: a source prompt and a destination prompt
# Output: an edited text-to-image diffusion model

# Load the base text-to-image diffusion model
model = stable_diffusion.load_model()

# Encode the source and destination prompts using the text encoder
text_encoder = transformers.AutoModel.from_pretrained("gpt2")
source_embeddings = text_encoder(source_prompt)["last_hidden_state"]
destination_embeddings = text_encoder(destination_prompt)["last_hidden_state"]

# Initialize the projection matrices in the cross-attention layers
num_layers = len(model.cross_attention) # number of cross-attention layers
hidden_size = model.cross_attention[0].hidden_size # hidden size of cross-attention layers
projection_matrices = [torch.randn(hidden_size, hidden_size) for i in range(num_layers)] # random initialization

# Define the distance function between the projected embeddings
def distance_function(source_embeddings, destination_embeddings, projection_matrices):
  # Project the source and destination embeddings using the projection matrices
  projected_source_embeddings = [torch.matmul(source_embeddings, projection_matrix) for projection_matrix in projection_matrices]
  projected_destination_embeddings = [torch.matmul(destination_embeddings, projection_matrix) for projection_matrix in projection_matrices]
  # Compute the cosine similarity and KL divergence between the projected embeddings
  cosine_similarity = sum([F.cosine_similarity(projected_source_embedding, projected_destination_embedding) for projected_source_embedding, projected_destination_embedding in zip(projected_source_embeddings, projected_destination_embeddings)])
  kl_divergence = sum([F.kl_div(projected_source_embedding, projected_destination_embedding) for projected_source_embedding, projected_destination_embedding in zip(projected_source_embeddings, projected_destination_embeddings)])
  # Return a weighted combination of the cosine similarity and KL divergence
  alpha = 0.5 # weight for cosine similarity
  beta = 0.5 # weight for KL divergence
  return alpha * cosine_similarity + beta * kl_divergence

# Minimize the distance function using gradient descent
num_iterations = 10 # number of iterations
learning_rate = 0.01 # learning rate
for i in range(num_iterations):
  # Compute the gradient of the distance function with respect to the projection matrices
  gradient = torch.autograd.grad(distance_function(source_embeddings, destination_embeddings, projection_matrices), projection_matrices)
  # Update the projection matrices using the gradient and a learning rate
  projection_matrices = [projection_matrix - learning_rate * grad for projection_matrix, grad in zip(projection_matrices, gradient)]

# Replace the projection matrices in the cross-attention layers of the base model with the updated ones
for i in range(num_layers):
  model.cross_attention[i].projection_matrix = projection_matrices[i]

# Return the edited model
return model
```