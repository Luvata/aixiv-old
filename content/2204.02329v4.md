---
title: 2204.02329v4 Can language models learn from explanations in context?
date: 2022-04-03
---

# [Can language models learn from explanations in context?](http://arxiv.org/abs/2204.02329v4)

authors: Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, Felix Hill


## What, Why and How

[1]: https://arxiv.org/pdf/2204.02329v4.pdf "arXiv:2204.02329v4 [cs.CL] 10 Oct 2022"
[2]: https://arxiv.org/abs/2204.02329 "Can language models learn from explanations in context?"
[3]: http://export.arxiv.org/abs/2010.02329v4 "[2010.02329v4] InfoBERT: Improving Robustness of Language Models from ..."

Here is a summary of the paper at http://arxiv.org/pdf/2204.02329v4 in terms of What, Why, and How:

- **What**: The paper investigates whether language models (LMs) can learn from explanations of few-shot examples in context, and how different types of explanations affect their performance on 40 challenging tasks[^1^][1].
- **Why**: The paper aims to improve the in-context learning abilities of LMs, which can perform new tasks by adapting to a few examples within their context window, but without training. The paper argues that explanations can help LMs infer the correct task and generalize better, as they do for humans[^1^][1].
- **How**: The paper annotates questions from 40 tasks with answer explanations and various matched control explanations. It evaluates how different types of explanations, instructions, and controls affect zero- and few-shot performance of LMs. It analyzes the results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2204.02329v4.pdf "arXiv:2204.02329v4 [cs.CL] 10 Oct 2022"
[2]: https://arxiv.org/abs/2204.02329 "Can language models learn from explanations in context?"
[3]: http://export.arxiv.org/abs/2204.03322 "[2204.03322] Gravitational entanglement and the mass contribution of ..."

According to the paper at http://arxiv.org/pdf/2204.02329v4, the main contributions are:

- The paper annotates 40 diverse, challenging language tasks with explanations of examples, and releases these annotations[^1^][1].
- The paper evaluates several LMs after prompting with or without few-shot examples, explanations, instructions, and control conditions[^1^][1].
- The paper finds that explanations can improve performance -- even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone[^1^][1].
- The paper shows that even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit[^1^][1].
- The paper provides a comprehensive analysis of the results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models[^1^][1].

## Method Summary

[1]: https://arxiv.org/pdf/2204.02329v4.pdf "arXiv:2204.02329v4 [cs.CL] 10 Oct 2022"
[2]: https://arxiv.org/abs/2204.02329 "Can language models learn from explanations in context?"
[3]: http://export.arxiv.org/abs/2010.02329v4 "[2010.02329v4] InfoBERT: Improving Robustness of Language Models from ..."

Here is a summary of the method section of the paper at http://arxiv.org/pdf/2204.02329v4:

- The paper uses 40 language tasks from the LAMA benchmark (Petroni et al., 2019), which cover a variety of domains and formats, such as arithmetic, trivia, logic, and word sense disambiguation[^1^][1].
- The paper annotates each question from these tasks with an answer explanation, which is a natural language sentence that connects the question and the answer by highlighting the relevant task principle or reasoning process[^1^][1].
- The paper also creates various control explanations for each question, which are either irrelevant to the task, or only restate the question or the answer without providing any additional information[^1^][1].
- The paper evaluates four LMs of different sizes (GPT-2 small, medium, large, and XL) on these tasks in zero-shot and few-shot settings[^1^][1].
- The paper prompts each LM with different combinations of instructions, examples, explanations, and controls, and measures their accuracy on answering the questions[^1^][1].
- The paper analyzes the results using multilevel modeling techniques (Gelman and Hill, 2007), which account for the nested dependencies among conditions, tasks, prompts, and models[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define 40 language tasks from LAMA benchmark
tasks = load_tasks_from_LAMA()

# Annotate each question with an answer explanation and various control explanations
for task in tasks:
  for question in task.questions:
    question.answer_explanation = write_explanation(question, task)
    question.control_explanations = write_controls(question, task)

# Evaluate four LMs of different sizes on these tasks
models = [GPT2_small, GPT2_medium, GPT2_large, GPT2_XL]
for model in models:
  for task in tasks:
    # Prompt the model with different combinations of instructions, examples, explanations, and controls
    for condition in [zero_shot, few_shot]:
      for explanation_type in [answer_explanation, control_explanation]:
        prompt = generate_prompt(task, condition, explanation_type)
        # Measure the accuracy of the model on answering the questions
        accuracy = evaluate_model(model, prompt, task.questions)

# Analyze the results using multilevel modeling techniques
results = multilevel_model(models, tasks, conditions, explanation_types, accuracies)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import transformers
import pandas as pd
import numpy as np
import statsmodels.api as sm

# Define 40 language tasks from LAMA benchmark
tasks = load_tasks_from_LAMA()
# Each task is a dictionary with keys: name, description, questions
# Each question is a dictionary with keys: text, answer

# Annotate each question with an answer explanation and various control explanations
for task in tasks:
  for question in task.questions:
    # Write an answer explanation that connects the question and the answer by highlighting the relevant task principle or reasoning process
    question.answer_explanation = write_explanation(question, task)
    # Write various control explanations that are either irrelevant to the task, or only restate the question or the answer without providing any additional information
    question.control_explanations = write_controls(question, task)

# Evaluate four LMs of different sizes on these tasks
models = [GPT2_small, GPT2_medium, GPT2_large, GPT2_XL]
# Load the models and the tokenizer from transformers library
tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')
for model_name in models:
  model = transformers.GPT2LMHeadModel.from_pretrained(model_name)
  # Set the model to evaluation mode
  model.eval()
  for task in tasks:
    # Prompt the model with different combinations of instructions, examples, explanations, and controls
    for condition in [zero_shot, few_shot]:
      for explanation_type in [answer_explanation, control_explanation]:
        # Generate a prompt that contains an instruction for the task, optionally followed by a few examples with or without explanations or controls
        prompt = generate_prompt(task, condition, explanation_type)
        # Encode the prompt using the tokenizer
        prompt_ids = tokenizer.encode(prompt)
        # Measure the accuracy of the model on answering the questions
        accuracy = 0
        for question in task.questions:
          # Append the question to the prompt and encode it
          query = prompt + '\n' + question.text + '\n'
          query_ids = tokenizer.encode(query)
          # Generate an answer from the model using beam search
          output_ids = model.generate(input_ids=query_ids, num_beams=5)
          # Decode the output and extract the answer
          output = tokenizer.decode(output_ids)
          answer = output.split('\n')[-1]
          # Compare the answer with the ground truth and update the accuracy
          if answer == question.answer:
            accuracy += 1
        accuracy /= len(task.questions)

# Analyze the results using multilevel modeling techniques
# Create a data frame with columns: model, task, condition, explanation_type, accuracy
data = pd.DataFrame(columns=['model', 'task', 'condition', 'explanation_type', 'accuracy'])
# Populate the data frame with the results from the previous step
data = fill_data(data, models, tasks, conditions, explanation_types, accuracies)
# Fit a multilevel model with random intercepts for tasks and prompts, and fixed effects for models, conditions, and explanation types
formula = 'accuracy ~ model + condition + explanation_type + (1|task) + (1|prompt)'
model = sm.MixedLM.from_formula(formula=formula, data=data)
results = model.fit()
# Print the summary of the results
print(results.summary())
```