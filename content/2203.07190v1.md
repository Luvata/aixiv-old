---
title: 2203.07190v1 CLIP Models are Few-shot Learners  Empirical Studies on VQA and Visual Entailment
date: 2022-03-08
---

# [CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment](http://arxiv.org/abs/2203.07190v1)

authors: Haoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, Furu Wei


## What, Why and How

[1]: https://arxiv.org/abs/2203.07190v1 "[2203.07190v1] CLIP Models are Few-shot Learners: Empirical Studies on ..."
[2]: https://arxiv.org/pdf/2203.07190v1.pdf "arXiv:2203.07190v1 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2303.07190v1.pdf "PDF for 2303.07190v1 - arXiv.org"

Here is a summary of the paper:

```
What: The paper studies the few-shot learning capabilities of CLIP models on vision-language understanding tasks such as visual question answering and visual entailment.

Why: The paper aims to show that CLIP models can leverage natural language supervision to perform well on vision-language tasks without additional pre-training or large-scale annotated data.

How: The paper evaluates CLIP models on zero-shot and few-shot settings on visual question answering and visual entailment tasks. The paper also proposes a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task.
```

## Main Contributions

[1]: https://arxiv.org/abs/2203.07190v1 "[2203.07190v1] CLIP Models are Few-shot Learners: Empirical Studies on ..."
[2]: https://arxiv.org/pdf/2203.07190v1.pdf "arXiv:2203.07190v1 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2303.07190v1.pdf "PDF for 2303.07190v1 - arXiv.org"

Here is a list of the contributions of the paper:

```
- The paper demonstrates that CLIP models can perform well on vision-language understanding tasks such as visual question answering and visual entailment in zero-shot and few-shot settings, without additional pre-training or large-scale annotated data.
- The paper shows that CLIP models can leverage natural language supervision to learn visual concepts and cross-modality transfer capabilities.
- The paper proposes a parameter-efficient fine-tuning strategy that uses language prompts to boost the few-shot performance on the vqa task.
```

## Method Summary

[1]: https://arxiv.org/abs/2203.07190v1 "[2203.07190v1] CLIP Models are Few-shot Learners: Empirical Studies on ..."
[2]: https://arxiv.org/pdf/2203.07190v1.pdf "arXiv:2203.07190v1 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2303.07190v1.pdf "PDF for 2303.07190v1 - arXiv.org"

Here is a summary of the method section of the paper:

```
The paper uses two vision-language understanding tasks: visual question answering and visual entailment, to evaluate the few-shot learning capabilities of CLIP models. The paper uses two versions of CLIP models: CLIP-ViT-B/32 and CLIP-RN50x4, as the baselines. The paper also compares the CLIP models with other vision-language pre-trained models such as LXMERT, UNITER, and OSCAR.

For the visual question answering task, the paper uses the VQA v2.0 dataset, which contains 1.1 million image-question-answer triplets. The paper uses language prompts to formulate the questions and answers as natural sentences, and feeds them to the CLIP text encoder. The paper also proposes a parameter-efficient fine-tuning strategy that only updates the last layer of the text encoder and keeps the visual encoder fixed. The paper evaluates the zero-shot and few-shot performance of CLIP models on this task.

For the visual entailment task, the paper uses the SNLI-VE dataset, which contains 550k image-hypothesis pairs with three entailment labels: entailment, contradiction, and neutral. The paper uses language prompts to formulate the hypotheses as natural sentences, and feeds them to the CLIP text encoder. The paper also uses caption-hypothesis pairs from SNLI dataset to pre-train the text encoder in a zero-shot cross-modality transfer setting. The paper evaluates the zero-shot performance of CLIP models on this task.
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the CLIP model with a visual encoder and a text encoder
clip_model = CLIP(visual_encoder, text_encoder)

# Define the language prompts for the VQA and VE tasks
vqa_prompt = "Question: {question} Answer: {answer}"
ve_prompt = "Hypothesis: {hypothesis} Label: {label}"

# Define the datasets for the VQA and VE tasks
vqa_dataset = VQA_v2.0()
ve_dataset = SNLI-VE()

# Define the fine-tuning strategy for the VQA task
fine_tune(clip_model, vqa_dataset):
  # Freeze the visual encoder and the first n-1 layers of the text encoder
  clip_model.visual_encoder.requires_grad = False
  for layer in clip_model.text_encoder[:-1]:
    layer.requires_grad = False
  
  # Initialize a new linear layer as the classifier
  classifier = Linear(clip_model.text_encoder[-1].output_dim, vqa_dataset.num_answers)

  # Train the classifier and the last layer of the text encoder using cross-entropy loss
  for batch in vqa_dataset:
    # Get the images, questions, and answers from the batch
    images, questions, answers = batch

    # Encode the images using the visual encoder
    image_features = clip_model.visual_encoder(images)

    # Encode the questions and answers using language prompts and the text encoder
    text_features = clip_model.text_encoder([vqa_prompt.format(question=q, answer=a) for q, a in zip(questions, answers)])

    # Compute the logits of the answers using the classifier
    logits = classifier(text_features)

    # Compute the cross-entropy loss between the logits and the true answers
    loss = cross_entropy(logits, answers)

    # Update the parameters of the classifier and the last layer of the text encoder
    loss.backward()
    optimizer.step()

# Define the zero-shot evaluation function for the VQA task
zero_shot_eval(clip_model, vqa_dataset):
  # Initialize a list to store the predictions
  predictions = []

  # Loop over the test set of the vqa dataset
  for batch in vqa_dataset.test_set:
    # Get the images and questions from the batch
    images, questions = batch

    # Encode the images using the visual encoder
    image_features = clip_model.visual_encoder(images)

    # Loop over all possible answers in the vqa dataset
    for answer in vqa_dataset.all_answers:
      # Encode each answer using language prompts and the text encoder
      answer_features = clip_model.text_encoder([vqa_prompt.format(question=q, answer=answer) for q in questions])

      # Compute the cosine similarity between image features and answer features
      similarity = cosine_similarity(image_features, answer_features)

      # Append the similarity scores to the predictions list
      predictions.append(similarity)
    
    # Find the answer with the highest similarity score for each question
    predictions = argmax(predictions, axis=0)

  # Compute the accuracy of the predictions against the true answers
  accuracy = accuracy_score(predictions, vqa_dataset.test_answers)

  # Return the accuracy
  return accuracy

# Define the zero-shot cross-modality transfer function for the VE task
zero_shot_transfer(clip_model, ve_dataset):
  # Pre-train the text encoder using caption-hypothesis pairs from SNLI dataset
  pre_train(clip_model.text_encoder, snli_dataset):
    # Train the text encoder using cross-entropy loss
    for batch in snli_dataset:
      # Get the captions, hypotheses, and labels from the batch
      captions, hypotheses, labels = batch

      # Encode the captions and hypotheses using language prompts and the text encoder
      caption_features = clip_model.text_encoder([ve_prompt.format(hypothesis=h, label=l) for h, l in zip(hypotheses, labels)])
      hypothesis_features = clip_model.text_encoder([ve_prompt.format(hypothesis=h, label=l) for h, l in zip(captions, labels)])

      # Compute the logits of the labels using dot product similarity
      logits = dot_product(caption_features, hypothesis_features)

      # Compute the cross-entropy loss between the logits and the true labels
      loss = cross_entropy(logits, labels)

      # Update the parameters of the text encoder
      loss.backward()
      optimizer.step()
  
  # Evaluate the zero-shot performance on VE task using image-hypothesis pairs from SNLI-VE dataset
  zero_shot_eval(clip_model, ve_dataset):
    # Initialize a list to store the predictions
    predictions = []

    # Loop over the test set of ve dataset
    for batch in ve_dataset.test_set:
      # Get images and hypotheses from batch 
      images, hypotheses = batch

      # Encode images using visual encoder
      image_features = clip_model.visual_encoder(images)

      # Loop over all possible labels in ve dataset
      for label in ve_dataset.all_labels:
        # Encode each label using language prompts and text encoder
        label_features = clip_model.text_encoder([ve_prompt.format(hypothesis=h, label=label) for h in hypotheses])

        # Compute the cosine similarity between image features and label features
        similarity = cosine_similarity(image_features, label_features)

        # Append the similarity scores to the predictions list
        predictions.append(similarity)
      
      # Find the label with the highest similarity score for each hypothesis
      predictions = argmax(predictions, axis=0)

    # Compute the accuracy of the predictions against the true labels
    accuracy = accuracy_score(predictions, ve_dataset.test_labels)

    # Return the accuracy
    return accuracy
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score

# Define the hyperparameters
batch_size = 32
num_epochs = 10
learning_rate = 1e-4
num_answers = 3129 # number of unique answers in VQA v2.0 dataset
num_labels = 3 # number of entailment labels in SNLI-VE dataset

# Load the CLIP model with a visual encoder and a text encoder
clip_model = transformers.CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

# Load the VQA v2.0 dataset using torchvision
vqa_dataset = torchvision.datasets.VQA(root="./data", version="v2", transform=torchvision.transforms.ToTensor())

# Load the SNLI-VE dataset using pandas
ve_dataset = pd.read_csv("./data/snli_ve.csv")

# Load the SNLI dataset using pandas
snli_dataset = pd.read_csv("./data/snli.csv")

# Define the language prompts for the VQA and VE tasks
vqa_prompt = "Question: {question} Answer: {answer}"
ve_prompt = "Hypothesis: {hypothesis} Label: {label}"

# Define the fine-tuning strategy for the VQA task
def fine_tune(clip_model, vqa_dataset):
  # Freeze the visual encoder and the first n-1 layers of the text encoder
  clip_model.visual_encoder.requires_grad = False
  for layer in clip_model.text_encoder[:-1]:
    layer.requires_grad = False
  
  # Initialize a new linear layer as the classifier
  classifier = torch.nn.Linear(clip_model.text_encoder[-1].output_dim, num_answers)

  # Initialize an optimizer and a scheduler for the trainable parameters
  optimizer = torch.optim.Adam([classifier.parameters(), clip_model.text_encoder[-1].parameters()], lr=learning_rate)
  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

  # Initialize a data loader for the vqa dataset
  data_loader = torch.utils.data.DataLoader(vqa_dataset, batch_size=batch_size, shuffle=True)

  # Train the classifier and the last layer of the text encoder using cross-entropy loss
  for epoch in range(num_epochs):
    # Initialize a variable to store the running loss
    running_loss = 0.0

    # Loop over the data loader
    for i, batch in enumerate(data_loader):
      # Get the images, questions, and answers from the batch
      images, questions, answers = batch

      # Move the tensors to the device (cpu or gpu)
      images = images.to(device)
      questions = questions.to(device)
      answers = answers.to(device)

      # Encode the images using the visual encoder
      image_features = clip_model.visual_encoder(images)

      # Encode the questions and answers using language prompts and the text encoder
      text_features = clip_model.text_encoder([vqa_prompt.format(question=q, answer=a) for q, a in zip(questions, answers)])

      # Compute the logits of the answers using the classifier
      logits = classifier(text_features)

      # Compute the cross-entropy loss between the logits and the true answers
      loss = torch.nn.CrossEntropyLoss()(logits, answers)

      # Update the running loss
      running_loss += loss.item()

      # Zero the gradients of the optimizer
      optimizer.zero_grad()

      # Backpropagate the loss and update the parameters of the classifier and the last layer of the text encoder
      loss.backward()
      optimizer.step()

      # Print the statistics every 200 batches
      if (i+1) % 200 == 0:
        print(f"[{epoch+1}, {i+1}] loss: {running_loss / 200}")
        running_loss = 0.0
    
    # Update the learning rate scheduler
    scheduler.step()
  
  # Save the fine-tuned model and classifier
  torch.save(clip_model.state_dict(), "./model/clip_model.pth")
  torch.save(classifier.state_dict(), "./model/classifier.pth")

# Define the zero-shot evaluation function for the VQA task
def zero_shot_eval(clip_model, vqa_dataset):
  # Initialize a list to store the predictions
  predictions = []

  # Initialize a data loader for the test set of vqa dataset with batch size of 1 (for simplicity)
  data_loader = torch.utils.data.DataLoader(vqa_dataset.test_set, batch_size=1, shuffle=False)

  # Loop over the data loader
  for i, batch in enumerate(data_loader):
    # Get the image and question from batch 
    image, question = batch

    # Move tensors to device (cpu or gpu)
    image = image.to(device)
    question = question.to(device)

    # Encode the image using the visual encoder
    image_feature = clip_model.visual_encoder(image)

    # Initialize a list to store the similarity scores for each answer
    similarity_scores = []

    # Loop over all possible answers in the vqa dataset
    for answer in vqa_dataset.all_answers:
      # Encode the answer using language prompts and the text encoder
      answer_feature = clip_model.text_encoder(vqa_prompt.format(question=question, answer=answer))

      # Compute the cosine similarity between image feature and answer feature
      similarity = torch.nn.CosineSimilarity()(image_feature, answer_feature)

      # Append the similarity score to the list
      similarity_scores.append(similarity)
    
    # Find the answer with the highest similarity score
    prediction = torch.argmax(torch.tensor(similarity_scores))

    # Append the prediction to the predictions list
    predictions.append(prediction)
  
  # Compute the accuracy of the predictions against the true answers
  accuracy = accuracy_score(predictions, vqa_dataset.test_answers)

  # Return the accuracy
  return accuracy

# Define the zero-shot cross-modality transfer function for the VE task
def zero_shot_transfer(clip_model, ve_dataset):
  # Pre-train the text encoder using caption-hypothesis pairs from SNLI dataset
  def pre_train(clip_model, snli_dataset):
    # Initialize an optimizer and a scheduler for the text encoder parameters
    optimizer = torch.optim.Adam(clip_model.text_encoder.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

    # Initialize a data loader for the snli dataset
    data_loader = torch.utils.data.DataLoader(snli_dataset, batch_size=batch_size, shuffle=True)

    # Train the text encoder using cross-entropy loss
    for epoch in range(num_epochs):
      # Initialize a variable to store the running loss
      running_loss = 0.0

      # Loop over the data loader
      for i, batch in enumerate(data_loader):
        # Get the captions, hypotheses, and labels from the batch
        captions, hypotheses, labels = batch

        # Move tensors to device (cpu or gpu)
        captions = captions.to(device)
        hypotheses = hypotheses.to(device)
        labels = labels.to(device)

        # Encode the captions and hypotheses using language prompts and the text encoder
        caption_features = clip_model.text_encoder([ve_prompt.format(hypothesis=h, label=l) for h, l in zip(hypotheses, labels)])
        hypothesis_features = clip_model.text_encoder([ve_prompt.format(hypothesis=h, label=l) for h, l in zip(captions, labels)])

        # Compute the logits of the labels using dot product similarity
        logits = torch.matmul(caption_features, hypothesis_features.T)

        # Compute the cross-entropy loss between the logits and the true labels
        loss = torch.nn.CrossEntropyLoss()(logits, labels)

        # Update the running loss
        running_loss += loss.item()

        # Zero the gradients of the optimizer
        optimizer.zero_grad()

        # Backpropagate the loss and update the parameters of the text encoder
        loss.backward()
        optimizer.step()

        # Print statistics every 200 batches
        if (i+1) % 200 == 0:
          print(f"[{epoch+1}, {i+1}] loss: {running_loss / 200}")
          running_loss = 0.0
      
      # Update learning rate scheduler
      scheduler.step()
    
    # Save pre-trained text encoder 
    torch.save(clip_model.text_encoder.state_dict(), "./model/text_encoder.pth")
  
  # Evaluate zero-shot performance on VE task using image-hypothesis pairs from SNLI-VE dataset
  def zero_shot_eval(clip_model, ve_dataset):
    # Initialize a list to store predictions 
    predictions = []

    # Initialize a data loader for test set of ve dataset with batch size of 1 (for simplicity)
    data_loader = torch.utils.data.DataLoader(ve_dataset.test_set, batch_size=1, shuffle=False)

    # Loop over data loader 
    for i, batch in enumerate(data_loader):
      # Get images and hypotheses from batch 
      images, hypotheses = batch

      # Move tensors to device (cpu or gpu)
      images = images.to(device)
      hypotheses = hypotheses.to(device)

      # Encode images using visual encoder 
      image_features = clip_model.visual_encoder(images)

      # Initialize a list to store similarity scores for each label 
      similarity_scores = []

      # Loop over all possible labels in ve dataset 
      for label in ve_dataset.all_labels:
        # Encode each label using language prompts and text encoder 
        label_features = clip_model.text_encoder(ve_prompt.format(hypothesis=hypotheses, label=label))

        # Compute cosine similarity between image features and label features 
        similarity