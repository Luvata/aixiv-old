---
title: 2205.14870v2 Compressible-composable NeRF via Rank-residual Decomposition
date: 2022-05-15
---

# [Compressible-composable NeRF via Rank-residual Decomposition](http://arxiv.org/abs/2205.14870v2)

authors: Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, Gang Zeng


## What, Why and How

[1]: https://arxiv.org/pdf/2205.14870v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2205.14870v2.pdf "arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.14870v2 "[2212.14870v2] Light Axiodilatons: Matter Couplings, Weak-Scale ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a neural field representation that enables efficient and convenient manipulation of 3D models for photo-realistic rendering. The representation is based on a hybrid tensor rank decomposition of the scene without neural networks.
- **Why**: The paper aims to overcome the limitations of existing Neural Radiance Field (NeRF) methods, which are either restricted by a shared renderer network, or suffer from large model size. The paper also wants to enable extra capability of compression and composition for NeRF models.
- **How**: The paper introduces a rank-residual learning strategy to encourage the preservation of primary information in lower ranks. The model size can then be dynamically adjusted by rank truncation to control the levels of detail. Furthermore, different models can be arbitrarily transformed and composed into one scene by concatenating along the rank dimension. The paper demonstrates that the proposed method can achieve comparable rendering quality to state-of-the-art methods, while enabling extra capability of compression and composition.

## Main Contributions

[1]: https://arxiv.org/pdf/2205.14870v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2205.14870v2.pdf "arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.14870v2 "[2212.14870v2] Light Axiodilatons: Matter Couplings, Weak-Scale ..."

The paper at [^1^][1] claims the following contributions:

- **A novel neural field representation** that enables efficient and convenient manipulation of 3D models for photo-realistic rendering. The representation is based on a hybrid tensor rank decomposition of the scene without neural networks.
- **A rank-residual learning strategy** that encourages the preservation of primary information in lower ranks. The model size can then be dynamically adjusted by rank truncation to control the levels of detail, achieving near-optimal compression without extra optimization.
- **A compressible-composable NeRF framework** that allows different models to be arbitrarily transformed and composed into one scene by concatenating along the rank dimension. The growth of storage cost can also be mitigated by compressing the unimportant objects in the composed scene.
- **Extensive experiments** that demonstrate the effectiveness and efficiency of the proposed method on various datasets and tasks, such as rendering quality, compression ratio, composition quality, and editing capability.

## Method Summary

[1]: https://arxiv.org/pdf/2205.14870v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2205.14870v2.pdf "arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.14870v2 "[2212.14870v2] Light Axiodilatons: Matter Couplings, Weak-Scale ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first introduces the **preliminaries** of NeRF and tensor rank decomposition, and defines the problem of compressible-composable NeRF representation.
- The paper then presents the **proposed method** of learning a hybrid tensor rank decomposition of the scene without neural networks. The method consists of three steps: 1) sampling points along rays and encoding them into a 3D tensor; 2) applying a rank-residual learning strategy to decompose the tensor into a low-rank core tensor and a residual tensor; 3) reconstructing the radiance field from the decomposed tensors and rendering images using volume rendering.
- The paper also describes how to achieve **compression** and **composition** using the proposed representation. For compression, the paper shows how to truncate the rank of the core tensor to reduce the model size and control the levels of detail. For composition, the paper shows how to concatenate different models along the rank dimension to form a new scene, and how to compress the unimportant objects in the composed scene to save storage cost.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a set of images and camera poses
# Output: a compressible-composable NeRF representation

# Step 1: Sampling and encoding
for each image and camera pose:
  sample points along rays according to a stratified sampling scheme
  encode each point into a feature vector using positional encoding
  stack the feature vectors along the ray dimension to form a 2D matrix
  stack the matrices along the image dimension to form a 3D tensor

# Step 2: Rank-residual decomposition
initialize a low-rank core tensor and a residual tensor randomly
while not converged:
  compute the reconstruction loss between the original tensor and the sum of the core tensor and the residual tensor
  update the core tensor and the residual tensor using gradient descent

# Step 3: Radiance field reconstruction and rendering
for each point along a ray:
  extract its feature vector from the original tensor
  multiply it with the core tensor to get a low-rank feature vector
  add it with the corresponding feature vector from the residual tensor to get a full-rank feature vector
  decode it into density and color using two linear layers
  accumulate density and color along the ray using volume rendering
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: a set of images I and camera poses P
# Output: a compressible-composable NeRF representation T_c and T_r

# Hyperparameters: L (number of frequency bands for positional encoding), R (initial rank for core tensor), N_r (number of samples per ray), N_i (number of iterations for decomposition), lr (learning rate for decomposition), D (dimension of feature vector), C (dimension of color vector)

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Define the positional encoding function
def positional_encoding(x, L):
  # x: a scalar or a vector of size D
  # L: the number of frequency bands
  # return: a vector of size 2 * L * D
  x = x.unsqueeze(-1) # make x a column vector
  freqs = torch.pow(2.0, torch.arange(0, L)) # a vector of size L
  freqs = freqs.unsqueeze(-1) # make freqs a column vector
  x = x * freqs # broadcasted multiplication, a matrix of size D x L
  x = torch.cat([torch.sin(x), torch.cos(x)], dim=-1) # a matrix of size D x 2L
  x = x.flatten() # a vector of size 2 * L * D
  return x

# Step 1: Sampling and encoding
T = torch.zeros(N_r, len(I), 2 * L * D) # initialize the original tensor
for i in range(len(I)): # loop over images
  image = I[i] # get the image
  pose = P[i] # get the camera pose
  rays = get_rays(image, pose) # get the rays for the image, a matrix of size H x W x 3
  rays = rays.reshape(-1, 3) # flatten the rays, a matrix of size H * W x 3
  for j in range(len(rays)): # loop over rays
    ray = rays[j] # get the ray direction, a vector of size 3
    t_vals = np.linspace(0, 1, N_r) # get the sample points along the ray, a vector of size N_r
    X = ray * t_vals[:, None] # get the sample coordinates, a matrix of size N_r x 3
    X = torch.from_numpy(X).float() # convert to torch tensor
    for k in range(len(X)): # loop over sample points
      x = X[k] # get the sample point, a vector of size 3
      x = positional_encoding(x, L) # encode the point into a feature vector, a vector of size 2 * L * D
      T[k, i, :] = x # store the feature vector into the original tensor

# Step 2: Rank-residual decomposition
T_c = torch.randn(N_r, R, 2 * L * D) # initialize the core tensor randomly
T_r = torch.randn(N_r, len(I), 2 * L * D) # initialize the residual tensor randomly
optimizer = optim.Adam([T_c, T_r], lr=lr) # define the optimizer for decomposition
criterion = nn.MSELoss() # define the loss function for decomposition
for i in range(N_i): # loop over iterations
  optimizer.zero_grad() # reset the gradients
  T_hat = torch.einsum('nrd,nid->nri', T_c, T_r) + T_r # compute the reconstructed tensor using Einstein summation notation
  loss = criterion(T_hat, T) # compute the reconstruction loss
  loss.backward() # compute the gradients
  optimizer.step() # update the core tensor and the residual tensor

# Step 3: Radiance field reconstruction and rendering
decoder_d = nn.Linear(2 * L * D, 1) # define the decoder for density using a linear layer
decoder_c = nn.Linear(2 * L * D, C) # define the decoder for color using a linear layer
images = [] # initialize an empty list for rendered images
for i in range(len(I)): # loop over images
  image = torch.zeros(H, W, C) # initialize an empty image
  rays = get_rays(I[i], P[i]) # get the rays for the image, a matrix of size H x W x 3 
  rays = rays.reshape(-1, 3) # flatten the rays, a matrix of size H * W x 3
  for j in range(len(rays)): # loop over rays
    ray = rays[j] # get the ray direction, a vector of size 3
    t_vals = np.linspace(0, 1, N_r) # get the sample points along the ray, a vector of size N_r
    X = ray * t_vals[:, None] # get the sample coordinates, a matrix of size N_r x 3
    X = torch.from_numpy(X).float() # convert to torch tensor
    F = torch.zeros(N_r, 2 * L * D) # initialize an empty matrix for feature vectors
    for k in range(len(X)): # loop over sample points
      x = X[k] # get the sample point, a vector of size 3
      x = positional_encoding(x, L) # encode the point into a feature vector, a vector of size 2 * L * D
      f_c = torch.matmul(T_c[k], x) # multiply the feature vector with the core tensor, a vector of size R
      f_r = T_r[k, i] # get the corresponding feature vector from the residual tensor, a vector of size 2 * L * D
      f = f_c + f_r # add the two feature vectors to get a full-rank feature vector, a vector of size 2 * L * D
      F[k] = f # store the feature vector into the matrix
    sigma = decoder_d(F) # decode the feature vectors into density values, a vector of size N_r
    rgb = decoder_c(F) # decode the feature vectors into color values, a matrix of size N_r x C
    alpha = 1 - torch.exp(-sigma * delta) # compute the alpha values for each sample point, a vector of size N_r
    weights = alpha * torch.cumprod(torch.cat([torch.ones(1), 1 - alpha[:-1]]), dim=0) # compute the weights for each sample point, a vector of size N_r
    color = torch.sum(weights[:, None] * rgb, dim=0) # compute the accumulated color along the ray, a vector of size C
    image[j // W, j % W] = color # store the color into the image
  images.append(image) # append the image to the list

# Return the compressible-composable NeRF representation and the rendered images
return T_c, T_r, images

```