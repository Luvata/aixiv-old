---
title: 2003.00196v3 First Order Motion Model for Image Animation
date: 2020-03-01
---

# [First Order Motion Model for Image Animation](http://arxiv.org/abs/2003.00196v3)

authors: Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe


## What, Why and How

[1]: https://arxiv.org/abs/2003.00196v3 "[2003.00196v3] First Order Motion Model for Image Animation - arXiv.org"
[2]: https://arxiv.org/pdf/2003.00196v3.pdf "First Order Motion Model for Image Animation - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2003.00196 "[2003.00196] First Order Motion Model for Image Animation"

Here is a summary of the paper:

- **What**: The paper proposes a method for image animation, which is the task of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. For example, a face image of a certain person can be animated following the facial expressions of another individual.
- **Why**: The paper aims to address the problem of image animation without using any annotation or prior information about the specific object to animate. This makes the method object-agnostic, meaning that it can be applied to any object category once trained on a set of videos depicting objects of the same class.
- **How**: The paper uses a self-supervised formulation to decouple appearance and motion information. It introduces a representation consisting of a set of learned keypoints along with their local affine transformations, which capture the first order motion of the object. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. The paper evaluates the method on diverse benchmarks and on a variety of object categories, showing that it outperforms existing methods.

## Main Contributions

The paper claims the following contributions:

- A novel self-supervised formulation for image animation that does not require any annotation or prior information about the object to animate.
- A first order motion representation that consists of a set of learned keypoints and their local affine transformations, which can capture complex motions and deformations of the object.
- A generator network that can handle occlusions and synthesize realistic videos by combining the appearance and motion information.
- An extensive experimental evaluation on diverse benchmarks and object categories, demonstrating the superiority of the proposed method over existing approaches.

## Method Summary

The method section of the paper describes the proposed framework for image animation. It consists of three main components: an unsupervised keypoint detector, a motion extraction module, and a generator network.

- The unsupervised keypoint detector is a convolutional neural network that takes an image as input and outputs a set of keypoints and their corresponding heatmaps. The keypoints are learned in a self-supervised manner by minimizing a reconstruction loss between the input image and a warped image obtained by applying a random affine transformation to the keypoints. The keypoints are encouraged to be sparse, uniform, and consistent across different images of the same object category.
- The motion extraction module is responsible for extracting the first order motion representation from a driving video. It consists of two steps: keypoint detection and motion estimation. In the first step, the unsupervised keypoint detector is applied to each frame of the driving video to obtain the keypoints and their heatmaps. In the second step, the motion estimation module computes the local affine transformations between the keypoints in consecutive frames using optical flow. The motion representation is then formed by concatenating the heatmaps and the affine matrices for each keypoint.
- The generator network is a conditional generative adversarial network that takes a source image and a motion representation as inputs and outputs a synthesized video. The generator network consists of two parts: an occlusion-aware network and an appearance transfer network. The occlusion-aware network predicts an occlusion mask that indicates which regions of the source image are occluded by the motion. The appearance transfer network warps the source image according to the motion representation and blends it with the occlusion mask to produce the final output. The generator network is trained using adversarial, perceptual, and identity losses to ensure realistic and consistent results.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the unsupervised keypoint detector network
keypoint_detector = CNN()

# Define the generator network
generator = Conditional_GAN()

# Define the losses
reconstruction_loss = L1_loss()
adversarial_loss = Hinge_loss()
perceptual_loss = VGG_loss()
identity_loss = L1_loss()

# Train the keypoint detector network
for each image in the training set:
  # Generate a random affine transformation
  T = random_affine()
  # Apply the transformation to the image
  warped_image = warp(image, T)
  # Predict the keypoints and heatmaps for the image and the warped image
  keypoints, heatmaps = keypoint_detector(image)
  warped_keypoints, warped_heatmaps = keypoint_detector(warped_image)
  # Apply the inverse transformation to the warped keypoints
  inverse_keypoints = warp(warped_keypoints, T^-1)
  # Compute the reconstruction loss between the keypoints and the inverse keypoints
  loss = reconstruction_loss(keypoints, inverse_keypoints)
  # Update the keypoint detector network parameters
  keypoint_detector.backward(loss)

# Train the generator network
for each pair of source image and driving video in the training set:
  # Extract the motion representation from the driving video
  motion_representation = []
  for each frame in the driving video:
    # Predict the keypoints and heatmaps for the frame
    keypoints, heatmaps = keypoint_detector(frame)
    # Compute the optical flow between the current frame and the previous frame
    flow = optical_flow(frame, previous_frame)
    # Estimate the local affine transformations for each keypoint using the optical flow
    affine_matrices = estimate_affine(keypoints, flow)
    # Concatenate the heatmaps and affine matrices for each keypoint
    motion_representation.append(concat(heatmaps, affine_matrices))
  
  # Generate a video from the source image and the motion representation
  generated_video = []
  for each motion in motion_representation:
    # Generate a frame from the source image and the motion using the generator network
    generated_frame = generator(source_image, motion)
    # Append the generated frame to the generated video
    generated_video.append(generated_frame)

  # Compute the adversarial loss between the generated video and the driving video
  adv_loss = adversarial_loss(generated_video, driving_video)
  # Compute the perceptual loss between the generated video and the driving video
  perc_loss = perceptual_loss(generated_video, driving_video)
  # Compute the identity loss between the generated video and the source image
  id_loss = identity_loss(generated_video, source_image)
  # Compute the total loss as a weighted sum of the individual losses
  total_loss = lambda_adv * adv_loss + lambda_perc * perc_loss + lambda_id * id_loss
  # Update the generator network parameters
  generator.backward(total_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np
import cv2

# Define some hyperparameters
num_keypoints = 10 # The number of keypoints to detect
num_channels = 3 # The number of channels in the images and videos
image_size = 256 # The size of the images and videos
batch_size = 16 # The size of the mini-batches
num_epochs = 100 # The number of epochs to train
learning_rate = 0.0002 # The learning rate for the optimizer
lambda_adv = 1.0 # The weight for the adversarial loss
lambda_perc = 10.0 # The weight for the perceptual loss
lambda_id = 0.5 * lambda_perc # The weight for the identity loss

# Define the unsupervised keypoint detector network
class KeypointDetector(nn.Module):
  def __init__(self):
    super(KeypointDetector, self).__init__()
    # Define the encoder part of the network
    self.encoder = nn.Sequential(
      nn.Conv2d(num_channels, 32, kernel_size=7, stride=1, padding=3),
      nn.ReLU(),
      nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
      nn.ReLU(),
      nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
      nn.ReLU(),
      nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
      nn.ReLU(),
    )
    # Define the decoder part of the network
    self.decoder = nn.Sequential(
      nn.ConvTranspose2d(256 + num_keypoints + 1, 128, kernel_size=4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(128 + num_keypoints + 1, 64, kernel_size=4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(64 + num_keypoints + 1 , num_channels + num_keypoints + 1 , kernel_size=4 , stride=2 , padding=1),
    )
    # Define the keypoint predictor part of the network
    self.keypoint_predictor = nn.Sequential(
      nn.Conv2d(256 , num_keypoints , kernel_size = 7 , stride = 1 , padding = 3)
    )

  def forward(self , x):
    # Encode the input image
    x = self.encoder(x)
    # Predict the keypoints and heatmaps from the encoded features
    keypoints = self.keypoint_predictor(x)
    heatmaps = F.softmax(keypoints , dim = -1)
    # Concatenate the encoded features , keypoints and heatmaps along the channel dimension
    x = torch.cat([x , keypoints , heatmaps] , dim = 1)
    # Decode the concatenated features to reconstruct the input image and predict an occlusion mask
    x = self.decoder(x)
    reconstruction = x[:, :num_channels]
    occlusion_mask = x[:, num_channels]
    return keypoints , heatmaps , reconstruction , occlusion_mask

# Define the generator network
class Generator(nn.Module):
  def __init__(self):
    super(Generator , self).__init__()
    # Define the occlusion-aware network part of the generator
    self.occlusion_aware_network = nn.Sequential(
      nn.Conv2d(num_channels + num_keypoints * (num_channels + 1) , num_channels * (num_channels + 1) , kernel_size = 3 , stride = 1 , padding = 1),
      nn.ReLU(),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      nn.Conv2d(num_channels * (num_channels + 1) , num_channels , kernel_size = 3 , stride = 1 , padding = 1),
      nn.Sigmoid()
    )
    # Define the appearance transfer network part of the generator
    self.appearance_transfer_network = nn.Sequential(
      nn.Conv2d(num_channels + num_keypoints * (num_channels + 1) , num_channels * (num_channels + 1) , kernel_size = 3 , stride = 1 , padding = 1),
      nn.ReLU(),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      ResidualBlock(num_channels * (num_channels + 1)),
      nn.Conv2d(num_channels * (num_channels + 1) , num_channels , kernel_size = 3 , stride = 1 , padding = 1),
    )

  def forward(self , source_image , motion_representation):
    # Concatenate the source image and the motion representation along the channel dimension
    x = torch.cat([source_image , motion_representation] , dim = 1)
    # Predict the occlusion mask from the occlusion-aware network
    occlusion_mask = self.occlusion_aware_network(x)
    # Warp the source image according to the motion representation
    warped_source_image = warp(source_image , motion_representation)
    # Predict the appearance transfer mask from the appearance transfer network
    appearance_transfer_mask = self.appearance_transfer_network(x)
    # Blend the warped source image and the appearance transfer mask using the occlusion mask
    generated_frame = warped_source_image * occlusion_mask + appearance_transfer_mask * (1 - occlusion_mask)
    return generated_frame

# Define a residual block for the generator network
class ResidualBlock(nn.Module):
  def __init__(self , channels):
    super(ResidualBlock , self).__init__()
    self.conv1 = nn.Conv2d(channels , channels , kernel_size = 3 , stride = 1 , padding = 1)
    self.conv2 = nn.Conv2d(channels , channels , kernel_size = 3 , stride = 1 , padding = 1)
    self.relu = nn.ReLU()

  def forward(self , x):
    residual = x
    x = self.relu(self.conv1(x))
    x = self.conv2(x)
    x += residual
    return x

# Define the discriminator network
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Define the discriminator network
    self.discriminator = nn.Sequential(
      nn.Conv2d(num_channels, 64, kernel_size=4, stride=2, padding=1),
      nn.LeakyReLU(0.2),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
      nn.InstanceNorm2d(128),
      nn.LeakyReLU(0.2),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
      nn.InstanceNorm2d(256),
      nn.LeakyReLU(0.2),
      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
      nn.InstanceNorm2d(512),
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, num_keypoints+3, kernel_size=4, stride=2, padding=0),
    )

  def forward(self, x):
    # Apply the discriminator network to the input video
    x = self.discriminator(x)
    # Return the output logits
    return x

# Define a function to warp an image according to a motion representation
def warp(image, motion_representation):
  # Extract the heatmaps and affine matrices from the motion representation
  heatmaps = motion_representation[:, :num_keypoints]
  affine_matrices = motion_representation[:, num_keypoints:]
  # Reshape the affine matrices to have shape [batch_size, num_keypoints, num_channels+1, num_channels+1]
  affine_matrices = affine_matrices.view(batch_size, num_keypoints, num_channels+1, num_channels+1)
  # Initialize a grid of coordinates for the image
  grid_x, grid_y = torch.meshgrid(torch.arange(image_size), torch.arange(image_size))
  grid_x = grid_x.float().to(device)