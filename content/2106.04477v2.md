---
title: 2106.04477v2 MoCo-Flow  Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras
date: 2021-06-05
---

# [MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras](http://arxiv.org/abs/2106.04477v2)

authors: Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen


## What, Why and How

[1]: https://arxiv.org/pdf/2106.04477v2.pdf "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in ..."
[2]: https://arxiv.org/abs/2106.04477 "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in ..."
[3]: http://export.arxiv.org/abs/2203.04477v2 "[2203.04477v2] On the late-time tails of massive perturbations in ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces **Neural Motion Consensus Flow (MoCo-Flow)**, a representation that models dynamic humans in stationary monocular cameras using a 4D continuous time-variant function.
- **Why**: The paper aims to address the challenging problem of synthesizing novel views of dynamic humans from stationary monocular cameras, which is a specialized but desirable setup that does not require static scenes, controlled environments, or specialized capture hardware. The problem is significantly more under-constrained and ill-posed than techniques that exploit multi-view observations.
- **How**: The paper learns the proposed representation by optimizing for a dynamic scene that minimizes the total rendering error, over all the observed images. The paper proposes a carefully designed optimization scheme, which includes a dedicated initialization step and is constrained by a motion consensus regularization on the estimated motion flow. The paper evaluates MoCo-Flow on several datasets that contain human motions of varying complexity, and compares it to several baselines and ablated variations of the methods, showing the efficacy and merits of the proposed approach.

## Main Contributions

[1]: https://arxiv.org/pdf/2106.04477v2.pdf "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in ..."
[2]: https://arxiv.org/abs/2106.04477 "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in ..."
[3]: http://export.arxiv.org/abs/2203.04477v2 "[2203.04477v2] On the late-time tails of massive perturbations in ..."

According to the paper at [^1^][1], the main contributions are:

- **A novel representation for dynamic humans in stationary monocular cameras** that uses a 4D continuous time-variant function to model the appearance, geometry, and motion of the subjects.
- **A dedicated initialization step** that estimates a coarse motion flow and a canonical static scene from the input video using a neural network and a differentiable renderer.
- **A motion consensus regularization** that enforces consistency and smoothness of the motion flow across different frames and viewpoints.
- **An extensive evaluation** of the proposed method on several datasets that contain human motions of varying complexity, and a comparison with several baselines and ablated variations of the methods, showing the efficacy and merits of the proposed approach.

## Method Summary

[1]: https://arxiv.org/pdf/2106.04477v2.pdf "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in ..."
[2]: https://arxiv.org/abs/2106.04477 "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in ..."
[3]: http://export.arxiv.org/abs/2203.04477v2 "[2203.04477v2] On the late-time tails of massive perturbations in ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a **4D continuous time-variant function** that models the appearance, geometry, and motion of dynamic humans in stationary monocular cameras. The function takes as input a 4D coordinate (x,y,z,t) and outputs a 3D coordinate (r,g,b) that represents the color of the point at that location and time.
- The paper learns the function by **optimizing for a dynamic scene** that minimizes the total rendering error, over all the observed images. The rendering error is defined as the difference between the observed image and the image synthesized by sampling the function at the corresponding coordinates and times.
- The paper introduces a **dedicated initialization step** that estimates a coarse motion flow and a canonical static scene from the input video using a neural network and a differentiable renderer. The motion flow maps each point in the canonical space to its corresponding point in the observation space at each frame. The canonical static scene represents the appearance and geometry of the subjects in a reference frame.
- The paper applies a **motion consensus regularization** that enforces consistency and smoothness of the motion flow across different frames and viewpoints. The regularization term penalizes large deviations of the motion flow from its initial estimate, as well as large variations of the motion flow between neighboring points and frames.
- The paper uses an **iterative optimization scheme** that alternates between optimizing for the function parameters and optimizing for the motion flow. The optimization is performed using gradient-based methods with adaptive learning rates.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a video captured from a stationary monocular camera
# Output: a 4D continuous time-variant function that models the dynamic scene

# Initialization step
motion_flow, canonical_scene = initialize(video) # estimate a coarse motion flow and a canonical static scene using a neural network and a differentiable renderer

# Optimization step
function_parameters = random_initialize() # initialize the function parameters randomly
for iteration in range(max_iterations):
  # Optimize for the function parameters
  for frame in video:
    observation = video[frame] # get the observed image at the current frame
    synthesis = sample(function_parameters, motion_flow[frame]) # synthesize an image by sampling the function at the coordinates and times mapped by the motion flow
    rendering_error = compute_error(observation, synthesis) # compute the rendering error between the observed and synthesized images
    function_parameters = update(function_parameters, rendering_error) # update the function parameters using gradient-based methods
  
  # Optimize for the motion flow
  for frame in video:
    observation = video[frame] # get the observed image at the current frame
    synthesis = sample(function_parameters, motion_flow[frame]) # synthesize an image by sampling the function at the coordinates and times mapped by the motion flow
    rendering_error = compute_error(observation, synthesis) # compute the rendering error between the observed and synthesized images
    motion_consensus = compute_consensus(motion_flow[frame], motion_flow) # compute the motion consensus regularization term that penalizes large deviations and variations of the motion flow
    motion_flow[frame] = update(motion_flow[frame], rendering_error + motion_consensus) # update the motion flow using gradient-based methods

# Return the learned function
return lambda x,y,z,t: sample(function_parameters, (x,y,z,t))
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a video captured from a stationary monocular camera
# Output: a 4D continuous time-variant function that models the dynamic scene

# Define some hyperparameters
max_iterations = 100 # maximum number of iterations for the optimization
learning_rate = 0.01 # learning rate for the gradient-based methods
lambda = 0.1 # weight for the motion consensus regularization term

# Define some helper functions
def initialize(video):
  # Estimate a coarse motion flow and a canonical static scene from the input video using a neural network and a differentiable renderer
  # Input: a video of shape (T, H, W, 3) where T is the number of frames, H and W are the height and width of each frame, and 3 is the number of color channels
  # Output: a motion flow of shape (T, H, W, 4) where each element is a 4D coordinate (x,y,z,t) in the canonical space, and a canonical static scene of shape (H, W, 3) where each element is a 3D color (r,g,b) in the reference frame
  
  # Initialize a neural network that takes an image as input and outputs a depth map and an optical flow map
  network = NeuralNetwork(input_shape=(H, W, 3), output_shape=(H, W, 2))
  
  # Initialize a differentiable renderer that takes a depth map and an optical flow map as input and outputs an image
  renderer = DifferentiableRenderer(input_shape=(H, W, 2), output_shape=(H, W, 3))
  
  # Initialize a motion flow and a canonical static scene randomly
  motion_flow = random_initialize(shape=(T, H, W, 4))
  canonical_scene = random_initialize(shape=(H, W, 3))
  
  # Optimize for the motion flow and the canonical static scene by minimizing the reconstruction error between the input video and the rendered images
  for iteration in range(max_iterations):
    for frame in range(T):
      # Get the observed image at the current frame
      observation = video[frame]
      
      # Get the depth map and the optical flow map from the neural network
      depth_map, optical_flow_map = network(observation)
      
      # Get the rendered image from the differentiable renderer
      synthesis = renderer(depth_map, optical_flow_map)
      
      # Compute the reconstruction error between the observed and rendered images
      reconstruction_error = compute_error(observation, synthesis)
      
      # Update the neural network parameters using gradient-based methods
      network_parameters = network.get_parameters()
      network_gradients = compute_gradients(reconstruction_error, network_parameters)
      network_parameters = update(network_parameters, network_gradients, learning_rate)
      network.set_parameters(network_parameters)
      
      # Update the motion flow using gradient-based methods
      motion_flow_gradients = compute_gradients(reconstruction_error, motion_flow[frame])
      motion_flow[frame] = update(motion_flow[frame], motion_flow_gradients, learning_rate)
      
    # Update the canonical static scene using gradient-based methods
    canonical_scene_gradients = compute_gradients(reconstruction_error, canonical_scene)
    canonical_scene = update(canonical_scene, canonical_scene_gradients, learning_rate)
  
  # Return the motion flow and the canonical static scene
  return motion_flow, canonical_scene

def sample(function_parameters, coordinates):
  # Sample the function at the given coordinates using trilinear interpolation
  # Input: function parameters of shape (N,) where N is the number of parameters, and coordinates of shape (...,4) where each element is a 4D coordinate (x,y,z,t)
  # Output: colors of shape (...,3) where each element is a 3D color (r,g,b) sampled from the function
  
  # Reshape the function parameters into a grid of shape (M,M,M,M) where M is the resolution of the grid
  grid = reshape(function_parameters, shape=(M,M,M,M))
  
  # Get the x,y,z,t components of the coordinates
  x,y,z,t = split(coordinates, axis=-1)
  
  # Normalize the coordinates to be in [0,M-1]
  x,y,z,t = normalize(x,y,z,t)
  
  # Get the lower and upper indices of the grid cells that contain the coordinates
  i0,j0,k0,l0 = floor(x,y,z,t)
  i1,j1,k1,l1 = ceil(x,y,z,t)
  
  # Get the fractional parts of the coordinates
  xd,yd,zd,td = x-i0,y-j0,z-k0,t-l0
  
  # Get the values of the grid at the lower and upper indices
  c0000 = grid[i0,j0,k0,l0]
  c0001 = grid[i0,j0,k0,l1]
  c0010 = grid[i0,j0,k1,l0]
  c0011 = grid[i0,j0,k1,l1]
  c0100 = grid[i0,j1,k0,l0]
  c0101 = grid[i0,j1,k0,l1]
  c0110 = grid[i0,j1,k1,l0]
  c0111 = grid[i0,j1,k1,l1]
  c1000 = grid[i1,j0,k0,l0]
  c1001 = grid[i1,j0,k0,l1]
  c1010 = grid[i1,j0,k1,l0]
  c1011 = grid[i1,j0,k1,l1]
  c1100 = grid[i1,j1,k0,l0]
  c1101 = grid[i1,j1,k0,l1]
  c1110 = grid[i1,j1,k1,l0]
  c1111 = grid[i1,j1,k1,l1]
  
  # Perform trilinear interpolation to get the colors
  colors = (c0000 * (1-xd) + c1000 * xd) * (1-yd) * (1-zd) * (1-td) + \
           (c0001 * (1-xd) + c1001 * xd) * (1-yd) * (1-zd) * td + \
           (c0010 * (1-xd) + c1010 * xd) * (1-yd) * zd * (1-td) + \
           (c0011 * (1-xd) + c1011 * xd) * (1-yd) * zd * td + \
           (c0100 * (1-xd) + c1100 * xd) * yd * (1-zd) * (1-td) + \
           (c010