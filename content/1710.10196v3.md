---
title: 1710.10196v3 Progressive Growing of GANs for Improved Quality, Stability, and Variation
date: 2017-10-11
---

# [Progressive Growing of GANs for Improved Quality, Stability, and Variation](http://arxiv.org/abs/1710.10196v3)

authors: Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen


## What, Why and How

[1]: https://arxiv.org/abs/1710.10196 "[1710.10196] Progressive Growing of GANs for Improved Quality ..."
[2]: http://export.arxiv.org/abs/1710.10196 "[1710.10196] Progressive Growing of GANs for Improved Quality ..."
[3]: https://m.thepaper.cn/baijiahao_18962546 "鄂维南：从数学角度，理解机器学习的“黑魔法”，并应用于更广泛的科学问题"
[4]: https://arxiv.org/pdf/1710.10196v3.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
Progressive Growing of GANs for Improved Quality, Stability, and Variation

What: The paper proposes a new training methodology for generative adversarial networks (GANs), which are a type of neural network that can generate realistic images from random noise. The main idea is to grow both the generator and discriminator networks progressively, starting from a low resolution and adding new layers that model finer details as the training progresses.

Why: The paper aims to improve the quality, stability, and variation of the images generated by GANs, which are often plagued by problems such as mode collapse, artifacts, and blurriness. The paper also introduces a new metric for evaluating GAN results, and constructs a higher-quality version of the CelebA dataset, which is a popular benchmark for face generation.

How: The paper implements the progressive growing technique by using a smooth transition phase between each resolution level, where the new layers are faded in gradually. The paper also uses several tricks to stabilize the training, such as minibatch standard deviation, equalized learning rate, pixelwise feature vector normalization, and heuristic continuation. The paper evaluates the proposed method on several datasets, such as CelebA, LSUN, and CIFAR10, and shows that it can produce high-quality images at 1024x1024 resolution, achieve state-of-the-art inception scores on unsupervised CIFAR10, and increase the variation in generated images.
```


## Main Contributions

The contributions of this paper are:

- A new training methodology for GANs that grows both the generator and discriminator networks progressively, starting from a low resolution and adding new layers that model finer details as the training progresses.
- A new metric for evaluating GAN results, both in terms of image quality and variation, based on perceptual similarity and diversity measures.
- A higher-quality version of the CelebA dataset, which is a popular benchmark for face generation, obtained by applying super-resolution and alignment techniques to the original images.
- Experimental results that demonstrate the effectiveness of the proposed method on several datasets, such as CelebA, LSUN, and CIFAR10, and show that it can produce high-quality images at 1024x1024 resolution, achieve state-of-the-art inception scores on unsupervised CIFAR10, and increase the variation in generated images.

## Method Summary

[1]: https://arxiv.org/abs/1710.10196 "[1710.10196] Progressive Growing of GANs for Improved Quality ..."
[2]: http://export.arxiv.org/abs/1710.10196 "[1710.10196] Progressive Growing of GANs for Improved Quality ..."
[3]: https://arxiv.org/pdf/1710.10196v3.pdf "arXiv.org e-Print archive"

Here is a summary of the method section:

```
The method section of the paper consists of four subsections: progressive growing, minibatch standard deviation, equalized learning rate, and pixelwise feature vector normalization.

- Progressive growing: The authors describe how they implement the progressive growing technique by using a smooth transition phase between each resolution level, where the new layers are faded in gradually using a weight parameter alpha. They also explain how they initialize the new layers with a constant value to avoid sudden changes in the output distribution. They show that this technique improves the stability and quality of the training, and allows them to generate images up to 1024x1024 resolution.

- Minibatch standard deviation: The authors introduce a new layer that computes the standard deviation of the feature maps across the minibatch, and appends it as an extra channel to the input of the discriminator. They argue that this layer helps the discriminator to detect mode collapse, and encourages the generator to produce more diverse images.

- Equalized learning rate: The authors propose a new way to set the learning rate for each layer, based on the He's initializer. They claim that this method helps to balance the magnitude of the gradients across different layers, and avoids undesired correlations between them.

- Pixelwise feature vector normalization: The authors propose a new normalization technique for the generator, where they divide each feature vector by its L2 norm. They claim that this technique helps to stabilize the training by preventing the escalation of signal magnitudes, and also improves the visual quality of the generated images by reducing artifacts and color saturation.
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the generator and discriminator networks
generator = Generator()
discriminator = Discriminator()

# Define the loss function and optimizer
loss = WassersteinLoss()
optimizer = Adam()

# Define the resolution levels and the transition phases
resolutions = [4, 8, 16, 32, 64, 128, 256, 512, 1024]
phases = ["stabilize", "fade"]

# Loop over the resolution levels
for res in resolutions:

  # Loop over the transition phases
  for phase in phases:

    # Adjust the network architectures according to the resolution and phase
    generator.adjust(res, phase)
    discriminator.adjust(res, phase)

    # Loop over the training iterations
    for iter in range(max_iters):

      # Sample random noise vectors
      z = sample_noise(batch_size)

      # Generate fake images from noise vectors
      fake_images = generator(z)

      # Sample real images from the dataset
      real_images = sample_images(batch_size, res)

      # Compute the discriminator outputs for real and fake images
      real_outputs = discriminator(real_images)
      fake_outputs = discriminator(fake_images)

      # Compute the generator and discriminator losses
      gen_loss = loss.generator_loss(fake_outputs)
      dis_loss = loss.discriminator_loss(real_outputs, fake_outputs)

      # Update the generator and discriminator parameters
      optimizer.update(gen_loss, generator.parameters())
      optimizer.update(dis_loss, discriminator.parameters())

      # Update the alpha parameter for the transition phase
      if phase == "fade":
        alpha = min(alpha + alpha_step, 1.0)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Define the generator network
class Generator:

  # Initialize the network with a base resolution and a latent size
  def __init__(self, base_res, latent_size):

    # Define the input layer that takes a noise vector
    self.input = Dense(latent_size)

    # Define the first convolutional block that produces a 4x4 image
    self.conv1 = Conv2D(base_res, kernel_size=4, padding="same")
    self.norm1 = PixelwiseFeatureVectorNorm()
    self.act1 = LeakyReLU(0.2)

    # Define an empty list of convolutional blocks for higher resolutions
    self.convs = []

    # Define an empty list of toRGB layers for each resolution level
    self.toRGBs = []

  # Adjust the network architecture according to the resolution and phase
  def adjust(self, res, phase):

    # If the resolution is higher than the base resolution
    if res > base_res:

      # Compute the index of the current resolution level
      index = int(log2(res / base_res))

      # If the phase is "stabilize"
      if phase == "stabilize":

        # Add a new convolutional block that upsamples the previous output and applies two convolutions
        self.convs.append([
          Upsample(),
          Conv2D(res, kernel_size=3, padding="same"),
          PixelwiseFeatureVectorNorm(),
          LeakyReLU(0.2),
          Conv2D(res, kernel_size=3, padding="same"),
          PixelwiseFeatureVectorNorm(),
          LeakyReLU(0.2)
        ])

        # Add a new toRGB layer that converts the output to RGB image
        self.toRGBs.append(Conv2D(3, kernel_size=1, padding="same"))

      # If the phase is "fade"
      if phase == "fade":

        # Initialize the alpha parameter for the smooth transition
        self.alpha = 0.0

        # Add a new toRGB layer that converts the output to RGB image
        self.toRGBs.append(Conv2D(3, kernel_size=1, padding="same"))

  # Forward pass of the network
  def forward(self, z):

    # Apply the input layer to the noise vector
    x = self.input(z)

    # Reshape the output to a 4x4 feature map
    x = reshape(x, [4, 4])

    # Apply the first convolutional block
    x = self.conv1(x)
    x = self.norm1(x)
    x = self.act1(x)

    # Loop over the convolutional blocks for higher resolutions
    for i in range(len(self.convs)):

      # Apply the i-th convolutional block
      for layer in self.convs[i]:
        x = layer(x)

      # If the phase is "fade" and alpha is less than 1.0
      if phase == "fade" and self.alpha < 1.0:

        # Get the previous resolution level output by downsampling the current output
        x_prev = downsample(x)

        # Convert both outputs to RGB images using the corresponding toRGB layers
        x_rgb = self.toRGBs[i+1](x)
        x_prev_rgb = self.toRGBs[i](x_prev)

        # Interpolate between the two RGB images using alpha as the weight
        x_rgb = (1 - self.alpha) * x_prev_rgb + self.alpha * x_rgb

      # Else, convert the output to RGB image using the last toRGB layer
      else:
        x_rgb = self.toRGBs[-1](x)

    # Return the RGB image as the output of the generator
    return x_rgb

# Define the discriminator network
class Discriminator:

  # Initialize the network with a base resolution and an epsilon value for numerical stability
  def __init__(self, base_res, epsilon):

    # Define an empty list of fromRGB layers for each resolution level
    self.fromRGBs = []

    # Define an empty list of convolutional blocks for higher resolutions
    self.convs = []

    # Define the last convolutional block that reduces a 4x4 image to a single feature map
    self.conv_last = Conv2D(base_res, kernel_size=4, padding="same")
    self.act_last = LeakyReLU(0.2)

    # Define a minibatch standard deviation layer that appends an extra channel to the input
    self.minibatch_stddev = MinibatchStdDev(epsilon)

    # Define an output layer that produces a scalar score for each image
    self.output = Dense(1)

  # Adjust the network architecture according to the resolution and phase
  def adjust(self, res, phase):

    # If the resolution is higher than the base resolution
    if res > base_res:

      # Compute the index of the current resolution level
      index = int(log2(res / base_res))

      # If the phase is "stabilize"
      if phase == "stabilize":

        # Add a new fromRGB layer that converts the RGB image to a feature map
        self.fromRGBs.append(Conv2D(res, kernel_size=1, padding="same"))

        # Add a new convolutional block that applies two convolutions and downsamples the input
        self.convs.append([
          Conv2D(res, kernel_size=3, padding="same"),
          LeakyReLU(0.2),
          Conv2D(res, kernel_size=3, padding="same"),
          LeakyReLU(0.2),
          Downsample()
        ])

      # If the phase is "fade"
      if phase == "fade":

        # Initialize the alpha parameter for the smooth transition
        self.alpha = 0.0

        # Add a new fromRGB layer that converts the RGB image to a feature map
        self.fromRGBs.append(Conv2D(res, kernel_size=1, padding="same"))

  # Forward pass of the network
  def forward(self, x):

    # Get the current resolution level
    res = x.shape[1]

    # If the resolution is higher than the base resolution
    if res > base_res:

      # Compute the index of the current resolution level
      index = int(log2(res / base_res))

      # If the phase is "fade" and alpha is less than 1.0
      if phase == "fade" and self.alpha < 1.0:

        # Convert the input to a feature map using the last fromRGB layer
        y = self.fromRGBs[-1](x)

        # Get the previous resolution level input by upsampling the current input
        x_prev = upsample(x)

        # Convert the previous input to a feature map using the second last fromRGB layer
        y_prev = self.fromRGBs[-2](x_prev)

        # Interpolate between the two feature maps using alpha as the weight
        y = (1 - self.alpha) * y_prev + self.alpha * y

      # Else, convert the input to a feature map using the last fromRGB layer
      else:
        y = self.fromRGBs[-1](x)

      # Apply the corresponding convolutional block
      for layer in self.convs[index-1]:
        y = layer(y)

    # Else, convert the input to a feature map using the first fromRGB layer
    else:
      y = self.fromRGBs[0](x)

    # Loop over the convolutional blocks for lower resolutions in reverse order
    for i in range(len(self.convs)-1, -1, -1):

      # Apply the i-th convolutional block
      for layer in self.convs[i]:
        y = layer(y)

    # Apply the last convolutional block
    y = self.conv_last(y)
    y = self.act_last(y)

    # Apply the minibatch standard deviation layer
    y = self.minibatch_stddev(y)

    # Reshape the output to a 1D vector
    y = flatten(y)

    # Apply the output layer to get the score for each image
    y = self.output(y)

    # Return the score as the output of the discriminator
    return y

# Define a class for pixelwise feature vector normalization
class PixelwiseFeatureVectorNorm:

  # Initialize the layer with an epsilon value for numerical stability
  def __init__(self, epsilon):

    # Store the epsilon value as an attribute
    self.epsilon = epsilon

  # Forward pass of the layer
  def forward(self, x):

    # Compute the L2 norm of each feature vector across the channel dimension
    norm = sqrt(sum(x**2, axis=1) + self.epsilon)

    # Divide each feature vector by its norm
    x = x / norm

    # Return the normalized output
    return x

# Define a class for minibatch standard deviation
class MinibatchStdDev:

  # Initialize the layer with an epsilon value for numerical stability
  def __init__(self, epsilon):

    # Store the epsilon value as an attribute
    self.epsilon = epsilon

  # Forward pass of the layer
  def forward(self, x):

    # Compute the mean and variance of each feature map across the minibatch dimension
    mean = mean(x, axis=0)
    var = mean((x - mean)**2, axis=0)

    # Compute the standard deviation and average it over all pixels and channels
    stddev = sqrt(var + self.epsilon)
    stddev_avg = mean(stddev)

    # Append it as an extra channel to the input
    x = concat(x, stddev_avg