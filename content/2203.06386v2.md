---
title: 2203.06386v2 Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation
date: 2022-03-07
---

# [Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation](http://arxiv.org/abs/2203.06386v2)

authors: Wenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Pascale Fung


## What, Why and How

[1]: https://arxiv.org/pdf/2203.06386 "arXiv:2203.06386v2 [cs.CL] 30 Mar 2022"
[2]: https://arxiv.org/abs/2203.06386 "[2203.06386] Enabling Multimodal Generation on CLIP via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.06386v2 "[2203.06386v2] Enabling Multimodal Generation on CLIP via Vision ..."

Here is a summary of the paper:

```
What: The paper proposes a method to enable multimodal generation on CLIP, a dual-stream vision-language pre-training model, by augmenting it with a pre-trained language model and performing vision-language knowledge distillation.

Why: The paper aims to address the problem that CLIP and other similar models are not capable of multimodal generative tasks due to the weak text encoder, which limits their applications and versatility.

How: The paper leverages a pre-trained language model (PLM) as a teacher and distills its textual knowledge to the text encoder of CLIP, which acts as a student. The distillation is done by minimizing the KL divergence between the teacher and student outputs on a large-scale text corpus. The resulting model can perform multimodal generation tasks such as image captioning and visual question answering in a zero-shot manner, as well as maintain the original language understanding and generation ability of the PLM.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2203.06386 "arXiv:2203.06386v2 [cs.CL] 30 Mar 2022"
[2]: https://arxiv.org/abs/2203.06386 "[2203.06386] Enabling Multimodal Generation on CLIP via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.06386v2 "[2203.06386v2] Enabling Multimodal Generation on CLIP via Vision ..."

Here is a list of the contributions of the paper:

```
- The paper proposes a novel method to enable multimodal generation on CLIP, a dual-stream vision-language pre-training model, by augmenting it with a pre-trained language model and performing vision-language knowledge distillation.
- The paper demonstrates that the proposed method is data- and computation-efficient compared to pre-training from scratch, and can achieve strong zero-shot performance on multimodal generation tasks, such as open-ended visual question answering and image captioning, surpassing the previous state-of-the-art models.
- The paper shows that the proposed method can maintain the original textual language understanding and generation ability of the pre-trained language model, which makes the resulting model versatile for both multimodal and unimodal tasks.
```

## Method Summary

[1]: https://arxiv.org/pdf/2203.06386 "arXiv:2203.06386v2 [cs.CL] 30 Mar 2022"
[2]: https://arxiv.org/abs/2203.06386 "[2203.06386] Enabling Multimodal Generation on CLIP via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.06386v2 "[2203.06386v2] Enabling Multimodal Generation on CLIP via Vision ..."

Here is a summary of the method section of the paper:

```
The method section of the paper consists of three subsections: vision-language knowledge distillation (VLKD), multimodal generation, and experimental setup.

- VLKD: The authors describe how they augment the dual-stream VLP model (CLIP) with a textual pre-trained language model (PLM) via knowledge distillation. They use a large-scale text corpus as the distillation data and minimize the KL divergence between the teacher (PLM) and student (CLIP) outputs on the masked language modeling (MLM) objective. They also introduce a temperature parameter to control the sharpness of the teacher output distribution and a scaling factor to balance the distillation loss and the original contrastive loss of CLIP.

- Multimodal generation: The authors explain how they use the resulting model to perform multimodal generation tasks, such as image captioning and visual question answering, in a zero-shot manner. They use a special token <IMG> to indicate the image input and feed it to both the image encoder and text encoder of CLIP. They then use beam search to generate text conditioned on the image representation.

- Experimental setup: The authors provide details on the data, model, and evaluation metrics used in their experiments. They use BookCorpus (Zhu et al., 2015) as the distillation data, GPT-2 (Radford et al., 2019) as the PLM, and CLIP-RN50x4 (Radford et al., 2021) as the VLP model. They evaluate their model on two multimodal generation tasks: open-ended VQA on VQAv2 (Goyal et al., 2017) and image captioning on COCO (Chen et al., 2015). They also evaluate their model on two unimodal language generation tasks: text summarization on CNN/Daily Mail (Hermann et al., 2015) and text style transfer on Yelp (Li et al., 2018).
```

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```
# Define the dual-stream VLP model (CLIP) and the textual pre-trained language model (PLM)
clip = DualStreamVLP()
plm = PretrainedLM()

# Define the distillation loss function
def distillation_loss(clip_output, plm_output, temperature, scaling_factor):
  # Compute the KL divergence between the teacher and student outputs on MLM objective
  kl_loss = KL(plm_output / temperature, clip_output / temperature)
  # Compute the original contrastive loss of CLIP
  contrastive_loss = clip.contrastive_loss()
  # Return the weighted sum of the two losses
  return scaling_factor * kl_loss + contrastive_loss

# Define the multimodal generation function
def multimodal_generation(image, clip, plm, beam_size):
  # Encode the image with both the image encoder and text encoder of CLIP
  image_repr = clip.image_encoder(image)
  text_repr = clip.text_encoder(<IMG>)
  # Concatenate the image and text representations
  input_repr = concatenate(image_repr, text_repr)
  # Use beam search to generate text conditioned on the input representation
  output_text = beam_search(input_repr, clip, plm, beam_size)
  # Return the output text
  return output_text

# Load the distillation data (BookCorpus)
distillation_data = load_data(BookCorpus)

# Train the model using VLKD
for batch in distillation_data:
  # Mask some tokens in the batch
  masked_batch = mask_tokens(batch)
  # Get the teacher output from PLM
  plm_output = plm(masked_batch)
  # Get the student output from CLIP
  clip_output = clip.text_encoder(masked_batch)
  # Compute the distillation loss
  loss = distillation_loss(clip_output, plm_output, temperature, scaling_factor)
  # Update the model parameters using gradient descent
  clip.update_parameters(loss)

# Evaluate the model on multimodal generation tasks
for task in [VQAv2, COCO]:
  # Load the task data
  task_data = load_data(task)
  # Initialize a list to store the generated texts
  generated_texts = []
  for image in task_data:
    # Generate text for the image using multimodal generation function
    text = multimodal_generation(image, clip, plm, beam_size)
    # Append the text to the list
    generated_texts.append(text)
  # Evaluate the generated texts using task-specific metrics
  evaluate(generated_texts, task)

# Evaluate the model on unimodal language generation tasks
for task in [CNN/Daily Mail, Yelp]:
  # Load the task data
  task_data = load_data(task)
  # Initialize a list to store the generated texts
  generated_texts = []
  for text in task_data:
    # Generate text for the text using PLM
    text = plm.generate(text)
    # Append the text to the list
    generated_texts.append(text)
  # Evaluate the generated texts using task-specific metrics
  evaluate(generated_texts, task)
```


## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2203.06386 "arXiv:2203.06386v2 [cs.CL] 30 Mar 2022"
[2]: https://arxiv.org/abs/2203.06386 "[2203.06386] Enabling Multimodal Generation on CLIP via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.06386v2 "[2203.06386v2] Enabling Multimodal Generation on CLIP via Vision ..."

Here is the detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import transformers
import datasets
import numpy as np

# Define the dual-stream VLP model (CLIP) and the textual pre-trained language model (PLM)
clip = transformers.CLIPModel.from_pretrained("openai/clip-rn50x4")
plm = transformers.AutoModelForCausalLM.from_pretrained("gpt2")

# Define the distillation loss function
def distillation_loss(clip_output, plm_output, temperature, scaling_factor):
  # Compute the KL divergence between the teacher and student outputs on MLM objective
  kl_loss = torch.nn.functional.kl_div(plm_output.log_softmax(dim=-1) / temperature, clip_output.softmax(dim=-1) / temperature, reduction="batchmean")
  # Compute the original contrastive loss of CLIP
  contrastive_loss = clip.loss(clip_output.logits_per_image, clip_output.logits_per_text)
  # Return the weighted sum of the two losses
  return scaling_factor * kl_loss + contrastive_loss

# Define the multimodal generation function
def multimodal_generation(image, clip, plm, beam_size):
  # Encode the image with both the image encoder and text encoder of CLIP
  image_repr = clip.image_encoder(image.unsqueeze(0)).float()
  text_repr = clip.text_encoder(torch.tensor([[49406]])) # <IMG> token id is 49406
  # Concatenate the image and text representations
  input_repr = torch.cat([image_repr, text_repr], dim=1)
  # Use beam search to generate text conditioned on the input representation
  output_text = plm.generate(input_ids=input_repr, num_beams=beam_size)
  # Return the output text
  return output_text

# Load the distillation data (BookCorpus)
distillation_data = datasets.load_dataset("bookcorpus")

# Train the model using VLKD
# Set the hyperparameters
temperature = 2.0 # temperature parameter for distillation loss
scaling_factor = 0.01 # scaling factor for distillation loss
batch_size = 64 # batch size for training
num_epochs = 10 # number of epochs for training
learning_rate = 1e-4 # learning rate for training
# Create a data loader for distillation data
data_loader = torch.utils.data.DataLoader(distillation_data["train"], batch_size=batch_size, shuffle=True)
# Create an optimizer for updating model parameters
optimizer = torch.optim.Adam(clip.parameters(), lr=learning_rate)
# Set the model to training mode
clip.train()
for epoch in range(num_epochs):
  # Loop over batches of data
  for batch in data_loader:
    # Mask some tokens in the batch using a random mask probability of 0.15
    masked_batch = transformers.DataCollatorForLanguageModeling(tokenizer=plm.tokenizer, mlm_probability=0.15)(batch)
    # Get the teacher output from PLM
    plm_output = plm(masked_batch["input_ids"]).logits
    # Get the student output from CLIP
    clip_output = clip.text_encoder(masked_batch["input_ids"])
    # Compute the distillation loss
    loss = distillation_loss(clip_output, plm_output, temperature, scaling_factor)
    # Zero out the gradients
    optimizer.zero_grad()
    # Backpropagate the loss
    loss.backward()
    # Update the model parameters using gradient descent
    optimizer.step()
  # Print the epoch and loss information
  print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Evaluate the model on multimodal generation tasks
# Set the hyperparameters
beam_size = 5 # beam size for generation
max_length = 20 # maximum length for generation

for task in [VQAv2, COCO]:
  # Load the task data
  task_data = datasets.load_dataset(task)["validation"]
  # Create a data loader for task data
  data_loader = torch.utils.data.DataLoader(task_data, batch_size=1, shuffle=False)
  # Initialize a list to store the generated texts and references
  generated_texts = []
  references = []
  for image in data_loader:
    # Generate text for the image using multimodal generation function
    text = multimodal_generation(image, clip, plm, beam_size)
    # Append the text to the list of generated texts
    generated_texts.append(text)
    # Append the reference text to the list of references (depending on the task)
    if task == VQAv2:
      references.append(image["answer"])
    elif task == COCO:
      references.append(image["caption"])
  
  # Evaluate the generated texts using task-specific metrics
  if task == VQAv2:
    # Use accuracy as the metric for VQAv2
    accuracy = np.mean([generated_texts[i] == references[i] for i in range(len(generated_texts))])
    print(f"Accuracy on {task}: {accuracy}")
  elif task == COCO:
    # Use BLEU as the metric for COCO
    bleu = datasets.load_metric("bleu")
    score = bleu.compute(predictions=generated_texts, references=references)
    print(f"BLEU on {task}: {score}")

# Evaluate the model on unimodal language generation tasks
# Set the hyperparameters
beam_size = 5 # beam size for generation
max_length = 100 # maximum length for generation

for task in [CNN/Daily Mail, Yelp]:
  # Load the task data
  task_data = datasets.load_dataset(task)["validation"]
  # Create a data loader for task data
  data_loader = torch.utils.data.DataLoader(task_data, batch_size=1, shuffle=False)
  # Initialize a list to store the generated texts and references
  generated_texts = []
  references = []
  for text in data_loader:
    # Generate text for the text using PLM
    text = plm.generate(text, num_beams=beam_size, max_length=max_length)
    # Append the text to the list of generated texts
    generated_texts.append(text)
    # Append the reference text to the list of references (depending on the task)
    if task == CNN/Daily Mail:
      references.append(text["summary"])
    elif task == Yelp:
      references.append(text["target"])
  
  # Evaluate the generated texts using task-specific metrics
  if task == CNN/Daily Mail:
    # Use ROUGE as the metric for CNN/Daily Mail
    rouge = datasets.load_metric("rouge")
    score = rouge.compute(predictions=generated_texts, references=references)
    print(f"ROUGE on {task}: {score}")
  elif task == Yelp:
    # Use BLEU as the metric for Yelp
    bleu = datasets.load_metric("bleu")
    score = bleu.compute(predictions=generated_texts, references=references)
    print(f"BLEU on {task}: {score}")
```