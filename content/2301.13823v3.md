---
title: 2301.13823v3 Grounding Language Models to Images for Multimodal Inputs and Outputs
date: 2023-01-14
---

# [Grounding Language Models to Images for Multimodal Inputs and Outputs](http://arxiv.org/abs/2301.13823v3)

authors: Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried


## What, Why and How

[1]: https://arxiv.org/pdf/2301.13823v3.pdf "Abstract arXiv:2301.13823v3 [cs.CL] 1 Jun 2023"
[2]: https://arxiv.org/abs/2301.13823 "[2301.13823] Grounding Language Models to Images for ... - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2301.13823v3 "[2301.13823v3] Grounding Language Models to Images for Multimodal ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data.
- **Why**: The paper aims to leverage the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation, and equip them with exciting new vision-and-language capabilities such as multimodal dialogue, generation, and contextual image retrieval from conversations.
- **How**: The paper keeps the language model frozen, and finetunes input and output linear layers to enable cross-modality interactions. The input layer maps visual embeddings to the language model's input space, while the output layer maps the language model's output logits to a set of candidate images. The paper evaluates the method on several grounded tasks such as captioning, dialogue, and retrieval, and showcases compelling interactive abilities.

## Main Contributions

Some of the contributions of this paper are:

- It introduces a simple and efficient method to ground pretrained text-only language models to the visual domain, without modifying or retraining the language model.
- It demonstrates that the method can process and generate arbitrarily interleaved image-and-text data, and achieve strong zero-shot performance on several grounded tasks such as captioning, dialogue, and retrieval.
- It showcases the interactive abilities of the method, such as generating coherent text outputs interleaved with relevant retrieved images, and answering questions based on visual inputs.

## Method Summary

The method section of the paper describes the proposed approach to ground a pretrained text-only language model to the visual domain. The approach consists of three main components:

- A visual encoder that extracts visual embeddings from images using a pretrained convolutional neural network (CNN).
- An input layer that maps the visual embeddings to the language model's input space using a learned linear transformation.
- An output layer that maps the language model's output logits to a set of candidate images using a learned linear transformation.

The paper also explains how the input and output layers are finetuned on a multimodal dataset, and how the method handles arbitrarily interleaved image-and-text inputs and outputs. The paper also provides details on the implementation and hyperparameters of the method.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a pretrained text-only language model (LM)
LM = load_pretrained_language_model()

# Define a pretrained visual encoder (VE)
VE = load_pretrained_visual_encoder()

# Define input and output linear layers (IL and OL)
IL = Linear(input_size, LM.hidden_size)
OL = Linear(LM.hidden_size, output_size)

# Finetune IL and OL on a multimodal dataset
for batch in multimodal_dataset:
  # Get image-and-text inputs and outputs
  inputs, outputs = batch
  # Extract visual embeddings from images using VE
  visual_embeddings = VE(inputs.images)
  # Map visual embeddings to LM input space using IL
  lm_inputs = IL(visual_embeddings)
  # Concatenate lm_inputs with text inputs
  lm_inputs = concatenate(lm_inputs, inputs.texts)
  # Get LM output logits
  lm_logits = LM(lm_inputs)
  # Map LM output logits to candidate images using OL
  image_logits = OL(lm_logits)
  # Compute loss and update parameters of IL and OL
  loss = compute_loss(image_logits, outputs.images)
  update_parameters(IL, OL, loss)

# Use the method to process and generate interleaved image-and-text data
for input in interleaved_image_text_data:
  # Extract visual embeddings from images using VE
  visual_embeddings = VE(input.images)
  # Map visual embeddings to LM input space using IL
  lm_inputs = IL(visual_embeddings)
  # Concatenate lm_inputs with text inputs
  lm_inputs = concatenate(lm_inputs, input.texts)
  # Get LM output logits
  lm_logits = LM(lm_inputs)
  # Map LM output logits to candidate images using OL
  image_logits = OL(lm_logits)
  # Generate text outputs from LM logits
  text_outputs = generate_text_from_logits(lm_logits)
  # Retrieve images from image logits
  image_outputs = retrieve_images_from_logits(image_logits)
  # Interleave text and image outputs
  outputs = interleave(text_outputs, image_outputs)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import transformers

# Define constants
BATCH_SIZE = 32
EPOCHS = 10
LEARNING_RATE = 1e-4
INPUT_SIZE = 2048 # visual embedding size
OUTPUT_SIZE = 1000 # number of candidate images
LM_HIDDEN_SIZE = 768 # language model hidden size
LM_VOCAB_SIZE = 50257 # language model vocabulary size

# Load a pretrained text-only language model (LM) and tokenizer
LM = transformers.GPT2LMHeadModel.from_pretrained("gpt2")
TOKENIZER = transformers.GPT2Tokenizer.from_pretrained("gpt2")

# Load a pretrained visual encoder (VE)
VE = torchvision.models.resnet50(pretrained=True)
# Remove the last fully connected layer
VE.fc = torch.nn.Identity()

# Define input and output linear layers (IL and OL)
IL = torch.nn.Linear(INPUT_SIZE, LM_HIDDEN_SIZE)
OL = torch.nn.Linear(LM_HIDDEN_SIZE, OUTPUT_SIZE)

# Define an optimizer and a loss function
OPTIMIZER = torch.optim.Adam(params=[IL.parameters(), OL.parameters()], lr=LEARNING_RATE)
LOSS_FN = torch.nn.CrossEntropyLoss()

# Load a multimodal dataset
MULTIMODAL_DATASET = load_multimodal_dataset()
# Create a data loader
MULTIMODAL_DATALOADER = torch.utils.data.DataLoader(MULTIMODAL_DATASET, batch_size=BATCH_SIZE, shuffle=True)

# Finetune IL and OL on the multimodal dataset
for epoch in range(EPOCHS):
  for batch in MULTIMODAL_DATALOADER:
    # Get image-and-text inputs and outputs
    inputs, outputs = batch
    # Extract visual embeddings from images using VE
    visual_embeddings = VE(inputs.images)
    # Map visual embeddings to LM input space using IL
    lm_inputs = IL(visual_embeddings)
    # Concatenate lm_inputs with text inputs
    lm_inputs = torch.cat([lm_inputs, TOKENIZER(inputs.texts, return_tensors="pt").input_ids], dim=1)
    # Get LM output logits
    lm_logits = LM(lm_inputs).logits
    # Map LM output logits to candidate images using OL
    image_logits = OL(lm_logits)
    # Compute loss and update parameters of IL and OL
    loss = LOSS_FN(image_logits, outputs.images)
    OPTIMIZER.zero_grad()
    loss.backward()
    OPTIMIZER.step()

# Use the method to process and generate interleaved image-and-text data
for input in interleaved_image_text_data:
  # Extract visual embeddings from images using VE
  visual_embeddings = VE(input.images)
  # Map visual embeddings to LM input space using IL
  lm_inputs = IL(visual_embeddings)
  # Concatenate lm_inputs with text inputs
  lm_inputs = torch.cat([lm_inputs, TOKENIZER(input.texts, return_tensors="pt").input_ids], dim=1)
  # Get LM output logits
  lm_logits = LM(lm_inputs).logits
  # Map LM output logits to candidate images using OL
  image_logits = OL(lm_logits)
  # Generate text outputs from LM logits
  text_outputs = TOKENIZER.decode(torch.argmax(lm_logits, dim=-1))
  # Retrieve images from image logits
  image_outputs = retrieve_images_from_logits(image_logits)
  # Interleave text and image outputs
  outputs = interleave(text_outputs, image_outputs)
```