---
title: 1912.04958v2 Analyzing and Improving the Image Quality of StyleGAN
date: 2019-12-05
---

# [Analyzing and Improving the Image Quality of StyleGAN](http://arxiv.org/abs/1912.04958v2)

authors: Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila


## What, Why and How

[1]: https://arxiv.org/abs/1912.04958 "[1912.04958] Analyzing and Improving the Image Quality of StyleGAN"
[2]: https://arxiv.org/pdf/1912.04958v2.pdf "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/1912.04958.pdf "arXiv.org e-Print archive"

Sure, I can try to summarize the paper for you. Here is my attempt:

```
The paper is about analyzing and improving the image quality of StyleGAN, a state-of-the-art method for generating realistic images.

The paper's main contributions are:

- What: The paper identifies and fixes two types of artifacts in StyleGAN: blob-like artifacts caused by improper normalization, and phase artifacts caused by progressive growing.
- Why: The paper aims to improve the image quality and diversity of StyleGAN, as well as make it easier to invert the generator and attribute images to a specific network.
- How: The paper proposes three changes in StyleGAN: replacing AdaIN with a new normalization method called weight demodulation, revisiting progressive growing with a smooth resolution transition, and regularizing the generator with a path length penalty that encourages good conditioning.
```

## Main Contributions

The paper's contributions are:

- A new normalization method for the generator that removes blob-like artifacts and improves image quality.
- A smooth resolution transition for progressive growing that eliminates phase artifacts and stabilizes training.
- A path length regularizer for the generator that improves conditioning, inversion, and attribution of images.

## Method Summary

The method section of the paper describes the three proposed changes in detail:

- Weight demodulation: The paper replaces AdaIN with a new normalization method that modulates the weights of the convolutional layers instead of the feature maps. This ensures that the variance of the feature maps is independent of the styles, and avoids creating blob-like artifacts. The paper also introduces a scaling factor to control the strength of the modulation.
- Smooth resolution transition: The paper revisits progressive growing and proposes a smooth transition between resolutions instead of a hard switch. This is achieved by using a weighted average of two resolutions during the transition phase, and gradually changing the weights over time. This avoids creating phase artifacts and improves the stability of training.
- Path length regularizer: The paper regularizes the generator by penalizing large changes in the output image with respect to small changes in the intermediate latent code. This encourages the generator to have good conditioning and smoothness in the latent space, which makes it easier to invert the generator and attribute images to a specific network. The paper also introduces a heuristic to adapt the regularization strength dynamically.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```
# Define the generator network
def generator(z):
  # Map the input latent code z to an intermediate latent code w
  w = mapping_network(z)
  # Initialize the output image x
  x = None
  # Loop over the resolution levels from low to high
  for r in resolutions:
    # Modulate the weights of the convolutional layer with the style w
    w_mod = weight_modulation(w, conv_layer[r])
    # Apply the convolutional layer to the output image x
    x = conv_layer[r](x, w_mod)
    # Demodulate the weights to normalize the feature maps
    x = weight_demodulation(x, w_mod)
    # Add noise to the output image x
    x = x + noise[r]
    # Apply a non-linearity to the output image x
    x = activation(x)
    # If in transition phase, blend the output image x with the previous resolution
    if transition_phase:
      alpha = get_alpha() # Get the blending factor
      x_prev = upsample(x_prev) # Upsample the previous resolution
      x = alpha * x + (1 - alpha) * x_prev # Blend the two resolutions
    # Store the output image x for the next iteration
    x_prev = x
  # Return the final output image x
  return x

# Define the path length regularizer
def path_length_regularizer(z):
  # Compute the output image x and its Jacobian J with respect to w
  x, J = jacobian(generator, z)
  # Compute the path length L as the mean squared norm of J
  L = mean(square(norm(J)))
  # Compute the penalty as the absolute difference between L and a target value E
  penalty = abs(L - E)
  # Update E using a moving average of L
  E = update_ema(E, L)
  # Return the penalty
  return penalty

# Define the training loop
def train():
  # Initialize the generator and discriminator networks
  G = generator()
  D = discriminator()
  # Initialize the optimizers and loss functions
  opt_G = optimizer_G()
  opt_D = optimizer_D()
  loss_G = loss_G()
  loss_D = loss_D()
  # Loop over the training iterations
  for i in iterations:
    # Sample a batch of latent codes z
    z = sample_z(batch_size)
    # Generate a batch of fake images x_fake using G
    x_fake = G(z)
    # Sample a batch of real images x_real from the data
    x_real = sample_data(batch_size)
    # Compute the discriminator outputs for real and fake images
    y_real = D(x_real)
    y_fake = D(x_fake)
    # Compute the generator and discriminator losses
    L_G = loss_G(y_fake)
    L_D = loss_D(y_real, y_fake)
    # Compute the path length regularizer for G
    R_G = path_length_regularizer(z)
    # Update G and D using gradient descent
    opt_G.step(L_G + R_G)
    opt_D.step(L_D)

```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch # PyTorch
import torch.nn as nn # Neural network module
import torch.nn.functional as F # Functional module
import torch.optim as optim # Optimizer module
import torchvision # Vision module
import numpy as np # NumPy

# Define some hyperparameters
z_dim = 512 # Dimension of the input latent code z
w_dim = 512 # Dimension of the intermediate latent code w
n_mapping = 8 # Number of layers in the mapping network
n_channel = 3 # Number of channels in the output image
n_noise = 1 # Number of noise inputs per resolution level
n_style = 2 # Number of style inputs per resolution level
resolutions = [4, 8, 16, 32, 64, 128, 256, 512, 1024] # List of resolution levels
channels = [512, 512, 512, 512, 256, 128, 64, 32, 16] # List of channel numbers per resolution level
batch_size = 16 # Batch size for training
lr_G = 0.002 # Learning rate for generator
lr_D = 0.002 # Learning rate for discriminator
beta1 = 0.0 # Beta1 for Adam optimizer
beta2 = 0.99 # Beta2 for Adam optimizer
ema_decay = 0.999 # Decay rate for exponential moving average
pl_mean = 0.0 # Initial value for path length mean
pl_decay = 0.01 # Decay rate for path length mean update
pl_weight = 2.0 # Weight for path length regularizer

# Define a custom layer for weight modulation and demodulation
class ModulatedConv2d(nn.Module):
  def __init__(self, in_channel, out_channel, kernel_size, style_dim):
    super().__init__()
    # Initialize the convolutional weight and bias
    self.weight = nn.Parameter(torch.randn(out_channel, in_channel, kernel_size, kernel_size))
    self.bias = nn.Parameter(torch.zeros(out_channel))
    # Initialize the style modulation layer
    self.modulation = nn.Linear(style_dim, in_channel)
    self.modulation.bias.data.fill_(1) # Initialize the bias to 1

  def forward(self, x, style):
    # Get the batch size and number of channels
    batch_size, in_channel = x.shape[0], x.shape[1]
    # Modulate the weight with the style
    style = self.modulation(style).view(batch_size, in_channel, 1, 1)
    weight = self.weight * style 
    # Demodulate the weight by dividing by its norm
    demod = torch.rsqrt(weight.pow(2).sum([1,2,3]) + 1e-8).view(batch_size, -1, 1, 1)
    weight = weight * demod 
    weight = weight.view(batch_size * out_channel, in_channel, kernel_size, kernel_size)
    # Reshape the input x for convolution
    x = x.view(1, batch_size * in_channel, *x.shape[2:])
    # Apply the convolution with the modulated and demodulated weight
    x = F.conv2d(x, weight, padding=kernel_size//2)
    x = x.view(batch_size * out_channel)
    x += self.bias.view(1,-1)
    return x

# Define a custom layer for noise injection
class NoiseInjection(nn.Module):
  def __init__(self):
    super().__init__()
    # Initialize the noise strength as a learnable parameter
    self.weight = nn.Parameter(torch.zeros(1))

  def forward(self,x ,noise):
    return x + self.weight * noise

# Define a custom layer for constant input
class ConstantInput(nn.Module):
  def __init__(self,in_channel):
    super().__init__()
    # Initialize a constant input as a learnable parameter 
    self.input = nn.Parameter(torch.randn(1,in_channel))

  def forward(self,batch_size):
    return self.input.repeat(batch_size)

# Define the generator network
class Generator(nn.Module):
  
```