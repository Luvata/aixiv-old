---
title: 2106.02795v3 Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding
date: 2021-06-03
---

# [Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding](http://arxiv.org/abs/2106.02795v3)

authors: Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, Samy Bengio


## What, Why and How

[1]: https://arxiv.org/pdf/2106.02795v3.pdf "arXiv:2106.02795v3 [cs.LG] 9 Nov 2021"
[2]: https://arxiv.org/abs/2106.02795 "[2106.02795] Learnable Fourier Features for Multi-Dimensional Spatial ..."
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2106.02795?context=cs "[2106.02795] Learnable Fourier Features for Multi-Dimensional Spatial ..."

Here is a summary of the paper based on the search results:

- **What**: The paper proposes a novel positional encoding method based on learnable Fourier features for attention-based deep model architectures such as Transformer.
- **Why**: The paper aims to address sequences or images where the position of information matters, and to capture distances or more complex positional relationships in a spatial multi-dimensional position, e.g., pixel positions on an image.
- **How**: The paper represents each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron. The paper evaluates the proposed method on several public benchmark tasks and shows that it outperforms existing methods by both improving the accuracy and allowing faster convergence.

## Main Contributions

[1]: https://arxiv.org/pdf/2106.02795v3.pdf "arXiv:2106.02795v3 [cs.LG] 9 Nov 2021"
[2]: https://arxiv.org/abs/2106.02795 "[2106.02795] Learnable Fourier Features for Multi ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2106.05979 "[2106.05979] Two-loop QCD penguin contribution to the width difference ..."

According to the search results, the main contributions of this paper are:

- It proposes a novel positional encoding method based on learnable Fourier features for attention-based deep model architectures such as Transformer.
- It shows that the proposed method can capture distances or more complex positional relationships in a spatial multi-dimensional position, e.g., pixel positions on an image, which are not well captured by existing methods.
- It evaluates the proposed method on several public benchmark tasks, such as image classification, image generation, and object detection, and shows that it outperforms existing methods by both improving the accuracy and allowing faster convergence.

## Method Summary

[1]: https://arxiv.org/pdf/2106.02795v3.pdf "arXiv:2106.02795v3 [cs.LG] 9 Nov 2021"
[2]: https://arxiv.org/abs/2106.02795 "[2106.02795] Learnable Fourier Features for Multi ... - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2106.02795?context=cs "[2106.02795] Learnable Fourier Features for Multi-Dimensional Spatial ..."

Here is a summary of the method section based on the search results:

- The paper proposes a novel positional encoding method based on learnable Fourier features, which are inspired by the Fourier feature mapping [12].
- The paper defines the learnable Fourier feature mapping as a function that maps a position vector x to a high-dimensional vector z, where z is obtained by applying a linear transformation W to x and then applying a sinusoidal function element-wise.
- The paper further modulates the learnable Fourier feature mapping with a multi-layer perceptron (MLP) to obtain the final positional encoding, which can capture more complex positional relationships than the sinusoidal function alone.
- The paper applies the proposed positional encoding method to different attention-based deep model architectures, such as Transformer [38], Vision Transformer [16], and DETR [3], and shows how to integrate it with different input modalities, such as text, image, and video.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the learnable Fourier feature mapping function
def LFFM(x, W):
  # x: position vector of shape [batch_size, dim]
  # W: learnable weight matrix of shape [dim, num_features]
  # return: Fourier feature vector of shape [batch_size, num_features]
  z = sin(W.T @ x) # apply linear transformation and sinusoidal function
  return z

# Define the positional encoding function
def PE(x, MLP):
  # x: position vector of shape [batch_size, dim]
  # MLP: multi-layer perceptron with learnable parameters
  # return: positional encoding vector of shape [batch_size, hidden_size]
  z = LFFM(x, W) # apply learnable Fourier feature mapping
  p = MLP(z) # apply multi-layer perceptron
  return p

# Define the attention-based deep model architecture
def Model(inputs):
  # inputs: input data of different modalities, such as text, image, or video
  # return: output predictions of different tasks, such as classification, generation, or detection
  x = Embedding(inputs) # embed the input data into a vector space
  p = PE(x, MLP) # obtain the positional encoding for each input position
  h = x + p # combine the input embedding and the positional encoding
  h = Attention(h) # apply attention mechanism to the combined representation
  y = Output(h) # generate output predictions based on the attention output
  return y
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the hyperparameters
dim = 512 # dimension of the input embedding and the positional encoding
num_features = 256 # number of features for the Fourier feature mapping
hidden_size = 1024 # hidden size of the multi-layer perceptron
num_heads = 8 # number of attention heads
num_layers = 6 # number of attention layers
dropout = 0.1 # dropout rate

# Define the learnable Fourier feature mapping function
def LFFM(x, W):
  # x: position vector of shape [batch_size, dim]
  # W: learnable weight matrix of shape [dim, num_features]
  # return: Fourier feature vector of shape [batch_size, num_features]
  z = torch.sin(torch.matmul(x, W)) # apply linear transformation and sinusoidal function
  return z

# Define the multi-layer perceptron class
class MLP(nn.Module):
  def __init__(self, input_size, hidden_size, output_size, dropout):
    super(MLP, self).__init__()
    self.linear1 = nn.Linear(input_size, hidden_size) # first linear layer
    self.linear2 = nn.Linear(hidden_size, output_size) # second linear layer
    self.dropout = nn.Dropout(dropout) # dropout layer

  def forward(self, x):
    # x: input vector of shape [batch_size, input_size]
    # return: output vector of shape [batch_size, output_size]
    x = self.linear1(x) # apply first linear layer
    x = F.gelu(x) # apply gelu activation function
    x = self.dropout(x) # apply dropout
    x = self.linear2(x) # apply second linear layer
    return x

# Define the positional encoding function
def PE(x, W, MLP):
  # x: position vector of shape [batch_size, dim]
  # W: learnable weight matrix of shape [dim, num_features]
  # MLP: multi-layer perceptron with learnable parameters
  # return: positional encoding vector of shape [batch_size, dim]
  z = LFFM(x, W) # apply learnable Fourier feature mapping
  p = MLP(z) # apply multi-layer perceptron
  return p

# Define the scaled dot-product attention function
def ScaledDotProductAttention(Q, K, V, mask):
  # Q: query matrix of shape [batch_size, num_heads, seq_len, dim // num_heads]
  # K: key matrix of shape [batch_size, num_heads, seq_len, dim // num_heads]
  # V: value matrix of shape [batch_size, num_heads, seq_len, dim // num_heads]
  # mask: attention mask of shape [batch_size, num_heads, seq_len, seq_len] or None
  # return: attention output matrix of shape [batch_size, num_heads, seq_len, dim // num_heads] and attention weights matrix of shape [batch_size, num_heads, seq_len, seq_len]
  d_k = Q.size(-1) # get the dimension of the query and key vectors
  scores = torch.matmul(Q, K.transpose(-2,-1)) / np.sqrt(d_k) # compute the scaled dot-product scores of shape [batch_size, num_heads, seq_len, seq_len]
  if mask is not None:
    scores = scores.masked_fill(mask == 0, -1e9) # apply the mask to the scores if not None
  weights = F.softmax(scores, dim=-1) # compute the softmax normalized weights of shape [batch_size, num_heads, seq_len, seq_len]
  weights = F.dropout(weights, dropout) # apply dropout to the weights
  output = torch.matmul(weights,V) # compute the attention output by weighted sum of the values of shape [batch_size,num_heads ,seq_len ,dim // num_heads]
  return output ,weights

# Define the multi-head attention class
class MultiHeadAttention(nn.Module):
  def __init__(self,dim,num_heads ,dropout):
    super(MultiHeadAttention,self).__init__()
    self.dim = dim # dimension of the input and output vectors
    self.num_heads = num_heads # number of attention heads
    self.dropout = dropout # dropout rate

    assert dim % num_heads ==0 , "dim must be divisible by num_heads"

    self.d_k = dim // num_heads # dimension of each query and key vector

    self.W_Q = nn.Linear(dim,dim) # linear layer for query projection
    self.W_K = nn.Linear(dim,dim) # linear layer for key projection
    self.W_V = nn.Linear(dim,dim) # linear layer for value projection
    self.W_O = nn.Linear(dim,dim) # linear layer for output projection

  def forward(self,x,mask=None):
    # x: input vector of shape [batch_size,seq_len,dim]
    # mask: attention mask of shape [batch_size,seq_len,seq_len] or None
    # return: attention output vector of shape [batch_size,seq_len,dim] and attention weights matrix of shape [batch_size,num_heads,seq_len,seq_len]

    batch_size ,seq_len ,_ = x.size() # get the batch size and sequence length

    Q = self.W_Q(x) # project the input to the query space of shape [batch_size,seq_len,dim]
    K = self.W_K(x) # project the input to the key space of shape [batch_size,seq_len,dim]
    V = self.W_V(x) # project the input to the value space of shape [batch_size,seq_len,dim]

    Q = Q.view(batch_size,self.num_heads,seq_len,self.d_k) # reshape the query matrix to [batch_size,num_heads,seq_len,d_k]
    K = K.view(batch_size,self.num_heads,seq_len,self.d_k) # reshape the key matrix to [batch_size,num_heads,seq_len,d_k]
    V = V.view(batch_size,self.num_heads,seq_len,self.d_k) # reshape the value matrix to [batch_size,num_heads,seq_len,d_k]

    if mask is not None:
      mask = mask.unsqueeze(1) # unsqueeze the mask to [batch_size,1,seq_len,seq_len] to match the number of heads

    output ,weights = ScaledDotProductAttention(Q,K,V,mask) # apply scaled dot-product attention of shape [batch_size,num_heads ,seq_len ,d_k] and [batch_size,num_heads ,seq_len ,seq_len]

    output = output.transpose(1,2).contiguous().view(batch_size,seq_len,-1) # reshape the output to [batch_size,seq_len,dim]

    output = self.W_O(output) # project the output to the original dimension of shape [batch_size,seq_len,dim]

    return output ,weights

# Define the feed-forward network class
class FeedForwardNetwork(nn.Module):
  def __init__(self,dim ,hidden_size ,dropout):
    super(FeedForwardNetwork,self).__init__()
    self.linear1 = nn.Linear(dim ,hidden_size) # first linear layer
    self.linear2 = nn.Linear(hidden_size ,dim) # second linear layer
    self.dropout = nn.Dropout(dropout) # dropout layer

  def forward(self,x):
    # x: input vector of shape [batch_size,seq_len,dim]
    # return: output vector of shape [batch_size,seq_len,dim]
    x = self.linear1(x) # apply first linear layer
    x = F.gelu(x) # apply gelu activation function
    x = self.dropout(x) # apply dropout
    x = self.linear2(x) # apply second linear layer
    return x

# Define the layer normalization class
class LayerNorm(nn.Module):
  def __init__(self,dim ,eps=1e-6):
    super(LayerNorm,self).__init__()
    self.gamma = nn.Parameter(torch.ones(dim)) # learnable scale parameter
    self.beta = nn.Parameter(torch.zeros(dim)) # learnable shift parameter
    self.eps = eps # small constant for numerical stability

  def forward(self,x):
    # x: input vector of shape [batch_size,seq_len,dim]
    # return: normalized output vector of shape [batch_size,seq_len,dim]
    mean = x.mean(dim=-1 ,keepdim=True) # compute the mean along the last dimension
    std = x.std(dim=-1 ,keepdim=True) # compute the standard deviation along the last dimension
    x = (x - mean) / (std + self.eps) # normalize the input by subtracting the mean and dividing by the standard deviation
    x = self.gamma * x + self.beta # apply the scale and shift parameters
    return x

# Define the residual connection class
class ResidualConnection(nn.Module):
  def __init__(self,dim ,dropout):
    super(ResidualConnection,self).__init__()
    self.norm = LayerNorm(dim) # layer normalization layer
    self.dropout = nn.Dropout(dropout) # dropout layer

  def forward(self,x ,sublayer):
    # x: input vector of shape [batch_size,seq_len,dim]
    # sublayer: a function that applies a sublayer to the input, such as attention or feed-forward network
    # return: output vector of shape [batch_size,seq_len,dim]