---
title: 2306.04121v1 Matte Anything  Interactive Natural Image Matting with Segment Anything Models
date: 2023-06-05
---

# [Matte Anything: Interactive Natural Image Matting with Segment Anything Models](http://arxiv.org/abs/2306.04121v1)

authors: Jingfeng Yao, Xinggang Wang, Lang Ye, Wenyu Liu


## What, Why and How

[1]: https://arxiv.org/pdf/2306.04121v1.pdf "arXiv:2306.04121v1 [cs.CV] 7 Jun 2023"
[2]: https://arxiv.org/abs/2306.04121 "[2306.04121] Matte Anything: Interactive Natural Image Matting with ..."
[3]: http://export.arxiv.org/abs/2306.04121 "[2306.04121] Matte Anything: Interactive Natural Image Matting with ..."

Here is a summary of the paper:

- **What**: The paper proposes Matte Anything model (MatAny), an interactive natural image matting model that can produce high-quality transparency maps (alpha-mattes) with various simple hints from the user, such as scribbles, clicks, or bounding boxes[^1^][1].
- **Why**: The paper aims to address the issue of labor-intensive and time-consuming production of trimaps, which are hint maps that divide an image into foreground, background, and unknown regions for image matting[^1^][1].
- **How**: The paper leverages task-specific vision models to enhance the performance of natural image matting. Specifically, it uses the segment anything model (SAM) to predict high-quality contour with user interaction and an open-vocabulary (OV) detector to predict the transparency of any object. Subsequently, a pretrained image matting model generates alpha mattes with pseudo trimaps that are automatically generated with contour and transparency prediction[^1^][1] [^2^][2].


## Main Contributions

[1]: https://arxiv.org/pdf/2306.04121v1.pdf "arXiv:2306.04121v1 [cs.CV] 7 Jun 2023"
[2]: https://arxiv.org/abs/2306.04121 "[2306.04121] Matte Anything: Interactive Natural Image Matting with ..."
[3]: http://export.arxiv.org/abs/2306.04121 "[2306.04121] Matte Anything: Interactive Natural Image Matting with ..."

According to the paper, the main contributions are:

- **MatAny**: A novel interactive natural image matting model that can produce high-quality alpha-mattes with various simple hints from the user, such as scribbles, clicks, or bounding boxes[^1^][1].
- **Pseudo trimap generation**: A key insight of MatAny that can generate pseudo trimaps automatically with contour and transparency prediction, without requiring manual labeling[^1^][1].
- **Task-specific vision models**: A combination of orthogonal vision models that enhance the performance of natural image matting, including the segment anything model (SAM) for contour prediction, the open-vocabulary (OV) detector for transparency prediction, and a pretrained image matting model for alpha-matte generation[^1^][1] [^2^][2].
- **Evaluation and comparison**: A comprehensive evaluation of MatAny against several current image matting algorithms on various datasets and metrics, demonstrating the significant potential of MatAny[^1^][1].


## Method Summary

[1]: https://arxiv.org/pdf/2306.04121v1.pdf "arXiv:2306.04121v1 [cs.CV] 7 Jun 2023"
[2]: https://arxiv.org/abs/2306.04121 "[2306.04121] Matte Anything: Interactive Natural Image Matting with ..."
[3]: http://export.arxiv.org/abs/2306.04121 "[2306.04121] Matte Anything: Interactive Natural Image Matting with ..."

Here is a summary of the method section of the paper:

- The paper describes the Matte Anything model (MatAny), which consists of three main components: user interaction, pseudo trimap generation, and natural image matting[^1^][1].
- User interaction: The paper supports various types of user interaction methods, such as scribbles, clicks, or bounding boxes, to indicate the object of interest for image matting. The paper uses the segment anything model (SAM) to predict the contour of the object with user interaction. SAM is a pretrained model that can segment any object with a few clicks or strokes[^1^][1].
- Pseudo trimap generation: The paper generates pseudo trimaps automatically with contour and transparency prediction. The paper uses an open-vocabulary (OV) detector to predict the transparency of any object. OV detector is a pretrained model that can detect any object with a bounding box and a class name. The paper combines the contour and transparency predictions to generate pseudo trimaps that divide an image into foreground, background, and unknown regions[^1^][1].
- Natural image matting: The paper uses a pretrained image matting model to generate alpha mattes with pseudo trimaps. The paper adopts the DIM model [43] as the image matting model, which is a state-of-the-art deep learning model for natural image matting. The paper feeds both the original image and the pseudo trimap as input to the DIM model and obtains the alpha matte as output[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Matte Anything model (MatAny)
class MatAny:
  # Initialize the model with pretrained models
  def __init__(self):
    self.SAM = load_segment_anything_model()
    self.OV = load_open_vocabulary_detector()
    self.DIM = load_deep_image_matting_model()

  # Generate alpha matte with user interaction and image
  def generate_alpha_matte(self, user_interaction, image):
    # Predict contour with user interaction and SAM
    contour = self.SAM.predict(user_interaction, image)
    # Predict transparency with image and OV
    transparency = self.OV.predict(image)
    # Generate pseudo trimap with contour and transparency
    pseudo_trimap = self.generate_pseudo_trimap(contour, transparency)
    # Generate alpha matte with image and pseudo trimap and DIM
    alpha_matte = self.DIM.predict(image, pseudo_trimap)
    # Return alpha matte
    return alpha_matte

  # Generate pseudo trimap with contour and transparency
  def generate_pseudo_trimap(self, contour, transparency):
    # Initialize pseudo trimap as a zero matrix
    pseudo_trimap = zeros_like(contour)
    # Set foreground region as 1 where contour is 1
    pseudo_trimap[contour == 1] = 1
    # Set background region as 0 where transparency is 0
    pseudo_trimap[transparency == 0] = 0
    # Set unknown region as 0.5 where neither foreground nor background
    pseudo_trimap[(pseudo_trimap != 1) & (pseudo_trimap != 0)] = 0.5
    # Return pseudo trimap
    return pseudo_trimap
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the Matte Anything model (MatAny)
class MatAny:
  # Initialize the model with pretrained models
  def __init__(self):
    # Load the segment anything model (SAM) from https://github.com/hustvl/SAM
    self.SAM = torch.hub.load('hustvl/SAM', 'sam_resnet50', pretrained=True)
    # Load the open vocabulary detector (OV) from https://github.com/hustvl/OV
    self.OV = torch.hub.load('hustvl/OV', 'ov_resnet50', pretrained=True)
    # Load the deep image matting model (DIM) from https://github.com/foamliu/Deep-Image-Matting-PyTorch
    self.DIM = torch.hub.load('foamliu/Deep-Image-Matting-PyTorch', 'dim_resnet50', pretrained=True)

  # Generate alpha matte with user interaction and image
  def generate_alpha_matte(self, user_interaction, image):
    # Convert user interaction and image to tensors
    user_interaction = torch.from_numpy(user_interaction).float()
    image = torch.from_numpy(image).float()
    # Normalize user interaction and image
    user_interaction = user_interaction / 255.0
    image = image / 255.0
    # Predict contour with user interaction and SAM
    contour = self.SAM.predict(user_interaction, image)
    # Predict transparency with image and OV
    transparency = self.OV.predict(image)
    # Generate pseudo trimap with contour and transparency
    pseudo_trimap = self.generate_pseudo_trimap(contour, transparency)
    # Generate alpha matte with image and pseudo trimap and DIM
    alpha_matte = self.DIM.predict(image, pseudo_trimap)
    # Return alpha matte
    return alpha_matte

  # Generate pseudo trimap with contour and transparency
  def generate_pseudo_trimap(self, contour, transparency):
    # Initialize pseudo trimap as a zero tensor
    pseudo_trimap = torch.zeros_like(contour)
    # Set foreground region as 1 where contour is 1
    pseudo_trimap[contour == 1] = 1
    # Set background region as 0 where transparency is 0
    pseudo_trimap[transparency == 0] = 0
    # Set unknown region as 0.5 where neither foreground nor background
    pseudo_trimap[(pseudo_trimap != 1) & (pseudo_trimap != 0)] = 0.5
    # Return pseudo trimap
    return pseudo_trimap

# Define a function to load an image from a file path and resize it to a fixed size
def load_image(file_path, size=(320, 320)):
  # Read the image using OpenCV
  image = cv2.imread(file_path)
  # Convert the image from BGR to RGB color space
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  # Resize the image to the given size using bilinear interpolation
  image = cv2.resize(image, size, interpolation=cv2.INTER_LINEAR)
  # Return the image as a numpy array
  return np.array(image)

# Define a function to generate a user interaction from a bounding box and an image size
def generate_user_interaction(bbox, size=(320, 320)):
  # Initialize user interaction as a zero matrix of the given size and one channel
  user_interaction = np.zeros((size[0], size[1], 1), dtype=np.uint8)
  # Draw a rectangle on the user interaction using the bounding box coordinates and a white color (255)
  user_interaction = cv2.rectangle(user_interaction, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255), -1)
  # Return the user interaction as a numpy array
  return np.array(user_interaction)

# Define a function to save an alpha matte to a file path as a grayscale image
def save_alpha_matte(alpha_matte, file_path):
  # Convert alpha matte from tensor to numpy array and detach it from computation graph
  alpha_matte = alpha_matte.numpy().detach()
  # Rescale alpha matte from [0, 1] to [0, 255]
  alpha_matte = alpha_matte * 255.0
  # Convert alpha matte to uint8 type
  alpha_matte = alpha_matte.astype(np.uint8)
  # Save alpha matte as a grayscale image using OpenCV
  cv2.imwrite(file_path, alpha_matte)

# Create an instance of the Matte Anything model
matany = MatAny()

# Load an image from a file path
image = load_image('image.jpg')

# Generate a user interaction from a bounding box
user_interaction = generate_user_interaction([50, 50, 200, 200])

# Generate an alpha matte with the image and user interaction
alpha_matte = matany.generate_alpha_matte(user_interaction, image)

# Save the alpha matte to a file path
save_alpha_matte(alpha_matte, 'alpha_matte.jpg')
```