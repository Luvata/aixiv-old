---
title: 2006.11239v2 Denoising Diffusion Probabilistic Models
date: 2020-06-12
---

# [Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2006.11239v2)

authors: Jonathan Ho, Ajay Jain, Pieter Abbeel


## What, Why and How

[1]: https://arxiv.org/abs/2006.11239 "[2006.11239] Denoising Diffusion Probabilistic Models - arXiv.org"
[2]: https://arxiv.org/pdf/2006.11239v2.pdf "Denoising Diffusion Probabilistic Models - arXiv.org"
[3]: https://www.researchgate.net/publication/371123922_Negative-prompt_Inversion_Fast_Image_Inversion_for_Editing_with_Text-guided_Diffusion_Models "Negative-prompt Inversion: Fast Image Inversion for ... - ResearchGate"

Here is a summary of the paper:

- **What**: The paper presents a novel method for image synthesis using diffusion probabilistic models, a class of latent variable models inspired by nonequilibrium thermodynamics[^1^][1].
- **Why**: The paper aims to overcome the limitations of existing generative models such as GANs, autoregressive models, flows, and VAEs, and to achieve high quality samples with low computational cost and flexible sampling schemes[^1^][1].
- **How**: The paper introduces a weighted variational bound based on a connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and shows how to train and sample from these models using a progressive lossy decompression scheme that generalizes autoregressive decoding[^1^][1]. The paper also demonstrates the effectiveness of the proposed method on various image datasets and compares it with existing methods[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- A novel weighted variational bound for training diffusion probabilistic models that is derived from denoising score matching with Langevin dynamics.
- A progressive lossy decompression scheme for sampling from diffusion probabilistic models that can be seen as a generalization of autoregressive decoding.
- State-of-the-art results on unconditional CIFAR10 and high quality samples on 256x256 LSUN using diffusion probabilistic models.
- A comparison of diffusion probabilistic models with other generative models in terms of sample quality, computational cost, and sampling flexibility.

## Method Summary

[1]: https://arxiv.org/abs/2006.11239 "[2006.11239] Denoising Diffusion Probabilistic Models - arXiv.org"
[2]: https://arxiv.org/pdf/2006.11239v2.pdf "Denoising Diffusion Probabilistic Models - arXiv.org"
[3]: https://pubmed.ncbi.nlm.nih.gov/15447808/ "How to write the methods section of a research paper - PubMed"

Here is a summary of the method section of the paper:

- The method section consists of four subsections: diffusion probabilistic models, denoising score matching with Langevin dynamics, weighted variational bound, and sampling with progressive lossy decompression[^1^][2].
- In the first subsection, the paper defines diffusion probabilistic models as a class of latent variable models that generate data by reversing a Markov chain that gradually adds noise to the data until reaching a simple prior distribution[^1^][2].
- In the second subsection, the paper shows how to train diffusion probabilistic models using denoising score matching with Langevin dynamics, which is a method for estimating the score function (i.e., the gradient of the log-density) of a distribution from noisy samples[^1^][2].
- In the third subsection, the paper derives a weighted variational bound for training diffusion probabilistic models that is tighter than the standard variational bound and matches the denoising score matching objective in the limit[^1^][2].
- In the fourth subsection, the paper proposes a progressive lossy decompression scheme for sampling from diffusion probabilistic models that can be seen as a generalization of autoregressive decoding, where each sample is obtained by iteratively applying a learned denoising function to a noisy version of the previous sample[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion probabilistic model
class DiffusionModel:
  def __init__(self, T, beta):
    # T: number of diffusion steps
    # beta: noise level schedule
    self.T = T
    self.beta = beta

  def forward(self, x):
    # x: data sample
    # Return: a sequence of noisy samples from x0 to xT
    xt = x
    samples = [xt]
    for t in range(1, self.T + 1):
      # Add Gaussian noise to xt
      epsilon = torch.randn_like(xt)
      xt = sqrt(1 - self.beta[t]) * xt + sqrt(self.beta[t]) * epsilon
      samples.append(xt)
    return samples

  def reverse(self, xT):
    # xT: sample from the prior distribution
    # Return: a sample from the data distribution
    xt = xT
    for t in reversed(range(1, self.T + 1)):
      # Apply a learned denoising function to xt
      xt = denoise(xt, t)
    return xt

# Define the denoising score matching objective
def score_matching_loss(xt, t):
  # xt: noisy sample at step t
  # t: diffusion step index
  # Return: the score matching loss for xt
  # Estimate the score of xt using a neural network
  score_hat = score_net(xt, t)
  # Compute the true score of xt analytically
  score_true = (xt - sqrt(1 - self.beta[t]) * denoise(xt, t)) / self.beta[t]
  # Compute the loss as the squared error between the estimated and true scores
  loss = torch.mean((score_hat - score_true) ** 2)
  return loss

# Define the weighted variational bound objective
def variational_loss(x0, xT):
  # x0: data sample
  # xT: noisy sample at step T
  # Return: the variational loss for x0 and xT
  # Compute the log-likelihood of x0 under the data distribution using a neural network
  logp_x0 = likelihood_net(x0)
  # Compute the log-likelihood of xT under the prior distribution analytically
  logp_xT = torch.sum(-0.5 * (xT ** 2 + log(2 * pi)), dim=-1)
  # Compute the KL divergence between the diffusion process and the reverse process using a neural network
  kl = kl_net(x0, xT)
  # Compute the loss as a weighted combination of the negative log-likelihoods and the KL divergence
  loss = torch.mean(-logp_x0 - logp_xT + kl)
  return loss

# Define the progressive lossy decompression scheme for sampling
def sample():
  # Return: a sample from the data distribution
  # Sample from the prior distribution (a standard Gaussian)
  xT = torch.randn(batch_size, data_dim)
  # Reverse the diffusion process using the learned denoising function
  x0 = DiffusionModel.reverse(xT)
  return x0

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
batch_size = 64 # batch size for training and sampling
data_dim = 3 * 32 * 32 # dimension of the data (CIFAR10 images)
T = 1000 # number of diffusion steps
beta_min = 1e-5 # minimum noise level
beta_max = 0.02 # maximum noise level
beta_schedule = "cosine" # noise level schedule ("linear" or "cosine")
score_hidden_dim = 256 # hidden dimension of the score network
score_num_layers = 8 # number of layers of the score network
likelihood_hidden_dim = 256 # hidden dimension of the likelihood network
likelihood_num_layers = 8 # number of layers of the likelihood network
kl_hidden_dim = 256 # hidden dimension of the KL network
kl_num_layers = 8 # number of layers of the KL network
denoise_hidden_dim = 256 # hidden dimension of the denoise network
denoise_num_layers = 8 # number of layers of the denoise network
learning_rate = 1e-4 # learning rate for optimization
num_epochs = 100 # number of epochs for training

# Define the data loader for CIFAR10 dataset
transform = transforms.Compose([
    transforms.ToTensor(), # convert images to tensors
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalize images to [-1, 1] range
])
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

# Define the diffusion probabilistic model class
class DiffusionModel(nn.Module):
  def __init__(self, T, beta):
    super(DiffusionModel, self).__init__()
    # T: number of diffusion steps
    # beta: noise level schedule
    self.T = T
    self.beta = beta

  def forward(self, x):
    # x: data sample (shape: [batch_size, data_dim])
    # Return: a sequence of noisy samples from x0 to xT (shape: [T + 1, batch_size, data_dim])
    xt = x
    samples = [xt]
    for t in range(1, self.T + 1):
      # Add Gaussian noise to xt (shape: [batch_size, data_dim])
      epsilon = torch.randn_like(xt)
      xt = torch.sqrt(1 - self.beta[t]) * xt + torch.sqrt(self.beta[t]) * epsilon
      samples.append(xt)
    return torch.stack(samples) # stack the samples along a new dimension

  def reverse(self, xT):
    # xT: sample from the prior distribution (shape: [batch_size, data_dim])
    # Return: a sample from the data distribution (shape: [batch_size, data_dim])
    xt = xT
    for t in reversed(range(1, self.T + 1)):
      # Apply a learned denoising function to xt (shape: [batch_size, data_dim])
      xt = denoise(xt, t)
    return xt

# Define the noise level schedule function
def get_beta(T, beta_min, beta_max, beta_schedule):
  # T: number of diffusion steps
  # beta_min: minimum noise level
  # beta_max: maximum noise level
  # beta_schedule: noise level schedule ("linear" or "cosine")
  # Return: a tensor of noise levels from beta_1 to beta_T (shape: [T + 1])
  if beta_schedule == "linear":
    # Use a linear schedule from beta_min to beta_max
    beta_t = torch.linspace(beta_min, beta_max, T) 
  elif beta_schedule == "cosine":
    # Use a cosine schedule from beta_min to beta_max
    alpha_t = torch.linspace(0, pi / 2, T) 
    beta_t = beta_min + (beta_max - beta_min) * torch.sin(alpha_t) ** 2 
  else:
    raise ValueError("Invalid beta schedule")
  # Prepend beta_0 (zero noise) to the schedule
  beta_0 = torch.tensor([0.0]) 
  return torch.cat([beta_0, beta_t])

# Define the score network class
class ScoreNet(nn.Module):
  def __init__(self, data_dim, hidden_dim, num_layers):
    super(ScoreNet, self).__init__()
    # data_dim: dimension of the data
    # hidden_dim: hidden dimension of the network
    # num_layers: number of layers of the network
    self.data_dim = data_dim
    self.hidden_dim = hidden_dim
    self.num_layers = num_layers
    # Define the input layer
    self.input_layer = nn.Linear(data_dim + 1, hidden_dim) # add 1 for the diffusion step index
    # Define the hidden layers
    self.hidden_layers = nn.ModuleList()
    for i in range(num_layers - 1):
      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))
    # Define the output layer
    self.output_layer = nn.Linear(hidden_dim, data_dim) # output the score vector

  def forward(self, x, t):
    # x: noisy sample at step t (shape: [batch_size, data_dim])
    # t: diffusion step index (scalar)
    # Return: the estimated score of x (shape: [batch_size, data_dim])
    # Concatenate x and t along the last dimension
    xt = torch.cat([x, torch.full_like(x[:, :1], t)], dim=-1) 
    # Pass xt through the input layer and apply ReLU activation
    h = torch.relu(self.input_layer(xt))
    # Pass h through the hidden layers and apply ReLU activation
    for layer in self.hidden_layers:
      h = torch.relu(layer(h))
    # Pass h through the output layer and return the score vector
    score = self.output_layer(h)
    return score

# Define the likelihood network class
class LikelihoodNet(nn.Module):
  def __init__(self, data_dim, hidden_dim, num_layers):
    super(LikelihoodNet, self).__init__()
    # data_dim: dimension of the data
    # hidden_dim: hidden dimension of the network
    # num_layers: number of layers of the network
    self.data_dim = data_dim
    self.hidden_dim = hidden_dim
    self.num_layers = num_layers
    # Define the input layer
    self.input_layer = nn.Linear(data_dim, hidden_dim)
    # Define the hidden layers
    self.hidden_layers = nn.ModuleList()
    for i in range(num_layers - 1):
      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))
    # Define the output layer
    self.output_layer = nn.Linear(hidden_dim, 1) # output a scalar log-likelihood

  def forward(self, x):
    # x: data sample (shape: [batch_size, data_dim])
    # Return: the log-likelihood of x under the data distribution (shape: [batch_size])
    # Pass x through the input layer and apply ReLU activation
    h = torch.relu(self.input_layer(x))
    # Pass h through the hidden layers and apply ReLU activation
    for layer in self.hidden_layers:
      h = torch.relu(layer(h))
    # Pass h through the output layer and return the log-likelihood scalar
    logp = self.output_layer(h).squeeze()
    return logp

# Define the KL network class
class KLNet(nn.Module):
  def __init__(self, data_dim, hidden_dim, num_layers):
    super(KLNet, self).__init__()
    # data_dim: dimension of the data
    # hidden_dim: hidden dimension of the network
    # num_layers: number of layers of the network
    self.data_dim = data_dim
    self.hidden_dim = hidden_dim
    self.num_layers = num_layers