---
title: 2204.12696v1 Grasping the Arrow of Time from the Singularity  Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN
date: 2022-04-13
---

# [Grasping the Arrow of Time from the Singularity: Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN](http://arxiv.org/abs/2204.12696v1)

authors: Qiucheng Wu, Yifan Jiang, Junru Wu, Kai Wang, Gong Zhang, Humphrey Shi, Zhangyang Wang, Shiyu Chang


## What, Why and How

[1]: https://arxiv.org/abs/2204.12696 "[2204.12696] Grasping the Arrow of Time from the ... - arXiv.org"
[2]: https://arxiv.org/pdf/2207.12696v1 "Advanced Conditional Variational Autoencoders (A-CVAE): Towards arXiv ..."
[3]: http://export.arxiv.org/abs/2211.12696v1 "[2211.12696v1] Analytic construction of sphaleron-like solution ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper studies the motion features in the latent space of StyleGAN, a generative model for realistic and controllable image editing. It proposes a technique to extract low-rank spaces that represent small, local movements (called "micromotion") from the latent space of StyleGAN, using text or video clips as anchors. It shows that these micromotion features can be easily manipulated and transferred across different face images and domains.
- **Why**: The paper aims to explore whether StyleGAN knows anything about temporal motion, as it was only trained on static images. It also wants to demonstrate that micromotion features can be used for direct and effortless image editing, with high robustness, low computational overhead, and impressive domain transferability.
- **How**: The paper uses a pre-trained StyleGAN-v2 model for face generation and applies singular value decomposition (SVD) to extract low-rank spaces from the latent space of StyleGAN. It then uses text or video clips as anchors to decode the editing direction for micromotion features, such as expression, head movement, and aging effect. It applies an affine transformation over the latent feature of a target face image to generate its micromotion variations. It also shows that the same micromotion subspace can be transferred to other unseen face images, even those from vastly different domains (such as oil painting, cartoon, and sculpture faces). It presents various examples of applying its technique to manipulate faces and compares its results with existing methods.

## Main Contributions

According to the paper, its main contributions are:

- It hypothesizes and demonstrates that a series of meaningful, natural, and versatile micromotion features can be represented in low-rank spaces extracted from the latent space of a conventionally pre-trained StyleGAN-v2 model for face generation, with the guidance of proper anchors in the form of either short text or video clips.
- It shows that such micromotion subspace, even learned from just single target face, can be painlessly transferred to other unseen face images, even those from vastly different domains. It reveals that the local feature geometry corresponding to one type of micromotion is aligned across different face subjects, and hence that StyleGAN-v2 is indeed "secretly" aware of the subject-disentangled feature variations caused by that micromotion.
- It presents various successful examples of applying its low-dimensional micromotion subspace technique to directly and effortlessly manipulate faces, showing high robustness, low computational overhead, and impressive domain transferability. It also proposes a new metric for open-domain dialogues, which can objectively evaluate the interpretability of the latent space distribution.

## Method Summary

[1]: https://arxiv.org/abs/2204.12696 "[2204.12696] Grasping the Arrow of Time from the ... - arXiv.org"
[2]: https://arxiv.org/abs/2204.07756 "Visual Attention Methods in Deep Learning: An In-Depth Survey"
[3]: http://export.arxiv.org/abs/2211.12696v1 "[2211.12696v1] Analytic construction of sphaleron-like solution ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper uses a pre-trained StyleGAN-v2 model for face generation and applies singular value decomposition (SVD) to extract low-rank spaces from the latent space of StyleGAN. It then uses text or video clips as anchors to decode the editing direction for micromotion features, such as expression, head movement, and aging effect. It applies an affine transformation over the latent feature of a target face image to generate its micromotion variations. It also shows that the same micromotion subspace can be transferred to other unseen face images, even those from vastly different domains (such as oil painting, cartoon, and sculpture faces).
- The paper describes the details of its technique in four steps: 1) extracting low-rank spaces from StyleGAN latent space using SVD; 2) decoding micromotion editing direction from text or video anchors; 3) applying affine transformation to latent feature for micromotion generation; and 4) transferring micromotion subspace across different face images and domains.
- The paper also proposes a new metric for open-domain dialogues, which can objectively evaluate the interpretability of the latent space distribution. It defines the metric as the ratio of the variance explained by the first principal component (PC1) to the total variance of the latent space. It argues that a higher ratio indicates a more interpretable latent space, as PC1 captures most of the information and variation in the data.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load a pre-trained StyleGAN-v2 model for face generation
model = load_stylegan_v2()

# Define a target face image and its latent feature
target_image = load_image()
target_feature = model.encode(target_image)

# Define an anchor in the form of text or video clip
anchor = load_anchor()

# Extract low-rank spaces from StyleGAN latent space using SVD
low_rank_spaces = svd(model.latent_space)

# Decode micromotion editing direction from anchor
editing_direction = decode(anchor, low_rank_spaces)

# Apply affine transformation to latent feature for micromotion generation
micromotion_feature = affine_transform(target_feature, editing_direction)

# Generate micromotion image from micromotion feature
micromotion_image = model.decode(micromotion_feature)

# Transfer micromotion subspace to another face image or domain
other_image = load_other_image()
other_feature = model.encode(other_image)
other_micromotion_feature = affine_transform(other_feature, editing_direction)
other_micromotion_image = model.decode(other_micromotion_feature)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import torch
import torchvision
import dlib
import face_recognition
import cv2

# Load a pre-trained StyleGAN-v2 model for face generation
model = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-512', pretrained=True, useGPU=True)

# Define a target face image and its latent feature
target_image = cv2.imread('target.jpg')
target_feature = model.buildNoiseData(1) # random latent feature

# Define an anchor in the form of text or video clip
anchor = 'smile' # text anchor

# Extract low-rank spaces from StyleGAN latent space using SVD
latent_space = model.buildNoiseData(1000) # random latent features
u, s, v = np.linalg.svd(latent_space, full_matrices=False) # SVD decomposition
low_rank_spaces = u[:, :10] # keep the first 10 singular vectors

# Decode micromotion editing direction from anchor
if isinstance(anchor, str): # text anchor
  # Use a pre-trained classifier to predict the expression label from the anchor text
  classifier = load_expression_classifier()
  label = classifier.predict(anchor)
  # Use a pre-trained regressor to predict the expression intensity from the anchor text
  regressor = load_expression_regressor()
  intensity = regressor.predict(anchor)
  # Find the latent features that match the expression label and intensity using the classifier and regressor
  matching_features = []
  for feature in latent_space:
    image = model.test(feature)
    pred_label = classifier.predict(image)
    pred_intensity = regressor.predict(image)
    if pred_label == label and pred_intensity == intensity:
      matching_features.append(feature)
  # Compute the average of the matching features as the editing direction
  editing_direction = np.mean(matching_features, axis=0)
elif isinstance(anchor, np.ndarray): # video clip anchor
  # Use face_recognition library to extract face landmarks from the video frames
  landmarks = []
  for frame in anchor:
    face_locations = face_recognition.face_locations(frame)
    face_landmarks = face_recognition.face_landmarks(frame, face_locations)
    landmarks.append(face_landmarks[0]) # assume only one face in the video
  # Use dlib library to compute the optical flow of the face landmarks across the video frames
  optical_flow = []
  for i in range(len(landmarks) - 1):
    prev_landmarks = landmarks[i]
    next_landmarks = landmarks[i + 1]
    prev_points = np.array(list(prev_landmarks.values())).reshape(-1, 2)
    next_points = np.array(list(next_landmarks.values())).reshape(-1, 2)
    flow = dlib.correlation_tracker()
    flow.start_track(prev_points, next_points)
    optical_flow.append(flow.get_position())
  # Compute the average of the optical flow as the editing direction
  editing_direction = np.mean(optical_flow, axis=0)

# Apply affine transformation to latent feature for micromotion generation
micromotion_feature = target_feature + editing_direction

# Generate micromotion image from micromotion feature
micromotion_image = model.test(micromotion_feature)

# Transfer micromotion subspace to another face image or domain
other_image = cv2.imread('other.jpg')
other_feature = model.buildNoiseData(1) # random latent feature
other_micromotion_feature = other_feature + editing_direction
other_micromotion_image = model.test(other_micromotion_feature)
```