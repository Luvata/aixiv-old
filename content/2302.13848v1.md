---
title: 2302.13848v1 ELITE  Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation
date: 2023-02-14
---

# [ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation](http://arxiv.org/abs/2302.13848v1)

authors: Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, Wangmeng Zuo


## What, Why and How

[1]: https://arxiv.org/pdf/2302.13848v1 "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized ..."
[2]: https://arxiv.org/abs/2302.13848 "[2302.13848] ELITE: Encoding Visual Concepts into Textual Embeddings ..."
[3]: https://arxiv.org/pdf/2301.13848v1 "arXiv:2301.13848v1 [cs.CL] 31 Jan 2023"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a learning-based encoder for customized text-to-image generation, which can encode visual concepts into textual embeddings that can be used to synthesize or edit images with text prompts.
- **Why**: The paper aims to address the limitations of existing optimization-based methods for customized text-to-image generation, which are slow, memory-intensive, and prone to mode collapse or semantic drift.
- **How**: The paper introduces a global mapping network and a local mapping network to encode visual concepts into textual embeddings. The global mapping network projects the hierarchical features of an image into multiple "new" words in the textual word embedding space, one for the primary concept and others for auxiliary concepts. The local mapping network injects the encoded patch features into cross attention layers to provide omitted details. The paper evaluates the proposed method on various user-defined concepts and shows that it achieves higher fidelity, robustness, and speed than optimization-based methods.


## Main Contributions

[1]: https://arxiv.org/pdf/2302.13848v1 "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized ..."
[2]: https://arxiv.org/abs/2302.13848 "[2302.13848] ELITE: Encoding Visual Concepts into Textual Embeddings ..."
[3]: https://arxiv.org/pdf/2301.13848v1 "arXiv:2301.13848v1 [cs.CL] 31 Jan 2023"

According to the paper at [^1^][1], the main contributions are:

- **A learning-based encoder for customized text-to-image generation**, which can encode visual concepts into textual embeddings that can be used to synthesize or edit images with text prompts.
- **A global mapping network and a local mapping network** to encode visual concepts into textual embeddings. The global mapping network projects the hierarchical features of an image into multiple "new" words in the textual word embedding space, one for the primary concept and others for auxiliary concepts. The local mapping network injects the encoded patch features into cross attention layers to provide omitted details.
- **Extensive experiments and comparisons** with prior optimization-based approaches on a variety of user-defined concepts, demonstrating that the proposed method enables more high-fidelity inversion and robust editability with a significantly faster encoding process.


## Method Summary

[1]: https://arxiv.org/pdf/2302.13848v1 "ELITE: Encoding Visual Concepts into Textual Embeddings for Customized ..."
[2]: https://arxiv.org/abs/2302.13848 "[2302.13848] ELITE: Encoding Visual Concepts into Textual Embeddings ..."
[3]: https://arxiv.org/pdf/2301.13848v1 "arXiv:2301.13848v1 [cs.CL] 31 Jan 2023"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a learning-based encoder for customized text-to-image generation, which consists of a global mapping network and a local mapping network.
- The global mapping network takes an image as input and extracts hierarchical features using a pre-trained VGG-19 network. Then, it projects the features into multiple "new" words in the textual word embedding space, one for the primary concept and others for auxiliary concepts. The primary word is used to represent the main visual concept of the image, while the auxiliary words are used to exclude irrelevant disturbances (e.g., background). The projection is done by minimizing the distance between the image features and the word embeddings in a joint space.
- The local mapping network takes the same image as input and divides it into patches. Then, it encodes each patch feature into a textual embedding using a fully connected layer. The patch embeddings are injected into cross attention layers of a pre-trained text-to-image diffusion model to provide omitted details for the image generation. The injection is done by replacing some of the original text embeddings with the patch embeddings according to their similarity scores.
- The paper trains the encoder using a two-stage strategy. In the first stage, it trains the global mapping network using a large-scale image-text dataset (Conceptual Captions). In the second stage, it trains the local mapping network using a small set of user-provided images for each concept. The paper uses gradient descent to optimize the encoder parameters.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the encoder
encoder = Encoder(global_mapping_network, local_mapping_network)

# Define the text-to-image diffusion model
diffusion_model = DiffusionModel(pretrained=True)

# Train the encoder
# Stage 1: train the global mapping network
for image, text in image_text_dataset:
  # Extract hierarchical features from image
  features = global_mapping_network.extract_features(image)
  # Project features into word embeddings
  words = global_mapping_network.project_features(features)
  # Compute loss between words and text
  loss = compute_loss(words, text)
  # Update global mapping network parameters
  global_mapping_network.update_parameters(loss)

# Stage 2: train the local mapping network
for concept, images in user_defined_concepts:
  for image in images:
    # Divide image into patches
    patches = local_mapping_network.divide_image(image)
    # Encode patch features into textual embeddings
    patch_embeddings = local_mapping_network.encode_patches(patches)
    # Inject patch embeddings into cross attention layers of diffusion model
    diffusion_model.inject_patch_embeddings(patch_embeddings)
    # Generate image from text and patch embeddings
    generated_image = diffusion_model.generate_image(text, patch_embeddings)
    # Compute loss between generated image and original image
    loss = compute_loss(generated_image, image)
    # Update local mapping network parameters
    local_mapping_network.update_parameters(loss)

# Use the encoder for customized text-to-image generation
for concept, images in user_defined_concepts:
  for image in images:
    # Encode image into textual embeddings using encoder
    words, patch_embeddings = encoder.encode_image(image)
    # Generate or edit image from text and patch embeddings using diffusion model
    generated_image = diffusion_model.generate_image(text, patch_embeddings)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import libraries
import torch
import torchvision
import numpy as np
import transformers

# Define the encoder
class Encoder:
  def __init__(self, global_mapping_network, local_mapping_network):
    self.global_mapping_network = global_mapping_network
    self.local_mapping_network = local_mapping_network

  def encode_image(self, image):
    # Encode image into textual embeddings using global and local mapping networks
    words = self.global_mapping_network.encode_image(image)
    patch_embeddings = self.local_mapping_network.encode_image(image)
    return words, patch_embeddings

# Define the global mapping network
class GlobalMappingNetwork:
  def __init__(self, word_embedding_dim, joint_embedding_dim):
    # Initialize a pre-trained VGG-19 network for feature extraction
    self.vgg = torchvision.models.vgg19(pretrained=True)
    # Freeze the parameters of VGG-19
    for param in self.vgg.parameters():
      param.requires_grad = False
    # Initialize a fully connected layer for each feature level (conv1_2, conv2_2, conv3_4, conv4_4, conv5_4)
    self.fc_layers = [torch.nn.Linear(64*224*224, word_embedding_dim),
                      torch.nn.Linear(128*112*112, word_embedding_dim),
                      torch.nn.Linear(256*56*56, word_embedding_dim),
                      torch.nn.Linear(512*28*28, word_embedding_dim),
                      torch.nn.Linear(512*14*14, word_embedding_dim)]
    # Initialize a joint embedding layer to project image features and word embeddings into a joint space
    self.joint_embedding_layer = torch.nn.Linear(word_embedding_dim + joint_embedding_dim, joint_embedding_dim)

  def extract_features(self, image):
    # Extract hierarchical features from image using VGG-19
    features = []
    x = image
    for i in range(5):
      for j in range(i+1):
        x = self.vgg.features[i*5+j](x)
      features.append(x)
    return features

  def project_features(self, features):
    # Project features into word embeddings using fully connected layers
    words = []
    for i in range(5):
      x = features[i].view(-1) # flatten the feature map
      x = self.fc_layers[i](x) # project into word embedding space
      x = torch.nn.functional.normalize(x) # normalize the word embedding
      words.append(x)
    return words

  def compute_loss(self, words, text):
    # Compute loss between words and text using cosine similarity and cross entropy
    loss = 0
    text_embeddings = transformers.BertModel.from_pretrained('bert-base-uncased')(text)[0] # get text embeddings from BERT
    for i in range(5):
      x = torch.cat([words[i], text_embeddings], dim=-1) # concatenate word embedding and text embedding
      x = self.joint_embedding_layer(x) # project into joint space
      x = torch.nn.functional.normalize(x) # normalize the joint embedding
      sim = torch.matmul(x, x.t()) # compute cosine similarity matrix
      target = torch.eye(sim.size(0)) # create target matrix with ones on diagonal and zeros elsewhere
      loss += torch.nn.functional.cross_entropy(sim, target) # compute cross entropy loss
    return loss

  def update_parameters(self, loss):
    # Update parameters using gradient descent
    optimizer = torch.optim.Adam(self.parameters(), lr=0.001) # create optimizer
    optimizer.zero_grad() # clear previous gradients
    loss.backward() # compute gradients
    optimizer.step() # update parameters

  def encode_image(self, image):
    # Encode image into textual embeddings using global mapping network
    features = self.extract_features(image) # extract hierarchical features from image
    words = self.project_features(features) # project features into word embeddings
    return words

# Define the local mapping network
class LocalMappingNetwork:
  def __init__(self, patch_size, patch_embedding_dim):
    # Initialize patch size and patch embedding dimension
    self.patch_size = patch_size
    self.patch_embedding_dim = patch_embedding_dim
    # Initialize a fully connected layer to encode patch features into textual embeddings
    self.fc_layer = torch.nn.Linear(patch_size * patch_size * 3, patch_embedding_dim)

  def divide_image(self, image):
    # Divide image into patches of size patch_size * patch_size * 3
    patches = []
    h, w, c = image.shape # get height, width and channel of image
    for i in range(0, h, patch_size):
      for j in range(0, w, patch_size):
        patch = image[i:i+patch_size, j:j+patch_size, :] # get patch from image
        patches.append(patch)
    return patches

  def encode_patches(self, patches):
    # Encode patch features into textual embeddings using fully connected layer
    patch_embeddings = []
    for patch in patches:
      x = patch.view(-1) # flatten the patch
      x = self.fc_layer(x) # project into textual embedding space
      x = torch.nn.functional.normalize(x) # normalize the textual embedding
      patch_embeddings.append(x)
    return patch_embeddings

  def inject_patch_embeddings(self, patch_embeddings, diffusion_model):
    # Inject patch embeddings into cross attention layers of diffusion model
    for i in range(len(diffusion_model.cross_attention_layers)):
      layer = diffusion_model.cross_attention_layers[i] # get cross attention layer
      text_embeddings = layer.get_text_embeddings() # get text embeddings from layer
      sim = torch.matmul(patch_embeddings, text_embeddings.t()) # compute similarity matrix between patch embeddings and text embeddings
      indices = torch.argmax(sim, dim=-1) # get indices of most similar text embeddings for each patch embedding
      for j in range(len(patch_embeddings)):
        index = indices[j] # get index of most similar text embedding for j-th patch embedding
        text_embeddings[index] = patch_embeddings[j] # replace text embedding with patch embedding
      layer.set_text_embeddings(text_embeddings) # set text embeddings for layer

  def compute_loss(self, generated_image, original_image):
    # Compute loss between generated image and original image using L1 norm
    loss = torch.nn.functional.l1_loss(generated_image, original_image) # compute L1 loss
    return loss

  def update_parameters(self, loss):
    # Update parameters using gradient descent
    optimizer = torch.optim.Adam(self.parameters(), lr=0.001) # create optimizer
    optimizer.zero_grad() # clear previous gradients
    loss.backward() # compute gradients
    optimizer.step() # update parameters

  def encode_image(self, image):
    # Encode image into textual embeddings using local mapping network
    patches = self.divide_image(image) # divide image into patches
    patch_embeddings = self.encode_patches(patches) # encode patch features into textual embeddings
    return patch_embeddings

# Define the text-to-image diffusion model
class DiffusionModel:
  def __init__(self, pretrained=True):
    # Initialize a pre-trained diffusion model for text-to-image generation
    self.model = torchvision.models.DDPM(pretrained='text-to-image') if pretrained else torchvision.models.DDPM()
    # Initialize cross attention layers to inject textual embeddings into the model
    self.cross_attention_layers = [CrossAttentionLayer(i) for i in range(16)] # create 16 cross attention layers

  def inject_patch_embeddings(self, patch_embeddings):
    # Inject patch embeddings into cross attention layers of diffusion model
    for layer in self.cross_attention_layers:
      layer.inject_patch_embeddings(patch_embeddings) # inject patch embeddings into layer

  def generate_image(self, text, patch_embeddings):
    # Generate image from text and patch embeddings using diffusion model
    text_embeddings = transformers.BertModel.from_pretrained('bert-base-uncased')(text)[0] # get text embeddings from BERT
    self.inject_patch_embeddings(patch_embeddings) # inject patch embeddings into cross attention layers
    noise = torch.randn(1, 3, 256, 256) # create random noise as input
    generated_image = self.model.reverse(noise, text_embeddings) # generate image from noise and text embeddings using diffusion model
    return generated_image

# Define the cross attention layer
class CrossAttentionLayer:
  def __init__(self, index):
    # Initialize the index of the cross attention layer in the diffusion model
    self.index = index
    # Initialize a cross attention module to inject textual embeddings into the diffusion model
    self.cross_attention_module = torch.nn.MultiheadAttention(embed_dim=768, num_heads=12)

  def get_text_embeddings(self):
    # Get text embeddings from the cross attention module
    return self.cross_attention_module.in_proj_k.weight.t() # get key projection matrix as text embeddings

  def set_text_embeddings(self, text_embeddings):
    # Set text embeddings for the cross attention module
    self.cross_attention_module.in_proj_k.weight.copy_(text_embeddings.t()) # copy text embeddings to key projection matrix

  def inject_patch_embeddings(self, patch_embeddings):
    # Inject patch embeddings into the cross attention module
    text_embeddings = self.get_text_embeddings() # get text embeddings from module
    sim = torch.matmul(patch_embeddings, text_embeddings.t()) # compute similarity matrix between patch embeddings and text embeddings
    indices = torch.argmax(sim, dim=-1) # get indices of most similar text embeddings for each patch embedding
    for i in range(len(patch_embeddings)):