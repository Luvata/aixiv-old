---
title: 2211.11738v3 SPARF  Neural Radiance Fields from Sparse and Noisy Poses
date: 2022-11-12
---

# [SPARF: Neural Radiance Fields from Sparse and Noisy Poses](http://arxiv.org/abs/2211.11738v3)

authors: Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, Federico Tombari


## What, Why and How

[1]: https://arxiv.org/abs/2211.11738 "SPARF: Neural Radiance Fields from Sparse and Noisy Poses"
[2]: https://arxiv.org/pdf/2211.11738 "PDF for 2211.11738 - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2211.11738 "[2211.11738] SPARF: Neural Radiance Fields from Sparse and Noisy Poses"

Here is a summary of the paper:

- **What**: The paper proposes a novel method called **Sparse Pose Adjusting Radiance Field (SPARF)** to synthesize photorealistic novel views from sparse and noisy input images using neural radiance fields (NeRF).
- **Why**: The paper aims to overcome the limitations of existing NeRF methods that require dense input views with highly accurate camera poses, which are not available in many real-world scenarios.
- **How**: The paper exploits multi-view geometry constraints to jointly learn the NeRF and refine the camera poses. It uses pixel matches extracted between the input views to optimize a multi-view correspondence objective and a depth consistency loss. It also introduces a novel pose adjustment module that dynamically adjusts the camera poses during inference. The paper demonstrates the effectiveness of SPARF on multiple challenging datasets and sets a new state of the art in the sparse-view regime[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/2211.11738 "SPARF: Neural Radiance Fields from Sparse and Noisy Poses"
[2]: https://arxiv.org/pdf/2211.11738 "PDF for 2211.11738 - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2211.11738 "[2211.11738] SPARF: Neural Radiance Fields from Sparse and Noisy Poses"

According to the paper[^1^][1], the main contributions are:

- A novel method called **SPARF** that can synthesize photorealistic novel views from sparse and noisy input images using NeRF.
- A novel pose adjustment module that dynamically adjusts the camera poses during inference to improve the quality of the novel views.
- A multi-view correspondence objective and a depth consistency loss that jointly optimize the scene and camera poses using pixel matches and multi-view geometry constraints.
- A new state of the art in the sparse-view regime on multiple challenging datasets, including real-world scenes captured with a smartphone camera.


## Method Summary

[1]: https://arxiv.org/abs/2211.11738 "SPARF: Neural Radiance Fields from Sparse and Noisy Poses"
[2]: https://arxiv.org/pdf/2211.11738 "PDF for 2211.11738 - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2211.11738 "[2211.11738] SPARF: Neural Radiance Fields from Sparse and Noisy Poses"

Here is a summary of the method section of the paper:

- The paper presents a novel method called **SPARF** that can synthesize photorealistic novel views from sparse and noisy input images using NeRF.
- The paper first reviews the basics of NeRF, which is a neural network that maps a 5D input (3D location and 2D viewing direction) to a 4D output (RGB color and density).
- The paper then introduces the main components of SPARF, which are:
    - A **pose adjustment module** that dynamically adjusts the camera poses during inference to improve the quality of the novel views. It uses a differentiable ray tracing algorithm to compute the pixel coordinates of each ray sample in each input view, and then optimizes the camera poses using gradient descent to minimize the reprojection error between the input views and the novel view.
    - A **multi-view correspondence objective** that jointly optimizes the scene and camera poses using pixel matches extracted between the input views. It uses a feature extractor network to compute feature descriptors for each pixel in each input view, and then matches them using nearest neighbor search. It then enforces the optimized scene and camera poses to converge to a global and geometrically accurate solution by minimizing the distance between the matched pixels in different views.
    - A **depth consistency loss** that further encourages the reconstructed scene to be consistent from any viewpoint. It uses a depth extractor network to compute depth maps for each input view, and then penalizes the difference between the depth maps and the predicted depths from NeRF along each ray.
- The paper also describes some implementation details, such as how to handle occlusions and transparency, how to initialize and update the camera poses, how to sample rays and pixels, and how to train and evaluate SPARF.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the NeRF network that maps a 5D input to a 4D output
NeRF = NeuralNetwork()

# Define the feature extractor network that computes feature descriptors for each pixel
FeatureExtractor = NeuralNetwork()

# Define the depth extractor network that computes depth maps for each input view
DepthExtractor = NeuralNetwork()

# Initialize the camera poses using Structure from Motion (SfM) or other methods
CameraPoses = InitializeCameraPoses()

# For each training iteration
for iteration in range(max_iterations):

    # Sample a batch of rays from the input views
    Rays = SampleRays(InputViews)

    # For each ray in the batch
    for ray in Rays:

        # Compute the pixel coordinates of each ray sample in each input view using the pose adjustment module
        PixelCoordinates = PoseAdjustmentModule(ray, CameraPoses)

        # Compute the feature descriptors for each pixel coordinate using the feature extractor network
        FeatureDescriptors = FeatureExtractor(PixelCoordinates)

        # Match the feature descriptors between the input views using nearest neighbor search
        PixelMatches = NearestNeighborSearch(FeatureDescriptors)

        # Compute the multi-view correspondence objective using the pixel matches
        CorrespondenceLoss = ComputeCorrespondenceLoss(PixelMatches)

        # Compute the depth maps for each input view using the depth extractor network
        DepthMaps = DepthExtractor(InputViews)

        # Compute the depth consistency loss using the depth maps and the predicted depths from NeRF
        DepthConsistencyLoss = ComputeDepthConsistencyLoss(DepthMaps, NeRF)

        # Compute the total loss as a weighted sum of the correspondence loss and the depth consistency loss
        TotalLoss = CorrespondenceLoss + DepthConsistencyLoss

        # Update the NeRF network and the camera poses using gradient descent to minimize the total loss
        NeRF, CameraPoses = GradientDescent(TotalLoss, NeRF, CameraPoses)

# To synthesize a novel view from a given camera pose
NovelView = SynthesizeNovelView(NeRF, CameraPose)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.models as models

# Define some hyperparameters
num_iterations = 100000 # The number of training iterations
batch_size = 1024 # The number of rays in each batch
num_samples = 64 # The number of samples along each ray
num_views = 3 # The number of input views
num_features = 128 # The dimension of the feature descriptors
num_hidden = 256 # The dimension of the hidden layers in NeRF
num_layers = 8 # The number of layers in NeRF
learning_rate = 1e-4 # The learning rate for gradient descent
weight_correspondence = 1.0 # The weight for the correspondence loss
weight_depth = 0.1 # The weight for the depth consistency loss

# Define the NeRF network that maps a 5D input to a 4D output
class NeRF(nn.Module):
    def __init__(self):
        super(NeRF, self).__init__()
        # Define the input layer that takes a 5D input (3D location and 2D viewing direction)
        self.input_layer = nn.Linear(5, num_hidden)
        # Define the hidden layers that use ReLU activation and positional encoding
        self.hidden_layers = nn.ModuleList()
        for i in range(num_layers):
            self.hidden_layers.append(nn.Linear(num_hidden, num_hidden))
        # Define the output layer that produces a 4D output (RGB color and density)
        self.output_layer = nn.Linear(num_hidden, 4)

    def forward(self, x):
        # Apply the input layer and ReLU activation
        x = F.relu(self.input_layer(x))
        # Apply the hidden layers and ReLU activation
        for layer in self.hidden_layers:
            x = F.relu(layer(x))
        # Apply the output layer and sigmoid activation for the color and softplus activation for the density
        x = self.output_layer(x)
        color = torch.sigmoid(x[:, :3])
        density = F.softplus(x[:, 3])
        return color, density

# Define the feature extractor network that computes feature descriptors for each pixel
# Use a pretrained ResNet-18 model and extract features from the last convolutional layer
FeatureExtractor = models.resnet18(pretrained=True)
FeatureExtractor.fc = nn.Identity()

# Define the depth extractor network that computes depth maps for each input view
# Use a pretrained MiDaS model (https://github.com/intel-isl/MiDaS) and extract depth maps from the last layer
DepthExtractor = torch.hub.load("intel-isl/MiDaS", "MiDaS")

# Initialize the camera poses using Structure from Motion (SfM) or other methods
# Assume that the camera poses are stored in a numpy array of shape (num_views, 4, 4)
CameraPoses = np.load("camera_poses.npy")

# Convert the camera poses to torch tensors and move them to GPU if available
CameraPoses = torch.from_numpy(CameraPoses).float()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CameraPoses = CameraPoses.to(device)

# Load the input views as torch tensors of shape (num_views, 3, height, width) and move them to GPU if available
InputViews = torch.load("input_views.pt")
InputViews = InputViews.to(device)

# Create an instance of the NeRF network and move it to GPU if available
NeRF = NeRF().to(device)

# Create an optimizer for the NeRF network and the camera poses using Adam
optimizer = optim.Adam([NeRF.parameters(), CameraPoses], lr=learning_rate)

# Define a function to sample rays from the input views
def SampleRays(InputViews):
    # Randomly select a view index from [0, num_views)
    view_index = np.random.randint(0, num_views)
    # Get the corresponding input view and camera pose
    input_view = InputViews[view_index]
    camera_pose = CameraPoses[view_index]
    # Get the height and width of the input view
    height, width = input_view.shape[1:]
    # Randomly sample a batch of pixel coordinates from [0, height) x [0, width)
    pixel_coordinates = np.random.randint(0, max(height, width), size=(batch_size, 2))
    pixel_coordinates[:, 0] = np.clip(pixel_coordinates[:, 0], 0, height - 1)
    pixel_coordinates[:, 1] = np.clip(pixel_coordinates[:, 1], 0, width - 1)
    # Convert the pixel coordinates to torch tensors and move them to GPU if available
    pixel_coordinates = torch.from_numpy(pixel_coordinates).float().to(device)
    # Normalize the pixel coordinates to [-1, 1] range
    pixel_coordinates = (pixel_coordinates - 0.5 * torch.tensor([height, width]).to(device)) / (0.5 * torch.tensor([height, width]).to(device))
    # Compute the ray directions for each pixel coordinate using the camera intrinsics
    # Assume that the camera intrinsics are stored in a torch tensor of shape (3, 3) and moved to GPU if available
    camera_intrinsics = torch.load("camera_intrinsics.pt").to(device)
    ray_directions = torch.matmul(torch.inverse(camera_intrinsics), torch.cat([pixel_coordinates, torch.ones(batch_size, 1).to(device)], dim=1).unsqueeze(2)).squeeze(2)
    ray_directions = ray_directions / torch.norm(ray_directions, dim=1, keepdim=True)
    # Compute the ray origins for each pixel coordinate using the camera pose
    ray_origins = camera_pose[:3, 3].unsqueeze(0).repeat(batch_size, 1)
    # Compute the near and far bounds for each ray using the depth maps
    # Assume that the depth maps are stored in a torch tensor of shape (num_views, height, width) and moved to GPU if available
    depth_maps = torch.load("depth_maps.pt").to(device)
    near_bounds = depth_maps[view_index, pixel_coordinates[:, 0].long(), pixel_coordinates[:, 1].long()]
    far_bounds = near_bounds + 10.0 # A heuristic value
    # Sample num_samples points along each ray using stratified sampling and inverse CDF
    t_samples = torch.linspace(0.0, 1.0, steps=num_samples + 1).to(device)
    t_samples = t_samples[:-1] + (t_samples[1:] - t_samples[:-1]) * torch.rand(batch_size, num_samples).to(device)
    t_samples = near_bounds.unsqueeze(1) + (far_bounds - near_bounds).unsqueeze(1) * t_samples
    # Compute the 3D locations for each sample point using the ray origins and directions
    locations = ray_origins.unsqueeze(1) + ray_directions.unsqueeze(1) * t_samples.unsqueeze(2)
    # Return the view index, input view, camera pose, pixel coordinates, ray origins, ray directions, and locations
    return view_index, input_view, camera_pose, pixel_coordinates, ray_origins, ray_directions, locations

# Define a function to compute the multi-view correspondence objective using the pixel matches
def ComputeCorrespondenceLoss(PixelMatches):
    # Initialize the correspondence loss to zero
    correspondence_loss = 0.0
    # For each pair of views in [0, num_views) x [0, num_views)
    for i in range(num_views):
        for j in range(num_views):
            # If i is not equal to j
            if i != j:
                # Get the matched pixel coordinates between view i and view j
                pixel_matches_ij = PixelMatches[i][j]
                # Compute the L2 distance between the matched pixel coordinates
                distance_ij = torch.norm(pixel_matches_ij[:, :2] - pixel_matches_ij[:, 2:], dim=1)
                # Add the mean distance to the correspondence loss
                correspondence_loss += torch.mean(distance_ij)
    # Return the correspondence loss divided by num_views * (num_views - 1)
    return correspondence_loss / (num_views * (num_views - 1))

# Define a function to compute the depth consistency loss using the depth maps and the predicted depths from NeRF
def ComputeDepthConsistencyLoss(DepthMaps, NeRF):
    # Initialize the depth consistency loss to zero
    depth_consistency_loss = 0.0
    # For each view in [0, num_views)
    for i in range(num_views):
        # Get the input view and camera pose for view i
        input_view_i = InputViews[i]
        camera_pose_i = CameraPoses[i]
        # Get the height and width of the input view
        height_i, width_i = input_view_i.shape[1:]
        # Create a grid of pixel coordinates for view i
        pixel_coordinates_i = torch.meshgrid(torch.arange(height_i), torch.arange(width_i))
        pixel_coordinates_i = torch.stack(pixel_coordinates_i[::-1], dim=2).float().to(device)
        # Normalize the pixel coordinates to [-1, 1] range
        pixel_coordinates_i = (pixel_coordinates_i - 0.5 * torch.tensor([width_i, height_i]).