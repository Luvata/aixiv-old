---
title: 2306.00310v1 Prompt Algebra for Task Composition
date: 2023-06-01
---

# [Prompt Algebra for Task Composition](http://arxiv.org/abs/2306.00310v1)

authors: Pramuditha Perera, Matthew Trager, Luca Zancato, Alessandro Achille, Stefano Soatto


## What, Why and How

[1]: https://arxiv.org/abs/2306.00310 "[2306.00310] Prompt Algebra for Task Composition - arXiv.org"
[2]: https://arxiv.org/pdf/2206.00310.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2305.00310v1 "[2305.00310v1] Nanoscale creep mechanism of clay through MD modeling ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper investigates whether prompts learned independently for different tasks can be later combined through prompt algebra to obtain a model that supports composition of tasks. It considers Visual Language Models (VLM) with prompt tuning as the base classifier and formally defines the notion of prompt algebra. It proposes constrained prompt tuning to improve performance of the composite classifier.
- **Why**: The paper aims to address the challenge of task composition in VLMs, which is the ability to perform multiple tasks simultaneously without retraining or fine-tuning the model. It also seeks to reduce the number of parameters and computational cost required for prompt tuning by leveraging existing prompts and vocabulary.
- **How**: The paper introduces a method of constrained prompt tuning, which restricts the prompts to appear in the lower dimensional subspace spanned by the basis vectors of the pre-trained vocabulary. It also adds regularization to ensure that the learned prompt is grounded correctly to the existing pre-trained vocabulary. It evaluates the effectiveness of its method on object classification and object-attribute classification datasets, and compares it with the best base model and other baselines.

## Main Contributions

The paper claims to make the following contributions:

- It formally defines the notion of prompt algebra and shows how it can be used to compose tasks in VLMs.
- It proposes a method of constrained prompt tuning that improves the performance of the composite classifier and reduces the number of parameters and computational cost required for prompt tuning.
- It demonstrates the effectiveness of its method on object classification and object-attribute classification datasets, and shows that it can achieve comparable or better results than the best base model and other baselines.

## Method Summary

Here is a summary of the method section of the paper:

- The paper considers VLMs with prompt tuning as the base classifier, which is a method of appending a prompt to the input and using the last token of the prompt as the classifier output. The prompt is learned by gradient descent on a task-specific loss function.
- The paper defines prompt algebra as a set of operations that can be applied to prompts to obtain new prompts for composite tasks. The operations include concatenation, addition, multiplication, and permutation. The paper also defines the notion of prompt compatibility, which is a condition that ensures that the composite prompt can be interpreted by the VLM.
- The paper proposes a method of constrained prompt tuning, which restricts the prompts to appear in the lower dimensional subspace spanned by the basis vectors of the pre-trained vocabulary. This reduces the number of parameters and computational cost required for prompt tuning, and also ensures that the learned prompt is grounded correctly to the existing pre-trained vocabulary. The paper also adds regularization to the loss function to encourage orthogonality and sparsity of the prompts.
- The paper evaluates its method on object classification and object-attribute classification datasets, and compares it with the best base model and other baselines. The paper uses CLIP as the VLM and uses zero-shot, few-shot, and full-shot settings for prompt tuning. The paper reports the classification accuracy and the number of parameters for each setting and each dataset.

## Pseudo Code

Here is a possible pseudo code to implement the paper:

```python
# Load the pre-trained VLM and the vocabulary
VLM = load_model("CLIP")
vocab = load_vocab("CLIP")

# Define the prompt algebra operations
def concat(p1, p2):
  # Concatenate two prompts
  return p1 + p2

def add(p1, p2):
  # Add two prompts element-wise
  return [p1[i] + p2[i] for i in range(len(p1))]

def mul(p1, p2):
  # Multiply two prompts element-wise
  return [p1[i] * p2[i] for i in range(len(p1))]

def perm(p1, p2):
  # Permute two prompts according to a random permutation
  perm = random_permutation(len(p1))
  return [p1[perm[i]] + p2[perm[i]] for i in range(len(p1))]

# Define the prompt compatibility condition
def compatible(p1, p2):
  # Check if two prompts have the same length and the same last token
  return len(p1) == len(p2) and p1[-1] == p2[-1]

# Define the constrained prompt tuning method
def constrained_prompt_tuning(input, output, prompt, vocab, alpha, beta):
  # Input: a set of input-output pairs for a task
  # Output: a tuned prompt for the task
  # Prompt: an initial prompt for the task
  # Vocab: the pre-trained vocabulary
  # Alpha: a regularization parameter for orthogonality
  # Beta: a regularization parameter for sparsity

  # Project the prompt and the vocabulary to a lower dimensional subspace
  U, S, V = SVD(vocab)
  k = rank(prompt)
  U_k = U[:, :k]
  S_k = S[:k]
  V_k = V[:k, :]
  prompt_k = prompt @ V_k.T
  vocab_k = vocab @ V_k.T

  # Initialize the optimizer and the loss function
  optimizer = Adam(prompt_k)
  loss_function = CrossEntropyLoss()

  # Loop until convergence or maximum iterations
  while not converged or max_iter:
    # Shuffle the input-output pairs
    shuffle(input, output)

    # Loop over each input-output pair
    for x, y in zip(input, output):
      # Append the prompt to the input and get the VLM output
      x_p = concat(x, prompt_k)
      y_p = VLM(x_p)

      # Compute the loss and the regularization terms
      loss = loss_function(y_p, y)
      ortho = alpha * norm(prompt_k.T @ prompt_k - I)
      sparse = beta * norm(prompt_k @ S_k @ U_k.T - prompt)
      total_loss = loss + ortho + sparse

      # Update the prompt using gradient descent
      optimizer.zero_grad()
      total_loss.backward()
      optimizer.step()

    # Check the convergence criterion
    if total_loss < epsilon:
      converged = True

  # Return the tuned prompt projected back to the original space
  return prompt_k @ V_k

# Define the composite task and the base prompts
composite_task = "object-attribute classification"
base_prompts = ["object classification", "attribute classification"]

# Load the input-output pairs for each base task
input_object, output_object = load_data("object classification")
input_attribute, output_attribute = load_data("attribute classification")

# Initialize and tune the prompts for each base task using zero-shot, few-shot, or full-shot settings
prompt_object = initialize_prompt("object classification")
prompt_attribute = initialize_prompt("attribute classification")
prompt_object = constrained_prompt_tuning(input_object, output_object, prompt_object, vocab, alpha, beta)
prompt_attribute = constrained_prompt_tuning(input_attribute, output_attribute, prompt_attribute, vocab, alpha, beta)

# Check if the base prompts are compatible and compose them using one of the prompt algebra operations
if compatible(prompt_object, prompt_attribute):
  prompt_composite = concat(prompt_object, prompt_attribute) # or add or mul or perm

# Evaluate the composite prompt on the composite task and compare it with the best base model and other baselines
input_composite, output_composite = load_data("object-attribute classification")
accuracy_composite = evaluate(prompt_composite, input_composite, output_composite)
accuracy_best_base_model = evaluate(best_base_model, input_composite, output_composite)
accuracy_baselines = evaluate(baselines, input_composite, output_composite)
compare(accuracy_composite, accuracy_best_base_model, accuracy_baselines)
```