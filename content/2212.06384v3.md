---
title: 2212.06384v3 PV3D  A 3D Generative Model for Portrait Video Generation
date: 2022-12-07
---

# [PV3D: A 3D Generative Model for Portrait Video Generation](http://arxiv.org/abs/2212.06384v3)

authors: Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Wenqing Zhang, Song Bai, Jiashi Feng, Mike Zheng Shou


## What, Why and How

[1]: https://arxiv.org/pdf/2210.06384v3.pdf "GMPF : Well-Tuned Gradual Magnitude Pruning Can Outperform ... - arXiv.org"
[2]: https://arxiv.org/abs/2212.12372 "[2212.12372] Factoring integers with sublinear resources on a ..."
[3]: https://arxiv.org/pdf/2212.06384.pdf "arXiv.org e-Print archive"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a simple and general variant of gradual magnitude pruning (GMP) for large language models (LLMs), called GMPF, which can match and sometimes outperform more complex state-of-the-art methods.
- **Why**: The paper aims to provide a strong baseline for future work on pruning LLMs, and to highlight the importance of parameter tuning for baselines.
- **How**: The paper experiments with different settings of GMPF on the BERT model and various downstream tasks, such as question-answering and text classification. The paper also compares GMPF with other methods, such as Lottery Ticket, MvP, Prune OFA, and oBERT.


## Main Contributions

[1]: https://arxiv.org/pdf/2210.06384v3.pdf "GMPF : Well-Tuned Gradual Magnitude Pruning Can Outperform ... - arXiv.org"
[2]: https://arxiv.org/abs/2212.12372 "[2212.12372] Factoring integers with sublinear resources on a ..."
[3]: https://arxiv.org/pdf/2212.06384.pdf "arXiv.org e-Print archive"

The paper claims the following contributions:

- It revisits the performance of the classic gradual magnitude pruning (GMP) baseline for large language models, focusing on the classic BERT benchmark on various popular tasks.
- It shows that a simple and general variant of GMP, which it calls GMPF, can match and sometimes outperform more complex state-of-the-art methods, such as Lottery Ticket, MvP, Prune OFA, and oBERT.
- It provides a simple yet strong baseline for future work, highlights the importance of parameter tuning for baselines, and even improves the performance of the state-of-the-art second-order pruning method in this setting.


## Method Summary

[1]: https://arxiv.org/pdf/2210.06384v3.pdf "GMPF : Well-Tuned Gradual Magnitude Pruning Can Outperform ... - arXiv.org"
[2]: https://arxiv.org/abs/2212.12372 "[2212.12372] Factoring integers with sublinear resources on a ..."
[3]: https://arxiv.org/pdf/2212.06384.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the GMPF algorithm, which consists of three steps: pruning, fine-tuning, and freezing. Pruning removes a fraction of weights with the smallest magnitude. Fine-tuning updates the remaining weights with a learning rate schedule. Freezing fixes a fraction of weights with the largest magnitude and prevents them from being pruned or updated.
- The paper introduces two hyperparameters for GMPF: the pruning rate and the freezing rate. The pruning rate controls how fast the model is pruned. The freezing rate controls how fast the model is frozen. The paper uses grid search to find the optimal values for these hyperparameters for each task and sparsity level.
- The paper evaluates GMPF on the BERT model and four downstream tasks: SQuAD v1.1, MNLI, SST-2, and CoLA. The paper compares GMPF with four state-of-the-art methods: Lottery Ticket, MvP, Prune OFA, and oBERT. The paper also analyzes the effect of different hyperparameters and pruning regimes on the performance of GMPF.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# GMPF algorithm
Input: pre-trained model M, pruning rate p, freezing rate f, learning rate schedule L
Output: pruned and fine-tuned model M'
Initialize: mask matrix W with all ones
For each epoch e in E:
  # Pruning step
  Sort the absolute values of the weights in M by magnitude
  Set the smallest p fraction of weights to zero in W
  Multiply M by W element-wise to obtain the pruned model M'
  # Fine-tuning step
  Train M' on the downstream task with learning rate L(e)
  # Freezing step
  Sort the absolute values of the weights in M' by magnitude
  Set the largest f fraction of weights to one in W
  Set the rest of the weights to zero in W
  Multiply M' by W element-wise to obtain the frozen model M'
Return M'
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# GMPF algorithm
Input: pre-trained model M, pruning rate p, freezing rate f, learning rate schedule L
Output: pruned and fine-tuned model M'
Initialize: mask matrix W with all ones, shape of W is same as M
For each epoch e in E:
  # Pruning step
  Flatten the weights in M into a one-dimensional vector V
  Sort the absolute values of V by magnitude and store the indices in I
  Compute the number of weights to prune n = p * length of V
  Set the first n elements of I to zero in W
  Reshape W to the original shape of M
  Multiply M by W element-wise to obtain the pruned model M'
  # Fine-tuning step
  Train M' on the downstream task with learning rate L(e) using gradient descent
  # Freezing step
  Flatten the weights in M' into a one-dimensional vector V'
  Sort the absolute values of V' by magnitude and store the indices in I'
  Compute the number of weights to freeze m = f * length of V'
  Set the last m elements of I' to one in W
  Set the rest of the elements of I' to zero in W
  Reshape W to the original shape of M'
  Multiply M' by W element-wise to obtain the frozen model M'
Return M'
```