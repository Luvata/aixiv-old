---
title: 2103.15679v1 Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers
date: 2021-03-16
---

# [Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](http://arxiv.org/abs/2103.15679v1)

authors: Hila Chefer, Shir Gur, Lior Wolf


## What, Why and How

[1]: https://arxiv.org/abs/2103.15679 "[2103.15679] Generic Attention-model Explainability for Interpreting Bi ..."
[2]: https://arxiv.org/pdf/2103.15679.pdf "arXiv:2103.15679v1 [cs.CV] 29 Mar 2021"
[3]: http://export.arxiv.org/abs/2103.15679 "[2103.15679] Generic Attention-model Explainability for Interpreting Bi ..."

Here is a summary of the paper:

- **What**: The paper proposes a method to explain predictions by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions, which are commonly used for multi-modal reasoning tasks such as visual question answering.
- **Why**: The paper aims to address the challenge of interpreting the interactions between different modalities in Transformers, which are not well handled by existing explainability methods that rely on self-attention only.
- **How**: The paper provides generic solutions for three types of attention mechanisms: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. The paper also introduces a novel metric to evaluate the quality of explanations for bi-modal Transformers. The paper shows that its method is superior to all existing methods which are adapted from single modality explainability[^1^][1] [^2^][2].

## Main Contributions

The paper claims the following contributions:

- The first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions.
- Generic solutions for three types of attention mechanisms: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention.
- A novel metric to evaluate the quality of explanations for bi-modal Transformers.
- Experimental results that demonstrate the superiority of its method over existing methods which are adapted from single modality explainability.

## Method Summary

[1]: https://arxiv.org/pdf/2103.15679.pdf "arXiv:2103.15679v1 [cs.CV] 29 Mar 2021"
[2]: https://arxiv.org/abs/2103.15679 "[2103.15679] Generic Attention-model Explainability for Interpreting Bi ..."
[3]: https://www.researchgate.net/publication/353060433_Dephasing_of_Majorana_qubits_due_to_quasistatic_disorder/fulltext/60e65b8d30e8e50c01eb6824/Dephasing-of-Majorana-qubits-due-to-quasistatic-disorder.pdf "arXiv:2106.15679v1 [cond-mat.mes-hall] 29 Jun 2021 - ResearchGate"

Here is a summary of the method section of the paper:

- The paper defines a **prediction explanation** as a set of input tokens that are relevant to the prediction, and a **prediction explanation score** as a measure of how well the explanation matches the prediction.
- The paper proposes a generic framework for computing prediction explanation scores for any Transformer-based architecture, based on the following steps:
    - **Attention aggregation**: The paper aggregates multiple attention maps from different attention mechanisms into a single attention map that captures the interactions between different modalities and layers.
    - **Attention normalization**: The paper normalizes the aggregated attention map to ensure that the sum of attention weights for each input token is equal to one.
    - **Attention thresholding**: The paper applies a threshold to the normalized attention map to select the most relevant input tokens for the prediction.
    - **Attention evaluation**: The paper evaluates the quality of the selected input tokens by comparing them with a ground-truth explanation, using a novel metric called **bi-modal explanation quality (BMEQ)**, which measures both the precision and recall of the explanation across different modalities.
- The paper applies its framework to three types of attention mechanisms: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. The paper describes how to aggregate, normalize, threshold and evaluate attention maps for each type of attention mechanism.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a prediction explanation as a set of input tokens that are relevant to the prediction
# Define a prediction explanation score as a measure of how well the explanation matches the prediction

# Define a generic framework for computing prediction explanation scores for any Transformer-based architecture
def compute_prediction_explanation_score(input, output, model, attention_type):
  # Aggregate multiple attention maps from different attention mechanisms into a single attention map
  attention_map = aggregate_attention(input, output, model, attention_type)
  # Normalize the aggregated attention map to ensure that the sum of attention weights for each input token is equal to one
  attention_map = normalize_attention(attention_map)
  # Apply a threshold to the normalized attention map to select the most relevant input tokens for the prediction
  explanation = threshold_attention(attention_map)
  # Evaluate the quality of the selected input tokens by comparing them with a ground-truth explanation
  score = evaluate_explanation(explanation, ground_truth)
  return score

# Define how to aggregate, normalize, threshold and evaluate attention maps for different types of attention mechanisms
def aggregate_attention(input, output, model, attention_type):
  if attention_type == "pure self-attention":
    # Sum the self-attention weights across all layers and heads
    return sum(model.self_attention(input))
  elif attention_type == "self-attention combined with co-attention":
    # Sum the self-attention weights and the co-attention weights across all layers and heads
    return sum(model.self_attention(input)) + sum(model.co_attention(input, output))
  elif attention_type == "encoder-decoder attention":
    # Sum the encoder-decoder attention weights across all layers and heads
    return sum(model.encoder_decoder_attention(input, output))

def normalize_attention(attention_map):
  # Divide each attention weight by the sum of attention weights for each input token
  return attention_map / sum(attention_map)

def threshold_attention(attention_map):
  # Select the input tokens whose attention weights are above a certain threshold
  return [token for token in input if attention_map[token] > threshold]

def evaluate_explanation(explanation, ground_truth):
  # Compute the bi-modal explanation quality (BMEQ) metric, which measures both the precision and recall of the explanation across different modalities
  return BMEQ(explanation, ground_truth)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import transformers
import numpy as np

# Define a prediction explanation as a set of input tokens that are relevant to the prediction
# Define a prediction explanation score as a measure of how well the explanation matches the prediction

# Define a generic framework for computing prediction explanation scores for any Transformer-based architecture
def compute_prediction_explanation_score(input, output, model, attention_type):
  # Aggregate multiple attention maps from different attention mechanisms into a single attention map
  attention_map = aggregate_attention(input, output, model, attention_type)
  # Normalize the aggregated attention map to ensure that the sum of attention weights for each input token is equal to one
  attention_map = normalize_attention(attention_map)
  # Apply a threshold to the normalized attention map to select the most relevant input tokens for the prediction
  explanation = threshold_attention(attention_map)
  # Evaluate the quality of the selected input tokens by comparing them with a ground-truth explanation
  score = evaluate_explanation(explanation, ground_truth)
  return score

# Define how to aggregate, normalize, threshold and evaluate attention maps for different types of attention mechanisms
def aggregate_attention(input, output, model, attention_type):
  # Initialize an empty tensor to store the aggregated attention map
  attention_map = torch.zeros_like(input)
  # Get the number of layers and heads in the model
  num_layers = model.config.num_hidden_layers
  num_heads = model.config.num_attention_heads
  # Loop over each layer and head in the model
  for layer in range(num_layers):
    for head in range(num_heads):
      if attention_type == "pure self-attention":
        # Get the self-attention weights for the current layer and head
        self_attention = model.encoder.layer[layer].attention.self(input)[0][head]
        # Add the self-attention weights to the aggregated attention map
        attention_map += self_attention
      elif attention_type == "self-attention combined with co-attention":
        # Get the self-attention weights and the co-attention weights for the current layer and head
        self_attention = model.encoder.layer[layer].attention.self(input)[0][head]
        co_attention = model.encoder.layer[layer].crossattention(input, output)[0][head]
        # Add the self-attention weights and the co-attention weights to the aggregated attention map
        attention_map += self_attention + co_attention
      elif attention_type == "encoder-decoder attention":
        # Get the encoder-decoder attention weights for the current layer and head
        encoder_decoder_attention = model.decoder.layer[layer].crossattention(input, output)[0][head]
        # Add the encoder-decoder attention weights to the aggregated attention map
        attention_map += encoder_decoder_attention
  return attention_map

def normalize_attention(attention_map):
  # Divide each attention weight by the sum of attention weights for each input token
  return attention_map / torch.sum(attention_map, dim=-1, keepdim=True)

def threshold_attention(attention_map):
  # Select the input tokens whose attention weights are above a certain threshold (e.g., 0.5)
  return [token for token in input if attention_map[token] > 0.5]

def evaluate_explanation(explanation, ground_truth):
  # Compute the bi-modal explanation quality (BMEQ) metric, which measures both the precision and recall of the explanation across different modalities
  # BMEQ is defined as: BMEQ = (1 - alpha) * precision + alpha * recall,
  # where alpha is a parameter that balances between precision and recall (e.g., alpha = 0.5)
  # Precision is defined as: precision = |explanation ∩ ground_truth| / |explanation|
  # Recall is defined as: recall = |explanation ∩ ground_truth| / |ground_truth|
  
  # Compute the intersection between explanation and ground_truth
  intersection = set(explanation) & set(ground_truth)
  # Compute the precision and recall scores
  precision = len(intersection) / len(explanation)
  recall = len(intersection) / len(ground_truth)
  # Compute the BMEQ score with alpha = 0.5
  BMEQ = (1 - 0.5) * precision + 0.5 * recall
  return BMEQ
```