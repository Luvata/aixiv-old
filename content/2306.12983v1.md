---
title: 2306.12983v1 Towards More Realistic Membership Inference Attacks on Large Diffusion Models
date: 2023-06-13
---

# [Towards More Realistic Membership Inference Attacks on Large Diffusion Models](http://arxiv.org/abs/2306.12983v1)

authors: Jan Dubiński, Antoni Kowalczuk, Stanisław Pawlak, Przemysław Rokita, Tomasz Trzciński, Paweł Morawiecki


## What, Why and How

[1]: https://arxiv.org/pdf/2306.12983v1.pdf "Warsaw University of Technology Artificial Intelligence ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.12983 "[2306.12983] Towards More Realistic Membership Inference Attacks on ..."
[3]: http://export.arxiv.org/abs/2306.12983 "[2306.12983] Towards More Realistic Membership Inference Attacks on ..."

Here is a summary of the paper:

- **What**: The paper examines the problem of membership inference attacks on large diffusion models, such as Stable Diffusion and Midjourney, which are generative models that can create realistic and diverse images from internet-sourced data. Membership inference attacks aim to determine if a specific image was used in the training set of a model, which can pose privacy and copyright risks.
- **Why**: The paper addresses the challenge of designing a fair evaluation framework to answer the membership question, as previous methods do not provide a full understanding of the effectiveness of membership inference attacks. The paper also proposes new attacks and compares them with existing ones on Stable Diffusion.
- **How**: The paper proposes a methodology to establish a fair evaluation setup that considers the distributional shift between the training and test sets, the model architecture and hyperparameters, and the attack scenario. The paper then applies this setup to Stable Diffusion and performs several membership attacks, including baseline loss thresholding, gradient-based methods, and POKEMON (a novel attack based on perceptual similarity). The paper evaluates the attacks using ROC curves and AUC scores, and discusses the implications of the results.



## Main Contributions

[1]: https://arxiv.org/pdf/2306.12983v1.pdf "Warsaw University of Technology Artificial Intelligence ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.12983 "[2306.12983] Towards More Realistic Membership Inference Attacks on ..."
[3]: http://export.arxiv.org/abs/2306.12983 "[2306.12983] Towards More Realistic Membership Inference Attacks on ..."

According to the paper, the main contributions are:

- **A fair evaluation framework** for membership inference attacks on large diffusion models, which considers the distributional shift between the training and test sets, the model architecture and hyperparameters, and the attack scenario.
- **A novel membership inference attack** called POKEMON, which leverages perceptual similarity between images to infer membership status.
- **A comprehensive evaluation** of several membership inference attacks on Stable Diffusion, a state-of-the-art diffusion model, using the proposed evaluation framework.
- **A discussion** of the implications of the results for the privacy and security of large diffusion models and their users.


## Method Summary

[1]: https://arxiv.org/pdf/2306.12983v1.pdf "Warsaw University of Technology Artificial Intelligence ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.12983 "[2306.12983] Towards More Realistic Membership Inference Attacks on ..."
[3]: http://export.arxiv.org/abs/2306.12983 "[2306.12983] Towards More Realistic Membership Inference Attacks on ..."

Here is a summary of the method section of the paper:

- The paper first describes the **diffusion model** framework, which consists of a forward diffusion process that adds noise to an image until it becomes a Gaussian noise sample, and a reverse diffusion process that removes noise from a Gaussian noise sample until it becomes an image. The paper also introduces the **Stable Diffusion** model, which is a diffusion model that uses a single neural network to parameterize the reverse diffusion process and can generate high-quality images at various resolutions.
- The paper then defines the **membership inference attack** problem, which is to determine whether a given image was used in the training set of a diffusion model or not. The paper considers two attack scenarios: **black-box** and **white-box**, depending on whether the attacker has access to the model parameters and gradients or not. The paper also defines the **evaluation metrics** for membership inference attacks, which are the receiver operating characteristic (ROC) curve and the area under the curve (AUC) score.
- The paper then proposes a **fair evaluation setup** for membership inference attacks on large diffusion models, which consists of three steps: 1) selecting a representative test set that matches the distribution of the training set, 2) choosing an appropriate model architecture and hyperparameters that reflect the state-of-the-art performance, and 3) defining a realistic attack scenario that accounts for the model access and the attack budget.
- The paper then presents several **membership inference attacks** on Stable Diffusion, including: 1) baseline loss thresholding, which uses the reconstruction loss of an image as a membership indicator, 2) gradient-based methods, which use the gradient norm or direction of an image as a membership indicator, and 3) POKEMON, which is a novel attack that uses perceptual similarity between images as a membership indicator. The paper also describes how to implement each attack in both black-box and white-box scenarios.
- The paper then reports the **experimental results** of the membership inference attacks on Stable Diffusion using the proposed evaluation setup. The paper compares the ROC curves and AUC scores of different attacks and discusses their strengths and weaknesses. The paper also analyzes the impact of various factors on the attack performance, such as the image resolution, the noise level, and the perceptual similarity metric.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the diffusion model framework
function forward_diffusion(image):
  # Add noise to the image until it becomes a Gaussian noise sample
  for t in range(0, T):
    image = image + sqrt(beta_t) * noise
  return image

function reverse_diffusion(noise):
  # Remove noise from the Gaussian noise sample until it becomes an image
  for t in range(T, 0, -1):
    noise = noise - sqrt(beta_t) * noise
    noise = noise / (1 - alpha_t)
    noise = noise + model(noise, t) # model is a neural network
  return noise

# Define the membership inference attack problem
function membership_inference(image, model):
  # Determine whether the image was used in the training set of the model or not
  if attack_scenario == "black-box":
    # Use only the model outputs to infer membership
    indicator = black_box_attack(image, model)
  else if attack_scenario == "white-box":
    # Use the model parameters and gradients to infer membership
    indicator = white_box_attack(image, model)
  return indicator

# Define the evaluation metrics for membership inference attacks
function evaluate_attack(attack, test_set, model):
  # Compute the ROC curve and AUC score of the attack on the test set
  true_labels = [] # list of binary labels indicating whether an image is in the training set or not
  predicted_scores = [] # list of scores indicating the confidence of membership inference
  for image in test_set:
    true_label = image.label
    predicted_score = attack(image, model)
    true_labels.append(true_label)
    predicted_scores.append(predicted_score)
  roc_curve = compute_roc_curve(true_labels, predicted_scores)
  auc_score = compute_auc_score(roc_curve)
  return roc_curve, auc_score

# Define the fair evaluation setup for membership inference attacks on large diffusion models
function fair_evaluation_setup():
  # Select a representative test set that matches the distribution of the training set
  test_set = select_test_set(training_set)
  # Choose an appropriate model architecture and hyperparameters that reflect the state-of-the-art performance
  model = choose_model(training_set)
  # Define a realistic attack scenario that accounts for the model access and the attack budget
  attack_scenario = define_attack_scenario(model)
  return test_set, model, attack_scenario

# Define several membership inference attacks on Stable Diffusion
function baseline_loss_thresholding(image, model):
  # Use the reconstruction loss of an image as a membership indicator
  noise = forward_diffusion(image) # add noise to the image
  reconstruction = reverse_diffusion(noise) # remove noise from the noisy image
  loss = compute_loss(image, reconstruction) # compute the reconstruction loss
  if loss < threshold: # compare the loss with a predefined threshold
    score = 1 # high score means high confidence of membership
  else:
    score = 0 # low score means low confidence of membership
  return score

function gradient_based_method(image, model):
  # Use the gradient norm or direction of an image as a membership indicator
  if method == "norm":
    gradient = compute_gradient_norm(image, model) # compute the gradient norm of the image with respect to the model parameters
  else if method == "direction":
    gradient = compute_gradient_direction(image, model) # compute the gradient direction of the image with respect to the model parameters
  score = gradient # use the gradient as the score
  return score

function POKEMON(image, model):
  # Use perceptual similarity between images as a membership indicator
  noise = forward_diffusion(image) # add noise to the image
  reconstruction = reverse_diffusion(noise) # remove noise from the noisy image
  similarity = compute_similarity(image, reconstruction) # compute the perceptual similarity between the original and reconstructed images using a metric such as SSIM or CLIP
  score = similarity # use the similarity as the score
  return score

# Report the experimental results of the membership inference attacks on Stable Diffusion using the proposed evaluation setup
function experimental_results():
  # Apply the fair evaluation setup to Stable Diffusion and perform several membership inference attacks on it
  test_set, model, attack_scenario = fair_evaluation_setup()
  attacks = [baseline_loss_thresholding, gradient_based_method, POKEMON]
  results = {} # dictionary to store the results of each attack
  for attack in attacks:
    roc_curve, auc_score = evaluate_attack(attack, test_set, model)
    results[attack] = (roc_curve, auc_score)
  
  # Compare the ROC curves and AUC scores of different attacks and discuss their strengths and weaknesses
  compare_results(results)
  discuss_results(results)

  # Analyze the impact of various factors on the attack performance, such as the image resolution, the noise level, and the perceptual similarity metric
  analyze_factors(results)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch # for tensor operations and neural network modules
import torchvision # for image processing and datasets
import numpy as np # for numerical computations
import matplotlib.pyplot as plt # for plotting
import skimage.metrics # for SSIM computation
import clip # for CLIP computation
import sklearn.metrics # for ROC and AUC computation

# Define the diffusion model framework
function forward_diffusion(image):
  # Add noise to the image until it becomes a Gaussian noise sample
  # Input: image: a tensor of shape (C, H, W) representing an RGB image
  # Output: noise: a tensor of shape (C, H, W) representing a Gaussian noise sample
  beta = 0.0001 # the noise level parameter
  T = 1000 # the number of diffusion steps
  for t in range(0, T):
    image = image + torch.sqrt(beta) * torch.randn_like(image) # add Gaussian noise to the image
  return image

function reverse_diffusion(noise):
  # Remove noise from the Gaussian noise sample until it becomes an image
  # Input: noise: a tensor of shape (C, H, W) representing a Gaussian noise sample
  # Output: image: a tensor of shape (C, H, W) representing an RGB image
  beta = 0.0001 # the noise level parameter
  T = 1000 # the number of diffusion steps
  alpha = 1 - beta * (1 - torch.exp(-torch.arange(T))) # the alpha coefficients for each diffusion step
  model = torch.nn.Conv2d(C, C, 3, padding=1) # the neural network that parameterizes the reverse diffusion process (a simple convolutional layer for illustration)
  for t in range(T, 0, -1):
    noise = noise - torch.sqrt(beta) * torch.randn_like(noise) # remove Gaussian noise from the noisy image
    noise = noise / alpha[t] # rescale the noisy image by alpha coefficient
    noise = noise + model(noise, t) # add the model output to the noisy image
  return noise

# Define the membership inference attack problem
function membership_inference(image, model):
  # Determine whether the image was used in the training set of the model or not
  # Input: image: a tensor of shape (C, H, W) representing an RGB image
  #        model: a diffusion model object that implements forward_diffusion and reverse_diffusion methods
  # Output: indicator: a boolean value indicating whether the image is in the training set or not
  if attack_scenario == "black-box":
    # Use only the model outputs to infer membership
    indicator = black_box_attack(image, model)
  else if attack_scenario == "white-box":
    # Use the model parameters and gradients to infer membership
    indicator = white_box_attack(image, model)
  return indicator

# Define the evaluation metrics for membership inference attacks
function evaluate_attack(attack, test_set, model):
  # Compute the ROC curve and AUC score of the attack on the test set
  # Input: attack: a function that implements a membership inference attack method
  #        test_set: a list of tuples of (image, label), where image is a tensor of shape (C, H, W) representing an RGB image,
  #                  and label is a boolean value indicating whether the image is in the training set or not
  #        model: a diffusion model object that implements forward_diffusion and reverse_diffusion methods
  # Output: roc_curve: a tuple of (fpr, tpr, thresholds), where fpr is an array of false positive rates,
  #                    tpr is an array of true positive rates, and thresholds is an array of decision thresholds for different points on the ROC curve 
  #         auc_score: a scalar value representing the area under the ROC curve 
  true_labels = [] # list of binary labels indicating whether an image is in the training set or not
  predicted_scores = [] # list of scores indicating the confidence of membership inference
  for image, label in test_set:
    true_label = label.item() # convert label tensor to scalar value
    predicted_score = attack(image, model).item() # convert score tensor to scalar value
    true_labels.append(true_label)
    predicted_scores.append(predicted_score)
  
  roc_curve = sklearn.metrics.roc_curve(true_labels, predicted_scores) # compute the ROC curve using sklearn library 
  auc_score = sklearn.metrics.auc(roc_curve[0], roc_curve[1]) # compute the AUC score using sklearn library
  return roc_curve, auc_score

# Define the fair evaluation setup for membership inference attacks on large diffusion models
function fair_evaluation_setup():
  # Select a representative test set that matches the distribution of the training set
  # Output: test_set: a list of tuples of (image, label), where image is a tensor of shape (C, H, W) representing an RGB image,
  #                  and label is a boolean value indicating whether the image is in the training set or not
  training_set = torchvision.datasets.ImageFolder("training_data") # load the training set from a folder of images
  test_set = torchvision.datasets.ImageFolder("test_data") # load the test set from a folder of images
  test_set = filter_test_set(test_set, training_set) # filter out the images in the test set that are not in the same distribution as the training set
  
  # Choose an appropriate model architecture and hyperparameters that reflect the state-of-the-art performance
  # Output: model: a diffusion model object that implements forward_diffusion and reverse_diffusion methods
  model = StableDiffusion("model.pth") # load the Stable Diffusion model from a file of pre-trained parameters
  
  # Define a realistic attack scenario that accounts for the model access and the attack budget
  # Output: attack_scenario: a string value indicating whether the attack scenario is "black-box" or "white-box"
  attack_scenario = "black-box" # assume the attacker has no access to the model parameters and gradients
  
  return test_set, model, attack_scenario

# Define several membership inference attacks on Stable Diffusion
function baseline_loss_thresholding(image, model):
  # Use the reconstruction loss of an image as a membership indicator
  # Input: image: a tensor of shape (C, H, W) representing an RGB image
  #        model: a diffusion model object that implements forward_diffusion and reverse_diffusion methods
  # Output: score: a scalar value indicating the confidence of membership inference
  noise = model.forward_diffusion(image) # add noise to the image
  reconstruction = model.reverse_diffusion(noise) # remove noise from the noisy image
  loss = torch.nn.functional.mse_loss(image, reconstruction) # compute the mean squared error loss between the original and reconstructed images
  threshold = 0.01 # define a threshold for the loss value
  if loss < threshold: # compare the loss with the threshold
    score = 1 # high score means high confidence of membership
  else:
    score = 0 # low score means low confidence of membership
  return score

function gradient_based_method(image, model):
  # Use the gradient norm or direction of an image as a membership indicator
  # Input: image: a tensor of shape (C, H, W) representing an RGB image
  #        model: a diffusion model object that implements forward_diffusion and reverse_diffusion methods
  # Output: score: a scalar value indicating the confidence of membership inference
  if method == "norm":
    gradient = torch.autograd.grad(model.forward_diffusion(image).sum(), image)[0] # compute the gradient of the noisy image with respect to the original image using torch.autograd.grad function 
    score = torch.norm(gradient) # compute the norm of the gradient as the score
  else if method == "direction":
    gradient = torch.autograd.grad(model.forward_diffusion(image).sum(), image)[0] # compute the gradient of the noisy image with respect to the original image using torch.autograd.grad function 
    score = torch.dot(gradient.flatten(), image.flatten()) / (torch.norm(gradient) * torch.norm(image)) # compute the cosine similarity between the gradient and the original image as the score
  return score

function POKEMON(image, model):
  # Use perceptual similarity between images as a membership indicator
  # Input: image: a tensor of shape (C, H, W) representing an RGB image
  #        model: a diffusion model object that implements forward_diffusion and reverse_diffusion methods
  # Output: score: a scalar value indicating the confidence of membership inference
  noise = model.forward_diffusion(image) # add noise to the image
  reconstruction = model.reverse_diffusion(noise) # remove noise from the noisy image
  
  if metric == "SSIM":
    similarity = skimage.metrics.structural_similarity(image.numpy(), reconstruction.numpy(), multichannel=True) # compute the structural similarity index measure (SSIM) between the original and reconstructed images using skimage library 
    score = similarity # use SSIM as score
  
  else if metric == "CLIP":
    clip_model, preprocess = clip.load("ViT-B/32", device="cpu") # load CLIP model and preprocessing function from clip library 
    with torch.no_grad():
      image_features = clip_model.encode_image(preprocess(image.unsqueeze