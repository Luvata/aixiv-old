---
title: 2103.00020v1 Learning Transferable Visual Models From Natural Language Supervision
date: 2021-03-01
---

# [Learning Transferable Visual Models From Natural Language Supervision](http://arxiv.org/abs/2103.00020v1)

authors: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever


## What, Why and How

[1]: https://arxiv.org/pdf/2103.00020v1.pdf "Learning Transferable Visual Models From Natural Language ... - arXiv.org"
[2]: https://arxiv.org/abs/2103.00020 "[2103.00020] Learning Transferable Visual Models From ... - arXiv.org"
[3]: https://scholar.archive.org/work/gavjndrh4za4fddbkyh6akvnk4 "Learning Transferable Visual Models From Natural Language Supervision"
[4]: https://ucladeepvision.github.io/CS188-Projects-2023Winter/2023/01/29/team36-transformer-images.html "Language Representation for Computer Vision"

Here is a summary of the paper:

- **What**: The paper presents a method for learning image representations from natural language supervision, using a large-scale dataset of (image, text) pairs collected from the internet.
- **Why**: The paper aims to overcome the limitations of existing computer vision systems that are trained on fixed sets of predetermined object categories, which require additional labeled data for new visual concepts. The paper also seeks to leverage the rich and diverse source of supervision available in web text, which can describe various aspects of images beyond object labels.
- **How**: The paper proposes a simple pre-training task of predicting which caption goes with which image, using a contrastive learning objective. The paper uses a Transformer-based architecture that jointly encodes images and text into a shared latent space, where the similarity between image and text embeddings is maximized. The paper evaluates the performance of the model on over 30 downstream computer vision tasks, spanning different domains and modalities. The paper shows that the model can transfer to these tasks without any fine-tuning, using natural language as a flexible interface to reference or describe visual concepts. The paper also demonstrates that the model can generate novel images from text descriptions, using a gradient-based optimization technique.[^1^][1] [^2^][2]

## Main Contributions

Some of the contributions of this paper are:

- It introduces a novel pre-training task of learning image representations from natural language supervision, which can capture a wide range of visual concepts and attributes.
- It constructs a large-scale dataset of 400 million (image, text) pairs from the internet, which covers diverse domains and modalities.
- It demonstrates that the pre-trained model can zero-shot transfer to over 30 downstream computer vision tasks, often matching or exceeding the performance of fully supervised baselines.
- It shows that the pre-trained model can generate realistic and diverse images from text descriptions, using a simple gradient-based optimization technique.

## Method Summary

Here is a summary of the method section of the paper:

- The paper uses a Transformer-based architecture that consists of two encoders: an image encoder and a text encoder. The image encoder is a ResNet-50 that extracts features from patches of the input image. The text encoder is a 12-layer Transformer that processes the input caption. Both encoders produce embeddings of size 512 for each patch or token.
- The paper applies a linear projection layer to the embeddings of both encoders, followed by a layer normalization. The projected embeddings are then used to compute the cosine similarity between each image patch and each text token. The paper uses a temperature parameter to scale the similarities before applying a softmax function.
- The paper defines a contrastive learning objective that maximizes the mutual information between an image and its caption, while minimizing the information between an image and other captions in the same batch. The paper uses a cross-entropy loss to optimize this objective, using the similarities as logits and the ground-truth matches as labels.
- The paper pre-trains the model on a large-scale dataset of 400 million (image, text) pairs, which are obtained from various sources such as Wikipedia, Reddit, GPT-3, etc. The paper applies data augmentation techniques such as random cropping, color jittering, and text shuffling to increase the diversity and robustness of the data. The paper also uses mixed precision training and gradient checkpointing to reduce the memory consumption and speed up the training process.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the model architecture
image_encoder = ResNet50()
text_encoder = Transformer(num_layers=12)
projection_layer = Linear(in_features=512, out_features=512)
layer_norm = LayerNorm()

# Define the contrastive learning objective
def contrastive_loss(image, text, temperature):
  # Encode the image and text
  image_features = image_encoder(image) # shape: [batch_size, num_patches, 512]
  text_features = text_encoder(text) # shape: [batch_size, num_tokens, 512]
  # Project and normalize the features
  image_features = layer_norm(projection_layer(image_features)) # shape: [batch_size, num_patches, 512]
  text_features = layer_norm(projection_layer(text_features)) # shape: [batch_size, num_tokens, 512]
  # Compute the cosine similarity between each patch and token
  similarities = torch.matmul(image_features, text_features.transpose(-1, -2)) # shape: [batch_size, num_patches, num_tokens]
  similarities = similarities / temperature # scale by a temperature parameter
  # Compute the cross-entropy loss using the ground-truth matches as labels
  labels = torch.eye(num_patches).repeat(batch_size, 1) # shape: [batch_size, num_patches]
  loss = cross_entropy(similarities, labels) # average over the batch
  return loss

# Pre-train the model on a large-scale dataset
dataset = load_dataset("image_text_pairs") # a dataset of 400 million (image, text) pairs
optimizer = Adam(model.parameters()) # use an optimizer of your choice
for batch in dataset: # iterate over the batches
  # Apply data augmentation to the batch
  batch["image"] = augment_image(batch["image"]) # apply random cropping, color jittering, etc.
  batch["text"] = augment_text(batch["text"]) # apply text shuffling, etc.
  # Compute the loss and update the model parameters
  loss = contrastive_loss(batch["image"], batch["text"], temperature=0.07)
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms
import transformers
import datasets

# Define the model architecture
class CLIP(nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pre-trained ResNet-50 as the image encoder
    self.image_encoder = models.resnet50(pretrained=True)
    # Remove the last fully connected layer
    self.image_encoder.fc = nn.Identity()
    # Use a pre-trained BERT-base as the text encoder
    self.text_encoder = transformers.BertModel.from_pretrained("bert-base-uncased")
    # Define a linear projection layer with 512 output features
    self.projection_layer = nn.Linear(in_features=512, out_features=512)
    # Define a layer normalization layer
    self.layer_norm = nn.LayerNorm(512)

  def forward(self, image, text):
    # Encode the image and text
    image_features = self.image_encoder(image) # shape: [batch_size, 512]
    text_features = self.text_encoder(text).last_hidden_state[:, 0, :] # shape: [batch_size, 512]
    # Project and normalize the features
    image_features = self.layer_norm(self.projection_layer(image_features)) # shape: [batch_size, 512]
    text_features = self.layer_norm(self.projection_layer(text_features)) # shape: [batch_size, 512]
    return image_features, text_features

# Define the contrastive learning objective
def contrastive_loss(image_features, text_features, temperature):
  # Compute the cosine similarity between each image and text pair
  similarities = torch.matmul(image_features, text_features.transpose(-1, -2)) # shape: [batch_size, batch_size]
  similarities = similarities / temperature # scale by a temperature parameter
  # Compute the cross-entropy loss using the ground-truth matches as labels
  labels = torch.arange(batch_size).to(device) # shape: [batch_size]
  loss = F.cross_entropy(similarities, labels) # average over the batch
  return loss

# Pre-train the model on a large-scale dataset
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # use GPU if available
model = CLIP().to(device) # move the model to the device
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # use Adam optimizer with a learning rate of 1e-4
dataset = datasets.load_dataset("image_text_pairs") # load a dataset of 400 million (image, text) pairs
dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True) # create a dataloader with a batch size of 256 and shuffle the data

# Define some data augmentation transforms for images and text
image_transforms = transforms.Compose([
  transforms.RandomResizedCrop(224), # randomly crop and resize the image to 224x224 pixels
  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4), # randomly change the brightness, contrast and saturation of the image
  transforms.ToTensor(), # convert the image to a tensor
])

text_transforms = lambda x: " ".join(torch.randperm(len(x.split())).tolist()) # randomly shuffle the words in the text

# Train the model for some epochs
num_epochs = 10 # choose the number of epochs to train
for epoch in range(num_epochs):
  for batch in dataloader: # iterate over the batches
    # Apply data augmentation to the batch
    batch["image"] = image_transforms(batch["image"]).to(device) # apply image transforms and move to device
    batch["text"] = text_transforms(batch["text"]).to(device) # apply text transforms and move to device
    # Compute the loss and update the model parameters
    image_features, text_features = model(batch["image"], batch["text"]) # forward pass of the model
    loss = contrastive_loss(image_features, text_features, temperature=0.07) # compute the contrastive loss with a temperature of 0.07
    loss.backward() # compute the gradients
    optimizer.step() # update the parameters
    optimizer.zero_grad() # reset the gradients

```