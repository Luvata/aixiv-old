---
title: 2305.10973v1 Drag Your GAN  Interactive Point-based Manipulation on the Generative Image Manifold
date: 2023-05-11
---

# [Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold](http://arxiv.org/abs/2305.10973v1)

authors: Xingang Pan, Ayush Tewari, Thomas LeimkÃ¼hler, Lingjie Liu, Abhimitra Meka, Christian Theobalt


## What, Why and How

[1]: https://arxiv.org/abs/2305.10973 "[2305.10973] Drag Your GAN: Interactive Point-based Manipulation on the ..."
[2]: http://export.arxiv.org/abs/2305.10973 "[2305.10973] Drag Your GAN: Interactive Point-based Manipulation on the ..."
[3]: https://arxiv.org/pdf/2305.10973.pdf "arXiv.org"

Here is a summary of the paper:

- **What**: The paper proposes a new method for interactive point-based manipulation on the generative image manifold of a GAN, called DragGAN. It allows users to "drag" any points of the image to precisely reach target points, thus manipulating the pose, shape, expression, and layout of the generated objects.
- **Why**: The paper aims to address the limitations of existing approaches for controlling GANs, which often require manually annotated training data or a prior 3D model, and lack flexibility, precision, and generality. The paper also demonstrates the advantages of DragGAN over prior approaches in the tasks of image manipulation and point tracking, as well as the manipulation of real images through GAN inversion.
- **How**: The paper introduces two main components of DragGAN: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. The paper also presents a project page[^1^][1] with code, data, and videos.

## Main Contributions

According to the paper, the main contributions are:

- A novel method for interactive point-based manipulation on the generative image manifold of a GAN, which enables precise control over where pixels go and realistic outputs even for challenging scenarios.
- A feature-based motion supervision that guides the handle point to move towards the target position, and a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points.
- Extensive experiments and comparisons with prior approaches on diverse categories such as animals, cars, humans, landscapes, etc., as well as real images through GAN inversion.

## Method Summary

Here is a summary of the method section:

- The paper assumes a pre-trained GAN that can generate images from latent vectors. The goal is to find a latent vector that can produce an image that matches the user's manipulation of the handle points.
- The paper defines a handle point as a point on the image that the user can drag to a target position. The paper also defines a feature-based motion supervision loss that measures the distance between the features of the handle point and the target point in the generator feature space. This loss encourages the handle point to move towards the target position in the image space.
- The paper introduces a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. The paper uses a feature extractor network to obtain the features of the handle points and the target points, and then computes a similarity map between them. The paper then uses a peak finding algorithm to locate the position of the handle points in the similarity map, and updates their coordinates accordingly.
- The paper optimizes the latent vector by minimizing a combination of losses, including the feature-based motion supervision loss, an image reconstruction loss, and a perceptual loss. The paper also applies some regularization techniques such as clipping and projection to ensure that the latent vector stays within the valid range and close to the original one.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained GAN G, an image x, a set of handle points H, and a set of target points T
# Output: a manipulated image x'

# Initialize the latent vector z with GAN inversion
z = invert_GAN(G, x)

# Initialize the feature extractor network F
F = initialize_feature_extractor()

# Repeat until convergence or maximum iterations
while not converged or not max_iterations:

  # Generate an image from the latent vector
  x' = G(z)

  # Extract the features of the handle points and the target points
  F_h = F(x', H)
  F_t = F(x', T)

  # Compute the feature-based motion supervision loss
  L_motion = distance(F_h, F_t)

  # Compute the image reconstruction loss
  L_recon = distance(x', x)

  # Compute the perceptual loss
  L_perceptual = distance(VGG(x'), VGG(x))

  # Compute the total loss
  L_total = L_motion + lambda1 * L_recon + lambda2 * L_perceptual

  # Update the latent vector by gradient descent
  z = z - alpha * grad(L_total, z)

  # Clip and project the latent vector to ensure validity and closeness
  z = clip(z, min_z, max_z)
  z = project(z, original_z, epsilon)

  # Update the handle points by point tracking
  H = track_points(F_h, F_t)

# Return the manipulated image
return x'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
max_iterations = 1000 # maximum number of iterations
alpha = 0.01 # learning rate for latent vector optimization
lambda1 = 0.01 # weight for image reconstruction loss
lambda2 = 0.01 # weight for perceptual loss
epsilon = 0.01 # threshold for projection regularization
min_z = -1 # minimum value for latent vector
max_z = 1 # maximum value for latent vector

# Load the pre-trained GAN model
G = load_GAN_model()

# Load the image and resize it to the GAN output size
x = load_image()
x = resize(x, G.output_size)

# Invert the GAN to obtain the initial latent vector
z = invert_GAN(G, x)

# Define the feature extractor network as a pre-trained VGG network
F = torchvision.models.vgg16(pretrained=True)
F.eval() # set to evaluation mode

# Define the VGG network for perceptual loss
VGG = torchvision.models.vgg16(pretrained=True)
VGG.eval() # set to evaluation mode

# Define the distance function as L2 norm
def distance(a, b):
  return torch.norm(a - b)

# Define the function to clip the latent vector to a valid range
def clip(z, min_z, max_z):
  return torch.clamp(z, min_z, max_z)

# Define the function to project the latent vector to be close to the original one
def project(z, original_z, epsilon):
  diff = z - original_z
  norm = torch.norm(diff)
  if norm > epsilon:
    z = original_z + epsilon * diff / norm
  return z

# Define the function to track the handle points by similarity map and peak finding
def track_points(F_h, F_t):
  # Compute the similarity map between the handle features and the target features
  S = torch.matmul(F_h, F_t.transpose(0, 1))

  # Find the peaks in the similarity map by non-maximum suppression
  P = non_maximum_suppression(S)

  # Update the handle points by the peak coordinates
  H = P.coordinates()
  return H

# Initialize the handle points and the target points randomly
H = np.random.randint(0, G.output_size, (10, 2)) # 10 handle points with x and y coordinates
T = np.random.randint(0, G.output_size, (10, 2)) # 10 target points with x and y coordinates

# Repeat until convergence or maximum iterations
for i in range(max_iterations):

  # Generate an image from the latent vector
  x' = G(z)

  # Extract the features of the handle points and the target points from the feature extractor network
  F_h = F(x', H)
  F_t = F(x', T)

  # Compute the feature-based motion supervision loss
  L_motion = distance(F_h, F_t)

  # Compute the image reconstruction loss
  L_recon = distance(x', x)

  # Compute the perceptual loss using the VGG network features
  L_perceptual = distance(VGG(x'), VGG(x))

  # Compute the total loss
  L_total = L_motion + lambda1 * L_recon + lambda2 * L_perceptual

  # Update the latent vector by gradient descent
  z = z - alpha * grad(L_total, z)

  # Clip and project the latent vector to ensure validity and closeness
  z = clip(z, min_z, max_z)
  z = project(z, original_z, epsilon)

  # Update the handle points by point tracking
  H = track_points(F_h, F_t)

# Return the manipulated image
return x'
```