---
title: 2305.11520v4 Late-Constraint Diffusion Guidance for Controllable Image Synthesis
date: 2023-05-12
---

# [Late-Constraint Diffusion Guidance for Controllable Image Synthesis](http://arxiv.org/abs/2305.11520v4)

authors: Chang Liu, Dong Liu


## What, Why and How

[1]: https://arxiv.org/abs/2305.11520 "[2305.11520] Late-Constraint Diffusion Guidance for Controllable Image ..."
[2]: https://arxiv.org/pdf/2305.11520.pdf "Late-Constraint Diffusion Guidance for Controllable Image ... - arXiv.org"
[3]: https://arxiv.org/pdf/2305.11520v4.pdf "No PDF for 2305.11520v4 - arXiv.org"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new approach for controllable image synthesis using diffusion models, which can incorporate various external conditions such as layout, color, structure, object shape, etc. The approach is called **late-constraint diffusion guidance**, which constrains the output of the diffusion network to be aligned with the required conditions, rather than regularizing the intermediate features as in previous methods.
- **Why**: The paper aims to address the limitations of existing methods for controllable image synthesis using diffusion models, which are known as **early-constraint** methods. These methods have difficulties in handling multiple conditions with a single solution, and they require much training cost and result in non-generalizable solutions. The paper claims that the proposed late-constraint approach can overcome these difficulties and achieve better performance and generalization ability.
- **How**: The paper introduces a lightweight **condition adapter** to establish the correlation between external conditions and internal representations of diffusion models. During the iterative denoising process, the conditional guidance is sent into the corresponding condition adapter to manipulate the sampling process with the established correlation. The paper also proposes a **timestep resampling** method and an **early stopping** technique to improve the quality of synthesized images and comply with the guidance. The paper evaluates the proposed method on several datasets and tasks, such as COCO-Stuff [4], CUB [51], CelebA-HQ [25], ImageNet [11], etc., and demonstrates its advantages over existing methods and its applications in various scenarios.


## Main Contributions

According to the paper, the main contributions are:

- They propose a new approach for controllable image synthesis using diffusion models, which is called **late-constraint diffusion guidance**. This approach can handle multiple conditions with a single solution and generalize better to unseen conditions.
- They introduce a lightweight **condition adapter** to learn the correlation between external conditions and internal representations of diffusion models, and use it to manipulate the sampling process during the denoising process.
- They propose a **timestep resampling** method and an **early stopping** technique to improve the quality of synthesized images and comply with the guidance.
- They conduct extensive experiments on several datasets and tasks, and demonstrate the superiority of their method over existing methods and its applications in various scenarios.


## Method Summary

Here is a summary of the method section of the paper:

- The paper first reviews the background of diffusion models, which are generative models that learn to invert a Markov chain that gradually adds Gaussian noise to a data distribution. The paper also reviews the existing methods for controllable image synthesis using diffusion models, which are called **early-constraint** methods. These methods incorporate external conditions as regularization terms on the intermediate features of the diffusion denoising network, which is the network that learns to reverse the diffusion process and generate images from noise.
- The paper then introduces the proposed **late-constraint** approach, which differs from the early-constraint methods in that it constrains the output of the diffusion network to be aligned with the required conditions, rather than regularizing the intermediate features. The paper argues that this approach can handle multiple conditions with a single solution and generalize better to unseen conditions, as well as reducing the training cost and complexity.
- The paper describes the key component of the late-constraint approach, which is the **condition adapter**. The condition adapter is a lightweight network that learns to map external conditions to internal representations of diffusion models. The paper defines two types of condition adapters: **feature adapter** and **noise adapter**. The feature adapter modifies the features of the diffusion network at each timestep according to the condition, while the noise adapter modifies the noise samples that are added to the features at each timestep according to the condition. The paper shows how to train the condition adapters using a contrastive loss function that encourages the alignment between the condition and the output of the diffusion network.
- The paper also proposes two techniques to improve the performance of the late-constraint approach: **timestep resampling** and **early stopping**. Timestep resampling is a method that randomly samples timesteps from a predefined distribution during the denoising process, rather than following a fixed order. This method can increase the diversity and quality of synthesized images by exploring different paths of denoising. Early stopping is a technique that stops the denoising process before reaching the final timestep, based on a predefined criterion. This technique can prevent overfitting to the condition and preserve more details and realism of synthesized images.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the diffusion model
diffusion_model = DiffusionModel()

# Define the condition adapter
condition_adapter = ConditionAdapter()

# Define the contrastive loss function
contrastive_loss = ContrastiveLoss()

# Train the condition adapter
for each batch of images and conditions:
  # Forward the images through the diffusion model
  features, outputs = diffusion_model(images)
  # Forward the conditions through the condition adapter
  feature_adaptations, noise_adaptations = condition_adapter(conditions)
  # Compute the contrastive loss between the outputs and the conditions
  loss = contrastive_loss(outputs, conditions)
  # Update the condition adapter parameters
  condition_adapter.backward(loss)

# Synthesize images with the late-constraint approach
for each condition:
  # Initialize a noise image
  image = sample_noise()
  # Sample a sequence of timesteps from a predefined distribution
  timesteps = sample_timesteps()
  # Denoise the image iteratively
  for timestep in timesteps:
    # Forward the condition through the condition adapter
    feature_adaptation, noise_adaptation = condition_adapter(condition)
    # Forward the image through the diffusion model
    feature, output = diffusion_model(image, timestep)
    # Apply the feature adaptation and noise adaptation to the feature and output
    feature = feature + feature_adaptation
    output = output + noise_adaptation
    # Sample a new image from the output distribution
    image = sample_image(output)
    # Stop the denoising process if a predefined criterion is met
    if early_stopping_criterion(image, condition):
      break
  # Return the synthesized image
  return image

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
num_timesteps = 1000 # The number of timesteps for the diffusion model
num_layers = 12 # The number of layers for the diffusion denoising network
num_channels = 256 # The number of channels for the diffusion denoising network
num_classes = 1000 # The number of classes for the ImageNet dataset
image_size = 256 # The size of the input images
batch_size = 16 # The size of the training batch
learning_rate = 1e-4 # The learning rate for the condition adapter
temperature = 0.07 # The temperature for the contrastive loss function
resample_prob = 0.9 # The probability of resampling timesteps during synthesis
early_stopping_threshold = 0.9 # The threshold for early stopping criterion

# Define the diffusion model
class DiffusionModel(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Initialize the diffusion coefficients
    self.betas = torch.exp(torch.linspace(np.log(1e-4), np.log(2e-2), num_timesteps))
    self.alphas = 1 - self.betas
    self.alphas_bar = torch.cumprod(self.alphas, dim=0)
    self.alphas_bar_prev = torch.cat([torch.tensor([1.]), self.alphas_bar[:-1]])
    self.sqrt_alphas_bar = torch.sqrt(self.alphas_bar)
    self.sqrt_one_minus_alphas_bar = torch.sqrt(1 - self.alphas_bar)
    self.sqrt_betas = torch.sqrt(self.betas)
    # Initialize the diffusion denoising network
    self.denoising_network = DenoisingNetwork(num_layers, num_channels, num_classes)

  def forward(self, x, t=None):
    # If no timestep is given, use all timesteps
    if t is None:
      t = torch.arange(num_timesteps).repeat(x.shape[0], 1)
    # Convert t to one-hot encoding
    t_onehot = torch.nn.functional.one_hot(t, num_classes=num_timesteps).float()
    # Compute the noise scale
    noise_scale = self.sqrt_alphas_bar_prev[t] * x.shape[-1] ** (3 / 2)
    # Add noise to the input image
    x_noisy = x + torch.randn_like(x) * noise_scale.view(-1, 1, 1, 1)
    # Normalize the noisy image by the noise level
    x_normalized = x_noisy / self.sqrt_alphas_bar[t].view(-1, 1, 1, 1)
    # Forward the normalized image and timestep through the denoising network
    feature, output = self.denoising_network(x_normalized, t_onehot)
    # Compute the output distribution parameters
    mean = output[:, :3]
    std = torch.exp(output[:, 3:])
    # Return the feature and output distribution parameters
    return feature, (mean, std)

# Define the denoising network
class DenoisingNetwork(torch.nn.Module):
  def __init__(self, num_layers, num_channels, num_classes):
    super().__init__()
    # Initialize the input layer
    self.input_layer = InputLayer(3 + num_timesteps)
    # Initialize the residual blocks
    self.resblocks = torch.nn.ModuleList([ResBlock(num_channels) for _ in range(num_layers)])
    # Initialize the output layer
    self.output_layer = OutputLayer(num_channels, 6)

  def forward(self, x, t):
    # Concatenate the input image and timestep
    x = torch.cat([x, t], dim=1)
    # Forward through the input layer
    x = self.input_layer(x)
    # Forward through the residual blocks
    for resblock in self.resblocks:
      x = resblock(x)
    # Forward through the output layer
    x = self.output_layer(x)
    # Split the output into feature and output parts
    feature, output = torch.split(x, [num_channels - 6, 6], dim=1)
    # Return the feature and output parts
    return feature, output

# Define the input layer
class InputLayer(torch.nn.Module):
  def __init__(self, in_channels):
    super().__init__()
    # Initialize a convolutional layer with kernel size 3 and padding 1
    self.conv = torch.nn.Conv2d(in_channels, num_channels, kernel_size=3, padding=1)

  def forward(self, x):
    # Forward through the convolutional layer
    x = self.conv(x)
    # Return the output
    return x

# Define the residual block
class ResBlock(torch.nn.Module):
  def __init__(self, num_channels):
    super().__init__()
    # Initialize two convolutional layers with kernel size 3 and padding 1
    self.conv1 = torch.nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)
    self.conv2 = torch.nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)

  def forward(self, x):
    # Save the input as a skip connection
    skip = x
    # Forward through the first convolutional layer
    x = self.conv1(x)
    # Apply ReLU activation
    x = torch.nn.functional.relu(x)
    # Forward through the second convolutional layer
    x = self.conv2(x)
    # Add the skip connection
    x = x + skip
    # Return the output
    return x

# Define the output layer
class OutputLayer(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    # Initialize a convolutional layer with kernel size 3 and padding 1
    self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

  def forward(self, x):
    # Forward through the convolutional layer
    x = self.conv(x)
    # Return the output
    return x

# Define the condition adapter
class ConditionAdapter(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Initialize two linear layers for feature adaptation and noise adaptation respectively
    self.feature_adapter = torch.nn.Linear(condition_size, num_channels)
    self.noise_adapter = torch.nn.Linear(condition_size, num_channels)

  def forward(self, condition):
    # Forward the condition through the feature adapter and noise adapter respectively
    feature_adaptation = self.feature_adapter(condition)
    noise_adaptation = self.noise_adapter(condition)
    # Return the feature adaptation and noise adaptation
    return feature_adaptation, noise_adaptation

# Define the contrastive loss function
def contrastive_loss(outputs, conditions):
  # Compute the cosine similarity between outputs and conditions
  similarity = torch.nn.functional.cosine_similarity(outputs, conditions, dim=1)
  # Compute the logits by scaling the similarity by the temperature
  logits = similarity / temperature
  # Compute the labels by assigning positive pairs to 1 and negative pairs to 0
  labels = torch.eye(batch_size).to(device)
  # Compute the cross entropy loss between logits and labels
  loss = torch.nn.functional.cross_entropy(logits, labels)
  # Return the loss
  return loss

# Define a function to sample an image from a distribution parameterized by mean and std
def sample_image(mean, std):
  # Sample a noise image from a standard normal distribution
  noise = torch.randn_like(mean)
  # Scale and shift the noise image by the mean and std
  image = noise * std + mean
  # Clamp the image to be between 0 and 1
  image = torch.clamp(image, 0., 1.)
  # Return the image
  return image

# Define a function to sample a sequence of timesteps from a predefined distribution with resampling probability
def sample_timesteps():
  # Initialize an empty list to store the sampled timesteps
  timesteps = []
  # Initialize a variable to store the current timestep index
  current_timestep_index = num_timesteps - 1
  # Loop until reaching the first timestep index or exceeding the maximum number of timesteps for synthesis
  while current_timestep_index >=0 and len(timesteps) < max_synthesis_timesteps:
    # Append the current timestep index to the list of sampled timesteps
    timesteps.append(current_timestep_index)
    # Sample a new timestep index from a predefined distribution with resampling probability
    if np.random.rand() < resample_prob:
      current_timestep_index = np.random.choice(num_timesteps, p=timestep_distribution)
    else:
      current_timestep_index -= 1
  # Convert the list of sampled timesteps to a tensor and reverse it to ascending order
  timesteps = torch.tensor(timesteps).flip(dims=[0])
  # Return the tensor of sampled timesteps
  return timesteps

# Define a function to check if an early stopping criterion is met based on a threshold of cosine similarity between image and condition
def early_stopping_criterion(image, condition):
  # Compute the cosine similarity between image and condition
  similarity = torch.nn.functional.cosine_similarity(image.flatten(), condition.flatten())
  # Check if the similarity is greater than or equal to the threshold
  if similarity >= early_stopping_threshold:
    return True