---
title: 2302.05905v2 Single Motion Diffusion
date: 2023-02-06
---

# [Single Motion Diffusion](http://arxiv.org/abs/2302.05905v2)

authors: Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H. Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2302.05905 "[2302.05905] Single Motion Diffusion - arXiv.org"
[2]: https://arxiv.org/pdf/2302.05905.pdf "Single Motion Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/1609.05905v2 "[1609.05905v2] Supergravity Backgrounds for Four-Dimensional Maximally ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a Single Motion Diffusion Model (SinMDM), a model designed to learn the internal motifs of a single motion sequence with arbitrary topology and synthesize motions of arbitrary length that are faithful to them.
- **Why**: The paper aims to address the challenge of synthesizing realistic animations of humans, animals, and even imaginary creatures, which have limited or no available data for the motion domain. The paper also aims to provide a versatile model that can be applied in various contexts, such as in-betweening, motion expansion, style transfer, and crowd animation.
- **How**: The paper harnesses the power of diffusion models and presents a denoising network designed specifically for the task of learning from a single input motion. The paper uses a transformer-based architecture that avoids overfitting by using local attention layers that narrow the receptive field, and encourages motion diversity by using relative positional embedding. The paper evaluates the model on various datasets and applications and compares it with existing methods.



## Main Contributions

[1]: https://arxiv.org/abs/2302.05905 "[2302.05905] Single Motion Diffusion - arXiv.org"
[2]: https://arxiv.org/pdf/2302.05905.pdf "Single Motion Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/1609.05905v2 "[1609.05905v2] Supergravity Backgrounds for Four-Dimensional Maximally ..."

The paper[^1^][1] lists the following contributions:

- **A novel diffusion model for single motion synthesis** that learns the internal motifs of a single motion sequence with arbitrary topology and synthesizes motions of arbitrary length that are faithful to them.
- **A transformer-based denoising network** that avoids overfitting by using local attention layers that narrow the receptive field, and encourages motion diversity by using relative positional embedding.
- **A versatile model for various applications** such as spatial and temporal in-betweening, motion expansion, style transfer, and crowd animation, that can be facilitated at inference time without additional training.
- **An extensive evaluation** of the model on various datasets and applications, showing that it outperforms existing methods both in quality and time-space efficiency.


## Method Summary

[1]: https://arxiv.org/abs/2302.05905 "[2302.05905] Single Motion Diffusion - arXiv.org"
[2]: https://arxiv.org/pdf/2302.05905.pdf "Single Motion Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/1609.05905v2 "[1609.05905v2] Supergravity Backgrounds for Four-Dimensional Maximally ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces the **Single Motion Diffusion Model (SinMDM)**, which is based on the diffusion probabilistic model [Sohl-Dickstein et al. 2015]. The model learns to synthesize motions by reversing a Markov chain that gradually corrupts the input motion with Gaussian noise. The model consists of two components: a **denoising network** and a **sampling algorithm**.
- The **denoising network** is a transformer-based network that takes as input a noisy motion sequence and outputs a less noisy one. The network uses local attention layers that limit the receptive field to a fixed window size, which prevents overfitting and preserves local motion patterns. The network also uses relative positional embedding to capture the temporal order and diversity of the motion. The network is trained by minimizing the mean squared error (MSE) between the output and the ground truth motion at each noise level.
- The **sampling algorithm** is an iterative process that starts from a fully corrupted motion sequence and applies the denoising network at each step to reduce the noise level until reaching the original noise-free motion. The algorithm can generate motions of arbitrary length by repeating or concatenating the input motion as needed. The algorithm can also control the diversity of the output by adjusting the noise level or using different random seeds.
- The paper demonstrates how SinMDM can be applied in various contexts, such as **spatial and temporal in-betweening**, **motion expansion**, **style transfer**, and **crowd animation**. The paper also shows how SinMDM can handle motions of arbitrary skeleton topology, such as humans, animals, and imaginary creatures.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```
# Define the parameters of the diffusion model
beta = noise level schedule
T = number of noise levels
sigma = standard deviation of Gaussian noise

# Define the denoising network
D = transformer-based network with local attention and relative positional embedding

# Train the denoising network on a single motion sequence
for t in range(T):
  # Corrupt the motion sequence with Gaussian noise
  x_t = sqrt(1 - beta_t) * x_0 + sqrt(beta_t) * epsilon
  # Predict a less noisy motion sequence
  x_hat_t = D(x_t, t)
  # Minimize the MSE loss
  loss = MSE(x_hat_t, x_0)
  update D with loss

# Sample a new motion sequence from the diffusion model
# Initialize the motion sequence with Gaussian noise
x_T = epsilon
for t in reversed(range(T)):
  # Apply the denoising network to reduce the noise level
  x_hat_t = D(x_T, t)
  # Add some residual noise to increase diversity
  eta = N(0, sigma_t^2 * (1 - beta_t) / beta_t)
  # Update the motion sequence
  x_t = (x_T - sqrt(beta_t) * eta) / sqrt(1 - beta_t)
  # Optionally, repeat or concatenate the motion sequence to change the length
return x_0 # The final noise-free motion sequence
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the parameters of the diffusion model
beta = np.linspace(1e-4, 0.02, 1000) # noise level schedule
T = len(beta) # number of noise levels
sigma = np.sqrt(beta * (1 - beta[::-1])) # standard deviation of Gaussian noise

# Define the hyperparameters of the denoising network
d_model = 256 # dimension of the hidden state
n_head = 8 # number of attention heads
d_ff = 1024 # dimension of the feed-forward layer
n_layer = 6 # number of transformer layers
window_size = 64 # size of the local attention window
dropout = 0.1 # dropout rate

# Define the denoising network
class SinMDM(nn.Module):
  def __init__(self):
    super(SinMDM, self).__init__()
    # Embed the noise level t as a learnable vector
    self.t_embedding = nn.Embedding(T, d_model)
    # Encode the motion sequence x_t as a sequence of joint angles
    self.angle_encoder = AngleEncoder(d_model)
    # Apply a transformer-based network with local attention and relative positional embedding
    self.transformer = Transformer(d_model, n_head, d_ff, n_layer, window_size, dropout)
    # Decode the motion sequence x_hat_t as a sequence of joint angles
    self.angle_decoder = AngleDecoder(d_model)

  def forward(self, x_t, t):
    # Embed the noise level t
    t_emb = self.t_embedding(t)
    # Encode the motion sequence x_t
    x_enc = self.angle_encoder(x_t)
    # Apply the transformer network
    x_trans = self.transformer(x_enc, t_emb)
    # Decode the motion sequence x_hat_t
    x_hat_t = self.angle_decoder(x_trans)
    return x_hat_t

# Define the angle encoder and decoder
class AngleEncoder(nn.Module):
  def __init__(self, d_model):
    super(AngleEncoder, self).__init__()
    # A linear layer to project the joint angles to d_model dimension
    self.linear = nn.Linear(3, d_model)

  def forward(self, x):
    # Reshape x from (batch_size, seq_len, num_joints, 3) to (batch_size, seq_len * num_joints, 3)
    x = x.view(x.size(0), -1, 3)
    # Apply the linear layer
    x = self.linear(x)
    return x

class AngleDecoder(nn.Module):
  def __init__(self, d_model):
    super(AngleDecoder, self).__init__()
    # A linear layer to project the hidden state to joint angles
    self.linear = nn.Linear(d_model, 3)

  def forward(self, x):
    # Apply the linear layer
    x = self.linear(x)
    # Reshape x from (batch_size, seq_len * num_joints, 3) to (batch_size, seq_len, num_joints, 3)
    x = x.view(x.size(0), -1, num_joints, 3)
    return x

# Define the transformer network with local attention and relative positional embedding
class Transformer(nn.Module):
  def __init__(self, d_model, n_head, d_ff, n_layer, window_size, dropout):
    super(Transformer, self).__init__()
    # A list of transformer layers
    self.layers = nn.ModuleList([TransformerLayer(d_model, n_head, d_ff, window_size, dropout) for _ in range(n_layer)])
  
  def forward(self, x, t):
    # Add the noise level embedding to the input
    x = x + t.unsqueeze(1)
    # Apply the transformer layers
    for layer in self.layers:
      x = layer(x)
    return x

# Define the transformer layer with local attention and relative positional embedding
class TransformerLayer(nn.Module):
  def __init__(self, d_model, n_head, d_ff, window_size, dropout):
    super(TransformerLayer, self).__init__()
    # A local attention layer with relative positional embedding
    self.attn = LocalAttention(d_model, n_head, window_size)
    # A feed-forward layer with ReLU activation
    self.ffn = nn.Sequential(
      nn.Linear(d_model, d_ff),
      nn.ReLU(),
      nn.Linear(d_ff,d_model),
      )
    
    # Layer normalization and dropout
    self.norm1 = nn.LayerNorm(d_model)
    self.norm2 = nn.LayerNorm(d_model)
    self.dropout1 = nn.Dropout(dropout)
    self.dropout2 = nn.Dropout(dropout)

  def forward(self, x):
    # Apply the attention layer
    x_attn = self.attn(x)
    # Add and normalize
    x = self.norm1(x + self.dropout1(x_attn))
    # Apply the feed-forward layer
    x_ffn = self.ffn(x)
    # Add and normalize
    x = self.norm2(x + self.dropout2(x_ffn))
    return x

# Define the local attention layer with relative positional embedding
class LocalAttention(nn.Module):
  def __init__(self, d_model, n_head, window_size):
    super(LocalAttention, self).__init__()
    # Check if d_model is divisible by n_head
    assert d_model % n_head == 0
    # The dimension of each head
    self.d_head = d_model // n_head
    # The number of heads
    self.n_head = n_head
    # The size of the local window
    self.window_size = window_size
    # Linear layers for projecting the queries, keys, values, and outputs
    self.q_proj = nn.Linear(d_model, d_model)
    self.k_proj = nn.Linear(d_model, d_model)
    self.v_proj = nn.Linear(d_model, d_model)
    self.o_proj = nn.Linear(d_model, d_model)
    # A learnable relative positional embedding matrix
    self.r_emb = nn.Parameter(torch.randn(window_size * 2 + 1, self.d_head))
  
  def forward(self, x):
    # Get the batch size and sequence length
    batch_size, seq_len, _ = x.size()
    # Project the queries, keys, and values
    q = self.q_proj(x) # (batch_size, seq_len, d_model)
    k = self.k_proj(x) # (batch_size, seq_len, d_model)
    v = self.v_proj(x) # (batch_size, seq_len, d_model)
    
    # Reshape and split the heads
    q = q.view(batch_size, seq_len, self.n_head, self.d_head).transpose(1, 2) # (batch_size, n_head, seq_len, d_head)
    k = k.view(batch_size, seq_len, self.n_head, self.d_head).transpose(1, 2) # (batch_size, n_head, seq_len, d_head)
    v = v.view(batch_size, seq_len, self.n_head, self.d_head).transpose(1, 2) # (batch_size, n_head, seq_len, d_head)

    # Compute the attention score for each local window
    score = torch.empty(batch_size, self.n_head, seq_len, self.window_size * 2 + 1) # (batch_size,n_head ,seq_len ,window_size * 2 + 1)
    
    for i in range(-self.window_size ,self.window_size + 1):
      if i < 0:
        score[:, :, -i: ,i] = torch.einsum('bnid,bnjd->bnij', q[:, :, :-i], k[:, :, i:]) + self.r_emb[self.window_size + i].unsqueeze(0).unsqueeze(0).unsqueeze(-2) 
      elif i > 0:
        score[:, :, :-i ,i] = torch.einsum('bnid,bnjd->bnij', q[:, :, i:], k[:, :, :-i]) + self.r_emb[self.window_size + i].unsqueeze(0).unsqueeze(0).unsqueeze(-2) 
      else:
        score[:, :, :, i] = torch.einsum('bnid,bnjd->bnij', q ,k) + self.r_emb[self.window_size + i].unsqueeze(0).unsqueeze(0).unsqueeze(-2) 
    
     # Apply softmax to get the attention weights
     weight = F.softmax(score / np.sqrt(self.d_head), dim=-1) # (batch_size,n_head ,seq_len ,window_size * 2 + 1)

     # Compute the weighted sum of the values for each local window
     output = torch.empty(batch_size,self.n_head ,seq_len ,self.d_head) # (batch_size,n_head ,seq_len ,d_head)

     for i in range(-self.window_size ,self.window_size + 1):
       if i < 0:
         output[:, :, -i:] += weight[:, :, -i:, i].unsqueeze(-1) * v[:, :, i:] 
       elif i > 0:
         output[:, :, :-i] += weight[:, :, :-i ,i].unsqueeze(-1) * v[:, :, :-i] 
       else:
         output += weight[:, :, :,