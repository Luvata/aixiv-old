---
title: 2106.09679v1 JOKR  Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting
date: 2021-06-10
---

# [JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting](http://arxiv.org/abs/2106.09679v1)

authors: Ron Mokady, Rotem Tzaban, Sagie Benaim, Amit H. Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2106.09679 "[2106.09679] JOKR: Joint Keypoint Representation for ... - arXiv.org"
[2]: https://arxiv.org/pdf/2106.09679v1 "arXiv.org"
[3]: http://export.arxiv.org/abs/2206.09679v1 "[2206.09679v1] Phoebe: QoS-Aware Distributed Stream Processing through ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel method for unsupervised cross-domain motion retargeting in videos, which can transfer the motion of one object to another object with different shape, appearance and orientation. The method is based on a joint keypoint representation (JOKR) that captures the common motion between the source and target videos, and a learned affine transformation that aligns the keypoints across domains. The method can handle various scenarios such as animals, flowers and humans, and enables intuitive control and editing of the generated videos.
- **Why**: The paper aims to address the limitations of existing methods for unsupervised motion retargeting, which either require specific object priors or fail to handle different shapes and orientations. The paper also aims to demonstrate the benefits of using a geometry-driven representation that is affine invariant and disentangled from appearance and motion style.
- **How**: The paper introduces a deep neural network that consists of three components: a keypoint extractor that maps each video frame to a set of keypoints, a domain confusion module that enforces the keypoints of both videos to be indistinguishable, and a video generator that synthesizes new frames based on the keypoints and the target appearance. The network is trained with an adversarial loss, a reconstruction loss, a cycle-consistency loss and an affine loss. The paper evaluates the method both qualitatively and quantitatively on various cross-domain video pairs, and compares it with state-of-the-art alternatives. The paper also shows some applications of the method such as temporal coherence, manual editing and interpolation.

## Main Contributions

[1]: https://arxiv.org/abs/2106.09679 "[2106.09679] JOKR: Joint Keypoint Representation for ... - arXiv.org"
[2]: https://arxiv.org/pdf/2106.09679v1 "arXiv.org"
[3]: http://export.arxiv.org/abs/2206.09679v1 "[2206.09679v1] Phoebe: QoS-Aware Distributed Stream Processing through ..."

The paper claims the following contributions:

- **A novel method for unsupervised cross-domain motion retargeting in videos**, which can transfer the motion of one object to another object with different shape, appearance and orientation, without requiring any object prior or data collection.
- **A joint keypoint representation (JOKR)** that captures the common motion between the source and target videos, and is enforced to be domain-invariant by a domain confusion term.
- **A learned affine transformation** that aligns the keypoints across domains, and augments the representation to be affine invariant. This enables the method to handle cases where the objects are of different proportions or orientations, and broadens the variety of possible retargeting pairs.
- **A geometry-driven representation** that enables further intuitive control and editing of the generated videos, such as temporal coherence, manual editing and interpolation.
- **Comprehensive experiments** on various cross-domain video pairs, such as different animals, different flowers, and humans. The paper demonstrates that the method handles various cross-domain scenarios, and shows superior temporal coherency and visual quality compared to state-of-the-art alternatives, through statistical metrics and a user study. The paper also provides source code and videos at this https URL.

## Method Summary

[1]: https://arxiv.org/abs/2106.09679 "[2106.09679] JOKR: Joint Keypoint Representation for ... - arXiv.org"
[2]: https://arxiv.org/pdf/2106.09679v1 "arXiv.org"
[3]: http://export.arxiv.org/abs/2206.09679v1 "[2206.09679v1] Phoebe: QoS-Aware Distributed Stream Processing through ..."

Here is a summary of the method section of the paper:

- The paper introduces a deep neural network that consists of three components: a keypoint extractor, a domain confusion module, and a video generator.
- The keypoint extractor maps each video frame to a set of keypoints that represent the salient features of the object. The keypoints are learned in an unsupervised manner by minimizing a reconstruction loss between the original frame and a synthesized frame based on the keypoints and the appearance code. The keypoints are also constrained to be sparse and evenly distributed by a sparsity loss and a diversity loss.
- The domain confusion module enforces the keypoints of both videos to be indistinguishable by using a domain classifier that tries to predict which video the keypoints belong to, and a domain confusion loss that tries to fool the classifier. This encourages disentanglement between the parts of the motion that are common to the two domains, and their distinctive appearance and motion style.
- The video generator synthesizes new frames based on the keypoints and the target appearance code. The generator consists of two sub-networks: a spatial transformer network that applies a learned affine transformation between the keypoints of the source and target videos, and a conditional generative adversarial network that generates realistic frames conditioned on the transformed keypoints and the target appearance code. The generator is trained with an adversarial loss, a reconstruction loss, a cycle-consistency loss and an affine loss.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: source video S, target video T
# Output: retargeted video R

# Define network components
keypoint_extractor = KeypointExtractor()
domain_classifier = DomainClassifier()
video_generator = VideoGenerator()

# Initialize network parameters
theta_k, theta_c, theta_g = initialize_parameters()

# Train the network
for epoch in range(num_epochs):
  # Sample a batch of frames from S and T
  S_batch = sample_frames(S)
  T_batch = sample_frames(T)

  # Extract keypoints and appearance codes from S and T
  K_S, A_S = keypoint_extractor(S_batch)
  K_T, A_T = keypoint_extractor(T_batch)

  # Apply domain confusion between K_S and K_T
  D_S = domain_classifier(K_S)
  D_T = domain_classifier(K_T)
  L_confusion = domain_confusion_loss(D_S, D_T)

  # Apply affine transformation between K_S and K_T
  K_ST = spatial_transformer(K_S, theta_g)
  K_TS = spatial_transformer(K_T, theta_g)

  # Generate frames from keypoints and appearance codes
  S_hat = video_generator(K_S, A_S)
  T_hat = video_generator(K_T, A_T)
  S_cycle = video_generator(K_TS, A_S)
  T_cycle = video_generator(K_ST, A_T)

  # Compute reconstruction loss
  L_recon = reconstruction_loss(S_batch, S_hat) + reconstruction_loss(T_batch, T_hat)

  # Compute cycle-consistency loss
  L_cycle = cycle_consistency_loss(S_batch, S_cycle) + cycle_consistency_loss(T_batch, T_cycle)

  # Compute adversarial loss
  L_adv = adversarial_loss(S_hat) + adversarial_loss(T_hat)

  # Compute affine loss
  L_affine = affine_loss(K_ST) + affine_loss(K_TS)

  # Compute total loss
  L_total = L_confusion + L_recon + L_cycle + L_adv + L_affine

  # Update network parameters
  theta_k, theta_c, theta_g = update_parameters(theta_k, theta_c, theta_g, L_total)

# Retarget the motion from S to T
R = video_generator(K_ST, A_T)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import numpy as np

# Define hyperparameters
num_epochs = 100
batch_size = 16
num_keypoints = 10
learning_rate = 0.0002
lambda_confusion = 1.0
lambda_recon = 10.0
lambda_cycle = 10.0
lambda_adv = 1.0
lambda_affine = 1.0

# Define network components

# KeypointExtractor: a convolutional encoder-decoder network that maps each video frame to a set of keypoints and an appearance code
class KeypointExtractor(nn.Module):
  def __init__(self):
    super(KeypointExtractor, self).__init__()
    # Encoder: a series of convolutional layers with downsampling and leaky ReLU activation
    self.encoder = nn.Sequential(
      nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1), # output: (64, 128, 128)
      nn.LeakyReLU(0.2),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # output: (128, 64, 64)
      nn.BatchNorm2d(128),
      nn.LeakyReLU(0.2),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # output: (256, 32, 32)
      nn.BatchNorm2d(256),
      nn.LeakyReLU(0.2),
      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), # output: (512, 16, 16)
      nn.BatchNorm2d(512),
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1), # output: (1024, 8, 8)
      nn.BatchNorm2d(1024),
      nn.LeakyReLU(0.2),
    )
    # Decoder: a series of convolutional layers with upsampling and ReLU activation
    self.decoder = nn.Sequential(
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # output: (512, 16, 16)
      nn.BatchNorm2d(512),
      nn.ReLU(),
      nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), # output: (256, 32, 32)
      nn.BatchNorm2d(256),
      nn.ReLU(),
      nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # output: (128, 64 ,64)
      nn.BatchNorm2d(128),
      nn.ReLU(),
      nn.ConvTranspose2d(128 ,64 ,kernel_size=4 ,stride=2 ,padding=1), # output: (64 ,128 ,128)
      nn.BatchNorm2d(64),
      nn.ReLU(),
    )
    # Keypoint head: a convolutional layer that outputs a heatmap for each keypoint
    self.keypoint_head = nn.Conv2d(64 ,num_keypoints ,kernel_size=1) # output: (num_keypoints ,128 ,128)
    # Appearance head: a convolutional layer that outputs an appearance code for each frame
    self.appearance_head = nn.Conv2d(64 ,3 ,kernel_size=1) # output: (3 ,128 ,128)

  def forward(self ,x):
    # x: input video frame of shape (3 ,256 ,256)
    # Encode the frame to a latent representation
    z = self.encoder(x) # z: latent representation of shape (1024 ,8 ,8)
    # Decode the latent representation to a feature map
    f = self.decoder(z) # f: feature map of shape (64 ,128 ,128)
    # Extract keypoints and appearance code from the feature map
    k = self.keypoint_head(f) # k: keypoint heatmap of shape (num_keypoints ,128 ,128)
    a = self.appearance_head(f) # a: appearance code of shape (3 ,128 ,128)
    return k ,a

# DomainClassifier: a fully-connected network that predicts which video the keypoints belong to
class DomainClassifier(nn.Module):
  def __init__(self):
    super(DomainClassifier, self).__init__()
    # Flatten the keypoints to a vector
    self.flatten = nn.Flatten()
    # Define a series of linear layers with dropout and leaky ReLU activation
    self.linear = nn.Sequential(
      nn.Linear(num_keypoints * 128 * 128, 512),
      nn.Dropout(0.2),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 256),
      nn.Dropout(0.2),
      nn.LeakyReLU(0.2),
      nn.Linear(256, 128),
      nn.Dropout(0.2),
      nn.LeakyReLU(0.2),
      nn.Linear(128, 1),
    )

  def forward(self ,k):
    # k: keypoint heatmap of shape (num_keypoints ,128 ,128)
    # Flatten the keypoints to a vector
    k_flat = self.flatten(k) # k_flat: flattened keypoints of shape (num_keypoints * 128 * 128)
    # Predict the domain label
    d = self.linear(k_flat) # d: domain label of shape (1)
    return d

# VideoGenerator: a network that synthesizes new frames based on the keypoints and the target appearance code
class VideoGenerator(nn.Module):
  def __init__(self):
    super(VideoGenerator, self).__init__()
    # Spatial transformer network: a network that applies a learned affine transformation between the keypoints of the source and target videos
    self.stn = SpatialTransformerNetwork()
    # Conditional generative adversarial network: a network that generates realistic frames conditioned on the transformed keypoints and the target appearance code
    self.cgan = ConditionalGAN()

  def forward(self ,k_s ,a_t):
    # k_s: source keypoint heatmap of shape (num_keypoints ,128 ,128)
    # a_t: target appearance code of shape (3 ,128 ,128)
    # Apply affine transformation to the source keypoints
    k_st = self.stn(k_s) # k_st: transformed source keypoint heatmap of shape (num_keypoints ,128 ,128)
    # Generate new frames conditioned on the transformed keypoints and the target appearance code
    x_hat = self.cgan(k_st ,a_t) # x_hat: generated frame of shape (3 ,256 ,256)
    return x_hat

# SpatialTransformerNetwork: a network that applies a learned affine transformation between the keypoints of the source and target videos
class SpatialTransformerNetwork(nn.Module):
  def __init__(self):
    super(SpatialTransformerNetwork, self).__init__()
    # Localization network: a convolutional network that predicts the parameters of the affine transformation
    self.localization = nn.Sequential(
      nn.Conv2d(num_keypoints, 32, kernel_size=7), # output: (32, 122, 122)
      nn.MaxPool2d(2, stride=2), # output: (32, 61, 61)
      nn.ReLU(),
      nn.Conv2d(32, 64, kernel_size=5), # output: (64, 57, 57)
      nn.MaxPool2d(2, stride=2), # output: (64, 28, 28)
      nn.ReLU(),
    )
    # Flatten the output of the localization network to a vector
    self.flatten = nn.Flatten()
    # Regressor: a linear layer that outputs six parameters for the affine transformation
    self.regressor = nn.Linear(64 * 28 * 28, 6)

  def forward(self ,k_s):
    # k_s: source keypoint heatmap of shape (num_keypoints ,128 ,128)
    # Predict the parameters of the affine transformation
    theta = self.localization(k_s) # theta: localization output of shape (64 ,28 ,28)
    theta = self.flatten(theta) # theta: flattened localization output of shape (64 * 28 * 28)
    theta = self.regressor(theta) # theta: affine parameters of shape (6)
    theta = theta.view(-1 ,2 ,3) # theta: reshaped affine parameters of shape (2 ,3)
    # Apply the affine transformation to the source keypoints
    grid = F.affine_grid(theta ,k_s.size()) # grid: sampling grid of shape (num_keypoints ,128 ,128 ,2)
    k_st = F.grid_sample(k_s ,grid) # k_st: transformed source keypoint heatmap of shape (num_keypoints ,128 ,128)
    return k_st

# ConditionalGAN: a network that generates realistic frames conditioned on the transformed keypoints and the target appearance code
class ConditionalGAN(nn.Module):
  def __init__(self):
    super(ConditionalGAN, self).__init__()
    # Generator: a U-Net-like network that generates new frames from the transformed keypoints and the