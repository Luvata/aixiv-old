---
title: 2212.09748v2 Scalable Diffusion Models with Transformers
date: 2022-12-10
---

# [Scalable Diffusion Models with Transformers](http://arxiv.org/abs/2212.09748v2)

authors: William Peebles, Saining Xie


## What, Why and How

[1]: https://arxiv.org/abs/2212.09748 "[2212.09748] Scalable Diffusion Models with Transformers - arXiv.org"
[2]: https://arxiv.org/abs/1311.2524 "[1311.2524] Rich feature hierarchies for accurate object detection and ..."
[3]: http://export.arxiv.org/abs/2203.09748v2 "[2203.09748v2] Convex Optimization-Based Structure-Preserving Filter ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new class of diffusion models based on the transformer architecture for image generation. Diffusion models are a type of generative models that learn to reverse a stochastic process that gradually corrupts an image until it becomes pure noise. Transformers are a type of neural network that use attention mechanisms to capture long-range dependencies among data.
- **Why**: The paper aims to improve the scalability and performance of diffusion models by replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. U-Nets are a type of convolutional neural network that have skip connections between encoder and decoder layers. Latent patches are low-dimensional representations of image patches obtained by applying a linear projection to the original pixels.
- **How**: The paper introduces Diffusion Transformers (DiTs), which consist of three components: a patch embedding layer, a transformer encoder-decoder, and a patch reconstruction layer. The patch embedding layer maps each image patch to a latent vector using a learned linear projection. The transformer encoder-decoder takes the latent vectors as input and applies multiple layers of self-attention and feed-forward networks to model the conditional distribution of the clean image given the corrupted image. The patch reconstruction layer converts the output latent vectors back to image patches using another learned linear projection. The paper also analyzes the scalability of DiTs in terms of forward pass complexity as measured by Gflops, and shows that DiTs with higher Gflops achieve lower FID scores on ImageNet benchmarks. FID is a metric that measures the quality and diversity of generated images by comparing their feature statistics with those of real images.

[^1^][1]: [2212.09748] Scalable Diffusion Models with Transformers - arXiv.org


## Main Contributions

The paper claims to make the following contributions:

- It proposes a new class of diffusion models based on the transformer architecture, which can scale to high-resolution images and outperform previous diffusion models on ImageNet benchmarks.
- It introduces a patch embedding layer and a patch reconstruction layer to enable transformers to operate on latent patches, which reduces the memory and computational requirements of the model.
- It provides a theoretical analysis of the forward pass complexity of diffusion models in terms of Gflops, and empirically shows that increasing the Gflops of DiTs leads to lower FID scores.
- It presents extensive ablation studies and qualitative results to demonstrate the effectiveness and diversity of DiTs for image generation.

## Method Summary

The method section of the paper consists of four subsections:

- **Diffusion Models**: This subsection reviews the basics of diffusion models, which are a type of generative models that learn to reverse a stochastic process that gradually corrupts an image until it becomes pure noise. The paper focuses on latent diffusion models, which use a latent variable to model the conditional distribution of the clean image given the corrupted image at each diffusion step.
- **Diffusion Transformers**: This subsection introduces the main contribution of the paper, which is a new class of diffusion models based on the transformer architecture. The paper proposes Diffusion Transformers (DiTs), which consist of three components: a patch embedding layer, a transformer encoder-decoder, and a patch reconstruction layer. The patch embedding layer maps each image patch to a latent vector using a learned linear projection. The transformer encoder-decoder takes the latent vectors as input and applies multiple layers of self-attention and feed-forward networks to model the conditional distribution of the clean image given the corrupted image. The patch reconstruction layer converts the output latent vectors back to image patches using another learned linear projection.
- **Forward Pass Complexity**: This subsection provides a theoretical analysis of the forward pass complexity of diffusion models in terms of Gflops, which is defined as the number of floating-point operations per second. The paper shows that the forward pass complexity of DiTs is proportional to the number of input tokens, the transformer depth, and the transformer width. The paper also empirically shows that increasing the Gflops of DiTs leads to lower FID scores on ImageNet benchmarks.
- **Implementation Details**: This subsection describes the details of the model architecture, training procedure, and evaluation metrics used in the paper. The paper uses a patch size of 16x16 for all experiments, and varies the number of input tokens from 256 to 1024. The paper uses a transformer depth of 12 or 24 layers, and a transformer width of 512 or 1024 hidden units. The paper uses AdamW as the optimizer, and trains the models for 300k iterations with a batch size of 64 or 128. The paper uses FID as the main evaluation metric, and also reports IS and PRD scores for some experiments. IS is a metric that measures the diversity of generated images by computing their entropy. PRD is a metric that measures the quality and diversity of generated images by comparing their precision and recall with respect to real images.

: [2212.09748] Scalable Diffusion Models with Transformers - arXiv.org


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the patch embedding layer
def patch_embedding(x):
  # x is a batch of images of size B x H x W x C
  # Divide x into patches of size P x P
  patches = split_patches(x, P)
  # Flatten patches to vectors of size P*P*C
  patches = flatten(patches)
  # Project patches to latent vectors of size D using a learned matrix E
  latent_vectors = matmul(patches, E)
  # Return latent vectors of size B x N x D, where N = H*W/P*P
  return latent_vectors

# Define the patch reconstruction layer
def patch_reconstruction(z):
  # z is a batch of latent vectors of size B x N x D
  # Project latent vectors to patch vectors of size P*P*C using a learned matrix R
  patch_vectors = matmul(z, R)
  # Reshape patch vectors to patches of size P x P x C
  patches = reshape(patch_vectors, P, P, C)
  # Stitch patches together to form images of size H x W x C
  images = stitch_patches(patches, H, W)
  # Return images of size B x H x W x C
  return images

# Define the transformer encoder-decoder
def transformer_encoder_decoder(z):
  # z is a batch of latent vectors of size B x N x D
  # Apply L layers of self-attention and feed-forward networks to z
  for l in range(L):
    # Apply self-attention to z with residual connection and layer normalization
    z = layer_norm(z + self_attention(z))
    # Apply feed-forward network to z with residual connection and layer normalization
    z = layer_norm(z + feed_forward(z))
  # Return the output latent vectors of size B x N x D
  return z

# Define the diffusion model
def diffusion_model(x):
  # x is a batch of clean images of size B x H x W x C
  # Initialize the noise level beta and the loss function L
  beta = initial_beta()
  L = zero()
  # Loop over T diffusion steps from t = T-1 to t = 0
  for t in reversed(range(T)):
    # Corrupt the clean image with Gaussian noise according to beta
    epsilon = sample_gaussian_noise(beta)
    y = x + epsilon
    # Embed the corrupted image into latent vectors using patch embedding layer
    z = patch_embedding(y)
    # Encode and decode the latent vectors using transformer encoder-decoder
    z_hat = transformer_encoder_decoder(z)
    # Reconstruct the corrupted image from the output latent vectors using patch reconstruction layer
    y_hat = patch_reconstruction(z_hat)
    # Compute the negative log-likelihood loss between y and y_hat and add it to L
    L += nll_loss(y, y_hat)
    # Update beta according to a predefined schedule
    beta = update_beta(beta)
  # Return the total loss L
  return L

# Train the diffusion model on a dataset of images X using AdamW optimizer
def train_diffusion_model(X):
  # Initialize the model parameters randomly
  initialize_parameters()
  # Loop over K iterations with a batch size of B
  for k in range(K):
    # Sample a batch of images from X
    x = sample_batch(X, B)
    # Compute the loss of the diffusion model on x
    L = diffusion_model(x)
    # Update the model parameters using AdamW optimizer to minimize L
    update_parameters(L)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import tensorflow as tf

# Define some hyperparameters
P = 16 # patch size
D = 512 # latent dimension
L = 12 # transformer depth
H = 8 # transformer head number
F = 2048 # transformer feed-forward dimension
T = 1000 # diffusion steps
K = 300000 # training iterations
B = 64 # batch size
LR = 1e-4 # learning rate
WD = 1e-4 # weight decay

# Define the patch embedding layer as a subclass of tf.keras.layers.Layer
class PatchEmbedding(tf.keras.layers.Layer):
  def __init__(self, P, D):
    super(PatchEmbedding, self).__init__()
    # Initialize the projection matrix E with random values from a normal distribution
    self.E = tf.Variable(tf.random.normal((P*P*3, D)), trainable=True)

  def call(self, x):
    # x is a batch of images of size B x H x W x C
    # Divide x into patches of size P x P using tf.image.extract_patches
    patches = tf.image.extract_patches(x, sizes=[1, P, P, 1], strides=[1, P, P, 1], rates=[1, 1, 1, 1], padding='VALID')
    # Flatten patches to vectors of size P*P*C using tf.reshape
    patches = tf.reshape(patches, (-1, P*P*3))
    # Project patches to latent vectors of size D using tf.matmul and self.E
    latent_vectors = tf.matmul(patches, self.E)
    # Return latent vectors of size B x N x D, where N = H*W/P*P using tf.reshape
    return tf.reshape(latent_vectors, (-1, x.shape[1]//P * x.shape[2]//P, D))

# Define the patch reconstruction layer as a subclass of tf.keras.layers.Layer
class PatchReconstruction(tf.keras.layers.Layer):
  def __init__(self, P, D):
    super(PatchReconstruction, self).__init__()
    # Initialize the projection matrix R with random values from a normal distribution
    self.R = tf.Variable(tf.random.normal((D, P*P*3)), trainable=True)

  def call(self, z):
    # z is a batch of latent vectors of size B x N x D
    # Project latent vectors to patch vectors of size P*P*C using tf.matmul and self.R
    patch_vectors = tf.matmul(z, self.R)
    # Reshape patch vectors to patches of size P x P x C using tf.reshape
    patches = tf.reshape(patch_vectors, (-1, z.shape[1], P, P, 3))
    # Stitch patches together to form images of size H x W x C using tf.image.batch_to_space_nd
    images = tf.image.batch_to_space_nd(patches, crops=[[0,0],[0,0]], block_shape=P)
    # Return images of size B x H x W x C
    return images

# Define the transformer encoder-decoder as a subclass of tf.keras.Model
class TransformerEncoderDecoder(tf.keras.Model):
  def __init__(self, L, H, F):
    super(TransformerEncoderDecoder, self).__init__()
    # Initialize L layers of multi-head self-attention and feed-forward networks using tf.keras.layers.MultiHeadAttention and tf.keras.layers.Dense
    self.attention_layers = [tf.keras.layers.MultiHeadAttention(num_heads=H) for _ in range(L)]
    self.feed_forward_layers = [tf.keras.layers.Dense(F) for _ in range(L)]
  
  def call(self, z):
    # z is a batch of latent vectors of size B x N x D
    # Apply L layers of self-attention and feed-forward networks to z with residual connection and layer normalization using tf.keras.layers.Add and tf.keras.layers.LayerNormalization
    for i in range(L):
      # Apply self-attention to z with residual connection and layer normalization
      z_att = self.attention_layers[i](z,z,z)
      z = tf.keras.layers.Add()([z,z_att])
      z = tf.keras.layers.LayerNormalization()(z)
      # Apply feed-forward network to z with residual connection and layer normalization
      z_ffn = self.feed_forward_layers[i](z)
      z = tf.keras.layers.Add()([z,z_ffn])
      z = tf.keras.layers.LayerNormalization()(z)
    # Return the output latent vectors of size B x N x D
    return z

# Define the diffusion model as a subclass of tf.keras.Model
class DiffusionModel(tf.keras.Model):
  def __init__(self, P, D, L, H, F, T):
    super(DiffusionModel, self).__init__()
    # Initialize the patch embedding layer, the patch reconstruction layer, and the transformer encoder-decoder
    self.patch_embedding = PatchEmbedding(P, D)
    self.patch_reconstruction = PatchReconstruction(P, D)
    self.transformer_encoder_decoder = TransformerEncoderDecoder(L, H, F)
    # Initialize the noise level beta and the loss function
    self.beta = tf.Variable(tf.ones(T), trainable=False)
    self.loss = tf.keras.losses.MeanSquaredError()

  def call(self, x):
    # x is a batch of clean images of size B x H x W x C
    # Loop over T diffusion steps from t = T-1 to t = 0
    for t in reversed(range(T)):
      # Corrupt the clean image with Gaussian noise according to beta using tf.random.normal and tf.math.sqrt
      epsilon = tf.random.normal(x.shape) * tf.math.sqrt(self.beta[t])
      y = x + epsilon
      # Embed the corrupted image into latent vectors using patch embedding layer
      z = self.patch_embedding(y)
      # Encode and decode the latent vectors using transformer encoder-decoder
      z_hat = self.transformer_encoder_decoder(z)
      # Reconstruct the corrupted image from the output latent vectors using patch reconstruction layer
      y_hat = self.patch_reconstruction(z_hat)
      # Compute the negative log-likelihood loss between y and y_hat and add it to L using self.loss and tf.math.log
      L += -tf.math.log(self.loss(y, y_hat))
      # Update beta according to a predefined schedule using tf.assign
      self.beta.assign(update_beta(self.beta))
    # Return the total loss L
    return L

# Define a function to update beta according to a predefined schedule
def update_beta(beta):
  # beta is a tensor of size T
  # Use a cosine schedule as described in the paper using tf.math.cos and tf.constant
  alpha_bar = tf.constant(0.9999)
  alpha_0 = tf.constant(1e-4)
  alpha_T = alpha_bar + (1 - alpha_bar) * (1 + tf.math.cos(np.pi * T)) / 2
  alpha_t = alpha_bar + (1 - alpha_bar) * (1 + tf.math.cos(np.pi * t / T)) / 2
  beta_t = (alpha_t - alpha_0) / (alpha_T - alpha_0) * (1 - alpha_0) + alpha_0
  # Return the updated beta as a tensor of size T using tf.reshape
  return tf.reshape(beta_t, (-1))

# Train the diffusion model on a dataset of images X using AdamW optimizer
def train_diffusion_model(X):
  # Initialize the model with random values from a normal distribution using DiffusionModel and tf.random.normal
  model = DiffusionModel(P, D, L, H, F, T)
  model.build((B,H,W,C))
  model.set_weights(tf.random.normal(model.get_weights()))
  # Initialize the optimizer with learning rate LR and weight decay WD using tf.keras.optimizers.AdamW
  optimizer = tf.keras.optimizers.AdamW(learning_rate=LR, weight_decay=WD)
  # Loop over K iterations with a batch size of B using tf.data.Dataset.from_tensor_slices and .batch
  dataset = tf.data.Dataset.from_tensor_slices(X).batch(B)
  for k in range(K):
    # Sample a batch of images from X using .next and .as_numpy_iterator
    x = next(dataset.as_numpy_iterator())
    # Compute the loss of the diffusion model on x using model.call and tf.GradientTape
    with tf.GradientTape() as tape:
      L = model(x)
    # Update the model parameters using optimizer.apply_gradients and tape.gradient to minimize L
    gradients = tape.gradient(L, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```