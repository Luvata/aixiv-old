---
title: 2306.06874v2 VillanDiffusion  A Unified Backdoor Attack Framework for Diffusion Models
date: 2023-06-07
---

# [VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models](http://arxiv.org/abs/2306.06874v2)

authors: Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho


## What, Why and How

[1]: https://arxiv.org/pdf/2306.06874v2.pdf "VillanDiffusion: A Unified Backdoor Attack Framework for ... - arXiv.org"
[2]: https://arxiv.org/pdf/2106.06874v2.pdf "Alessandro Torrielli Department of Mathematics, University ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/pdf/2102.06874v2 "arXiv:2102.06874v2 [astro-ph.CO] 21 Jun 2021"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- What: The paper presents a unified backdoor attack framework for diffusion models (DMs), which are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. The framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations.
- Why: The paper aims to expand the current scope of backdoor analysis for DMs, which have been shown to be vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. The paper also provides new insights into caption-based backdoor attacks on DMs.
- How: The paper proposes a method called VillanDiffusion, which consists of three steps: (1) embedding a backdoor trigger into the model parameters or the training data; (2) generating backdoored samples using different samplers; and (3) evaluating the attack performance using various metrics. The paper also compares VillanDiffusion with existing backdoor attack methods on DMs and demonstrates its effectiveness and versatility.



## Main Contributions

[1]: https://arxiv.org/pdf/2306.06874v2.pdf "VillanDiffusion: A Unified Backdoor Attack Framework for ... - arXiv.org"
[2]: https://arxiv.org/pdf/2106.06874v2.pdf "Alessandro Torrielli Department of Mathematics, University ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/pdf/2102.06874v2 "arXiv:2102.06874v2 [astro-ph.CO] 21 Jun 2021"

According to the paper[^1^][1], the main contributions are:

- A unified backdoor attack framework for diffusion models (DMs) that covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations.
- A novel method called VillanDiffusion that embeds a backdoor trigger into the model parameters or the training data, generates backdoored samples using different samplers, and evaluates the attack performance using various metrics.
- A comprehensive experimental study that compares VillanDiffusion with existing backdoor attack methods on DMs and demonstrates its effectiveness and versatility on different DM configurations and samplers.
- A new insight into caption-based backdoor attacks on DMs that shows how modifying the text encoder can lead to different attack behaviors and how to mitigate them by using a robust text encoder.


## Method Summary

[1]: https://arxiv.org/pdf/2306.06874v2.pdf "VillanDiffusion: A Unified Backdoor Attack Framework for ... - arXiv.org"
[2]: https://arxiv.org/pdf/2106.06874v2.pdf "Alessandro Torrielli Department of Mathematics, University ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/pdf/2102.06874v2 "arXiv:2102.06874v2 [astro-ph.CO] 21 Jun 2021"

The method section of the paper[^1^][1] describes the following steps:

- Embedding a backdoor trigger into the model parameters or the training data. The paper considers two types of triggers: image triggers and caption triggers. Image triggers are small patches that are added to the input images, while caption triggers are words or phrases that are inserted into the input captions. The paper also considers two types of embedding methods: parameter embedding and data embedding. Parameter embedding modifies the model parameters to embed the trigger, while data embedding modifies the training data to embed the trigger.
- Generating backdoored samples using different samplers. The paper considers three types of samplers: ancestral sampler, DPM Solver , and DEIS . Ancestral sampler is a basic sampler that generates samples by reversing the diffusion process from noise to data. DPM Solver and DEIS are advanced samplers that use diffusion probabilistic models (DPMs) and diffusion energy-inspired samplers (DEISs) to generate samples more efficiently and accurately.
- Evaluating the attack performance using various metrics. The paper considers four metrics: attack success rate (ASR), attack success rate under perturbation (ASR-P), attack success rate under defense (ASR-D), and normality score (NS). ASR measures the percentage of backdoored samples that exhibit the desired behavior when the trigger is present. ASR-P measures the robustness of the attack under different types of perturbations, such as Gaussian noise, JPEG compression, and cropping. ASR-D measures the resistance of the attack against different types of defenses, such as fine-tuning, pruning, and distillation. NS measures the quality of the backdoored samples compared to normal samples when the trigger is absent.


## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper:

```python
# Define the backdoor trigger (image or caption) and the embedding method (parameter or data)
trigger = create_trigger(type="image", shape=(3, 3), color="red")
embedding = create_embedding(method="parameter", model=ddpm)

# Embed the trigger into the model parameters or the training data
embedding.embed(trigger)

# Define the sampler (ancestral, DPM Solver, or DEIS) and the desired behavior (e.g. generate a specific image)
sampler = create_sampler(type="ancestral", model=ddpm)
behavior = create_behavior(type="image", content="cat")

# Generate backdoored samples using the sampler
samples = sampler.generate(trigger, behavior)

# Define the metrics (ASR, ASR-P, ASR-D, NS) and the perturbations and defenses to apply
metrics = create_metrics(["ASR", "ASR-P", "ASR-D", "NS"])
perturbations = create_perturbations(["noise", "compression", "cropping"])
defenses = create_defenses(["fine-tuning", "pruning", "distillation"])

# Evaluate the attack performance using the metrics
for metric in metrics:
  score = metric.evaluate(samples, trigger, behavior, perturbations, defenses)
  print(score)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt

# Define the backdoor trigger (image or caption) and the embedding method (parameter or data)
class Trigger:
  def __init__(self, type, shape=None, color=None, text=None):
    self.type = type # "image" or "caption"
    if self.type == "image":
      self.shape = shape # a tuple of (height, width)
      self.color = color # a string of "red", "green", or "blue"
      self.image = self.create_image() # a tensor of shape (3, height, width)
    elif self.type == "caption":
      self.text = text # a string of words or phrases
      self.tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2') # a tokenizer for captions
      self.caption = self.create_caption() # a tensor of shape (1, length)

  def create_image(self):
    # Create a black image of the given shape
    image = torch.zeros((3, self.shape[0], self.shape[1]))
    # Fill the center pixel with the given color
    if self.color == "red":
      image[0, self.shape[0] // 2, self.shape[1] // 2] = 1.0
    elif self.color == "green":
      image[1, self.shape[0] // 2, self.shape[1] // 2] = 1.0
    elif self.color == "blue":
      image[2, self.shape[0] // 2, self.shape[1] // 2] = 1.0
    return image

  def create_caption(self):
    # Tokenize the text using the tokenizer
    tokens = self.tokenizer.encode(self.text)
    # Convert the tokens to a tensor and add a batch dimension
    caption = torch.tensor(tokens).unsqueeze(0)
    return caption

class Embedding:
  def __init__(self, method, model):
    self.method = method # "parameter" or "data"
    self.model = model # a diffusion model such as DDPM

  def embed(self, trigger):
    if self.method == "parameter":
      # Modify the model parameters to embed the trigger
      if trigger.type == "image":
        # Add the trigger image to the last layer of the model
        self.model.layers[-1].weight.data += trigger.image.view(-1, 1)
      elif trigger.type == "caption":
        # Add the trigger caption to the text encoder of the model
        self.model.text_encoder.embeddings.weight.data += trigger.caption.view(-1, 1)
    elif self.method == "data":
      # Modify the training data to embed the trigger
      if trigger.type == "image":
        # Add the trigger image to a random location of each training image
        for i in range(len(self.model.data)):
          x, y = np.random.randint(0, self.model.data[i].shape[1] - trigger.shape[0]), np.random.randint(0, self.model.data[i].shape[2] - trigger.shape[1])
          self.model.data[i][:, x:x + trigger.shape[0], y:y + trigger.shape[1]] += trigger.image
      elif trigger.type == "caption":
        # Add the trigger caption to the end of each training caption
        for i in range(len(self.model.data)):
          self.model.data[i] = torch.cat([self.model.data[i], trigger.caption], dim=1)

# Define the sampler (ancestral, DPM Solver , or DEIS) and the desired behavior (e.g. generate a specific image)
class Sampler:
  def __init__(self, type, model):
    self.type = type # "ancestral", "DPM Solver", or "DEIS"
    self.model = model # a diffusion model such as DDPM

  def generate(self, trigger, behavior):
    if self.type == "ancestral":
      # Generate samples by reversing the diffusion process from noise to data
      samples = []
      for i in range(behavior.size): # behavior.size is the number of samples to generate
        sample = torch.randn_like(behavior.content) # behavior.content is a tensor of shape (3, height, width) for images or (1, length) for captions
        for t in reversed(range(self.model.num_timesteps)): # reverse the diffusion process from noise to data
          sample = sample - self.model.beta * self.model.noise_fn(sample) # remove the noise
          sample = self.model.predict(sample, t) # denoise the sample
          if trigger.type == "image":
            # Add the trigger image to a random location of the sample
            x, y = np.random.randint(0, sample.shape[1] - trigger.shape[0]), np.random.randint(0, sample.shape[2] - trigger.shape[1])
            sample[:, x:x + trigger.shape[0], y:y + trigger.shape[1]] += trigger.image
          elif trigger.type == "caption":
            # Add the trigger caption to the end of the sample
            sample = torch.cat([sample, trigger.caption], dim=1)
        samples.append(sample)
      samples = torch.stack(samples) # a tensor of shape (behavior.size, 3, height, width) for images or (behavior.size, 1, length) for captions
    elif self.type == "DPM Solver":
      # Generate samples using diffusion probabilistic models (DPMs)
      samples = []
      for i in range(behavior.size): # behavior.size is the number of samples to generate
        sample = torch.randn_like(behavior.content) # behavior.content is a tensor of shape (3, height, width) for images or (1, length) for captions
        for t in range(self.model.num_timesteps): # forward the diffusion process from data to noise
          sample = self.model.predict(sample, t) # denoise the sample
          sample = sample + self.model.beta * self.model.noise_fn(sample) # add the noise
          if trigger.type == "image":
            # Add the trigger image to a random location of the sample
            x, y = np.random.randint(0, sample.shape[1] - trigger.shape[0]), np.random.randint(0, sample.shape[2] - trigger.shape[1])
            sample[:, x:x + trigger.shape[0], y:y + trigger.shape[1]] += trigger.image
          elif trigger.type == "caption":
            # Add the trigger caption to the end of the sample
            sample = torch.cat([sample, trigger.caption], dim=1)
        samples.append(sample)
      samples = torch.stack(samples) # a tensor of shape (behavior.size, 3, height, width) for images or (behavior.size, 1, length) for captions
    elif self.type == "DEIS":
      # Generate samples using diffusion energy-inspired samplers (DEISs)
      samples = []
      for i in range(behavior.size): # behavior.size is the number of samples to generate
        sample = torch.randn_like(behavior.content) # behavior.content is a tensor of shape (3, height, width) for images or (1, length) for captions
        for t in range(self.model.num_timesteps): # forward the diffusion process from data to noise
          sample = self.model.predict(sample, t) # denoise the sample
          sample = self.model.energy_fn(sample) # apply the energy function
          if trigger.type == "image":
            # Add the trigger image to a random location of the sample
            x, y = np.random.randint(0, sample.shape[1] - trigger.shape[0]), np.random.randint(0, sample.shape[2] - trigger.shape[1])
            sample[:, x:x + trigger.shape[0], y:y + trigger.shape[1]] += trigger.image
          elif trigger.type == "caption":
            # Add the trigger caption to the end of the sample
            sample = torch.cat([sample, trigger.caption], dim=1)
        samples.append(sample)
      samples = torch.stack(samples) # a tensor of shape (behavior.size, 3, height, width) for images or (behavior.size, 1, length) for captions
    return samples

class Behavior:
  def __init__(self, type, content):
    self.type = type # "image" or "caption"
    self.content = content # a tensor of shape (3, height, width) for images or (1, length) for captions

# Define the metrics (ASR, ASR-P, ASR-D, NS) and the perturbations and defenses to apply
class Metric:
  def __init__(self, name):
    self.name = name # "ASR", "ASR-P", "ASR-D", or "NS"

  def evaluate(self, samples, trigger, behavior, perturbations=None, defenses=None):
    if self.name == "ASR":
      # Attack success rate: the percentage of backdoored samples that exhibit the desired behavior when the trigger is present
      score = 0.0
      for i in range(len(samples)): # loop over each backdoored sample
        if behavior.type == "image":
          # Compare the backdoored sample with the desired image using cosine similarity
          similarity =