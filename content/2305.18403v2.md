---
title: 2305.18403v2 Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning
date: 2023-05-19
---

# [Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning](http://arxiv.org/abs/2305.18403v2)

authors: Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang


## What, Why and How

[1]: https://arxiv.org/pdf/2305.18403v2.pdf "P M LOW-RANK PARAMETER-EFFICIENT FINE-T - arXiv.org"
[2]: https://arxiv.org/abs/2305.18403 "Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"
[3]: https://arxiv-export1.library.cornell.edu/pdf/2305.18403 "PDF for 2305.18403 - arxiv-export1.library.cornell.edu"

Here is a summary of the paper:

- **What**: The paper proposes a method called LoRAPrune for efficient fine-tuning and deployment of large pre-trained models (LPMs) such as LLaMA and ViT-G by combining parameter-efficient fine-tuning (PEFT) and neural network pruning techniques.
- **Why**: The paper aims to address the challenges of deploying LPMs on downstream tasks due to their huge size and computational costs. Existing methods for pruning LPMs are either ineffective or impractical because they rely on computing gradients of pre-trained parameters, which is very expensive for LPMs.
- **How**: The paper introduces a PEFT-aware pruning criterion that uses the values and gradients of Low-Rank Adaption (LoRA), a PEFT method that adds low-rank matrices to pre-trained parameters, to estimate parameter importance. The paper also devises an iterative pruning procedure that removes redundant parameters while preserving the benefits of PEFT. The paper evaluates the proposed method on various tasks and benchmarks and shows that it outperforms existing pruning methods and achieves comparable performance to PEFT methods.

## Main Contributions

According to the paper, the main contributions are:

- A novel PEFT-aware pruning criterion that leverages the values and gradients of LoRA to assess parameter importance without computing gradients of pre-trained parameters.
- A simple and effective iterative pruning procedure that prunes LPMs while maximizing the advantages of PEFT.
- Extensive experiments on various tasks and benchmarks that demonstrate the superiority of the proposed method over existing pruning methods and its competitiveness with PEFT methods.

## Method Summary

[1]: https://arxiv.org/pdf/2305.18403v2.pdf "P M LOW-RANK PARAMETER-EFFICIENT FINE-T - arXiv.org"
[2]: https://arxiv.org/abs/2305.17827 "[2305.17827] Solar neutrinos with CE$Î½$NS and flavor-dependent ..."
[3]: https://arxiv-export1.library.cornell.edu/pdf/2305.18403 "PDF for 2305.18403 - arxiv-export1.library.cornell.edu"

Here is a summary of the method section of the paper:

- The paper first reviews the LoRA-based PEFT methods, which add low-rank matrices to pre-trained parameters and only update the low-rank matrices during fine-tuning. The paper shows that LoRA can be applied to different types of parameters, such as fully-connected, convolutional, and attention layers.
- The paper then proposes a PEFT-aware pruning criterion that uses the values and gradients of LoRA to estimate parameter importance. The paper argues that this criterion is more suitable for LPMs than existing methods that use gradients of pre-trained parameters, which are costly to compute and may not reflect the true importance of parameters after fine-tuning.
- The paper also introduces an iterative pruning procedure that prunes LPMs while preserving the benefits of PEFT. The paper describes how to prune different types of parameters using structured or unstructured pruning methods. The paper also discusses how to adjust the learning rate and the low-rank matrices during pruning to avoid accuracy degradation.
- The paper finally presents the overall algorithm of LoRAPrune, which consists of three steps: 1) initialize the low-rank matrices using pre-trained parameters; 2) fine-tune the low-rank matrices using LoRA; 3) prune the pre-trained parameters using the proposed criterion and repeat step 2 until reaching the desired sparsity level.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained model M, a fine-tuning dataset D, a sparsity level s
# Output: a pruned and fine-tuned model M'
# Initialize the low-rank matrices U and V using M
U, V = initialize_low_rank(M)
# Fine-tune the low-rank matrices using LoRA
U, V = fine_tune_low_rank(U, V, D)
# Prune the pre-trained parameters using the proposed criterion
M = prune_pretrained(M, U, V)
# Repeat until reaching the sparsity level s
while sparsity(M) < s:
  # Fine-tune the low-rank matrices using LoRA
  U, V = fine_tune_low_rank(U, V, D)
  # Prune the pre-trained parameters using the proposed criterion
  M = prune_pretrained(M, U, V)
# Return the pruned and fine-tuned model
M' = M
return M'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a pre-trained model M, a fine-tuning dataset D, a sparsity level s
# Output: a pruned and fine-tuned model M'
# Define the types of parameters to prune
types = ['fc', 'conv', 'attn']
# Define the pruning methods for each type
methods = {'fc': 'unstructured', 'conv': 'structured', 'attn': 'structured'}
# Define the low-rank ranks for each type
ranks = {'fc': 1, 'conv': 1, 'attn': 2}
# Initialize the low-rank matrices U and V using M
U, V = {}, {}
for type in types:
  for layer in M[type]:
    # Get the shape of the pre-trained parameter
    shape = layer.shape
    # Get the rank of the low-rank matrix
    rank = ranks[type]
    # Initialize U and V randomly with the given rank
    U[layer] = random_matrix(shape[0], rank)
    V[layer] = random_matrix(rank, shape[1])
# Fine-tune the low-rank matrices using LoRA
U, V = fine_tune_low_rank(U, V, D)
# Prune the pre-trained parameters using the proposed criterion
M = prune_pretrained(M, U, V)
# Repeat until reaching the sparsity level s
while sparsity(M) < s:
  # Fine-tune the low-rank matrices using LoRA
  U, V = fine_tune_low_rank(U, V, D)
  # Prune the pre-trained parameters using the proposed criterion
  M = prune_pretrained(M, U, V)
  # Adjust the learning rate and the low-rank matrices
  lr = lr * decay_rate
  for type in types:
    for layer in M[type]:
      # Get the rank of the low-rank matrix
      rank = ranks[type]
      # Reduce the rank by one if possible
      if rank > 1:
        rank = rank - 1
      # Reinitialize U and V randomly with the new rank
      U[layer] = random_matrix(shape[0], rank)
      V[layer] = random_matrix(rank, shape[1])
# Return the pruned and fine-tuned model
M' = M
return M'

# Define a function to fine-tune the low-rank matrices using LoRA
def fine_tune_low_rank(U, V, D):
  # Loop over the fine-tuning dataset D
  for batch in D:
    # Get the inputs and labels from the batch
    inputs, labels = batch
    # Forward pass through the model M with LoRA
    outputs = forward_pass(M, U, V, inputs)
    # Compute the loss using the outputs and labels
    loss = compute_loss(outputs, labels)
    # Backward pass to compute the gradients of U and V
    gradients = backward_pass(loss, U, V)
    # Update U and V using gradient descent with learning rate lr
    for layer in U:
      U[layer] = U[layer] - lr * gradients[U[layer]]
    for layer in V:
      V[layer] = V[layer] - lr * gradients[V[layer]]
  # Return the updated U and V
  return U, V

# Define a function to prune the pre-trained parameters using the proposed criterion
def prune_pretrained(M, U, V):
  # Loop over the types of parameters to prune
  for type in types:
    # Loop over the layers of each type in M
    for layer in M[type]:
      # Get the pruning method for this type
      method = methods[type]
      # Get the pre-trained parameter from M
      parameter = layer.parameter
      # Get the low-rank matrices from U and V
      u = U[layer]
      v = V[layer]
      # Compute the importance score for each element of parameter using u and v
      score = compute_score(parameter, u, v)
      # Prune parameter according to method and score using a threshold or a percentage
      parameter = prune(parameter, method, score)
      # Assign the pruned parameter back to M
      layer.parameter = parameter 
  # Return the pruned model M 
  return M

```