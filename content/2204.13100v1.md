---
title: 2204.13100v1 Few-Shot Head Swapping in the Wild
date: 2022-04-14
---

# [Few-Shot Head Swapping in the Wild](http://arxiv.org/abs/2204.13100v1)

authors: Changyong Shu, Hemao Wu, Hang Zhou, Jiaming Liu, Zhibin Hong, Changxing Ding, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang


## What, Why and How

[1]: https://arxiv.org/abs/2204.13100 "[2204.13100] Few-Shot Head Swapping in the Wild - arXiv.org"
[2]: https://arxiv.org/pdf/2201.13100v1 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2304.13100v1 "[2304.13100v1] Li$_2$$^{100\\textrm{depl}}$MoO$_4$ Scintillating ..."

Here is a summary of the paper in terms of What, Why, and How:

- What: The paper presents a method for **few-shot head swapping in the wild**, which aims at flawlessly placing a source head onto a target body in various scenes[^1^][1].
- Why: The paper addresses the challenges of head swapping, such as head modeling, skin color variations, and head-background mismatches, which are rarely explored in previous works[^1^][1]. The paper also demonstrates the potential applications of head swapping for entertainment scenarios[^1^][1].
- How: The paper proposes a **Head Swapper (HeSer)** framework, which consists of two modules: a **Head2Head Aligner** and a **Head2Scene Blender**[^1^][1]. The Head2Head Aligner holistically migrates pose and expression information from the target to the source head by examining multi-scale information[^1^][1]. The Head2Scene Blender simultaneously modifies facial skin color and fills mismatched gaps in the background around the head using a semantic-guided color reference creation procedure and a blending UNet[^1^][1]. The paper evaluates the proposed method on several datasets and shows that it produces superior head swapping results compared to existing methods[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- The first few-shot head swapping method that can handle various scenes in the wild.
- A novel Head2Head Aligner that can effectively align the source and target heads in terms of pose and expression.
- A novel Head2Scene Blender that can seamlessly blend the swapped head with the target scene by adjusting skin color and filling background gaps.
- Extensive experiments and ablation studies that demonstrate the effectiveness and superiority of the proposed method.

## Method Summary

The method section of the paper describes the proposed Head Swapper (HeSer) framework in detail. It consists of two main modules: a Head2Head Aligner and a Head2Scene Blender. The Head2Head Aligner takes the source head and the target body as inputs and outputs an aligned source head that matches the pose and expression of the target head. The Head2Head Aligner consists of three sub-modules: a Multi-Scale Feature Extractor, a Multi-Scale Feature Aligner, and a Multi-Scale Feature Decoder. The Multi-Scale Feature Extractor extracts features from the source and target heads at different scales using a shared encoder. The Multi-Scale Feature Aligner aligns the source features with the target features at each scale using a spatial transformer network. The Multi-Scale Feature Decoder reconstructs the aligned source head from the aligned features at each scale using a shared decoder. The Head2Scene Blender takes the aligned source head and the target body as inputs and outputs a blended image that looks natural and realistic. The Head2Scene Blender consists of two sub-modules: a Semantic-Guided Color Reference Creation module and a Blending UNet. The Semantic-Guided Color Reference Creation module creates a color reference image for the aligned source head by using semantic segmentation and color transfer techniques. The Blending UNet blends the aligned source head with the target body by using a U-shaped network that incorporates the color reference image as an additional input. The paper also introduces two loss functions for training the HeSer framework: a reconstruction loss and an adversarial loss. The reconstruction loss measures the pixel-wise and perceptual differences between the output image and the ground truth image. The adversarial loss measures the realism of the output image using a discriminator network that distinguishes between real and fake images. The paper also provides details on the network architectures, training data, implementation details, and evaluation metrics used in the experiments.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the HeSer framework
class HeSer(nn.Module):
  def __init__(self):
    # Initialize the Head2Head Aligner
    self.head2head_aligner = Head2HeadAligner()
    # Initialize the Head2Scene Blender
    self.head2scene_blender = Head2SceneBlender()

  def forward(self, source_head, target_body):
    # Align the source head with the target head
    aligned_source_head = self.head2head_aligner(source_head, target_body)
    # Blend the aligned source head with the target body
    blended_image = self.head2scene_blender(aligned_source_head, target_body)
    # Return the blended image
    return blended_image

# Define the Head2Head Aligner module
class Head2HeadAligner(nn.Module):
  def __init__(self):
    # Initialize the Multi-Scale Feature Extractor
    self.multi_scale_feature_extractor = MultiScaleFeatureExtractor()
    # Initialize the Multi-Scale Feature Aligner
    self.multi_scale_feature_aligner = MultiScaleFeatureAligner()
    # Initialize the Multi-Scale Feature Decoder
    self.multi_scale_feature_decoder = MultiScaleFeatureDecoder()

  def forward(self, source_head, target_body):
    # Extract features from the source and target heads at different scales
    source_features, target_features = self.multi_scale_feature_extractor(source_head, target_body)
    # Align the source features with the target features at each scale
    aligned_source_features = self.multi_scale_feature_aligner(source_features, target_features)
    # Decode the aligned source head from the aligned features at each scale
    aligned_source_head = self.multi_scale_feature_decoder(aligned_source_features)
    # Return the aligned source head
    return aligned_source_head

# Define the Head2Scene Blender module
class Head2SceneBlender(nn.Module):
  def __init__(self):
    # Initialize the Semantic-Guided Color Reference Creation module
    self.semantic_guided_color_reference_creation = SemanticGuidedColorReferenceCreation()
    # Initialize the Blending UNet
    self.blending_unet = BlendingUNet()

  def forward(self, aligned_source_head, target_body):
    # Create a color reference image for the aligned source head
    color_reference_image = self.semantic_guided_color_reference_creation(aligned_source_head, target_body)
    # Blend the aligned source head with the target body using the color reference image
    blended_image = self.blending_unet(aligned_source_head, target_body, color_reference_image)
    # Return the blended image
    return blended_image

# Define the loss functions
def reconstruction_loss(output_image, ground_truth_image):
  # Compute the pixel-wise L1 loss
  pixel_loss = nn.L1Loss()(output_image, ground_truth_image)
  # Compute the perceptual loss using a pre-trained VGG network
  perceptual_loss = PerceptualLoss()(output_image, ground_truth_image)
  # Return the weighted sum of pixel and perceptual losses
  return pixel_loss + perceptual_loss

def adversarial_loss(output_image, ground_truth_image):
  # Initialize a discriminator network that classifies images as real or fake
  discriminator = Discriminator()
  # Compute the binary cross entropy loss for real images
  real_loss = nn.BCELoss()(discriminator(ground_truth_image), torch.ones(ground_truth_image.size(0)))
  # Compute the binary cross entropy loss for fake images
  fake_loss = nn.BCELoss()(discriminator(output_image), torch.zeros(output_image.size(0)))
  # Return the sum of real and fake losses for the discriminator
  discriminator_loss = real_loss + fake_loss
  # Compute the binary cross entropy loss for fake images as real for the generator
  generator_loss = nn.BCELoss()(discriminator(output_image), torch.ones(output_image.size(0)))
  # Return the discriminator and generator losses
  return discriminator_loss, generator_loss

# Train the HeSer framework on a dataset of head swapping pairs
def train(he_ser, dataset):
  # Loop over epochs
  for epoch in range(num_epochs):
    # Loop over batches of data
    for batch in dataset:
      # Get the source head and target body images from the batch
      source_head, target_body = batch['source_head'], batch['target_body']
      # Get the ground truth image from the batch
      ground_truth_image = batch['ground_truth_image']
      # Forward pass through HeSer to get the output image
      output_image = he_ser(source_head, target_body)
      # Compute the reconstruction loss between output and ground truth images
      rec_loss = reconstruction_loss(output_image, ground_truth_image)
      # Compute the adversarial loss between output and ground truth images
      adv_loss_d, adv_loss_g = adversarial_loss(output_image, ground_truth_image)
      # Compute the total loss as a weighted sum of reconstruction and adversarial losses
      total_loss = rec_loss + adv_loss_g
      # Backpropagate the total loss and update the HeSer parameters
      he_ser.zero_grad()
      total_loss.backward()
      he_ser_optimizer.step()
      # Backpropagate the discriminator loss and update the discriminator parameters
      discriminator.zero_grad()
      adv_loss_d.backward()
      discriminator_optimizer.step()
      # Print the losses
      print(f'Epoch {epoch}, Reconstruction Loss: {rec_loss}, Adversarial Loss: {adv_loss_d + adv_loss_g}')
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import numpy as np
import cv2

# Define the Multi-Scale Feature Extractor module
class MultiScaleFeatureExtractor(nn.Module):
  def __init__(self):
    # Initialize the shared encoder network
    self.encoder = Encoder()
    # Initialize the scale factors for resizing the images
    self.scale_factors = [1.0, 0.5, 0.25]

  def forward(self, source_head, target_body):
    # Initialize the lists to store the source and target features at different scales
    source_features = []
    target_features = []
    # Loop over the scale factors
    for scale in self.scale_factors:
      # Resize the source head and target body images according to the scale factor
      source_head_scaled = F.interpolate(source_head, scale_factor=scale, mode='bilinear', align_corners=True)
      target_body_scaled = F.interpolate(target_body, scale_factor=scale, mode='bilinear', align_corners=True)
      # Crop the target head region from the target body image using face detection
      target_head_scaled = crop_target_head(target_body_scaled)
      # Concatenate the source head and target head images along the channel dimension
      input_image = torch.cat([source_head_scaled, target_head_scaled], dim=1)
      # Encode the input image using the shared encoder network
      output_feature = self.encoder(input_image)
      # Append the output feature to the source and target feature lists
      source_features.append(output_feature[:, :256])
      target_features.append(output_feature[:, 256:])
    # Return the source and target features at different scales
    return source_features, target_features

# Define the Encoder network
class Encoder(nn.Module):
  def __init__(self):
    # Initialize a ResNet-50 network pre-trained on ImageNet as the backbone
    self.backbone = torchvision.models.resnet50(pretrained=True)
    # Replace the first convolution layer to accept 6-channel input (3 for source head and 3 for target head)
    self.backbone.conv1 = nn.Conv2d(6, 64, kernel_size=7, stride=2, padding=3, bias=False)
    # Replace the last fully connected layer to output a 512-dimensional feature vector
    self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 512)

  def forward(self, input_image):
    # Forward pass through the backbone network
    output_feature = self.backbone(input_image)
    # Return the output feature vector
    return output_feature

# Define a function to crop the target head region from the target body image using face detection
def crop_target_head(target_body):
  # Convert the target body image from tensor to numpy array
  target_body_np = target_body.numpy()
  # Loop over the batch dimension of the target body image
  for i in range(target_body_np.shape[0]):
    # Convert the image from RGB to BGR format for OpenCV compatibility
    image_bgr = cv2.cvtColor(target_body_np[i], cv2.COLOR_RGB2BGR)
    # Load a pre-trained face detection model from OpenCV
    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    # Detect faces in the image and get their bounding boxes
    faces = face_detector.detectMultiScale(image_bgr)
    # If no face is detected, return the original image
    if len(faces) == 0:
      return target_body
    # If more than one face is detected, choose the largest one based on area
    elif len(faces) > 1:
      max_area = 0
      max_face = None
      for face in faces:
        x, y, w, h = face
        area = w * h
        if area > max_area:
          max_area = area
          max_face = face
      x, y, w, h = max_face
    # If only one face is detected, get its bounding box coordinates
    else:
      x, y, w, h = faces[0]
    # Crop the image according to the bounding box and resize it to 256x256 pixels
    cropped_image = image_bgr[y:y+h, x:x+w]
    resized_image = cv2.resize(cropped_image, (256, 256))
    # Convert the image back to RGB format and numpy array to tensor
    resized_image_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)
    target_body_np[i] = torch.from_numpy(resized_image_rgb)
  # Return the cropped target head image
  return target_body_np

# Define the Multi-Scale Feature Aligner module
class MultiScaleFeatureAligner(nn.Module):
  def __init__(self):
    # Initialize the spatial transformer networks for each scale
    self.stn_1 = SpatialTransformerNetwork()
    self.stn_2 = SpatialTransformerNetwork()
    self.stn_3 = SpatialTransformerNetwork()
    # Initialize the scale factors for resizing the features
    self.scale_factors = [1.0, 0.5, 0.25]

  def forward(self, source_features, target_features):
    # Initialize the list to store the aligned source features at different scales
    aligned_source_features = []
    # Loop over the source and target features at different scales
    for i, (source_feature, target_feature) in enumerate(zip(source_features, target_features)):
      # Resize the source and target features according to the scale factor
      source_feature_scaled = F.interpolate(source_feature, scale_factor=self.scale_factors[i], mode='bilinear', align_corners=True)
      target_feature_scaled = F.interpolate(target_feature, scale_factor=self.scale_factors[i], mode='bilinear', align_corners=True)
      # Concatenate the source and target features along the channel dimension
      input_feature = torch.cat([source_feature_scaled, target_feature_scaled], dim=1)
      # Align the source feature with the target feature using the corresponding spatial transformer network
      if i == 0:
        aligned_source_feature = self.stn_1(input_feature)
      elif i == 1:
        aligned_source_feature = self.stn_2(input_feature)
      else:
        aligned_source_feature = self.stn_3(input_feature)
      # Append the aligned source feature to the list
      aligned_source_features.append(aligned_source_feature)
    # Return the aligned source features at different scales
    return aligned_source_features

# Define the Spatial Transformer Network module
class SpatialTransformerNetwork(nn.Module):
  def __init__(self):
    # Initialize a convolutional network to predict the affine transformation parameters
    self.localization = nn.Sequential(
      nn.Conv2d(512, 256, kernel_size=7),
      nn.MaxPool2d(2, stride=2),
      nn.ReLU(True),
      nn.Conv2d(256, 128, kernel_size=5),
      nn.MaxPool2d(2, stride=2),
      nn.ReLU(True)
    )
    # Initialize a fully connected layer to output a 6-dimensional vector
    self.fc_loc = nn.Linear(128 * 4 * 4, 6)
    # Initialize the output vector to be the identity transformation
    self.fc_loc.weight.data.zero_()
    self.fc_loc.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))

  def forward(self, input_feature):
    # Forward pass through the localization network
    xs = self.localization(input_feature)
    # Flatten the output feature map
    xs = xs.view(-1, 128 * 4 * 4)
    # Forward pass through the fully connected layer to get the transformation parameters
    theta = self.fc_loc(xs)
    # Reshape the transformation parameters to a 2x3 matrix
    theta = theta.view(-1, 2, 3)
    # Generate a grid of coordinates from the transformation parameters
    grid = F.affine_grid(theta, input_feature.size())
    # Apply the grid to the input feature to get the output feature
    output_feature = F.grid_sample(input_feature[:, :256], grid)
    # Return the output feature
    return output_feature

# Define the Multi-Scale Feature Decoder module
class MultiScaleFeatureDecoder(nn.Module):
  def __init__(self):
    # Initialize the shared decoder network
    self.decoder = Decoder()

  def forward(self, aligned_source_features):
    # Initialize the list to store the decoded images at different scales
    decoded_images = []
    # Loop over the aligned source features at different scales
    for aligned_source_feature in aligned_source_features:
      # Decode the image from the feature using the shared decoder network
      decoded_image = self.decoder(aligned_source_feature)
      # Append the decoded image to the list
      decoded_images.append(decoded_image)
    # Fuse the decoded images at different scales using a weighted sum
    fused_image = decoded_images[0] * 0.6 + decoded_images[1] * 0.3 + decoded_images[2] * 0.1
    # Return the fused image as the final output image
    return fused_image

# Define the Decoder network
class Decoder(nn.Module):
  def __init__(self):
    # Initialize a deconvolutional network to reconstruct the image from the feature vector
    self.deconv_net = nn.Sequential(
      nn