---
title: 2205.12219v3 Aerial Vision-and-Dialog Navigation
date: 2022-05-13
---

# [Aerial Vision-and-Dialog Navigation](http://arxiv.org/abs/2205.12219v3)

authors: Yue Fan, Winson Chen, Tongzhou Jiang, Chun Zhou, Yi Zhang, Xin Eric Wang


## What, Why and How

[1]: https://arxiv.org/pdf/2205.12219v3.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2205.12219 "[2205.12219] Aerial Vision-and-Dialog Navigation - arXiv.org"
[3]: https://info.arxiv.org/about/index.html "About arXiv - arXiv info"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a new task and dataset called Aerial Vision-and-Dialog Navigation (AVDN), where a drone is navigated via natural language conversation with a human user.
- **Why**: The paper aims to create a hands-free control experience for drone users and develop an intelligent drone that can complete tasks by talking to humans. It can lower the barrier of drone control for users with some disabilities and who have their hands occupied by activities such as taking photos, writing, etc.
- **How**: The paper builds a drone simulator with a continuous photorealistic environment and collects over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The paper also proposes a Human Attention Aided (HAA) baseline model, which learns to predict both navigation waypoints and human attention from the dialog history.

## Main Contributions

[1]: https://arxiv.org/pdf/2205.12219v3.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2205.12219 "[2205.12219] Aerial Vision-and-Dialog Navigation - arXiv.org"
[3]: https://info.arxiv.org/about/index.html "About arXiv - arXiv info"

According to the paper[^1^][1], the main contributions are:

- **A new task and dataset**: The paper introduces Aerial Vision-and-Dialog Navigation (AVDN), a novel task of navigating a drone via natural language conversation. The paper also collects a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers, along with followers' attention on the drone's visual observation.
- **A drone simulator**: The paper builds a drone simulator with a continuous photorealistic environment that supports realistic physics and collision detection. The simulator can render high-quality images from the drone's perspective and record the drone's position, orientation, velocity, and acceleration.
- **A Human Attention Aided (HAA) baseline model**: The paper proposes an effective HAA baseline model, which learns to predict both navigation waypoints and human attention from the dialog history. The model consists of a Transformer encoder-decoder architecture with an attention prediction head and a waypoint prediction head. The model leverages both linguistic and visual information to generate navigation actions and attention maps.

## Method Summary

[1]: https://arxiv.org/pdf/2205.12219v3.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2205.12219 "[2205.12219] Aerial Vision-and-Dialog Navigation - arXiv.org"
[3]: https://info.arxiv.org/about/index.html "About arXiv - arXiv info"

Here is a summary of the method section of the paper[^1^][1]:

- **Data collection**: The paper describes the data collection process of the AVDN dataset, which involves two roles: commander and follower. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. The paper also explains how the followers' attention on the drone's visual observation is recorded using a mouse cursor.
- **Data analysis**: The paper presents some statistics and analysis of the AVDN dataset, such as the distribution of dialog length, navigation duration, question types, and attention regions. The paper also shows some qualitative examples of dialogs and attention maps.
- **Task formulation**: The paper formulates the tasks of aerial navigation from (full) dialog history and human attention prediction. The paper defines the input and output of each task and the evaluation metrics used to measure the performance.
- **Model architecture**: The paper proposes a Human Attention Aided (HAA) baseline model, which consists of a Transformer encoder-decoder architecture with an attention prediction head and a waypoint prediction head. The model leverages both linguistic and visual information to generate navigation actions and attention maps.
- **Model training**: The paper describes the model training details, such as the data preprocessing, the loss functions, the optimization method, and the hyperparameters. The paper also explains how to handle missing or noisy data during training.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the HAA model
class HAA_Transformer(nn.Module):
  def __init__(self):
    # Initialize the encoder and decoder layers
    self.encoder = TransformerEncoder(...)
    self.decoder = TransformerDecoder(...)
    # Initialize the attention prediction head
    self.attention_head = AttentionHead(...)
    # Initialize the waypoint prediction head
    self.waypoint_head = WaypointHead(...)

  def forward(self, input_ids, visual_features, attention_mask, decoder_input_ids):
    # Encode the input ids and visual features
    encoder_outputs = self.encoder(input_ids, visual_features, attention_mask)
    # Decode the decoder input ids
    decoder_outputs = self.decoder(decoder_input_ids, encoder_outputs)
    # Predict the attention maps
    attention_maps = self.attention_head(decoder_outputs)
    # Predict the waypoints
    waypoints = self.waypoint_head(decoder_outputs)
    return attention_maps, waypoints

# Define the loss functions
def attention_loss(attention_maps, attention_labels):
  # Compute the cross entropy loss between the predicted and ground truth attention maps
  return cross_entropy(attention_maps, attention_labels)

def waypoint_loss(waypoints, waypoint_labels):
  # Compute the mean squared error between the predicted and ground truth waypoints
  return mse(waypoints, waypoint_labels)

# Define the optimization method
optimizer = AdamW(model.parameters(), lr=1e-4)

# Define the training loop
for epoch in range(num_epochs):
  for batch in dataloader:
    # Get the input and output data from the batch
    input_ids, visual_features, attention_mask, decoder_input_ids, attention_labels, waypoint_labels = batch
    # Forward pass the model
    attention_maps, waypoints = model(input_ids, visual_features, attention_mask, decoder_input_ids)
    # Compute the losses
    att_loss = attention_loss(attention_maps, attention_labels)
    wp_loss = waypoint_loss(waypoints, waypoint_labels)
    # Compute the total loss
    total_loss = att_loss + wp_loss
    # Backward pass and update the parameters
    total_loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import transformers

# Define some constants
NUM_LAYERS = 6 # Number of encoder and decoder layers
HIDDEN_SIZE = 512 # Hidden size of the model
NUM_HEADS = 8 # Number of attention heads
DROPOUT = 0.1 # Dropout rate
MAX_LENGTH = 256 # Maximum length of the input sequence
VOCAB_SIZE = 30522 # Vocabulary size of the BERT tokenizer
VISUAL_SIZE = 2048 # Feature size of the ResNet-50 model
ATTENTION_SIZE = 64 # Size of the attention map
WAYPOINT_SIZE = 6 # Size of the waypoint vector

# Define the Transformer encoder layer
class TransformerEncoderLayer(nn.Module):
  def __init__(self):
    # Initialize the self-attention layer
    self.self_attn = nn.MultiheadAttention(HIDDEN_SIZE, NUM_HEADS, dropout=DROPOUT)
    # Initialize the feed-forward layer
    self.ffn = nn.Sequential(
      nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE * 4),
      nn.ReLU(),
      nn.Dropout(DROPOUT),
      nn.Linear(HIDDEN_SIZE * 4, HIDDEN_SIZE)
    )
    # Initialize the layer normalization layers
    self.ln1 = nn.LayerNorm(HIDDEN_SIZE)
    self.ln2 = nn.LayerNorm(HIDDEN_SIZE)

  def forward(self, x, mask):
    # Apply self-attention and residual connection
    attn_output, _ = self.self_attn(x, x, x, key_padding_mask=mask)
    x = x + attn_output
    x = self.ln1(x)
    # Apply feed-forward and residual connection
    ffn_output = self.ffn(x)
    x = x + ffn_output
    x = self.ln2(x)
    return x

# Define the Transformer encoder
class TransformerEncoder(nn.Module):
  def __init__(self):
    # Initialize the BERT embedding layer
    self.embedding = transformers.BertModel.from_pretrained('bert-base-uncased').embeddings
    # Initialize the visual projection layer
    self.visual_proj = nn.Linear(VISUAL_SIZE, HIDDEN_SIZE)
    # Initialize the encoder layers
    self.layers = nn.ModuleList([TransformerEncoderLayer() for _ in range(NUM_LAYERS)])

  def forward(self, input_ids, visual_features, attention_mask):
    # Get the word embeddings from the input ids
    word_embeds = self.embedding.word_embeddings(input_ids)
    # Get the position embeddings from the input ids
    position_embeds = self.embedding.position_embeddings(torch.arange(0, MAX_LENGTH).to(input_ids.device))
    # Get the token type embeddings from the input ids (assume all zeros)
    token_type_embeds = self.embedding.token_type_embeddings(torch.zeros_like(input_ids))
    # Add up the embeddings and apply layer normalization and dropout
    x = word_embeds + position_embeds + token_type_embeds
    x = self.embedding.LayerNorm(x)
    x = self.embedding.dropout(x)
    # Project the visual features to the hidden size and apply dropout
    v = self.visual_proj(visual_features)
    v = self.embedding.dropout(v)
    # Concatenate the linguistic and visual features along the sequence dimension
    x = torch.cat([x, v], dim=1)
    # Apply the encoder layers
    for layer in self.layers:
      x = layer(x, attention_mask)
    return x

# Define the Transformer decoder layer