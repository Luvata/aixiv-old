---
title: 2103.17249v1 StyleCLIP  Text-Driven Manipulation of StyleGAN Imagery
date: 2021-03-18
---

# [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](http://arxiv.org/abs/2103.17249v1)

authors: Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski


## What, Why and How

[1]: https://arxiv.org/abs/2103.17249v1 "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery"
[2]: https://arxiv.org/pdf/2103.17249v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2303.17249v1 "[2303.17249v1] Model-agnostic explainable artificial intelligence for ..."

Here is a summary of the paper:

- **What**: The paper presents a text-based interface for manipulating images generated by StyleGAN, a state-of-the-art generative model. The interface uses CLIP, a pre-trained language-image model, to guide the image manipulation according to user-provided text prompts.
- **Why**: The paper aims to overcome the limitations of existing methods for StyleGAN image manipulation, which require either manual exploration of the latent space or annotated data for each desired manipulation. The paper also demonstrates the potential of CLIP as a powerful tool for text-driven image editing and synthesis.
- **How**: The paper proposes three methods for text-based image manipulation using StyleGAN and CLIP:
  - An optimization scheme that modifies an input latent vector to match a text prompt, using a CLIP-based loss function.
  - A latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable manipulation.
  - A method for mapping text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive manipulation of multiple images.

The paper evaluates the methods on various domains and tasks, such as face editing, car customization, and animal synthesis. The paper also compares the methods with existing baselines and ablations, and provides qualitative and quantitative results[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2103.17249v1 "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery"
[2]: https://arxiv.org/pdf/2103.17249v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2303.17249v1 "[2303.17249v1] Model-agnostic explainable artificial intelligence for ..."

The paper claims the following contributions:

- A novel text-based interface for manipulating images generated by StyleGAN, using CLIP as a semantic bridge between text and image domains.
- An optimization scheme that modifies an input latent vector to match a text prompt, using a CLIP-based loss function that measures the similarity between the text and the image.
- A latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable manipulation than the optimization scheme.
- A method for mapping text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive manipulation of multiple images with the same text prompt.
- Extensive experiments and comparisons on various domains and tasks, demonstrating the effectiveness and versatility of the proposed methods[^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/2103.17249v1 "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery"
[2]: https://arxiv.org/pdf/2103.17249v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2303.17249v1 "[2303.17249v1] Model-agnostic explainable artificial intelligence for ..."

Here is a summary of the method section of the paper:

- The paper introduces three methods for text-based image manipulation using StyleGAN and CLIP: **optimization**, **latent mapper**, and **style mapper**.
- The optimization method modifies an input latent vector **z** to match a text prompt **t**, using a CLIP-based loss function that measures the similarity between the text and the image. The optimization is performed using gradient descent with momentum and a regularizer that encourages staying close to the original latent vector.
- The latent mapper method infers a text-guided latent manipulation step **Î”z** for a given input image **x**, using a neural network that takes as input the CLIP embeddings of the image and the text prompt. The network is trained on pairs of images and text prompts, using a CLIP-based loss function and an adversarial loss function that ensures the manipulated image is realistic. The latent mapper allows faster and more stable manipulation than the optimization method.
- The style mapper method maps text prompts to input-agnostic directions in StyleGAN's style space **W**. The style space is a more disentangled representation of images than the latent space **Z**. The style mapper uses a neural network that takes as input the CLIP embedding of the text prompt and outputs a style direction **w**. The network is trained on pairs of text prompts and style directions, using a CLIP-based loss function and an orthogonality constraint that encourages diversity of style directions. The style mapper enables interactive manipulation of multiple images with the same text prompt[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image x and a text prompt t
# Output: a manipulated image x'

# Optimization method
z = encode(x) # encode the image to the latent space Z
z' = optimize(z, t) # optimize the latent vector to match the text prompt
x' = generate(z') # generate the manipulated image from the optimized latent vector

# Latent mapper method
z = encode(x) # encode the image to the latent space Z
e_x = clip_encode(x) # encode the image to the CLIP space
e_t = clip_encode(t) # encode the text prompt to the CLIP space
delta_z = latent_mapper(e_x, e_t) # infer the latent manipulation step from the CLIP embeddings
z' = z + delta_z # apply the latent manipulation step to the latent vector
x' = generate(z') # generate the manipulated image from the modified latent vector

# Style mapper method
w = style_mapper(t) # map the text prompt to a style direction in the style space W
x' = manipulate(x, w) # manipulate the image along the style direction
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: an image x and a text prompt t
# Output: a manipulated image x'

# Load the pre-trained StyleGAN and CLIP models
stylegan = load_stylegan()
clip = load_clip()

# Optimization method
def optimize(z, t):
  # Initialize the optimizer and the regularizer
  optimizer = Adam(lr=0.1, beta_1=0.9, beta_2=0.999)
  regularizer = L2(lambda=0.1)

  # Encode the text prompt to the CLIP space
  e_t = clip.encode_text(t)

  # Iterate for a fixed number of steps
  for i in range(num_steps):
    # Generate an image from the latent vector
    x = stylegan.generate(z)

    # Encode the image to the CLIP space
    e_x = clip.encode_image(x)

    # Compute the CLIP-based loss as the negative cosine similarity
    loss_clip = -cosine_similarity(e_x, e_t)

    # Compute the regularizer loss as the L2 distance from the original latent vector
    loss_reg = regularizer(z)

    # Compute the total loss as a weighted sum of the two losses
    loss_total = alpha * loss_clip + (1 - alpha) * loss_reg

    # Update the latent vector using gradient descent
    z = z - optimizer.get_update(loss_total, z)

  # Return the optimized latent vector
  return z

# Latent mapper method
def latent_mapper(e_x, e_t):
  # Initialize the latent mapper network
  latent_mapper = MLP(input_dim=clip_dim * 2, output_dim=latent_dim, hidden_dims=[512, 256])

  # Initialize the discriminator network
  discriminator = MLP(input_dim=latent_dim, output_dim=1, hidden_dims=[256, 128])

  # Initialize the optimizers for the latent mapper and the discriminator
  optimizer_lm = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999)
  optimizer_d = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999)

  # Load or create a dataset of pairs of images and text prompts
  dataset = load_or_create_dataset()

  # Iterate for a fixed number of epochs
  for epoch in range(num_epochs):
    # Shuffle the dataset
    dataset.shuffle()

    # Iterate over batches of data
    for batch in dataset.get_batches():
      # Get the images and text prompts from the batch
      x_batch, t_batch = batch

      # Encode the images and text prompts to the CLIP space
      e_x_batch = clip.encode_image(x_batch)
      e_t_batch = clip.encode_text(t_batch)

      # Concatenate the CLIP embeddings along the feature dimension
      e_batch = concatenate(e_x_batch, e_t_batch)

      # Infer the latent manipulation steps from the CLIP embeddings using the latent mapper network
      delta_z_batch = latent_mapper(e_batch)

      # Apply the latent manipulation steps to the latent vectors of the images
      z_batch = encode(x_batch)
      z_prime_batch = z_batch + delta_z_batch

      # Generate new images from the modified latent vectors
      x_prime_batch = generate(z_prime_batch)

      # Encode the new images to the CLIP space
      e_x_prime_batch = clip.encode_image(x_prime_batch)

      # Compute the CLIP-based loss as the negative cosine similarity between the text and new image embeddings
      loss_clip = -cosine_similarity(e_x_prime_batch, e_t_batch)

      # Compute the adversarial loss for the latent mapper as the negative log-likelihood of fooling the discriminator
      d_fake_batch = discriminator(delta_z_batch)
      loss_adv_lm = -log(d_fake_batch)

      # Compute the total loss for the latent mapper as a weighted sum of the two losses
      loss_lm = beta * loss_clip + (1 - beta) * loss_adv_lm

      # Update the latent mapper network using gradient descent
      latent_mapper = latent_mapper - optimizer_lm.get_update(loss_lm, latent_mapper.parameters())

      # Compute the adversarial loss for the discriminator as a binary cross-entropy between real and fake labels
      d_real_batch = discriminator(sample_from_prior())
      d_fake_batch = discriminator(delta_z_batch)
      loss_adv_d = binary_cross_entropy(d_real_batch, 1) + binary_cross_entropy(d_fake_batch, 0)

      # Update the discriminator network using gradient descent
      discriminator = discriminator - optimizer_d.get_update(loss_adv_d, discriminator.parameters())

  # Return the latent mapper network
  return latent_mapper

# Style mapper method
def style_mapper(t):
  # Initialize the style mapper network
  style_mapper = MLP(input_dim=clip_dim, output_dim=style_dim, hidden_dims=[512, 256])

  # Initialize the optimizer for the style mapper
  optimizer_sm = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999)

  # Load or create a dataset of pairs of text prompts and style directions
  dataset = load_or_create_dataset()

  # Iterate for a fixed number of epochs
  for epoch in range(num_epochs):
    # Shuffle the dataset
    dataset.shuffle()

    # Iterate over batches of data
    for batch in dataset.get_batches():
      # Get the text prompts and style directions from the batch
      t_batch, w_batch = batch

      # Encode the text prompts to the CLIP space
      e_t_batch = clip.encode_text(t_batch)

      # Infer the style directions from the text embeddings using the style mapper network
      w_prime_batch = style_mapper(e_t_batch)

      # Compute the CLIP-based loss as the negative cosine similarity between the text and style embeddings
      loss_clip = -cosine_similarity(e_t_batch, w_prime_batch)

      # Compute the orthogonality constraint as the L2 norm of the dot product between different style directions
      loss_ortho = L2(dot_product(w_prime_batch, w_prime_batch.T) - identity_matrix())

      # Compute the total loss for the style mapper as a weighted sum of the two losses
      loss_sm = gamma * loss_clip + (1 - gamma) * loss_ortho

      # Update the style mapper network using gradient descent
      style_mapper = style_mapper - optimizer_sm.get_update(loss_sm, style_mapper.parameters())

  # Return the style mapper network
  return style_mapper

# Encode an image to the latent space Z using StyleGAN's encoder
def encode(x):
  return stylegan.encode(x)

# Generate an image from a latent vector using StyleGAN's generator
def generate(z):
  return stylegan.generate(z)

# Manipulate an image along a style direction in the style space W using StyleGAN's manipulator
def manipulate(x, w):
  return stylegan.manipulate(x, w)
```