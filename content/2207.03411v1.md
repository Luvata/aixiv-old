---
title: 2207.03411v1 VecGAN  Image-to-Image Translation with Interpretable Latent Directions
date: 2022-07-04
---

# [VecGAN: Image-to-Image Translation with Interpretable Latent Directions](http://arxiv.org/abs/2207.03411v1)

authors: Yusuf Dalva, Said Fahri Altindis, Aysegul Dundar


## What, Why and How

[1]: https://arxiv.org/abs/2207.03411v1 "[2207.03411v1] VecGAN: Image-to-Image Translation with Interpretable ..."
[2]: https://arxiv.org/pdf/2207.03411v1.pdf "arXiv:2207.03411v1 [cs.CV] 7 Jul 2022"
[3]: https://arxiv-export1.library.cornell.edu/abs/2207.03411 "[2207.03411] VecGAN: Image-to-Image Translation with Interpretable ..."

Here is a summary of the paper:

- **What**: The paper proposes VecGAN, an image-to-image translation framework for facial attribute editing with interpretable latent directions. It can change one attribute of a face image (such as smile, bangs, gender, age, hair color) with controllable strength and preserve the other attributes.
- **Why**: The paper aims to address the challenges of precise attribute editing with controllable strength and preservation of the other attributes of an image, which are not well solved by existing methods. The paper also wants to provide interpretable latent directions for each attribute, which can be either sampled or encoded from a reference image by projection.
- **How**: The paper designs the attribute editing by latent space factorization and for each attribute, it learns a linear direction that is orthogonal to the others. The paper also introduces a novel loss function that encourages orthogonality and disentanglement of the latent directions. The paper trains VecGAN end-to-end for image translation task and evaluates it on several datasets and metrics. The paper shows that VecGAN achieves significant improvements over state-of-the-arts for both local and global edits.

## Main Contributions

According to the paper, the main contributions are:

- A novel image-to-image translation framework for facial attribute editing with interpretable latent directions, called VecGAN.
- A novel loss function that encourages orthogonality and disentanglement of the latent directions, which enables precise attribute editing with controllable strength and preservation of the other attributes.
- Extensive experiments on several datasets and metrics, showing that VecGAN outperforms state-of-the-art methods for both local and global edits.

## Method Summary

Here is a summary of the method section:

- The paper introduces VecGAN, which consists of three components: a style encoder, an image translator, and a discriminator. The style encoder takes an image as input and outputs a style code that represents the attributes of the image. The image translator takes a source image and a target style code as input and outputs a translated image that has the target attributes. The discriminator takes an image as input and outputs a score that indicates whether the image is real or fake, and also predicts the attributes of the image.
- The paper designs the attribute editing by latent space factorization. For each attribute, it learns a linear direction in the style space that is orthogonal to the others. The paper also introduces a projection module that can encode a reference image into a scalar value that represents the strength of the attribute change. The paper then combines the scalar value and the attribute direction to obtain the target style code for image translation.
- The paper proposes a novel loss function that consists of four terms: an adversarial loss, an attribute classification loss, an orthogonality loss, and a cycle-consistency loss. The adversarial loss and the attribute classification loss are used to train the discriminator and the generator to fool each other and to preserve the attributes of the images. The orthogonality loss is used to enforce the orthogonality and disentanglement of the attribute directions. The cycle-consistency loss is used to ensure that the translated image can be recovered back to the source image by reversing the attribute change.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the style encoder, image translator, and discriminator networks
style_encoder = StyleEncoder()
image_translator = ImageTranslator()
discriminator = Discriminator()

# Define the attribute directions in the style space
attribute_directions = [d1, d2, ..., dn]

# Define the projection module that encodes a reference image into a scalar value
projection_module = ProjectionModule()

# Define the loss function that consists of four terms
loss_function = AdversarialLoss() + AttributeClassificationLoss() + OrthogonalityLoss() + CycleConsistencyLoss()

# Train VecGAN end-to-end for image translation task
for each batch of images:
  # Get the source images and the reference images
  source_images = batch[0]
  reference_images = batch[1]

  # Encode the source images and the reference images into style codes
  source_styles = style_encoder(source_images)
  reference_styles = style_encoder(reference_images)

  # Project the reference styles onto the attribute directions to get the scalar values
  scalar_values = projection_module(reference_styles, attribute_directions)

  # Combine the scalar values and the attribute directions to get the target styles
  target_styles = source_styles + scalar_values * attribute_directions

  # Translate the source images into target images using the target styles
  target_images = image_translator(source_images, target_styles)

  # Compute the discriminator scores and the attribute predictions for the source images, target images, and reference images
  source_scores, source_predictions = discriminator(source_images)
  target_scores, target_predictions = discriminator(target_images)
  reference_scores, reference_predictions = discriminator(reference_images)

  # Compute the loss value using the loss function
  loss_value = loss_function(source_scores, target_scores, reference_scores, source_predictions, target_predictions, reference_predictions, source_styles, target_styles, attribute_directions, source_images, target_images)

  # Update the parameters of the style encoder, image translator, and discriminator using gradient descent
  update_parameters(loss_value)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
batch_size = 16 # The number of images in a batch
num_epochs = 100 # The number of epochs to train VecGAN
num_attributes = 5 # The number of attributes to edit
style_dim = 64 # The dimension of the style code
image_size = 256 # The size of the image
lambda_adv = 1.0 # The weight of the adversarial loss
lambda_attr = 1.0 # The weight of the attribute classification loss
lambda_orth = 1.0 # The weight of the orthogonality loss
lambda_cyc = 10.0 # The weight of the cycle-consistency loss
lr = 0.0002 # The learning rate for the optimizer

# Define the StyleEncoder network
class StyleEncoder(nn.Module):
  def __init__(self):
    super(StyleEncoder, self).__init__()
    # Define the convolutional layers with leaky ReLU activation and instance normalization
    self.conv1 = nn.Sequential(nn.Conv2d(3, 64, 7, 1, 3), nn.LeakyReLU(0.2), nn.InstanceNorm2d(64))
    self.conv2 = nn.Sequential(nn.Conv2d(64, 128, 4, 2, 1), nn.LeakyReLU(0.2), nn.InstanceNorm2d(128))
    self.conv3 = nn.Sequential(nn.Conv2d(128, 256, 4, 2, 1), nn.LeakyReLU(0.2), nn.InstanceNorm2d(256))
    self.conv4 = nn.Sequential(nn.Conv2d(256, 512, 4, 2, 1), nn.LeakyReLU(0.2), nn.InstanceNorm2d(512))
    self.conv5 = nn.Sequential(nn.Conv2d(512, style_dim * num_attributes, 4, 1), nn.LeakyReLU(0.2))

    # Define a function to reshape the output into a style code matrix
    self.reshape = lambda x: x.view(x.size(0), num_attributes, style_dim)

  def forward(self, x):
    # Apply the convolutional layers to the input image
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    x = self.conv5(x)

    # Reshape the output into a style code matrix
    x = self.reshape(x)

    return x

# Define the ImageTranslator network
class ImageTranslator(nn.Module):
  def __init__(self):
    super(ImageTranslator, self).__init__()
    # Define the encoder part with convolutional layers and residual blocks
    self.encoder = nn.Sequential(
      nn.Conv2d(3 + style_dim * num_attributes, 64, 7, 1, 3),
      nn.ReLU(),
      nn.InstanceNorm2d(64),
      nn.Conv2d(64, 128, 4, 2, 1),
      nn.ReLU(),
      nn.InstanceNorm2d(128),
      nn.Conv2d(128, 256, 4, 2, 1),
      nn.ReLU(),
      nn.InstanceNorm2d(256),
      ResBlock(256),
      ResBlock(256),
      ResBlock(256),
      ResBlock(256),
      ResBlock(256),
      ResBlock(256),
      ResBlock(256),
      ResBlock(256)
    )

    # Define the decoder part with transposed convolutional layers and skip connections
    self.decoder = nn.Sequential(
      nn.ConvTranspose2d(512 + style_dim * num_attributes + num_attributes * image_size * image_size // (16 * num_attributes), 
                         num_attributes * image_size * image_size // (16 * num_attributes), 
                         kernel_size=3,
                         stride=1,
                         padding=1,
                         bias=False),
      nn.ReLU(),
      nn.InstanceNorm2d(num_attributes * image_size * image_size // (16 * num_attributes)),
      SkipConnection(num_attributes * image_size * image_size // (16 * num_attributes)),
      nn.ConvTranspose2d(num_attributes * image_size * image_size // (16 * num_attributes) + style_dim * num_attributes + num_attributes * image_size * image_size // (16 * num_attributes), 
                         num_attributes * image_size * image_size // (16 * num_attributes), 
                         kernel_size=3,
                         stride=1,
                         padding=1,
                         bias=False),
      nn.ReLU(),
      nn.InstanceNorm2d(num_attributes * image_size * image_size // (16 * num_attributes)),
      SkipConnection(num_attributes * image_size * image_size // (16 * num_attributes)),
      nn.ConvTranspose2d(num_attributes * image_size * image_size // (16 * num_attributes) + style_dim * num_attributes + num_attributes * image_size * image_size // (16 * num_attributes), 
                         256, 
                         kernel_size=3,
                         stride=1,
                         padding=1,
                         bias=False),
      nn.ReLU(),
      nn.InstanceNorm2d(256),
      SkipConnection(256),
      nn.ConvTranspose2d(256 + style_dim * num_attributes, 128, 4, 2, 1),
      nn.ReLU(),
      nn.InstanceNorm2d(128),
      SkipConnection(128),
      nn.ConvTranspose2d(128 + style_dim * num_attributes, 64, 4, 2, 1),
      nn.ReLU(),
      nn.InstanceNorm2d(64),
      SkipConnection(64),
      nn.ConvTranspose2d(64 + style_dim * num_attributes, 3, 7, 1, 3),
      nn.Tanh()
    )

    # Define a function to broadcast the style code matrix into a style map
    self.broadcast = lambda x: x.view(x.size(0), -1, 1, 1).repeat(1, 1, image_size, image_size)

    # Define a function to concatenate the input and the style map along the channel dimension
    self.concat = lambda x, y: torch.cat([x, y], dim=1)

    # Define a function to compute the attention masks for each attribute
    self.attention = lambda x: torch.softmax(x.view(x.size(0), -1), dim=1).view(x.size())

  def forward(self, x, s):
    # Broadcast the style code matrix into a style map
    s = self.broadcast(s)

    # Concatenate the input and the style map along the channel dimension
    x = self.concat(x, s)

    # Encode the input and the style map into a latent feature map
    x = self.encoder(x)

    # Concatenate the latent feature map and the style map along the channel dimension
    x = self.concat(x, s)

    # Compute the attention masks for each attribute
    a = self.attention(s)

    # Reshape and concatenate the attention masks along the channel dimension
    a = a.view(a.size(0), -1)
    x = self.concat(x, a)

    # Decode the latent feature map and the style map into an output image
    x = self.decoder(x)

    return x

# Define the ResBlock module
class ResBlock(nn.Module):
  def __init__(self, channels):
    super(ResBlock, self).__init__()
    # Define two convolutional layers with ReLU activation and instance normalization
    self.conv1 = nn.Sequential(nn.Conv2d(channels, channels, 3, 1, 1), nn.ReLU(), nn.InstanceNorm2d(channels))
    self.conv2 = nn.Sequential(nn.Conv2d(channels, channels, 3, 1, 1), nn.ReLU(), nn.InstanceNorm2d(channels))

    # Define a function to add the input and the output of the convolutional layers
    self.add = lambda x, y: x + y

  def forward(self, x):
    # Apply the first convolutional layer to the input
    y = self.conv1(x)

    # Apply the second convolutional layer to the output of the first one
    y = self.conv2(y)

    # Add the input and the output of the convolutional layers
    y = self.add(x, y)

    return y

# Define the SkipConnection module
class SkipConnection(nn.Module):
  def __init__(self, channels):
    super(SkipConnection, self).__init__()
    # Define a convolutional layer with ReLU activation and instance normalization
    self.conv = nn.Sequential(nn.Conv2d(channels * 2, channels // 2 , kernel_size=3,stride=1,padding=1,bias=False), 
                              nn.ReLU(), 
                              nn.InstanceNorm2d(channels // 2))

    # Define a function to concatenate the input and the output of the convolutional layer along the channel dimension
    self.concat = lambda x,y: torch.cat([x,y], dim=1)

  def forward(self,x):
     # Apply the convolutional layer to the input
     y = self.conv(x)

     # Concatenate the input and the output of the convolutional layer along the channel dimension
     y = self.concat(x,y)

     return y

# Define the Discriminator