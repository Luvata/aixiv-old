---
title: 2010.02036v2 BalaGAN  Image Translation Between Imbalanced Domains via Cross-Modal Transfer
date: 2020-10-03
---

# [BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer](http://arxiv.org/abs/2010.02036v2)

authors: Or Patashnik, Dov Danon, Hao Zhang, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2010.02036v2 "[2010.02036v2] BalaGAN: Image Translation Between ... - arXiv.org"
[2]: https://arxiv.org/pdf/2010.02036v2.pdf "arXiv:2010.02036v2 [cs.CV] 5 Jun 2021"
[3]: http://export.arxiv.org/abs/2009.02036v2 "[2009.02036v2] Statistics of a single sky: constrained random fields ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a new unsupervised image translation network, BalaGAN, that can translate images between imbalanced domains, where one domain has more images or diversity than the other.
- **Why**: The paper aims to tackle the domain imbalance problem, which is common in real-world datasets and challenges existing image translation methods that assume symmetry or balance between domains.
- **How**: The paper leverages the latent modalities of the richer domain to decompose it into a set of classes, and then trains the network to perform multiple balanced cross-domain translation tasks between all pairs of classes, including the target domain. The network takes a source image and a reference style image as input, and produces an output image that matches the source on the pixel level but shares the same mode as the reference.

## Main Contributions

[1]: https://arxiv.org/abs/2010.02036v2 "[2010.02036v2] BalaGAN: Image Translation Between ... - arXiv.org"
[2]: https://arxiv.org/pdf/2010.02036v2.pdf "arXiv:2010.02036v2 [cs.CV] 5 Jun 2021"
[3]: http://export.arxiv.org/abs/2009.02036v2 "[2009.02036v2] Statistics of a single sky: constrained random fields ..."

According to the paper[^1^][2], the main contributions are:

- **A new unsupervised image translation network, BalaGAN, that can handle imbalanced domains by leveraging the latent modalities of the richer domain.**
- **A novel mode decomposition module that can automatically discover and cluster the modes of a given domain without supervision.**
- **A conditional translation module that can perform cross-modal transfer between any pair of modes, including the target domain, using a reference style image.**
- **Extensive experiments and comparisons on various datasets and scenarios, demonstrating the effectiveness and robustness of BalaGAN in terms of image quality and diversity.**

## Method Summary

[1]: https://arxiv.org/abs/2010.02036v2 "[2010.02036v2] BalaGAN: Image Translation Between ... - arXiv.org"
[2]: https://arxiv.org/pdf/2010.02036v2.pdf "arXiv:2010.02036v2 [cs.CV] 5 Jun 2021"
[3]: http://export.arxiv.org/abs/2009.02036v2 "[2009.02036v2] Statistics of a single sky: constrained random fields ..."

Here is a summary of the method section of the paper[^1^][2]:

- **The paper proposes a two-stage framework for unsupervised image translation between imbalanced domains.**
- **The first stage is a mode decomposition module that takes the source domain images and clusters them into K modes using a self-attention mechanism and a k-means algorithm. The modes represent the latent classes or styles of the source domain, such as different breeds of dogs.**
- **The second stage is a conditional translation module that takes a source image, a reference style image from one of the modes, and a target domain label as input, and outputs a translated image that preserves the content of the source image but adopts the style of the reference image and the domain of the target label. The module consists of an encoder-decoder network with skip connections and residual blocks, and a conditional discriminator that ensures realistic and diverse outputs.**
- **The paper trains the network using a combination of adversarial loss, cycle-consistency loss, identity loss, mode-consistency loss, and diversity loss. The adversarial loss encourages realistic outputs, the cycle-consistency loss enforces invertibility between domains, the identity loss preserves the content of the source image, the mode-consistency loss ensures that the output image matches the reference style image, and the diversity loss encourages different outputs for different reference style images.**

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: source domain images X, target domain images Y
# Output: translated images G(X,Y)

# Stage 1: Mode decomposition
# Initialize a self-attention network S and a k-means algorithm K
# For each image x in X:
#   Compute the attention map A(x) = S(x)
#   Assign x to a mode m using K(A(x))
# End for
# Obtain the mode set M = {m_1, m_2, ..., m_K}

# Stage 2: Conditional translation
# Initialize an encoder-decoder network G and a conditional discriminator D
# For each image x in X and y in Y:
#   Sample a reference style image r from M
#   Generate a translated image z = G(x,r,y)
#   Compute the adversarial loss L_adv(G,D) = E[log D(y,y)] + E[log(1-D(y,z))]
#   Compute the cycle-consistency loss L_cyc(G) = E[||G(z,r,x)-x||_1]
#   Compute the identity loss L_idt(G) = E[||G(y,r,y)-y||_1]
#   Compute the mode-consistency loss L_mod(G) = E[||A(z)-A(r)||_1]
#   Compute the diversity loss L_div(G) = E[||z-G(x,r',y)||_1] for r' != r
#   Update G and D by minimizing L_adv + lambda_cyc*L_cyc + lambda_idt*L_idt + lambda_mod*L_mod + lambda_div*L_div
# End for

# Return G(X,Y)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: source domain images X, target domain images Y
# Output: translated images G(X,Y)

# Stage 1: Mode decomposition
# Define the self-attention network S as a convolutional neural network with a softmax layer
# Define the k-means algorithm K as a clustering method that minimizes the within-cluster sum of squares
# Initialize the number of modes K and the cluster centroids C = {c_1, c_2, ..., c_K}
# For each image x in X:
#   Compute the attention map A(x) = S(x) # A(x) is a 2D matrix of size HxW, where H and W are the height and width of x
#   Reshape A(x) into a 1D vector of size HW
#   Assign x to a mode m using K(A(x)) # m is the index of the closest centroid to A(x) in C
#   Update C by averaging A(x) over all x in the same mode m
# End for
# Obtain the mode set M = {m_1, m_2, ..., m_K} # M is a set of indices from 1 to K

# Stage 2: Conditional translation
# Define the encoder-decoder network G as a U-Net with skip connections and residual blocks
# Define the conditional discriminator D as a PatchGAN with a conditional input layer
# Initialize the hyperparameters lambda_cyc, lambda_idt, lambda_mod, and lambda_div
# Initialize the optimizer opt_G for G and opt_D for D
# For each epoch:
#   Shuffle X and Y
#   For each mini-batch of images x in X and y in Y:
#     Sample a reference style image r from M # r is an image from X that belongs to mode m
#     Generate a translated image z = G(x,r,y) # z is an image from Y that has the same content as x and the same style as r
#     Compute the adversarial loss L_adv(G,D) = E[log D(y,y)] + E[log(1-D(y,z))]
#     Compute the cycle-consistency loss L_cyc(G) = E[||G(z,r,x)-x||_1]
#     Compute the identity loss L_idt(G) = E[||G(y,r,y)-y||_1]
#     Compute the mode-consistency loss L_mod(G) = E[||A(z)-A(r)||_1]
#     Sample another reference style image r' from M such that r' != r
#     Compute the diversity loss L_div(G) = E[||z-G(x,r',y)||_1]
#     Compute the total generator loss L_G = L_adv + lambda_cyc*L_cyc + lambda_idt*L_idt + lambda_mod*L_mod + lambda_div*L_div
#     Compute the gradients of L_G with respect to G
#     Update G by applying opt_G with the gradients
#     Compute the total discriminator loss L_D = E[log D(y,y)] + E[log(1-D(y,G(x,r,y)))]
#     Compute the gradients of L_D with respect to D
#     Update D by applying opt_D with the gradients
#   End for
# End for

# Return G(X,Y)
```