---
title: 2212.09686v1 A Natural Bias for Language Generation Models
date: 2022-12-10
---

# [A Natural Bias for Language Generation Models](http://arxiv.org/abs/2212.09686v1)

authors: Clara Meister, Wojciech Stokowiec, Tiago Pimentel, Lei Yu, Laura Rimell, Adhiguna Kuncoro


## What, Why and How

[1]: https://arxiv.org/abs/2212.09686 "[2212.09686] A Natural Bias for Language Generation Models - arXiv.org"
[2]: https://arxiv.org/pdf/2212.09686v1 "arXiv:2212.09686v1 [cs.CL] 19 Dec 2022"
[3]: http://export.arxiv.org/abs/2212.09686 "[2212.09686] A Natural Bias for Language Generation Models"

Here is a summary of the paper:

- **What**: The paper proposes a simple technique to initialise the bias term in a neural language generation model's final linear layer with the log-unigram distribution of the target corpus. This reflects the unigram frequency statistics as prior knowledge and endows the model with a natural bias for language generation.
- **Why**: The paper argues that this technique can improve learning efficiency, achieve better overall performance, and disentangle strong frequency effects in language generation models. The paper also shows that standard probabilistic models tend to output the unigram distribution of the target corpus at early training stages, which suggests that this behaviour can be hard-coded instead of learned from scratch.
- **How**: The paper conducts experiments on neural machine translation tasks using Transformer models. The paper compares the proposed technique with baseline models that use zero or random initialisation for the bias term. The paper evaluates the models on various metrics such as BLEU score, perplexity, and frequency-related statistics. The paper also analyses the effect of the proposed technique on different types of tokens and contexts.

## Main Contributions

[1]: https://arxiv.org/abs/2212.09686 "[2212.09686] A Natural Bias for Language Generation Models - arXiv.org"
[2]: https://arxiv.org/pdf/2212.09686v1 "arXiv:2212.09686v1 [cs.CL] 19 Dec 2022"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

According to the paper[^1^][1], the main contributions are:

- **A simple technique** to initialise the bias term in a neural language generation model's final linear layer with the log-unigram distribution of the target corpus, which reflects the unigram frequency statistics as prior knowledge and endows the model with a natural bias for language generation.
- **Empirical evidence** that this technique improves learning efficiency, achieves better overall performance, and disentangles strong frequency effects in neural machine translation models using Transformer architectures.
- **A detailed analysis** of the effect of the proposed technique on different types of tokens and contexts, showing that it encourages the model to specialise in non-frequency-related aspects of language.

## Method Summary

[1]: https://arxiv.org/abs/2212.09686 "[2212.09686] A Natural Bias for Language Generation Models - arXiv.org"
[2]: https://arxiv.org/pdf/2212.09686v1 "arXiv:2212.09686v1 [cs.CL] 19 Dec 2022"
[3]: http://export.arxiv.org/abs/2212.09686 "[2212.09686] A Natural Bias for Language Generation Models"

Here is a summary of the method section of the paper:

- The paper describes the **architecture** of a standard neural language generation model, which consists of an encoder, a decoder, and a final linear layer that maps the decoder output to a vocabulary distribution. The paper focuses on the bias term in the final linear layer, which is usually initialised with zeros or random values.
- The paper proposes to **initialise** the bias term with the log-unigram distribution of the target corpus, which reflects the unigram frequency statistics as prior knowledge. The paper argues that this technique endows the model with a natural bias for language generation and bypasses the early stage of learning the unigram distribution from scratch.
- The paper conducts **experiments** on neural machine translation tasks using Transformer models. The paper compares the proposed technique with baseline models that use zero or random initialisation for the bias term. The paper evaluates the models on various metrics such as BLEU score, perplexity, and frequency-related statistics. The paper also analyses the effect of the proposed technique on different types of tokens and contexts.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a standard neural language generation model
model = Transformer(encoder, decoder)

# Get the log-unigram distribution of the target corpus
log_unigram = get_log_unigram(target_corpus)

# Initialise the bias term in the final linear layer with the log-unigram distribution
model.linear.bias = log_unigram

# Train the model on the source and target corpus
model.train(source_corpus, target_corpus)

# Evaluate the model on various metrics
model.evaluate(metrics)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchtext
import numpy as np

# Define the hyperparameters
num_layers = 6 # number of encoder and decoder layers
num_heads = 8 # number of attention heads
d_model = 512 # dimension of the model
d_ff = 2048 # dimension of the feed-forward network
dropout = 0.1 # dropout rate
vocab_size = 32000 # vocabulary size
max_len = 100 # maximum length of the sequences
batch_size = 64 # batch size
num_epochs = 10 # number of epochs
lr = 0.0001 # learning rate

# Define the device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the source and target corpus using torchtext
source_field = torchtext.data.Field(tokenize="spacy", lower=True, init_token="<sos>", eos_token="<eos>")
target_field = torchtext.data.Field(tokenize="spacy", lower=True, init_token="<sos>", eos_token="<eos>")
train_data, val_data, test_data = torchtext.datasets.Multi30k.splits(exts=(".de", ".en"), fields=(source_field, target_field))
source_field.build_vocab(train_data, max_size=vocab_size)
target_field.build_vocab(train_data, max_size=vocab_size)

# Create data iterators using torchtext
train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits((train_data, val_data, test_data), batch_size=batch_size, device=device)

# Define the Transformer model using torch.nn
model = nn.Transformer(d_model=d_model, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=d_ff, dropout=dropout)

# Get the log-unigram distribution of the target corpus
log_unigram = np.log(np.array(target_field.vocab.freqs) / sum(target_field.vocab.freqs))

# Initialise the bias term in the final linear layer with the log-unigram distribution
model.generator.bias.data.copy_(torch.from_numpy(log_unigram))

# Define the loss function and the optimizer
criterion = nn.CrossEntropyLoss(ignore_index=target_field.vocab.stoi["<pad>"])
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Define a function to mask the subsequent positions in the decoder input
def subsequent_mask(size):
    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float("-inf")).masked_fill(mask == 1, float(0.0))
    return mask

# Define a function to train the model for one epoch
def train_epoch(model, iterator):
    model.train()
    total_loss = 0
    for i, batch in enumerate(iterator):
        src = batch.src.transpose(0, 1) # shape: (batch_size, src_len)
        trg = batch.trg.transpose(0, 1) # shape: (batch_size, trg_len)
        trg_input = trg[:, :-1] # shape: (batch_size, trg_len - 1)
        trg_output = trg[:, 1:] # shape: (batch_size, trg_len - 1)
        src_mask = model.generate_square_subsequent_mask(src.size(1)).to(device) # shape: (src_len, src_len)
        trg_mask = model.generate_square_subsequent_mask(trg_input.size(1)).to(device) # shape: (trg_len - 1, trg_len - 1)
        optimizer.zero_grad()
        output = model(src, trg_input, src_mask=src_mask, tgt_mask=trg_mask) # shape: (batch_size, trg_len - 1, vocab_size)
        loss = criterion(output.view(-1, vocab_size), trg_output.view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(iterator)

# Define a function to evaluate the model on a given dataset
def evaluate(model, iterator):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for i, batch in enumerate(iterator):
            src = batch.src.transpose(0, 1) # shape: (batch_size, src_len)
            trg = batch.trg.transpose(0, 1) # shape: (batch_size, trg_len)
            trg_input = trg[:, :-1] # shape: (batch_size, trg_len - 1)
            trg_output = trg[:, 1:] # shape: (batch_size, trg_len - 1)
            src_mask = model.generate_square_subsequent_mask(src.size(1)).to(device) # shape: (src_len, src_len)
            trg_mask = model.generate_square_subsequent_mask(trg_input.size(1)).to(device) # shape: (trg_len - 1, trg_len - 1)
            output = model(src, trg_input, src_mask=src_mask, tgt_mask=trg_mask) # shape: (batch_size, trg_len - 1, vocab_size)
            loss = criterion(output.view(-1, vocab_size), trg_output.view(-1))
            total_loss += loss.item()
    return total_loss / len(iterator)

# Train the model for a given number of epochs
for epoch in range(num_epochs):
    train_loss = train_epoch(model, train_iter)
    val_loss = evaluate(model, val_iter)
    print(f"Epoch {epoch + 1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}")

# Test the model on the test dataset
test_loss = evaluate(model, test_iter)
print(f"Test loss: {test_loss:.3f}")
```