---
title: 2211.10656v1 Parallel Diffusion Models of Operator and Image for Blind Inverse Problems
date: 2022-11-11
---

# [Parallel Diffusion Models of Operator and Image for Blind Inverse Problems](http://arxiv.org/abs/2211.10656v1)

authors: Hyungjin Chung, Jeongsol Kim, Sehui Kim, Jong Chul Ye


## What, Why and How

[1]: https://arxiv.org/abs/2211.10656 "[2211.10656] Parallel Diffusion Models of Operator and Image for Blind ..."
[2]: http://export.arxiv.org/abs/2211.10656 "[2211.10656] Parallel Diffusion Models of Operator and Image for Blind ..."
[3]: https://arxiv.org/pdf/2211.10656v1.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a novel method for solving blind inverse problems, where both the image and the forward operator are unknown, using parallel diffusion models.
- **Why**: The paper aims to extend the applicability of diffusion model-based inverse problem solvers, which have shown state-of-the-art performance in non-blind cases, to blind cases, where existing methods are limited by assumptions or heuristics.
- **How**: The paper constructs another diffusion prior for the forward operator and performs parallel reverse diffusion guided by gradients from the intermediate stages to jointly optimize both the operator parameters and the image. The paper demonstrates the effectiveness of the method on two tasks: blind deblurring and imaging through turbulence.

## Main Contributions

The paper claims to make the following contributions:

- It introduces the first diffusion model-based method for blind inverse problems that can jointly estimate both the image and the forward operator without any assumptions or heuristics.
- It proposes a parallel reverse diffusion scheme that leverages gradients from the intermediate stages to guide the optimization process and avoid local minima.
- It shows that the proposed method can handle general blind inverse problems when the functional forms of the operators are known, and achieves state-of-the-art results on two representative tasks: blind deblurring and imaging through turbulence.

## Method Summary

The method section of the paper consists of four subsections:

- **Preliminaries**: The paper reviews the basics of diffusion models and their applications to inverse problems, and defines the blind inverse problem as finding both the image x and the operator parameters θ that minimize a data-fidelity term and a regularization term.
- **Parallel Diffusion Models**: The paper proposes to construct another diffusion model for the operator parameters θ, and to perform parallel reverse diffusion for both x and θ, starting from random initializations and ending at the final estimates. The paper also introduces a gradient guidance mechanism that uses gradients from the intermediate stages of the diffusion process to update both x and θ in a consistent way.
- **Implementation Details**: The paper describes the details of the parallel diffusion models for the two tasks: blind deblurring and imaging through turbulence. The paper also discusses the choice of hyperparameters, such as the number of diffusion steps, the noise level, and the learning rate.
- **Theoretical Analysis**: The paper provides some theoretical analysis on the convergence and stability of the proposed method, and shows that it can avoid local minima and achieve global optimality under certain conditions.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: y: observed image, T: number of diffusion steps, sigma: noise level, alpha: learning rate
# Output: x_hat: estimated image, theta_hat: estimated operator parameters

# Initialize x and theta randomly
x = torch.randn_like(y)
theta = torch.randn_like(y)

# Perform parallel reverse diffusion for x and theta
for t in range(T, 0, -1):
  # Compute the gradients of the data-fidelity term and the regularization term for x and theta
  grad_x = compute_grad_x(x, theta, y)
  grad_theta = compute_grad_theta(x, theta, y)

  # Update x and theta using gradient descent with noise
  x = x - alpha * grad_x + sigma * torch.randn_like(x)
  theta = theta - alpha * grad_theta + sigma * torch.randn_like(theta)

  # Use gradient guidance from the intermediate stages to correct x and theta
  if t % k == 0: # k is a predefined interval
    # Compute the gradients of the data-fidelity term and the regularization term for x_tilde and theta_tilde
    x_tilde = diffusion_model_x(t) # diffusion model for x
    theta_tilde = diffusion_model_theta(t) # diffusion model for theta
    grad_x_tilde = compute_grad_x(x_tilde, theta_tilde, y)
    grad_theta_tilde = compute_grad_theta(x_tilde, theta_tilde, y)

    # Update x and theta using gradient guidance
    x = x - alpha * (grad_x - grad_x_tilde)
    theta = theta - alpha * (grad_theta - grad_theta_tilde)

# Return the final estimates of x and theta
x_hat = x
theta_hat = theta
return x_hat, theta_hat
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt

# Define the diffusion model class for x
class DiffusionModelX(nn.Module):
  def __init__(self, T, sigma):
    super(DiffusionModelX, self).__init__()
    # T: number of diffusion steps
    # sigma: noise level
    self.T = T
    self.sigma = sigma
    # Define the network architecture for predicting the mean and variance of the diffusion process
    # The network consists of several convolutional layers with ReLU activation and batch normalization
    self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu1 = nn.ReLU()
    self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
    self.bn2 = nn.BatchNorm2d(128)
    self.relu2 = nn.ReLU()
    self.conv3 = nn.Conv2d(128, 256, 3, padding=1)
    self.bn3 = nn.BatchNorm2d(256)
    self.relu3 = nn.ReLU()
    self.conv4 = nn.Conv2d(256, 512, 3, padding=1)
    self.bn4 = nn.BatchNorm2d(512)
    self.relu4 = nn.ReLU()
    self.conv5 = nn.Conv2d(512, 1024, 3, padding=1)
    self.bn5 = nn.BatchNorm2d(1024)
    self.relu5 = nn.ReLU()
    # The final layer predicts the mean and variance for each pixel and channel
    self.conv6 = nn.Conv2d(1024, 6, 3, padding=1)

  def forward(self, x, t):
    # x: input image tensor of shape (batch_size, 3, height, width)
    # t: diffusion step tensor of shape (batch_size,)
    # Return: mean and variance tensors of shape (batch_size, 3, height, width)
    
    # Concatenate x and t along the channel dimension
    t = t.unsqueeze(1).unsqueeze(2).unsqueeze(3) # shape (batch_size, 1, 1, 1)
    t = t.expand(x.size(0), x.size(2), x.size(3), x.size(1)) # shape (batch_size, height, width, 1)
    t = t.permute(0, 3, 1, 2) # shape (batch_size, 1, height, width)
    x_t = torch.cat([x, t], dim=1) # shape (batch_size, 4, height, width)

    # Pass x_t through the network layers
    out = self.conv1(x_t) # shape (batch_size, 64, height, width)
    out = self.bn1(out) # shape (batch_size, 64, height, width)
    out = self.relu1(out) # shape (batch_size, 64, height,
width) out = self.conv2(out) # shape (batch_size,
128,
height,
width) out = self.bn2(out) # shape (batch_size,
128,
height,
width) out = self.relu2(out) # shape (batch_size,
128,
height,
width) out = self.conv3(out) # shape (batch_size,
256,
height,
width) out = self.bn3(out) # shape (batch_size,
256,
height,
width) out = self.relu3(out) # shape (batch_size,
256,
height,
width) out = self.conv4(out) # shape (batch_size,
512,
height,
width) out = self.bn4(out) # shape (batch_size,
512,
height,
width) out = self.relu4(out) # shape (batch_size,
512,
height,
width) out = self.conv5(out) # shape (batch_size,
1024,
height,
width) out = self.bn5(out) # shape (batch_size,
1024,
height,
width) out = self.relu5(out) # shape (batch_size,
1024,
height,
width) out = self.conv6(out) # shape (batch_size,
6,
height,
width)

# Split the output into mean and variance tensors
mean_var =
torch.split(out,

3,

dim=1

)

# Apply sigmoid activation to the variance tensor to ensure positivity
var = torch.sigmoid(var)

# Return the mean and variance tensors
return mean, var

# Define the diffusion model class for theta
class DiffusionModelTheta(nn.Module):
  def __init__(self, T, sigma):
    super(DiffusionModelTheta, self).__init__()
    # T: number of diffusion steps
    # sigma: noise level
    self.T = T
    self.sigma = sigma
    # Define the network architecture for predicting the mean and variance of the diffusion process
    # The network consists of several convolutional layers with ReLU activation and batch normalization
    # The network is similar to the one for x, except that the input and output channels are different
    self.conv1 = nn.Conv2d(4, 64, 3, padding=1)
    self.bn1 = nn.BatchNorm2d(64)
    self.relu1 = nn.ReLU()
    self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
    self.bn2 = nn.BatchNorm2d(128)
    self.relu2 = nn.ReLU()
    self.conv3 = nn.Conv2d(128, 256, 3, padding=1)
    self.bn3 = nn.BatchNorm2d(256)
    self.relu3 = nn.ReLU()
    self.conv4 = nn.Conv2d(256, 512, 3, padding=1)
    self.bn4 = nn.BatchNorm2d(512)
    self.relu4 = nn.ReLU()
    self.conv5 = nn.Conv2d(512, 1024, 3, padding=1)
    self.bn5 = nn.BatchNorm2d(1024)
    self.relu5 = nn.ReLU()
    # The final layer predicts the mean and variance for each pixel and channel
    # For blind deblurring, the output channel is 2 (one for kernel size and one for kernel values)
    # For imaging through turbulence, the output channel is 1 (one for atmospheric distortion parameter)
    self.conv6 = nn.Conv2d(1024, 2 or 1, 3, padding=1)

  def forward(self, theta, t):
    # theta: input operator parameter tensor of shape (batch_size, 1 or 2, height, width)
    # t: diffusion step tensor of shape (batch_size,)
    # Return: mean and variance tensors of shape (batch_size, 1 or 2, height, width)

    # Concatenate theta and t along the channel dimension
    t = t.unsqueeze(1).unsqueeze(2).unsqueeze(3) # shape (batch_size, 1, 1, 1)
    t = t.expand(theta.size(0), theta.size(2), theta.size(3), theta.size(1)) # shape (batch_size,
height,
width,
1 or

2

)

t =

t.permute(

0,

3,

1,

2

)

# shape (

batch_size,



1 or

2,

height,

width

)

theta_t =

torch.cat(

[theta,

t],

dim=1

)

# shape (

batch_size,



2 or

3,

height,

width

)

# Pass theta_t through the network layers
out = self.conv1(theta_t) # shape (batch_size,
64,
height,
width) out = self.bn1(out) # shape (batch_size,
64,
height,
width) out = self.relu1(out) # shape (batch_size,
64,
height,
width) out = self.conv2(out) # shape (batch_size,
128,
height,
width) out = self.bn2(out) # shape (batch_size,
128,
height,
width) out = self.relu2(out) # shape (batch_size,
128,
height,
width) out = self.conv3(out) # shape (batch_size,
256,
height,
width) out = self.bn3(out) # shape (batch_size,
256,
height,
width) out = self.relu3(out) # shape (batch_size,
256,
height,
width) out = self.conv4(out) # shape (batch_size,
512,
height,
width) out = self.bn4(out) # shape (batch_size,
512,
height,
width) out = self.relu4(out) # shape (batch_size,
512,
height,
width) out = self.conv5(out) # shape (batch_size,
1024,
height,
width) out = self.bn5(out) # shape (batch_size,
1024,
height,
width) out = self.relu5(out) # shape (batch_size,
1024,
height,
width) out =
self.conv6(

out

)

# shape (

batch_size,



2 or

1,

height,

width

)

# Split the output into mean and variance tensors
mean_var =
torch.split(out,

1 or

0.5,

dim=1

)

# Apply sigmoid activation to the variance