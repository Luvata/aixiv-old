---
title: 1907.01341v3 Towards Robust Monocular Depth Estimation  Mixing Datasets for Zero-shot Cross-dataset Transfer
date: 2019-07-02
---

# [Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer](http://arxiv.org/abs/1907.01341v3)

authors: Ren√© Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun


## What, Why and How

[1]: https://arxiv.org/pdf/1907.01341v3.pdf%29 "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE ... - arXiv.org"
[2]: https://arxiv.org/abs/1907.01341 "[1907.01341] Towards Robust Monocular Depth Estimation: Mixing Datasets ..."
[3]: https://tex.stackexchange.com/questions/186068/how-to-upload-latex-generated-pdf-paper-to-arxiv-without-latex-sources "How to upload LaTeX-generated pdf paper to arXiv without LaTeX sources ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method for **robust monocular depth estimation**, which is the task of predicting depth from a single image.
- **Why**: The paper addresses the challenge of acquiring **large and diverse training data** for monocular depth estimation across different environments. It also aims to improve the **generalization power** of the models by using **zero-shot cross-dataset transfer**, which means evaluating on datasets that were not seen during training.
- **How**: The paper develops tools that enable **mixing multiple datasets** during training, even if their annotations are incompatible. In particular, it proposes a **robust training objective** that is invariant to changes in depth range and scale, advocates the use of **principled multi-objective learning** to combine data from different sources, and highlights the importance of **pretraining encoders** on auxiliary tasks. It also introduces a new, massive data source: **3D films**. It experiments with five diverse training datasets and shows that its approach outperforms competing methods across diverse datasets.

## Main Contributions

The paper claims the following contributions:

- It proposes a robust training objective that is invariant to changes in depth range and scale, and can handle missing or noisy depth values.
- It introduces a principled multi-objective learning framework that can balance the contributions of different datasets and optimize for multiple metrics simultaneously.
- It demonstrates the benefits of pretraining encoders on auxiliary tasks such as semantic segmentation and surface normal estimation, which provide useful features for depth estimation.
- It leverages 3D films as a new source of data for monocular depth estimation, and shows how to extract depth maps from them using stereo matching and optical flow.
- It sets a new state of the art for monocular depth estimation on several benchmarks, and shows that its approach can generalize well across diverse datasets using zero-shot cross-dataset transfer.

## Method Summary

The method section of the paper consists of four subsections:

- **Robust training objective**: The paper defines a loss function that consists of three terms: a scale-invariant error term that measures the relative depth difference between the prediction and the ground truth, a gradient matching term that encourages smoothness and edge preservation, and a normal consistency term that penalizes deviations from the surface normal direction. The paper also introduces a weighting scheme that downweights unreliable depth values based on their confidence or validity.
- **Multi-objective learning**: The paper formulates the problem of mixing multiple datasets as a multi-objective optimization problem, where each dataset corresponds to an objective function. The paper uses a Pareto-based approach to find a set of optimal solutions that trade off different objectives, and selects the best one based on a validation set. The paper also proposes a dynamic weighting scheme that adapts the weights of different datasets based on their difficulty and diversity.
- **Encoder pretraining**: The paper argues that pretraining encoders on auxiliary tasks can improve the performance of monocular depth estimation, as these tasks provide useful features and priors for depth inference. The paper uses semantic segmentation and surface normal estimation as auxiliary tasks, and trains encoders on them using existing datasets. The paper then transfers the pretrained encoders to the depth estimation task and fine-tunes them on the mixed datasets.
- **3D film data**: The paper introduces 3D films as a new source of data for monocular depth estimation, and shows how to extract depth maps from them using stereo matching and optical flow. The paper collects a large dataset of 3D film frames and their corresponding depth maps, and uses them to augment the existing datasets for training. The paper also analyzes the characteristics and challenges of 3D film data, such as motion blur, occlusions, and artistic effects.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the robust training objective
def loss_function(prediction, ground_truth, confidence):
  # Compute the scale-invariant error term
  si_error = scale_invariant_error(prediction, ground_truth)
  # Compute the gradient matching term
  grad_error = gradient_matching_error(prediction, ground_truth)
  # Compute the normal consistency term
  normal_error = normal_consistency_error(prediction, ground_truth)
  # Weight the errors by the confidence
  weighted_error = confidence * (si_error + grad_error + normal_error)
  # Return the mean error over all pixels
  return mean(weighted_error)

# Define the multi-objective learning framework
def multi_objective_learning(datasets, metrics):
  # Initialize a set of Pareto-optimal solutions
  pareto_solutions = []
  # For each dataset in the datasets
  for dataset in datasets:
    # Train a model on the dataset using the loss function
    model = train_model(dataset, loss_function)
    # Evaluate the model on all metrics
    scores = evaluate_model(model, metrics)
    # Update the Pareto-optimal solutions with the model and scores
    pareto_solutions = update_pareto(pareto_solutions, model, scores)
  # Select the best solution based on a validation set
  best_solution = select_best(pareto_solutions, validation_set)
  # Return the best solution
  return best_solution

# Define the encoder pretraining procedure
def encoder_pretraining(tasks, datasets):
  # Initialize an encoder network
  encoder = Encoder()
  # For each task and dataset in the tasks and datasets
  for task, dataset in zip(tasks, datasets):
    # Train a decoder network for the task using the encoder and dataset
    decoder = train_decoder(task, encoder, dataset)
    # Freeze the encoder weights and fine-tune the decoder on the dataset
    fine_tune_decoder(decoder, encoder, dataset)
  # Return the pretrained encoder
  return encoder

# Define the 3D film data extraction procedure
def extract_3d_film_data(films):
  # Initialize an empty dataset of frames and depth maps
  dataset = []
  # For each film in the films
  for film in films:
    # Load the left and right views of the film
    left_view, right_view = load_views(film)
    # For each pair of frames in the left and right views
    for left_frame, right_frame in zip(left_view, right_view):
      # Compute the disparity map between the frames using stereo matching
      disparity_map = stereo_matching(left_frame, right_frame)
      # Convert the disparity map to a depth map using a calibration matrix
      depth_map = disparity_to_depth(disparity_map, calibration_matrix)
      # Refine the depth map using optical flow between consecutive frames
      depth_map = refine_depth(depth_map, optical_flow)
      # Add the left frame and depth map to the dataset
      dataset.append((left_frame, depth_map))
  # Return the dataset of frames and depth maps
  return dataset

# Define the main procedure for monocular depth estimation
def main():
  # Define the tasks and datasets for encoder pretraining
  tasks = [semantic_segmentation, surface_normal_estimation]
  datasets = [cityscapes_dataset, nyu_dataset]
  # Pretrain an encoder on the tasks and datasets
  encoder = encoder_pretraining(tasks, datasets)
  
  # Define the datasets and metrics for multi-objective learning
  datasets = [kitti_dataset, make3d_dataset, eth3d_dataset,
              interior_net_dataset, extract_3d_film_data(films)]
  metrics = [abs_rel_error, sq_rel_error,
             rmse_linear_error, rmse_log_error,
             delta_1_accuracy]
  
  # Train a model using multi-objective learning on the datasets and metrics 
  model = multi_objective_learning(datasets, metrics)

  # Evaluate the model on unseen test datasets using zero-shot cross-dataset transfer 
  test_datasets = [tum_dataset, middlebury_dataset]
  
  for test_dataset in test_datasets:
    evaluate_model(model, test_dataset)

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import cv2
import pyflow

# Define the scale-invariant error term
def scale_invariant_error(prediction, ground_truth):
  # Compute the log difference between the prediction and the ground truth
  log_diff = torch.log(prediction + 1e-6) - torch.log(ground_truth + 1e-6)
  # Compute the mean and variance of the log difference
  mean = torch.mean(log_diff)
  var = torch.var(log_diff)
  # Compute the scale-invariant error term as the sum of variance and squared mean
  si_error = var + mean**2
  # Return the scale-invariant error term
  return si_error

# Define the gradient matching term
def gradient_matching_error(prediction, ground_truth):
  # Define the Sobel filters for x and y directions
  sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
  sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
  
  # Apply the Sobel filters to the prediction and the ground truth to get the gradients
  pred_grad_x = nn.functional.conv2d(prediction.unsqueeze(1), sobel_x.unsqueeze(0).unsqueeze(0))
  pred_grad_y = nn.functional.conv2d(prediction.unsqueeze(1), sobel_y.unsqueeze(0).unsqueeze(0))
  
  gt_grad_x = nn.functional.conv2d(ground_truth.unsqueeze(1), sobel_x.unsqueeze(0).unsqueeze(0))
  gt_grad_y = nn.functional.conv2d(ground_truth.unsqueeze(1), sobel_y.unsqueeze(0).unsqueeze(0))
  
  # Compute the L1 norm of the gradient difference between the prediction and the ground truth
  grad_error = torch.mean(torch.abs(pred_grad_x - gt_grad_x) + torch.abs(pred_grad_y - gt_grad_y))
  
  # Return the gradient matching term
  return grad_error

# Define the normal consistency term
def normal_consistency_error(prediction, ground_truth):
  # Define the Sobel filters for x and y directions
  sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
  sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
  
   # Apply the Sobel filters to the prediction and the ground truth to get the gradients
   pred_grad_x = nn.functional.conv2d(prediction.unsqueeze(1), sobel_x.unsqueeze(0).unsqueeze(0))
   pred_grad_y = nn.functional.conv2d(prediction.unsqueeze(1), sobel_y.unsqueeze(0).unsqueeze(0))
   
   gt_grad_x = nn.functional.conv2d(ground_truth.unsqueeze(1), sobel_x.unsqueeze(0).unsqueeze(0))
   gt_grad_y = nn.functional.conv2d(ground_truth.unsqueeze(1), sobel_y.unsqueeze(0).unsqueeze(0))
   
   # Compute the normal vectors from the gradients using cross product
   pred_normal = torch.cross(pred_grad_x.squeeze(), pred_grad_y.squeeze(), dim=1)
   gt_normal = torch.cross(gt_grad_x.squeeze(), gt_grad_y.squeeze(), dim=1)
   
   # Normalize the normal vectors to unit length
   pred_normal = nn.functional.normalize(pred_normal, dim=1)
   gt_normal = nn.functional.normalize(gt_normal, dim=1)
   
   # Compute the cosine similarity between the normal vectors
   cos_sim = torch.sum(pred_normal * gt_normal, dim=1)
   
   # Compute the normal consistency term as the mean of one minus cosine similarity
   normal_error = torch.mean(1 - cos_sim)
   
   # Return the normal consistency term
   return normal_error

# Define a function to compute confidence from validity mask or stereo confidence map
def compute_confidence(mask_or_map):
  
    if mask_or_map is None:
      # If no mask or map is given, return a tensor of ones with the same shape as prediction 
      return torch.ones_like(prediction)
    elif mask_or_map.ndim == prediction.ndim:
      # If mask or map has the same dimension as prediction, assume it is a stereo confidence map
      # Normalize the map to [0, 1] range and return it
      return (mask_or_map - mask_or_map.min()) / (mask_or_map.max() - mask_or_map.min())
    elif mask_or_map.ndim == prediction.ndim - 1:
      # If mask or map has one less dimension than prediction, assume it is a validity mask
      # Expand the mask to match the prediction shape and return it
      return mask_or_map.unsqueeze(1).expand_as(prediction)
    else:
      # Otherwise, raise an error
      raise ValueError("Invalid mask or map shape")

# Define the robust training objective
def loss_function(prediction, ground_truth, mask_or_map):
  # Compute the confidence from the mask or map
  confidence = compute_confidence(mask_or_map)
  # Compute the scale-invariant error term
  si_error = scale_invariant_error(prediction, ground_truth)
  # Compute the gradient matching term
  grad_error = gradient_matching_error(prediction, ground_truth)
  # Compute the normal consistency term
  normal_error = normal_consistency_error(prediction, ground_truth)
  # Weight the errors by the confidence
  weighted_error = confidence * (si_error + grad_error + normal_error)
  # Return the mean error over all pixels
  return torch.mean(weighted_error)

# Define a function to compute the Pareto dominance relation between two solutions
def pareto_dominates(solution1, solution2):
  # Unpack the scores of the two solutions
  scores1 = solution1["scores"]
  scores2 = solution2["scores"]
  
  # Initialize a flag to indicate if solution1 is better than solution2 on at least one metric
  better = False
  
  # For each pair of scores in the scores
  for score1, score2 in zip(scores1, scores2):
    # If solution1 is worse than solution2 on this metric, return False
    if score1 > score2:
      return False
    # If solution1 is better than solution2 on this metric, set the flag to True
    elif score1 < score2:
      better = True
  
  # Return the flag value
  return better

# Define a function to update the Pareto-optimal solutions with a new solution
def update_pareto(pareto_solutions, model, scores):
  
  # Initialize an empty list to store the updated Pareto-optimal solutions
  updated_pareto = []
  
  # Initialize a flag to indicate if the new solution is dominated by any existing solution
  dominated = False
  
  # For each existing solution in the Pareto-optimal solutions
  for solution in pareto_solutions:
    # If the new solution dominates the existing solution, discard the existing solution
    if pareto_dominates({"model": model, "scores": scores}, solution):
      continue
    # If the existing solution dominates the new solution, set the flag to True
    elif pareto_dominates(solution, {"model": model, "scores": scores}):
      dominated = True
    
    # Add the existing solution to the updated Pareto-optimal solutions
    updated_pareto.append(solution)
  
  # If the new solution is not dominated by any existing solution, add it to the updated Pareto-optimal solutions 
  if not dominated:
    updated_pareto.append({"model": model, "scores": scores})
  
  # Return the updated Pareto-optimal solutions 
  return updated_pareto

# Define a function to select the best solution based on a validation set 
def select_best(pareto_solutions, validation_set):
  
   # Initialize a variable to store the best solution and its score 
   best_solution = None 
   best_score = float("inf")
   
   # For each solution in the Pareto-optimal solutions 
   for solution in pareto_solutions: 
     # Evaluate the model of the solution on the validation set using the loss function 
     model = solution["model"] 
     prediction = model(validation_set["image"]) 
     ground_truth = validation_set["depth"] 
     mask_or_map = validation_set["mask_or_map"] 
     score = loss_function(prediction, ground_truth, mask_or_map) 
     
     # If the score is lower than the best score so far, update the best solution and its score 
     if score < best_score: 
       best_solution = solution 
       best_score = score
   
   # Return the best solution 
   return best_solution

# Define a function to train a model on a dataset using the loss function and an optimizer 
def train_model(dataset, loss_function, optimizer): 
  
   # Initialize a model with a pretrained encoder and a decoder network 
   encoder = models.resnet18(pretrained=True) 
   decoder = Decoder(encoder) 
   model = nn.Sequential(encoder, decoder)