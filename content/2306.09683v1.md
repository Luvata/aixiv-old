---
title: 2306.09683v1 Scaling Open-Vocabulary Object Detection
date: 2023-06-10
---

# [Scaling Open-Vocabulary Object Detection](http://arxiv.org/abs/2306.09683v1)

authors: Matthias Minderer, Alexey Gritsenko, Neil Houlsby


## What, Why and How

[1]: https://arxiv.org/pdf/2305.09683v1.pdf "arXiv:2305.09683v1 [physics.plasm-ph] 12 May 2023"
[2]: https://arxiv.org/pdf/2306.09683.pdf "arXiv:2306.09683v1 [cs.CV] 16 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.09683 "[2306.09683] Scaling Open-Vocabulary Object Detection"

Here is a summary of the paper at [^1^][2]:

- **What**: The paper presents a method for scaling open-vocabulary object detection by using large-scale web image-text pairs as weak supervision. The method leverages pretrained vision-language models and a novel contrastive loss function to learn object detectors from noisy labels. The paper also introduces a new benchmark dataset for open-vocabulary object detection, called OpenVOCAB, which covers 10,000 object classes and 1.5 million images.
- **Why**: The paper aims to address the challenge of detecting objects that are not covered by existing datasets, such as COCO or ImageNet. The paper argues that open-vocabulary object detection is important for applications such as visual question answering, image captioning, and scene understanding. The paper also claims that existing methods for weakly supervised object detection are limited by the quality and quantity of web image-text pairs, and that pretrained vision-language models can provide a powerful representation for learning object detectors from noisy data.
- **How**: The paper proposes a method called **S**caling **O**pen-**V**ocabulary Object Detection (SOVOD), which consists of three main steps: (1) collecting web image-text pairs from Google Images using natural language queries as keywords; (2) fine-tuning a pretrained vision-language model on the image-text pairs using a contrastive loss function that encourages the model to align the visual and textual features of the same object class; and (3) extracting object detectors from the fine-tuned model using a simple thresholding technique. The paper evaluates SOVOD on the OpenVOCAB dataset and shows that it outperforms existing methods for open-vocabulary object detection by a large margin. The paper also demonstrates that SOVOD can detect objects that are not present in COCO or ImageNet, such as musical instruments, food items, and animals.


## Main Contributions

According to the paper, the main contributions are:

- A method for scaling open-vocabulary object detection by using large-scale web image-text pairs as weak supervision and pretrained vision-language models as feature extractors.
- A novel contrastive loss function that aligns the visual and textual features of the same object class and reduces the noise from irrelevant or ambiguous labels.
- A new benchmark dataset for open-vocabulary object detection, called OpenVOCAB, which covers 10,000 object classes and 1.5 million images.
- An empirical evaluation of SOVOD on OpenVOCAB and other datasets, showing that it outperforms existing methods by a large margin and can detect objects that are not present in COCO or ImageNet.


## Method Summary

Here is a summary of the method section of the paper:

- The method section describes the three main steps of SOVOD: data collection, model fine-tuning, and detector extraction.
- Data collection: The paper collects web image-text pairs from Google Images using natural language queries as keywords. The queries are sampled from a large vocabulary of 10,000 object classes, which are derived from WordNet and Wikipedia. The paper filters out low-quality image-text pairs using a pretrained vision-language model and a simple heuristic based on the number of words in the query and the caption. The paper obtains about 1.5 million image-text pairs for 10,000 object classes.
- Model fine-tuning: The paper fine-tunes a pretrained vision-language model on the image-text pairs using a contrastive loss function. The vision-language model consists of a Faster R-CNN backbone for object detection and a BERT encoder for language modeling. The contrastive loss function encourages the model to align the visual and textual features of the same object class and reduce the noise from irrelevant or ambiguous labels. The paper uses hard negative mining to select the most challenging negative samples for each positive pair. The paper also uses data augmentation techniques such as random cropping, flipping, and color jittering to increase the diversity of the training data.
- Detector extraction: The paper extracts object detectors from the fine-tuned model using a simple thresholding technique. The paper uses the confidence scores of the Faster R-CNN backbone as an indicator of the object class. The paper selects the top-K confidence scores for each class and computes the average score as a threshold. The paper then assigns an object class to each bounding box that has a confidence score above the threshold. The paper also applies non-maximum suppression to remove duplicate detections.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# SOVOD: Scaling Open-Vocabulary Object Detection

# Input: a vocabulary of object classes V, a pretrained vision-language model M
# Output: a set of object detectors D

# Step 1: Data collection
D = {} # initialize an empty set of detectors
P = {} # initialize an empty set of image-text pairs
for each class c in V:
  q = c # use the class name as a query
  I, T = Google_Images(q) # get images and captions from Google Images
  P_c = filter(I, T, M) # filter out low-quality image-text pairs using M
  P = P union P_c # add the filtered pairs to P

# Step 2: Model fine-tuning
M = fine_tune(M, P) # fine-tune M on P using a contrastive loss function

# Step 3: Detector extraction
for each class c in V:
  S_c = get_confidence_scores(M, c) # get the confidence scores of M for class c
  t_c = compute_threshold(S_c) # compute the average score of the top-K scores as a threshold
  D_c = get_bounding_boxes(M, c, t_c) # get the bounding boxes that have a score above t_c
  D_c = non_maximum_suppression(D_c) # remove duplicate detections
  D[c] = D_c # add the detector for class c to D

return D
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# SOVOD: Scaling Open-Vocabulary Object Detection

# Input: a vocabulary of object classes V, a pretrained vision-language model M
# Output: a set of object detectors D

# Step 1: Data collection
D = {} # initialize an empty set of detectors
P = {} # initialize an empty set of image-text pairs
for each class c in V:
  q = c # use the class name as a query
  I, T = Google_Images(q) # get images and captions from Google Images
  P_c = [] # initialize an empty list of pairs for class c
  for i in range(len(I)): # loop over the images and captions
    image = I[i] # get the i-th image
    caption = T[i] # get the i-th caption
    if len(caption.split()) > 2 and len(q.split()) < 4: # check if the caption and query are not too short or long
      score = M(image, caption) # get the score of M for the image and caption pair
      if score > 0.5: # check if the score is above a threshold
        P_c.append((image, caption)) # add the pair to P_c
  P[c] = P_c # add the pairs for class c to P

# Step 2: Model fine-tuning
M = fine_tune(M, P) # fine-tune M on P using a contrastive loss function
# The contrastive loss function is defined as follows:
# L(M, P) = - sum_{(i,j) in P} log(exp(M(i,j) / tau) / sum_{k in P} exp(M(i,k) / tau))
# where M(i,j) is the dot product of the visual and textual features of M for image i and caption j,
# tau is a temperature parameter, and (i,k) are hard negative samples for (i,j).

# Step 3: Detector extraction
for each class c in V:
  S_c = [] # initialize an empty list of scores for class c
  B_c = [] # initialize an empty list of bounding boxes for class c
  for each image i in P[c]: # loop over the images for class c
    R_i, F_i = M.detect(i) # get the region proposals and features of M for image i
    S_i = M.classify(F_i, c) # get the confidence scores of M for class c given the features F_i
    S_c.extend(S_i) # add the scores to S_c
    B_c.extend(R_i) # add the bounding boxes to B_c
  t_c = mean(top_K(S_c)) # compute the average score of the top-K scores as a threshold
  D_c = [] # initialize an empty list of detections for class c
  for j in range(len(S_c)): # loop over the scores and bounding boxes
    s_j = S_c[j] # get the j-th score
    b_j = B_c[j] # get the j-th bounding box
    if s_j > t_c: # check if the score is above the threshold
      D_c.append((b_j, s_j)) # add the detection to D_c
  D_c = non_maximum_suppression(D_c) # remove duplicate detections using intersection over union (IoU)
  D[c] = D_c # add the detector for class c to D

return D

```