---
title: 2302.10167v2 Cross-domain Compositing with Pretrained Diffusion Models
date: 2023-02-11
---

# [Cross-domain Compositing with Pretrained Diffusion Models](http://arxiv.org/abs/2302.10167v2)

authors: Roy Hachnochi, Mingrui Zhao, Nadav Orzech, Rinon Gal, Ali Mahdavi-Amiri, Daniel Cohen-Or, Amit Haim Bermano


## What, Why and How

[1]: https://arxiv.org/abs/2302.10167 "Cross-domain Compositing with Pretrained Diffusion Models"
[2]: https://arxiv-export2.library.cornell.edu/abs/2302.10167v2 "[2302.10167v2] Cross-domain Compositing with Pretrained Diffusion Models"
[3]: https://arxiv.org/pdf/2302.10167v1.pdf "arXiv.org"

Here is a summary of the paper:

- **What**: The paper presents a method for cross-domain compositing with pretrained diffusion models, which are generative models that can synthesize realistic images from noise.
- **Why**: The paper aims to show that diffusion models can be used for various image editing tasks that involve combining objects from different domains, such as blending, immersion, texture-replacement, and stylization. The paper claims that this approach does not require any annotations or training, and produces higher quality and realistic results than prior work.
- **How**: The paper uses a localized, iterative refinement scheme that infuses the injected objects with contextual information derived from the background scene. The paper also allows the user to control the degree and types of changes the object may undergo. The paper evaluates the method on several datasets and tasks, and compares it with existing methods. The paper also demonstrates how the method can be used for data augmentation of downstream tasks.

## Main Contributions

[1]: https://arxiv.org/abs/2302.10167 "Cross-domain Compositing with Pretrained Diffusion Models"
[2]: https://arxiv-export2.library.cornell.edu/abs/2302.10167v2 "[2302.10167v2] Cross-domain Compositing with Pretrained Diffusion Models"
[3]: https://arxiv.org/pdf/2302.10167v1.pdf "arXiv.org"

The paper claims to make the following contributions:

- It demonstrates that **pretrained diffusion models** can be used for a wide range of **cross-domain compositing tasks**, such as image blending, object immersion, texture-replacement, and stylization[^1^][1] [^2^][2].
- It proposes a **localized, iterative refinement scheme** that infuses the injected objects with contextual information derived from the background scene, and enables control over the degree and types of changes the object may undergo[^1^][1] [^2^][2].
- It conducts a range of **qualitative and quantitative comparisons** to prior work, and exhibits that its method produces **higher quality and realistic results** without requiring any annotations or training[^1^][1] [^2^][2].
- It demonstrates how its method may be used for **data augmentation** of downstream tasks[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/abs/2302.10167 "Cross-domain Compositing with Pretrained Diffusion Models"
[2]: https://arxiv-export2.library.cornell.edu/abs/2302.10167v2 "[2302.10167v2] Cross-domain Compositing with Pretrained Diffusion Models"
[3]: https://arxiv.org/pdf/2302.10167v1.pdf "arXiv.org"

Here is a summary of the method section of the paper:

- The paper uses **pretrained diffusion models** that can synthesize realistic images from noise by reversing a Markov chain of Gaussian noise additions[^1^][1] [^2^][2].
- The paper assumes that the user provides an **input image** that contains a **background scene** and an **injected object** from a different domain[^1^][1] [^2^][2].
- The paper defines a **mask** that indicates the region of interest where the injected object is located, and a **conditioning image** that is obtained by applying the mask to the input image[^1^][1] [^2^][2].
- The paper applies the diffusion model to the input image in a **localized** manner, by only updating the pixels within the mask at each iteration[^1^][1] [^2^][2].
- The paper uses a **refinement scheme** that iteratively updates the mask and the conditioning image based on the output of the diffusion model, and infuses the injected object with contextual information derived from the background scene[^1^][1] [^2^][2].
- The paper allows the user to control the degree and types of changes the object may undergo by adjusting a **temperature parameter** that controls the variance of the noise added by the diffusion model, and a **blending parameter** that controls how much of the original object is preserved in the output[^1^][1] [^2^][2].

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2302.10167 "Cross-domain Compositing with Pretrained Diffusion Models"
[2]: https://arxiv-export2.library.cornell.edu/abs/2302.10167v2 "[2302.10167v2] Cross-domain Compositing with Pretrained Diffusion Models"
[3]: https://arxiv.org/pdf/2302.10167v1.pdf "arXiv.org"

Here is the high-level pseudo code for this paper:

```python
# Input: an image x that contains a background scene and an injected object
# Output: a composited image y that blends the object with the scene

# Load a pretrained diffusion model D
D = load_model()

# Define a mask m that indicates the region of interest where the object is located
m = define_mask(x)

# Define a conditioning image c that is obtained by applying the mask to the input image
c = apply_mask(x, m)

# Define a temperature parameter T and a blending parameter B
T = user_input()
B = user_input()

# Initialize the output image y as the input image x
y = x

# For each iteration t from 1 to T:
for t in range(1, T+1):

  # Update the output image y by applying the diffusion model D only within the mask m
  y = update_image(y, D, m, T, t)

  # Update the mask m by eroding it slightly
  m = erode_mask(m)

  # Update the conditioning image c by blending it with the output image y
  c = blend_image(c, y, m, B)

# Return the output image y
return y
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2302.10167 "Cross-domain Compositing with Pretrained Diffusion Models"
[2]: https://arxiv-export2.library.cornell.edu/abs/2302.10167v2 "[2302.10167v2] Cross-domain Compositing with Pretrained Diffusion Models"
[3]: https://arxiv.org/pdf/2302.10167v1.pdf "arXiv.org"

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import cv2
import torch
import torchvision

# Load a pretrained diffusion model D
D = torch.hub.load('openai/diffusion', 'imagenet_1024')

# Define a function to define a mask m that indicates the region of interest where the object is located
def define_mask(x):
  # Use a simple thresholding method to segment the object from the background
  # Convert the image to grayscale
  gray = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)
  # Apply Otsu's thresholding method
  _, m = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
  # Return the mask as a boolean array
  return m > 0

# Define a function to apply a mask m to an input image x and obtain a conditioning image c
def apply_mask(x, m):
  # Copy the input image
  c = x.copy()
  # Set the pixels outside the mask to zero
  c[~m] = 0
  # Return the conditioning image
  return c

# Define a function to update the output image y by applying the diffusion model D only within the mask m
def update_image(y, D, m, T, t):
  # Convert the output image y to a tensor and normalize it to [-1, 1]
  y = torchvision.transforms.ToTensor()(y).unsqueeze(0) * 2 - 1
  # Convert the mask m to a tensor and expand it to three channels
  m = torch.from_numpy(m).unsqueeze(0).unsqueeze(0).expand(-1, 3, -1, -1)
  # Compute the noise scale for the current iteration t
  noise_scale = np.sqrt(D.betas[T - t])
  # Sample Gaussian noise with zero mean and unit variance
  noise = torch.randn_like(y)
  # Add scaled noise to the output image y only within the mask m
  y = y + noise * noise_scale * m
  # Apply the diffusion model D to denoise the output image y conditioned on the mask m
  y = D.p_mean(y, m, T - t)
  # Clip the output image y to [-1, 1]
  y = torch.clamp(y, -1, 1)
  # Denormalize and convert the output image y to a numpy array
  y = (y + 1) / 2 * 255
  y = torchvision.transforms.ToPILImage()(y.squeeze(0)).convert('RGB')
  y = np.array(y)
  # Return the output image y
  return y

# Define a function to erode the mask m slightly at each iteration t
def erode_mask(m):
  # Define an erosion kernel size based on the iteration t
  kernel_size = max(1, int(10 - t / T * 10))
  # Define an erosion kernel with a circular shape
  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
  # Apply erosion to the mask m using the kernel
  m = cv2.erode(m.astype(np.uint8), kernel)
  # Return the eroded mask as a boolean array
  return m > 0

# Define a function to blend the conditioning image c with the output image y using a blending parameter B
def blend_image(c, y, m, B):
  # Convert the blending parameter B to a float between [0,1]
  B = float(B) / T
  # Blend the conditioning image c and the output image y using a weighted average based on B and m
  c = c * (1 - B) + y * B * m + c * (1 - m)
  # Return the blended conditioning image c as an integer array between [0,255]
  return c.astype(np.uint8)

# Input: an image x that contains a background scene and an injected object
# Output: a composited image y that blends the object with the scene

# Load an input image x from a file path or URL
x = cv2.imread('input.jpg')

# Define a mask m that indicates the region of interest where the object is located
m = define_mask(x)

# Define a conditioning image c that is obtained by applying the mask to the input image
c = apply_mask(x, m)

# Define a temperature parameter T and a blending parameter B
T = user_input()
B = user_input()

# Initialize the output image y as the input image x
y = x

# For each iteration t from 1 to T:
for t in range(1, T+1):

  # Update the output image y by applying the diffusion model D only within the mask m
  y = update_image(y, D, m, T, t)

  # Update the mask m by eroding it slightly
  m = erode_mask(m)

  # Update the conditioning image c by blending it with the output image y
  c = blend_image(c, y, m, B)

# Return the output image y
return y
```