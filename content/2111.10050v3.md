---
title: 2111.10050v3 Combined Scaling for Zero-shot Transfer Learning
date: 2021-11-11
---

# [Combined Scaling for Zero-shot Transfer Learning](http://arxiv.org/abs/2111.10050v3)

authors: Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, Quoc V. Le


## What, Why and How

[1]: https://arxiv.org/abs/2111.10050v3 "[2111.10050v3] Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2111.10050v3.pdf "Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2111.10050v3 "arXiv.org"

Here is a summary of the paper:

```
What: The paper presents a combined scaling method - named BASIC - that achieves state-of-the-art performance on zero-shot transfer learning for image classification. BASIC scales up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size.

Why: The paper aims to eliminate the need for collecting labeled training data for every new application by using natural language as a weak supervision signal. The paper also investigates the benefits of large contrastive batch sizes and proposes methods to overcome the memory limit of accelerators.

How: The paper uses a dataset of 6.6B noisy image-text pairs, a model with 3B weights, and a batch size of 65536 to train BASIC. The paper also uses gradient checkpointing and model parallelism to reduce the memory consumption. The paper develops a theoretical framework to show that larger contrastive batch sizes lead to smaller generalization gaps.
```

## Main Contributions

[1]: https://arxiv.org/abs/2111.10050v3 "[2111.10050v3] Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2111.10050v3.pdf "Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2111.10050v3 "arXiv.org"

Here are some of the contributions of the paper:

```
- The paper proposes a combined scaling method - named BASIC - that achieves state-of-the-art performance on zero-shot transfer learning for image classification, surpassing best-published similar models - CLIP and ALIGN - by 9.3% on ImageNet ILSVRC-2012.
- The paper also shows significant improvements in robustness benchmarks, such as ImageNet-{A,R,V2,Sketch} and ObjectNet, where BASIC achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy.
- The paper scales up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size, using a dataset of 6.6B noisy image-text pairs, a model with 3B weights, and a batch size of 65536.
- The paper proposes two simple methods to overcome the memory limit of accelerators, such as GPUs and TPUs, which make use of gradient checkpointing and model parallelism.
- The paper develops a theoretical framework to show that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC.
```

## Method Summary

[1]: https://arxiv.org/abs/2111.10050v3 "[2111.10050v3] Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2111.10050v3.pdf "Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2111.10050v3 "arXiv.org"

Here is a summary of the method section of the paper:

```
The paper uses a contrastive learning framework that learns to align image and text representations in a shared latent space. The framework consists of three components: an image encoder, a text encoder, and a contrastive loss function. The image encoder is a convolutional neural network (CNN) that takes an image as input and outputs a feature vector. The text encoder is a transformer-based model that takes a text caption as input and outputs a feature vector. The contrastive loss function measures the similarity between the image and text features and encourages them to be close for positive pairs (image-text pairs from the same data point) and far for negative pairs (image-text pairs from different data points).

The paper scales up the contrastive learning framework in three dimensions: data size, model size, and batch size. The paper uses a large-scale dataset of 6.6B noisy image-text pairs collected from the web, which covers diverse domains and languages. The paper also uses large models with up to 3B weights, which are composed of 96-layer CNNs and 48-layer transformers. The paper also uses large batch sizes of up to 65536, which are distributed across multiple accelerators.

The paper proposes two simple methods to overcome the memory limit of accelerators, such as GPUs and TPUs. The first method is gradient checkpointing, which saves intermediate activations during the forward pass and recomputes them during the backward pass, reducing the memory consumption by trading off computation time. The second method is model parallelism, which splits the model parameters across different devices and performs communication between them during training, reducing the memory consumption by trading off communication cost.

The paper develops a theoretical framework to show that larger contrastive batch sizes lead to smaller generalization gaps for image-text models such as BASIC. The framework is based on the Rademacher complexity, which measures the expressiveness of a hypothesis class. The paper shows that larger contrastive batch sizes reduce the Rademacher complexity of image-text models by increasing the diversity of negative pairs and decreasing the variance of similarity scores.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2111.10050v3 "[2111.10050v3] Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2111.10050v3.pdf "Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2111.10050v3 "arXiv.org"

Here is a high-level pseudo code for the paper:

```
# Define the image encoder, a CNN with 96 layers
image_encoder = CNN(96)

# Define the text encoder, a transformer with 48 layers
text_encoder = Transformer(48)

# Define the contrastive loss function
contrastive_loss = NT-Xent()

# Load the dataset of 6.6B noisy image-text pairs
dataset = load_dataset("6.6B")

# Split the dataset into batches of size 65536
batches = split_dataset(dataset, 65536)

# Train the model for N epochs
for epoch in range(N):

  # Shuffle the batches
  shuffle(batches)

  # Loop over the batches
  for batch in batches:

    # Apply gradient checkpointing and model parallelism to save memory
    with gradient_checkpointing() and model_parallelism():

      # Get the images and texts from the batch
      images, texts = batch

      # Encode the images and texts
      image_features = image_encoder(images)
      text_features = text_encoder(texts)

      # Compute the contrastive loss
      loss = contrastive_loss(image_features, text_features)

      # Update the model parameters using gradient descent
      update_parameters(loss)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2111.10050v3 "[2111.10050v3] Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2111.10050v3.pdf "Combined Scaling for Zero-shot Transfer Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2111.10050v3 "arXiv.org"

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the image encoder, a CNN with 96 layers and 3B weights
image_encoder = torchvision.models.resnet152(pretrained=True)
image_encoder.fc = torch.nn.Linear(image_encoder.fc.in_features, 512)
image_encoder = torch.nn.Sequential(*[image_encoder, *torch.nn.ModuleList([torch.nn.Conv2d(512, 512, 3, padding=1) for _ in range(48)])])

# Define the text encoder, a transformer with 48 layers and 3B weights
text_encoder = transformers.AutoModel.from_pretrained("bert-large-uncased")
text_encoder.encoder = torch.nn.ModuleList([text_encoder.encoder.layer[i] for i in range(0, 24, 2)])
text_encoder.pooler.dense = torch.nn.Linear(text_encoder.pooler.dense.in_features, 512)

# Define the contrastive loss function, NT-Xent with temperature 0.07
contrastive_loss = torch.nn.CrossEntropyLoss()
temperature = 0.07

# Load the dataset of 6.6B noisy image-text pairs
dataset = torchvision.datasets.ImageFolder("6.6B", transform=torchvision.transforms.ToTensor())

# Split the dataset into batches of size 65536
batch_size = 65536
batches = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the optimizer and the learning rate scheduler
optimizer = torch.optim.Adam([*image_encoder.parameters(), *text_encoder.parameters()], lr=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(batches))

# Train the model for N epochs
N = 10
for epoch in range(N):

  # Loop over the batches
  for batch in batches:

    # Apply gradient checkpointing and model parallelism to save memory
    with torch.utils.checkpoint.checkpoint_sequential(image_encoder, len(image_encoder)) as image_encoder_parallel:
      with torch.utils.checkpoint.checkpoint_sequential(text_encoder, len(text_encoder)) as text_encoder_parallel:

        # Get the images and texts from the batch
        images, texts = batch

        # Encode the images and texts
        image_features = image_encoder_parallel(images)
        text_features = text_encoder_parallel(texts)

        # Normalize the features to unit length
        image_features = image_features / torch.norm(image_features, dim=1, keepdim=True)
        text_features = text_features / torch.norm(text_features, dim=1, keepdim=True)

        # Compute the similarity matrix between image and text features
        similarity_matrix = torch.matmul(image_features, text_features.t()) / temperature

        # Compute the contrastive loss for image-text and text-image pairs
        labels = torch.arange(batch_size)
        loss_image_text = contrastive_loss(similarity_matrix, labels)
        loss_text_image = contrastive_loss(similarity_matrix.t(), labels)
        loss = (loss_image_text + loss_text_image) / 2

        # Update the model parameters using gradient descent
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

    # Print the loss value
    print(f"Epoch {epoch}, Batch {batch}, Loss {loss.item()}")
```