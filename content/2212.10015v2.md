---
title: 2212.10015v2 Benchmarking Spatial Relationships in Text-to-Image Generation
date: 2022-12-11
---

# [Benchmarking Spatial Relationships in Text-to-Image Generation](http://arxiv.org/abs/2212.10015v2)

authors: Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang


## What, Why and How

[1]: https://arxiv.org/pdf/2212.10015v2.pdf "arXiv:2212.10015v2 [cs.CV] 6 Jun 2023"
[2]: https://arxiv.org/abs/2212.10015 "Benchmarking Spatial Relationships in Text-to-Image Generation"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.10015 "[2212.10015] Benchmarking Spatial Relationships in Text-to-Image Generation"

Here is a summary of the paper:

- **What**: The paper presents a new evaluation metric (VISOR) and a new dataset (SR2D) for benchmarking the spatial understanding capabilities of text-to-image synthesis (T2I) models.
- **Why**: The paper argues that spatial understanding is a fundamental aspect of computer vision and grounded language understanding, but existing T2I models are severely limited in their ability to generate correct spatial relationships among objects.
- **How**: The paper constructs an automated evaluation pipeline that uses computer vision techniques to recognize objects and their spatial relationships in the generated images. The paper evaluates several state-of-the-art T2I models on the SR2D dataset and analyzes their biases and artifacts. The paper also conducts a human study to validate the alignment between VISOR and human judgement.


## Main Contributions

[1]: https://arxiv.org/pdf/2212.10015v2.pdf "arXiv:2212.10015v2 [cs.CV] 6 Jun 2023"
[2]: https://arxiv.org/abs/2212.10015 "Benchmarking Spatial Relationships in Text-to-Image Generation"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.10015 "[2212.10015] Benchmarking Spatial Relationships in Text-to-Image Generation"

According to the paper[^1^][1], the main contributions are:

- **VISOR**: A novel evaluation metric that measures how accurately the spatial relationship described in text is generated in the image. VISOR is based on computer vision techniques and does not require human annotations or ground-truth images.
- **SR2D**: A large-scale dataset that contains sentences describing two objects and the spatial relationship between them. SR2D covers a diverse set of objects and spatial relationships and can be used to benchmark T2I models on their spatial understanding capabilities.
- **Evaluation and Analysis**: A comprehensive evaluation and analysis of several state-of-the-art T2I models on the SR2D dataset using VISOR. The paper reveals the limitations and biases of existing T2I models and provides insights for future research directions.


## Method Summary

[1]: https://arxiv.org/pdf/2212.10015v2.pdf "arXiv:2212.10015v2 [cs.CV] 6 Jun 2023"
[2]: https://arxiv.org/abs/2212.10015 "Benchmarking Spatial Relationships in Text-to-Image Generation"
[3]: https://arxiv-export2.library.cornell.edu/abs/2212.10015 "[2212.10015] Benchmarking Spatial Relationships in Text-to-Image Generation"

Here is a summary of the method section of the paper:

- **VISOR**: The paper proposes a metric that computes the similarity between the spatial relationship described in text and the spatial relationship detected in the image. The paper uses a pre-trained object detector (Faster R-CNN [37]) to locate and label objects in the image, and then computes the relative position and orientation of each pair of objects using their bounding boxes. The paper defines four types of spatial relationships: left/right, above/below, inside/outside, and near/far. The paper computes a score for each type of relationship based on the distance and angle between the objects, and then aggregates them into a final score using a weighted average. The paper also introduces a confidence score that measures how confident the object detector is about the presence and location of the objects.
- **SR2D**: The paper introduces a dataset that contains 20,000 sentences describing two objects and the spatial relationship between them. The paper uses a template-based approach to generate sentences that cover a diverse set of objects and spatial relationships. The paper uses 100 common objects from the COCO dataset [27] and 16 spatial relationships from the SpatialVOC2K dataset [17]. The paper ensures that each object appears at least once in each spatial relationship category, and that each sentence is grammatically correct and unambiguous. The paper also provides human annotations for the object labels and locations for a subset of 2,000 sentences.
- **Evaluation and Analysis**: The paper evaluates several state-of-the-art T2I models on the SR2D dataset using VISOR. The paper uses four models: DALL-E [12], DALLE-v2 [36], CLIP-Draw [13], and VQGAN-CLIP [14]. The paper also compares VISOR with two baseline metrics: FID [18] and CLIP-score [13]. The paper analyzes the results from different perspectives, such as object frequency, object order, relationship type, relationship direction, and relationship equivalence. The paper also conducts a human study to validate the alignment between VISOR and human judgement.


## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the objects and spatial relationships
objects = ["apple", "banana", "carrot", ...] # 100 objects from COCO dataset
relationships = ["left", "right", "above", "below", ...] # 16 relationships from SpatialVOC2K dataset

# Generate the SR2D dataset
SR2D = [] # an empty list to store sentences
for each object1 in objects:
  for each object2 in objects:
    for each relationship in relationships:
      # Generate a sentence using a template
      sentence = f"A {object1} is {relationship} of a {object2}."
      # Add the sentence to the SR2D list
      SR2D.append(sentence)

# Evaluate the T2I models on the SR2D dataset using VISOR
T2I_models = [DALL-E, DALLE-v2, CLIP-Draw, VQGAN-CLIP] # four state-of-the-art models
for each model in T2I_models:
  VISOR_score = 0 # initialize the score to zero
  for each sentence in SR2D:
    # Generate an image using the model
    image = model.generate(sentence)
    # Detect the objects and their bounding boxes using Faster R-CNN
    objects, boxes = Faster_RCNN.detect(image)
    # Compute the spatial relationship score for each pair of objects using their boxes
    relationship_score = compute_relationship_score(objects, boxes, sentence)
    # Compute the confidence score for the object detection using their boxes
    confidence_score = compute_confidence_score(objects, boxes)
    # Update the VISOR score using a weighted average of the relationship score and the confidence score
    VISOR_score += weight * relationship_score + (1 - weight) * confidence_score
  # Normalize the VISOR score by the number of sentences
  VISOR_score /= len(SR2D)
  # Print the VISOR score for the model
  print(f"VISOR score for {model} is {VISOR_score}")
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torchvision
import clip
import dalle_pytorch
import clip_draw
import taming

# Define the objects and spatial relationships
objects = ["apple", "banana", "carrot", ...] # 100 objects from COCO dataset
relationships = ["left", "right", "above", "below", ...] # 16 relationships from SpatialVOC2K dataset

# Define the thresholds for the spatial relationship categories
left_threshold = 0.5 # the minimum horizontal distance ratio for left relationship
right_threshold = 0.5 # the minimum horizontal distance ratio for right relationship
above_threshold = 0.5 # the minimum vertical distance ratio for above relationship
below_threshold = 0.5 # the minimum vertical distance ratio for below relationship
inside_threshold = 0.8 # the minimum intersection over union ratio for inside relationship
outside_threshold = 0.2 # the maximum intersection over union ratio for outside relationship
near_threshold = 0.2 # the maximum euclidean distance ratio for near relationship
far_threshold = 0.8 # the minimum euclidean distance ratio for far relationship

# Define the weights for the VISOR metric
relationship_weight = 0.8 # the weight for the relationship score
confidence_weight = 0.2 # the weight for the confidence score

# Generate the SR2D dataset
SR2D = [] # an empty list to store sentences and annotations
for each object1 in objects:
  for each object2 in objects:
    for each relationship in relationships:
      # Generate a sentence using a template
      sentence = f"A {object1} is {relationship} of a {object2}."
      # Generate a ground-truth image using a synthetic scene generator (optional)
      image = generate_image(object1, object2, relationship)
      # Detect the objects and their bounding boxes using Faster R-CNN on the ground-truth image (optional)
      objects, boxes = Faster_RCNN.detect(image)
      # Add the sentence, image, objects, and boxes to the SR2D list (optional)
      SR2D.append((sentence, image, objects, boxes))

# Load the T2I models and their parameters
DALL_E = dalle_pytorch.DALLE.load_from_checkpoint("dall_e.ckpt") # load DALL-E model from checkpoint
DALLE_v2 = dalle_pytorch.DALLE.load_from_checkpoint("dalle_v2.ckpt") # load DALLE-v2 model from checkpoint
CLIP_Draw = clip_draw.CLIPDraw.load_from_checkpoint("clip_draw.ckpt") # load CLIP-Draw model from checkpoint
VQGAN_CLIP = taming.models.vqgan.VQModel.load_from_checkpoint("vqgan_clip.ckpt") # load VQGAN-CLIP model from checkpoint

T2I_models = [DALL_E, DALLE_v2, CLIP_Draw, VQGAN_CLIP] # a list of T2I models

# Load the CLIP model and its tokenizer
CLIP_model, CLIP_preprocess = clip.load("ViT-B/32", device="cuda") # load CLIP model and preprocess function
CLIP_tokenizer = clip.simple_tokenizer.SimpleTokenizer() # load CLIP tokenizer

# Load the Faster R-CNN model and its transform function
Faster_RCNN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # load Faster R-CNN model with pretrained weights
Faster_RCNN.eval() # set the model to evaluation mode
Faster_RCNN.to("cuda") # move the model to GPU device
Faster_RCNN_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) # define a transform function to convert PIL image to tensor

# Define a function to compute the spatial relationship score between two objects using their bounding boxes and text description
def compute_relationship_score(objects, boxes, text):
  # Initialize the score to zero
  score = 0

  # Extract the object names and the relationship from the text using a simple parser (can be improved with NLP techniques)
  object1, relationship, object2 = text.split()

  # Find the indices of the objects in the detected list (assuming they are present and unique)
  index1 = objects.index(object1)
  index2 = objects.index(object2)

  # Get the bounding boxes of the objects (assuming they are in [x1, y1, x2, y2] format)
  box1 = boxes[index1]
  box2 = boxes[index2]

  # Compute the width and height of the image
  width = max(box1[2], box2[2])
  height = max(box1[3], box2[3])

  # Compute the center coordinates of the objects
  cx1 = (box1[0] + box1[2]) / 2
  cy1 = (box1[1] + box1[3]) / 2
  cx2 = (box2[0] + box2[2]) / 2
  cy2 = (box2[1] + box2[3]) / 2

  # Compute the horizontal and vertical distance ratios between the objects
  dx = abs(cx1 - cx2) / width
  dy = abs(cy1 - cy2) / height

  # Compute the euclidean distance ratio between the objects
  d = np.sqrt(dx**2 + dy**2)

  # Compute the intersection over union ratio between the objects
  iou = compute_iou(box1, box2)

  # Compute the angle between the objects
  angle = np.arctan2(cy2 - cy1, cx2 - cx1)

  # Check the relationship type and compute the score accordingly
  if relationship == "left":
    # The score is high if the second object is to the left of the first object and the horizontal distance is large
    score = (cx2 < cx1) * dx
  elif relationship == "right":
    # The score is high if the second object is to the right of the first object and the horizontal distance is large
    score = (cx2 > cx1) * dx
  elif relationship == "above":
    # The score is high if the second object is above the first object and the vertical distance is large
    score = (cy2 < cy1) * dy
  elif relationship == "below":
    # The score is high if the second object is below the first object and the vertical distance is large
    score = (cy2 > cy1) * dy
  elif relationship == "inside":
    # The score is high if the intersection over union ratio is large
    score = iou
  elif relationship == "outside":
    # The score is high if the intersection over union ratio is small
    score = 1 - iou
  elif relationship == "near":
    # The score is high if the euclidean distance ratio is small
    score = 1 - d
  elif relationship == "far":
    # The score is high if the euclidean distance ratio is large
    score = d

  # Return the score
  return score

# Define a function to compute the intersection over union ratio between two bounding boxes
def compute_iou(box1, box2):
  # Compute the area of each box
  area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
  area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])

  # Compute the coordinates of the intersection rectangle
  x1 = max(box1[0], box2[0])
  y1 = max(box1[1], box2[1])
  x2 = min(box1[2], box2[2])
  y2 = min(box1[3], box2[3])

  # Compute the area of the intersection rectangle
  inter_area = max(0, x2 - x1) * max(0, y2 - y1)

  # Compute the union area by adding both areas and subtracting the intersection area
  union_area = area1 + area2 - inter_area

  # Compute the intersection over union ratio by dividing the intersection area by the union area
  iou = inter_area / union_area

  # Return the iou ratio
  return iou

# Define a function to compute the confidence score for object detection using their bounding boxes
def compute_confidence_score(objects, boxes):
  # Initialize the score to zero
  score = 0

  # Loop through each object and its bounding box
  for object, box in zip(objects, boxes):
    # Get the width and height of the bounding box
    w = box[2] - box[0]
    h = box[3] - box[1]

    # Compute a confidence score based on some heuristic (can be improved with other methods)
    confidence = w * h

    # Update the score by adding the confidence value
    score += confidence

  # Normalize the score by dividing by some constant (can be improved with other methods)
  score /= len(objects)

  # Return the score
  return score

# Evaluate the T2I models on the SR2D dataset using VISOR
for each model in T2