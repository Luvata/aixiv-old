---
title: 2111.03930v2 Tip-Adapter  Training-free CLIP-Adapter for Better Vision-Language Modeling
date: 2021-11-04
---

# [Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling](http://arxiv.org/abs/2111.03930v2)

authors: Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li


## What, Why and How

[1]: https://arxiv.org/abs/2111.03930 "[2111.03930] Tip-Adapter: Training-free CLIP-Adapter for Better Vision ..."
[2]: http://export.arxiv.org/abs/2111.03930v1 "[2111.03930v1] Tip-Adapter: Training-free CLIP-Adapter for ... - arXiv"
[3]: https://arxiv.org/pdf/2111.03930v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper proposes a novel method called **Tip-Adapter** for improving the few-shot classification performance of **CLIP**, a contrastive vision-language pre-training model that learns visual representations from large-scale image-text pairs.
- **Why**: The paper aims to address the limitations of **CLIP-Adapter**, a previous method that fine-tunes a lightweight residual feature adapter on top of CLIP, but requires extra training and computational resources. The paper also wants to leverage the training-free and zero-shot advantages of CLIP for few-shot learning.
- **How**: The paper introduces **Tip-Adapter**, which does not require any back propagation for training the adapter, but creates the weights by a key-value cache model constructed from the few-shot training set. This is a non-parametric way of acquiring well-performed adapter weights without any training, which is both efficient and effective. The paper also shows that the performance of Tip-Adapter can be further boosted by fine-tuning the adapter for only a few epochs with super-fast convergence speed. The paper conducts extensive experiments on ImageNet and other 10 datasets to demonstrate the superiority of Tip-Adapter over CLIP-Adapter and other baselines.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel method called **Tip-Adapter** that inherits CLIP's training-free and zero-shot advantages and performs comparably or even better than CLIP-Adapter for few-shot classification.
- It introduces a key-value cache model that creates the adapter weights from the few-shot training set without any back propagation, which is both efficient and effective.
- It shows that the performance of Tip-Adapter can be further improved by fine-tuning the adapter for only a few epochs with super-fast convergence speed.
- It conducts extensive experiments on ImageNet and other 10 datasets to demonstrate the superiority of Tip-Adapter over CLIP-Adapter and other baselines.

## Method Summary

Here is a summary of the method section of the paper:

- The paper first reviews the CLIP model, which consists of a vision encoder and a text encoder that are trained to maximize the similarity between image-text pairs from a large-scale dataset. The paper also reviews the CLIP-Adapter model, which adds a residual feature adapter to the vision encoder and fine-tunes it on a few-shot training set to improve the classification performance.
- The paper then introduces the Tip-Adapter model, which does not require any fine-tuning of the adapter, but creates the adapter weights by a key-value cache model. The key-value cache model is constructed from the few-shot training set by using the CLIP vision encoder as the key extractor and a linear projection layer as the value extractor. The adapter weights are then computed by a weighted sum of the values based on the similarity between the keys and the input features.
- The paper also shows that the Tip-Adapter model can be further improved by fine-tuning the adapter for only a few epochs with super-fast convergence speed. The paper argues that this is because the Tip-Adapter model provides a good initialization for the adapter weights that are close to the optimal solution. The paper also discusses some implementation details and ablation studies of the Tip-Adapter model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load the pre-trained CLIP model
clip = load_clip_model()

# Load the few-shot training set
train_set = load_train_set()

# Construct the key-value cache model from the train set
cache = Key_Value_Cache()
for image, label in train_set:
  # Extract the key from the image using the CLIP vision encoder
  key = clip.vision_encoder(image)
  # Extract the value from the label using a linear projection layer
  value = linear_projection(label)
  # Store the key-value pair in the cache
  cache.store(key, value)

# Define the Tip-Adapter model
tip_adapter = Tip_Adapter()
for input in inputs:
  # Extract the input features using the CLIP vision encoder
  input_features = clip.vision_encoder(input)
  # Compute the adapter weights by a weighted sum of the values based on the similarity between the keys and the input features
  adapter_weights = cache.weighted_sum(input_features)
  # Apply the adapter weights to the input features
  output_features = tip_adapter.apply(input_features, adapter_weights)
  # Compute the logits using the CLIP text encoder and a linear classifier layer
  logits = linear_classifier(clip.text_encoder(output_features))
  # Return the logits
  return logits

# Optionally, fine-tune the Tip-Adapter model for a few epochs
tip_adapter.fine_tune(train_set)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip

# Load the pre-trained CLIP model
clip_model = clip.load("ViT-B/32", jit=False)

# Load the few-shot training set
train_set = torchvision.datasets.ImageFolder("train_data")
train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)

# Define the number of classes and the dimension of the adapter
num_classes = len(train_set.classes)
adapter_dim = 512

# Define the linear projection layer for the value extractor
value_layer = torch.nn.Linear(num_classes, adapter_dim)

# Define the linear classifier layer for the logits
classifier_layer = torch.nn.Linear(clip_model.text_projection.shape[0], num_classes)

# Construct the key-value cache model from the train set
cache = {}
for images, labels in train_loader:
  # Move the images and labels to the device
  images = images.to(device)
  labels = labels.to(device)
  # Extract the keys from the images using the CLIP vision encoder
  keys = clip_model.encode_image(images)
  # Extract the values from the labels using the linear projection layer
  values = value_layer(torch.nn.functional.one_hot(labels, num_classes))
  # Store the key-value pairs in the cache
  for i in range(len(keys)):
    cache[keys[i]] = values[i]

# Define the Tip-Adapter model
class Tip_Adapter(torch.nn.Module):
  def __init__(self):
    super(Tip_Adapter, self).__init__()
    # Define the adapter layer as a residual connection with a linear transformation
    self.adapter_layer = torch.nn.Sequential(
      torch.nn.Linear(adapter_dim, adapter_dim),
      torch.nn.ReLU(),
      torch.nn.Linear(adapter_dim, adapter_dim)
    )

  def forward(self, input_features):
    # Compute the adapter weights by a weighted sum of the values based on the similarity between the keys and the input features
    similarities = torch.matmul(input_features, torch.stack(list(cache.keys())).T) / input_features.shape[-1]**0.5
    weights = torch.matmul(similarities, torch.stack(list(cache.values())))
    # Apply the adapter weights to the input features
    output_features = input_features + self.adapter_layer(weights)
    # Return the output features
    return output_features

# Instantiate the Tip-Adapter model and move it to the device
tip_adapter = Tip_Adapter().to(device)

# Define the loss function and the optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(tip_adapter.parameters(), lr=0.001)

# Optionally, fine-tune the Tip-Adapter model for a few epochs
num_epochs = 5
for epoch in range(num_epochs):
  # Set the model to training mode
  tip_adapter.train()
  # Loop over the train loader
  for images, labels in train_loader:
    # Move the images and labels to the device
    images = images.to(device)
    labels = labels.to(device)
    # Extract the input features using the CLIP vision encoder
    input_features = clip_model.encode_image(images)
    # Pass the input features through the Tip-Adapter model
    output_features = tip_adapter(input_features)
    # Compute the logits using the CLIP text encoder and a linear classifier layer
    logits = classifier_layer(clip_model.encode_text(output_features))
    # Compute the loss
    loss = criterion(logits, labels)
    # Zero out the gradients
    optimizer.zero_grad()
    # Backpropagate the loss
    loss.backward()
    # Update the parameters
    optimizer.step()
  # Print out the epoch and loss information
  print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```