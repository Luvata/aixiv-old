---
title: 2306.16928v1 One-2-3-45  Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization
date: 2023-06-17
---

# [One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization](http://arxiv.org/abs/2306.16928v1)

authors: Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, Hao Su


## What, Why and How

[1]: https://arxiv.org/abs/2306.16928 "[2306.16928] One-2-3-45: Any Single Image to 3D Mesh in 45 ... - arXiv.org"
[2]: https://arxiv.org/abs/2203.16928v1 "Neural Architecture Search for Speech Emotion Recognition"
[3]: http://export.arxiv.org/abs/2306.16928 "[2306.16928] One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds ..."

Here is a summary of the paper:

```
What: The paper proposes a novel method for single image 3D reconstruction that can generate a full 360-degree 3D textured mesh in 45 seconds without per-shape optimization.

Why: The paper aims to overcome the limitations of existing methods that rely on costly optimizations, produce 3D inconsistent results, and have poor geometry.

How: The paper uses a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then lifts them up to 3D space using an SDF-based neural surface reconstruction method with several training strategies.
```

## Main Contributions

[1]: https://arxiv.org/abs/2306.16928 "[2306.16928] One-2-3-45: Any Single Image to 3D Mesh in 45 ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.16997v1 "arXiv:2306.16997v1 [cs.CV] 29 Jun 2023"
[3]: https://arxiv.org/pdf/2306.16098v1.pdf "arXiv:2306.16098v1 [eess.IV] 28 Jun 2023"

Here are some of the contributions of the paper:

- A novel method for single image 3D reconstruction that can generate a full 360-degree 3D textured mesh in 45 seconds without per-shape optimization
- A view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view
- An SDF-based neural surface reconstruction method with several training strategies to enable the reconstruction of 360-degree meshes
- A demonstration of the superiority of the proposed method in terms of both mesh quality and runtime on both synthetic data and in-the-wild images
- A seamless support for the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models

## Method Summary

[1]: https://arxiv.org/abs/2306.16928 "[2306.16928] One-2-3-45: Any Single Image to 3D Mesh in 45 ... - arXiv.org"
[2]: https://arxiv.org/abs/2305.16928 "[2305.16928] Measurements of the azimuthal anisotropy of ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.16928 "[2306.16928] One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds ..."

Here is a summary of the method section:

```
The method consists of two main components: a 2D diffusion model and a 3D reconstruction module. The 2D diffusion model, Zero123, takes a single image as input and generates multi-view images for the input view by conditioning on the camera pose. The 3D reconstruction module takes the multi-view images as input and reconstructs a 3D mesh using an SDF-based neural surface reconstruction method. The method uses several training strategies to improve the quality and consistency of the 3D mesh, such as view consistency loss, texture consistency loss, and mesh regularization loss. The method can also handle text-to-3D task by replacing the input image with a text-to-image diffusion model output.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.16928 "[2306.16928] One-2-3-45: Any Single Image to 3D Mesh in 45 ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.16928v1.pdf "ABSTRACT arXiv:2203.16928v1 [cs.SD] 31 Mar 2022"
[3]: https://arxiv.org/abs/2303.16928 "Title: MadMax electrodynamics: Solutions and basic properties - arXiv.org"

Here is a possible pseudo code for the paper:

```
# Input: a single image I
# Output: a 3D textured mesh M

# Define the 2D diffusion model Zero123
Zero123 = DiffusionModel()

# Define the 3D reconstruction module SDFNet
SDFNet = SDFReconstruction()

# Generate multi-view images for the input image
multi_view_images = Zero123(I)

# Reconstruct a 3D mesh from the multi-view images
M = SDFNet(multi_view_images)

# Return the 3D mesh
return M
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2203.16928v1.pdf "ABSTRACT arXiv:2203.16928v1 [cs.SD] 31 Mar 2022"
[2]: https://arxiv.org/pdf/2303.16928v1 "arXiv:2303.16928v1 [gr-qc] 29 Mar 2023"
[3]: https://arxiv.org/abs/2203.16928 "Neural Architecture Search for Speech Emotion Recognition"

Here is a possible detailed pseudo code for the paper:

```
# Input: a single image I
# Output: a 3D textured mesh M

# Define the hyperparameters
num_views = 8 # number of views to generate
num_channels = 3 # number of channels in images
num_layers = 4 # number of layers in diffusion model
num_filters = 64 # number of filters in diffusion model
num_epochs = 100 # number of epochs for training
batch_size = 32 # batch size for training
learning_rate = 0.001 # learning rate for training
sdf_size = 256 # size of SDF grid
sdf_samples = 10000 # number of SDF samples per batch

# Define the loss functions
view_consistency_loss = L1Loss() # loss for view consistency
texture_consistency_loss = L1Loss() # loss for texture consistency
mesh_regularization_loss = L2Loss() # loss for mesh regularization

# Define the optimizer
optimizer = AdamOptimizer(learning_rate)

# Define the 2D diffusion model Zero123
Zero123 = DiffusionModel(num_channels, num_layers, num_filters)

# Define the 3D reconstruction module SDFNet
SDFNet = SDFReconstruction(sdf_size, sdf_samples)

# Load the training data
data_loader = DataLoader(batch_size)

# Train the model
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the input image and the ground truth mesh
    I, M_gt = batch

    # Generate multi-view images for the input image
    multi_view_images = Zero123(I, num_views)

    # Reconstruct a 3D mesh from the multi-view images
    M_pred = SDFNet(multi_view_images)

    # Compute the view consistency loss
    L_view = view_consistency_loss(multi_view_images, I)

    # Compute the texture consistency loss
    L_texture = texture_consistency_loss(M_pred.texture, M_gt.texture)

    # Compute the mesh regularization loss
    L_mesh = mesh_regularization_loss(M_pred.vertices, M_gt.vertices)

    # Compute the total loss
    L_total = L_view + L_texture + L_mesh

    # Update the model parameters
    optimizer.zero_grad()
    L_total.backward()
    optimizer.step()

# Save the model
save_model(Zero123, SDFNet)

# Test the model on a new image I_test
I_test = load_image()
multi_view_images_test = Zero123(I_test, num_views)
M_test = SDFNet(multi_view_images_test)
return M_test
```