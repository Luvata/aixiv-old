---
title: 1910.01442v2 CLEVRER  CoLlision Events for Video REpresentation and Reasoning
date: 2019-10-02
---

# [CLEVRER: CoLlision Events for Video REpresentation and Reasoning](http://arxiv.org/abs/1910.01442v2)

authors: Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum


## What, Why and How

[1]: https://arxiv.org/abs/1910.01442v2 "CLEVRER: CoLlision Events for Video REpresentation and Reasoning"
[2]: https://arxiv.org/pdf/1910.01442v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1912.01442v2 "[1912.01442v2] Revealing Roaming on the Double Morse Potential Energy ..."

Here is a summary of the paper:

- **What**: The paper introduces a new video dataset called CLEVRER, which stands for CoLlision Events for Video REpresentation and Reasoning. The dataset contains synthetic videos of objects with simple visual appearance and various types of questions that require reasoning about temporal and causal events.
- **Why**: The paper aims to explore the temporal and causal structures behind videos of objects, which is a core aspect of human intelligence. The paper argues that most video reasoning benchmarks focus on pattern recognition from complex visual and language input, instead of on causal structure. The paper also wants to evaluate different models for visual reasoning on their dataset and identify the challenges and limitations of current approaches.
- **How**: The paper generates the videos using a physics engine and a rendering engine, and annotates them with four types of questions: descriptive (e.g., "what color"), explanatory (e.g., "what is responsible for"), predictive (e.g., "what will happen next"), and counterfactual (e.g., "what if"). The paper evaluates various state-of-the-art models for visual reasoning on their benchmark, such as neural module networks, relational networks, graph neural networks, and memory networks. The paper also studies an oracle model that explicitly combines perception and causal reasoning via symbolic representations. The paper reports the results and analysis of the models on different types of questions and reasoning skills.

You can find more details about the paper at [^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/1910.01442 "CLEVRER: CoLlision Events for Video REpresentation and Reasoning"
[2]: https://arxiv.org/pdf/1910.01442v1.pdf "arXiv:1910.01442v1 [cs.CV] 3 Oct 2019"
[3]: https://mitibmwatsonailab.mit.edu/research/blog/clevrer-the-first-video-dataset-for-neuro-symbolic-reasoning/ "CLEVRER: The first video dataset for neuro-symbolic reasoning"

According to the paper and the related sources, some of the contributions of this paper are:

- It introduces a new video dataset called CLEVRER, which is the first video dataset for neuro-symbolic reasoning[^1^][3].
- It covers a wide range of reasoning tasks, including descriptive, explanatory, predictive, and counterfactual questions, which are motivated by the theory of human causal judgment[^2^][1] [^3^][2].
- It provides a systematic evaluation of various state-of-the-art models for visual reasoning on their dataset, and identifies the challenges and limitations of current approaches[^2^][1] [^3^][2].
- It studies an oracle model that explicitly combines perception and causal reasoning via symbolic representations, and shows that it outperforms other models on the causal tasks[^2^][1] [^3^][2].

## Method Summary

[1]: https://www.scribbr.com/apa-style/methods-section/ "How to Write an APA Methods Section | With Examples - Scribbr"
[2]: https://pubmed.ncbi.nlm.nih.gov/15447808/ "How to write the methods section of a research paper - PubMed"

Here is a summary of the method section of the paper:

- The method section of the paper describes how the authors generated and annotated the CLEVRER dataset, and how they evaluated different models for visual reasoning on their dataset[^1^][1] [^2^][2].
- The authors used a physics engine and a rendering engine to create synthetic videos of objects with simple visual appearance and various collision events[^1^][1] [^2^][2].
- The authors annotated each video with four types of questions: descriptive, explanatory, predictive, and counterfactual, which require different levels of reasoning skills[^1^][1] [^2^][2].
- The authors also provided ground-truth answers, explanations, and scene graphs for each question[^1^][1] [^2^][2].
- The authors evaluated various state-of-the-art models for visual reasoning on their dataset, such as neural module networks, relational networks, graph neural networks, and memory networks[^1^][1] [^2^][2].
- The authors also studied an oracle model that explicitly combines perception and causal reasoning via symbolic representations[^1^][1] [^2^][2].
- The authors reported the results and analysis of the models on different types of questions and reasoning skills[^1^][1] [^2^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Generate videos
Initialize a physics engine and a rendering engine
For each video:
  Randomly sample the number and properties of objects
  Randomly sample the initial positions and velocities of objects
  Simulate the motion and collision of objects using the physics engine
  Render the video frames using the rendering engine
  Save the video and the metadata

# Annotate questions
For each video:
  For each question type (descriptive, explanatory, predictive, counterfactual):
    Randomly sample a question template
    Fill in the template with relevant information from the metadata
    Generate the answer, explanation, and scene graph for the question
    Save the question, answer, explanation, and scene graph

# Evaluate models
For each model:
  Train the model on a subset of videos and questions
  Test the model on a different subset of videos and questions
  Compute the accuracy and explanation quality for each question type and reasoning skill
  Compare the model with other models and an oracle model
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Generate videos
Import a physics engine (e.g., PyBullet) and a rendering engine (e.g., Blender)
Define the object types (e.g., sphere, cube, cylinder) and properties (e.g., color, size, mass)
Define the scene parameters (e.g., camera position, lighting, background)
Define the simulation parameters (e.g., time step, gravity, friction)
Initialize an empty list of videos and metadata
For i = 1 to N: # N is the number of videos to generate
  Initialize an empty dictionary for metadata
  Randomly sample the number of objects (n) from a range (e.g., 3 to 5)
  Initialize an empty list of objects
  For j = 1 to n:
    Randomly sample an object type from the object types
    Randomly sample an object color from the object colors
    Randomly sample an object size from a range (e.g., 0.5 to 1.5)
    Randomly sample an object mass from a range (e.g., 0.5 to 1.5)
    Create an object with the sampled type, color, size, and mass using the physics engine
    Randomly sample an initial position for the object within the scene boundaries
    Randomly sample an initial velocity for the object within a range (e.g., -5 to 5)
    Set the position and velocity of the object using the physics engine
    Add the object to the list of objects
    Add the object properties and initial state to the metadata
  Initialize an empty list of frames
  For t = 1 to T: # T is the number of frames per video (e.g., 100)
    Step the simulation using the physics engine
    For each object in the list of objects:
      Get the current position and velocity of the object using the physics engine
      Add the current state of the object to the metadata
    Render the scene using the rendering engine
    Get the image of the rendered frame
    Add the image to the list of frames
  Concatenate the frames into a video
  Add the video and the metadata to the list of videos and metadata

# Annotate questions
Import a natural language processing library (e.g., NLTK) and a scene graph library (e.g., Scene Graph Parser)
Define the question types (descriptive, explanatory, predictive, counterfactual) and subtypes (e.g., color, count, collision, responsible, next-to)
Define the question templates for each question type and subtype (e.g., "What color is {object}?", "How many {objects} are there?", "Did {object1} collide with {object2}?", "What caused {object} to move?", "What will happen to {object} in the next frame?", "What if {object} had a different initial velocity?")
Initialize an empty list of questions and answers
For each video and metadata in the list of videos and metadata:
  For each question type in the question types:
    For each question subtype in the question subtypes:
      Randomly sample a question template for the question type and subtype
      Fill in the template with relevant information from the metadata (e.g., object names, colors, counts, states)
      Generate the answer for the question based on the metadata and logic rules (e.g., color matching, counting, collision detection, causal attribution, future prediction, counterfactual simulation)
      Generate a natural language explanation for the answer based on templates and logic rules (e.g., "The {object} is {color} because it was initialized with that color.", "There are {number} {objects} because they were randomly sampled.", "Yes, {object1} collided with {object2} because their positions overlapped at frame {number}.", "The {object} moved because it was hit by another object.", "The {object} will stop moving in the next frame because it will reach the boundary of the scene.", "If {object} had a different initial velocity, it would have moved in a different direction.")
      Generate a scene graph for the question and answer using the scene graph library (e.g., a graph with nodes for objects and edges for relations)
      Save the question, answer, explanation, and scene graph to the list of questions and answers

# Evaluate models
Import a machine learning library (e.g., PyTorch) and a model zoo (e.g., Hugging Face Transformers)
Define or import different models for visual reasoning (e.g., neural module networks, relational networks, graph neural networks, memory networks)
Define or import an oracle model that explicitly combines perception and causal reasoning via symbolic representations
Split the list of videos and questions and answers into train, validation, and test sets
For each model in the models:
  Train the model on the train set using a loss function (e.g., cross-entropy) and an optimizer (e.g., Adam)
  Validate the model on the validation set using a metric (e.g., accuracy)
  Test the model on the test set using the same metric
  Compute the accuracy and explanation quality for each question type and subtype, and for each reasoning skill (e.g., query, compare, calculate, exist, count, spatial, temporal, causal)
  Compare the model with other models and the oracle model using statistical tests (e.g., t-test)
```