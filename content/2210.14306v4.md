---
title: 2210.14306v4 Reading Between the Lines  Modeling User Behavior and Costs in AI-Assisted Programming
date: 2022-10-15
---

# [Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming](http://arxiv.org/abs/2210.14306v4)

authors: Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz


## What, Why and How

[1]: https://arxiv.org/pdf/2210.14306v4.pdf "Reading Between the Lines: Modeling User Behavior and Costs in AI ..."
[2]: https://arxiv.org/abs/2210.14306 "[2210.14306] Reading Between the Lines: Modeling User Behavior and ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2210.14306 "[2210.14306] Reading Between the Lines: Modeling User Behavior and ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a study of how programmers interact with GitHub Copilot, a code-recommendation system based on large language models, and introduces CUPS, a taxonomy of common programmer activities when using such systems.
- **Why**: The paper aims to understand the behavior and costs of programmers when using code-recommendation systems, and to inform future interventions to improve AI-assisted programming and human-AI interaction.
- **How**: The paper conducts a study with 21 programmers who complete coding tasks and retrospectively label their sessions with CUPS. The paper analyzes over 3000 label instances, and visualizes the results with timelines and state machines to profile programmer-CodeRec interaction. The paper also discusses novel insights and implications derived from the analysis.



## Main Contributions

[1]: https://arxiv.org/pdf/2210.14306v4.pdf "Reading Between the Lines: Modeling User Behavior and Costs in AI ..."
[2]: https://arxiv.org/abs/2210.14306 "[2210.14306] Reading Between the Lines: Modeling User Behavior and ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2210.14306 "[2210.14306] Reading Between the Lines: Modeling User Behavior and ..."

According to the paper[^1^][1], the main contributions are:

- **CUPS**: A taxonomy of 12 programmer activities common to AI code completion systems, such as Writing New Functionality, Editing Written Code, Debugging/Testing Code, etc.
- **Labeling Tool**: A tool that allows programmers to retrospectively label their coding sessions with CUPS, and provides visual feedback and statistics on their labels.
- **Study**: A study with 21 programmers who completed coding tasks and used the labeling tool to annotate their sessions with CUPS. The study collected over 3000 label instances and 67 code suggestions.
- **Analysis**: An analysis of the labeled data using timelines and state machines to profile programmer-CodeRec interaction, revealing the distribution and patterns of programmer behavior, as well as inefficiencies and time costs.
- **Insights and Implications**: A discussion of novel insights and implications derived from the analysis, such as the trade-offs between suggestion quality and programmer effort, the impact of suggestion frequency and diversity on programmer behavior, and the need for new interface designs and metrics for AI-assisted programming.

## Method Summary

[1]: https://arxiv.org/pdf/2210.14306v4.pdf "Reading Between the Lines: Modeling User Behavior and Costs in AI ..."
[2]: https://arxiv.org/abs/2210.14306 "[2210.14306] Reading Between the Lines: Modeling User Behavior and ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2210.14306 "[2210.14306] Reading Between the Lines: Modeling User Behavior and ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the design and implementation of CUPS, a taxonomy of 12 programmer activities common to AI code completion systems, such as Writing New Functionality, Editing Written Code, Debugging/Testing Code, etc. The paper explains the rationale and criteria for each activity, and provides examples and guidelines for labeling them.
- The paper also describes the development of a labeling tool that allows programmers to retrospectively label their coding sessions with CUPS, and provides visual feedback and statistics on their labels. The paper discusses the features and limitations of the tool, and how it can be used for future studies and evaluations.
- The paper reports on a study with 21 programmers who completed coding tasks and used the labeling tool to annotate their sessions with CUPS. The paper details the recruitment process, the task design, the data collection procedure, and the data quality checks.
- The paper presents an analysis of the labeled data using timelines and state machines to profile programmer-CodeRec interaction. The paper describes the metrics and methods used for the analysis, such as transition probabilities, dwell times, suggestion acceptance rates, etc. The paper also provides visualizations and tables to illustrate the results.


## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```
# Define CUPS taxonomy of 12 programmer activities
CUPS = ["Writing New Functionality", "Editing Written Code", "Debugging/Testing Code", ...]

# Develop labeling tool that allows programmers to label their sessions with CUPS
labeling_tool = create_labeling_tool(CUPS)

# Recruit 21 programmers and assign them coding tasks
programmers = recruit_programmers(21)
tasks = create_tasks()

# Collect coding sessions and labels from programmers using GitHub Copilot and labeling tool
sessions = []
labels = []
for programmer in programmers:
  for task in tasks:
    session = record_session(programmer, task, GitHub_Copilot)
    label = get_label(programmer, session, labeling_tool)
    sessions.append(session)
    labels.append(label)

# Analyze labeled data using timelines and state machines
metrics = ["transition probabilities", "dwell times", "suggestion acceptance rates", ...]
methods = ["timelines", "state machines", ...]
results = analyze_data(sessions, labels, metrics, methods)

# Visualize and discuss results
visualize_results(results)
discuss_insights_and_implications(results)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```
# Define CUPS taxonomy of 12 programmer activities
CUPS = ["Writing New Functionality", "Editing Written Code", "Debugging/Testing Code", ...]

# Develop labeling tool that allows programmers to label their sessions with CUPS
labeling_tool = create_labeling_tool(CUPS)
  # Create a GUI that shows the coding session as a video with playback controls
  # Create a panel that shows the CUPS categories and their descriptions
  # Allow the programmer to select a time range and assign a CUPS label to it
  # Validate the labels and prevent overlaps or gaps
  # Provide visual feedback and statistics on the labels, such as histograms and pie charts
  # Save the labels as a CSV file with columns for session ID, start time, end time, and CUPS label

# Recruit 21 programmers and assign them coding tasks
programmers = recruit_programmers(21)
  # Use online platforms or mailing lists to advertise the study and recruit volunteers
  # Screen the volunteers based on their programming experience and familiarity with GitHub Copilot
  # Obtain informed consent and demographic information from the selected participants
tasks = create_tasks()
  # Choose tasks that cover different programming languages and domains
  # Design tasks that are moderately complex and require multiple steps to complete
  # Provide task descriptions and instructions in a document or a webpage

# Collect coding sessions and labels from programmers using GitHub Copilot and labeling tool
sessions = []
labels = []
for programmer in programmers:
  for task in tasks:
    session = record_session(programmer, task, GitHub_Copilot)
      # Ask the programmer to use Visual Studio Code with GitHub Copilot extension
      # Ask the programmer to complete the task within a time limit (e.g., 30 minutes)
      # Record the screen, keyboard, mouse, and audio of the programmer using a screen capture software
      # Save the session as a video file with a unique ID
    label = get_label(programmer, session, labeling_tool)
      # Ask the programmer to use the labeling tool to annotate their session with CUPS
      # Provide guidance and examples on how to use the tool and apply the labels
      # Allow the programmer to review and edit their labels before submitting them
    sessions.append(session)
    labels.append(label)

# Analyze labeled data using timelines and state machines
metrics = ["transition probabilities", "dwell times", "suggestion acceptance rates", ...]
methods = ["timelines", "state machines", ...]
results = analyze_data(sessions, labels, metrics, methods)
  # Load the sessions and labels from the video and CSV files
  # Extract features from the sessions, such as code edits, suggestions shown, suggestions accepted, etc.
  # Compute metrics from the labels, such as transition probabilities between CUPS states, dwell times in each state, suggestion acceptance rates in each state, etc.
  # Apply methods to visualize and summarize the data, such as timelines that show the sequence of CUPS states and suggestions for each session, state machines that show the transitions between CUPS states and their probabilities for each programmer or task, etc.

# Visualize and discuss results
visualize_results(results)
  # Plot histograms, pie charts, box plots, etc. to show the distribution of metrics across programmers or tasks
  # Plot timelines and state machines to show the patterns of programmer behavior and interaction with GitHub Copilot
  # Highlight interesting or surprising findings from the plots, such as outliers or trends
discuss_insights_and_implications(results)
  # Interpret the results in terms of how they reflect or affect programmer productivity and satisfaction
  # Identify inefficiencies and time costs in programmer-CodeRec interaction, such as waiting for suggestions, rejecting suggestions, debugging suggestions, etc.
  # Suggest ways to improve AI-assisted programming and human-AI interaction, such as new interface designs that provide more control or feedback to programmers, new metrics that capture programmer effort or quality of suggestions, etc.
```