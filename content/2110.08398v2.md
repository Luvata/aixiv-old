---
title: 2110.08398v2 Mind the Gap  Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks
date: 2021-10-09
---

# [Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks](http://arxiv.org/abs/2110.08398v2)

authors: Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka


## What, Why and How

[1]: https://arxiv.org/abs/2110.08398v2 "[2110.08398v2] Mind the Gap: Domain Gap Control for Single ... - arXiv.org"
[2]: https://arxiv.org/abs/2210.08398 "[2210.08398] SPIDR: SDF-based Neural Point Fields for ... - arXiv.org"
[3]: https://arxiv.org/pdf/2110.08398 "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: A new method for one shot domain adaptation for generative adversarial networks (GANs) that can translate images from one domain to another using a single reference image and control the domain gap.

Why: To achieve higher visual quality and more degrees of freedom in domain adaptation compared to the current state of the art methods.

How: By building on a pre-trained StyleGAN generator as GAN and a pre-trained CLIP model for representing the domain gap, and proposing several new regularizers for controlling the domain gap to optimize the weights of the pre-trained StyleGAN generator.
```

Source: [^1^][1]

## Main Contributions

[1]: https://arxiv.org/abs/2110.08398v2 "[2110.08398v2] Mind the Gap: Domain Gap Control for Single ... - arXiv.org"
[2]: https://arxiv.org/abs/2210.08398 "[2210.08398] SPIDR: SDF-based Neural Point Fields for ... - arXiv.org"
[3]: https://arxiv.org/pdf/2110.08398 "arXiv.org e-Print archive"

The paper claims to make the following contributions:

- A new method for one shot domain adaptation for GANs that can translate images from one domain to another using a single reference image and control the domain gap.
- A novel use of a pre-trained CLIP model for representing the domain gap and optimizing the weights of a pre-trained StyleGAN generator to output images in the target domain.
- Several new regularizers for controlling the domain gap and preventing overfitting to the reference image.
- Significant visual improvements over the state of the art methods and multiple applications that highlight improved control.

Source: [^1^][1]

## Method Summary

[1]: https://arxiv.org/abs/2110.08398v2 "[2110.08398v2] Mind the Gap: Domain Gap Control for Single ... - arXiv.org"
[2]: https://arxiv.org/abs/2210.08398 "[2210.08398] SPIDR: SDF-based Neural Point Fields for ... - arXiv.org"
[3]: https://arxiv.org/pdf/2110.08398 "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

```
The method consists of three main steps: 

1. Preparing the input data: The input data consists of a trained GAN that can produce images in domain A and a single reference image I_B from domain B. The reference image is resized and cropped to match the resolution of the GAN output. The GAN is assumed to be a StyleGAN generator that takes a latent code z as input and outputs an image x_A in domain A.

2. Defining the domain gap: The domain gap is defined as the difference between the distributions of images in domain A and domain B. The domain gap is represented by a pre-trained CLIP model that can encode both images and text into a common embedding space. The CLIP model is used to compute the similarity between x_A and I_B, as well as between x_A and a text description T_B of domain B. The text description T_B is manually provided by the user or automatically generated from I_B using a captioning model.

3. Optimizing the GAN weights: The goal is to optimize the weights of the StyleGAN generator such that it can output an image x_B in domain B given any latent code z. The optimization problem is formulated as minimizing a loss function that consists of four terms: 

- A CLIP loss that measures the similarity between x_B and I_B, as well as between x_B and T_B, using the CLIP model.
- A perceptual loss that measures the feature distance between x_B and I_B using a pre-trained VGG network.
- A style loss that measures the style distance between x_B and I_B using a pre-trained VGG network.
- A regularization loss that prevents overfitting to I_B by penalizing large changes in the GAN weights, large changes in the latent code z, and large changes in the image statistics.

The optimization problem is solved using gradient descent with Adam optimizer.
```

Source: [^1^][1]

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2110.08398v2 "[2110.08398v2] Mind the Gap: Domain Gap Control for Single ... - arXiv.org"
[2]: https://arxiv.org/abs/2210.08398 "[2210.08398] SPIDR: SDF-based Neural Point Fields for ... - arXiv.org"
[3]: https://arxiv.org/pdf/2110.08398 "arXiv.org e-Print archive"

Here is a high-level pseudo code for the paper:

```
# Input: a trained GAN that can produce images in domain A, a single reference image I_B from domain B, and a text description T_B of domain B
# Output: an optimized GAN that can produce images in domain B

# Load the pre-trained StyleGAN generator G and the pre-trained CLIP model C
G = load_stylegan()
C = load_clip()

# Resize and crop I_B to match the resolution of G
I_B = resize_and_crop(I_B)

# Initialize the GAN weights W and the latent code z randomly
W = random_init()
z = random_init()

# Define the loss function L as a combination of four terms: CLIP loss, perceptual loss, style loss, and regularization loss
L = clip_loss + perceptual_loss + style_loss + regularization_loss

# Define the hyperparameters for the optimization: learning rate, number of iterations, and weight coefficients for each loss term
lr = 0.01
n_iter = 1000
alpha = 0.5 # weight coefficient for CLIP loss
beta = 0.1 # weight coefficient for perceptual loss
gamma = 0.1 # weight coefficient for style loss
delta = 0.01 # weight coefficient for regularization loss

# Optimize the GAN weights W and the latent code z using gradient descent with Adam optimizer
for i in range(n_iter):
  # Generate an image x_A in domain A using G and z
  x_A = G(z)
  
  # Generate an image x_B in domain B using G and W
  x_B = G(W)
  
  # Compute the CLIP loss as the negative similarity between x_B and I_B, as well as between x_B and T_B, using C
  clip_loss = - (C.similarity(x_B, I_B) + C.similarity(x_B, T_B))
  
  # Compute the perceptual loss as the feature distance between x_B and I_B using a pre-trained VGG network
  perceptual_loss = VGG.distance(x_B, I_B)
  
  # Compute the style loss as the style distance between x_B and I_B using a pre-trained VGG network
  style_loss = VGG.style_distance(x_B, I_B)
  
  # Compute the regularization loss as a combination of three terms: weight decay, latent decay, and image statistics decay
  regularization_loss = weight_decay(W) + latent_decay(z) + image_statistics_decay(x_A, x_B)
  
  # Compute the total loss as a weighted sum of the four loss terms
  L = alpha * clip_loss + beta * perceptual_loss + gamma * style_loss + delta * regularization_loss
  
  # Compute the gradients of L with respect to W and z
  dL_dW, dL_dz = gradients(L, W, z)
  
  # Update W and z using Adam optimizer with learning rate lr
  W = adam_update(W, dL_dW, lr)
  z = adam_update(z, dL_dz, lr)

# Return the optimized GAN weights W
return W
```

Source: [^1^][1]


## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2110.08398v2 "[2110.08398v2] Mind the Gap: Domain Gap Control for Single ... - arXiv.org"
[2]: https://arxiv.org/abs/2210.08398 "[2210.08398] SPIDR: SDF-based Neural Point Fields for ... - arXiv.org"
[3]: https://arxiv.org/pdf/2110.08398 "arXiv.org e-Print archive"

Here is a detailed pseudo code for the paper:

```
# Import the necessary libraries
import torch
import torchvision
import clip
import stylegan

# Define the constants and hyperparameters
IMAGE_SIZE = 256 # the resolution of the GAN output and the reference image
LR = 0.01 # the learning rate for the optimization
N_ITER = 1000 # the number of iterations for the optimization
ALPHA = 0.5 # the weight coefficient for CLIP loss
BETA = 0.1 # the weight coefficient for perceptual loss
GAMMA = 0.1 # the weight coefficient for style loss
DELTA = 0.01 # the weight coefficient for regularization loss

# Define the functions for computing each loss term

def clip_loss(x_B, I_B, T_B):
  # Compute the CLIP loss as the negative similarity between x_B and I_B, as well as between x_B and T_B, using C
  
  # Encode x_B and I_B into image embeddings using C
  x_B_emb = C.encode_image(x_B)
  I_B_emb = C.encode_image(I_B)
  
  # Encode T_B into text embedding using C
  T_B_emb = C.encode_text(T_B)
  
  # Compute the cosine similarity between x_B_emb and I_B_emb, as well as between x_B_emb and T_B_emb
  sim_xB_IB = torch.cosine_similarity(x_B_emb, I_B_emb, dim=-1)
  sim_xB_TB = torch.cosine_similarity(x_B_emb, T_B_emb, dim=-1)
  
  # Return the negative sum of sim_xB_IB and sim_xB_TB as the CLIP loss
  return - (sim_xB_IB + sim_xB_TB)

def perceptual_loss(x_B, I_B):
  # Compute the perceptual loss as the feature distance between x_B and I_B using a pre-trained VGG network
  
  # Load a pre-trained VGG network and set it to evaluation mode
  vgg = torchvision.models.vgg16(pretrained=True).eval()
  
  # Extract a list of intermediate layers from vgg
  layers = [vgg.features[i] for i in [3, 8, 15, 22, 29]]
  
  # Define a function to compute the feature maps of an image using vgg and layers
  def get_features(image):
    # Initialize an empty list to store the feature maps
    features = []
    
    # Loop through each layer in layers
    for layer in layers:
      # Apply the layer to the image and store the output as image
      image = layer(image)
      
      # Append image to features
      features.append(image)
    
    # Return features as a list of feature maps
    return features
  
  # Compute the feature maps of x_B and I_B using get_features
  x_B_features = get_features(x_B)
  I_B_features = get_features(I_B)
  
  # Initialize a variable to store the perceptual loss
  loss = 0
  
  # Loop through each pair of feature maps from x_B_features and I_B_features
  for x_f, I_f in zip(x_B_features, I_B_features):
    # Compute the mean squared error between x_f and I_f and add it to loss
    loss += torch.nn.functional.mse_loss(x_f, I_f)
  
  # Return loss as the perceptual loss
  return loss

def style_loss(x_B, I_B):
  # Compute the style loss as the style distance between x_B and I_B using a pre-trained VGG network
  
  # Load a pre-trained VGG network and set it to evaluation mode
  vgg = torchvision.models.vgg16(pretrained=True).eval()
  
  # Extract a list of intermediate layers from vgg
  layers = [vgg.features[i] for i in [3, 8, 15, 22, 29]]
  
   # Define a function to compute the gram matrix of a feature map
   def gram_matrix(feature_map):
     # Reshape feature_map into a matrix of shape (C, H*W) where C is the number of channels and H and W are height and width
     matrix = feature_map.view(feature_map.size(1), -1)
     
     # Compute and return the gram matrix as the matrix multiplication of matrix and its transpose divided by the number of elements in matrix
     return torch.mm(matrix, matrix.t()) / matrix.numel()
  
   # Define a function to compute the style features of an image using vgg and layers
   def get_style_features(image):
     # Initialize an empty list to store the style features
     features = []
     
     # Loop through each layer in layers
     for layer in layers:
       # Apply the layer to the image and store the output as image
       image = layer(image)
       
       # Compute the gram matrix of image and append it to features
       features.append(gram_matrix(image))
     
     # Return features as a list of style features
     return features
  
  # Compute the style features of x_B and I_B using get_style_features
  x_B_features = get_style_features(x_B)
  I_B_features = get_style_features(I_B)
  
  # Initialize a variable to store the style loss
  loss = 0
  
  # Loop through each pair of style features from x_B_features and I_B_features
  for x_f, I_f in zip(x_B_features, I_B_features):
    # Compute the mean squared error between x_f and I_f and add it to loss
    loss += torch.nn.functional.mse_loss(x_f, I_f)
  
  # Return loss as the style loss
  return loss

def regularization_loss(W, z, x_A, x_B):
  # Compute the regularization loss as a combination of three terms: weight decay, latent decay, and image statistics decay
  
  # Compute the weight decay as the L2 norm of W
  weight_decay = torch.norm(W)
  
  # Compute the latent decay as the L2 norm of z
  latent_decay = torch.norm(z)
  
  # Compute the image statistics decay as the sum of three terms: mean difference, standard deviation difference, and histogram difference between x_A and x_B
  
  # Compute the mean difference as the absolute difference between the means of x_A and x_B along the channel dimension
  mean_diff = torch.abs(torch.mean(x_A, dim=1) - torch.mean(x_B, dim=1))
  
  # Compute the standard deviation difference as the absolute difference between the standard deviations of x_A and x_B along the channel dimension
  std_diff = torch.abs(torch.std(x_A, dim=1) - torch.std(x_B, dim=1))
  
  # Compute the histogram difference as the earth mover's distance between the histograms of x_A and x_B along the channel dimension
  hist_diff = torch.histc(x_A, bins=256, dim=1) - torch.histc(x_B, bins=256, dim=1)
  hist_diff = torch.sqrt(torch.sum(hist_diff ** 2, dim=-1))
  
  # Sum up mean_diff, std_diff, and hist_diff to get image_statistics_decay
  image_statistics_decay = mean_diff + std_diff + hist_diff
  
  # Return the sum of weight_decay, latent_decay, and image_statistics_decay as the regularization loss
  return weight_decay + latent_decay + image_statistics_decay

# Define the main function for one shot domain adaptation

def one_shot_domain_adaptation(GAN_A, I_B, T_B):
  
  # Input: a trained GAN that can produce images in domain A, a single reference image I_B from domain B, and a text description T_B of domain B
  # Output: an optimized GAN that can produce images in domain B
  
  # Load the pre-trained StyleGAN generator G and the pre-trained CLIP model C
  G = load_stylegan()
  C = load_clip()
  
  # Resize and crop I_B to match the resolution of G
  I_B = resize_and_crop(I_B)
  
  # Initialize the GAN weights W and the latent code z randomly
  W = random_init()
  z = random_init()
  
   # Define an Adam optimizer for updating W and z with learning rate LR
   optimizer = torch.optim.Adam([W, z], lr=LR)
   
   # Define a loop for N_ITER iterations
   for i in range(N_ITER):
     
     # Generate an image x_A in domain A using G and z
     x_A = G(z)
     
     # Generate an image x_B in domain B using G and W
     x_B = G(W)
     
     # Compute the CLIP loss using clip_loss function
     clip_loss = clip_loss(x_B, I_B, T_B)
     
     # Compute the perceptual loss using perceptual_loss function
     perceptual_loss = perceptual_loss(x_B, I_B)
     
     # Compute the style loss using style_loss function
     style_loss = style_loss(x_B, I_B)
     
     # Compute the regularization loss using regularization_loss function
     regularization_loss = regularization_loss(W, z, x_A, x_B)
     
     # Compute the total loss as a weighted sum of the four loss terms with coefficients ALPHA, BETA, GAMMA, and DELTA
     total_loss = ALPHA * clip_loss + BETA * perceptual_loss + GAMMA * style_loss + DELTA * regularization_loss
     
     # Zero out the gradients of W and z
     optimizer.zero_grad()
     
     # Backpropagate the total loss to compute the gradients