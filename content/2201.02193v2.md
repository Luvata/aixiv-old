---
title: 2201.02193v2 Realistic Full-Body Anonymization with Surface-Guided GANs
date: 2022-01-03
---

# [Realistic Full-Body Anonymization with Surface-Guided GANs](http://arxiv.org/abs/2201.02193v2)

authors: Håkon Hukkelås, Morten Smebye, Rudolf Mester, Frank Lindseth


## What, Why and How

[1]: https://arxiv.org/pdf/2201.02193v2.pdf "arXiv:2201.02193v2 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2201.02193 "Realistic Full-Body Anonymization with Surface-Guided GANs"
[3]: http://export.arxiv.org/abs/2201.02193 "[2201.02193] Realistic Full-Body Anonymization with Surface-Guided GANs"

Here is a summary of the paper:

- **What**: The paper proposes a new method for realistic full-body anonymization with surface-guided generative adversarial networks (GANs) [^1^][1].
- **Why**: The paper aims to address the privacy and data usability challenges of using in-the-wild images for training computer vision models. Existing methods for image anonymization either focus on faces only or do not handle variations in the background [^1^][1].
- **How**: The paper introduces Variational Surface-Adaptive Modulation (V-SAM) that embeds surface information throughout the generator, and a discriminator surface supervision loss that encourages the generator to synthesize high quality humans with diverse appearances [^1^][1]. The paper also uses Continuous Surface Embeddings (CSE)  to guide the generator with pixel-to-surface correspondences between an image and a canonical 3D surface [^1^][1]. The paper evaluates the proposed method on several datasets and metrics, and shows that it outperforms existing methods in terms of image quality, diversity, privacy, and data usability [^1^][1].

## Main Contributions

The paper claims the following contributions:

- A novel method for realistic full-body anonymization with surface-guided GANs that can handle complex and varying scenes .
- A new generator module called V-SAM that modulates the features based on surface information .
- A new discriminator loss that enforces surface consistency between the input and the output .
- A comprehensive evaluation of the proposed method on several datasets and metrics, showing its superiority over existing methods in terms of image quality, diversity, privacy, and data usability .


## Method Summary

The method section of the paper describes the proposed surface-guided GANs for full-body anonymization. The paper first introduces the problem formulation and the notation used in the paper. Then, the paper explains the main components of the proposed method: the generator, the discriminator, and the losses. The generator consists of an encoder-decoder network with skip connections and V-SAM modules that modulate the features based on surface information. The discriminator is a patch-based network that classifies each patch as real or fake, and also predicts the surface coordinates for each pixel. The losses include an adversarial loss, a reconstruction loss, a perceptual loss, a style loss, a diversity loss, and a discriminator surface supervision loss. The paper also discusses some implementation details and training strategies.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image x with humans
# Output: an anonymized image y with realistic humans

# Get the surface coordinates s for each pixel in x using CSE
s = CSE(x)

# Encode x into a latent representation z using an encoder network E
z = E(x)

# Decode z into y using a decoder network D with V-SAM modules
y = D(z, s)

# Classify y as real or fake using a discriminator network D
p = D(y)

# Predict the surface coordinates s' for each pixel in y using D
s' = D(y)

# Define the losses
L_adv = adversarial_loss(p, real_label) # encourage y to be realistic
L_rec = reconstruction_loss(y, x) # encourage y to preserve the background and pose of x
L_per = perceptual_loss(y, x) # encourage y to have similar high-level features as x
L_sty = style_loss(y, x) # encourage y to have similar style statistics as x
L_div = diversity_loss(y, z) # encourage y to be diverse given different z
L_sur = surface_loss(s', s) # encourage y to have consistent surface coordinates as x

# Define the total loss
L_total = L_adv + L_rec + L_per + L_sty + L_div + L_sur

# Update the generator and the discriminator using gradient descent
update_parameters(L_total)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import cv2

# Define the hyperparameters
batch_size = 16 # the number of images in a batch
image_size = 256 # the size of the input and output images
latent_size = 256 # the size of the latent representation
num_blocks = 6 # the number of residual blocks in the encoder and decoder
num_channels = 64 # the number of channels in the first convolutional layer
num_classes = 6890 # the number of surface coordinates in CSE
lambda_adv = 1.0 # the weight for the adversarial loss
lambda_rec = 10.0 # the weight for the reconstruction loss
lambda_per = 10.0 # the weight for the perceptual loss
lambda_sty = 250.0 # the weight for the style loss
lambda_div = 0.1 # the weight for the diversity loss
lambda_sur = 1.0 # the weight for the surface loss
learning_rate = 0.0002 # the learning rate for the optimizer
beta1 = 0.5 # the beta1 parameter for the optimizer
beta2 = 0.999 # the beta2 parameter for the optimizer

# Define the CSE network (pre-trained on SMPL-X dataset)
class CSE(nn.Module):
    def __init__(self):
        super(CSE, self).__init__()
        self.model = models.resnet50(pretrained=True) # use a ResNet-50 backbone
        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes) # replace the last layer with a linear layer
    
    def forward(self, x):
        x = self.model(x) # pass x through the ResNet-50 backbone
        x = torch.sigmoid(x) # apply a sigmoid activation to get surface coordinates in [0, 1]
        return x

# Define the encoder network
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.conv1 = nn.Conv2d(3, num_channels, kernel_size=7, stride=1, padding=3) # a convolutional layer with 7x7 kernel and 3 input channels (RGB)
        self.in1 = nn.InstanceNorm2d(num_channels) # an instance normalization layer
        self.relu = nn.ReLU(inplace=True) # a ReLU activation layer
        self.conv2 = nn.Conv2d(num_channels, num_channels * 2, kernel_size=3, stride=2, padding=1) # a convolutional layer with 3x3 kernel and stride 2 (downsampling)
        self.in2 = nn.InstanceNorm2d(num_channels * 2) # an instance normalization layer
        self.conv3 = nn.Conv2d(num_channels * 2, num_channels * 4, kernel_size=3, stride=2, padding=1) # a convolutional layer with 3x3 kernel and stride 2 (downsampling)
        self.in3 = nn.InstanceNorm2d(num_channels * 4) # an instance normalization layer
        
        self.res_blocks = nn.ModuleList() # a list of residual blocks
        for i in range(num_blocks):
            self.res_blocks.append(ResBlock(num_channels * 4)) # each residual block has num_channels * 4 input and output channels
        
        self.conv4 = nn.Conv2d(num_channels * 4, latent_size, kernel_size=3, stride=1, padding=1) # a convolutional layer with 3x3 kernel and latent_size output channels
    
    def forward(self, x):
        x = self.conv1(x) # pass x through the first convolutional layer
        x = self.in1(x) # apply instance normalization
        x = self.relu(x) # apply ReLU activation
        
        x = self.conv2(x) # pass x through the second convolutional layer
        x = self.in2(x) # apply instance normalization
        x = self.relu(x) # apply ReLU activation
        
        x = self.conv3(x) # pass x through the third convolutional layer
        x = self.in3(x) # apply instance normalization
        x = self.relu(x) # apply ReLU activation
        
        for block in self.res_blocks: # pass x through each residual block
            x = block(x)
        
        x = self.conv4(x) # pass x through the fourth convolutional layer
        
        return x

# Define the residual block
class ResBlock(nn.Module):
    def __init__(self, channels):
        super(ResBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1) # a convolutional layer with 3x3 kernel and same input and output channels
        self.in1 = nn.InstanceNorm2d(channels) # an instance normalization layer
        self.relu = nn.ReLU(inplace=True) # a ReLU activation layer
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1) # a convolutional layer with 3x3 kernel and same input and output channels
        self.in2 = nn.InstanceNorm2d(channels) # an instance normalization layer
    
    def forward(self, x):
        identity = x # save the input as identity
        x = self.conv1(x) # pass x through the first convolutional layer
        x = self.in1(x) # apply instance normalization
        x = self.relu(x) # apply ReLU activation
        x = self.conv2(x) # pass x through the second convolutional layer
        x = self.in2(x) # apply instance normalization
        x = x + identity # add the identity to the output
        return x

# Define the V-SAM module
class VSAM(nn.Module):
    def __init__(self, channels):
        super(VSAM, self).__init__()
        self.fc_gamma = nn.Linear(num_classes, channels) # a linear layer to map surface coordinates to gamma parameters
        self.fc_beta = nn.Linear(num_classes, channels) # a linear layer to map surface coordinates to beta parameters
    
    def forward(self, x, s):
        gamma = self.fc_gamma(s) # get the gamma parameters from surface coordinates
        beta = self.fc_beta(s) # get the beta parameters from surface coordinates
        gamma = gamma.unsqueeze(2).unsqueeze(3) # add two dimensions to match the shape of x
        beta = beta.unsqueeze(2).unsqueeze(3) # add two dimensions to match the shape of x
        x = gamma * x + beta # modulate x with gamma and beta
        return x

# Define the decoder network
class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        
        self.res_blocks = nn.ModuleList() # a list of residual blocks with V-SAM modules
        for i in range(num_blocks):
            self.res_blocks.append(nn.Sequential(
                ResBlock(latent_size), # a residual block with latent_size input and output channels
                VSAM(latent_size) # a V-SAM module with latent_size input and output channels
            ))
        
        self.deconv1 = nn.ConvTranspose2d(latent_size, num_channels * 4, kernel_size=3, stride=1, padding=1) # a transposed convolutional layer with 3x3 kernel and num_channels * 4 output channels
        self.in1 = nn.InstanceNorm2d(num_channels * 4) # an instance normalization layer
        self.relu = nn.ReLU(inplace=True) # a ReLU activation layer
        
        self.deconv2 = nn.ConvTranspose2d(num_channels * 4, num_channels * 2, kernel_size=3, stride=2, padding=1, output_padding=1) # a transposed convolutional layer with 3x3 kernel and stride 2 (upsampling)
        self.in2 = nn.InstanceNorm2d(num_channels * 2) # an instance normalization layer
        
        self.deconv3 = nn.ConvTranspose2d(num_channels * 2, num_channels, kernel_size=3, stride=2, padding=1, output_padding=1) # a transposed convolutional layer with 3x3 kernel and stride 2 (upsampling)
        self.in3 = nn.InstanceNorm2d(num_channels) # an instance normalization layer
        
        self.deconv4 = nn.ConvTranspose2d(num_channels, 3, kernel_size=7, stride=1, padding=3) # a transposed convolutional layer with 7x7 kernel and 3 output channels (RGB)
    
    def forward(self, z, s):
        
        for block in self.res_blocks: # pass z through each residual block with V-SAM module
            z = block(z, s)
        
        z = self.deconv1(z) # pass z through the first transposed convolutional layer
        z = self.in1(z) # apply instance normalization
        z = self.relu(z) # apply ReLU activation
        
        z = self.deconv2(z) # pass z through the second transposed convolutional layer
        z = self.in2(z) # apply instance normalization
        z = self.relu(z)