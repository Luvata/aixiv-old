---
title: 1511.02799v4 Neural Module Networks
date: 2015-11-03
---

# [Neural Module Networks](http://arxiv.org/abs/1511.02799v4)

authors: Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein


## What, Why and How

[1]: https://arxiv.org/pdf/1511.02799v4.pdf "Deep Compositional Question Answering with Neural Module Networks"
[2]: https://arxiv.org/abs/1511.02799 "[1511.02799] Neural Module Networks - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/1511.02799 "[1511.02799] Neural Module Networks"

Here is a summary of the paper:

- **What**: The paper proposes a method for visual question answering using neural module networks, which are deep networks composed of reusable neural modules that are dynamically instantiated based on the linguistic structure of the question.
- **Why**: The paper aims to exploit both the representational capacity of deep networks and the compositional nature of natural language questions, which can share substructures and require different types of reasoning.
- **How**: The paper describes a procedure for constructing and learning neural module networks, which involves parsing the question into a layout of modules, selecting appropriate modules for each layout element, and jointly training the modules and the parser. The paper evaluates the method on two datasets: VQA natural image dataset and a new dataset of complex questions about abstract shapes. The paper reports state-of-the-art results on both datasets.

## Main Contributions

Some of the contributions of this paper are:

- It introduces the concept of neural module networks, which are deep networks composed of reusable neural modules that can be dynamically assembled based on the linguistic structure of the question.
- It presents a method for constructing and learning neural module networks, which involves parsing the question into a layout of modules, selecting appropriate modules for each layout element, and jointly training the modules and the parser.
- It evaluates the method on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.
- It demonstrates that neural module networks can perform various types of reasoning, such as counting, comparing, locating, and identifying objects and attributes in images.

## Method Summary

[1]: https://arxiv.org/pdf/1511.02799v4.pdf "Deep Compositional Question Answering with Neural Module Networks"
[2]: https://arxiv.org/abs/1511.02799 "[1511.02799] Neural Module Networks - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/1511.02799 "[1511.02799] Neural Module Networks"

Here is a summary of the method section of the paper:

- The method section describes how to construct and learn neural module networks for visual question answering.
- The method consists of three main steps: parsing the question into a layout of modules, selecting appropriate modules for each layout element, and jointly training the modules and the parser.
- The parsing step uses a dependency parser to analyze the syntactic structure of the question and map it to a layout of modules. The layout specifies the type and order of modules to be composed into a network. For example, the question "where is the dog?" is parsed into a layout of "find" followed by "locate".
- The module selection step chooses a specific module for each layout element based on the question word and the image features. The module is a neural network that performs a specific function, such as finding objects, classifying colors, counting numbers, etc. The module selection is done by a softmax classifier that takes as input the question word embedding and the image features extracted by a convolutional neural network (CNN).
- The training step optimizes the parameters of the modules and the parser jointly using backpropagation and stochastic gradient descent. The objective function is the cross-entropy loss between the predicted answer and the ground-truth answer. The paper also introduces a regularization term that encourages the modules to be reusable across different questions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image I and a question Q
# Output: an answer A

# Extract image features using a CNN
F = CNN(I)

# Parse the question into a layout of modules using a dependency parser
L = parse(Q)

# Initialize an empty list of modules
M = []

# For each layout element in L
for l in L:
  # Select a module for l based on the question word and the image features
  m = select_module(l, Q, F)
  # Append m to M
  M.append(m)

# Compose the modules in M into a network N
N = compose(M)

# Forward propagate the image features F through N to get the answer A
A = N(F)

# Return A
return A
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision.models as models
import spacy

# Define the hyperparameters
num_modules = 10 # the number of modules to choose from
num_answers = 1000 # the number of possible answers
embed_size = 300 # the size of word embeddings
hidden_size = 512 # the size of hidden states in LSTM and modules
reg_lambda = 0.1 # the regularization coefficient

# Load the pretrained CNN model (ResNet-152)
cnn = models.resnet152(pretrained=True)
# Remove the last fully connected layer
cnn.fc = nn.Identity()
# Freeze the parameters of the CNN
for param in cnn.parameters():
  param.requires_grad = False

# Load the pretrained word embeddings (GloVe)
embeddings = torchtext.vocab.GloVe(name='6B', dim=embed_size)

# Load the dependency parser (spaCy)
parser = spacy.load('en_core_web_sm')

# Define a dictionary that maps question words to module types
question_word_to_module_type = {
  'what': 'find',
  'where': 'locate',
  'how': 'measure',
  'is': 'verify',
  'are': 'verify',
  'which': 'choose',
  'who': 'identify',
  'when': 'date',
  'why': 'reason',
  'do': 'action'
}

# Define a dictionary that maps module types to module classes
module_type_to_module_class = {
  'find': FindModule,
  'locate': LocateModule,
  'measure': MeasureModule,
  'verify': VerifyModule,
  'choose': ChooseModule,
  'identify': IdentifyModule,
  'date': DateModule,
  'reason': ReasonModule,
  'action': ActionModule
}

# Define a LSTM model for encoding the question
lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, batch_first=True)

# Define a softmax classifier for selecting modules based on question words and image features
module_selector = nn.Linear(hidden_size + cnn.fc.in_features, num_modules)

# Define a list of modules to choose from
modules = [module_type_to_module_class[module_type]() for module_type in module_type_to_module_class]

# Define a cross-entropy loss function for training
criterion = nn.CrossEntropyLoss()

# Define an optimizer for updating the parameters of the LSTM, the module selector, and the modules
optimizer = torch.optim.Adam([{'params': lstm.parameters()},
                              {'params': module_selector.parameters()},
                              {'params': [module.parameters() for module in modules]}])

# Define a function for parsing a question into a layout of modules using a dependency parser
def parse(question):
  # Tokenize and parse the question using spaCy
  doc = parser(question)
  # Initialize an empty list of layout elements
  layout = []
  # For each token in the question
  for token in doc:
    # If the token is a question word (e.g. what, where, how, etc.)
    if token.tag_ == 'WDT' or token.tag_ == 'WP' or token.tag_ == 'WRB':
      # Get the module type corresponding to the question word
      module_type = question_word_to_module_type[token.text.lower()]
      # Append the module type to the layout
      layout.append(module_type)
    # If the token is a preposition (e.g. on, under, over, etc.)
    elif token.dep_ == 'prep':
      # Append a "relocate" module type to the layout
      layout.append('relocate')
    # If the token is a comparative adjective or adverb (e.g. bigger, smaller, faster, etc.)
    elif token.tag_ == 'JJR' or token.tag_ == 'RBR':
      # Append a "compare" module type to the layout
      layout.append('compare')
    # If the token is a coordinating conjunction (e.g. and, or, but, etc.)
    elif token.dep_ == 'cc':
      # Append an "and" or "or" module type to the layout depending on the token text
      if token.text.lower() == 'and':
        layout.append('and')
      elif token.text.lower() == 'or':
        layout.append('or')
    # If the token is an auxiliary verb (e.g. is, are, do, does, etc.)
    elif token.dep_ == 'aux':
      # Skip this token as it does not affect the layout
      continue
    # Otherwise
    else:
      # Append a "describe" module type to the layout
      layout.append('describe')
  # Return the layout
  return layout

# Define a function for selecting modules for each layout element based on the question word and the image features
def select_module(layout_element, question_word, image_features):
  # Get the word embedding for the question word
  word_embedding = embeddings[question_word]
  # Concatenate the word embedding and the image features
  input_features = torch.cat([word_embedding, image_features], dim=-1)
  # Pass the input features through the module selector
  output_scores = module_selector(input_features)
  # Get the index of the module with the highest score
  module_index = torch.argmax(output_scores, dim=-1)
  # Return the module at that index
  return modules[module_index]

# Define a function for composing the modules into a network
def compose(modules):
  # Initialize an empty list of network layers
  network_layers = []
  # For each module in modules
  for module in modules:
    # Append the module to the network layers
    network_layers.append(module)
    # If the module is not the last one
    if module != modules[-1]:
      # Append a ReLU activation function to the network layers
      network_layers.append(nn.ReLU())
  # Create a sequential network from the network layers
  network = nn.Sequential(*network_layers)
  # Return the network
  return network

# Define a function for training the model on a batch of data
def train(batch):
  # Unpack the batch into images, questions, and answers
  images, questions, answers = batch
  # Extract image features using the CNN
  image_features = cnn(images)
  # Encode the questions using the LSTM
  question_embeddings, _ = lstm(questions)
  # Get the last hidden state of the LSTM as the question representation
  question_representations = question_embeddings[:, -1, :]
  # Initialize an empty list of layouts
  layouts = []
  # Initialize an empty list of selected modules
  selected_modules = []
  # For each question in questions
  for question in questions:
    # Parse the question into a layout of modules using the parse function
    layout = parse(question)
    # Append the layout to the layouts list
    layouts.append(layout)
    # Initialize an empty list of modules for this question
    modules = []
    # For each layout element in layout
    for layout_element in layout:
      # Select a module for this layout element based on the question word and the image features using the select_module function
      module = select_module(layout_element, question[0], image_features[0])
      # Append the module to the modules list
      modules.append(module)
    # Append the modules list to the selected_modules list
    selected_modules.append(modules)
  
  # Initialize an empty list of networks
  networks = []
  # For each selected modules list in selected_modules
  for selected_modules in selected_modules:
    # Compose the selected modules into a network using the compose function
    network = compose(selected_modules)
    # Append the network to the networks list
    networks.append(network)

  # Initialize an empty list of predictions
  predictions = []
  # For each network in networks and image feature in image_features
  for network, image_feature in zip(networks, image_features):
    # Forward propagate the image feature through the network to get a prediction
    prediction = network(image_feature)
    # Append the prediction to the predictions list
    predictions.append(prediction)

  # Stack the predictions into a tensor of shape (batch_size, num_answers)
  predictions = torch.stack(predictions, dim=0)

  # Compute the cross-entropy loss between the predictions and the answers using the criterion function
  loss = criterion(predictions, answers)

  # Compute a regularization term that penalizes modules that are not used frequently across different questions using reg_lambda and torch.norm functions 
  reg_term = reg_lambda * sum([torch.norm(module.weight) for module in modules])

  # Add the regularization term to the loss 
  loss += reg_term

  # Zero out the gradients of all parameters 
  optimizer.zero_grad()

  # Backpropagate the loss through all parameters 
  loss.backward()

  # Update all parameters using optimizer 
  optimizer.step()

# Define a function for testing (or validating) the model on a batch of data 
def test(batch):
   ... (similar to train but without updating parameters or computing regularization term) ...
```