---
title: 2202.08614v2 Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time
date: 2022-02-09
---

# [Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time](http://arxiv.org/abs/2202.08614v2)

authors: Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Lan Xu, Jingyi Yu


## What, Why and How

[1]: https://arxiv.org/abs/2202.08614 "[2202.08614] Fourier PlenOctrees for Dynamic Radiance Field Rendering ..."
[2]: https://arxiv.org/pdf/2207.08614v2.pdf "αi αn q αn β < θn 1 k d arXiv:2207.08614v2 [math.NT] 30 Jul 2022"
[3]: http://export.arxiv.org/abs/2106.08614v2 "[2106.08614v2] Electric-dual BPS Vortices in The Generalized Self-dual ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a novel technique called Fourier PlenOctree (FPO) for efficient neural modeling and real-time rendering of dynamic scenes captured under the free-view video (FVV) setting.
- **Why**: The paper aims to overcome the limitations of existing methods such as Neural Radiance Field (NeRF) and PlenOctree that are mainly designed for static objects and cannot handle dynamic scenes with complex motions and deformations.
- **How**: The paper proposes a novel combination of generalized NeRF, PlenOctree representation, volumetric fusion and Fourier transform. The paper introduces a coarse-to-fine fusion scheme to construct the FPO tree via spatial blending, and a Fourier coefficient network to model the time-varying density and color attributes of the dynamic scenes. The paper also shows how to train the FPO directly on the leaves of a union PlenOctree structure of the dynamic sequence. The paper demonstrates that the FPO enables compact memory overload, efficient fine-tuning and high visual quality for the free-viewpoint rendering of unseen dynamic scenes.

## Main Contributions

According to the paper, the main contributions are:

- A novel Fourier PlenOctree (FPO) technique that combines generalized NeRF, PlenOctree representation, volumetric fusion and Fourier transform for efficient neural modeling and real-time rendering of dynamic scenes captured under the FVV setting.
- A novel coarse-to-fine fusion scheme that leverages the generalizable NeRF technique to generate the FPO tree via spatial blending.
- A novel Fourier coefficient network that models the time-varying density and color attributes of the dynamic scenes and supports efficient fine-tuning.
- Extensive experiments that show that the proposed method is 3000 times faster than the original NeRF and achieves over an order of magnitude acceleration over state-of-the-art methods while preserving high visual quality for the free-viewpoint rendering of unseen dynamic scenes.

## Method Summary

[1]: https://arxiv.org/abs/2202.08614 "[2202.08614] Fourier PlenOctrees for Dynamic Radiance Field Rendering ..."
[2]: https://arxiv.org/pdf/2112.08614v2.pdf "arXiv:2112.08614v2 [cs.CL] 5 May 2022"
[3]: http://export.arxiv.org/abs/2108.08614v2 "[2108.08614v2] UNIQORN: Unified Question Answering over RDF Knowledge ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first introduces the generalized NeRF (G-NeRF) model that can handle arbitrary camera poses and scene geometries by using a positional encoding scheme and a multi-layer perceptron network.
- The paper then describes the PlenOctree representation that can efficiently store and render the radiance field of a static scene by using an octree structure and a hierarchical sampling strategy.
- The paper then proposes the Fourier PlenOctree (FPO) technique that extends the PlenOctree representation to dynamic scenes by modeling the time-varying density and color attributes of the radiance field with Fourier coefficients.
- The paper then presents the coarse-to-fine fusion scheme that constructs the FPO tree by blending the G-NeRF outputs of different frames at different octree levels, and pruning the redundant nodes based on a density threshold.
- The paper then explains how to train the FPO network by optimizing the Fourier coefficients on the leaves of a union PlenOctree structure of the dynamic sequence, and how to fine-tune the FPO network for unseen frames by using a temporal consistency loss.
- The paper finally discusses some implementation details and ablation studies of the proposed method.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a dynamic sequence of frames and camera poses
# Output: a Fourier PlenOctree (FPO) network for rendering

# Step 1: Generalized NeRF (G-NeRF) model
# Define a positional encoding scheme for 3D points and 2D directions
# Define a multi-layer perceptron network with skip connections
# For each frame in the sequence:
    # Sample points along each camera ray and encode them with the positional encoding scheme
    # Feed the encoded points and directions to the network and get the density and color outputs
    # Compute the radiance field of the frame by using volume rendering

# Step 2: PlenOctree representation
# Define an octree structure with eight children per node and a leaf size threshold
# Define a hierarchical sampling strategy that samples more points near the leaf nodes
# For each frame in the sequence:
    # Initialize an empty octree and a queue of root nodes
    # While the queue is not empty:
        # Pop a node from the queue
        # If the node size is smaller than the leaf size threshold:
            # Store the node as a leaf and assign it a density and color value from G-NeRF
        # Else:
            # Split the node into eight children and push them to the queue
    # Prune the octree by removing nodes with zero density

# Step 3: Fourier PlenOctree (FPO) technique
# Define a Fourier coefficient network that takes a time input and outputs a density and color coefficient vector
# Define a union PlenOctree structure that merges the octrees of all frames in the sequence
# For each leaf node in the union PlenOctree:
    # Initialize the Fourier coefficient network with random weights
    # For each frame in the sequence:
        # If the leaf node belongs to the frame's octree:
            # Get the density and color value of the node from G-NeRF
            # Get the density and color coefficient vector of the node from the Fourier coefficient network
            # Compute the reconstruction loss between the value and the coefficient vector
    # Update the weights of the Fourier coefficient network by minimizing the reconstruction loss

# Step 4: Coarse-to-fine fusion scheme
# Define a spatial blending function that takes two density and color values and outputs a blended value
# Define a density threshold for pruning nodes
# For each level of the union PlenOctree from coarse to fine:
    # For each node in the level:
        # If the node has more than one child from different frames:
            # Blend the density and color values of the children by using the spatial blending function
            # Assign the blended value to the node and remove its children
        # Else if the node has only one child from one frame:
            # Assign the child's value to the node and remove its child
        # Else if the node has no child:
            # Assign zero density and color to the node
    # Prune the level by removing nodes with density lower than the threshold

# Step 5: FPO fine-tuning
# Define a temporal consistency loss that measures the smoothness of FPO outputs across frames
# For each unseen frame in the sequence:
    # Sample points along each camera ray and encode them with the positional encoding scheme
    # Traverse the FPO tree and find the leaf nodes that contain the sampled points
    # Get the density and color coefficient vectors of the leaf nodes from the Fourier coefficient network
    # Compute the radiance field of the frame by using volume rendering and Fourier transform
    # Compute the image reconstruction loss between FPO output and ground truth image
    # Compute the temporal consistency loss between FPO outputs of adjacent frames
    # Update the weights of FPO network by minimizing both losses

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
from PIL import Image

# Define constants
L = 10 # number of frequency bands for positional encoding
D = 256 # dimension of hidden layers in MLP network
K = 4 # number of Fourier coefficients for density and color
T = 32 # number of frames in the sequence
H = 256 # height of the image
W = 256 # width of the image
N_rays = H * W # number of camera rays per image
N_samples = 64 # number of samples per ray
N_fine = 128 # number of fine samples per ray
leaf_size = 0.01 # threshold for leaf node size
density_threshold = 1e-5 # threshold for pruning nodes
lambda_temporal = 1.0 # weight for temporal consistency loss
learning_rate = 1e-4 # learning rate for optimization

# Define helper functions
def load_data():
    """
    Load the dynamic sequence of frames and camera poses from disk.
    Return a list of images and a list of poses.
    """
    images = []
    poses = []
    for i in range(T):
        image = Image.open(f"frame_{i}.png")
        image = T.ToTensor()(image)
        images.append(image)
        pose = np.load(f"pose_{i}.npy")
        poses.append(pose)
    return images, poses

def positional_encoding(x):
    """
    Apply positional encoding to a tensor x of shape (..., C).
    Return a tensor of shape (..., C * (2L + 1)).
    """
    C = x.shape[-1]
    freqs = 2 ** torch.linspace(0, L - 1, L) # (L,)
    freqs = freqs.view(1, -1, 1) # (1, L, 1)
    x = x.unsqueeze(-2) # (..., 1, C)
    x = x * freqs # (..., L, C)
    x = torch.cat([x.sin(), x.cos()], dim=-2) # (..., 2L, C)
    x = x.reshape(*x.shape[:-2], -1) # (..., C * 2L)
    x = torch.cat([x, x[..., :C]], dim=-1) # (..., C * (2L + 1))
    return x

def volume_rendering(density, color, t_vals):
    """
    Apply volume rendering to density, color and t_vals tensors of shape (N_rays, N_samples).
    Return a tensor of shape (N_rays, 3) representing the rendered radiance field.
    """
    delta_t = t_vals[..., 1:] - t_vals[..., :-1] # (N_rays, N_samples - 1)
    alpha = 1 - torch.exp(-density * delta_t) # (N_rays, N_samples - 1)
    weights = alpha * torch.cumprod(torch.cat([torch.ones_like(alpha[..., :1]), 1 - alpha + 1e-10], dim=-1), dim=-1) # (N_rays, N_samples)
    radiance = torch.sum(weights[..., None] * color, dim=-2) # (N_rays, 3)
    return radiance

def spatial_blending(value_1, value_2):
    """
    Apply spatial blending to two tensors value_1 and value_2 of shape (..., C).
    Return a tensor of shape (..., C) representing the blended value.
    """
    weight_1 = torch.sigmoid(value_1[..., :1]) # (..., 1)
    weight_2 = torch.sigmoid(value_2[..., :1]) # (..., 1)
    weight_sum = weight_1 + weight_2 + 1e-10 # (..., 1)
    value = (weight_1 * value_1 + weight_2 * value_2) / weight_sum # (..., C)
    return value

def fourier_transform(coef, t):
    """
    Apply Fourier transform to a tensor coef of shape (..., K) and a tensor t of shape (...).
    Return a tensor of shape (...) representing the Fourier series approximation.
    """
    freqs = torch.arange(K // 2) # (K // 2,)
    freqs = freqs.view(1, -1) # (1, K // 2)
    t = t.unsqueeze(-1) # (..., 1)
    t = t * freqs # (..., K // 2)
    t = torch.cat([t.sin(), t.cos()], dim=-1) # (..., K)
    value = torch.sum(coef * t, dim=-1) # (...)
    return value

# Define model classes
class MLP(nn.Module):
    """
    A multi-layer perceptron network with skip connections.
    """
    def __init__(self, in_dim, out_dim, hidden_dim=D, num_layers=4):
        super().__init__()
        self.layers = nn.ModuleList()
        for i in range(num_layers):
            if i == 0:
                self.layers.append(nn.Linear(in_dim, hidden_dim))
            elif i == num_layers - 1:
                self.layers.append(nn.Linear(hidden_dim + in_dim, out_dim))
            else:
                self.layers.append(nn.Linear(hidden_dim + in_dim if i % 2 == 0 else hidden_dim, hidden_dim))
    
    def forward(self, x):
        """
        Forward pass of the network.
        Input: a tensor x of shape (..., in_dim).
        Output: a tensor of shape (..., out_dim).
        """
        h = x
        for i, layer in enumerate(self.layers):
            if i % 2 == 0:
                h = torch.cat([h, x], dim=-1) if i > 0 else h
            h = F.relu(layer(h)) if i < len(self.layers) - 1 else layer(h)
        return h

class GNeRF(nn.Module):
    """
    A generalized NeRF model that can handle arbitrary camera poses and scene geometries.
    """
    def __init__(self):
        super().__init__()
        self.net_density = MLP(3 * (2 * L + 1), D + 1) # network for density prediction
        self.net_color = MLP(D + 2 * (2 * L + 1), 3) # network for color prediction
    
    def forward(self, p, d):
        """
        Forward pass of the model.
        Input: a tensor p of shape (N_rays, N_samples, 3) representing the sampled points,
               a tensor d of shape (N_rays, N_samples, 3) representing the viewing directions.
        Output: a tensor density of shape (N_rays, N_samples) representing the density values,
                a tensor color of shape (N_rays, N_samples, 3) representing the color values.
        """
        p = positional_encoding(p) # (N_rays, N_samples, 3 * (2 * L + 1))
        d = positional_encoding(d) # (N_rays, N_samples, 3 * (2 * L + 1))
        h = self.net_density(p) # (N_rays, N_samples, D + 1)
        density = F.relu(h[..., -1]) # (N_rays, N_samples)
        h = h[..., :-1] # (N_rays, N_samples, D)
        h = torch.cat([h, p, d], dim=-1) # (N_rays, N_samples, D + 2 * (2 * L + 1))
        color = torch.sigmoid(self.net_color(h)) # (N_rays, N_samples, 3)
        return density, color

class PlenOctree(nn.Module):
    """
    A PlenOctree representation that can efficiently store and render the radiance field of a static scene.
    """
    def __init__(self):
        super().__init__()
        self.root = None # root node of the octree
    
    def build(self, p, density, color):
        """
        Build the octree from the sampled points and their density and color values.
        Input: a tensor p of shape (N_rays * N_samples, 3) representing the sampled points,
               a tensor density of shape (N_rays * N_samples,) representing the density values,
               a tensor color of shape (N_rays * N_samples, 3) representing the color values.
        Output: None
        """
        # Initialize the root node with the bounding box of the points
        p_min = torch.min(p, dim=0)[0] # (3,)
        p_max = torch.max(p, dim=0)[0] # (3,)
        center = (p_min + p_max) / 2 # (3,)
        size = torch.max(p_max - p_min) # scalar
        self.root = Node(center, size)

        # Build the octree recursively
        queue = [self.root] # queue of nodes to be processed
        while queue:
            node = queue.pop(0) # get the current node
            if node.size < leaf_size: # check if the node is small enough to be a leaf
                node.is