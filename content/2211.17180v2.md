---
title: 2211.17180v2 Nonlinear Advantage  Trained Networks Might Not Be As Complex as You Think
date: 2022-11-18
---

# [Nonlinear Advantage: Trained Networks Might Not Be As Complex as You Think](http://arxiv.org/abs/2211.17180v2)

authors: Christian H. X. Ali Mehmeti-Göpel, Jan Disselhoff


## What, Why and How

[1]: https://arxiv.org/pdf/2211.17180v2.pdf "arXiv:2211.17180v2 [cs.LG] 1 Jun 2023"
[2]: https://arxiv.org/abs/2211.17180 "[2211.17180] Average Path Length: Sparsification of Nonlinearties ..."
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2211.17180 "[2211.17180] Nonlinear Advantage: Trained Networks Might Not Be As ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper is about an empirical study of the behaviour of deep networks when fully linearizing some of its feature channels through a sparsity prior on the overall number of nonlinear units in the network.
- **Why**: The paper aims to investigate how much the network function can be simplified towards linearity before performance collapses, and how the remaining nonlinear units organize into distinct structures under sparsity pressure.
- **How**: The paper performs experiments on image classification and machine translation tasks using common convolutional and transformer architectures with sparsified PReLUs. The paper introduces a measure called average path length, representing the average number of active nonlinearities encountered along a path in the network graph. The paper makes several observations on the performance, depth, width, and training dynamics of the partially linearized networks.

## Main Contributions

[1]: https://arxiv.org/pdf/2211.17180v2.pdf "arXiv:2211.17180v2 [cs.LG] 1 Jun 2023"
[2]: https://arxiv.org/abs/2211.17180 "[2211.17180] Average Path Length: Sparsification of Nonlinearties ..."
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2211.17180 "[2211.17180] Nonlinear Advantage: Trained Networks Might Not Be As ..."

According to the paper at [^1^][1], the main contributions are:

- They introduce a measure called **average path length**, representing the average number of active nonlinearities encountered along a path in the network graph, to characterize the depth of the partially linearized networks.
- They perform experiments on image classification and machine translation tasks using common convolutional and transformer architectures with sparsified PReLUs, and observe how much they can simplify the network function towards linearity before performance collapses.
- They observe a significant **performance gap** when reducing nonlinearity in the network function early on as opposed to late in training, in-line with recent observations on the time-evolution of the data-dependent NTK.
- They find that after training, they are able to linearize a significant number of nonlinear units while maintaining a high performance, indicating that much of a network’s expressivity remains unused but helps gradient descent in early stages of training.
- They find that under sparsity pressure, the remaining nonlinear units organize into distinct structures, forming **core-networks** of near constant effective depth and width, which in turn depend on task difficulty.

## Method Summary

[1]: https://arxiv.org/pdf/2211.17180v2.pdf "arXiv:2211.17180v2 [cs.LG] 1 Jun 2023"
[2]: https://arxiv.org/abs/2211.17180 "[2211.17180] Average Path Length: Sparsification of Nonlinearties ..."
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2211.17180 "[2211.17180] Nonlinear Advantage: Trained Networks Might Not Be As ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper uses **sparsified PReLUs** as the activation functions for the networks, which are parametric rectified linear units with an additional sparsity parameter that controls the fraction of linearized units.
- The paper defines the **average path length** as the average number of active nonlinearities encountered along a path in the network graph, and computes it by counting the number of active PReLUs for each input sample and averaging over the samples and layers.
- The paper performs experiments on two tasks: image classification using CIFAR-10 and CIFAR-100 datasets with ResNet-18 and ResNet-50 architectures, and machine translation using WMT14 En-De dataset with Transformer-base architecture.
- The paper applies sparsity pressure on the networks by setting different values for the sparsity parameter, and compares the performance, depth, width, and training dynamics of the partially linearized networks with the baseline networks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the sparsified PReLU activation function
def sparsified_prelu(x, alpha, beta):
  # x: input tensor
  # alpha: learnable parameter for negative slope
  # beta: sparsity parameter for fraction of linearized units
  # return: output tensor
  mask = torch.rand_like(x) < beta # create a random mask with sparsity beta
  y = torch.where(x > 0, x, alpha * x) # apply PReLU activation
  y[mask] = x[mask] # linearize the masked units
  return y

# Define the average path length measure
def average_path_length(network, inputs):
  # network: a deep network with sparsified PReLUs as activation functions
  # inputs: a batch of input samples
  # return: the average number of active nonlinearities per path
  total_count = 0 # initialize the total count of active nonlinearities
  for layer in network.layers:
    if layer is sparsified_prelu: # only consider the sparsified PReLU layers
      outputs = layer(inputs) # compute the outputs of the layer
      active_count = torch.sum(outputs != inputs) # count the number of active nonlinearities
      total_count += active_count # update the total count
      inputs = outputs # update the inputs for the next layer
  average_count = total_count / (len(inputs) * len(network.layers)) # compute the average count
  return average_count

# Perform experiments on image classification and machine translation tasks
for task in [image_classification, machine_translation]:
  for dataset in task.datasets:
    for architecture in task.architectures:
      network = architecture() # initialize the network with sparsified PReLUs
      baseline = architecture() # initialize the baseline network with regular PReLUs
      for sparsity in [0.0, 0.1, ..., 0.9]: # try different values of sparsity parameter
        network.set_sparsity(sparsity) # set the sparsity parameter for the network
        train(network, dataset) # train the network on the dataset
        test(network, dataset) # test the network on the dataset
        apl = average_path_length(network, dataset) # compute the average path length of the network
        compare(network, baseline, apl) # compare the performance, depth, width, and training dynamics of the network and the baseline

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision
import torchtext

# Define the sparsified PReLU activation function as a custom module
class SparsifiedPReLU(nn.Module):
  def __init__(self, num_parameters=1, init=0.25, sparsity=0.0):
    # num_parameters: number of learnable parameters for negative slope
    # init: initial value for the parameters
    # sparsity: fraction of linearized units
    super(SparsifiedPReLU, self).__init__()
    self.num_parameters = num_parameters
    self.sparsity = sparsity
    self.weight = nn.Parameter(torch.Tensor(num_parameters).fill_(init)) # create the learnable parameter tensor

  def forward(self, x):
    # x: input tensor
    # return: output tensor
    mask = torch.rand_like(x) < self.sparsity # create a random mask with sparsity beta
    y = torch.where(x > 0, x, self.weight * x) # apply PReLU activation
    y[mask] = x[mask] # linearize the masked units
    return y

  def set_sparsity(self, sparsity):
    # sparsity: fraction of linearized units
    # set the sparsity parameter for the module
    self.sparsity = sparsity

# Define the average path length measure as a function
def average_path_length(network, inputs):
  # network: a deep network with sparsified PReLUs as activation functions
  # inputs: a batch of input samples
  # return: the average number of active nonlinearities per path
  total_count = 0 # initialize the total count of active nonlinearities
  for layer in network.modules():
    if isinstance(layer, SparsifiedPReLU): # only consider the sparsified PReLU layers
      outputs = layer(inputs) # compute the outputs of the layer
      active_count = torch.sum(outputs != inputs) # count the number of active nonlinearities
      total_count += active_count # update the total count
      inputs = outputs # update the inputs for the next layer
  average_count = total_count / (len(inputs) * len(list(network.modules()))) # compute the average count
  return average_count

# Define the image classification task as a class
class ImageClassification:
  def __init__(self):
    self.datasets = [torchvision.datasets.CIFAR10, torchvision.datasets.CIFAR100] # list of datasets to use
    self.architectures = [torchvision.models.resnet18, torchvision.models.resnet50] # list of architectures to use

  def train(self, network, dataset):
    # network: a deep network with sparsified PReLUs as activation functions
    # dataset: an image classification dataset
    # train the network on the dataset using standard procedures such as data loading, augmentation, optimization, loss function, etc.

  def test(self, network, dataset):
    # network: a deep network with sparsified PReLUs as activation functions
    # dataset: an image classification dataset
    # test the network on the dataset using standard procedures such as data loading, evaluation metrics, etc.

# Define the machine translation task as a class
class MachineTranslation:
  def __init__(self):
    self.datasets = [torchtext.datasets.WMT14] # list of datasets to use
    self.architectures = [torch.nn.Transformer] # list of architectures to use

  def train(self, network, dataset):
    # network: a deep network with sparsified PReLUs as activation functions
    # dataset: a machine translation dataset
    # train the network on the dataset using standard procedures such as data loading, tokenization, optimization, loss function, etc.

  def test(self, network, dataset):
    # network: a deep network with sparsified PReLUs as activation functions
    # dataset: a machine translation dataset
    # test the network on the dataset using standard procedures such as data loading, tokenization, evaluation metrics, etc.

# Define a function to compare the performance, depth, width, and training dynamics of two networks
def compare(network1, network2, apl1=None, apl2=None):
  # network1: a deep network with sparsified PReLUs as activation functions (the partially linearized network)
  # network2: a deep network with regular PReLUs as activation functions (the baseline network)
  # apl1: the average path length of network1 (optional)
  # apl2: the average path length of network2 (optional)
  # compare the two networks using standard procedures such as plotting, printing, logging, etc.

# Perform experiments on image classification and machine translation tasks
for task in [ImageClassification(), MachineTranslation()]:
  for dataset in task.datasets:
    for architecture in task.architectures:
      network = architecture() # initialize the network with sparsified PReLUs
      baseline = architecture() # initialize the baseline network with regular PReLUs
      for sparsity in [0.0, 0.1, ..., 0.9]: # try different values of sparsity parameter
        network.set_sparsity(sparsity) # set the sparsity parameter for the network
        task.train(network, dataset) # train the network on the dataset
        task.test(network, dataset) # test the network on the dataset
        apl = average_path_length(network, dataset) # compute the average path length of the network
        compare(network, baseline, apl) # compare the performance, depth, width, and training dynamics of the network and the baseline

```