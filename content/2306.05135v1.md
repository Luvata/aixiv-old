---
title: 2306.05135v1 Does Image Anonymization Impact Computer Vision Training?
date: 2023-06-06
---

# [Does Image Anonymization Impact Computer Vision Training?](http://arxiv.org/abs/2306.05135v1)

authors: Håkon Hukkelås, Frank Lindseth


## What, Why and How

[1]: https://arxiv.org/abs/2306.05135 "Does Image Anonymization Impact Computer Vision Training?"
[2]: https://arxiv.org/pdf/2306.05135v1.pdf "arXiv:2306.05135v1 [cs.CV] 8 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.05135 "[2306.05135] Does Image Anonymization Impact Computer Vision Training?"

Here is a summary of the paper[^1^][1] [^2^][2]:

**What**: The paper investigates the impact of image anonymization for training computer vision models on key computer vision tasks (detection, instance segmentation, and pose estimation).

**Why**: Image anonymization is widely adapted in practice to comply with privacy regulations in many regions. However, anonymization often degrades the quality of the data, reducing its utility for computer vision development.

**How**: The paper benchmarks the recognition drop on common detection datasets, where it evaluates both traditional and realistic anonymization for faces and full bodies. The paper demonstrates that realistic anonymization can enable privacy-preserving computer vision development with minimal performance degradation across a range of important computer vision benchmarks.


## Main Contributions

[1]: https://arxiv.org/abs/2306.05135 "Does Image Anonymization Impact Computer Vision Training?"
[2]: https://arxiv.org/pdf/2306.05135v1.pdf "arXiv:2306.05135v1 [cs.CV] 8 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.05135 "[2306.05135] Does Image Anonymization Impact Computer Vision Training?"

According to the paper[^1^][1] [^2^][2], the main contributions are:

- They provide a comprehensive study of the impact of image anonymization for training computer vision models on key computer vision tasks (detection, instance segmentation, and pose estimation).
- They evaluate both traditional and realistic anonymization for faces and full bodies on common detection datasets (COCO, WIDER FACE, and CrowdPose).
- They demonstrate that traditional image anonymization substantially impacts final model performance, particularly when anonymizing the full body.
- They show that realistic anonymization can mitigate this decrease in performance, where their experiments reflect a minimal performance drop for face anonymization.
- They illustrate that realistic anonymization can enable privacy-preserving computer vision development with minimal performance degradation across a range of important computer vision benchmarks.


## Method Summary

[1]: https://arxiv.org/abs/2306.05135 "Does Image Anonymization Impact Computer Vision Training?"
[2]: https://arxiv.org/pdf/2306.05135v1.pdf "arXiv:2306.05135v1 [cs.CV] 8 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.05135 "[2306.05135] Does Image Anonymization Impact Computer Vision Training?"

Here is a summary of the method section of the paper[^1^][1] [^2^][2]:

- The paper proposes two types of image anonymization: traditional and realistic. Traditional anonymization includes methods such as blurring, pixelation, and masking. Realistic anonymization involves replacing the original face or body with a synthetic one that preserves the pose and appearance.
- The paper uses a generative adversarial network (GAN) to create realistic anonymized images. The GAN consists of a generator and a discriminator. The generator takes an input image and a segmentation mask as inputs and outputs a realistic anonymized image. The discriminator tries to distinguish between real and fake images and provides feedback to the generator.
- The paper evaluates the impact of image anonymization for training computer vision models on three tasks: detection, instance segmentation, and pose estimation. The paper uses three datasets for these tasks: COCO, WIDER FACE, and CrowdPose. The paper compares the performance of models trained on original images, traditional anonymized images, and realistic anonymized images.
- The paper measures the performance drop as the difference between the mean average precision (mAP) of models trained on original images and models trained on anonymized images. The paper also analyzes the performance drop for different levels of anonymization (low, medium, high) and different categories of objects (person, face, animal, vehicle, etc.).


## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper:

```python
# Define the generator and discriminator networks
generator = GAN_Generator()
discriminator = GAN_Discriminator()

# Define the loss functions for the generator and discriminator
generator_loss = adversarial_loss + reconstruction_loss + perceptual_loss
discriminator_loss = binary_cross_entropy

# Define the optimizer for the generator and discriminator
optimizer = Adam(lr=0.0002, beta_1=0.5)

# Define the number of epochs and batch size for training
epochs = 100
batch_size = 16

# Loop over the epochs
for epoch in range(epochs):

  # Loop over the batches of images and masks
  for batch_images, batch_masks in data_loader:

    # Generate realistic anonymized images using the generator
    fake_images = generator(batch_images, batch_masks)

    # Compute the discriminator outputs for real and fake images
    real_outputs = discriminator(batch_images)
    fake_outputs = discriminator(fake_images)

    # Compute the generator and discriminator losses
    g_loss = generator_loss(real_outputs, fake_outputs, batch_images, fake_images)
    d_loss = discriminator_loss(real_outputs, fake_outputs)

    # Update the generator and discriminator parameters using the optimizer
    optimizer.minimize(g_loss, generator.parameters())
    optimizer.minimize(d_loss, discriminator.parameters())

  # Save the generator and discriminator models periodically
  if epoch % 10 == 0:
    save_model(generator, "generator_epoch_" + str(epoch) + ".pth")
    save_model(discriminator, "discriminator_epoch_" + str(epoch) + ".pth")

# Load a trained generator model
generator = load_model("generator_final.pth")

# Loop over the test images and masks
for test_image, test_mask in test_data_loader:

  # Generate realistic anonymized image using the generator
  fake_image = generator(test_image, test_mask)

  # Evaluate the performance of computer vision models on original and anonymized images
  original_mAP = evaluate_cv_model(test_image)
  fake_mAP = evaluate_cv_model(fake_image)

  # Compute the performance drop as the difference between original and anonymized mAPs
  performance_drop = original_mAP - fake_mAP

  # Report the performance drop for different tasks, datasets, levels of anonymization, and categories of objects
  report_performance_drop(performance_drop)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the generator network
class GAN_Generator(torch.nn.Module):

  # Initialize the network
  def __init__(self):

    # Call the parent class constructor
    super(GAN_Generator, self).__init__()

    # Define the encoder network
    self.encoder = torch.nn.Sequential(
      # Convolutional layer with 3 input channels, 64 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(3, 64, 4, 2, 1),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 64 input channels, 128 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(64, 128, 4, 2, 1),
      # Batch normalization layer with 128 features and momentum of 0.8
      torch.nn.BatchNorm2d(128, 0.8),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 128 input channels, 256 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(128, 256, 4, 2, 1),
      # Batch normalization layer with 256 features and momentum of 0.8
      torch.nn.BatchNorm2d(256, 0.8),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 256 input channels, 512 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(256, 512, 4, 2, 1),
      # Batch normalization layer with 512 features and momentum of 0.8
      torch.nn.BatchNorm2d(512, 0.8),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2)
    )

    # Define the decoder network
    self.decoder = torch.nn.Sequential(
      # Transposed convolutional layer with 512 input channels, 256 output channels, 
      # kernel size of (4x4), stride of (1x1), padding of (0x0), output_padding of (0x0)
      torch.nn.ConvTranspose2d(512 + (3 * (1 + (1 + (1 + (1 + (1 + (1 + (1 + (1)))))))))), 
                               out_channels=256,
                               kernel_size=(4x4), 
                               stride=(1x1), 
                               padding=(0x0), 
                               output_padding=(0x0)),
      # Batch normalization layer with momentum of .8
      torch.nn.BatchNorm2d(num_features=256,
                           momentum=.8),
      # ReLU activation function
      torch.nn.ReLU(),
      
      # Transposed convolutional layer with input channels equal to the number of output channels from previous layer plus 
      # three times the number of masks plus one for each mask in the previous layer,
      # output channels equal to half the number of input channels,
      # kernel size of (4x4), stride of (2x2), padding of (1x1)
      torch.nn.ConvTranspose2d(in_channels=256 + (3 * (1 + (1 + (1 + (1 + (1 + (1 + (1)))))))), 
                               out_channels=128,
                               kernel_size=(4x4), 
                               stride=(2x2), 
                               padding=(1x1)),
      
      # Batch normalization layer with momentum of .8
      torch.nn.BatchNorm2d(num_features=128,
                           momentum=.8),
      
      # ReLU activation function
      torch.nn.ReLU(),
      
       # Transposed convolutional layer with input channels equal to the number of output channels from previous layer plus 
       # three times the number of masks plus one for each mask in the previous layer,
       # output channels equal to half the number of input channels,
       # kernel size of (4x4), stride of (2x2), padding of (1x1)
      torch.nn.ConvTranspose2d(in_channels=128 + (3 * (1 + (1 + (1 + (1 + (1 + (1))))))), 
                               out_channels=64,
                               kernel_size=(4x4), 
                               stride=(2x2), 
                               padding=(1x1)),
      
      # Batch normalization layer with momentum of .8
      torch.nn.BatchNorm2d(num_features=64,
                           momentum=.8),
      
      # ReLU activation function
      torch.nn.ReLU(),
      
       # Transposed convolutional layer with input channels equal to the number of output channels from previous layer plus 
       # three times the number of masks plus one for each mask in the previous layer,
       # output channels equal to 3,
       # kernel size of (4x4), stride of (2x2), padding of (1x1)
      torch.nn.ConvTranspose2d(in_channels=64 + (3 * (1 + (1 + (1 + (1)))))), 
                               out_channels=3,
                               kernel_size=(4x4), 
                               stride=(2x2), 
                               padding=(1x1)),
      
      # Tanh activation function
      torch.nn.Tanh()
    )

  # Define the forward pass of the network
  def forward(self, x, mask):

    # Encode the input image using the encoder network
    x = self.encoder(x)

    # Concatenate the encoded image with the mask
    x = torch.cat([x, mask], 1)

    # Decode the concatenated image using the decoder network
    x = self.decoder(x)

    # Return the decoded image
    return x

# Define the discriminator network
class GAN_Discriminator(torch.nn.Module):

  # Initialize the network
  def __init__(self):

    # Call the parent class constructor
    super(GAN_Discriminator, self).__init__()

    # Define the network layers
    self.layers = torch.nn.Sequential(
      # Convolutional layer with 3 input channels, 64 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(3, 64, 4, 2, 1),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 64 input channels, 128 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(64, 128, 4, 2, 1),
      # Batch normalization layer with 128 features and momentum of 0.8
      torch.nn.BatchNorm2d(128, 0.8),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 128 input channels, 256 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(128, 256, 4, 2, 1),
      # Batch normalization layer with 256 features and momentum of 0.8
      torch.nn.BatchNorm2d(256, 0.8),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 256 input channels, 512 output channels, 4x4 kernel size, 2 stride, and 1 padding
      torch.nn.Conv2d(256, 512, 4, 2, 1),
      # Batch normalization layer with 512 features and momentum of 0.8
      torch.nn.BatchNorm2d(512, 0.8),
      # Leaky ReLU activation function with negative slope of 0.2
      torch.nn.LeakyReLU(0.2),
      # Convolutional layer with 512 input channels, 1 output channel, 
      # kernel size equal to the image size divided by a factor of (16x16), stride of (1x1), padding of (0x0)
      torch.nn.Conv2d(512,
                      out_channels=1,
                      kernel_size=(image_size / (16 * downscale_factor)), 
                      stride=(1 * downscale_factor), 
                      padding=(0 * downscale_factor)),
      
      # Sigmoid activation function
      torch.nn.Sigmoid()
    )

  
   def forward(self,x):
     return self.layers(x)

# Define the loss functions for the generator and discriminator

# Define the adversarial loss as binary cross entropy loss
adversarial_loss = torch.nn.BCELoss()

# Define the reconstruction loss as L1 loss between original and anonymized images
reconstruction_loss = torch.nn.L1Loss()

# Define the