---
title: 2111.08851v5 Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities
date: 2021-11-09
---

# [Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities](http://arxiv.org/abs/2111.08851v5)

authors: Xintong Shi, Wenzhi Cao, Sebastian Raschka


## What, Why and How

[1]: https://arxiv.org/abs/2111.08851 "[2111.08851] Deep Neural Networks for Rank-Consistent Ordinal ..."
[2]: https://arxiv.org/pdf/2111.08851v1.pdf "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On ..."
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2111.08851v5 "[2111.08851v5] Deep Neural Networks for Rank-Consistent Ordinal ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new method for rank-consistent ordinal regression based on conditional probabilities (CORN) for deep neural networks.
- **Why**: The paper addresses the problem of ordinal regression, where the target labels have an intrinsic order but not a quantitative distance. Conventional classification losses ignore this ordering information and are suboptimal for ordinal regression. Existing ordinal regression methods for deep neural networks, such as CORAL, achieve rank consistency by imposing a weight-sharing constraint in the output layer, which could limit the expressiveness of the network.
- **How**: The paper achieves rank consistency by a novel training scheme that uses conditional training sets to obtain the unconditional rank probabilities through applying the chain rule for conditional probability distributions. The paper shows that CORN does not require a weight-sharing constraint and can be applied to any deep neural network architecture. The paper evaluates CORN on various datasets and demonstrates its superior performance compared to CORAL and other baselines.

## Main Contributions

[1]: https://arxiv.org/abs/2111.08851 "[2111.08851] Deep Neural Networks for Rank-Consistent Ordinal ..."
[2]: https://arxiv.org/pdf/2111.08851v1.pdf "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On ..."
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2111.08851v5 "[2111.08851v5] Deep Neural Networks for Rank-Consistent Ordinal ..."

The paper claims the following contributions[^1^][1]:

- **A new method for rank-consistent ordinal regression based on conditional probabilities (CORN) that does not require a weight-sharing constraint in the output layer of a deep neural network.**
- **A novel training scheme that uses conditional training sets to obtain the unconditional rank probabilities through applying the chain rule for conditional probability distributions.**
- **An empirical evaluation of CORN on various datasets and comparison with CORAL and other baselines, showing that CORN outperforms them in terms of accuracy and mean absolute error.**

## Method Summary

[1]: https://arxiv.org/abs/2111.08851 "[2111.08851] Deep Neural Networks for Rank-Consistent Ordinal ..."
[2]: https://arxiv.org/pdf/2111.08851v1.pdf "Deep Neural Networks for Rank-Consistent Ordinal Regression Based On ..."
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2111.08851v5 "[2111.08851v5] Deep Neural Networks for Rank-Consistent Ordinal ..."

Here is a summary of the method section of the paper[^1^][2]:

- **The paper introduces the CORN framework, which consists of two components: a deep neural network (DNN) and a conditional training scheme.**
- **The DNN can be any architecture that takes an input feature vector and outputs a vector of K logits, where K is the number of ordinal ranks. The logits are then passed through a softmax function to obtain the conditional rank probabilities.**
- **The conditional training scheme uses K-1 binary training sets, each corresponding to a rank threshold. For example, the first training set contains all the samples with rank 1 as positive and all the others as negative. The second training set contains all the samples with rank 1 or 2 as positive and all the others as negative, and so on.**
- **The paper trains the DNN on each training set separately using binary cross-entropy loss and obtains K-1 conditional rank probability models. Then, the paper applies the chain rule for conditional probability distributions to obtain the unconditional rank probabilities from the conditional ones.**
- **The paper shows that CORN achieves rank consistency, meaning that the unconditional rank probabilities are monotonically non-decreasing with respect to the ordinal ranks. The paper also shows that CORN does not require a weight-sharing constraint in the output layer of the DNN, unlike CORAL.**

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a DNN with any architecture
DNN = create_dnn(input_size, output_size)

# Define a softmax function
softmax = lambda x: np.exp(x) / np.sum(np.exp(x))

# Define a binary cross-entropy loss function
bce_loss = lambda y_true, y_pred: -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Define the number of ordinal ranks
K = number_of_ranks

# Define the original training set
X = input_features
Y = ordinal_labels

# Initialize a list of conditional rank probability models
models = []

# For each rank threshold from 1 to K-1
for k in range(1, K):

  # Create a binary training set based on the rank threshold
  X_k = X
  Y_k = (Y <= k).astype(int)

  # Train the DNN on the binary training set using bce_loss
  DNN.fit(X_k, Y_k, loss=bce_loss)

  # Save the trained DNN as a conditional rank probability model
  models.append(DNN)

# Initialize a list of unconditional rank probabilities
probs = []

# For each ordinal rank from 1 to K
for k in range(1, K+1):

  # If k is 1, the unconditional rank probability is equal to the conditional rank probability given by the first model
  if k == 1:
    prob_k = softmax(models[0].predict(X))[0]

  # If k is K, the unconditional rank probability is equal to one minus the conditional rank probability given by the last model
  elif k == K:
    prob_k = 1 - softmax(models[-1].predict(X))[1]

  # Otherwise, the unconditional rank probability is equal to the product of the conditional rank probabilities given by two adjacent models
  else:
    prob_k = softmax(models[k-2].predict(X))[1] * softmax(models[k-1].predict(X))[0]

  # Append the unconditional rank probability to the list
  probs.append(prob_k)

# Return the list of unconditional rank probabilities as the final output
return probs
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import tensorflow as tf

# Define a function to create a DNN with any architecture
def create_dnn(input_size, output_size, hidden_sizes, activation):

  # Create an input layer with the given input size
  inputs = tf.keras.layers.Input(shape=(input_size,))

  # Create a variable to store the output of each layer
  outputs = inputs

  # For each hidden size in the list of hidden sizes
  for hidden_size in hidden_sizes:

    # Create a dense layer with the given hidden size and activation function
    outputs = tf.keras.layers.Dense(hidden_size, activation=activation)(outputs)

  # Create a final dense layer with the given output size and no activation function
  outputs = tf.keras.layers.Dense(output_size)(outputs)

  # Create a DNN model with the input layer and the output layer
  model = tf.keras.Model(inputs=inputs, outputs=outputs)

  # Return the DNN model
  return model

# Define a softmax function
softmax = lambda x: np.exp(x) / np.sum(np.exp(x))

# Define a binary cross-entropy loss function
bce_loss = lambda y_true, y_pred: -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

# Define the number of ordinal ranks
K = number_of_ranks

# Define the original training set
X = input_features
Y = ordinal_labels

# Define the input size and the output size for the DNN
input_size = X.shape[1]
output_size = 2

# Define the list of hidden sizes and the activation function for the DNN
hidden_sizes = [64, 32, 16]
activation = 'relu'

# Define the number of epochs and the batch size for training the DNN
epochs = 100
batch_size = 32

# Initialize a list of conditional rank probability models
models = []

# For each rank threshold from 1 to K-1
for k in range(1, K):

  # Create a binary training set based on the rank threshold
  X_k = X
  Y_k = (Y <= k).astype(int)

  # Create a DNN with the given parameters
  DNN = create_dnn(input_size, output_size, hidden_sizes, activation)

  # Compile the DNN with an optimizer and a loss function
  DNN.compile(optimizer='adam', loss=bce_loss)

  # Train the DNN on the binary training set using bce_loss for the given number of epochs and batch size
  DNN.fit(X_k, Y_k, epochs=epochs, batch_size=batch_size)

  # Save the trained DNN as a conditional rank probability model
  models.append(DNN)

# Initialize a list of unconditional rank probabilities
probs = []

# For each ordinal rank from 1 to K
for k in range(1, K+1):

  # If k is 1, the unconditional rank probability is equal to the conditional rank probability given by the first model
  if k == 1:
    prob_k = softmax(models[0].predict(X))[0]

  # If k is K, the unconditional rank probability is equal to one minus the conditional rank probability given by the last model
  elif k == K:
    prob_k = 1 - softmax(models[-1].predict(X))[1]

  # Otherwise, the unconditional rank probability is equal to the product of the conditional rank probabilities given by two adjacent models
  else:
    prob_k = softmax(models[k-2].predict(X))[1] * softmax(models[k-1].predict(X))[0]

  # Append the unconditional rank probability to the list
  probs.append(prob_k)

# Return the list of unconditional rank probabilities as the final output
return probs
```