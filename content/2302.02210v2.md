---
title: 2302.02210v2 Oscillation-free Quantization for Low-bit Vision Transformers
date: 2023-02-03
---

# [Oscillation-free Quantization for Low-bit Vision Transformers](http://arxiv.org/abs/2302.02210v2)

authors: Shih-Yang Liu, Zechun Liu, Kwang-Ting Cheng


## What, Why and How

[1]: https://arxiv.org/abs/2302.02210 "Oscillation-free Quantization for Low-bit Vision Transformers"
[2]: https://arxiv.org/pdf/2302.02210.pdf "arXiv:2302.02210v1 [cs.CV] 4 Feb 2023"
[3]: https://arxiv-export2.library.cornell.edu/abs/2302.02210v2 "[2302.02210v2] Oscillation-free Quantization for Low-bit Vision ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes three techniques to reduce weight oscillation in quantization-aware training of vision transformers (ViT), namely statistical weight quantization (StatsQ), confidence-guided annealing (CGA), and query-key reparameterization (QKR).
- **Why**: Weight oscillation is a problem that causes quantized weights to frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. The paper discovers that the learnable scaling factor, a common setting in quantization, aggravates weight oscillation. The paper also finds that the interdependence between quantized weights in query and key of a self-attention layer makes ViT vulnerable to oscillation.
- **How**: StatsQ improves quantization robustness by using the statistics of the full-precision weights instead of the learnable scaling factor. CGA freezes the weights with high confidence and calms the oscillating weights. QKR resolves the query-key intertwined oscillation and mitigates the resulting gradient misestimation by reparameterizing the query and key weights. The paper shows that these techniques achieve substantial accuracy improvement on ImageNet for low-bit ViT models.

This summary is based on the abstract[^1^][1] and introduction[^2^][2] of the paper.

## Main Contributions

[1]: https://arxiv.org/abs/2302.02210 "Oscillation-free Quantization for Low-bit Vision Transformers"
[2]: https://arxiv.org/pdf/2302.02210.pdf "arXiv:2302.02210v1 [cs.CV] 4 Feb 2023"
[3]: https://arxiv-export2.library.cornell.edu/abs/2302.02210v2 "[2302.02210v2] Oscillation-free Quantization for Low-bit Vision ..."

The paper lists the following contributions[^1^][2]:

- It reveals the connection between the learnable scaling factor and quantized weight oscillation and shows that the learnable scaling factor exacerbates weight oscillation.
- It proposes a novel statistical weight quantization (StatsQ) method that uses the statistics of the full-precision weights to determine the quantization range and level, which improves quantization robustness compared to the learnable-scale-based method.
- It introduces a confidence-guided annealing (CGA) technique that freezes the weights with high confidence and calms the oscillating weights during quantization-aware training.
- It devises a query-key reparameterization (QKR) technique that resolves the query-key intertwined oscillation in self-attention layers and mitigates the resulting gradient misestimation.
- It demonstrates that these techniques can significantly improve the accuracy of low-bit ViT models on ImageNet and outperform the previous state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/abs/2302.02210 "Oscillation-free Quantization for Low-bit Vision Transformers"
[2]: https://arxiv.org/pdf/2302.02210.pdf "arXiv:2302.02210v1 [cs.CV] 4 Feb 2023"
[3]: https://arxiv-export2.library.cornell.edu/abs/2302.02210v2 "[2302.02210v2] Oscillation-free Quantization for Low-bit Vision ..."

Here is a summary of the method section of the paper[^1^][2]:

- The paper first introduces the quantization-aware training (QAT) framework and the learnable scaling factor that is commonly used to determine the quantization range and level for each weight tensor.
- The paper then analyzes the causes and effects of weight oscillation in QAT and shows that the learnable scaling factor can lead to oscillation due to its gradient dependency on the quantized weights.
- The paper proposes a statistical weight quantization (StatsQ) method that uses the statistics of the full-precision weights, such as mean and standard deviation, to determine the quantization range and level. The paper shows that StatsQ can reduce oscillation and improve quantization robustness compared to the learnable-scale-based method.
- The paper also introduces a confidence-guided annealing (CGA) technique that freezes the weights with high confidence and calms the oscillating weights during QAT. The paper defines the confidence of a weight as the ratio of its distance to the nearest quantized level over its distance to the farthest quantized level. The paper shows that CGA can further improve the accuracy of low-bit ViT models.
- The paper further devises a query-key reparameterization (QKR) technique that resolves the query-key intertwined oscillation in self-attention layers and mitigates the resulting gradient misestimation. The paper reparameterizes the query and key weights as a product of two low-rank matrices and applies StatsQ and CGA to each matrix separately. The paper shows that QKR can significantly reduce oscillation and gradient error in self-attention layers.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the quantization function Q(x) that maps a full-precision value x to a quantized value
# Define the dequantization function Q^{-1}(x) that maps a quantized value x to a full-precision value
# Define the confidence function C(x) that measures the confidence of a quantized value x

# Initialize the full-precision model parameters W
# Initialize the quantization range and level parameters R and L
# Initialize the low-rank matrices U and V for query-key reparameterization

# For each training iteration:
  # Forward pass:
    # For each weight tensor w in W:
      # If w is in a self-attention layer:
        # Reshape w into a matrix shape
        # Apply query-key reparameterization: w = UV^T
        # Apply statistical weight quantization (StatsQ) to U and V separately:
          # Compute the statistics of U and V, such as mean and standard deviation
          # Compute the quantization range and level parameters R and L based on the statistics
          # Quantize U and V using Q(U) and Q(V) with R and L
        # Dequantize U and V using Q^{-1}(U) and Q^{-1}(V) with R and L
        # Compute the dequantized weight matrix w' = Q^{-1}(U)V^T
      # Else:
        # Apply StatsQ to w:
          # Compute the statistics of w, such as mean and standard deviation
          # Compute the quantization range and level parameters R and L based on the statistics
          # Quantize w using Q(w) with R and L
        # Dequantize w using Q^{-1}(w) with R and L
        # Compute the dequantized weight tensor w'
    # Use w' instead of w to compute the model output y
    # Compute the loss function L(y, y*) with respect to the target y*
  
  # Backward pass:
    # Compute the gradients of L with respect to w' using backpropagation
    # For each weight tensor w in W:
      # If w is in a self-attention layer:
        # Reshape w into a matrix shape
        # Apply query-key reparameterization: w = UV^T
        # Apply confidence-guided annealing (CGA) to U and V separately:
          # Compute the confidence of U and V using C(U) and C(V)
          # Freeze the elements of U and V with high confidence by setting their gradients to zero
          # Anneal the elements of U and V with low confidence by scaling their gradients by a factor alpha < 1
        # Compute the gradients of L with respect to U and V using chain rule
      # Else:
        # Apply CGA to w:
          # Compute the confidence of w using C(w)
          # Freeze the elements of w with high confidence by setting their gradients to zero
          # Anneal the elements of w with low confidence by scaling their gradients by a factor alpha < 1
        # Compute the gradients of L with respect to w using chain rule
  
  # Update step:
    # Update W using gradient descent or other optimization algorithms

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Define the hyperparameters
num_bits = 2 # number of bits for quantization
alpha = 0.9 # annealing factor for CGA
num_epochs = 100 # number of training epochs
batch_size = 256 # batch size for training
learning_rate = 0.01 # learning rate for optimization

# Define the quantization function Q(x) that maps a full-precision value x to a quantized value
# Assume x is a tensor of shape [N, C, H, W] or [N, C]
# Assume R and L are tensors of shape [C] that store the quantization range and level parameters for each channel
def Q(x, R, L):
  # Clip x to the range [-R, R]
  x = torch.clamp(x, -R, R)
  # Scale x to the range [0, 2^num_bits - 1]
  x = (x + R) / (2 * R) * (2 ** num_bits - 1)
  # Round x to the nearest integer
  x = torch.round(x)
  # Cast x to integer type
  x = x.to(torch.int)
  # Return the quantized tensor x
  return x

# Define the dequantization function Q^{-1}(x) that maps a quantized value x to a full-precision value
# Assume x is a tensor of shape [N, C, H, W] or [N, C]
# Assume R and L are tensors of shape [C] that store the quantization range and level parameters for each channel
def Q_inv(x, R, L):
  # Cast x to float type
  x = x.to(torch.float)
  # Scale x back to the range [-R, R]
  x = (x / (2 ** num_bits - 1)) * (2 * R) - R
  # Return the dequantized tensor x
  return x

# Define the confidence function C(x) that measures the confidence of a quantized value x
# Assume x is a tensor of shape [N, C, H, W] or [N, C]
# Assume R and L are tensors of shape [C] that store the quantization range and level parameters for each channel
def C(x, R, L):
  # Compute the distance of x to the nearest quantized level
  d_min = torch.min(torch.abs(x - L), dim=-1)[0]
  # Compute the distance of x to the farthest quantized level
  d_max = torch.max(torch.abs(x - L), dim=-1)[0]
  # Compute the confidence of x as the ratio of d_min over d_max
  c = d_min / d_max
  # Return the confidence tensor c
  return c

# Define the StatsQ method that uses the statistics of the full-precision weights to determine the quantization range and level parameters
# Assume w is a tensor of shape [N, C, H, W] or [N, C] that represents a full-precision weight tensor
def StatsQ(w):
  # Compute the mean and standard deviation of w along the channel dimension
  mean = torch.mean(w, dim=-1)
  std = torch.std(w, dim=-1)
  # Compute the quantization range parameter R as k times the standard deviation plus a small epsilon
  k = 2 ** (num_bits - 1) - 0.5 # k is chosen such that R covers at least 99.99% of the data for num_bits >=2 
  epsilon = 1e-5 # epsilon is added to avoid zero division
  R = k * std + epsilon
  # Compute the quantization level parameter L as a linearly spaced vector from -R to R with 2^num_bits elements
  L = torch.linspace(-R, R, steps=2 ** num_bits)
  # Quantize w using Q(w) with R and L
  w_q = Q(w, R, L)
  # Dequantize w using Q^{-1}(w) with R and L
  w_dq = Q_inv(w_q, R, L)
  # Return the dequantized weight tensor w_dq and the quantization range and level parameters R and L
  return w_dq, R, L

# Define the CGA method that freezes the weights with high confidence and calms the oscillating weights during QAT
# Assume w is a tensor of shape [N, C, H, W] or [N, C] that represents a quantized weight tensor
# Assume dw is a tensor of shape [N, C, H, W] or [N, C] that represents the gradient of the loss with respect to w
# Assume R and L are tensors of shape [C] that store the quantization range and level parameters for each channel
def CGA(w, dw, R, L):
  # Compute the confidence of w using C(w) with R and L
  c = C(w, R, L)
  # Freeze the elements of w with high confidence by setting their gradients to zero
  dw[c > 0.5] = 0
  # Anneal the elements of w with low confidence by scaling their gradients by a factor alpha < 1
  dw[c <= 0.5] = alpha * dw[c <= 0.5]
  # Return the modified gradient tensor dw
  return dw

# Define the QKR method that reparameterizes the query and key weights as a product of two low-rank matrices and applies StatsQ and CGA to each matrix separately
# Assume w is a tensor of shape [N, C, H, W] or [N, C] that represents a full-precision weight tensor for query or key in a self-attention layer
# Assume dw is a tensor of shape [N, C, H, W] or [N, C] that represents the gradient of the loss with respect to w
def QKR(w, dw):
  # Reshape w into a matrix shape of [N, C]
  w = w.view(N, C)
  # Apply query-key reparameterization: w = UV^T, where U and V are matrices of shape [N, C/2]
  U = w[:, :C/2]
  V = w[:, C/2:]
  # Apply StatsQ to U and V separately and get the dequantized matrices U_dq and V_dq and the quantization parameters R_U, L_U, R_V, and L_V
  U_dq, R_U, L_U = StatsQ(U)
  V_dq, R_V, L_V = StatsQ(V)
  # Compute the dequantized weight matrix w_dq = U_dqV_dq^T
  w_dq = torch.matmul(U_dq, V_dq.T)
  # Reshape w_dq back to the original shape of w
  w_dq = w_dq.view(*w.shape)
  # Reshape dw into a matrix shape of [N, C]
  dw = dw.view(N, C)
  # Compute the gradients of the loss with respect to U and V using chain rule: dU = d(UV^T)V = dwV and dV = U^Td(UV^T) = U^Tdw
  dU = torch.matmul(dw, V)
  dV = torch.matmul(U.T, dw)
  # Apply CGA to U and V separately and get the modified gradients dU and dV
  dU = CGA(U, dU, R_U, L_U)
  dV = CGA(V, dV, R_V, L_V)
  # Compute the modified gradient of the loss with respect to w using chain rule: dw = dU * V^T + U * dV^T
  dw = torch.matmul(dU * V.T) + torch.matmul(U * dV.T)
  # Reshape dw back to the original shape of w
  dw = dw.view(*w.shape)
  # Return the dequantized weight tensor w_dq and the modified gradient tensor dw
  return w_dq, dw

# Load the ImageNet dataset and apply data augmentation and normalization
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
])
test_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
])
train_dataset = datasets.ImageNet(root='data', split='train', transform=train_transform)
test_dataset = datasets.ImageNet(root='data', split='val', transform=test_transform)

# Create data loaders for training and testing
train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=batch_size