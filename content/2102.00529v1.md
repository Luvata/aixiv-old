---
title: 2102.00529v1 Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers
date: 2021-02-01
---

# [Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](http://arxiv.org/abs/2102.00529v1)

authors: Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, Aida Nematzadeh


## What, Why and How

[1]: https://arxiv.org/abs/2102.00529v1 "[2102.00529v1] Decoupling the Role of Data, Attention, and Losses in ..."
[2]: https://arxiv.org/pdf/2102.00529v1.pdf "arXiv:2102.00529v1 [cs.CL] 31 Jan 2021"
[3]: https://arxiv.org/pdf/2101.00529v1.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: The paper investigates how pretraining data, attention mechanism, and loss functions affect the performance of multimodal transformers on zero-shot image retrieval tasks.

Why: The paper aims to understand the factors that contribute to the success of multimodal transformers, which have shown impressive results on various language and vision tasks.

How: The paper experiments with six different pretraining datasets, two types of attention mechanisms (modality-specific and multimodal), and three types of loss functions (masked language modeling, contrastive, and masked region modeling). The paper evaluates the models on Flickr30k and MSCOCO image retrieval tasks.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2102.00529v1.pdf "arXiv:2102.00529v1 [cs.CL] 31 Jan 2021"
[2]: https://arxiv.org/abs/2102.00529 "[2102.00529] Decoupling the Role of Data, Attention, and Losses in ..."
[3]: https://arxiv.org/pdf/2101.00529v1.pdf "arXiv.org e-Print archive"

Some of the contributions of this paper are:

- It establishes a baseline model that is trained on the same pretraining data as multimodal transformers but with a different architecture.
- It compares the performance of multimodal transformers on six different pretraining datasets and analyzes the impact of dataset noise and language similarity on zero-shot image retrieval tasks.
- It studies the effect of attention mechanisms on multimodal transformers and shows that multimodal attention can outperform modality-specific attention.
- It evaluates the role of loss functions on multimodal transformers and shows that contrastive losses do not improve performance as much as masked language modeling and masked region modeling losses.

## Method Summary

[1]: https://arxiv.org/pdf/2102.00529v1.pdf "arXiv:2102.00529v1 [cs.CL] 31 Jan 2021"
[2]: https://arxiv.org/abs/2102.00529 "[2102.00529] Decoupling the Role of Data, Attention, and Losses in ..."
[3]: https://arxiv.org/pdf/2101.00529v1.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

```
The paper uses three types of models to compare different aspects of multimodal transformers: a baseline model (BL) that uses a two-stream architecture with separate encoders for image and text, a modality-specific attention model (MSA) that uses a transformer with separate self-attention heads for image and text, and a multimodal attention model (MMA) that uses a transformer with cross-modal attention heads. The paper also varies the pretraining data, the attention mechanism, and the loss functions for each model. The paper uses six pretraining datasets: Conceptual Captions (CC), CC+STAIR Captions (CC+STAIR), CC+Open Images (CC+OI), CC+OI+STAIR, CC+OI+STAIR+SBU, and CC+OI+STAIR+SBU+Flickr30k. The paper uses two types of attention mechanisms: modality-specific and multimodal. The paper uses three types of loss functions: masked language modeling (MLM), contrastive (CON), and masked region modeling (MRM). The paper evaluates the models on zero-shot image retrieval tasks on Flickr30k and MSCOCO datasets.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2102.00529v1.pdf "arXiv:2102.00529v1 [cs.CL] 31 Jan 2021"
[2]: https://arxiv.org/abs/2102.00529 "[2102.00529] Decoupling the Role of Data, Attention, and Losses in ..."
[3]: https://arxiv.org/pdf/2101.00529v1.pdf "arXiv.org e-Print archive"

Here is a high-level pseudo code for the paper:

```
# Define three types of models: BL, MSA, and MMA
BL = TwoStreamEncoder(image_encoder, text_encoder)
MSA = Transformer(image_encoder, text_encoder, modality_specific_attention)
MMA = Transformer(image_encoder, text_encoder, multimodal_attention)

# Define six pretraining datasets: CC, CC+STAIR, CC+OI, CC+OI+STAIR, CC+OI+STAIR+SBU, and CC+OI+STAIR+SBU+Flickr30k
datasets = [CC, CC+STAIR, CC+OI, CC+OI+STAIR, CC+OI+STAIR+SBU, CC+OI+STAIR+SBU+Flickr30k]

# Define three loss functions: MLM, CON, and MRM
losses = [MLM, CON, MRM]

# For each model type
for model in [BL, MSA, MMA]:
  # For each dataset
  for dataset in datasets:
    # For each loss function
    for loss in losses:
      # Pretrain the model on the dataset using the loss function
      model.pretrain(dataset, loss)
      # Evaluate the model on zero-shot image retrieval tasks on Flickr30k and MSCOCO
      model.evaluate(Flickr30k, MSCOCO)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2102.00529v1.pdf "arXiv:2102.00529v1 [cs.CL] 31 Jan 2021"
[2]: https://arxiv.org/abs/2102.00529 "[2102.00529] Decoupling the Role of Data, Attention, and Losses in ..."
[3]: https://arxiv.org/pdf/2101.00529v1.pdf "arXiv.org e-Print archive"

Here is a detailed pseudo code to implement the paper:

```
# Define the image encoder as a ResNet-50 model
image_encoder = ResNet50()

# Define the text encoder as a BERT model
text_encoder = BERT()

# Define the modality-specific attention as a transformer layer with separate self-attention heads for image and text
modality_specific_attention = TransformerLayer(image_self_attention, text_self_attention)

# Define the multimodal attention as a transformer layer with cross-modal attention heads for image and text
multimodal_attention = TransformerLayer(image_text_attention, text_image_attention)

# Define the masked language modeling loss as the cross-entropy loss between the predicted and true tokens
MLM = CrossEntropyLoss()

# Define the contrastive loss as the negative log-likelihood of matching image-text pairs
CON = NLLLoss()

# Define the masked region modeling loss as the cross-entropy loss between the predicted and true regions
MRM = CrossEntropyLoss()

# Define the baseline model as a two-stream encoder with separate image and text encoders
BL = TwoStreamEncoder(image_encoder, text_encoder)

# Define the modality-specific attention model as a transformer with separate self-attention heads for image and text
MSA = Transformer(image_encoder, text_encoder, modality_specific_attention)

# Define the multimodal attention model as a transformer with cross-modal attention heads for image and text
MMA = Transformer(image_encoder, text_encoder, multimodal_attention)

# Define six pretraining datasets: CC, CC+STAIR, CC+OI, CC+OI+STAIR, CC+OI+STAIR+SBU, and CC+OI+STAIR+SBU+Flickr30k
datasets = [CC, CC+STAIR, CC+OI, CC+OI+STAIR, CC+OI+STAIR+SBU, CC+OI+STAIR+SBU+Flickr30k]

# Define three loss functions: MLM, CON, and MRM
losses = [MLM, CON, MRM]

# For each model type
for model in [BL, MSA, MMA]:
  # For each dataset
  for dataset in datasets:
    # For each loss function
    for loss in losses:
      # Initialize an optimizer
      optimizer = Adam(model.parameters())
      # For each batch of image-text pairs in the dataset
      for batch in dataset:
        # Get the images and texts from the batch
        images, texts = batch
        # Forward pass the images and texts through the model
        outputs = model(images, texts)
        # Compute the loss based on the outputs and the loss function
        loss_value = loss(outputs)
        # Backward pass the loss to update the gradients
        loss_value.backward()
        # Update the model parameters using the optimizer
        optimizer.step()
      # Evaluate the model on zero-shot image retrieval tasks on Flickr30k and MSCOCO
      model.evaluate(Flickr30k, MSCOCO)
```