---
title: 2202.04200v1 MaskGIT  Masked Generative Image Transformer
date: 2022-02-05
---

# [MaskGIT: Masked Generative Image Transformer](http://arxiv.org/abs/2202.04200v1)

authors: Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, William T. Freeman


## What, Why and How

[1]: https://arxiv.org/abs/2202.04200 "[2202.04200] MaskGIT: Masked Generative Image Transformer - arXiv.org"
[2]: https://www.marktechpost.com/2022/03/22/google-research-proposes-maskgit-a-new-deep-learning-technique-based-on-bi-directional-generative-transformers-for-high-quality-and-fast-image-synthesis/ "Google Research Proposes MaskGIT: A New Deep Learning ... - MarkTechPost"
[3]: https://arxiv.org/pdf/2202.04200.pdf "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/2211.04200v1 "[2211.04200v1] Intelligent Surface Enabled Sensing-Assisted Communication"

Here is a summary of the paper:

- **What**: The paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which is called MaskGIT. MaskGIT can generate high-fidelity and high-resolution images by predicting randomly masked tokens in all directions, and can also perform various image editing tasks such as inpainting, extrapolation, and manipulation.
- **Why**: The paper aims to overcome the limitations of existing generative transformer models, which treat an image naively as a sequence of tokens and decode an image sequentially following the raster scan ordering. This strategy is neither optimal nor efficient for image synthesis, as it ignores the spatial structure and correlation of images, and requires a large number of decoding steps.
- **How**: The paper introduces a bidirectional transformer decoder that can attend to tokens in all directions during training and inference. During training, MaskGIT learns to predict randomly masked tokens by conditioning on the unmasked tokens. At inference time, MaskGIT generates all tokens of an image simultaneously in the first iteration, and then refines the image iteratively conditioned on the previous generation. The paper evaluates MaskGIT on the ImageNet dataset and shows that it outperforms the state-of-the-art transformer model in terms of image quality and diversity. The paper also demonstrates that MaskGIT can be easily extended to various image editing tasks by modifying the masking scheme and the conditioning inputs.

The summary is based on the information from [^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2202.04200 "[2202.04200] MaskGIT: Masked Generative Image Transformer - arXiv.org"
[2]: https://www.marktechpost.com/2022/03/22/google-research-proposes-maskgit-a-new-deep-learning-technique-based-on-bi-directional-generative-transformers-for-high-quality-and-fast-image-synthesis/ "Google Research Proposes MaskGIT: A New Deep Learning ... - MarkTechPost"
[3]: https://arxiv.org/pdf/2202.04200.pdf "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/2211.04200v1 "[2211.04200v1] Intelligent Surface Enabled Sensing-Assisted Communication"

The paper claims the following contributions:

- **The first bidirectional generative transformer for image synthesis**: The paper introduces MaskGIT, a novel image synthesis paradigm that uses a bidirectional transformer decoder to predict randomly masked tokens in all directions. MaskGIT can generate high-fidelity and high-resolution images by leveraging the spatial structure and correlation of images.
- **The first iterative refinement scheme for generative transformers**: The paper proposes a novel inference scheme that generates all tokens of an image simultaneously in the first iteration, and then refines the image iteratively conditioned on the previous generation. This scheme can accelerate autoregressive decoding by up to 64x and improve image quality and diversity.
- **The first general framework for image editing with generative transformers**: The paper shows that MaskGIT can be easily extended to various image editing tasks by modifying the masking scheme and the conditioning inputs. The paper demonstrates that MaskGIT can perform inpainting, extrapolation, and manipulation with high quality and flexibility.

The contributions are based on the information from [^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/2202.04200 "[2202.04200] MaskGIT: Masked Generative Image Transformer - arXiv.org"
[2]: https://www.marktechpost.com/2022/03/22/google-research-proposes-maskgit-a-new-deep-learning-technique-based-on-bi-directional-generative-transformers-for-high-quality-and-fast-image-synthesis/ "Google Research Proposes MaskGIT: A New Deep Learning ... - MarkTechPost"
[3]: https://arxiv.org/pdf/2202.04200.pdf "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/2303.04200v1 "[2303.04200v1] Stratified Vector Bundles: Examples and Constructions"

Here is a summary of the method section of the paper:

- **Model architecture**: The paper adopts the transformer architecture as the backbone of MaskGIT. The model consists of an encoder and a decoder, both of which are composed of multiple layers of self-attention and feed-forward networks. The encoder takes an image as input and encodes it into a sequence of tokens. The decoder takes a sequence of masked tokens as input and predicts the unmasked tokens by attending to the encoder output and the decoder input. The paper uses a patch-based tokenization scheme to convert an image into a sequence of tokens, and uses a discrete variational autoencoder (DVAE) to compress the token space. The paper also introduces a novel positional encoding scheme that encodes both the absolute and relative positions of tokens in an image.
- **Training objective**: The paper uses a masked language modeling (MLM) objective to train MaskGIT. The objective is to maximize the likelihood of predicting randomly masked tokens given the unmasked tokens. The paper uses a dynamic masking scheme that randomly masks a certain percentage of tokens in each iteration. The paper also applies label smoothing and dropout to regularize the training process.
- **Inference scheme**: The paper proposes a novel inference scheme that generates an image iteratively using MaskGIT. In the first iteration, the model generates all tokens of an image simultaneously by masking all tokens and predicting them from scratch. In the subsequent iterations, the model refines the image by masking a subset of tokens and predicting them conditioned on the previous generation. The paper uses a progressive masking scheme that gradually decreases the masking ratio in each iteration. The paper also introduces a temperature annealing scheme that gradually decreases the temperature of the softmax function in each iteration to sharpen the output distribution.

The summary is based on the information from [^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder and decoder of MaskGIT
encoder = TransformerEncoder()
decoder = TransformerDecoder()

# Define the patch-based tokenization and DVAE
tokenizer = PatchTokenizer()
dvae = DiscreteVAE()

# Define the positional encoding scheme
pos_encoder = PositionalEncoder()

# Define the hyperparameters
num_iterations = 10 # number of inference iterations
masking_ratio = 0.8 # initial masking ratio
masking_decay = 0.1 # masking ratio decay rate
temperature = 1.0 # initial temperature
temperature_decay = 0.1 # temperature decay rate

# Define the input image
image = load_image()

# Tokenize the image and encode it with DVAE
tokens = tokenizer.tokenize(image)
tokens = dvae.encode(tokens)

# Encode the tokens with positional encoding and encoder
tokens = pos_encoder.encode(tokens)
encoder_output = encoder(tokens)

# Initialize the decoder input as masked tokens
decoder_input = mask_all(tokens)

# Iterate over the inference iterations
for i in range(num_iterations):
  # Decode the masked tokens with decoder
  decoder_output = decoder(decoder_input, encoder_output)

  # Sample the unmasked tokens from the decoder output with temperature
  unmasked_tokens = sample(decoder_output, temperature)

  # Update the decoder input by replacing the masked tokens with unmasked tokens
  decoder_input = replace(decoder_input, unmasked_tokens)

  # Update the masking ratio and temperature by applying decay rates
  masking_ratio = masking_ratio - masking_decay
  temperature = temperature - temperature_decay

  # Mask a subset of tokens according to the masking ratio
  decoder_input = mask_subset(decoder_input, masking_ratio)

# Decode the final tokens with DVAE and detokenize them to get the output image
output_tokens = dvae.decode(decoder_input)
output_image = tokenizer.detokenize(output_tokens)

# Display the output image
display(output_image)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the encoder and decoder of MaskGIT
# The encoder and decoder are composed of multiple layers of self-attention and feed-forward networks
# The encoder and decoder use layer normalization and residual connections
# The encoder and decoder use a shared embedding matrix for the tokens
class TransformerEncoder(torch.nn.Module):
  def __init__(self, num_layers, num_heads, hidden_size, dropout):
    super().__init__()
    # Define the embedding matrix for the tokens
    self.embedding = torch.nn.Embedding(num_embeddings=8192, embedding_dim=hidden_size)
    # Define the encoder layers
    self.layers = torch.nn.ModuleList([TransformerEncoderLayer(num_heads, hidden_size, dropout) for _ in range(num_layers)])

  def forward(self, tokens):
    # Embed the tokens
    x = self.embedding(tokens)
    # Apply the encoder layers
    for layer in self.layers:
      x = layer(x)
    # Return the encoder output
    return x

class TransformerDecoder(torch.nn.Module):
  def __init__(self, num_layers, num_heads, hidden_size, dropout):
    super().__init__()
    # Define the embedding matrix for the tokens (shared with the encoder)
    self.embedding = torch.nn.Embedding(num_embeddings=8192, embedding_dim=hidden_size)
    # Define the decoder layers
    self.layers = torch.nn.ModuleList([TransformerDecoderLayer(num_heads, hidden_size, dropout) for _ in range(num_layers)])

  def forward(self, tokens, encoder_output):
    # Embed the tokens
    x = self.embedding(tokens)
    # Apply the decoder layers
    for layer in self.layers:
      x = layer(x, encoder_output)
    # Return the decoder output
    return x

class TransformerEncoderLayer(torch.nn.Module):
  def __init__(self, num_heads, hidden_size, dropout):
    super().__init__()
    # Define the self-attention sublayer
    self.self_attention = MultiHeadAttention(num_heads, hidden_size, dropout)
    # Define the feed-forward sublayer
    self.feed_forward = FeedForward(hidden_size, dropout)
    # Define the layer normalization
    self.layer_norm = torch.nn.LayerNorm(hidden_size)

  def forward(self, x):
    # Apply the self-attention sublayer with residual connection and layer normalization
    x = self.layer_norm(x + self.self_attention(x))
    # Apply the feed-forward sublayer with residual connection and layer normalization
    x = self.layer_norm(x + self.feed_forward(x))
    # Return the output
    return x

class TransformerDecoderLayer(torch.nn.Module):
  def __init__(self, num_heads, hidden_size, dropout):
    super().__init__()
    # Define the masked self-attention sublayer
    self.masked_self_attention = MultiHeadAttention(num_heads, hidden_size, dropout)
    # Define the cross-attention sublayer
    self.cross_attention = MultiHeadAttention(num_heads, hidden_size, dropout)
    # Define the feed-forward sublayer
    self.feed_forward = FeedForward(hidden_size, dropout)
    # Define the layer normalization
    self.layer_norm = torch.nn.LayerNorm(hidden_size)

  def forward(self, x, encoder_output):
    # Apply the masked self-attention sublayer with residual connection and layer normalization
    x = self.layer_norm(x + self.masked_self_attention(x))
    # Apply the cross-attention sublayer with residual connection and layer normalization
    x = self.layer_norm(x + self.cross_attention(x, encoder_output))
    # Apply the feed-forward sublayer with residual connection and layer normalization
    x = self.layer_norm(x + self.feed_forward(x))
    # Return the output
    return x

class MultiHeadAttention(torch.nn.Module):
  def __init__(self, num_heads, hidden_size, dropout):
    super().__init__()
    # Define the projection matrices for query, key and value
    self.query_proj = torch.nn.Linear(hidden_size, hidden_size)
    self.key_proj = torch.nn.Linear(hidden_size, hidden_size)
    self.value_proj = torch.nn.Linear(hidden_size, hidden_size)
    # Define the output projection matrix
    self.output_proj = torch.nn.Linear(hidden_size, hidden_size)
    # Define the number of heads and head size
    self.num_heads = num_heads
    self.head_size = hidden_size // num_heads
    # Define the dropout layer
    self.dropout = torch.nn.Dropout(dropout)

  def forward(self, query, key=None, value=None):
    

```