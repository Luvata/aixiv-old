---
title: 1512.04150v1 Learning Deep Features for Discriminative Localization
date: 2015-12-05
---

# [Learning Deep Features for Discriminative Localization](http://arxiv.org/abs/1512.04150v1)

authors: Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba


## What, Why and How

[1]: https://arxiv.org/abs/1512.04150 "[1512.04150] Learning Deep Features for Discriminative Localization"
[2]: https://arxiv.org/pdf/1512.04150v1.pdf "Learning Deep Features for Discriminative Localization - arXiv.org"
[3]: http://export.arxiv.org/abs/1511.04150v1 "[1511.04150v1] Deep Mean Maps - export.arxiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a technique called **class activation mapping (CAM)** that allows a convolutional neural network (CNN) trained on image-level labels to perform object localization and explain its predictions.
- **Why**: The paper aims to address the limitations of existing methods for weakly-supervised object localization and visualization of CNNs, such as relying on additional supervision, post-processing steps, or specific network architectures.
- **How**: The paper leverages the global average pooling layer in CNNs to generate a class-specific activation map that highlights the discriminative image regions for each class. The paper also shows how CAM can be used for various tasks such as action recognition, scene recognition, and fine-grained recognition.

## Main Contributions

[1]: https://arxiv.org/abs/1512.04150 "[1512.04150] Learning Deep Features for Discriminative Localization"
[2]: https://arxiv.org/pdf/1512.04150v1.pdf "Learning Deep Features for Discriminative Localization - arXiv.org"
[3]: http://export.arxiv.org/abs/1511.04150v1 "[1511.04150v1] Deep Mean Maps - export.arxiv.org"

According to the paper[^1^][1], the main contributions are:

- **A simple technique to enable a CNN trained on image-level labels to perform object localization and explain its predictions** by generating a class-specific activation map that highlights the discriminative image regions for each class.
- **A comprehensive evaluation of the proposed technique on various tasks and datasets** such as object localization on ILSVRC 2014, action recognition on Stanford 40 Actions, scene recognition on SUN397, and fine-grained recognition on CUB-200-2011.
- **A demonstration of the generalizability and transferability of the proposed technique** by applying it to different network architectures, different levels of supervision, and different domains of images.

## Method Summary

[1]: https://arxiv.org/abs/1512.04150 "[1512.04150] Learning Deep Features for Discriminative Localization"
[2]: https://arxiv.org/pdf/1512.04150v1.pdf "Learning Deep Features for Discriminative Localization - arXiv.org"
[3]: http://export.arxiv.org/abs/1511.04150v1 "[1511.04150v1] Deep Mean Maps - export.arxiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a technique called **class activation mapping (CAM)** that generates a class-specific activation map for each image by using the weights of the last convolutional layer as the importance of each unit for a given class.
- The paper modifies the global average pooling layer in CNNs to make it compatible with CAM by replacing the fully-connected layer with a convolutional layer with **C** filters, where **C** is the number of classes, and applying global average pooling on the output feature maps.
- The paper shows how CAM can be used for various tasks such as object localization, action recognition, scene recognition, and fine-grained recognition by applying a threshold on the activation map to obtain a bounding box or a segmentation mask for the predicted class.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image I and a CNN model M trained on image-level labels
# Output: a class-specific activation map A and a predicted class label c

# Step 1: Modify the global average pooling layer in M
# Replace the fully-connected layer with a convolutional layer with C filters
# Apply global average pooling on the output feature maps

# Step 2: Forward pass the image I through M
# Obtain the output feature maps F of shape (H, W, C)
# Obtain the class scores S of shape (C)

# Step 3: Compute the class activation mapping A
# A = zeros(H, W) # initialize an empty activation map
# c = argmax(S) # get the predicted class label
# for i in range(H):
#     for j in range(W):
#         A[i][j] = F[i][j][c] # assign the activation value of the predicted class

# Step 4: Return A and c
return A, c
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing and pre-trained models
import numpy as np # for numerical operations
import cv2 # for computer vision tasks

# Define some constants
IMAGE_SIZE = 224 # the input image size for the CNN model
THRESHOLD = 0.5 # the threshold for obtaining a bounding box from the activation map

# Load a pre-trained CNN model from torchvision
model = torchvision.models.resnet18(pretrained=True) # use ResNet-18 as an example

# Modify the global average pooling layer in the model
num_classes = model.fc.in_features # get the number of classes from the original fully-connected layer
model.fc = torch.nn.Conv2d(num_classes, num_classes, kernel_size=1) # replace the fully-connected layer with a 1x1 convolutional layer
model.avgpool = torch.nn.Identity() # remove the global average pooling layer

# Load an image from a file
image = cv2.imread("image.jpg") # read the image as a numpy array
image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE)) # resize the image to match the input size of the model
image = image / 255.0 # normalize the pixel values to [0, 1]
image = image.transpose(2, 0, 1) # transpose the image to have the channel dimension first
image = torch.from_numpy(image).float() # convert the image to a torch tensor
image = image.unsqueeze(0) # add a batch dimension to the image

# Forward pass the image through the model
model.eval() # set the model to evaluation mode
with torch.no_grad(): # disable gradient computation
    feature_maps, class_scores = model(image) # get the output feature maps and class scores from the model
    feature_maps = feature_maps.squeeze(0) # remove the batch dimension from the feature maps
    class_scores = class_scores.squeeze(0) # remove the batch dimension from the class scores

# Compute the class activation mapping
activation_map = torch.zeros(IMAGE_SIZE, IMAGE_SIZE) # initialize an empty activation map
predicted_class = torch.argmax(class_scores) # get the predicted class label
for i in range(IMAGE_SIZE):
    for j in range(IMAGE_SIZE):
        activation_map[i][j] = feature_maps[predicted_class][i][j] # assign the activation value of the predicted class

# Obtain a bounding box from the activation map
activation_map = activation_map.numpy() # convert the activation map to a numpy array
activation_map = (activation_map - np.min(activation_map)) / (np.max(activation_map) - np.min(activation_map)) # normalize the activation map to [0, 1]
mask = activation_map > THRESHOLD # create a binary mask from the activation map using the threshold
contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # find the contours of the mask
bounding_box = cv2.boundingRect(contours[0]) # get the bounding box of the largest contour

# Return the activation map, predicted class label and bounding box
return activation_map, predicted_class, bounding_box
```