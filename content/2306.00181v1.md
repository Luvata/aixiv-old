---
title: 2306.00181v1 Conditionally Strongly Log-Concave Generative Models
date: 2023-06-01
---

# [Conditionally Strongly Log-Concave Generative Models](http://arxiv.org/abs/2306.00181v1)

authors: Florentin Guth, Etienne Lempereur, Joan Bruna, St√©phane Mallat


## What, Why and How

[1]: https://arxiv.org/abs/2306.00181 "[2306.00181] Conditionally Strongly Log-Concave Generative Models"
[2]: https://arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."
[3]: http://export.arxiv.org/abs/2303.00181v1 "[2303.00181v1] Selectively Hard Negative Mining for Alleviating ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces **conditionally strongly log-concave (CSLC) models**, which factorize the data distribution into a product of conditional probability distributions that are strongly log-concave.
- **Why**: The paper aims to **bridge the gap** between deep image generative models and classical algorithms that offer theoretical guarantees. The former suffer from mode collapse or memorization issues, limiting their application to scientific data. The latter require restrictive assumptions such as log-concavity to escape the curse of dimensionality.
- **How**: The paper obtains the factorization with **orthogonal projectors** adapted to the data distribution. It leads to efficient parameter estimation and sampling algorithms, with theoretical guarantees, although the data distribution is not globally log-concave. The paper shows that several challenging multiscale processes are conditionally log-concave using wavelet packet orthogonal projectors. The paper also presents numerical results for physical fields such as the model and weak lensing convergence maps with higher resolution than in previous works.

## Main Contributions

According to the paper, the main contributions are:

- The definition and analysis of **conditionally strongly log-concave (CSLC) models**, which generalize the notion of log-concavity to conditional distributions.
- The design and implementation of **orthogonal projectors** that adapt to the data distribution and enable the factorization into CSLC models.
- The derivation of **efficient algorithms** for parameter estimation and sampling from CSLC models, with theoretical guarantees on the convergence rate and sample quality.
- The demonstration of the **applicability** of CSLC models to several challenging multiscale processes, such as model and weak lensing convergence maps, with higher resolution than in previous works.

## Method Summary

The method section of the paper consists of four subsections:

- **Orthogonal projectors**: The paper introduces a class of orthogonal projectors that are adapted to the data distribution and preserve its main features. The paper shows how to construct these projectors using wavelet packets and how to optimize them using gradient descent.
- **Conditionally strongly log-concave models**: The paper defines CSLC models as a product of conditional probability distributions that are strongly log-concave. The paper proves that CSLC models are well-defined and have desirable properties such as smoothness, convexity, and sub-Gaussianity.
- **Parameter estimation**: The paper proposes an algorithm for estimating the parameters of CSLC models using maximum likelihood. The paper shows that the algorithm converges at a linear rate and attains the optimal statistical accuracy up to a logarithmic factor.
- **Sampling**: The paper develops an algorithm for sampling from CSLC models using rejection sampling. The paper shows that the algorithm has a constant rejection rate and generates samples that are close to the true distribution in Wasserstein distance.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: data samples X = {x_1, ..., x_n} in R^d
# Output: a CSLC model p(x) = p_0(x) * prod_{k=1}^K p_k(x | P_k x)

# Step 1: Learn orthogonal projectors P_1, ..., P_K using wavelet packets
P = [] # list of projectors
for k in range(1, K+1):
  # Initialize P_k randomly
  P_k = random_orthogonal_projector(d)
  # Optimize P_k using gradient descent
  for t in range(max_iterations):
    # Compute the gradient of the objective function
    grad = compute_gradient(P_k, X)
    # Update P_k using a learning rate alpha
    P_k = P_k - alpha * grad
  # Add P_k to the list of projectors
  P.append(P_k)

# Step 2: Estimate the parameters of p_0 and p_1, ..., p_K using maximum likelihood
# Initialize the parameters randomly
theta_0 = random_parameters(p_0)
theta = [random_parameters(p_k) for k in range(1, K+1)]
# Optimize the parameters using gradient descent
for t in range(max_iterations):
  # Compute the gradient of the log-likelihood function
  grad_0 = compute_gradient(theta_0, X, p_0)
  grad = [compute_gradient(theta_k, X, p_k, P_k) for k in range(1, K+1)]
  # Update the parameters using a learning rate alpha
  theta_0 = theta_0 + alpha * grad_0
  theta = [theta_k + alpha * grad_k for k in range(1, K+1)]

# Step 3: Sample from the CSLC model using rejection sampling
# Set a constant M such that p(x) <= M * q(x) for all x, where q is a proposal distribution
M = compute_M(p, q)
# Generate samples from the CSLC model
samples = [] # list of samples
while len(samples) < n_samples:
  # Generate a sample x from q
  x = sample_from_q()
  # Generate a uniform random number u in [0, 1]
  u = random_uniform(0, 1)
  # Accept x with probability p(x) / (M * q(x))
  if u <= p(x) / (M * q(x)):
    samples.append(x)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: data samples X = {x_1, ..., x_n} in R^d
# Output: a CSLC model p(x) = p_0(x) * prod_{k=1}^K p_k(x | P_k x)

# Step 1: Learn orthogonal projectors P_1, ..., P_K using wavelet packets
P = [] # list of projectors
for k in range(1, K+1):
  # Initialize P_k randomly as a d x d orthogonal matrix
  P_k = random_orthogonal_matrix(d)
  # Optimize P_k using gradient descent
  for t in range(max_iterations):
    # Compute the gradient of the objective function
    # The objective function is the negative log-likelihood of the data under p_k(x | P_k x)
    # The gradient is given by - sum_{i=1}^n grad_p_k(x_i | P_k x_i) * (P_k^T x_i - theta_k)
    grad = 0
    for i in range(1, n+1):
      # Compute the gradient of p_k(x | P_k x) with respect to P_k
      # Assume that p_k(x | P_k x) is a Gaussian distribution with mean theta_k and covariance sigma_k^2 I
      # Then grad_p_k(x | P_k x) = - sigma_k^-2 * (x - theta_k) * (P_k x - theta_k)^T
      grad_p_k = - sigma_k^-2 * (X[i] - theta_k) * (P_k @ X[i] - theta_k).T
      # Update the gradient of the objective function
      grad = grad - grad_p_k @ (P_k.T @ X[i] - theta_k)
    # Update P_k using a learning rate alpha
    P_k = P_k - alpha * grad
    # Project P_k onto the orthogonal group using singular value decomposition
    U, S, V = svd(P_k)
    P_k = U @ V.T
  # Add P_k to the list of projectors
  P.append(P_k)

# Step 2: Estimate the parameters of p_0 and p_1, ..., p_K using maximum likelihood
# Initialize the parameters randomly
# Assume that p_0(x) is a Gaussian distribution with mean mu_0 and covariance Sigma_0
# Then theta_0 = (mu_0, Sigma_0)
mu_0 = random_vector(d)
Sigma_0 = random_positive_definite_matrix(d)
theta_0 = (mu_0, Sigma_0)
# Assume that p_k(x | P_k x) is a Gaussian distribution with mean theta_k and covariance sigma_k^2 I
# Then theta_k = (theta_k, sigma_k^2)
theta = [(random_scalar(), random_positive_scalar()) for k in range(1, K+1)]
# Optimize the parameters using gradient descent
for t in range(max_iterations):
  # Compute the gradient of the log-likelihood function
  # The log-likelihood function is given by sum_{i=1}^n log(p_0(x_i)) + sum_{k=1}^K log(p_k(x_i | P_k x_i))
  # The gradient with respect to mu_0 is given by sum_{i=1}^n Sigma_0^-1 * (x_i - mu_0)
  grad_mu_0 = 0
  for i in range(1, n+1):
    grad_mu_0 = grad_mu_0 + inv(Sigma_0) @ (X[i] - mu_0)
  # The gradient with respect to Sigma_0 is given by - n/2 * Sigma_0^-1 + sum_{i=1}^n Sigma_0^-1 * (x_i - mu_0) * (x_i - mu_0)^T * Sigma_0^-1
  grad_Sigma_0 = - n/2 * inv(Sigma_0)
  for i in range(1, n+1):
    grad_Sigma_0 = grad_Sigma_0 + inv(Sigma_0) @ (X[i] - mu_0) @ (X[i] - mu_0).T @ inv(Sigma_0)
  # The gradient with respect to theta_k is given by sum_{i=1}^n grad_theta_p_k(x_i | P_k x_i)
  # Assume that p_k(x | P_k x) is a Gaussian distribution with mean theta_k and covariance sigma_k^2 I
  # Then grad_theta_p_k(x | P_k x) = (sigma_k^-2 * (P_k x - theta_k), - sigma_k^-3 * ||P_k x - theta_k||^2 + sigma_k^-1)
  grad_theta = [0 for k in range(1, K+1)]
  for i in range(1, n+1):
    for k in range(1, K+1):
      # Compute the gradient of p_k(x | P_k x) with respect to theta_k
      grad_theta_p_k = (sigma_k^-2 * (P_k @ X[i] - theta_k), - sigma_k^-3 * norm(P_k @ X[i] - theta_k)^2 + sigma_k^-1)
      # Update the gradient of the log-likelihood function
      grad_theta[k] = grad_theta[k] + grad_theta_p_k
  # Update the parameters using a learning rate alpha
  mu_0 = mu_0 + alpha * grad_mu_0
  Sigma_0 = Sigma_0 + alpha * grad_Sigma_0
  # Project Sigma_0 onto the positive definite cone using eigenvalue decomposition
  D, V = eig(Sigma_0)
  D = max(D, 0) # clip negative eigenvalues to zero
  Sigma_0 = V @ diag(D) @ V.T
  theta_0 = (mu_0, Sigma_0)
  for k in range(1, K+1):
    theta_k = theta_k + alpha * grad_theta[k]
    # Project sigma_k^2 onto the positive real line
    sigma_k^2 = max(sigma_k^2, 0)
    theta[k] = (theta_k, sigma_k^2)

# Step 3: Sample from the CSLC model using rejection sampling
# Set a constant M such that p(x) <= M * q(x) for all x, where q is a proposal distribution
# Assume that q(x) is a Gaussian distribution with mean mu_q and covariance Sigma_q
# Then M can be computed as the maximum of p(x) / q(x) over a finite set of samples from q(x)
M = 0
for i in range(1, n_samples):
  # Generate a sample x from q
  x = sample_from_gaussian(mu_q, Sigma_q)
  # Compute the ratio p(x) / q(x)
  ratio = p(x) / q(x)
  # Update M if necessary
  if ratio > M:
    M = ratio
# Generate samples from the CSLC model
samples = [] # list of samples
while len(samples) < n_samples:
  # Generate a sample x from q
  x = sample_from_gaussian(mu_q, Sigma_q)
  # Generate a uniform random number u in [0, 1]
  u = random_uniform(0, 1)
  # Accept x with probability p(x) / (M * q(x))
  if u <= p(x) / (M * q(x)):
    samples.append(x)
```