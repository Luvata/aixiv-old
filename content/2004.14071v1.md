---
title: 2004.14071v1 Image Morphing with Perceptual Constraints and STN Alignment
date: 2020-04-15
---

# [Image Morphing with Perceptual Constraints and STN Alignment](http://arxiv.org/abs/2004.14071v1)

authors: Noa Fish, Richard Zhang, Lilach Perry, Daniel Cohen-Or, Eli Shechtman, Connelly Barnes


## What, Why and How

[1]: https://arxiv.org/abs/2004.14071v1 "Image Morphing with Perceptual Constraints and STN Alignment"
[2]: https://arxiv.org/abs/2004.14071 "Image Morphing with Perceptual Constraints and STN Alignment"
[3]: http://export.arxiv.org/abs/2303.14071v1 "[2303.14071v1] Improving Real-time Communication for Educational ..."

Here is a summary of the paper:

- **What**: The paper proposes a conditional GAN morphing framework that can synthesize smooth and plausible frames between two input images without explicit correspondences[^1^][1].
- **Why**: The paper aims to address the challenges of image morphing, such as maintaining fidelity, plausibility, and smoothness of the intermediate frames, as well as handling large shape and texture variations[^1^][1].
- **How**: The paper uses a grid-based freeform deformation spatial transformer to align the input images, and a conditional GAN to generate the intermediate frames. The GAN is trained with a perceptual similarity loss and a special protocol that produces sequences of frames. The paper also introduces a shape prior that enhances the plausibility of the intermediate frames[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- A conditional GAN morphing framework that can generate visually pleasing morphing effects between two input images without explicit correspondences.
- A grid-based freeform deformation spatial transformer that can align the input images and produce a smooth geometric transition.
- A shape prior that can improve the plausibility of the intermediate frames by enforcing consistency with the input images.
- A perceptual similarity loss and a special training protocol that can promote smooth transformation over time and avoid temporal artifacts.

## Method Summary

[1]: https://arxiv.org/abs/2004.14071v1 "Image Morphing with Perceptual Constraints and STN Alignment"
[2]: https://arxiv.org/pdf/2004.14071.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2205.14071v1 "[2205.14071v1] A Mechanically Assisted Examination of Vacuity and ..."

Here is a summary of the method section of the paper:

- The paper presents a conditional GAN morphing framework that consists of three main components: a spatial transformer network (STN), a generator network (G), and a discriminator network (D)[^1^][1].
- The STN takes two input images and predicts a grid-based freeform deformation (FFD) that aligns them. The FFD is applied to both input images to produce aligned images[^1^][1].
- The generator network takes the aligned images and a temporal parameter t that indicates the desired intermediate frame, and outputs a synthesized frame G(t)[^1^][1].
- The discriminator network takes either a real or a synthesized frame and tries to classify it as real or fake. It also tries to predict the temporal parameter t of the frame[^1^][1].
- The paper defines a shape prior loss that penalizes the deviation of the synthesized frame from the input images in terms of shape and texture[^1^][1].
- The paper also defines a perceptual similarity loss that measures the similarity between the synthesized frame and the real frame in terms of high-level features[^1^][1].
- The paper uses a special training protocol that produces sequences of frames for each pair of input images, and trains the generator and discriminator networks with these sequences[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the STN, G, and D networks
STN = SpatialTransformerNetwork()
G = GeneratorNetwork()
D = DiscriminatorNetwork()

# Define the shape prior loss and the perceptual similarity loss
shape_prior_loss = ShapePriorLoss()
perceptual_similarity_loss = PerceptualSimilarityLoss()

# Define the adversarial loss and the temporal loss
adversarial_loss = AdversarialLoss()
temporal_loss = TemporalLoss()

# Define the hyperparameters
lambda_s = 0.1 # weight for shape prior loss
lambda_p = 0.1 # weight for perceptual similarity loss
lambda_t = 0.1 # weight for temporal loss
num_frames = 10 # number of intermediate frames to generate

# Define the optimizer
optimizer = AdamOptimizer()

# Loop over the training data
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get a pair of input images
    x1, x2 = batch

    # Predict the FFD and align the input images
    ffd = STN(x1, x2)
    x1_a, x2_a = apply_ffd(x1, x2, ffd)

    # Generate a sequence of intermediate frames
    frames = []
    for t in range(1, num_frames + 1):
      # Normalize t to [0, 1]
      t_norm = t / (num_frames + 1)

      # Generate a frame using G
      frame = G(x1_a, x2_a, t_norm)

      # Append the frame to the sequence
      frames.append(frame)

    # Concatenate the input images and the frames
    sequence = torch.cat([x1_a] + frames + [x2_a], dim=0)

    # Compute the shape prior loss for each frame
    sp_loss = 0
    for frame in frames:
      sp_loss += shape_prior_loss(frame, x1_a, x2_a)

    # Compute the perceptual similarity loss for each pair of consecutive frames
    ps_loss = 0
    for i in range(len(sequence) - 1):
      ps_loss += perceptual_similarity_loss(sequence[i], sequence[i+1])

    # Compute the adversarial loss and the temporal loss for each frame
    adv_loss = 0
    tmp_loss = 0
    for i in range(len(frames)):
      # Get the frame and its corresponding t_norm
      frame = frames[i]
      t_norm = (i + 1) / (num_frames + 1)

      # Get the output of D for the frame
      pred_real, pred_t = D(frame)

      # Compute the adversarial loss and the temporal loss
      adv_loss += adversarial_loss(pred_real, True)
      tmp_loss += temporal_loss(pred_t, t_norm)

    # Compute the total generator loss
    gen_loss = adv_loss + lambda_s * sp_loss + lambda_p * ps_loss + lambda_t * tmp_loss

    # Update the parameters of G and STN using backpropagation and optimization
    optimizer.zero_grad()
    gen_loss.backward()
    optimizer.step()

    # Compute the adversarial loss and the temporal loss for each real image in the sequence
    adv_loss = 0
    tmp_loss = 0
    for i in range(len(sequence)):
      # Get the image and its corresponding t_norm
      image = sequence[i]
      t_norm = i / (num_frames + 1)

      # Get the output of D for the image
      pred_real, pred_t = D(image)

      # Compute the adversarial loss and the temporal loss
      adv_loss += adversarial_loss(pred_real, True)
      tmp_loss += temporal_loss(pred_t, t_norm)

    # Compute the total discriminator loss
    dis_loss = adv_loss + lambda_t * tmp_loss

    # Update the parameters of D using backpropagation and optimization
    optimizer.zero_grad()
    dis_loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import torchvision.transforms as transforms

# Define the STN network
class STN(nn.Module):
  def __init__(self):
    super(STN, self).__init__()

    # Define the encoder network that extracts features from the input images
    self.encoder = nn.Sequential(
      nn.Conv2d(3, 64, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(64, 128, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(128, 256, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(256, 512, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Flatten(),
      nn.Linear(512 * 8 * 8, 1024),
      nn.ReLU()
    )

    # Define the decoder network that predicts the FFD parameters from the encoded features
    self.decoder = nn.Sequential(
      nn.Linear(1024 * 2, 1024),
      nn.ReLU(),
      nn.Linear(1024, 256),
      nn.ReLU(),
      nn.Linear(256, 64 * 3 * 3) # The FFD has a 3x3 grid with 64 control points per grid point
    )

    # Initialize the FFD parameters to identity transformation
    self.init_ffd = torch.eye(3).repeat(64, 1).view(-1)

  def forward(self, x1, x2):
    # Encode the input images
    f1 = self.encoder(x1)
    f2 = self.encoder(x2)

    # Concatenate the encoded features
    f = torch.cat([f1, f2], dim=1)

    # Decode the FFD parameters
    ffd = self.decoder(f)

    # Add the initial FFD parameters to the decoded ones
    ffd = ffd + self.init_ffd

    # Reshape the FFD parameters to a batch of grids
    ffd = ffd.view(-1, 64, 3, 3)

    # Return the FFD parameters
    return ffd

# Define a function that applies the FFD to the input images
def apply_ffd(x1, x2, ffd):
  # Get the batch size and the image size
  batch_size = x1.size(0)
  image_size = x1.size(-1)

  # Create a regular grid of size image_size x image_size
  grid = torch.meshgrid(torch.linspace(-1, 1, image_size), torch.linspace(-1, 1, image_size))
  grid = torch.stack(grid[::-1], dim=-1).unsqueeze(0).repeat(batch_size, 1, 1, 1)

  # Reshape the grid to a batch of vectors of size image_size * image_size x 2
  grid = grid.view(batch_size, -1, 2)

  # Apply the FFD to each grid point using barycentric coordinates
  for i in range(3):
    for j in range(3):
      # Get the i-th and j-th control points of the FFD
      pi = ffd[:, i * image_size // 2 : (i + 1) * image_size // 2]
      pj = ffd[:, j * image_size // 2 : (j + 1) * image_size // 2]

      # Compute the barycentric coordinates of each grid point with respect to pi and pj
      alpha = (grid[:, :, i] - pi[:, :, i]) / (pj[:, :, i] - pi[:, :, i])
      beta = (grid[:, :, j] - pi[:, :, j]) / (pj[:, :, j] - pi[:, :, j])

      # Clamp the barycentric coordinates to [0, 1]
      alpha = torch.clamp(alpha, min=0.0, max=1.0)
      beta = torch.clamp(beta, min=0.0, max=1.0)

      # Compute the new grid point using barycentric interpolation
      grid[:, :, i] = alpha * pj[:, :, i] + (1 - alpha) * pi[:, :, i]
      grid[:, :, j] = beta * pj[:, :, j] + (1 - beta) * pi[:, :, j]

  # Reshape the grid back to a batch of grids of size image_size x image_size x 2
  grid = grid.view(batch_size, image_size, image_size, 2)

  # Apply the grid to the input images using bilinear sampling
  x1_a = F.grid_sample(x1, grid, align_corners=True)
  x2_a = F.grid_sample(x2, grid, align_corners=True)

  # Return the aligned images
  return x1_a, x2_a

# Define the generator network
class GeneratorNetwork(nn.Module):
  def __init__(self):
    super(GeneratorNetwork, self).__init__()

    # Define the encoder network that extracts features from the aligned images
    self.encoder = nn.Sequential(
      nn.Conv2d(3, 64, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(64, 128, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(128, 256, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2),
      nn.Conv2d(256, 512, 3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2)
    )

    # Define the decoder network that generates the intermediate frame from the encoded features and the temporal parameter
    self.decoder = nn.Sequential(
      nn.ConvTranspose2d(512 + 1, 256, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(256 + 1, 128, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(128 + 1, 64, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(64 + 1, 32, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(32 + 1, 3, 3, padding=1),
      nn.Tanh()
    )

    # Define a function that broadcasts the temporal parameter to a feature map
    self.broadcast = lambda x: x.unsqueeze(-1).unsqueeze(-1).repeat(1, 1, image_size // (2 ** num_layers), image_size // (2 ** num_layers))

    # Define the number of layers in the decoder network
    self.num_layers = len(self.decoder) // 2

    # Define the image size
    self.image_size = x1.size(-1)

  def forward(self, x1_a, x2_a, t):
    # Encode the aligned images
    f1_a = self.encoder(x1_a)
    f2_a = self.encoder(x2_a)

    # Concatenate the encoded features
    f_a = torch.cat([f1_a, f2_a], dim=1)

    # Broadcast the temporal parameter to a feature map
    t_map = self.broadcast(t)

    # Concatenate the temporal parameter to the encoded features
    f_t = torch.cat([f_a, t_map], dim=1)

    # Decode the intermediate frame from the concatenated features
    frame = f_t
    for i in range(self.num_layers):
      # Apply a convolutional transpose layer
      frame = self.decoder[i * 2](frame)

      # Broadcast the temporal parameter to a feature map
      t_map = self.broadcast(t)

      # Concatenate the temporal parameter to the feature map
      frame = torch.cat([frame, t_map], dim=1)

      # Apply a non-linear activation layer
      frame = self.decoder[i * 2 + 1](frame)

    # Return the intermediate frame
    return frame

# Define the discriminator network
class DiscriminatorNetwork(nn.Module):
  def __init__(self):
    super(DiscriminatorNetwork, self).__init__()

    # Define the encoder network that extracts features from the input frame
    self.encoder = nn.Sequential(
      nn.Conv2d(3 + 1, 64 + 8 * num_frames , kernel_size=4,stride=4,padding=0), 
        #(64+8*10) filters of size (4x4) with stride of (4x4) and no padding. 
        #(3+8*10) channels for input. 
        #(8*10) channels for temporal parameter.
        #(3) channels for input image.
        #(64) channels for output.
        #(8*10) channels for temporal parameter.
        #(8*10) channels for output.
        #(8*10) channels for output.
        #(8*10) channels for output.
        #(8