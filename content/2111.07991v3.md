---
title: 2111.07991v3 LiT  Zero-Shot Transfer with Locked-image text Tuning
date: 2021-11-08
---

# [LiT: Zero-Shot Transfer with Locked-image text Tuning](http://arxiv.org/abs/2111.07991v3)

authors: Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, Lucas Beyer


## What, Why and How

[1]: https://arxiv.org/abs/2111.07991 "[2111.07991] LiT: Zero-Shot Transfer with Locked-image text Tuning"
[2]: https://arxiv.org/pdf/2111.07991v3.pdf "LiT : Zero-Shot Transfer with Locked-image text Tuning - arXiv.org"
[3]: http://export.arxiv.org/abs/1804.07991v3 "[1804.07991v3] Modeling electromagnetics on cylindrical meshes with ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method called **Locked-image Tuning (LiT)** that uses contrastive training to align image and text models for zero-shot transfer to new vision tasks.
- **Why**: The paper aims to improve the data- and compute-efficiency of existing methods that use web-sourced paired image-text data for pre-training models for zero-shot transfer.
- **How**: The paper proposes to lock the pre-trained image model and only tune the text model using contrastive learning, which allows the text model to read out good representations from the image model for new tasks. The paper evaluates LiT on various pre-training methods, architectures, and datasets, and shows that it achieves state-of-the-art results on ImageNet and ObjectNet test sets.

## Main Contributions

[1]: https://arxiv.org/abs/2111.07991 "[2111.07991] LiT: Zero-Shot Transfer with Locked-image text Tuning"
[2]: https://arxiv.org/pdf/2111.07991v3.pdf "LiT : Zero-Shot Transfer with Locked-image text Tuning - arXiv.org"
[3]: http://export.arxiv.org/abs/1804.07991v3 "[1804.07991v3] Modeling electromagnetics on cylindrical meshes with ..."

According to the paper at [^1^][1], the main contributions are:

- **A novel method for zero-shot transfer with contrastive-tuning**, which locks the pre-trained image model and only tunes the text model using contrastive learning.
- **A comprehensive empirical study** of contrastive-tuning with different pre-training methods, architectures, and datasets, showing its wide applicability and robustness.
- **State-of-the-art results on ImageNet and ObjectNet test sets** with the transformer-based pre-trained ViT-g/14 model, as well as competitive results on several out-of-distribution test variants.

## Method Summary

[1]: https://arxiv.org/abs/2111.07991 "[2111.07991] LiT: Zero-Shot Transfer with Locked-image text Tuning"
[2]: https://arxiv.org/pdf/2111.07991v3.pdf "LiT : Zero-Shot Transfer with Locked-image text Tuning - arXiv.org"
[3]: http://export.arxiv.org/abs/1804.07991v3 "[1804.07991v3] Modeling electromagnetics on cylindrical meshes with ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces **contrastive-tuning**, a general framework for aligning image and text models using contrastive learning, which minimizes the distance between paired image and text embeddings and maximizes the distance between non-paired ones.
- The paper proposes a specific instance of contrastive-tuning called **Locked-image Tuning (LiT)**, which locks the parameters of the pre-trained image model and only tunes the text model using contrastive learning. This reduces the computational cost and avoids overfitting to the image-text data.
- The paper describes the details of the **contrastive loss function**, the **image-text dataset construction**, and the **zero-shot transfer protocol** used in LiT. The paper also discusses some design choices and ablation studies on LiT.

## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper at :

```
# Pre-train an image model on a large-scale image dataset
image_model = pretrain_image_model(image_dataset)

# Freeze the image model parameters
image_model.freeze()

# Initialize a text model randomly
text_model = init_text_model()

# Construct an image-text dataset from web sources
image_text_dataset = construct_image_text_dataset(web_sources)

# Tune the text model using contrastive learning with the image model
text_model = contrastive_tune(text_model, image_model, image_text_dataset)

# For zero-shot transfer to a new vision task, use the text model to encode textual class descriptions
text_embeddings = text_model.encode(textual_class_descriptions)

# Use the image model to encode test images and compare them with text embeddings
image_embeddings = image_model.encode(test_images)
predictions = compare(image_embeddings, text_embeddings)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper at :

```
# Import libraries
import torch
import torchvision
import transformers
import numpy as np
import requests
import json

# Define hyperparameters
image_model_name = "google/vit-g14" # name of the pre-trained image model
text_model_name = "bert-base-uncased" # name of the text model architecture
image_dataset_name = "imagenet2012" # name of the large-scale image dataset
web_sources = ["https://www.flickr.com/", "https://unsplash.com/"] # list of web sources for image-text data
image_text_dataset_size = 1000000 # size of the image-text dataset
contrastive_loss_temperature = 0.07 # temperature parameter for the contrastive loss function
learning_rate = 1e-4 # learning rate for the text model optimizer
batch_size = 256 # batch size for contrastive tuning
num_epochs = 10 # number of epochs for contrastive tuning
textual_class_descriptions = ["a dog", "a cat", "a bird", ...] # list of textual class descriptions for the new vision task
test_images = [...] # list of test images for the new vision task

# Load the pre-trained image model from torchvision
image_model = torchvision.models.vision_transformer(image_model_name, pretrained=True)

# Freeze the image model parameters
for param in image_model.parameters():
    param.requires_grad = False

# Initialize a text model from transformers
text_model = transformers.BertModel.from_pretrained(text_model_name)

# Load the large-scale image dataset from torchvision
image_dataset = torchvision.datasets.ImageNet(image_dataset_name)

# Pre-train the image model on the image dataset (optional, can skip if using a pre-trained model)
image_model.train()
image_optimizer = torch.optim.Adam(image_model.parameters(), lr=learning_rate)
for epoch in range(num_epochs):
    for images, labels in torch.utils.data.DataLoader(image_dataset, batch_size=batch_size, shuffle=True):
        # Forward pass
        image_embeddings = image_model(images)
        # Compute the cross-entropy loss with the labels
        loss = torch.nn.CrossEntropyLoss()(image_embeddings, labels)
        # Backward pass and update parameters
        loss.backward()
        image_optimizer.step()
        image_optimizer.zero_grad()

# Construct an image-text dataset from web sources
image_text_dataset = []
for source in web_sources:
    # Send a request to the source API to get image-text pairs
    response = requests.get(source + "/api")
    # Parse the response as JSON and extract the relevant fields
    data = json.loads(response.text)
    for item in data:
        image_url = item["image_url"]
        text_caption = item["text_caption"]
        # Download the image from the url and resize it to 224x224 pixels
        image = torchvision.transforms.Resize((224, 224))(torchvision.io.read_image(image_url))
        # Append the image-text pair to the dataset
        image_text_dataset.append((image, text_caption))
    # Break the loop if the dataset size is reached
    if len(image_text_dataset) >= image_text_dataset_size:
        break

# Tune the text model using contrastive learning with the image model
text_model.train()
text_optimizer = torch.optim.Adam(text_model.parameters(), lr=learning_rate)
for epoch in range(num_epochs):
    for images, texts in torch.utils.data.DataLoader(image_text_dataset, batch_size=batch_size, shuffle=True):
        # Forward pass for both models
        image_embeddings = image_model(images)
        text_embeddings = text_model(texts).last_hidden_state[:, 0, :]
        # Normalize the embeddings to unit length
        image_embeddings = torch.nn.functional.normalize(image_embeddings, dim=-1)
        text_embeddings = torch.nn.functional.normalize(text_embeddings, dim=-1)
        # Compute the cosine similarity matrix between image and text embeddings
        similarity_matrix = torch.matmul(image_embeddings, text_embeddings.t())
        # Compute the contrastive loss with temperature scaling and label smoothing
        labels = torch.arange(batch_size).to(similarity_matrix.device)
        loss = torch.nn.CrossEntropyLoss()(similarity_matrix / contrastive_loss_temperature, labels)
        # Backward pass and update parameters
        loss.backward()
        text_optimizer.step()
        text_optimizer.zero_grad()

# For zero-shot transfer to a new vision task, use the text model to encode textual class descriptions
text_model.eval()
with torch.no_grad():
    text_embeddings = text_model(textual_class_descriptions).last_hidden_state[:, 0, :]
    text_embeddings = torch.nn.functional.normalize(text_embeddings, dim=-1)

# Use the image model to encode test images and compare them with text embeddings
image_model.eval()
with torch.no_grad():
    image_embeddings = image_model(test_images)
    image_embeddings = torch.nn.functional.normalize(image_embeddings, dim=-1)
    similarity_matrix = torch.matmul(image_embeddings, text_embeddings.t())
    # Predict the class with the highest similarity score
    predictions = torch.argmax(similarity_matrix, dim=-1)
```