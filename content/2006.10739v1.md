---
title: 2006.10739v1 Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains
date: 2020-06-11
---

# [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](http://arxiv.org/abs/2006.10739v1)

authors: Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, Ren Ng


## What, Why and How

[1]: https://arxiv.org/abs/2006.10739 "[2006.10739] Fourier Features Let Networks Learn High Frequency ..."
[2]: https://arxiv.org/pdf/2006.10739v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2007.10739v1 "[2007.10739v1] A 750 MHz radio frequency quadrupole with trapezoidal ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to improve the performance of multilayer perceptrons (MLPs) for learning high-frequency functions in low-dimensional domains, such as 3D object and scene representation. The method uses a Fourier feature mapping to transform the input points into a higher-dimensional space that captures more spectral information.
- **Why**: The paper shows that standard MLPs suffer from a spectral bias that limits their ability to learn high-frequency functions, both theoretically and empirically. This bias is due to the smoothness of the neural tangent kernel (NTK) of MLPs, which determines their learning dynamics. The paper argues that this bias is undesirable for many computer vision and graphics applications that require high-frequency details.
- **How**: The paper introduces a Fourier feature mapping that transforms the input points into a higher-dimensional space by applying random sinusoidal functions. This mapping effectively changes the NTK of the MLP into a stationary kernel with a tunable bandwidth, which can be adjusted to match the frequency spectrum of the target function. The paper demonstrates that this simple technique significantly improves the performance of MLPs for various low-dimensional regression tasks, such as image reconstruction, shape representation, and neural rendering.

## Main Contributions

The paper claims to make the following contributions:

- It provides a theoretical analysis of the spectral bias of standard MLPs and shows that they fail to learn high-frequency functions in low-dimensional domains.
- It proposes a simple and general technique to overcome this bias by using a Fourier feature mapping that transforms the input points into a higher-dimensional space with more spectral information.
- It demonstrates the effectiveness of this technique for various low-dimensional regression tasks relevant to computer vision and graphics, such as image reconstruction, shape representation, and neural rendering.
- It suggests an approach for selecting problem-specific Fourier features that optimize the performance of MLPs for different tasks.

## Method Summary

[1]: https://arxiv.org/abs/2006.10739 "[2006.10739] Fourier Features Let Networks Learn High Frequency ..."
[2]: https://arxiv.org/pdf/2006.10739v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2007.10739v1 "[2007.10739v1] A 750 MHz radio frequency quadrupole with trapezoidal ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper first reviews the concept of neural tangent kernel (NTK), which is a function that measures the similarity between two inputs based on the gradient of the network output with respect to the network parameters. The paper shows that the NTK of a standard MLP is a smooth function that decays exponentially with the distance between the inputs, and that this smoothness implies a spectral bias that limits the ability of the MLP to learn high-frequency functions.
- The paper then introduces a Fourier feature mapping that transforms the input points into a higher-dimensional space by applying random sinusoidal functions with different frequencies and phases. The paper shows that this mapping changes the NTK of the MLP into a stationary kernel that has a constant value at zero distance and decays with a Gaussian envelope as the distance increases. The paper also shows that the bandwidth of this kernel, which determines how fast it decays, can be controlled by the distribution of the frequencies in the Fourier feature mapping.
- The paper then proposes an approach for selecting problem-specific Fourier features that optimize the performance of the MLP for different tasks. The paper suggests to use a random search algorithm to sample different frequency distributions and evaluate them on a validation set. The paper also provides some heuristics for choosing the initial range and scale of the frequencies based on the input domain and the target function.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a standard MLP with L layers and ReLU activations
def MLP(x):
  for l in range(L):
    x = ReLU(W[l] @ x + b[l])
  return x

# Define a Fourier feature mapping with D random sinusoidal functions
def Fourier_Feature_Mapping(x):
  # Sample D frequencies and phases from a given distribution
  B = sample_frequencies_and_phases(D)
  # Apply the sinusoidal functions to the input
  z = sin(B @ x)
  return z

# Define a loss function for the regression task
def Loss(y_true, y_pred):
  # Use mean squared error or other suitable metric
  return MSE(y_true, y_pred)

# Define a validation set to evaluate the performance of the MLP
X_val, y_val = get_validation_set()

# Define a random search algorithm to find the optimal frequency distribution
def Random_Search():
  # Initialize the best loss and the best frequency distribution
  best_loss = inf
  best_B = None
  # Repeat for a given number of iterations
  for i in range(N):
    # Sample a new frequency distribution from a given range and scale
    B = sample_new_frequency_distribution()
    # Transform the validation inputs using the Fourier feature mapping
    Z_val = Fourier_Feature_Mapping(X_val)
    # Train the MLP on the transformed inputs and the validation outputs
    MLP.train(Z_val, y_val)
    # Evaluate the MLP on the validation set and compute the loss
    y_pred = MLP(Z_val)
    loss = Loss(y_val, y_pred)
    # Update the best loss and the best frequency distribution if needed
    if loss < best_loss:
      best_loss = loss
      best_B = B
  # Return the best frequency distribution
  return best_B

# Run the random search algorithm to find the optimal frequency distribution
B_opt = Random_Search()

# Transform the input points using the optimal Fourier feature mapping
Z = Fourier_Feature_Mapping(X)

# Train the MLP on the transformed inputs and the target outputs
MLP.train(Z, y)

# Use the MLP to predict new outputs for new inputs
y_new = MLP(Fourier_Feature_Mapping(x_new))
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Define the hyperparameters
L = 4 # Number of layers in the MLP
D = 256 # Dimension of the Fourier feature mapping
N = 100 # Number of iterations for the random search
M = 1000 # Number of training examples
K = 100 # Number of validation examples

# Define a standard MLP with L layers and ReLU activations
class MLP(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(MLP, self).__init__()
    # Initialize the weights and biases randomly
    self.W = nn.ParameterList([nn.Parameter(torch.randn(input_dim, D)) for _ in range(L)])
    self.b = nn.ParameterList([nn.Parameter(torch.randn(D)) for _ in range(L)])
    self.W_out = nn.Parameter(torch.randn(D, output_dim))
    self.b_out = nn.Parameter(torch.randn(output_dim))
  
  def forward(self, x):
    # Apply the MLP to the input
    for l in range(L):
      x = torch.relu(self.W[l] @ x + self.b[l])
    x = self.W_out @ x + self.b_out
    return x

# Define a Fourier feature mapping with D random sinusoidal functions
def Fourier_Feature_Mapping(x, B):
  # Apply the sinusoidal functions to the input
  z = torch.sin(B @ x)
  return z

# Define a loss function for the regression task
def Loss(y_true, y_pred):
  # Use mean squared error or other suitable metric
  return torch.mean((y_true - y_pred) ** 2)

# Define a validation set to evaluate the performance of the MLP
X_val = torch.randn(K, 2) # Sample K random points in R^2
y_val = f(X_val) # Compute the target outputs using some function f

# Define a random search algorithm to find the optimal frequency distribution
def Random_Search():
  # Initialize the best loss and the best frequency distribution
  best_loss = float('inf')
  best_B = None
  # Initialize an MLP with input dimension D and output dimension 1
  mlp = MLP(D, 1)
  # Initialize an optimizer for the MLP parameters
  optimizer = optim.Adam(mlp.parameters(), lr=0.01)
  # Repeat for N iterations
  for i in range(N):
    # Sample a new frequency distribution from a uniform distribution in [0, pi]
    B = np.pi * torch.rand(D, 2)
    # Transform the validation inputs using the Fourier feature mapping
    Z_val = Fourier_Feature_Mapping(X_val, B)
    # Train the MLP on the transformed inputs and the validation outputs for one epoch
    optimizer.zero_grad()
    y_pred = mlp(Z_val)
    loss = Loss(y_val, y_pred)
    loss.backward()
    optimizer.step()
    # Evaluate the MLP on the validation set and compute the loss
    y_pred = mlp(Z_val)
    loss = Loss(y_val, y_pred)
    # Update the best loss and the best frequency distribution if needed
    if loss < best_loss:
      best_loss = loss.item()
      best_B = B.clone()
  # Return the best frequency distribution
  return best_B

# Run the random search algorithm to find the optimal frequency distribution
B_opt = Random_Search()

# Define a training set to train the MLP on the target function
X_train = torch.randn(M, 2) # Sample M random points in R^2
y_train = f(X_train) # Compute the target outputs using some function f

# Transform the training inputs using the optimal Fourier feature mapping
Z_train = Fourier_Feature_Mapping(X_train, B_opt)

# Train the MLP on the transformed inputs and the target outputs for some epochs
mlp.train(Z_train, y_train)

# Use the MLP to predict new outputs for new inputs
x_new = torch.randn(1, 2) # Sample a new point in R^2
y_new = mlp(Fourier_Feature_Mapping(x_new, B_opt)) # Predict its output using the MLP and the optimal Fourier feature mapping

```