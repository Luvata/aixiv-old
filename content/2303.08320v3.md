---
title: 2303.08320v3 VideoFusion  Decomposed Diffusion Models for High-Quality Video Generation
date: 2023-03-09
---

# [VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation](http://arxiv.org/abs/2303.08320v3)

authors: Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan


## What, Why and How

[1]: https://arxiv.org/abs/2303.08320v3 "[2303.08320v3] VideoFusion: Decomposed Diffusion Models for High ..."
[2]: https://arxiv.org/pdf/2303.08320v3.pdf "VideoFusion: Decomposed Diffusion Models for High-Quality ... - arXiv.org"
[3]: https://arxiv.org/pdf/2103.08320v3.pdf "No PDF for 2103.08320v3 - arXiv.org"
[4]: https://arxiv.org/pdf/2303.08320v3.pdf?ref=blog.paperspace.com "arXiv.org"

Here is a summary of the paper:

```
What: The paper proposes a new method for high-quality video generation using decomposed diffusion models, which separate the per-frame noise into a base noise and a residual noise.

Why: The paper aims to overcome the limitations of previous diffusion-based methods that ignore the content redundancy and temporal correlation in video data, and to leverage pre-trained image diffusion models for video generation.

How: The paper introduces a decomposed diffusion process that adds a base noise to all frames and a residual noise to each frame, and learns two networks to denoise them in reverse. The paper also shows how to use text as a condition for video generation by encoding text into base noise.
```

## Main Contributions

[1]: https://arxiv.org/abs/2303.08320v3 "[2303.08320v3] VideoFusion: Decomposed Diffusion Models for High ..."
[2]: https://arxiv.org/pdf/2303.08320v3.pdf "VideoFusion: Decomposed Diffusion Models for High-Quality ... - arXiv.org"
[3]: https://arxiv.org/pdf/2103.08320v3.pdf "No PDF for 2103.08320v3 - arXiv.org"
[4]: https://arxiv.org/pdf/2303.08320v3.pdf?ref=blog.paperspace.com "arXiv.org"

The paper claims to make the following contributions:

- A novel decomposed diffusion process for video generation that separates the per-frame noise into a base noise and a residual noise
- Two networks that learn to denoise the base noise and the residual noise in reverse, and can be trained end-to-end
- A text-conditioned video generation method that encodes text into base noise and uses pre-trained image diffusion models
- Extensive experiments on various datasets that demonstrate the superiority of the proposed method over existing methods in terms of video quality and diversity

## Method Summary

[1]: https://arxiv.org/abs/2303.08320v3 "[2303.08320v3] VideoFusion: Decomposed Diffusion Models for High ..."
[2]: https://arxiv.org/pdf/2303.08320v3.pdf "VideoFusion: Decomposed Diffusion Models for High-Quality ... - arXiv.org"
[3]: https://arxiv.org/pdf/2103.08320v3.pdf "No PDF for 2103.08320v3 - arXiv.org"
[4]: https://arxiv.org/pdf/2303.08320v3.pdf?ref=blog.paperspace.com "arXiv.org"

Here is a summary of the method section:

```
The method consists of three main components: a decomposed diffusion process, a denoising process, and a text-conditioned video generation module.

- The decomposed diffusion process takes a video clip x = (x1, ..., xT) as input and adds two types of noises to each frame: a base noise b that is shared by all frames and a residual noise r that is different for each frame. The output is a noisy video y = (y1, ..., yT) where yt = xt + b + rt. The base noise b is sampled from a Gaussian distribution N(0, σb2I) and the residual noise rt is sampled from N(0, σr2I). The noise variances σb2 and σr2 are determined by a noise schedule that controls the trade-off between content preservation and diversity.

- The denoising process aims to recover the original video x from the noisy video y by iteratively removing the base noise b and the residual noise r. The process uses two networks: a base denoiser B and a residual denoiser R. The base denoiser B takes y as input and outputs an estimate of b, denoted as b̂. The residual denoiser R takes y − b̂ as input and outputs an estimate of r, denoted as r̂. The final output is x̂ = y − b̂ − r̂. The two networks are trained jointly by minimizing the mean squared error (MSE) between x̂ and x.

- The text-conditioned video generation module allows generating videos based on natural language descriptions. The module uses a text encoder E that encodes a text input s into a latent vector z. The vector z is then used to generate the base noise b by passing it through a fully-connected layer followed by a reshape operation. The residual noise r is generated randomly as before. The noisy video y is then fed into the denoising process to obtain the final video x̂. The text encoder E is pre-trained on a large-scale text corpus and fine-tuned on paired text-video data.
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a video clip x or a text input s
# Output: a generated video x̂

# Decomposed diffusion process
if x is given:
  b = sample_base_noise() # sample base noise from N(0, σb2I)
  r = sample_residual_noise() # sample residual noise from N(0, σr2I) for each frame
  y = x + b + r # add noises to each frame
else if s is given:
  z = E(s) # encode text input into latent vector
  b = generate_base_noise(z) # generate base noise from latent vector
  r = sample_residual_noise() # sample residual noise from N(0, σr2I) for each frame
  y = b + r # generate noisy video from noises

# Denoising process
for t in range(T): # iterate over time steps
  b̂ = B(y) # estimate base noise using base denoiser network
  r̂ = R(y - b̂) # estimate residual noise using residual denoiser network
  x̂ = y - b̂ - r̂ # recover original video by removing noises

# Return the generated video
return x̂
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np

# Define hyperparameters
BATCH_SIZE = 16 # batch size for training and inference
VIDEO_LEN = 32 # number of frames in a video clip
VIDEO_SIZE = 64 # spatial resolution of a video frame
NOISE_DIM = 256 # dimension of the latent vector for text encoding
NOISE_SCHEDULE = [0.01, 0.02, ..., 0.99] # noise schedule for diffusion process
NUM_STEPS = len(NOISE_SCHEDULE) # number of steps for denoising process
LEARNING_RATE = 1e-4 # learning rate for optimization
BETA1 = 0.9 # beta1 parameter for Adam optimizer
BETA2 = 0.999 # beta2 parameter for Adam optimizer
LAMBDA = 0.1 # weight for KL divergence loss in text-conditioned video generation

# Define network architectures
class TextEncoder(nn.Module):
  # A text encoder network that encodes a text input into a latent vector
  def __init__(self):
    super(TextEncoder, self).__init__()
    self.bert = torchvision.models.bert(pretrained=True) # use pre-trained BERT model
    self.fc = nn.Linear(768, NOISE_DIM) # a fully-connected layer to map BERT output to noise dimension
  
  def forward(self, s):
    # Input: s, a text input of shape (batch_size, max_length)
    # Output: z, a latent vector of shape (batch_size, noise_dim)
    x = self.bert(s)[1] # get the pooled output from BERT, shape (batch_size, 768)
    z = self.fc(x) # map to noise dimension, shape (batch_size, noise_dim)
    return z

class BaseDenoiser(nn.Module):
  # A base denoiser network that estimates the base noise from a noisy video
  def __init__(self):
    super(BaseDenoiser, self).__init__()
    self.conv1 = nn.Conv3d(3, 64, kernel_size=3, padding=1) # a 3D convolutional layer with 64 filters
    self.conv2 = nn.Conv3d(64, 128, kernel_size=3, padding=1) # a 3D convolutional layer with 128 filters
    self.conv3 = nn.Conv3d(128, 256, kernel_size=3, padding=1) # a 3D convolutional layer with 256 filters
    self.conv4 = nn.Conv3d(256, 512, kernel_size=3, padding=1) # a 3D convolutional layer with 512 filters
    self.conv5 = nn.Conv3d(512, 1024, kernel_size=3, padding=1) # a 3D convolutional layer with 1024 filters
    self.fc1 = nn.Linear(1024 * VIDEO_LEN * VIDEO_SIZE * VIDEO_SIZE // (32 * 32), NOISE_DIM) # a fully-connected layer to map the flattened features to noise dimension
    self.fc2 = nn.Linear(NOISE_DIM + NUM_STEPS + VIDEO_LEN + VIDEO_SIZE + VIDEO_SIZE + NOISE_DIM, NOISE_DIM) # a fully-connected layer to combine the features with conditioning information
    self.fc3 = nn.Linear(NOISE_DIM, VIDEO_LEN * VIDEO_SIZE * VIDEO_SIZE * 3) # a fully-connected layer to map the features to base noise dimension
  
  def forward(self, y, t):
    # Input: y, a noisy video of shape (batch_size, video_len, video_size, video_size, 3)
    #        t, the current time step of shape (batch_size,)
    # Output: b̂, an estimate of the base noise of shape (batch_size, video_len * video_size * video_size * 3)
    x = F.relu(self.conv1(y)) # apply the first convolutional layer and ReLU activation, shape (batch_size, 64, video_len, video_size, video_size)
    x = F.max_pool3d(x, kernel_size=2) # apply max pooling with kernel size 2, shape (batch_size, 64, video_len // 2 , video_size // 2 , video_size //2 )
    x = F.relu(self.conv2(x)) # apply the second convolutional layer and ReLU activation