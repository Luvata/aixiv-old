---
title: 2305.10118v2 Bridging the Gap  Enhancing the Utility of Synthetic Data via Post-Processing Techniques
date: 2023-05-11
---

# [Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques](http://arxiv.org/abs/2305.10118v2)

authors: Andrea Lampis, Eugenio Lomurno, Matteo Matteucci


## What, Why and How

[1]: https://arxiv.org/abs/2305.10118 "[2305.10118] Bridging the Gap: Enhancing the Utility of Synthetic Data ..."
[2]: https://arxiv.org/pdf/2305.10118.pdf "arXiv:2305.10118v1 [cs.CV] 17 May 2023"
[3]: http://export.arxiv.org/abs/2001.10118v2 "[2001.10118v2] Complex nonlinear capacitance in outer hair cell macro ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes three novel post-processing techniques to improve the quality and diversity of synthetic datasets generated by Generative Adversarial Networks (GANs) for training classifiers that are evaluated on real-world images. The paper also introduces a pipeline called Gap Filler (GaFi) that applies these techniques in an optimal and coordinated manner to maximise classification accuracy on real-world data.
- **Why**: The paper aims to address the issue of the gap between the performance of classifiers trained on synthetic data and those trained on real data, which is caused by the inability of synthetic data to fully capture the complexity and diversity of real-world data. The paper also aims to reduce the cost and effort of data collection and annotation for training deep learning models, and to enable the use of synthetic data in cases where real data cannot be shared directly for privacy or sensitivity reasons.
- **How**: The paper explores the use of GANs to generate synthetic datasets for training classifiers on three benchmark datasets: Fashion-MNIST, CIFAR-10, and CIFAR-100. The paper proposes three post-processing techniques to enhance the utility of synthetic data: Dynamic Sample Filtering, which filters out low-quality samples based on their distance from the decision boundary; Dynamic Dataset Recycle, which recycles high-quality samples from previous iterations to increase the diversity of the synthetic dataset; and Expansion Trick, which expands the synthetic dataset by adding small perturbations to each sample. The paper also introduces GaFi, a pipeline that applies these techniques in a sequential order based on their effectiveness and computational cost. The paper evaluates the performance of GaFi by comparing the classification accuracy score (CAS) of classifiers trained on synthetic data with those trained on real data. The paper reports that GaFi achieves state-of-the-art results on all three datasets, reducing the gap with real-accuracy scores to an error of 2.03%, 1.78%, and 3.99%, respectively.


## Main Contributions

The contributions of this paper are:

- It proposes three novel post-processing techniques to improve the quality and diversity of synthetic datasets generated by GANs for training classifiers that are evaluated on real-world images.
- It introduces a pipeline called GaFi that applies these techniques in an optimal and coordinated manner to maximise classification accuracy on real-world data.
- It evaluates the performance of GaFi on three benchmark datasets and achieves state-of-the-art results in reducing the gap with real-accuracy scores.


## Method Summary

Here is a summary of the method section of the paper:

- The paper uses GANs to generate synthetic datasets for training classifiers on three benchmark datasets: Fashion-MNIST, CIFAR-10, and CIFAR-100. The paper uses the DCGAN architecture for the generator and discriminator networks, and trains them using the Adam optimizer with a learning rate of 0.0002 and a batch size of 128. The paper uses the inception score (IS) and the Fr√©chet inception distance (FID) to measure the quality and diversity of the synthetic datasets.
- The paper proposes three post-processing techniques to enhance the utility of synthetic data: Dynamic Sample Filtering (DSF), Dynamic Dataset Recycle (DDR), and Expansion Trick (ET). DSF filters out low-quality samples based on their distance from the decision boundary of a classifier trained on real data. DDR recycles high-quality samples from previous iterations to increase the diversity of the synthetic dataset. ET expands the synthetic dataset by adding small perturbations to each sample using Gaussian noise or random cropping.
- The paper introduces GaFi, a pipeline that applies these techniques in a sequential order based on their effectiveness and computational cost. GaFi consists of four steps: (1) Generate a synthetic dataset using GANs; (2) Apply DSF to filter out low-quality samples; (3) Apply DDR to recycle high-quality samples from previous iterations; (4) Apply ET to expand the synthetic dataset. The paper applies GaFi iteratively until the CAS reaches a predefined threshold or a maximum number of iterations is reached.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define GAN architecture and hyperparameters
generator = DCGAN()
discriminator = DCGAN()
optimizer = Adam(lr=0.0002)
batch_size = 128
# Define quality and diversity metrics
IS = inception_score()
FID = frechet_inception_distance()
# Define CAS threshold and maximum iterations
CAS_threshold = 0.95
max_iterations = 100
# Initialize synthetic dataset and CAS
synthetic_dataset = []
CAS = 0
# Iterate until CAS reaches threshold or maximum iterations
while CAS < CAS_threshold and iteration < max_iterations:
  # Generate synthetic dataset using GANs
  synthetic_dataset = generator.generate(batch_size)
  # Apply DSF to filter out low-quality samples
  synthetic_dataset = DSF(synthetic_dataset)
  # Apply DDR to recycle high-quality samples from previous iterations
  synthetic_dataset = DDR(synthetic_dataset)
  # Apply ET to expand the synthetic dataset
  synthetic_dataset = ET(synthetic_dataset)
  # Train a classifier on synthetic dataset and evaluate on real data
  classifier = train(synthetic_dataset)
  CAS = evaluate(classifier, real_data)
  # Increment iteration
  iteration += 1
# Return the final synthetic dataset and CAS
return synthetic_dataset, CAS
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers
from sklearn.metrics import accuracy_score
# Define GAN architecture and hyperparameters
generator = models.Sequential([
  layers.Dense(7*7*256, input_shape=(100,)),
  layers.BatchNormalization(),
  layers.LeakyReLU(),
  layers.Reshape((7, 7, 256)),
  layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same'),
  layers.BatchNormalization(),
  layers.LeakyReLU(),
  layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same'),
  layers.BatchNormalization(),
  layers.LeakyReLU(),
  layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='tanh')
])
discriminator = models.Sequential([
  layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1)),
  layers.LeakyReLU(),
  layers.Dropout(0.3),
  layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),
  layers.LeakyReLU(),
  layers.Dropout(0.3),
  layers.Flatten(),
  layers.Dense(1)
])
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss
def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)
generator_optimizer = optimizers.Adam(lr=0.0002)
discriminator_optimizer = optimizers.Adam(lr=0.0002)
@tf.function
def train_step(images):
    noise = tf.random.normal([batch_size, noise_dim])
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
      generated_images = generator(noise, training=True)
      real_output = discriminator(images, training=True)
      fake_output = discriminator(generated_images, training=True)
      gen_loss = generator_loss(fake_output)
      disc_loss = discriminator_loss(real_output, fake_output)
    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))
# Define quality and diversity metrics
def inception_score(images):
    # Compute the inception score using a pre-trained classifier
    pass
def frechet_inception_distance(images1, images2):
    # Compute the FID using a pre-trained classifier
    pass
# Define CAS threshold and maximum iterations
CAS_threshold = 0.95
max_iterations = 100
# Initialize synthetic dataset and CAS
synthetic_dataset = []
CAS = 0
# Iterate until CAS reaches threshold or maximum iterations
while CAS < CAS_threshold and iteration < max_iterations:
  # Generate synthetic dataset using GANs
  synthetic_dataset = []
  for i in range(num_batches):
    noise = tf.random.normal([batch_size, noise_dim])
    generated_images = generator(noise)
    synthetic_dataset.append(generated_images)