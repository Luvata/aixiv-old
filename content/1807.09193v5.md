---
title: 1807.09193v5 GRAINS  Generative Recursive Autoencoders for INdoor Scenes
date: 2018-07-10
---

# [GRAINS: Generative Recursive Autoencoders for INdoor Scenes](http://arxiv.org/abs/1807.09193v5)

authors: Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, Hao Zhang


## What, Why and How

[1]: https://arxiv.org/pdf/1807.09193v5.pdf "GRAINS: Generative Recursive Autoencoders for INdoor Scenes - arXiv.org"
[2]: https://arxiv.org/abs/1807.09193 "GRAINS: Generative Recursive Autoencoders for INdoor Scenes"
[3]: https://scholar.archive.org/work/swq7722vibhurkamcxoun2d2sa "GRAINS: Generative Recursive Autoencoders for INdoor Scenes"

Here is a summary of the paper:

- **What**: The paper presents a generative neural network called GRAINS (Generative Recursive Autoencoders for INdoor Scenes) that can generate plausible and diverse 3D indoor scenes in large quantities and varieties, easily and highly efficiently[^1^][1].
- **Why**: The paper aims to address the challenges of 3D scene synthesis, such as modeling complex scene structures, capturing object relations, and generating realistic scenes with high diversity[^1^][1]. The paper also shows applications of GRAINS for 3D scene modeling from 2D layouts, scene editing, and semantic scene segmentation[^1^][1].
- **How**: The paper leverages the key observation that indoor scene structures are inherently hierarchical and trains a variational recursive autoencoder (RvNN-VAE) on a dataset of annotated scene hierarchies[^1^][1]. The RvNN-VAE performs scene object grouping during its encoding phase and scene generation during decoding. Specifically, a set of encoders are recursively applied to group 3D objects based on support, surround, and co-occurrence relations in a scene, encoding information about object spatial properties, semantics, and their relative positioning with respect to other objects in the hierarchy[^1^][1]. By training a variational autoencoder (VAE), the resulting fixed-length codes roughly follow a Gaussian distribution. A novel 3D scene can be generated hierarchically by the decoder from a randomly sampled code from the learned distribution[^1^][1] [^2^][2].

## Main Contributions

According to the paper, the main contributions are:

- A generative recursive neural network (RvNN) based on a variational autoencoder (VAE) to learn hierarchical scene structures and generate plausible 3D indoor scenes in large quantities and varieties.
- A novel scene hierarchy annotation scheme that captures support, surround, and co-occurrence relations among 3D objects in a scene.
- A comprehensive evaluation of the proposed method on various aspects of 3D scene synthesis, such as scene plausibility, diversity, and structure preservation.
- Several applications of the proposed method for 3D scene modeling from 2D layouts, scene editing, and semantic scene segmentation via PointNet.

## Method Summary

The method section of the paper consists of four subsections:

- **Scene hierarchy annotation**: The paper describes how they annotate a large-scale dataset of 3D indoor scenes with hierarchical structures that capture support, surround, and co-occurrence relations among 3D objects. They also introduce a scene hierarchy grammar that defines the rules for scene object grouping and splitting.
- **RvNN-VAE architecture**: The paper presents the details of the RvNN-VAE architecture, which consists of a recursive encoder and a recursive decoder. The encoder takes a scene hierarchy as input and recursively applies a set of encoders to group 3D objects into higher-level nodes until reaching the root node. The decoder takes a latent code as input and recursively applies a set of decoders to split nodes into lower-level objects until reaching the leaf nodes. The paper also explains how they train the RvNN-VAE using a variational lower bound objective function that balances reconstruction and regularization terms.
- **Scene generation**: The paper describes how they generate novel 3D scenes using the trained RvNN-VAE. They first sample a latent code from a Gaussian distribution and feed it to the decoder to generate a scene hierarchy. Then, they use a post-processing step to refine the object positions and orientations based on collision detection and physical constraints. They also introduce a diversity measure to quantify the diversity of the generated scenes.
- **Applications**: The paper demonstrates several applications of the proposed method, such as 3D scene modeling from 2D layouts, scene editing, and semantic scene segmentation via PointNet. They show how they can use the RvNN-VAE to generate 3D scenes that match given 2D layouts or user edits, and how they can use the large quantity and variety of generated scenes to augment the training data for PointNet and improve its performance on semantic scene segmentation.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the RvNN-VAE architecture
class RvNN_VAE(nn.Module):
  def __init__(self):
    # Initialize the encoder and decoder modules
    self.encoder = Encoder()
    self.decoder = Decoder()
    # Initialize the parameters for the Gaussian prior and posterior
    self.mu_prior = nn.Parameter(torch.zeros(1, latent_dim))
    self.logvar_prior = nn.Parameter(torch.zeros(1, latent_dim))
    self.mu_posterior = nn.Linear(hidden_dim, latent_dim)
    self.logvar_posterior = nn.Linear(hidden_dim, latent_dim)

  def encode(self, node):
    # Recursively encode a scene hierarchy node and its children
    if node.is_leaf():
      # Encode a leaf node using its object features
      hidden = self.encoder.encode_leaf(node.features)
    else:
      # Encode a non-leaf node using its children's hidden states
      child_hiddens = [self.encode(child) for child in node.children]
      hidden = self.encoder.encode_nonleaf(child_hiddens)
    # Compute the posterior parameters for the node
    mu = self.mu_posterior(hidden)
    logvar = self.logvar_posterior(hidden)
    return hidden, mu, logvar

  def decode(self, z, node):
    # Recursively decode a latent code and a scene hierarchy node
    if node.is_leaf():
      # Decode a leaf node using its object features
      features = self.decoder.decode_leaf(z)
      node.features = features
    else:
      # Decode a non-leaf node using its children's latent codes
      child_zs = self.decoder.decode_nonleaf(z)
      for i in range(len(node.children)):
        self.decode(child_zs[i], node.children[i])

  def forward(self, root):
    # Encode the root node of the scene hierarchy
    hidden, mu, logvar = self.encode(root)
    # Sample a latent code from the posterior distribution
    z = reparameterize(mu, logvar)
    # Decode the latent code and the root node
    self.decode(z, root)
    return mu, logvar

# Define the loss function for the RvNN-VAE
def loss_function(root, mu, logvar):
  # Compute the reconstruction loss as the negative log-likelihood of the object features
  recon_loss = 0
  for leaf in root.get_leaves():
    recon_loss += -log_likelihood(leaf.features, leaf.recon_features)
  # Compute the regularization loss as the KL divergence between the posterior and prior distributions
  reg_loss = 0
  for node in root.get_nodes():
    reg_loss += kl_divergence(node.mu, node.logvar, mu_prior, logvar_prior)
  # Return the weighted sum of the reconstruction and regularization losses
  return recon_loss + beta * reg_loss

# Train the RvNN-VAE on a dataset of scene hierarchies
rvnn_vae = RvNN_VAE()
optimizer = Adam(rvnn_vae.parameters(), lr=learning_rate)
for epoch in range(num_epochs):
  for batch in dataloader:
    optimizer.zero_grad()
    # Forward pass
    mu, logvar = rvnn_vae(batch.root)
    # Compute loss
    loss = loss_function(batch.root, mu, logvar)
    # Backward pass and update parameters
    loss.backward()
    optimizer.step()

# Generate novel scenes using the trained RvNN-VAE
for i in range(num_scenes):
  # Sample a latent code from a Gaussian distribution
  z = torch.randn(1, latent_dim)
  # Sample a scene hierarchy from a scene hierarchy grammar
  root = sample_grammar()
  # Decode the latent code and the scene hierarchy
  rvnn_vae.decode(z, root)
  # Post-process the scene to refine object positions and orientations
  post_process(root)
  # Save the generated scene
  save_scene(root)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Define the scene hierarchy node class
class Node:
  def __init__(self, features=None, children=None):
    # Initialize the node with object features and children nodes
    self.features = features
    self.children = children or []
    # Initialize the node with latent code and hidden state
    self.z = None
    self.hidden = None
    # Initialize the node with posterior parameters
    self.mu = None
    self.logvar = None
    # Initialize the node with reconstructed features
    self.recon_features = None

  def is_leaf(self):
    # Return True if the node has no children, False otherwise
    return len(self.children) == 0

  def get_leaves(self):
    # Return a list of leaf nodes in the subtree rooted at this node
    if self.is_leaf():
      return [self]
    else:
      leaves = []
      for child in self.children:
        leaves.extend(child.get_leaves())
      return leaves

  def get_nodes(self):
    # Return a list of all nodes in the subtree rooted at this node
    nodes = [self]
    for child in self.children:
      nodes.extend(child.get_nodes())
    return nodes

# Define the encoder module class
class Encoder(nn.Module):
  def __init__(self):
    # Initialize the encoder with linear layers and activation functions
    self.leaf_linear = nn.Linear(feature_dim, hidden_dim)
    self.nonleaf_linear = nn.Linear(hidden_dim * num_children, hidden_dim)
    self.relu = nn.ReLU()
    self.tanh = nn.Tanh()

  def encode_leaf(self, features):
    # Encode a leaf node using its object features
    hidden = self.leaf_linear(features)
    hidden = self.relu(hidden)
    return hidden

  def encode_nonleaf(self, child_hiddens):
    # Encode a non-leaf node using its children's hidden states
    hidden = torch.cat(child_hiddens, dim=1)
    hidden = self.nonleaf_linear(hidden)
    hidden = self.tanh(hidden)
    return hidden

# Define the decoder module class
class Decoder(nn.Module):
  def __init__(self):
    # Initialize the decoder with linear layers and activation functions
    self.leaf_linear = nn.Linear(latent_dim, feature_dim)
    self.nonleaf_linear = nn.Linear(latent_dim, latent_dim * num_children)
    self.sigmoid = nn.Sigmoid()
    self.tanh = nn.Tanh()

  def decode_leaf(self, z):
    # Decode a leaf node using its latent code
    features = self.leaf_linear(z)
    features = self.sigmoid(features)
    return features

  def decode_nonleaf(self, z):
    # Decode a non-leaf node using its latent code
    child_zs = self.nonleaf_linear(z)
    child_zs = torch.split(child_zs, latent_dim, dim=1)
    child_zs = [self.tanh(child_z) for child_z in child_zs]
    return child_zs

# Define the reparameterization function
def reparameterize(mu, logvar):
  # Sample a latent code from a Gaussian distribution with mean mu and variance exp(logvar)
  std = torch.exp(0.5 * logvar)
  eps = torch.randn_like(std)
  z = mu + eps * std
  return z

# Define the log-likelihood function
def log_likelihood(features, recon_features):
  # Compute the log-likelihood of the object features given the reconstructed features using a Gaussian distribution with unit variance and mean recon_features
  ll = -0.5 * torch.sum((features - recon_features) ** 2 + math.log(2 * math.pi))
  return ll

# Define the KL divergence function
def kl_divergence(mu1, logvar1, mu2, logvar2):
  # Compute the KL divergence between two Gaussian distributions with mean mu1, mu2 and variance exp(logvar1), exp(logvar2)
  kl = 0.5 * torch.sum(logvar2 - logvar1 - 1 + (logvar1.exp() + (mu1 - mu2) ** 2) / logvar2.exp())
  return kl

# Define the post-processing function
def post_process(root):
  # Refine the object positions and orientations based on collision detection and physical constraints
  for node in root.get_nodes():
    if node.is_leaf():
      # Adjust the object position and orientation to avoid collision with other objects or scene boundaries
      adjust_position_and_orientation(node.features)
      # Adjust the object scale to match its semantic category
      adjust_scale(node.features)
    else:
      # Adjust the node position and orientation to align with its parent node
      align_with_parent(node.features, node.parent.features)

# Define the scene hierarchy grammar function
def sample_grammar():
  # Sample a scene hierarchy from a scene hierarchy grammar that defines the rules for scene object grouping and splitting
  # The grammar is based on the scene hierarchy annotation scheme that captures support, surround, and co-occurrence relations among 3D objects
  # The grammar is defined as a set of production rules of the form A -> B C D ..., where A is a non-terminal symbol and B C D ... are terminal or non-terminal symbols
  # The grammar also specifies the probabilities of each production rule
  # The grammar starts with a root symbol S and ends with a set of leaf symbols that represent object categories
  # For example, one possible grammar is:
  # S -> K (0.25) | B (0.25) | O (0.25) | L (0.25)
  # K -> T C F M (1.0)
  # B -> B1 B2 (1.0)
  # B1 -> B C D (1.0)
  # B2 -> W D (0.5) | E D (0.5)
  # O -> D T C (1.0)
  # L -> S C T (1.0)
  # T -> table (1.0)
  # C -> chair (1.0)
  # F -> fridge (1.0)
  # M -> microwave (1.0)
  # B -> bed (1.0)
  # W -> wardrobe (1.0)
  # E -> desk (1.0)
  # S -> sofa (1.0)
  # D -> drawer (1.0)

  # Initialize an empty stack and a root node
  stack = []
  root = Node()
  stack.append((root, "S"))
  # Repeat until the stack is empty
  while stack:
    # Pop a node and a symbol from the stack
    node, symbol = stack.pop()
    # If the symbol is non-terminal, sample a production rule based on its probability and push the children nodes and symbols to the stack
    if symbol in non_terminals:
      rule = sample_rule(symbol)
      for child_symbol in rule:
        child_node = Node()
        node.children.append(child_node)
        stack.append((child_node, child_symbol))
    # If the symbol is terminal, sample an object category and features from the symbol and assign them to the node
    else:
      category = sample_category(symbol)
      features = sample_features(category)
      node.features = features
  # Return the root node of the scene hierarchy
  return root

# Define the RvNN-VAE class
class RvNN_VAE(nn.Module):
  def __init__(self):
    # Initialize the encoder and decoder modules
    self.encoder = Encoder()
    self.decoder = Decoder()
    # Initialize the parameters for the Gaussian prior and posterior
    self.mu_prior = nn.Parameter(torch.zeros(1, latent_dim))
    self.logvar_prior = nn.Parameter(torch.zeros(1, latent_dim))
    self.mu_posterior = nn.Linear(hidden_dim, latent_dim)
    self.logvar_posterior = nn.Linear(hidden_dim, latent_dim)

  def encode(self, node):
    # Recursively encode a scene hierarchy node and its children
    if node.is_leaf():
      # Encode a leaf node using its object features
      hidden = self.encoder.encode_leaf(node.features)
    else:
      # Encode a non-leaf node using its children's hidden states
      child_hiddens = [self.encode(child) for child in node.children]
      hidden = self.encoder.encode_nonleaf(child_hiddens)
    # Compute the posterior parameters for the node
    mu = self.mu_posterior(hidden)
    logvar = self.logvar_posterior(hidden)
    return hidden, mu, logvar

  def decode(self, z, node):
    # Recursively decode a latent code and a scene hierarchy node
    if node.is_leaf():
      # Decode a leaf node using its latent code
      features = self.decoder.decode_leaf(z)
      node.features = features
    else:
      # Decode a non-leaf node using its children's latent codes
      child_zs = self.decoder.decode_nonleaf(z)
      for i in range(len(node.children)):
        self.decode(child_zs[i], node.children[i])

  def forward(self, root):
    # Encode the root node of the scene hierarchy
    hidden, mu, logvar = self.encode(root)
    # Sample a latent code from the posterior distribution
    z = reparameterize(mu, logvar)
    # Decode the latent code and the root node
    self.decode(z, root)
    return mu, logvar