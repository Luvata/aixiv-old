---
title: 2306.00042v1 Graph-based methods coupled with specific distributional distances for adversarial attack detection
date: 2023-06-01
---

# [Graph-based methods coupled with specific distributional distances for adversarial attack detection](http://arxiv.org/abs/2306.00042v1)

authors: Dwight Nwaigwe, Lucrezia Carboni, Martial Mermillod, Sophie Achard, Michel Dojat


## What, Why and How

[1]: https://arxiv.org/abs/2306.00042 "[2306.00042] Graph-based methods coupled with specific distributional ..."
[2]: https://arxiv.org/pdf/2306.00042 "Graph-based methods coupled with specific distributional ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00042 "[2306.00042] Graph-based methods coupled with specific distributional ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a novel approach of detection and interpretation of adversarial attacks from a graph perspective. Adversarial attacks are carefully perturbed inputs that cause a neural network to misclassify them.
- **Why**: The paper aims to investigate the inner workings of adversarial attacks and how they affect the neural network's architecture and behavior. The paper also claims that few existing methods study adversarial attacks from a graph theory perspective.
- **How**: The paper computes an associated sparse graph for each input to the neural network, based on the network's architecture and activation patterns. The paper then introduces specific measures based on graph properties and distributional distances to predict and interpret adversarial attacks. The paper evaluates the proposed approach on two datasets (MNIST and CIFAR10) and compares it with existing methods.

## Main Contributions

[1]: https://arxiv.org/abs/2306.00042 "[2306.00042] Graph-based methods coupled with specific distributional ..."
[2]: https://arxiv.org/pdf/2306.00042 "Graph-based methods coupled with specific distributional ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00042 "[2306.00042] Graph-based methods coupled with specific distributional ..."

The paper[^1^][1] claims to make the following contributions:

- It introduces a novel way of computing a sparse graph for each input to a neural network, based on the network's architecture and activation patterns.
- It proposes specific measures based on graph properties and distributional distances to predict and interpret adversarial attacks.
- It evaluates the proposed approach on two datasets (MNIST and CIFAR10) and compares it with existing methods.
- It shows that graphs-based approaches help to investigate the inner workings of adversarial attacks.

## Method Summary

[1]: https://arxiv.org/abs/2306.00042 "[2306.00042] Graph-based methods coupled with specific distributional ..."
[2]: https://arxiv.org/pdf/2306.00042 "Graph-based methods coupled with specific distributional ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00042 "[2306.00042] Graph-based methods coupled with specific distributional ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes how to compute a sparse graph for each input to a neural network, based on the network's architecture and activation patterns. The paper defines a graph as a set of nodes and edges, where each node represents a neuron and each edge represents a weight between two neurons. The paper uses a thresholding technique to sparsify the graph and reduce the number of edges.
- The paper introduces two types of measures based on graph properties and distributional distances to predict and interpret adversarial attacks. The first type of measure is based on the graph Laplacian matrix, which captures the connectivity and structure of the graph. The paper computes the eigenvalues and eigenvectors of the Laplacian matrix and uses them to define measures such as spectral gap, algebraic connectivity, Fiedler vector, and spectral clustering. The second type of measure is based on the Wasserstein distance, which quantifies the dissimilarity between two probability distributions. The paper computes the Wasserstein distance between the distributions of node activations or edge weights for benign and adversarial inputs.
- The paper evaluates the proposed approach on two datasets (MNIST and CIFAR10) and compares it with existing methods. The paper uses three types of adversarial attacks (FGSM, PGD, and CW) to generate adversarial examples for each dataset. The paper uses a convolutional neural network (CNN) as the classifier for both datasets. The paper reports the accuracy, precision, recall, and F1-score of the proposed approach and the baseline methods for detecting adversarial examples. The paper also provides some visualizations and interpretations of the graph measures for different inputs.

## Pseudo Code

Here is a possible pseudo code to implement the paper:

```python
# Import libraries
import numpy as np
import scipy as sp
import tensorflow as tf
import sklearn as sk
import matplotlib.pyplot as plt

# Load datasets
mnist = tf.keras.datasets.mnist
cifar10 = tf.keras.datasets.cifar10

# Define CNN model
def cnn_model(input_shape, num_classes):
  model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(num_classes)
  ])
  return model

# Define adversarial attacks
def fgsm(model, x, y, epsilon):
  # Compute the gradient of the loss with respect to the input
  with tf.GradientTape() as tape:
    tape.watch(x)
    prediction = model(x)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y, prediction)
  gradient = tape.gradient(loss, x)

  # Compute the sign of the gradient
```