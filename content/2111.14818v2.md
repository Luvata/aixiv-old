---
title: 2111.14818v2 Blended Diffusion for Text-driven Editing of Natural Images
date: 2021-11-15
---

# [Blended Diffusion for Text-driven Editing of Natural Images](http://arxiv.org/abs/2111.14818v2)

authors: Omri Avrahami, Dani Lischinski, Ohad Fried


## What, Why and How

[1]: https://arxiv.org/abs/2111.14818 "Blended Diffusion for Text-driven Editing of Natural Images"
[2]: https://arxiv.org/pdf/2111.14818 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2111.14818v1 "arXiv.org"

Here is a summary of the paper:

```
What: The paper introduces a method for text-driven editing of natural images, based on a natural language description and a region of interest mask.

Why: The paper aims to provide a highly intuitive interface for image editing, and to overcome the limitations of existing methods that either require pixel-level supervision, or produce unrealistic or inconsistent results.

How: The paper leverages and combines a pretrained language-image model (CLIP) to steer the edit towards a user-provided text prompt, with a denoising diffusion probabilistic model (DDPM) to generate natural-looking results. The paper also proposes a spatial blending technique to fuse the edited region with the unchanged parts of the image, and an augmentation scheme to mitigate adversarial results.
```


## Main Contributions

[1]: https://arxiv.org/abs/2111.14818 "Blended Diffusion for Text-driven Editing of Natural Images"
[2]: https://arxiv.org/pdf/2111.14818 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2111.14818v1 "arXiv.org"

The paper claims to make the following contributions:

- The first solution for performing local (region-based) edits in generic natural images, based on a natural language description and an ROI mask.
- A novel method for combining a pretrained language-image model (CLIP) with a denoising diffusion probabilistic model (DDPM) to generate natural-looking results that match the text prompt.
- A spatial blending technique to seamlessly fuse the edited region with the unchanged parts of the image, at a progression of noise levels.
- An augmentation scheme to mitigate adversarial results that may arise from the language-image model.
- A comprehensive evaluation of the proposed method against several baselines and related methods, both qualitatively and quantitatively, on various text-driven editing tasks.

## Method Summary

[1]: https://arxiv.org/abs/2111.14818 "Blended Diffusion for Text-driven Editing of Natural Images"
[2]: https://arxiv.org/pdf/2111.14818 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2111.14818v1 "arXiv.org"

Here is a summary of the method section of the paper:

```
The method consists of three main steps: text-guided diffusion, spatial blending, and augmentation.

Text-guided diffusion: The method uses a pretrained language-image model (CLIP) to compute a text-image similarity score, and a denoising diffusion probabilistic model (DDPM) to generate a latent code that corresponds to the edited region. The method iteratively updates the latent code by minimizing the negative log-likelihood of the DDPM and maximizing the text-image similarity score. The method also uses an ROI mask to restrict the diffusion process to the region of interest.

Spatial blending: The method uses a spatial blending technique to fuse the edited region with the unchanged parts of the image. The method blends noised versions of the input image with the local text-guided diffusion latent at a progression of noise levels, using a learned blending mask. The method also applies a Poisson blending operation to smooth the boundaries between the edited and unchanged regions.

Augmentation: The method uses an augmentation scheme to mitigate adversarial results that may arise from the language-image model. The method applies random crops, flips, rotations, and color jittering to both the input image and the text-guided diffusion latent during training and inference. The method also uses a diversity loss to encourage different augmentations to produce different results.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2111.14818 "Blended Diffusion for Text-driven Editing of Natural Images"
[2]: https://arxiv.org/pdf/2111.14818 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2111.14818v1 "arXiv.org"

Here is a high-level pseudo code for the paper:

```
# Input: an image x, a text prompt t, and an ROI mask m
# Output: an edited image y that matches the text prompt t in the ROI

# Load a pretrained language-image model (CLIP) and a denoising diffusion probabilistic model (DDPM)
clip = load_clip_model()
ddpm = load_ddpm_model()

# Initialize the latent code z with random noise
z = torch.randn_like(x)

# Define the loss function as a combination of the negative log-likelihood of the DDPM and the text-image similarity score of the CLIP
def loss(z, x, t, m):
  # Compute the negative log-likelihood of the DDPM
  nll = ddpm.nll(z, x)
  # Compute the text-image similarity score of the CLIP
  sim = clip.similarity(z * m, t)
  # Return the weighted sum of the two terms
  return nll + lambda * sim

# Optimize the latent code z by gradient descent
for i in range(num_iterations):
  # Apply a random augmentation to z and x
  z_aug, x_aug = augment(z, x)
  # Compute the loss and the gradient
  l = loss(z_aug, x_aug, t, m)
  g = grad(l, z_aug)
  # Update z with a learning rate
  z = z - lr * g

# Generate the edited image y from the latent code z using the DDPM
y = ddpm.sample(z)

# Blend the edited image y with the input image x using a spatial blending technique
y = blend(y, x, m)

# Return the edited image y
return y
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2111.14818 "Blended Diffusion for Text-driven Editing of Natural Images"
[2]: https://arxiv.org/pdf/2111.14818 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2111.14818v1 "arXiv.org"

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import clip
import ddpm

# Define some hyperparameters
num_iterations = 1000 # number of optimization iterations
lr = 0.01 # learning rate
lambda = 0.1 # weight for the text-image similarity term
num_timesteps = 1000 # number of diffusion timesteps
sigma = 0.01 # noise level for the Poisson blending

# Input: an image x, a text prompt t, and an ROI mask m
# Output: an edited image y that matches the text prompt t in the ROI

# Load a pretrained language-image model (CLIP) and a denoising diffusion probabilistic model (DDPM)
clip_model = clip.load("ViT-B/32")
ddpm_model = ddpm.load("imagenet64")

# Initialize the latent code z with random noise
z = torch.randn_like(x)

# Define the loss function as a combination of the negative log-likelihood of the DDPM and the text-image similarity score of the CLIP
def loss(z, x, t, m):
  # Compute the negative log-likelihood of the DDPM
  nll = ddpm_model.nll(z, x)
  # Compute the text-image similarity score of the CLIP
  sim = clip_model.similarity(z * m, t)
  # Return the weighted sum of the two terms
  return nll + lambda * sim

# Define an optimizer for gradient descent
optimizer = torch.optim.Adam([z], lr=lr)

# Define an augmentation transform for random crops, flips, rotations, and color jittering
transform = torchvision.transforms.Compose([
  torchvision.transforms.RandomResizedCrop(size=x.shape[-2:]),
  torchvision.transforms.RandomHorizontalFlip(),
  torchvision.transforms.RandomRotation(degrees=15),
  torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)
])

# Optimize the latent code z by gradient descent
for i in range(num_iterations):
  # Apply a random augmentation to z and x
  z_aug = transform(z)
  x_aug = transform(x)
  # Compute the loss and the gradient
  l = loss(z_aug, x_aug, t, m)
  g = torch.autograd.grad(l, z_aug)[0]
  # Update z with a learning rate
  z = z - lr * g

# Generate the edited image y from the latent code z using the DDPM
y = ddpm_model.sample(z)

# Blend the edited image y with the input image x using a spatial blending technique
def blend(y, x, m):
  # Initialize an empty image b
  b = torch.zeros_like(x)
  # Loop over the diffusion timesteps from coarse to fine
  for k in reversed(range(num_timesteps)):
    # Compute the noise level for this timestep
    beta_k = ddpm_model.betas[k]
    alpha_k = 1 - beta_k.cumprod(-1)
    sigma_k = (beta_k / alpha_k).sqrt()
    # Add noise to x and y according to sigma_k
    x_noised = x + torch.randn_like(x) * sigma_k.view(1, -1, 1, 1)
    y_noised = y + torch.randn_like(y) * sigma_k.view(1, -1, 1, 1)
    # Learn a blending mask m_k that minimizes the negative log-likelihood of b under DDPM
    m_k = torch.nn.Parameter(torch.ones_like(m))
    optimizer_m_k = torch.optim.Adam([m_k], lr=lr)
    for j in range(num_iterations):
      b_noised = b + torch.randn_like(b) * sigma_k.view(1, -1, 1, 1)
      nll_b = ddpm_model.nll(b_noised, x_noised)
      g_m_k = torch.autograd.grad(nll_b, m_k)[0]
      m_k = m_k - lr * g_m_k
    # Blend y_noised and x_noised using m_k and update b
    b = y_noised * m_k + x_noised * (1 - m_k)
  # Apply a Poisson blending operation to smooth the boundaries between b and x using sigma as noise level
  b_poisson = poisson_blend(b, x, m, sigma)
  # Return b_poisson as the final blended image
  return b_poisson

# Apply the blending function to y and x using m
y = blend(y, x, m)

# Return the edited image y
return y

```