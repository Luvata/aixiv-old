---
title: 2106.13884v2 Multimodal Few-Shot Learning with Frozen Language Models
date: 2021-06-14
---

# [Multimodal Few-Shot Learning with Frozen Language Models](http://arxiv.org/abs/2106.13884v2)

authors: Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, Felix Hill


## What, Why and How

[1]: https://arxiv.org/abs/2106.13884 "Title: Multimodal Few-Shot Learning with Frozen Language Models - arXiv.org"
[2]: https://arxiv.org/pdf/2106.13884.pdf "Multimodal Few-Shot Learning with Frozen Language Models - arXiv.org"
[3]: https://scholar.archive.org/work/7slfq2ejj5hsfl5pubzcm7avge "Multimodal Few-Shot Learning with Frozen Language Models"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method for transferring the few-shot learning ability of a pre-trained language model to a multimodal setting (vision and language).
- **Why**: The paper aims to give a pre-trained language model access to visual information in a way that extends its few-shot learning capabilities to a multimodal setting, without changing its weights.
- **How**: The paper trains a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings.

## Main Contributions

[1]: https://arxiv.org/abs/2106.13884 "Title: Multimodal Few-Shot Learning with Frozen Language Models - arXiv.org"
[2]: https://arxiv.org/pdf/2106.13884.pdf "Multimodal Few-Shot Learning with Frozen Language Models - arXiv.org"
[3]: https://scholar.archive.org/work/7slfq2ejj5hsfl5pubzcm7avge "Multimodal Few-Shot Learning with Frozen Language Models"

According to the paper at [^1^][1], the main contributions are:

- **A simple and effective method for transferring the few-shot learning ability of a pre-trained language model to a multimodal setting (vision and language) without changing its weights.**
- **A vision encoder that maps images into the word embedding space of a pre-trained language model such that the language model generates captions for those images.**
- **A multimodal few-shot learner that can learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings.**
- **An extensive evaluation of the multimodal few-shot learner on a variety of established and new benchmarks, demonstrating its ability to rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge.**

## Method Summary

[1]: https://arxiv.org/abs/2106.13884 "Title: Multimodal Few-Shot Learning with Frozen Language Models - arXiv.org"
[2]: https://arxiv.org/pdf/2106.13884.pdf "Multimodal Few-Shot Learning with Frozen Language Models - arXiv.org"
[3]: https://scholar.archive.org/work/7slfq2ejj5hsfl5pubzcm7avge "Multimodal Few-Shot Learning with Frozen Language Models"

Here is a summary of the method section of the paper at [^1^][1]:

- **The method consists of two main components: a vision encoder and a pre-trained language model.**
- **The vision encoder is a convolutional neural network that takes an image as input and outputs a sequence of continuous embeddings, each corresponding to a region of the image.**
- **The pre-trained language model is a large-scale auto-regressive transformer that takes a sequence of embeddings as input and generates text as output. The language model is frozen and not fine-tuned on any task.**
- **The vision encoder is trained to align the image embeddings with the word embeddings of the language model, such that the language model generates captions for the images when prompted with the image embeddings.**
- **The multimodal few-shot learner is formed by concatenating the image embeddings and the text embeddings as input to the language model. The text embeddings can be either natural language prompts or examples of tasks, such as questions, answers, labels, etc.**
- **The multimodal few-shot learner can learn new tasks by conditioning on examples, represented as a sequence of multiple interleaved image and text embeddings. The examples can be either given explicitly or retrieved from a large corpus of aligned image and caption data.**

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a vision encoder that takes an image and outputs a sequence of embeddings
vision_encoder = ConvNet()

# Load a pre-trained language model that takes a sequence of embeddings and outputs text
language_model = Transformer()
language_model.freeze()

# Train the vision encoder to align the image embeddings with the word embeddings of the language model
for image, caption in image_caption_data:
  image_embeddings = vision_encoder(image)
  caption_embeddings = language_model.word_embeddings(caption)
  loss = alignment_loss(image_embeddings, caption_embeddings)
  vision_encoder.backprop(loss)

# Define a multimodal few-shot learner that concatenates the image embeddings and the text embeddings as input to the language model
def multimodal_few_shot_learner(image, text):
  image_embeddings = vision_encoder(image)
  text_embeddings = language_model.word_embeddings(text)
  input_embeddings = concatenate(image_embeddings, text_embeddings)
  output_text = language_model.generate(input_embeddings)
  return output_text

# Use the multimodal few-shot learner to learn new tasks by conditioning on examples
examples = retrieve_examples(task) # or provide_examples(task)
for image, text in examples:
  output_text = multimodal_few_shot_learner(image, text)
  evaluate(output_text) # or interact(output_text)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers

# Define the hyperparameters
image_size = 224 # the size of the input image
num_regions = 49 # the number of regions to divide the image into
region_size = 16 # the size of each region
embedding_size = 768 # the size of the embeddings
vocab_size = 50257 # the size of the vocabulary
max_length = 64 # the maximum length of the text
batch_size = 32 # the size of the mini-batch
num_epochs = 10 # the number of epochs to train
learning_rate = 1e-4 # the learning rate for the optimizer

# Define a vision encoder that takes an image and outputs a sequence of embeddings
class VisionEncoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pre-trained ResNet-50 model as the backbone
    self.backbone = torchvision.models.resnet50(pretrained=True)
    # Remove the last layer and average pool
    self.backbone = torch.nn.Sequential(*list(self.backbone.children())[:-2])
    # Add a linear layer to project the features to the embedding size
    self.projection = torch.nn.Linear(2048, embedding_size)
  
  def forward(self, image):
    # Resize and normalize the image
    image = torchvision.transforms.Resize(image_size)(image)
    image = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
    # Extract the features from the backbone
    features = self.backbone(image)
    # Reshape and permute the features to (batch_size, num_regions, 2048)
    features = features.view(batch_size, 2048, num_regions).permute(0, 2, 1)
    # Project the features to the embedding size
    embeddings = self.projection(features)
    return embeddings

# Load a pre-trained GPT-2 language model that takes a sequence of embeddings and outputs text
language_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')
language_model.freeze()

# Define a loss function that measures the alignment between the image embeddings and the caption embeddings
def alignment_loss(image_embeddings, caption_embeddings):
  # Compute the cosine similarity between each pair of image and caption embeddings
  similarity = torch.nn.functional.cosine_similarity(image_embeddings.unsqueeze(1), caption_embeddings.unsqueeze(2), dim=-1)
  # Compute the softmax over the similarity matrix along each dimension
  row_softmax = torch.nn.functional.softmax(similarity, dim=1)
  col_softmax = torch.nn.functional.softmax(similarity, dim=2)
  # Compute the alignment loss as the negative log-likelihood of the diagonal entries
  loss = -torch.log(torch.diagonal(row_softmax * col_softmax, dim1=1, dim2=2) + 1e-8).mean()
  return loss

# Define an optimizer for the vision encoder parameters
optimizer = torch.optim.Adam(vision_encoder.parameters(), lr=learning_rate)

# Load a dataset of aligned image and caption data (e.g. COCO)
dataset = torchvision.datasets.CocoCaptions(root='images', annFile='annotations', transform=torchvision.transforms.ToTensor())
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Train the vision encoder to align the image embeddings with the word embeddings of the language model
for epoch in range(num_epochs):
  for batch in dataloader:
    # Get a batch of images and captions
    images, captions = batch
    # Convert the captions to token ids using the GPT-2 tokenizer
    captions = transformers.GPT2Tokenizer.from_pretrained('gpt2').batch_encode_plus(captions, padding=True, return_tensors='pt')
    # Get the image embeddings from the vision encoder
    image_embeddings = vision_encoder(images)
    # Get the caption embeddings from the language model word embeddings
    caption_embeddings = language_model.transformer.wte(captions['input_ids'])
    # Compute the alignment loss
    loss = alignment_loss(image_embeddings, caption_embeddings)
    # Backpropagate and update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  
  print(f'Epoch {epoch}, Loss {loss.item()}')

# Define a multimodal few-shot learner that concatenates the image embeddings and 
the text embeddings as input to the language model
def multimodal_few_shot_learner(image, text):
  # Get the image embeddings from the vision encoder
  image_embeddings = vision_encoder(image)
  # Convert the text to token ids using the GPT-2 tokenizer
  text = transformers.GPT2Tokenizer.from_pretrained('gpt2').encode_plus(text, return_tensors='pt')
  # Get the text embeddings from the language model word embeddings
  text_embeddings = language_model.transformer.wte(text['input_ids'])
  # Concatenate the image embeddings and the text embeddings
  input_embeddings = torch.cat([image_embeddings, text_embeddings], dim=1)
  # Generate text from the language model using the input embeddings
  output_text = language_model.generate(input_embeddings=input_embeddings, max_length=max_length)
  # Decode the output text using the GPT-2 tokenizer
  output_text = transformers.GPT2Tokenizer.from_pretrained('gpt2').decode(output_text[0])
  return output_text

# Use the multimodal few-shot learner to learn new tasks by conditioning on examples
examples = retrieve_examples(task) # or provide_examples(task)
for image, text in examples:
  output_text = multimodal_few_shot_learner(image, text)
  evaluate(output_text) # or interact(output_text)
```