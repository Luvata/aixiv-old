---
title: 2208.09021v3 VAuLT  Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media
date: 2022-08-10
---

# [VAuLT: Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media](http://arxiv.org/abs/2208.09021v3)

authors: Georgios Chochlakis, Tejas Srinivasan, Jesse Thomason, Shrikanth Narayanan


## What, Why and How

[1]: https://arxiv.org/pdf/2208.09021 "ABSTRACT arXiv:2208.09021v3 [cs.CV] 25 Jan 2023"
[2]: https://arxiv.org/abs/2208.09021 "[2208.09021] VAuLT: Augmenting the Vision-and-Language Transformer for ..."
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the paper:

- **What**: The paper proposes a new model called VAuLT (Vision-and-Augmented-Language Transformer) that extends the existing ViLT (Vision-and-Language Transformer) model for sentiment classification on social media data.
- **Why**: The paper argues that ViLT, which is pretrained on captioning datasets, lacks linguistic diversity and capacity to handle complex and affective text inputs that are common in social media data. The paper also aims to improve the efficiency of multimodal modeling by avoiding object detection and using linear projection of image patches instead.
- **How**: The paper introduces a novel way of augmenting the language input of ViLT with the output representations of a large language model (LM) like BERT. The paper shows that joint training of the LM and ViLT can improve the performance of sentiment classification on several datasets, such as TWITTER-2015, TWITTER-2017, MVSA-Single and MVSA-Multiple. The paper also provides an ablation study and a qualitative analysis to demonstrate the effectiveness of VAuLT.

## Main Contributions

The paper claims the following contributions:

- A new model, VAuLT, that augments the language input of ViLT with the output representations of a large LM like BERT.
- A demonstration of the benefits of VAuLT over ViLT on sentiment classification tasks on social media data, achieving state-of-the-art or comparable results on four datasets.
- An analysis of the language capacity of ViLT and the impact of VAuLT on different aspects of sentiment classification, such as target detection, polarity detection, and multimodal fusion.

## Method Summary

[1]: https://arxiv.org/pdf/2208.09021 "ABSTRACT arXiv:2208.09021v3 [cs.CV] 25 Jan 2023"
[2]: https://arxiv.org/abs/2208.09021 "[2208.09021] VAuLT: Augmenting the Vision-and-Language Transformer for ..."
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the method section of the paper:

- The paper describes the architecture of VAuLT, which consists of three main components: a **language encoder**, a **vision encoder**, and a **cross-modal encoder**.
- The language encoder is a pretrained LM like BERT that takes the text input and produces a sequence of hidden representations. The vision encoder is a pretrained ViLT model that takes the image input and produces a sequence of patch embeddings using a linear projection layer. The cross-modal encoder is also a pretrained ViLT model that takes the concatenated output of the language and vision encoders and produces a final sequence of multimodal representations.
- The paper introduces a novel way of **augmenting** the language input of ViLT with the output representations of BERT. The paper argues that this can enhance the language capacity of ViLT and enable better multimodal fusion. The paper also proposes a **residual connection** between the language encoder and the cross-modal encoder to preserve the original language information.
- The paper uses different **classification heads** on top of the cross-modal encoder for different sentiment classification tasks. For target-oriented sentiment classification, the paper uses a span extraction head that predicts the start and end indices of the target span and its polarity. For sentiment classification, the paper uses a linear layer that predicts the polarity of the whole post.
- The paper trains VAuLT on four datasets: TWITTER-2015, TWITTER-2017, MVSA-Single and MVSA-Multiple. The paper uses AdamW optimizer with a learning rate of 1e-5 and a batch size of 32. The paper fine-tunes BERT and ViLT jointly for 10 epochs on each dataset. The paper also uses data augmentation techniques such as backtranslation and mixup to increase the diversity and robustness of the training data.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the language encoder (BERT), the vision encoder (ViLT), and the cross-modal encoder (ViLT)
language_encoder = BERT(pretrained=True)
vision_encoder = ViLT(pretrained=True)
cross_modal_encoder = ViLT(pretrained=True)

# Define the classification head according to the task
if task == "target-oriented":
  classification_head = SpanExtractionHead()
elif task == "sentiment":
  classification_head = LinearLayer()

# Load the dataset and apply data augmentation techniques
dataset = load_dataset(task)
dataset = augment_dataset(dataset)

# Train VAuLT on the dataset
for epoch in range(10):
  for batch in dataset:
    # Get the text and image inputs from the batch
    text_input = batch["text"]
    image_input = batch["image"]

    # Encode the text input using BERT and get the output representations
    text_output = language_encoder(text_input)

    # Encode the image input using ViLT and get the patch embeddings
    image_output = vision_encoder(image_input)

    # Concatenate the text output and image output and feed them to the cross-modal encoder
    cross_modal_input = concatenate(text_output, image_output)
    cross_modal_output = cross_modal_encoder(cross_modal_input)

    # Add a residual connection between the text output and the cross-modal output
    cross_modal_output = cross_modal_output + text_output

    # Use the classification head to predict the sentiment labels
    predictions = classification_head(cross_modal_output)

    # Compute the loss and update the parameters of VAuLT
    loss = compute_loss(predictions, batch["labels"])
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import transformers
import datasets

# Define the hyperparameters
batch_size = 32
learning_rate = 1e-5
num_epochs = 10
max_length = 128
num_patches = 16

# Define the language encoder (BERT), the vision encoder (ViLT), and the cross-modal encoder (ViLT)
language_encoder = transformers.BertModel.from_pretrained("bert-base-uncased")
vision_encoder = transformers.ViTModel.from_pretrained("google/vit-base-patch16-224")
cross_modal_encoder = transformers.ViTModel.from_pretrained("google/vit-base-patch16-224")

# Define the classification head according to the task
if task == "target-oriented":
  # The span extraction head consists of two linear layers that predict the start and end indices of the target span and its polarity
  classification_head = torch.nn.Sequential(
    torch.nn.Linear(cross_modal_encoder.config.hidden_size, 2),
    torch.nn.Linear(cross_modal_encoder.config.hidden_size, 3)
  )
elif task == "sentiment":
  # The linear layer predicts the polarity of the whole post
  classification_head = torch.nn.Linear(cross_modal_encoder.config.hidden_size, 3)

# Load the dataset and apply data augmentation techniques
dataset = datasets.load_dataset(task)
dataset = augment_dataset(dataset)

# Create a data loader that batches and shuffles the data
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Create an optimizer that updates the parameters of VAuLT using AdamW algorithm
optimizer = transformers.AdamW(params=list(language_encoder.parameters()) + list(vision_encoder.parameters()) + list(cross_modal_encoder.parameters()) + list(classification_head.parameters()), lr=learning_rate)

# Train VAuLT on the dataset
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the text and image inputs from the batch
    text_input = batch["text"]
    image_input = batch["image"]

    # Tokenize the text input using BERT tokenizer and get the input ids and attention mask
    tokenizer = transformers.BertTokenizer.from_pretrained("bert-base-uncased")
    text_input = tokenizer(text_input, padding=True, truncation=True, max_length=max_length, return_tensors="pt")
    input_ids = text_input["input_ids"]
    attention_mask = text_input["attention_mask"]

    # Encode the text input using BERT and get the output representations (the last hidden states)
    text_output = language_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state

    # Encode the image input using ViLT and get the patch embeddings (the last hidden states)
    image_output = vision_encoder(pixel_values=image_input).last_hidden_state

    # Concatenate the text output and image output along the sequence dimension and feed them to the cross-modal encoder
    cross_modal_input = torch.cat([text_output, image_output], dim=1)
    cross_modal_output = cross_modal_encoder(inputs_embeds=cross_modal_input).last_hidden_state

    # Add a residual connection between the text output and the cross-modal output
    cross_modal_output = cross_modal_output + text_output

    # Use the classification head to predict the sentiment labels
    predictions = classification_head(cross_modal_output)

    # Compute the loss using cross entropy criterion and update the parameters of VAuLT
    loss = torch.nn.CrossEntropyLoss()(predictions, batch["labels"])
    loss.backward()
    optimizer.step()
```