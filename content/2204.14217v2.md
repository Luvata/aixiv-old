---
title: 2204.14217v2 CogView2  Faster and Better Text-to-Image Generation via Hierarchical Transformers
date: 2022-04-15
---

# [CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers](http://arxiv.org/abs/2204.14217v2)

authors: Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang


## What, Why and How

[1]: https://arxiv.org/abs/2204.14217 "[2204.14217] CogView2: Faster and Better Text-to-Image Generation via ..."
[2]: https://arxiv.org/pdf/2204.14217 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.14217v2 "[2112.14217v2] Efficient Automatic Differentiation of Implicit Functions"

Here is a summary of the paper:

- **What**: The paper proposes a new text-to-image system called CogView2, which uses hierarchical transformers and local parallel auto-regressive generation to produce high-resolution images from natural language descriptions.
- **Why**: The paper aims to overcome the limitations of existing transformer-based text-to-image models, such as slow generation, expensive high-resolution training, and unidirectionality.
- **How**: The paper pretrains a 6B-parameter transformer with a cross-modal general language model (CogLM) task, which learns to predict both text and image tokens from a large-scale corpus. The paper then finetunes the model for fast super-resolution, which upsamples low-resolution images to high-resolution ones based on the input text. The paper also introduces a hierarchical transformer architecture that splits the image into patches and generates them in parallel with local attention. The paper evaluates the model on various text-to-image tasks and shows that it can generate realistic and diverse images with fine details and support interactive text-guided editing.

## Main Contributions

[1]: https://arxiv.org/abs/2204.14217 "[2204.14217] CogView2: Faster and Better Text-to-Image Generation via ..."
[2]: https://arxiv.org/pdf/2204.14217 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.14217v2 "[2112.14217v2] Efficient Automatic Differentiation of Implicit Functions"

The paper claims the following contributions:

- It proposes a new text-to-image system called CogView2, which can generate high-resolution images from natural language descriptions in both Chinese and English[^1^][1] [^2^][2].
- It introduces a hierarchical transformer architecture that splits the image into patches and generates them in parallel with local attention, which reduces the complexity and improves the speed of generation[^1^][1] [^2^][2].
- It pretrains a 6B-parameter transformer with a cross-modal general language model (CogLM) task, which learns to predict both text and image tokens from a large-scale corpus[^1^][1] [^2^][2].
- It finetunes the model for fast super-resolution, which upsamples low-resolution images to high-resolution ones based on the input text[^1^][1] [^2^][2].
- It evaluates the model on various text-to-image tasks and shows that it can generate realistic and diverse images with fine details and support interactive text-guided editing[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/abs/2204.14217 "[2204.14217] CogView2: Faster and Better Text-to-Image Generation via ..."
[2]: https://arxiv.org/pdf/2204.14217 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.14217v2 "[2112.14217v2] Efficient Automatic Differentiation of Implicit Functions"

Here is a summary of the method section of the paper:

- The paper describes the hierarchical transformer architecture that consists of two levels: a global transformer and a local transformer[^1^][2].
- The global transformer takes the input text and a low-resolution image as inputs and generates a sequence of patches, each representing a region of the high-resolution image[^1^][2].
- The local transformer takes each patch as input and generates the corresponding image tokens in parallel with local attention, which only attends to nearby tokens within the same patch[^1^][2].
- The paper also explains how the model is pretrained with the cross-modal general language model (CogLM) task, which predicts both text and image tokens from a large-scale corpus of text-image pairs[^1^][2].
- The paper details how the model is finetuned for fast super-resolution, which upsamples low-resolution images to high-resolution ones based on the input text[^1^][2].
- The paper illustrates how the model can support interactive text-guided editing on images by allowing the user to modify the input text and generate new images accordingly[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the global transformer and the local transformer
global_transformer = Transformer(num_layers = L, num_heads = H, hidden_size = D)
local_transformer = Transformer(num_layers = L', num_heads = H', hidden_size = D')

# Pretrain the model with the CogLM task
for each (text, image) pair in the corpus:
  # Tokenize the text and the image
  text_tokens = tokenize(text)
  image_tokens = tokenize(image)
  
  # Mask some text and image tokens randomly
  text_mask = generate_mask(text_tokens)
  image_mask = generate_mask(image_tokens)
  
  # Concatenate the text and image tokens
  input_tokens = [CLS] + text_tokens + [SEP] + image_tokens + [SEP]
  
  # Feed the input tokens to the global transformer
  output_tokens = global_transformer(input_tokens)
  
  # Compute the cross entropy loss for the masked tokens
  loss = cross_entropy(output_tokens, input_tokens, mask = text_mask + image_mask)
  
  # Update the model parameters with gradient descent
  update_parameters(loss)

# Finetune the model for fast super-resolution
for each (text, low_res_image, high_res_image) triplet in the dataset:
  # Tokenize the text and the images
  text_tokens = tokenize(text)
  low_res_tokens = tokenize(low_res_image)
  high_res_tokens = tokenize(high_res_image)
  
  # Concatenate the text and low resolution tokens
  input_tokens = [CLS] + text_tokens + [SEP] + low_res_tokens + [SEP]
  
  # Feed the input tokens to the global transformer
  patch_tokens = global_transformer(input_tokens)
  
  # Split the patch tokens into patches
  patches = split(patch_tokens)
  
  # Feed each patch to the local transformer in parallel
  image_tokens = parallel(local_transformer(patch) for patch in patches)
  
  # Compute the cross entropy loss for the high resolution tokens
  loss = cross_entropy(image_tokens, high_res_tokens)
  
  # Update the model parameters with gradient descent
  update_parameters(loss)

# Generate images from text descriptions
for each text in the input:
  # Tokenize the text
  text_tokens = tokenize(text)
  
  # Generate a low resolution image randomly or from a sketch
  low_res_image = generate_low_res_image()
  
  # Tokenize the low resolution image
  low_res_tokens = tokenize(low_res_image)
  
  # Concatenate the text and low resolution tokens
  input_tokens = [CLS] + text_tokens + [SEP] + low_res_tokens + [SEP]
  
  # Feed the input tokens to the global transformer
  patch_tokens = global_transformer(input_tokens)
  
  # Split the patch tokens into patches
  patches = split(patch_tokens)
  
  # Feed each patch to the local transformer in parallel
  image_tokens = parallel(local_transformer(patch) for patch in patches)
  
  # Detokenize the image tokens
  image = detokenize(image_tokens)
  
  # Output the image
  output(image)

# Edit images with text modifications
for each (text, image) pair in the input:
  # Tokenize the text and the image
  text_tokens = tokenize(text)
  image_tokens = tokenize(image)
  
  # Modify some text tokens according to user's intention
  text_tokens = modify(text_tokens)
  
  # Concatenate the text and image tokens
  input_tokens = [CLS] + text_tokens + [SEP] + image_tokens + [SEP]
  
  # Feed the input tokens to the global transformer
  patch_tokens = global_transformer(input_tokens)
  
   # Split the patch tokens into patches
   patches = split(patch_tokens)

   # Feed each patch to the local transformer in parallel 
   image_tokens = parallel(local_transformer(patch) for patch in patches)

   # Detokenize the image tokens 
   image = detokenize(image_tokens)

   # Output the edited image 
   output(image) 
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define some hyperparameters
L = 32 # number of layers for the global transformer
H = 32 # number of heads for the global transformer
D = 2048 # hidden size for the global transformer
L' = 16 # number of layers for the local transformer
H' = 16 # number of heads for the local transformer
D' = 1024 # hidden size for the local transformer
V = 8192 # vocabulary size for the text and image tokens
P = 64 # patch size for the image tokens
B = 32 # batch size for training and inference
LR = 0.001 # learning rate for optimization
EPOCHS = 100 # number of epochs for pretraining and finetuning

# Define the tokenizer for text and image tokens
tokenizer = transformers.GPT2Tokenizer(vocab_size = V)

# Define the global transformer model
global_transformer = transformers.GPT2Model(
    vocab_size = V,
    n_layer = L,
    n_head = H,
    n_embd = D,
    add_cross_attention = True # enable cross attention between text and image tokens
)

# Define the local transformer model
local_transformer = transformers.GPT2Model(
    vocab_size = V,
    n_layer = L',
    n_head = H',
    n_embd = D',
    attention_window = P // 2 # use local attention within each patch
)

# Define the optimizer for pretraining and finetuning
optimizer = torch.optim.Adam(
    params = list(global_transformer.parameters()) + list(local_transformer.parameters()),
    lr = LR
)

# Define the loss function for pretraining and finetuning
loss_fn = torch.nn.CrossEntropyLoss()

# Load the corpus of text-image pairs for pretraining
corpus = load_corpus()

# Pretrain the model with the CogLM task
for epoch in range(EPOCHS):
  for batch in corpus.batch(B):
    # Tokenize the text and the image in each pair
    text_tokens = tokenizer(batch.text)
    image_tokens = tokenizer(batch.image)
    
    # Mask some text and image tokens randomly with a probability of 15%
    text_mask = torch.bernoulli(torch.full(text_tokens.shape, 0.15)).bool()
    image_mask = torch.bernoulli(torch.full(image_tokens.shape, 0.15)).bool()
    
    # Replace the masked tokens with a special [MASK] token
    text_tokens[text_mask] = tokenizer.mask_token_id
    image_tokens[image_mask] = tokenizer.mask_token_id
    
    # Concatenate the text and image tokens with special tokens [CLS] and [SEP]
    input_tokens = torch.cat([
        torch.full((B, 1), tokenizer.cls_token_id),
        text_tokens,
        torch.full((B, 1), tokenizer.sep_token_id),
        image_tokens,
        torch.full((B, 1), tokenizer.sep_token_id)
    ], dim = -1)
    
    # Feed the input tokens to the global transformer
    output_tokens, _ = global_transformer(input_tokens)
    
    # Compute the cross entropy loss for the masked tokens
    loss_text = loss_fn(output_tokens[:, :text_tokens.shape[-1]], text_tokens)
    loss_image = loss_fn(output_tokens[:, -image_tokens.shape[-1]:], image_tokens)
    loss_total = loss_text + loss_image
    
    # Update the model parameters with gradient descent
    optimizer.zero_grad()
    loss_total.backward()
    optimizer.step()

# Load the dataset of text-low_res_image-high_res_image triplets for finetuning
dataset = load_dataset()

# Finetune the model for fast super-resolution
for epoch in range(EPOCHS):
  for batch in dataset.batch(B):
    # Tokenize the text and the images in each triplet
    text_tokens = tokenizer(batch.text)
    low_res_tokens = tokenizer(batch.low_res_image)
    high_res_tokens = tokenizer(batch.high_res_image)
    
    # Concatenate the text and low resolution tokens with special tokens [CLS] and [SEP]
    input_tokens = torch.cat([
        torch.full((B, 1), tokenizer.cls_token_id),
        text_tokens,
        torch.full((B, 1), tokenizer.sep_token_id),
        low_res_tokens,
        torch.full((B, 1), tokenizer.sep_token_id)
     ], dim=-1)
     
     # Feed the input tokens to the global transformer 
     patch_tokens, _= global_transformer(input_tokens)

     # Split the patch tokens into patches of size P x P 
     patches = torch.reshape(patch_tokens, (B, -1, P, P))

     # Feed each patch to the local transformer in parallel 
     image_tokens = torch.cat([local_transformer(patch)[0] for patch in patches], dim=1)

     # Compute the cross entropy loss for the high resolution tokens 
     loss = loss_fn(image_tokens, high_res_tokens)

     # Update the model parameters with gradient descent 
     optimizer.zero_grad()
     loss.backward()
     optimizer.step()

# Load the input text for image generation
input_text = load_input_text()

# Generate images from text descriptions
for text in input_text:
  # Tokenize the text
  text_tokens = tokenizer(text)
  
  # Generate a low resolution image randomly or from a sketch
  low_res_image = generate_low_res_image()
  
  # Tokenize the low resolution image
  low_res_tokens = tokenizer(low_res_image)
  
  # Concatenate the text and low resolution tokens with special tokens [CLS] and [SEP]
  input_tokens = torch.cat([
      torch.full((1, 1), tokenizer.cls_token_id),
      text_tokens,
      torch.full((1, 1), tokenizer.sep_token_id),
      low_res_tokens,
      torch.full((1, 1), tokenizer.sep_token_id)
   ], dim=-1)
   
   # Feed the input tokens to the global transformer 
   patch_tokens, _= global_transformer(input_tokens)

   # Split the patch tokens into patches of size P x P 
   patches = torch.reshape(patch_tokens, (1, -1, P, P))

   # Feed each patch to the local transformer in parallel 
   image_tokens = torch.cat([local_transformer(patch)[0] for patch in patches], dim=1)

   # Detokenize the image tokens 
   image = tokenizer.decode(image_tokens)

   # Output the image 
   output(image)

# Load the input text-image pairs for image editing
input_pairs = load_input_pairs()

# Edit images with text modifications
for (text, image) in input_pairs:
  # Tokenize the text and the image
  text_tokens = tokenizer(text)
  image_tokens = tokenizer(image)
  
  # Modify some text tokens according to user's intention
  text_tokens = modify(text_tokens)
  
  # Concatenate the text and image tokens with special tokens [CLS] and [SEP]
  input_tokens = torch.cat([
      torch.full((1, 1), tokenizer.cls_token_id),
      text_tokens,
      torch.full((1, 1), tokenizer.sep_token_id),
      image_tokens,
      torch.full((1, 1), tokenizer.sep_token_id)
   ], dim=-1)
   
   # Feed the input tokens to the global transformer 
   patch_tokens, _= global_transformer(input_tokens)

   # Split the patch tokens into patches of size P x P 
   patches = torch.reshape(patch_tokens, (1, -1, P, P))

   # Feed each patch to the local transformer in parallel 
   image_tokens = torch.cat([local_transformer(patch)[0] for patch in patches], dim=1)

   # Detokenize the image tokens 
   image = tokenizer.decode(image_tokens)

   # Output the edited image 
   output(image) 
```