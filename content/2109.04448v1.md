---
title: 2109.04448v1 Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers
date: 2021-09-05
---

# [Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](http://arxiv.org/abs/2109.04448v1)

authors: Stella Frank, Emanuele Bugliarello, Desmond Elliott


## What, Why and How

[1]: https://arxiv.org/abs/2109.04448 "[2109.04448] Vision-and-Language or Vision-for-Language? On ... - arXiv.org"
[2]: https://arxiv.org/pdf/2109.04448v1.pdf "arXiv:2109.04448v1 [cs.CL] 9 Sep 2021"
[3]: http://export.arxiv.org/abs/2106.04448v1 "[2106.04448v1] Manifest Spacetime Supersymmetry and the Superstring"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a diagnostic method based on cross-modal input ablation to assess the extent to which pretrained vision-and-language BERT models integrate cross-modal information. The method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. The paper applies this method to three models: ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), and UNITER (Chen et al., 2020).
- **Why**: The paper aims to answer the question of whether these models are vision-and-language or vision-for-language, i.e., whether they use visual information for predicting text and vice versa. The paper argues that this question is important for understanding how these models learn cross-modal representations and for designing better models in the future.
- **How**: The paper uses two modality-specific tasks that mirror the model pretraining objectives: masked language modelling for text and masked region classification for images. The paper ablates inputs from one modality in three ways: none (no ablation), object (ablate only the region aligned with a masked word), and all (ablate the entire image or sentence). The paper measures the model performance by the accuracy of predicting the masked data. The paper also uses cross-modal grounding alignments obtained from Flickr30k Entities (Plummer et al., 2015) to select which regions or words to ablate. The paper compares the performance of the models across different ablation settings and modalities.

## Main Contributions

According to the paper, the main contributions are:

- A novel diagnostic method based on cross-modal input ablation to measure the cross-modal influence of pretrained vision-and-language BERT models.
- A comprehensive analysis of three state-of-the-art models using this method on two modality-specific tasks and a large-scale dataset with cross-modal grounding alignments.
- A finding that these models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.

## Method Summary

The method section of the paper describes the cross-modal input ablation method in detail. It consists of the following steps:

- Select a pretrained vision-and-language BERT model and a modality-specific task (masked language modelling or masked region classification).
- For each multimodal input (image and sentence pair), mask one or more tokens or regions randomly and obtain the cross-modal grounding alignments between them using Flickr30k Entities.
- Ablate inputs from one modality in three ways: none (no ablation), object (ablate only the region aligned with a masked word), and all (ablate the entire image or sentence). Ablation is done by replacing the input with a special token ([MASK] for text and [IMG] for images) or a zero vector (for image features).
- Feed the ablated input to the model and obtain the prediction for the masked data in the other modality. Compute the accuracy of the prediction by comparing it with the ground truth.
- Repeat the above steps for different ablation settings and modalities and compare the model performance across them.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a pretrained V&L BERT model and a modality-specific task
model = VLBERT()
task = MLM() # or MRC()

# Define a dataset of multimodal inputs and cross-modal grounding alignments
dataset = Flickr30kEntities()

# Define the ablation settings and modalities
ablation_settings = ["none", "object", "all"]
modalities = ["text", "image"]

# Define a function to ablate inputs from one modality
def ablate_input(input, modality, ablation_setting):
  # input is a tuple of (image, sentence)
  # modality is either "text" or "image"
  # ablation_setting is either "none", "object", or "all"
  if ablation_setting == "none":
    return input # no ablation
  elif ablation_setting == "object":
    # ablate only the region aligned with a masked word
    # or the word aligned with a masked region
    alignment = get_alignment(input) # from Flickr30k Entities
    if modality == "text":
      # replace the word with [MASK]
      input[1][alignment[1]] = "[MASK]"
    else:
      # replace the region with [IMG]
      input[0][alignment[0]] = "[IMG]"
    return input
  else:
    # ablate the entire image or sentence
    if modality == "text":
      # replace the sentence with [MASK]
      input[1] = ["[MASK]"] * len(input[1])
    else:
      # replace the image with zero vector
      input[0] = [0] * len(input[0])
    return input

# Define a function to compute the accuracy of predicting masked data
def compute_accuracy(prediction, ground_truth):
  # prediction and ground_truth are lists of tokens or regions
  # return the percentage of correct predictions
  correct = 0
  total = len(prediction)
  for i in range(total):
    if prediction[i] == ground_truth[i]:
      correct += 1
  return correct / total

# Loop over the dataset and the ablation settings and modalities
for input in dataset:
  # input is a tuple of (image, sentence)
  # mask one or more tokens or regions randomly
  masked_input, masked_data = mask_input(input)
  for ablation_setting in ablation_settings:
    for modality in modalities:
      # ablate inputs from one modality
      ablated_input = ablate_input(masked_input, modality, ablation_setting)
      # feed the ablated input to the model and get the prediction
      prediction = model(ablated_input, task)
      # compute the accuracy of the prediction
      accuracy = compute_accuracy(prediction, masked_data)
      # store the accuracy for later analysis
      results[ablation_setting][modality].append(accuracy)

# Analyze the results and compare the model performance across different settings and modalities
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import pandas as pd

# Define the pretrained V&L BERT models and the modality-specific tasks
models = {
  "vilbert": transformers.ViLBERTForPreTraining.from_pretrained("bert-base-uncased"),
  "lxmert": transformers.LxmertForPreTraining.from_pretrained("unc-nlp/lxmert-base-uncased"),
  "uniter": transformers.UniterForPreTraining.from_pretrained("chenrocks/uniter-base")
}
tasks = {
  "mlm": transformers.BertTokenizer.from_pretrained("bert-base-uncased"),
  "mrc": torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
}

# Define the dataset of multimodal inputs and cross-modal grounding alignments
dataset = pd.read_csv("flickr30k_entities.csv")
# The dataset has the following columns:
# image_id: the id of the image file
# sentence: the caption of the image
# phrase_ids: the ids of the phrases in the sentence that are aligned with regions in the image
# phrase_types: the types of the phrases (e.g. person, animal, clothing, etc.)
# x1, y1, x2, y2: the coordinates of the bounding boxes of the regions in the image

# Define the ablation settings and modalities
ablation_settings = ["none", "object", "all"]
modalities = ["text", "image"]

# Define a function to mask one or more tokens or regions randomly
def mask_input(input):
  # input is a tuple of (image, sentence)
  # return a tuple of (masked_input, masked_data)
  # masked_input is a tuple of (masked_image, masked_sentence)
  # masked_data is a list of tokens or regions that are masked
  # randomly choose a modality to mask
  modality = random.choice(modalities)
  if modality == "text":
    # tokenize the sentence using the MLM tokenizer
    tokens = tasks["mlm"].tokenize(input[1])
    # randomly choose one or more tokens to mask
    mask_indices = random.sample(range(len(tokens)), random.randint(1, len(tokens)))
    # replace the tokens with [MASK]
    masked_tokens = [tokens[i] if i not in mask_indices else "[MASK]" for i in range(len(tokens))]
    # join the masked tokens into a sentence
    masked_sentence = tasks["mlm"].convert_tokens_to_string(masked_tokens)
    # return the masked input and data
    return (input[0], masked_sentence), [tokens[i] for i in mask_indices]
  else:
    # load the image using torchvision
    image = torchvision.io.read_image(input[0])
    # randomly choose one or more regions to mask
    mask_indices = random.sample(range(len(input[1])), random.randint(1, len(input[1])))
    # replace the regions with [IMG]
    masked_regions = [input[1][i] if i not in mask_indices else "[IMG]" for i in range(len(input[1]))]
    # return the masked input and data
    return (image, masked_regions), [input[1][i] for i in mask_indices]

# Define a function to ablate inputs from one modality
def ablate_input(input, modality, ablation_setting):
  # input is a tuple of (image, sentence)
  # modality is either "text" or "image"
  # ablation_setting is either "none", "object", or "all"
  if ablation_setting == "none":
    return input # no ablation
  elif ablation_setting == "object":
    # ablate only the region aligned with a masked word
    # or the word aligned with a masked region
    alignment = get_alignment(input) # from Flickr30k Entities
    if modality == "text":
      # replace the word with [MASK]
      input[1][alignment[1]] = "[MASK]"
    else:
      # replace the region with [IMG]
      input[0][alignment[0]] = "[IMG]"
    return input
  else:
    # ablate the entire image or sentence
    if modality == "text":
      # replace the sentence with [MASK]
      input[1] = ["[MASK]"] * len(input[1])
    else:
      # replace the image with zero vector
      input[0] = torch.zeros_like(input[0])
    return input

# Define a function to compute the accuracy of predicting masked data
def compute_accuracy(prediction, ground_truth):
  # prediction and ground_truth are lists of tokens or regions
  # return the percentage of correct predictions
  correct = 0
  total = len(prediction)
  for i in range(total):
    if prediction[i] == ground_truth[i]:
      correct += 1
  return correct / total

# Define a dictionary to store the results
results = {}

# Loop over the models and the dataset
for model_name, model in models.items():
  # initialize the results for this model
  results[model_name] = {}
  for ablation_setting in ablation_settings:
    results[model_name][ablation_setting] = {}
    for modality in modalities:
      results[model_name][ablation_setting][modality] = []
  # loop over the dataset
  for input in dataset:
    # input is a tuple of (image_id, sentence, phrase_ids, phrase_types, x1, y1, x2, y2)
    # extract the image and sentence from the input
    image = input[0]
    sentence = input[1]
    # extract the regions and their types from the input
    regions = list(zip(input[4], input[5], input[6], input[7]))
    region_types = input[3]
    # mask one or more tokens or regions randomly
    masked_input, masked_data = mask_input((image, sentence))
    for ablation_setting in ablation_settings:
      for modality in modalities:
        # ablate inputs from one modality
        ablated_input = ablate_input(masked_input, modality, ablation_setting)
        # feed the ablated input to the model and get the prediction
        if modality == "text":
          # use the MLM task to predict the masked tokens
          prediction = model(ablated_input[0], ablated_input[1], tasks["mlm"])
        else:
          # use the MRC task to predict the masked regions
          prediction = model(ablated_input[0], ablated_input[1], tasks["mrc"])
        # compute the accuracy of the prediction
        accuracy = compute_accuracy(prediction, masked_data)
        # store the accuracy for later analysis
        results[model_name][ablation_setting][modality].append(accuracy)

# Analyze the results and compare the model performance across different settings and modalities
```