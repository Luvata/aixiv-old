---
title: 2208.08914v1 Prompt Vision Transformer for Domain Generalization
date: 2022-08-09
---

# [Prompt Vision Transformer for Domain Generalization](http://arxiv.org/abs/2208.08914v1)

authors: Zangwei Zheng, Xiangyu Yue, Kai Wang, Yang You


## What, Why and How

[1]: https://arxiv.org/abs/2208.08914v1 "Prompt Vision Transformer for Domain Generalization"
[2]: https://arxiv.org/pdf/2208.08914v1 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.08914v1 "[2112.08914v1] Characterizing and addressing the issue of oversmoothing ..."

Here is a summary of the paper at [^1^][1]:

- What: The paper proposes a novel approach called DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction using vision transformers (ViTs).
- Why: The paper aims to address the problem of domain generalization (DG), which is to learn a generalizable model from a set of source domains and perform well on the unseen target domain. The paper argues that previous DG methods cannot exploit the domain-specific knowledge efficiently or adapt to the target domain properly when using ViTs as backbones.
- How: The paper introduces domain prompts, which are prepended before ViT input tokens from the corresponding source domain, and a prompt adapter, which produces a suitable prompt for each input image based on the learned source domain prompts. The paper evaluates the proposed approach on four benchmark datasets and shows that it outperforms the state-of-the-art methods with a ViT backbone.

## Main Contributions

[1]: https://arxiv.org/abs/2208.08914v1 "Prompt Vision Transformer for Domain Generalization"
[2]: https://arxiv.org/pdf/2208.08914v1 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.08914v1 "[2112.08914v1] Characterizing and addressing the issue of oversmoothing ..."

According to the paper at [^1^][1], the main contributions are:

- The paper proposes a novel approach called DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction using vision transformers (ViTs).
- The paper introduces domain prompts, which are prepended before ViT input tokens from the corresponding source domain, and a prompt adapter, which produces a suitable prompt for each input image based on the learned source domain prompts.
- The paper shows that the proposed approach can exploit the similarity between the feature of the out-of-domain image and source domains to properly integrate the source domain knowledge.
- The paper evaluates the proposed approach on four benchmark datasets and shows that it outperforms the state-of-the-art methods with a ViT backbone.

## Method Summary

[1]: https://arxiv.org/abs/2208.08914v1 "Prompt Vision Transformer for Domain Generalization"
[2]: https://arxiv.org/pdf/2208.08914v1 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.08914v1 "[2112.08914v1] Characterizing and addressing the issue of oversmoothing ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first introduces the problem formulation of domain generalization and the architecture of vision transformers (ViTs).
- The paper then presents the proposed approach DoPrompt, which consists of two components: domain prompts and prompt adapter.
- Domain prompts are learned embeddings that are prepended before ViT input tokens from the corresponding source domain. They are optimized to capture domain-specific knowledge and interact with the whole network through self-attention mechanism.
- Prompt adapter is a module that produces a suitable prompt for each input image based on the learned source domain prompts. It consists of a feature extractor, a similarity predictor and a prompt generator. The feature extractor extracts a feature vector from the input image. The similarity predictor computes the similarity scores between the feature vector and each source domain prompt. The prompt generator generates an adapted prompt by taking a weighted sum of the source domain prompts according to the similarity scores.
- The paper describes the training and inference procedures of DoPrompt. During training, the model is optimized by minimizing a cross-entropy loss over all source domains. During inference, the model uses the adapted prompt generated by the prompt adapter to predict the class label for an out-of-domain image.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the ViT model
model = ViT()

# Initialize the domain prompts randomly
domain_prompts = [random_vector() for each source domain]

# Define the prompt adapter
prompt_adapter = PromptAdapter()

# Train the model and the domain prompts on source domains
for each epoch:
  for each batch of images and labels from source domains:
    # Prepend the corresponding domain prompt to each image
    input_tokens = [domain_prompt + image_to_tokens(image) for each image and domain_prompt in batch]
    # Forward pass through the model
    logits = model(input_tokens)
    # Compute the cross-entropy loss
    loss = cross_entropy(logits, labels)
    # Backward pass and update the model and the domain prompts
    loss.backward()
    optimizer.step()

# Test the model on target domain
for each batch of images from target domain:
  # Generate an adapted prompt for each image using the prompt adapter
  adapted_prompts = prompt_adapter(images, domain_prompts)
  # Prepend the adapted prompt to each image
  input_tokens = [adapted_prompt + image_to_tokens(image) for each image and adapted_prompt in batch]
  # Forward pass through the model
  logits = model(input_tokens)
  # Predict the class label with the highest probability
  predictions = argmax(logits, axis=1)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers

# Define the hyperparameters
num_source_domains = 4 # number of source domains
num_classes = 10 # number of classes
prompt_length = 16 # length of domain prompts and adapted prompts
hidden_size = 768 # hidden size of ViT
batch_size = 32 # batch size for training and testing
num_epochs = 100 # number of epochs for training
learning_rate = 1e-4 # learning rate for optimizer

# Load the source domain datasets and the target domain dataset
source_datasets = [load_dataset(domain) for domain in source_domains]
target_dataset = load_dataset(target_domain)

# Define the data loaders for source domains and target domain
source_loaders = [torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) for dataset in source_datasets]
target_loader = torch.utils.data.DataLoader(target_dataset, batch_size=batch_size, shuffle=False)

# Define the ViT model with a classification head
model = transformers.VisionTransformer(vit_type='vit_base_patch16_224', num_classes=num_classes)

# Initialize the domain prompts randomly from a normal distribution
domain_prompts = torch.randn(num_source_domains, prompt_length, hidden_size)

# Define the prompt adapter as a neural network with three layers: feature extractor, similarity predictor and prompt generator
prompt_adapter = torch.nn.Sequential(
  # Feature extractor: a convolutional layer followed by a global average pooling layer and a linear layer
  torch.nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),
  torch.nn.ReLU(),
  torch.nn.AdaptiveAvgPool2d((1, 1)),
  torch.nn.Flatten(),
  torch.nn.Linear(64, hidden_size),
  # Similarity predictor: a linear layer followed by a softmax layer
  torch.nn.Linear(hidden_size, num_source_domains),
  torch.nn.Softmax(dim=1),
  # Prompt generator: a linear layer that takes the weighted sum of domain prompts according to the similarity scores
  lambda x: x @ domain_prompts # matrix multiplication
)

# Define the optimizer for the model and the domain prompts
optimizer = torch.optim.Adam([model.parameters(), domain_prompts], lr=learning_rate)

# Define the cross-entropy loss function
criterion = torch.nn.CrossEntropyLoss()

# Train the model and the domain prompts on source domains
for epoch in range(num_epochs):
  # Set the model and the prompt adapter to training mode
  model.train()
  prompt_adapter.train()
  # Loop over each source domain
  for i in range(num_source_domains):
    # Loop over each batch of images and labels from the current source domain
    for images, labels in source_loaders[i]:
      # Prepend the corresponding domain prompt to each image and convert them to input tokens
      input_tokens = [domain_prompts[i] + image_to_tokens(image) for image in images]
      input_tokens = torch.stack(input_tokens) # stack them into a tensor of shape (batch_size, prompt_length + image_length, hidden_size)
      # Forward pass through the model
      logits = model(input_tokens) # output logits of shape (batch_size, num_classes)
      # Compute the cross-entropy loss
      loss = criterion(logits, labels)
      # Backward pass and update the model and the domain prompts
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

# Test the model on target domain
# Set the model and the prompt adapter to evaluation mode
model.eval()
prompt_adapter.eval()
# Initialize a list to store the predictions
predictions = []
# Loop over each batch of images from target domain
for images in target_loader:
  # Generate an adapted prompt for each image using the prompt adapter
  adapted_prompts = prompt_adapter(images) # output adapted prompts of shape (batch_size, prompt_length, hidden_size)
  # Prepend the adapted prompt to each image and convert them to input tokens
  input_tokens = [adapted_prompt + image_to_tokens(image) for image, adapted_prompt in zip(images, adapted_prompts)]
  input_tokens = torch.stack(input_tokens) # stack them into a tensor of shape (batch_size, prompt_length + image_length, hidden_size)
  # Forward pass through the model
  logits = model(input_tokens) # output logits of shape (batch_size, num_classes)
  # Predict the class label with the highest probability
  preds = torch.argmax(logits, dim=1) # output predictions of shape (batch_size,)
  # Append the predictions to the list
  predictions.extend(preds.tolist())

# Evaluate the accuracy of the predictions on target domain
accuracy = compute_accuracy(predictions, target_dataset.labels)
print(f'Accuracy on target domain: {accuracy}')
```