---
title: 2012.00926v2 pi-GAN  Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis
date: 2020-12-01
---

# [pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis](http://arxiv.org/abs/2012.00926v2)

authors: Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon Wetzstein


## What, Why and How

[1]: https://arxiv.org/abs/2012.00926 "[2012.00926] pi-GAN: Periodic Implicit Generative ... - arXiv.org"
[2]: https://arxiv.org/pdf/2012.00926v2.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2211.00926v2 "[2211.00926v2] On projective varieties of general type with many global ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a novel generative model, named **Periodic Implicit Generative Adversarial Networks (pi-GAN)**, for high-quality 3D-aware image synthesis.
- **Why**: The paper aims to address two limitations of existing approaches for 3D-aware image synthesis: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality.
- **How**: The paper leverages neural representations with **periodic activation functions** and **volumetric rendering** to represent scenes as view-consistent 3D representations with fine detail. The paper also introduces a new dataset of high-resolution natural images with corresponding depth maps and camera poses, named **DeepView**, to evaluate the proposed model. The paper shows that pi-GAN obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.

## Main Contributions

The paper claims the following contributions:

- A novel generative model, pi-GAN, that synthesizes high-quality 3D-aware images using periodic neural representations and volumetric rendering.
- A new dataset, DeepView, that provides high-resolution natural images with depth maps and camera poses for 3D-aware image synthesis evaluation.
- Extensive experiments and ablation studies that demonstrate the effectiveness and superiority of pi-GAN over existing methods on various datasets.

## Method Summary

[1]: https://arxiv.org/abs/2012.00926 "[2012.00926] pi-GAN: Periodic Implicit Generative ... - arXiv.org"
[2]: https://arxiv.org/pdf/2012.00926v2.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/1812.00926v2 "[1812.00926v2] Complex Structures for Klein-Gordon Theory on Globally ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces a novel generative model, pi-GAN, that consists of two main components: a **generator** and a **discriminator**.
- The generator takes as input a latent code z and a camera pose c, and outputs a 3D-aware image x. The generator consists of two sub-networks: a **representation network** and a **rendering network**.
- The representation network maps the latent code z to a periodic neural representation f, which is a function that maps 3D points to feature vectors. The representation network uses periodic activation functions such as sine and cosine to achieve high-frequency details and multi-view consistency.
- The rendering network takes the neural representation f and the camera pose c, and renders a 2D image x by performing volumetric rendering along the camera rays. The rendering network uses alpha compositing to blend the colors and opacities of the sampled points along each ray.
- The discriminator takes as input an image x and a camera pose c, and outputs a scalar value that indicates how realistic the image is. The discriminator consists of two sub-networks: a **patch-based network** and a **multi-view network**.
- The patch-based network operates on local image patches and uses convolutional layers to extract features. The patch-based network aims to capture the fine details and textures of the image.
- The multi-view network operates on global image features and uses fully connected layers to encode the camera pose. The multi-view network aims to capture the view-consistency and 3D structure of the scene.
- The paper trains pi-GAN using an adversarial loss that combines the outputs of the patch-based network and the multi-view network. The paper also uses an additional reconstruction loss that encourages pi-GAN to reproduce the input images when given their corresponding latent codes and camera poses.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator network
def generator(z, c):
  # Map the latent code z to a periodic neural representation f
  f = representation_network(z)
  # Render a 2D image x from the neural representation f and the camera pose c
  x = rendering_network(f, c)
  return x

# Define the discriminator network
def discriminator(x, c):
  # Extract local features from image patches
  p = patch_based_network(x)
  # Encode global features from image and camera pose
  v = multi_view_network(x, c)
  # Combine the local and global features to output a realism score
  s = p * v
  return s

# Define the adversarial loss function
def adversarial_loss(x_real, x_fake, c):
  # Compute the realism scores for real and fake images
  s_real = discriminator(x_real, c)
  s_fake = discriminator(x_fake, c)
  # Minimize the score for fake images and maximize the score for real images
  loss = -log(s_real) - log(1 - s_fake)
  return loss

# Define the reconstruction loss function
def reconstruction_loss(x_real, x_fake):
  # Compute the pixel-wise mean squared error between real and fake images
  loss = mean((x_real - x_fake) ** 2)
  return loss

# Define the training procedure
def train(data):
  # Loop over the data batches
  for x_real, c in data:
    # Sample a random latent code z
    z = sample_noise()
    # Generate a fake image x_fake from z and c
    x_fake = generator(z, c)
    # Compute the adversarial loss
    adv_loss = adversarial_loss(x_real, x_fake, c)
    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(x_real, generator(z_real, c))
    # Update the generator and discriminator parameters using gradient descent
    update_parameters(adv_loss + rec_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import numpy as np

# Define the hyperparameters
batch_size = 16 # The number of images per batch
latent_dim = 256 # The dimension of the latent code z
feature_dim = 64 # The dimension of the feature vector f(x)
hidden_dim = 128 # The dimension of the hidden layers
num_layers = 4 # The number of layers in the representation and rendering networks
num_samples = 64 # The number of samples per ray for volumetric rendering
learning_rate = 0.0002 # The learning rate for gradient descent
beta1 = 0.5 # The beta1 parameter for Adam optimizer
beta2 = 0.999 # The beta2 parameter for Adam optimizer
num_epochs = 100 # The number of epochs for training

# Define the periodic activation function
def periodic_activation(x):
  # Apply sine and cosine functions element-wise and concatenate them along the feature dimension
  return torch.cat((torch.sin(x), torch.cos(x)), dim=-1)

# Define the representation network
class RepresentationNetwork(nn.Module):
  def __init__(self):
    super(RepresentationNetwork, self).__init__()
    # Define a linear layer that maps the latent code z to a hidden vector h0
    self.fc0 = nn.Linear(latent_dim, hidden_dim)
    # Define a list of linear layers that map the hidden vector h_i to h_i+1 for i in [0, num_layers-1]
    self.fcs = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers)])
    # Define a linear layer that maps the hidden vector h_num_layers to a feature vector f(x)
    self.fc_out = nn.Linear(hidden_dim, feature_dim)

  def forward(self, z, x):
    # Reshape z to have shape (batch_size, latent_dim)
    z = z.view(-1, latent_dim)
    # Reshape x to have shape (batch_size * num_samples, 3)
    x = x.view(-1, 3)
    # Concatenate z and x along the feature dimension
    input = torch.cat((z, x), dim=-1)
    # Apply the first linear layer and the periodic activation function
    output = periodic_activation(self.fc0(input))
    # Apply the remaining linear layers and the periodic activation functions
    for fc in self.fcs:
      output = periodic_activation(fc(output))
    # Apply the final linear layer and return the feature vector f(x)
    output = self.fc_out(output)
    return output

# Define the rendering network
class RenderingNetwork(nn.Module):
  def __init__(self):
    super(RenderingNetwork, self).__init__()
    # Define a list of linear layers that map the feature vector f(x) to a color vector c(x) and an opacity scalar o(x)
    self.fcs = nn.ModuleList([nn.Linear(feature_dim, feature_dim) for _ in range(num_layers)])
    self.fc_color = nn.Linear(feature_dim, 3)
    self.fc_opacity = nn.Linear(feature_dim, 1)

  def forward(self, f):
    # Apply the linear layers and the periodic activation functions
    output = f
    for fc in self.fcs:
      output = periodic_activation(fc(output))
    # Apply the color linear layer and the sigmoid function to get c(x) in [0, 1]
    color = torch.sigmoid(self.fc_color(output))
    # Apply the opacity linear layer and the sigmoid function to get o(x) in [0, 1]
    opacity = torch.sigmoid(self.fc_opacity(output))
    return color, opacity

# Define the generator network
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the representation network
    self.representation_network = RepresentationNetwork()
    # Define the rendering network
    self.rendering_network = RenderingNetwork()

  def forward(self, z, c):
    # Get the camera parameters from c
    focal_length, camera_position, camera_direction, camera_up_vector = c
    # Compute the camera basis vectors from camera parameters
    camera_right_vector = torch.cross(camera_direction, camera_up_vector)
    camera_up_vector = torch.cross(camera_right_vector, camera_direction)
    camera_right_vector /= torch.norm(camera_right_vector)
    camera_up_vector /= torch.norm(camera_up_vector)