---
title: 2305.18107v2 Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off in Real-World Super-Resolution
date: 2023-05-19
---

# [Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off in Real-World Super-Resolution](http://arxiv.org/abs/2305.18107v2)

authors: Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Yulun Zhang, Wenming Yang


## What, Why and How

[1]: https://arxiv.org/abs/2305.18107 "[2305.18107] Crafting Training Degradation Distribution for the ..."
[2]: https://arxiv.org/pdf/2305.18107v2.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2305.18107v2 "[2305.18107v2] Crafting Training Degradation Distribution for the ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a novel approach to craft training degradation distributions for super-resolution (SR) techniques using a small set of reference images.
- **Why**: The paper aims to address the trade-off between generalization performance and restoration accuracy for SR techniques in real-world applications, where the degradation typically exhibits a limited variation range.
- **How**: The paper uses a binned representation of the degradation space and the Fréchet distance between degradation distributions to optimize the training degradation distribution for a given test set. The paper also proposes a method to select reference images that are representative of the test set. The paper evaluates the proposed approach on several real-world SR tasks and shows that it can significantly improve the performance of test images while preserving generalization capabilities.

## Main Contributions

According to the paper, the main contributions are:

- They propose a novel approach to craft training degradation distributions for SR techniques using a small set of reference images, which can balance the trade-off between generalization performance and restoration accuracy in real-world applications.
- They introduce a binned representation of the degradation space and the Fréchet distance between degradation distributions, which can effectively measure the similarity between training and test degradations and optimize the training degradation distribution accordingly.
- They develop a method to select reference images that are representative of the test set, which can reduce the dependence on human annotations and improve the efficiency of the proposed approach.
- They conduct extensive experiments on several real-world SR tasks, such as face SR, text SR, and video SR, and demonstrate that the proposed approach can significantly improve the performance of test images while preserving generalization capabilities.

## Method Summary

Here is a summary of the method section of the paper:

- The paper first defines the degradation space as a set of possible degradation parameters that can affect the quality of low-resolution (LR) images, such as blur kernel, noise level, and downsampling factor. The paper then proposes to use a binned representation of the degradation space, where each bin corresponds to a range of degradation parameters. The paper also defines the degradation distribution as the probability mass function over the bins, which reflects the frequency of occurrence of different degradations in a given image set.
- The paper then introduces the Fréchet distance as a metric to measure the similarity between two degradation distributions. The Fréchet distance is based on the Wasserstein distance, which is a distance between probability distributions that takes into account the underlying geometry of the degradation space. The paper shows that the Fréchet distance can capture the perceptual difference between two image sets with different degradations better than other metrics, such as KL divergence or JS divergence.
- The paper then proposes to optimize the training degradation distribution for a given test set by minimizing the Fréchet distance between them. The paper formulates this as a constrained optimization problem, where the objective function is the Fréchet distance and the constraints are derived from the binned representation of the degradation space. The paper solves this problem using an alternating direction method of multipliers (ADMM) algorithm, which iteratively updates the training degradation distribution and the dual variables until convergence.
- The paper then describes how to select reference images that are representative of the test set. The paper assumes that the test set has a known degradation distribution, which can be estimated from a small number of annotated images or inferred from metadata. The paper then proposes to select reference images that have similar degradations to the test set using a nearest neighbor search based on the Fréchet distance. The paper also proposes to use a diversity criterion to avoid selecting redundant reference images and ensure a good coverage of the test set.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a test set of LR images with a known degradation distribution P_test
# Output: a training degradation distribution P_train that minimizes the Fréchet distance to P_test

# Step 1: Define the degradation space and its binned representation
degradation_space = [blur_kernel, noise_level, downsampling_factor]
bins = create_bins(degradation_space) # divide the degradation space into bins

# Step 2: Define the Fréchet distance between two degradation distributions
def Fréchet_distance(P1, P2):
  # compute the Wasserstein distance between P1 and P2 using the Earth Mover's Distance algorithm
  W = Wasserstein_distance(P1, P2)
  # compute the mean and covariance of the degradation parameters in each bin
  mu1, sigma1 = compute_statistics(P1, bins)
  mu2, sigma2 = compute_statistics(P2, bins)
  # compute the Fréchet distance using the formula
  F = W + np.trace(sigma1 + sigma2 - 2 * np.sqrt(sigma1 @ sigma2)) + np.linalg.norm(mu1 - mu2)**2
  return F

# Step 3: Optimize the training degradation distribution using ADMM
# initialize P_train randomly
P_train = random_distribution(bins)
# initialize the dual variables
lambda = zeros(bins)
# set the ADMM parameters
rho = 0.01 # penalty parameter
epsilon = 0.001 # convergence criterion
# iterate until convergence
while True:
  # update P_train by solving a quadratic programming problem
  P_train = argmin_P_train(Fréchet_distance(P_train, P_test) + rho/2 * np.linalg.norm(P_train - lambda)**2)
  # update lambda by adding the scaled residual
  lambda = lambda + rho * (P_train - sum_to_one_constraint(P_train))
  # check the convergence condition
  if np.linalg.norm(P_train - sum_to_one_constraint(P_train)) < epsilon:
    break

# Step 4: Select reference images that are representative of the test set
# initialize an empty set of reference images
reference_images = []
# iterate over the bins in descending order of probability mass in P_test
for bin in sorted(bins, key=lambda x: P_test[x], reverse=True):
  # find the nearest neighbor of bin in P_train using the Fréchet distance
  nearest_bin = argmin_bin(Fréchet_distance(bin, bin'))
  # select a random image from nearest_bin that is not already in reference_images and satisfies the diversity criterion
  image = select_image(nearest_bin, reference_images, diversity_criterion)
  # add image to reference_images
  reference_images.append(image)

# return P_train and reference_images as the output
return P_train, reference_images
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import cv2
import scipy.optimize
import sklearn.metrics

# Define some constants
N = 100 # number of bins
M = 10 # number of reference images
K = 5 # number of nearest neighbors for diversity criterion

# Input: a test set of LR images with a known degradation distribution P_test
# Output: a training degradation distribution P_train that minimizes the Fréchet distance to P_test

# Step 1: Define the degradation space and its binned representation
# Define the ranges and steps of the degradation parameters
blur_kernel_range = [0.1, 5.0] # in pixels
blur_kernel_step = 0.1 # in pixels
noise_level_range = [0.0, 50.0] # in dB
noise_level_step = 1.0 # in dB
downsampling_factor_range = [2, 8] # in integers
downsampling_factor_step = 1 # in integers

# Create a list of all possible combinations of the degradation parameters
degradation_space = []
for blur_kernel in np.arange(blur_kernel_range[0], blur_kernel_range[1] + blur_kernel_step, blur_kernel_step):
  for noise_level in np.arange(noise_level_range[0], noise_level_range[1] + noise_level_step, noise_level_step):
    for downsampling_factor in np.arange(downsampling_factor_range[0], downsampling_factor_range[1] + downsampling_factor_step, downsampling_factor_step):
      degradation_space.append([blur_kernel, noise_level, downsampling_factor])

# Divide the degradation space into N bins using k-means clustering
bins = sklearn.cluster.KMeans(n_clusters=N).fit(degradation_space).cluster_centers_

# Step 2: Define the Fréchet distance between two degradation distributions
def Fréchet_distance(P1, P2):
  # Compute the Wasserstein distance between P1 and P2 using the Earth Mover's Distance algorithm
  W = scipy.stats.wasserstein_distance(P1, P2)
  # Compute the mean and covariance of the degradation parameters in each bin
  mu1 = np.average(bins, weights=P1, axis=0)
  sigma1 = np.cov(bins.T, aweights=P1)
  mu2 = np.average(bins, weights=P2, axis=0)
  sigma2 = np.cov(bins.T, aweights=P2)
  # Compute the Fréchet distance using the formula
  F = W + np.trace(sigma1 + sigma2 - 2 * np.sqrt(sigma1 @ sigma2)) + np.linalg.norm(mu1 - mu2)**2
  return F

# Step 3: Optimize the training degradation distribution using ADMM
# Initialize P_train randomly
P_train = np.random.dirichlet(np.ones(N))
# Initialize the dual variables
lambda = np.zeros(N)
# Set the ADMM parameters
rho = 0.01 # penalty parameter
epsilon = 0.001 # convergence criterion
# Define a function to enforce the sum-to-one constraint on P_train
def sum_to_one_constraint(P_train):
  return P_train / np.sum(P_train)
# Define a function to compute the objective function for P_train update
def objective_function(P_train):
  return Fréchet_distance(P_train, P_test) + rho/2 * np.linalg.norm(P_train - lambda)**2
# Iterate until convergence
while True:
  # Update P_train by solving a quadratic programming problem using scipy.optimize.minimize
  P_train = scipy.optimize.minimize(objective_function, P_train, bounds=[(0.0, None) for _ in range(N)]).x
  # Update lambda by adding the scaled residual
  lambda = lambda + rho * (P_train - sum_to_one_constraint(P_train))
  # Check the convergence condition
  if np.linalg.norm(P_train - sum_to_one_constraint(P_train)) < epsilon:
    break

# Step 4: Select reference images that are representative of the test set
# Initialize an empty set of reference images and their corresponding bins
reference_images = []
reference_bins = []
# Iterate over the bins in descending order of probability mass in P_test
for bin in sorted(bins, key=lambda x: P_test[x], reverse=True):
  # Find the nearest neighbor of bin in P_train using the Fréchet distance
  nearest_bin = bins[np.argmin([Fréchet_distance(bin, bin') for bin' in bins])]
  # Select a random image from nearest_bin that is not already in reference_images and satisfies the diversity criterion
  # The diversity criterion is to have a minimum distance to the K nearest neighbors in reference_bins
  candidates = [] # a list of candidate images and their distances to the K nearest neighbors
  for image in nearest_bin:
    if image not in reference_images:
      distances = [cv2.norm(image, image') for image' in reference_images] # compute the L2 norm between images
      if len(distances) < K: # if there are not enough neighbors, use a large distance as a placeholder
        distances += [np.inf] * (K - len(distances))
      candidates.append([image, np.min(distances)]) # add the image and its minimum distance to the candidates list
  # Sort the candidates by their minimum distance in descending order
  candidates = sorted(candidates, key=lambda x: x[1], reverse=True)
  # Select the first candidate as the reference image
  image = candidates[0][0]
  # Add image and nearest_bin to reference_images and reference_bins
  reference_images.append(image)
  reference_bins.append(nearest_bin)
  # Stop if M reference images have been selected
  if len(reference_images) == M:
    break

# Return P_train and reference_images as the output
return P_train, reference_images
```