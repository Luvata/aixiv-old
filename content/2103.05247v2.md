---
title: 2103.05247v2 Pretrained Transformers as Universal Computation Engines
date: 2021-03-06
---

# [Pretrained Transformers as Universal Computation Engines](http://arxiv.org/abs/2103.05247v2)

authors: Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch


## What, Why and How

[1]: https://arxiv.org/pdf/2103.05247v2 "Pretrained Transformers As Universal Computation Engines - arXiv.org"
[2]: https://arxiv.org/abs/2103.05247 "[2103.05247] Pretrained Transformers as Universal Computation Engines"
[3]: http://export.arxiv.org/abs/1802.05247v2 "[1802.05247v2] Fundamental Factorization of a GLSM, Part I: Construction"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper investigates the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning, such as numerical computation, vision, and protein fold prediction. It introduces a model called a Frozen Pretrained Transformer (FPT), which does not finetune the self-attention and feedforward layers of the residual blocks.
- **Why**: The paper aims to show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks, and that transformers are universal computation engines that can handle various types of sequences. It also compares the performance of a random initialized transformer to a random LSTM to analyze the architecture.
- **How**: The paper uses a variety of sequence classification tasks spanning different modalities as benchmarks to evaluate the FPT model. It also performs ablation studies and experiments with different input embeddings and output layers. It reports the test accuracy and compute efficiency of the FPT model and compares it with a full transformer and a full LSTM.

## Main Contributions

According to the paper, the main contributions are:

- **Introducing FPT**: A model that leverages a language-pretrained transformer without finetuning the self-attention and feedforward layers of the residual blocks. This reduces the number of trainable parameters and the compute cost while maintaining or improving performance on non-language tasks.
- **Demonstrating the universality of transformers**: Showing that transformers pretrained on natural language can generalize to other modalities with minimal finetuning, and that they outperform random LSTMs on various sequence classification tasks. This suggests that transformers are universal computation engines that can handle different types of sequences.
- **Analyzing the architecture**: Comparing the performance of a random initialized transformer to a random LSTM and finding that the former has an advantage in terms of test accuracy and compute efficiency. This indicates that the transformer architecture is more suitable for sequence modeling than the LSTM architecture.

## Method Summary

The method section of the paper describes the FPT model and the benchmarks used to evaluate it. The FPT model consists of a language-pretrained transformer with frozen self-attention and feedforward layers, and a task-specific input embedding and output layer. The input embedding maps the input sequence to a sequence of tokens that can be processed by the transformer, and the output layer maps the final hidden state of the transformer to a class label. The paper uses six benchmarks that span different modalities: Bit Memory, Bit XOR, ListOps, MNIST, CIFAR-10, and Homology. Each benchmark is formulated as a sequence classification task where the input is a sequence of bits, pixels, or amino acids, and the output is a binary or categorical label. The paper reports the test accuracy and compute efficiency of the FPT model on each benchmark and compares it with a full transformer and a full LSTM. The paper also performs ablation studies and experiments with different input embeddings and output layers to analyze the effect of these components on the performance of the FPT model.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Load a transformer pretrained on natural language
transformer = load_pretrained_transformer()

# Freeze the self-attention and feedforward layers of the transformer
for layer in transformer.layers:
  layer.self_attention.trainable = False
  layer.feedforward.trainable = False

# Define a task-specific input embedding and output layer
input_embedding = InputEmbedding()
output_layer = OutputLayer()

# Define the FPT model as a combination of the input embedding, transformer, and output layer
fpt_model = Sequential([input_embedding, transformer, output_layer])

# Train the FPT model on a sequence classification task
fpt_model.fit(train_data, train_labels)

# Evaluate the FPT model on the test data
fpt_model.evaluate(test_data, test_labels)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import transformers
import numpy as np

# Load a transformer pretrained on natural language (e.g. BERT)
transformer = transformers.BertModel.from_pretrained('bert-base-uncased')

# Freeze the self-attention and feedforward layers of the transformer
for layer in transformer.encoder.layer:
  for param in layer.attention.self.parameters():
    param.requires_grad = False
  for param in layer.intermediate.parameters():
    param.requires_grad = False
  for param in layer.output.parameters():
    param.requires_grad = False

# Define a task-specific input embedding and output layer
# For example, for the Bit Memory task, the input embedding is a linear layer that maps each bit to a 768-dimensional vector,
# and the output layer is a linear layer that maps the final hidden state of the transformer to a binary label
input_embedding = torch.nn.Linear(1, 768)
output_layer = torch.nn.Linear(768, 1)

# Define the FPT model as a combination of the input embedding, transformer, and output layer
fpt_model = torch.nn.Sequential(input_embedding, transformer, output_layer)

# Define the loss function and the optimizer
loss_function = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(fpt_model.parameters(), lr=0.001)

# Load the data for the sequence classification task
# For example, for the Bit Memory task, the data consists of sequences of bits with a target bit at the end
train_data, train_labels = load_bit_memory_data()
test_data, test_labels = load_bit_memory_data()

# Train the FPT model on the train data
for epoch in range(10):
  # Shuffle the train data
  permutation = np.random.permutation(len(train_data))
  train_data = train_data[permutation]
  train_labels = train_labels[permutation]

  # Loop over mini-batches of train data
  for i in range(0, len(train_data), 32):
    # Get the current mini-batch of data and labels
    batch_data = train_data[i:i+32]
    batch_labels = train_labels[i:i+32]

    # Forward pass: compute the logits of the FPT model on the batch data
    batch_logits = fpt_model(batch_data)

    # Compute the loss on the batch labels
    batch_loss = loss_function(batch_logits, batch_labels)

    # Backward pass: compute the gradients of the loss with respect to the model parameters
    batch_loss.backward()

    # Update the model parameters using the optimizer
    optimizer.step()

    # Zero out the gradients for the next iteration
    optimizer.zero_grad()

  # Print the epoch number and the average loss on the train data
  print(f'Epoch {epoch}, Loss: {batch_loss.item()}')

# Evaluate the FPT model on the test data
# Compute the logits of the FPT model on the test data
test_logits = fpt_model(test_data)

# Compute the binary predictions by applying a threshold of 0.5 to the logits
test_predictions = (test_logits > 0.5).float()

# Compute the accuracy of the predictions by comparing them with the test labels
test_accuracy = (test_predictions == test_labels).float().mean()

# Print the test accuracy
print(f'Test Accuracy: {test_accuracy.item()}')
```