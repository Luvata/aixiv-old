---
title: 2210.09304v1 Non-Contrastive Learning Meets Language-Image Pre-Training
date: 2022-10-10
---

# [Non-Contrastive Learning Meets Language-Image Pre-Training](http://arxiv.org/abs/2210.09304v1)

authors: Jinghao Zhou, Li Dong, Zhe Gan, Lijuan Wang, Furu Wei


## What, Why and How

[1]: https://arxiv.org/pdf/2210.09304v1 "Non-Contrastive Learning Meets Language-Image Pre-Training - arXiv.org"
[2]: https://arxiv.org/abs/2210.09304v1 "Non-Contrastive Learning Meets Language-Image Pre-Training"
[3]: http://export.arxiv.org/abs/2210.09304 "[2210.09304] Non-Contrastive Learning Meets Language-Image Pre-Training"

Here is a summary of the paper:

- **What**: The paper proposes a new framework for language-image pre-training called xCLIP, which combines contrastive and non-contrastive objectives to align images and texts.
- **Why**: The paper argues that contrastive learning alone is not optimal for learning from noisy web-crawled data, and that non-contrastive learning can complement it by enhancing feature semantics and representation learning.
- **How**: The paper introduces nCLIP, a non-contrastive variant of CLIP that uses probabilistic distributions as pseudo-labels to supervise the prediction from one modality to another. The paper then combines nCLIP and CLIP in a multi-tasking framework called xCLIP, and evaluates it on various downstream tasks such as zero-shot classification, retrieval, visual representation learning, and textual representation learning. The paper shows that xCLIP outperforms CLIP and other baselines on these tasks.

## Main Contributions

The paper claims to make the following contributions:

- It is the first work to explore the validity of non-contrastive language-image pre-training and to study its properties and limitations.
- It proposes a novel non-contrastive objective for language-image pre-training that uses probabilistic distributions as pseudo-labels to account for the loose correlation between images and texts.
- It introduces xCLIP, a multi-tasking framework that combines contrastive and non-contrastive objectives to leverage the best of both worlds.
- It conducts extensive experiments on various downstream tasks and demonstrates the effectiveness and superiority of xCLIP over CLIP and other baselines.

## Method Summary

Here is a summary of the method section of the paper:

- The paper first reviews the contrastive objective of CLIP, which maximizes the cosine similarity between the projected features of an image-text pair while minimizing it with other pairs in a batch.
- The paper then introduces the non-contrastive objective of nCLIP, which projects each modality into a high-dimensional probability distribution as the pseudo-label to supervise the prediction from the other modality. The paper uses a softmax function to obtain the distribution and a cross-entropy loss to measure the prediction error.
- The paper then describes the multi-tasking framework of xCLIP, which jointly optimizes the contrastive and non-contrastive objectives with a weighted sum of their losses. The paper also introduces a temperature parameter to control the sharpness of the distributions and a regularization term to encourage diversity among them.
- The paper finally presents the implementation details of xCLIP, such as the encoder architectures, the data sources, the optimization settings, and the hyperparameters. The paper also discusses some design choices and ablation studies.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the image encoder and the language encoder
image_encoder = ResNet50()
language_encoder = Transformer()

# Define the projection layers for each modality
image_projection = Linear(2048, 512)
language_projection = Linear(768, 512)

# Define the softmax layers for each modality
image_softmax = Linear(2048, 32768)
language_softmax = Linear(768, 32768)

# Define the loss functions for each objective
contrastive_loss = NTXentLoss()
non_contrastive_loss = CrossEntropyLoss()
regularization_loss = L2Norm()

# Define the hyperparameters
alpha = 0.5 # weight for contrastive loss
beta = 0.5 # weight for non-contrastive loss
gamma = 0.1 # weight for regularization loss
tau = 0.07 # temperature for softmax

# Loop over the batches of image-text pairs
for images, texts in data_loader:
  # Encode the images and texts
  image_features = image_encoder(images)
  text_features = language_encoder(texts)

  # Project the features to a lower dimension
  image_embeddings = image_projection(image_features)
  text_embeddings = language_projection(text_features)

  # Normalize the embeddings
  image_embeddings = normalize(image_embeddings)
  text_embeddings = normalize(text_embeddings)

  # Compute the contrastive loss
  clip_loss = contrastive_loss(image_embeddings, text_embeddings)

  # Project the features to a higher dimension and apply softmax
  image_distributions = softmax(image_softmax(image_features) / tau)
  text_distributions = softmax(language_softmax(text_features) / tau)

  # Compute the non-contrastive loss
  nclip_loss = non_contrastive_loss(image_distributions, text_distributions) + non_contrastive_loss(text_distributions, image_distributions)

  # Compute the regularization loss
  reg_loss = regularization_loss(image_distributions) + regularization_loss(text_distributions)

  # Compute the total loss
  total_loss = alpha * clip_loss + beta * nclip_loss + gamma * reg_loss

  # Update the parameters
  optimizer.zero_grad()
  total_loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision.models as models
import transformers
import numpy as np

# Define the image encoder and the language encoder
image_encoder = models.resnet50(pretrained=True)
image_encoder.fc = nn.Identity() # remove the last layer
language_encoder = transformers.AutoModel.from_pretrained("bert-base-uncased")

# Define the projection layers for each modality
image_projection = nn.Linear(2048, 512)
language_projection = nn.Linear(768, 512)

# Define the softmax layers for each modality
image_softmax = nn.Linear(2048, 32768)
language_softmax = nn.Linear(768, 32768)

# Define the loss functions for each objective
contrastive_loss = nn.CrossEntropyLoss() # use cross-entropy loss with logits as inputs
non_contrastive_loss = nn.KLDivLoss(reduction="batchmean") # use KL divergence loss with log-probabilities and probabilities as inputs
regularization_loss = nn.MSELoss() # use mean squared error loss

# Define the hyperparameters
alpha = 0.5 # weight for contrastive loss
beta = 0.5 # weight for non-contrastive loss
gamma = 0.1 # weight for regularization loss
tau = 0.07 # temperature for softmax
batch_size = 256 # batch size for training
num_epochs = 100 # number of epochs for training
learning_rate = 1e-4 # learning rate for optimizer

# Define the optimizer and the scheduler
optimizer = torch.optim.AdamW([image_encoder.parameters(), language_encoder.parameters(), image_projection.parameters(), language_projection.parameters(), image_softmax.parameters(), language_softmax.parameters()], lr=learning_rate)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Define the device to run on (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move the models and the loss functions to the device
image_encoder.to(device)
language_encoder.to(device)
image_projection.to(device)
language_projection.to(device)
image_softmax.to(device)
language_softmax.to(device)
contrastive_loss.to(device)
non_contrastive_loss.to(device)
regularization_loss.to(device)

# Define a function to normalize a tensor along a dimension
def normalize(x, dim=-1):
  return x / torch.norm(x, dim=dim, keepdim=True)

# Define a function to compute the logits for contrastive learning
def compute_logits(x, y):
  return torch.matmul(x, y.t()) # use matrix multiplication

# Define a function to compute the targets for contrastive learning
def compute_targets(batch_size):
  return torch.arange(batch_size).to(device) # use consecutive integers

# Define a function to compute the probabilities for non-contrastive learning
def compute_probabilities(x):
  return torch.softmax(x / tau, dim=-1) # use softmax with temperature

# Define a function to compute the log-probabilities for non-contrastive learning
def compute_log_probabilities(x):
  return torch.log_softmax(x / tau, dim=-1) # use log-softmax with temperature

# Define a function to compute the regularization term for non-contrastive learning
def compute_regularization(x):
  return torch.ones(x.size(-1)).to(device) / x.size(-1) # use uniform distribution

# Loop over the epochs
for epoch in range(num_epochs):
  # Loop over the batches of image-text pairs
  for images, texts in data_loader:
    # Move the images and texts to the device
    images = images.to(device)
    texts = texts.to(device)

    # Encode the images and texts
    image_features = image_encoder(images)
    text_features = language_encoder(texts).pooler_output

    # Project the features to a lower dimension
    image_embeddings = image_projection(image_features)
    text_embeddings = language_projection(text_features)

    # Normalize the embeddings
    image_embeddings = normalize(image_embeddings)
    text_embeddings = normalize(text_embeddings)

    # Compute the logits and targets for contrastive learning
    image_text_logits = compute_logits(image_embeddings, text_embeddings)
    text_image_logits = compute_logits(text_embeddings, image_embeddings)
    targets = compute_targets(batch_size)

    # Compute the contrastive loss
    clip_loss = contrastive_loss(image_text_logits, targets) + contrastive_loss(text_image_logits, targets)

    # Project the features to a higher dimension and apply softmax or log-softmax
    image_probabilities = compute_probabilities(image_softmax(image_features))
    text_probabilities = compute_probabilities(language_softmax(text_features))
    image_log_probabilities = compute_log_probabilities(image_softmax(image_features))
    text_log_probabilities = compute_log_probabilities(language_softmax(text_features))

    # Compute the non-contrastive loss
    nclip_loss = non_contrastive_loss(image_log_probabilities, text_probabilities) + non_contrastive_loss(text_log_probabilities, image_probabilities)

    # Compute the regularization loss
    reg_loss = regularization_loss(image_probabilities, compute_regularization(image_probabilities)) + regularization_loss(text_probabilities, compute_regularization(text_probabilities))

    # Compute the total loss
    total_loss = alpha * clip_loss + beta * nclip_loss + gamma * reg_loss

    # Update the parameters
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

  # Update the scheduler
  scheduler.step()

  # Print the epoch and the loss
  print(f"Epoch {epoch+1}, Loss: {total_loss.item():.4f}")
```