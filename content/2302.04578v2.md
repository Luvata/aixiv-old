---
title: 2302.04578v2 Adversarial Example Does Good  Preventing Painting Imitation from Diffusion Models via Adversarial Examples
date: 2023-02-05
---

# [Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples](http://arxiv.org/abs/2302.04578v2)

authors: Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, Haibing Guan


## What, Why and How

[1]: https://arxiv.org/pdf/2302.04578v2.pdf "Adversarial Example Does Good: Preventing Painting Imitation from ..."
[2]: https://arxiv.org/abs/2302.04578 "[2302.04578] Adversarial Example Does Good: Preventing Painting ..."
[3]: http://export.arxiv.org/abs/2103.04578v2 "[2103.04578v2] Testing Autonomous Systems with Believed Equivalence ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method to generate adversarial examples for diffusion models (DMs) to prevent them from imitating human-created paintings without authorization.
- **Why**: The paper addresses the problem of copyright violations by infringers who use unauthorized paintings to train DMs to generate novel paintings in a similar style, which harms the original artists' rights and interests.
- **How**: The paper first defines and evaluates the adversarial examples for DMs based on a theoretical framework. Then, it designs an algorithm called AdvDM, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. The paper shows that the generated adversarial examples can effectively hinder DMs from extracting the features of the original paintings, thus protecting them from imitation.


## Main Contributions

[1]: https://arxiv.org/pdf/2302.04578v2.pdf "Adversarial Example Does Good: Preventing Painting Imitation from ..."
[2]: https://arxiv.org/abs/2302.04578 "[2302.04578] Adversarial Example Does Good: Preventing Painting ..."
[3]: http://export.arxiv.org/abs/2103.04578v2 "[2103.04578v2] Testing Autonomous Systems with Believed Equivalence ..."

According to the paper at [^1^][1], the main contributions are:

- The paper is the **first** to explore and propose to utilize adversarial examples for DMs to protect human-created artworks from imitation.
- The paper builds a **theoretical framework** to define and evaluate the adversarial examples for DMs based on their conditional inference process and feature extraction mechanism.
- The paper designs a novel algorithm, named **AdvDM**, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs.
- The paper conducts **extensive experiments** on various DM-based AI-for-Art applications, such as textual inversion, style transfer, and image synthesis, and shows that the generated adversarial examples can effectively hinder DMs from extracting the features of the original paintings.


## Method Summary

[1]: https://arxiv.org/pdf/2302.04578v2.pdf "Adversarial Example Does Good: Preventing Painting Imitation from ..."
[2]: https://arxiv.org/abs/2302.04578 "[2302.04578] Adversarial Example Does Good: Preventing Painting ..."
[3]: http://export.arxiv.org/abs/2103.04578v2 "[2103.04578v2] Testing Autonomous Systems with Believed Equivalence ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first introduces some preliminaries on diffusion models (DMs) and adversarial examples, and defines the problem of generating adversarial examples for DMs based on their conditional inference process and feature extraction mechanism.
- The paper then proposes a theoretical framework to evaluate the adversarial examples for DMs based on two metrics: **adversarial strength** and **adversarial quality**. The former measures how much the adversarial examples can degrade the performance of DMs, while the latter measures how much the adversarial examples can preserve the appearance of the original images.
- The paper then presents a novel algorithm, named **AdvDM**, which exploits a Monte-Carlo estimation of adversarial examples for DMs by optimizing upon different latent variables sampled from the reverse process of DMs. The algorithm consists of three steps: **sampling**, **optimization**, and **reconstruction**. The sampling step draws multiple latent variables from different stages of the reverse process of DMs. The optimization step updates the latent variables to maximize the adversarial strength while maintaining the adversarial quality. The reconstruction step generates the adversarial examples by applying the forward process of DMs on the optimized latent variables.
- The paper then provides some theoretical analysis and empirical results to show the effectiveness and efficiency of AdvDM compared with existing methods. The paper also discusses some extensions and applications of AdvDM, such as generating universal adversarial examples, performing style transfer, and synthesizing images.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a clean image x, a diffusion model p(x|z), a feature extractor f(x), a target condition c
# Output: an adversarial example x' that satisfies c and degrades the performance of p(x|z)

# Step 1: Sampling
# Sample multiple latent variables z_0, z_1, ..., z_T from different stages of the reverse process of p(x|z)
# z_0 ~ N(0, I)
# z_t ~ q(z_t|x) for t = 1, ..., T

# Step 2: Optimization
# Define a loss function L that measures the adversarial strength and quality
# L = lambda * D(f(x), f(p(x|z_T))) - (1 - lambda) * ||x - p(x|z_0)||^2
# where D is a distance metric, lambda is a trade-off parameter
# Update z_t to maximize L using gradient ascent for t = 0, ..., T
# z_t = z_t + alpha * grad_z_t(L)

# Step 3: Reconstruction
# Generate the adversarial example x' by applying the forward process of p(x|z) on z_0
# x' = p(x|z_0)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt

# Define some hyperparameters
T = 1000 # number of stages in the reverse process of DMs
N = 10 # number of latent variables to sample per stage
lambda = 0.5 # trade-off parameter for adversarial strength and quality
alpha = 0.01 # learning rate for gradient ascent
num_iter = 100 # number of iterations for optimization

# Load a pre-trained diffusion model p(x|z) and a feature extractor f(x)
p = torch.hub.load('openai/guided-diffusion', 'imagenet_1024')
f = torchvision.models.resnet18(pretrained=True)
f.eval()

# Load a clean image x and resize it to 256x256
x = plt.imread('cat.jpg')
x = torchvision.transforms.Resize((256, 256))(x)

# Define a target condition c (e.g., a textual description)
c = "A photo of a dog"

# Step 1: Sampling
# Initialize an empty list to store the latent variables
z_list = []

# Sample z_0 from a standard normal distribution and append it to z_list
z_0 = torch.randn(1, 3, 256, 256)
z_list.append(z_0)

# Sample z_t from q(z_t|x) for t = 1, ..., T and append them to z_list
for t in range(1, T+1):
    # Compute the noise level beta_t
    beta_t = p.betas[t-1]

    # Compute the mean and variance of q(z_t|x)
    mean = (np.sqrt(1 - beta_t) * x + np.sqrt(beta_t) * torch.randn_like(x)) / np.sqrt(1 - p.betas[0])
    var = beta_t * (1 - beta_t) / (1 - p.betas[0])

    # Sample N latent variables from q(z_t|x) and stack them along the batch dimension
    z_t = mean + torch.sqrt(var) * torch.randn(N, 3, 256, 256)
    z_list.append(z_t)

# Step 2: Optimization
# Define a loss function L that measures the adversarial strength and quality
def L(z_list):
    # Reconstruct x' from z_0 using the forward process of p(x|z)
    x_prime = p.p_mean(z_list[0])

    # Compute the distance between f(x) and f(p(x|z_T))
    distance = torch.nn.MSELoss()(f(x), f(p.p_mean(z_list[-1])))

    # Compute the reconstruction error between x and x'
    error = torch.nn.MSELoss()(x, x_prime)

    # Return the weighted combination of distance and error
    return lambda * distance - (1 - lambda) * error

# Update z_t to maximize L using gradient ascent for t = 0, ..., T
for i in range(num_iter):
    # Compute the gradient of L with respect to z_t for t = 0, ..., T
    grad_list = torch.autograd.grad(L(z_list), z_list)

    # Update z_t using gradient ascent for t = 0, ..., T
    for t in range(T+1):
        z_list[t] = z_list[t] + alpha * grad_list[t]

# Step 3: Reconstruction
# Generate the adversarial example x' by applying the forward process of p(x|z) on z_0
x_prime = p.p_mean(z_list[0])

# Display x' and save it to a file
plt.imshow(x_prime)
plt.savefig('dog.jpg')
```