---
title: 2306.08276v1 TryOnDiffusion  A Tale of Two UNets
date: 2023-06-09
---

# [TryOnDiffusion: A Tale of Two UNets](http://arxiv.org/abs/2306.08276v1)

authors: Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, Ira Kemelmacher-Shlizerman


## What, Why and How

[1]: https://arxiv.org/abs/2306.08276 "[2306.08276] TryOnDiffusion: A Tale of Two UNets - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.08276 "[2306.08276] TryOnDiffusion: A Tale of Two UNets"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a diffusion-based architecture called **TryOnDiffusion** that can generate a realistic visualization of how a garment worn by one person might look on another person with a different pose and shape.
- **Why**: The paper aims to address the challenge of preserving the garment details and warping the garment to fit the target person in a single network, which previous methods either fail to do or do in a sequential manner.
- **How**: The paper introduces a novel architecture called **Parallel-UNet**, which unifies two UNets that operate on different scales of the input images. The key ideas behind Parallel-UNet are: 1) garment is warped implicitly via a cross attention mechanism, 2) garment warp and person blend happen as part of a unified process as opposed to a sequence of two separate tasks. The paper also uses diffusion models to train the Parallel-UNet in a self-supervised way.



## Main Contributions

[1]: https://arxiv.org/abs/2306.08276 "[2306.08276] TryOnDiffusion: A Tale of Two UNets - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.08276 "[2306.08276] TryOnDiffusion: A Tale of Two UNets"

The paper[^1^][1] claims the following contributions:

- **A novel diffusion-based architecture** that unifies two UNets (referred to as Parallel-UNet), which allows us to preserve garment details and warp the garment for significant pose and body change in a single network.
- **A cross attention mechanism** that implicitly warps the garment according to the target person's pose and shape, without requiring explicit warping or alignment modules.
- **A unified process of garment warp and person blend** that avoids artifacts and inconsistencies caused by sequential methods that first warp the garment and then blend it with the person.
- **A self-supervised training scheme** that leverages diffusion models to train the Parallel-UNet without requiring paired data or ground truth images.
- **State-of-the-art performance** on several benchmarks for virtual try-on, both qualitatively and quantitatively, compared to previous methods.


## Method Summary

[1]: https://arxiv.org/abs/2306.08276 "[2306.08276] TryOnDiffusion: A Tale of Two UNets - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.08276 "[2306.08276] TryOnDiffusion: A Tale of Two UNets"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a novel architecture called **Parallel-UNet**, which consists of two UNets that operate on different scales of the input images. The first UNet (referred to as **Coarse-UNet**) takes the low-resolution images of the person and the garment as inputs and outputs a low-resolution image of the person wearing the garment. The second UNet (referred to as **Fine-UNet**) takes the high-resolution images of the person and the garment as inputs and outputs a high-resolution image of the person wearing the garment.
- The paper uses a **cross attention mechanism** to implicitly warp the garment according to the target person's pose and shape. The cross attention mechanism computes the attention weights between the feature maps of the person and the garment at each scale and uses them to modulate the garment features before feeding them to the UNets. This way, the garment features are aligned with the person features without requiring explicit warping or alignment modules.
- The paper adopts a **unified process of garment warp and person blend** that avoids artifacts and inconsistencies caused by sequential methods that first warp the garment and then blend it with the person. The paper achieves this by using skip connections between the Coarse-UNet and Fine-UNet, which allow the Fine-UNet to refine the output of the Coarse-UNet with high-frequency details. The paper also uses residual blocks in both UNets, which enable them to learn additive corrections to the input images rather than generating them from scratch.
- The paper leverages **diffusion models** to train the Parallel-UNet in a self-supervised way. Diffusion models are generative models that learn to reverse a diffusion process that gradually corrupts an image with Gaussian noise. The paper uses diffusion models to generate corrupted versions of the input images and then trains the Parallel-UNet to reconstruct the original images from them. This way, the Parallel-UNet learns to generate realistic images without requiring paired data or ground truth images.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: person image P, garment image G
# Output: try-on image T

# Downsample P and G to low-resolution images P_low and G_low
P_low = downsample(P)
G_low = downsample(G)

# Compute cross attention weights between P_low and G_low
W_low = cross_attention(P_low, G_low)

# Modulate G_low with W_low
G_low_mod = modulate(G_low, W_low)

# Feed P_low and G_low_mod to Coarse-UNet and get low-resolution try-on image T_low
T_low = Coarse_UNet(P_low, G_low_mod)

# Upsample T_low to high-resolution image T_up
T_up = upsample(T_low)

# Compute cross attention weights between P and G
W_high = cross_attention(P, G)

# Modulate G with W_high
G_high_mod = modulate(G, W_high)

# Feed P, G_high_mod and T_up to Fine-UNet and get high-resolution try-on image T
T = Fine_UNet(P, G_high_mod, T_up)

# Return T as the output
return T
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: person image P, garment image G
# Output: try-on image T

# Define the number of scales S and the scale factor F
S = 4
F = 0.5

# Define the Coarse-UNet and Fine-UNet architectures
Coarse_UNet = UNet(input_channels=6, output_channels=3, num_blocks=5)
Fine_UNet = UNet(input_channels=9, output_channels=3, num_blocks=5)

# Define the cross attention function
def cross_attention(X, Y):
  # X and Y are feature maps of shape (B, C, H, W)
  # B is the batch size, C is the number of channels, H and W are the height and width
  # Compute the query, key and value matrices from X and Y
  Q = linear(X.reshape(B, C, -1)) # Q is of shape (B, C', HW)
  K = linear(Y.reshape(B, C, -1)) # K is of shape (B, C', HW)
  V = linear(Y.reshape(B, C, -1)) # V is of shape (B, C', HW)
  # Compute the attention weights by matrix multiplication and softmax
  W = softmax(Q.T @ K / sqrt(C')) # W is of shape (B, HW, HW)
  # Compute the output feature map by matrix multiplication and reshape
  Z = linear(W @ V).reshape(B, C', H, W) # Z is of shape (B, C', H, W)
  # Return Z as the output
  return Z

# Define the modulate function
def modulate(X, W):
  # X and W are feature maps of shape (B, C, H, W)
  # B is the batch size, C is the number of channels, H and W are the height and width
  # Modulate X with W by element-wise multiplication and normalization
  Z = X * W # Z is of shape (B, C, H, W)
  Z = normalize(Z) # Z is of shape (B, C, H, W)
  # Return Z as the output
  return Z

# Define the diffusion model function
def diffusion_model(X):
  # X is an image of shape (B, C, H, W)
  # B is the batch size, C is the number of channels, H and W are the height and width
  # Define the number of diffusion steps T and the noise level sigma
  T = 1000
  sigma = 0.01
  # Initialize a list to store the corrupted images at each step
  X_list = []
  # Loop over the diffusion steps from T to 1
  for t in range(T, 0, -1):
    # Compute the noise term epsilon from a Gaussian distribution
    epsilon = normal(0, sigma**2) # epsilon is of shape (B, C, H ,W)
    # Corrupt X with epsilon by element-wise addition
    X_t = X + epsilon # X_t is of shape (B ,C ,H ,W)
    # Append X_t to X_list
    X_list.append(X_t)
    # Update X with a reverse diffusion formula
    X = (X_t - sqrt(1 - sigma**2) * epsilon) / sigma # X is of shape (B ,C ,H ,W)
  # Return X_list as the output
  return X_list

# Downsample P and G to low-resolution images P_low and G_low at S scales
P_low_list = [P]
G_low_list = [G]
for s in range(S):
  P_low_list.append(downsample(P_low_list[-1], F))
  G_low_list.append(downsample(G_low_list[-1], F))
P_low = P_low_list[-1] # P_low is of shape (B ,3 ,H/S ,W/S)
G_low = G_low_list[-1] # G_low is of shape (B ,3 ,H/S ,W/S)

# Compute cross attention weights between P_low and G_low at each scale
W_low_list = []
for s in range(S):
  W_low_list.append(cross_attention(P_low_list[s], G_low_list[s])) 
W_low = W_low_list[-1] # W_low is of shape (B ,3 ,H/S ,W/S)

# Modulate G_low with W_low at each scale
G_low_mod_list = []
for s in range(S):
  G_low_mod_list.append(modulate(G_low_list[s], W_low_list[s]))
G_low_mod = G_low_mod_list[-1] # G_low_mod is of shape (B ,3 ,H/S ,W/S)

# Feed P_low and G_low_mod to Coarse-UNet and get low-resolution try-on image T_low
T_low = Coarse_UNet(concat(P_low, G_low_mod)) # T_low is of shape (B ,3 ,H/S ,W/S)

# Upsample T_low to high-resolution image T_up at S scales
T_up_list = [T_low]
for s in range(S):
  T_up_list.append(upsample(T_up_list[-1], F))
T_up = T_up_list[-1] # T_up is of shape (B ,3 ,H ,W)

# Compute cross attention weights between P and G at each scale
W_high_list = []
for s in range(S):
  W_high_list.append(cross_attention(P, G))
W_high = W_high_list[-1] # W_high is of shape (B ,3 ,H ,W)

# Modulate G with W_high at each scale
G_high_mod_list = []
for s in range(S):
  G_high_mod_list.append(modulate(G, W_high_list[s]))
G_high_mod = G_high_mod_list[-1] # G_high_mod is of shape (B ,3 ,H ,W)

# Feed P, G_high_mod and T_up to Fine-UNet and get high-resolution try-on image T
T = Fine_UNet(concat(P, G_high_mod, T_up)) # T is of shape (B ,3 ,H ,W)

# Return T as the output
return T

# Train the Coarse-UNet and Fine-UNet with diffusion models
# Define the loss function as the mean squared error
loss = MSE

# Loop over the training data
for P, G in data:
  # Generate corrupted versions of P, G and T using diffusion models
  P_list = diffusion_model(P)
  G_list = diffusion_model(G)
  T_list = diffusion_model(T)
  # Initialize the loss value to zero
  L = 0
  # Loop over the diffusion steps from 1 to T
  for t in range(1, T+1):
    # Feed P_t, G_t and T_t to the model and get the reconstructed images P_hat, G_hat and T_hat
    P_hat, G_hat, T_hat = model(P_list[t], G_list[t], T_list[t])
    # Compute the loss value by comparing P_hat, G_hat and T_hat with P_t-1, G_t-1 and T_t-1
    L += loss(P_hat, P_list[t-1]) + loss(G_hat, G_list[t-1]) + loss(T_hat, T_list[t-1])
  # Update the model parameters by minimizing the loss value
  update(model.parameters(), L)
```