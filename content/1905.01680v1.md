---
title: 1905.01680v1 Learning Character-Agnostic Motion for Motion Retargeting in 2D
date: 2019-05-02
---

# [Learning Character-Agnostic Motion for Motion Retargeting in 2D](http://arxiv.org/abs/1905.01680v1)

authors: Kfir Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1905.01680v1 "[1905.01680v1] Learning Character-Agnostic Motion for Motion ..."
[2]: https://arxiv.org/pdf/1905.01680v1.pdf "Learning Character-Agnostic Motion for Motion Retargeting in 2D - arXiv.org"
[3]: http://export.arxiv.org/abs/2105.01680v1 "[2105.01680v1] On Moment Matching for Stochastic Systems"

Here is a summary of the paper:

- **What**: The paper presents a new method for **retargeting video-captured motion between different human performers**, without the need to explicitly reconstruct 3D poses and/or camera parameters[^1^][1].
- **Why**: The paper aims to address the challenges of analyzing human motion in 2D videos, which has a wide variety of applications in computer vision and graphics, such as performance cloning, video-driven cartoons, and motion retrieval[^1^][1].
- **How**: The paper proposes to learn a high-level latent motion representation, which is invariant to the skeleton geometry and the camera view, by training a deep neural network to decompose temporal sequences of 2D poses into three components: motion, skeleton, and camera view-angle. The paper then shows how to re-combine motion with novel skeletons and camera views, and decode a retargeted temporal sequence[^1^][1]. The paper evaluates the method on a synthetic dataset and on videos in-the-wild, and demonstrates its advantages over existing retargeting methods[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- A novel method for learning a character-agnostic motion representation directly from 2D videos, without the need for 3D reconstruction or camera calibration.
- A framework for motion retargeting in 2D that can handle different skeleton geometries and camera views, and can generate realistic and smooth motion sequences.
- A synthetic dataset of 2D human poses with ground truth motion, skeleton, and camera view-angle labels, which can be used for training and evaluation of motion analysis methods.
- A comprehensive evaluation of the proposed method on synthetic and real-world videos, showing its effectiveness and robustness compared to existing methods.

## Method Summary

[1]: https://arxiv.org/pdf/1905.01680v1.pdf "Learning Character-Agnostic Motion for Motion Retargeting in 2D - arXiv.org"
[2]: https://arxiv.org/abs/1905.01680v1 "[1905.01680v1] Learning Character-Agnostic Motion for Motion ..."
[3]: http://export.arxiv.org/pdf/2003.01680v1 "arXiv:2003.01680v1 [cs.CL] 3 Mar 2020"

Here is a summary of the method section of the paper:

- The paper proposes a deep neural network that consists of three modules: an **encoder**, a **decoder**, and a **discriminator**[^1^][1].
- The encoder takes as input a temporal sequence of 2D poses, represented as heatmaps, and encodes it into a latent vector that captures the motion, skeleton, and camera view-angle information[^1^][1].
- The decoder takes as input a latent vector and a target skeleton and camera view-angle, and decodes it into a retargeted temporal sequence of 2D poses[^1^][1].
- The discriminator takes as input a temporal sequence of 2D poses and tries to classify it as real or fake, while also predicting the skeleton and camera view-angle labels[^1^][1].
- The network is trained using a synthetic dataset of 2D human poses with ground truth labels for motion, skeleton, and camera view-angle[^1^][1]. The paper uses two loss functions: a reconstruction loss that measures the similarity between the input and output poses, and an adversarial loss that encourages the network to generate realistic and diverse poses[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder, decoder and discriminator networks
encoder = Encoder()
decoder = Decoder()
discriminator = Discriminator()

# Define the reconstruction and adversarial loss functions
reconstruction_loss = L1Loss()
adversarial_loss = BCELoss()

# Load the synthetic dataset of 2D human poses
dataset = load_dataset()

# Train the network for a fixed number of epochs
for epoch in range(num_epochs):
  # Shuffle the dataset
  dataset.shuffle()
  # Loop over the batches of data
  for batch in dataset:
    # Get the input and target poses, skeletons and camera view-angles
    input_poses = batch["input_poses"]
    target_poses = batch["target_poses"]
    input_skeletons = batch["input_skeletons"]
    target_skeletons = batch["target_skeletons"]
    input_view_angles = batch["input_view_angles"]
    target_view_angles = batch["target_view_angles"]

    # Encode the input poses into latent vectors
    latent_vectors = encoder(input_poses)

    # Decode the latent vectors into retargeted poses
    output_poses = decoder(latent_vectors, target_skeletons, target_view_angles)

    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(output_poses, target_poses)

    # Discriminate the real and fake poses
    real_scores, real_skeletons, real_view_angles = discriminator(target_poses)
    fake_scores, fake_skeletons, fake_view_angles = discriminator(output_poses)

    # Compute the adversarial loss
    adv_loss = adversarial_loss(fake_scores, 1) + adversarial_loss(real_scores, 0)

    # Compute the total loss
    total_loss = rec_loss + adv_loss

    # Update the network parameters using backpropagation and gradient descent
    total_loss.backward()
    optimizer.step()

  # Print the epoch and loss information
  print(f"Epoch: {epoch}, Loss: {total_loss}")
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import numpy as np
import cv2

# Define the constants and hyperparameters
num_epochs = 100 # Number of training epochs
batch_size = 32 # Batch size for training
latent_dim = 128 # Dimension of the latent vector
pose_dim = 18 # Number of joints in the pose
heatmap_size = 64 # Size of the heatmap for each joint
num_channels = pose_dim # Number of channels in the input and output poses
num_filters = 64 # Number of filters in the convolutional layers
kernel_size = 3 # Kernel size for the convolutional layers
stride = 2 # Stride for the convolutional layers
padding = 1 # Padding for the convolutional layers
skeleton_dim = 2 # Dimension of the skeleton vector
view_angle_dim = 1 # Dimension of the view-angle vector

# Define the encoder network
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Define the convolutional layers
    self.conv1 = nn.Conv2d(num_channels, num_filters, kernel_size, stride, padding)
    self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size, stride, padding)
    self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size, stride, padding)
    self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size, stride, padding)
    self.conv5 = nn.Conv2d(num_filters * 8, num_filters * 16, kernel_size, stride, padding)
    # Define the activation function
    self.relu = nn.ReLU()
    # Define the fully connected layer
    self.fc = nn.Linear(num_filters * 16 * 2 * 2, latent_dim)

  def forward(self, x):
    # Apply the convolutional layers and activation function
    x = self.relu(self.conv1(x))
    x = self.relu(self.conv2(x))
    x = self.relu(self.conv3(x))
    x = self.relu(self.conv4(x))
    x = self.relu(self.conv5(x))
    # Flatten the output
    x = x.view(-1, num_filters * 16 * 2 * 2)
    # Apply the fully connected layer
    x = self.fc(x)
    return x

# Define the decoder network
class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    # Define the fully connected layer
    self.fc = nn.Linear(latent_dim + skeleton_dim + view_angle_dim, num_filters * 16 * 2 * 2)
    # Define the deconvolutional layers
    self.deconv1 = nn.ConvTranspose2d(num_filters * 16, num_filters * 8, kernel_size, stride, padding)
    self.deconv2 = nn.ConvTranspose2d(num_filters * 8, num_filters * 4, kernel_size, stride, padding)
    self.deconv3 = nn.ConvTranspose2d(num_filters * 4, num_filters * 2, kernel_size, stride, padding)
    self.deconv4 = nn.ConvTranspose2d(num_filters * 2, num_filters, kernel_size, stride, padding)
    self.deconv5 = nn.ConvTranspose2d(num_filters, num_channels, kernel_size - 1 , stride -1 , padding -1 )
    # Define the activation function
    self.relu = nn.ReLU()
  
  def forward(self, z):
    # Apply the fully connected layer and reshape the output
    z = self.fc(z)
    z = z.view(-1, num_filters * 16 , 2 , 2 )
    # Apply the deconvolutional layers and activation function
    z = self.relu(self.deconv1(z))
    z = self.relu(self.deconv2(z))
    z = self.relu(self.deconv3(z))
    z = self.relu(self.deconv4(z))
    z = self.deconv5(z)
    return z

# Define the discriminator network
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator ,self).__init__()