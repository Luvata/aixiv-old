---
title: 2112.03857v2 Grounded Language-Image Pre-training
date: 2021-12-04
---

# [Grounded Language-Image Pre-training](http://arxiv.org/abs/2112.03857v2)

authors: Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao


## What, Why and How

[1]: https://arxiv.org/pdf/2112.03857v2.pdf "Grounded Language-Image Pre-training - arXiv.org"
[2]: https://arxiv.org/abs/2112.03857 "[2112.03857] Grounded Language-Image Pre-training - arXiv.org"
[3]: http://export.arxiv.org/abs/1908.03857v2 "[1908.03857v2] Homotopy Invariance of the string topology coproduct"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase grounding for pre-training.
- **Why**: The paper aims to overcome the limitations of existing visual recognition models that are trained to predict a fixed set of pre-determined object categories, which limits their usability in real-world applications. The paper also aims to leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich.
- **How**: The paper pre-trains GLIP on 27M grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The paper evaluates GLIP on various object-level recognition tasks, such as COCO and LVIS object detection, and 13 downstream object detection tasks. The paper shows that GLIP achieves state-of-the-art results on COCO and LVIS, and strong zero-shot and few-shot transferability to other tasks.

## Main Contributions

According to the paper, the main contributions are:

- The paper proposes a novel grounded language-image pre-training (GLIP) model that unifies object detection and phrase grounding for pre-training.
- The paper introduces a self-training method to generate grounding boxes from image-text pairs, which enables GLIP to learn from large-scale web data.
- The paper demonstrates that GLIP learns an object-level, language-aware, and semantic-rich visual representation that can be easily transferred to various downstream tasks in zero-shot and few-shot settings.

## Method Summary

[1]: https://arxiv.org/abs/2112.03857 "[2112.03857] Grounded Language-Image Pre-training - arXiv.org"
[2]: https://arxiv.org/pdf/2112.03857v2.pdf "Grounded Language-Image Pre-training - arXiv.org"
[3]: http://export.arxiv.org/abs/1908.03857v2 "[1908.03857v2] Homotopy Invariance of the string topology coproduct"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces the GLIP model, which consists of a **visual backbone**, a **language encoder**, and a **detection head**. The visual backbone extracts features from images, the language encoder encodes text inputs into embeddings, and the detection head predicts bounding boxes and labels for objects in images.
- The paper defines two pre-training tasks for GLIP: **object detection** and **phrase grounding**. Object detection is a task of predicting bounding boxes and labels for objects in images, while phrase grounding is a task of identifying the fine-grained correspondence between phrases in a sentence and objects (or regions) in an image.
- The paper proposes a **unified loss function** that combines the losses of both tasks. The loss function consists of four terms: a **classification loss**, a **regression loss**, a **grounding loss**, and a **contrastive loss**. The classification loss and the regression loss are standard losses for object detection, while the grounding loss and the contrastive loss are novel losses for phrase grounding.
- The paper describes how to generate **grounding data** from image-text pairs, which are abundant on the web. The paper uses a **self-training** method that leverages an existing object detector to generate bounding boxes for objects mentioned in texts, and then filters out noisy boxes using a confidence threshold and a language model. The paper also uses human-annotated grounding data from existing datasets, such as Flickr30K Entities [46] and ReferItGame [29].
- The paper details the **pre-training procedure** of GLIP, which involves two stages: 1) pre-training on human-annotated grounding data only, and 2) pre-training on both human-annotated and web-crawled grounding data. The paper also describes the **data augmentation** techniques used for pre-training, such as random cropping, flipping, color jittering, etc.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the GLIP model
glip = GLIP(visual_backbone, language_encoder, detection_head)

# Define the unified loss function
loss = classification_loss + regression_loss + grounding_loss + contrastive_loss

# Generate grounding data from image-text pairs
grounding_data = []
for image, text in image_text_pairs:
  # Use an object detector to generate bounding boxes and labels
  boxes, labels = object_detector(image)
  # Filter out noisy boxes using a confidence threshold and a language model
  boxes, labels = filter(boxes, labels, text, confidence_threshold, language_model)
  # Add the image, text, boxes, and labels to the grounding data
  grounding_data.append((image, text, boxes, labels))

# Pre-train GLIP on human-annotated grounding data only
for epoch in range(num_epochs_stage_1):
  for batch in human_annotated_grounding_data:
    # Get the image, text, boxes, and labels from the batch
    image, text, boxes, labels = batch
    # Apply data augmentation to the image
    image = augment(image)
    # Forward pass GLIP on the image and text
    pred_boxes, pred_labels = glip(image, text)
    # Compute the loss
    loss = loss(pred_boxes, pred_labels, boxes, labels)
    # Backward pass and update parameters
    loss.backward()
    optimizer.step()

# Pre-train GLIP on both human-annotated and web-crawled grounding data
for epoch in range(num_epochs_stage_2):
  for batch in human_annotated_grounding_data + web_crawled_grounding_data:
    # Get the image, text, boxes, and labels from the batch
    image, text, boxes, labels = batch
    # Apply data augmentation to the image
    image = augment(image)
    # Forward pass GLIP on the image and text
    pred_boxes, pred_labels = glip(image, text)
    # Compute the loss
    loss = loss(pred_boxes, pred_labels, boxes, labels)
    # Backward pass and update parameters
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
num_epochs_stage_1 = 10 # Number of epochs for pre-training on human-annotated data only
num_epochs_stage_2 = 20 # Number of epochs for pre-training on both human-annotated and web-crawled data
batch_size = 32 # Batch size for pre-training
learning_rate = 1e-4 # Learning rate for pre-training
confidence_threshold = 0.7 # Confidence threshold for filtering noisy boxes
max_length = 64 # Maximum length of text inputs
num_classes = 1000 # Number of object classes for detection
num_queries = 100 # Number of queries for detection head
temperature = 0.07 # Temperature for contrastive loss

# Define the visual backbone
# Use a ResNet-50 with a Feature Pyramid Network (FPN) as in [36]
visual_backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet50', pretrained=True)

# Define the language encoder
# Use a BERT-base model as in [14]
language_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')

# Define the detection head
# Use a Transformer-based detection head as in [3]
detection_head = torchvision.models.detection.transformer.Transformer(num_classes=num_classes, num_queries=num_queries)

# Define the GLIP model
class GLIP(torch.nn.Module):
  def __init__(self, visual_backbone, language_encoder, detection_head):
    super().__init__()
    self.visual_backbone = visual_backbone # The visual backbone to extract features from images
    self.language_encoder = language_encoder # The language encoder to encode text inputs into embeddings
    self.detection_head = detection_head # The detection head to predict bounding boxes and labels

  def forward(self, image, text):
    # Extract features from the image using the visual backbone
    features = self.visual_backbone(image)
    # Encode the text input into embeddings using the language encoder
    embeddings = self.language_encoder(text).last_hidden_state[:,0,:] # Use the [CLS] token embedding as the text representation
    # Predict bounding boxes and labels using the detection head
    pred_boxes, pred_labels = self.detection_head(features, embeddings)
    return pred_boxes, pred_labels

# Instantiate the GLIP model
glip = GLIP(visual_backbone, language_encoder, detection_head)

# Define the classification loss
# Use a focal loss as in [36]
def classification_loss(pred_labels, labels):
  # Compute the focal loss between the predicted labels and the ground truth labels
  loss = -torch.sum(labels * (1 - pred_labels) ** 2 * torch.log(pred_labels + 1e-8), dim=-1)
  return loss.mean()

# Define the regression loss
# Use a generalized IoU loss as in [36]
def regression_loss(pred_boxes, boxes):
  # Compute the generalized IoU loss between the predicted boxes and the ground truth boxes
  loss = 1 - torchvision.ops.generalized_box_iou(pred_boxes, boxes)
  return loss.mean()

# Define the grounding loss
def grounding_loss(pred_boxes, pred_labels, boxes, labels):
  # Compute the grounding loss between the predicted boxes and labels and the ground truth boxes and labels
  # The grounding loss is defined as the sum of classification loss and regression loss weighted by a matching matrix M
  M = torchvision.ops.box_iou(boxes, pred_boxes) * labels @ pred_labels.T # M[i,j] is non-zero if box i matches with query j
  cls_loss = classification_loss(pred_labels, labels) # Classification loss for all queries
  reg_loss = regression_loss(pred_boxes, boxes) # Regression loss for all queries
  loss = torch.sum(M * (cls_loss + reg_loss)) / torch.sum(M + 1e-8) # Weighted sum of losses divided by number of matches
  return loss

# Define the contrastive loss
def contrastive_loss(pred_boxes, pred_labels, boxes, labels):
  # Compute the contrastive loss between the predicted boxes and labels and the ground truth boxes and labels
  # The contrastive loss is defined as a cross entropy loss with a similarity matrix S as targets and logits L as inputs 
  S = torchvision.ops.box_iou(boxes, pred_boxes) * labels @ pred_labels.T # S[i,j] is non-zero if box i is similar to query j 
  S = S / temperature # Scale down the similarity matrix by a temperature
  L = pred_labels @ labels.T # L[i,j] is the dot product between query i and box j
  L = L / temperature # Scale down the logits by a temperature
  loss = torch.nn.functional.cross_entropy(L, S) # Cross entropy loss with S as targets and L as inputs
  return loss

# Define the unified loss function
def loss(pred_boxes, pred_labels, boxes, labels):
  # Compute the unified loss function as the sum of classification loss, regression loss, grounding loss, and contrastive loss
  loss = classification_loss(pred_boxes, pred_labels, boxes, labels) + \
         regression_loss(pred_boxes, pred_labels, boxes, labels) + \
         grounding_loss(pred_boxes, pred_labels, boxes, labels) + \
         contrastive_loss(pred_boxes, pred_labels, boxes, labels)
  return loss

# Define the object detector
# Use a Faster R-CNN model with a ResNet-50 backbone as in [36]
object_detector = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# Define the language model
# Use a BERT-base model as in [14]
language_model = transformers.BertModel.from_pretrained('bert-base-uncased')

# Define the tokenizer
# Use a BERT tokenizer as in [14]
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

# Define the filter function
def filter(boxes, labels, text, confidence_threshold, language_model):
  # Filter out noisy boxes using a confidence threshold and a language model
  # A box is considered noisy if its confidence score is lower than the threshold or if its label is not mentioned in the text
  filtered_boxes = []
  filtered_labels = []
  for box, label in zip(boxes, labels):
    # Get the confidence score of the box
    confidence = box[-1]
    # Get the label of the box
    label = label[0]
    # Check if the confidence score is higher than the threshold
    if confidence > confidence_threshold:
      # Check if the label is mentioned in the text
      tokens = tokenizer.tokenize(label)
      ids = tokenizer.convert_tokens_to_ids(tokens)
      embeddings = language_model(torch.tensor([ids])).last_hidden_state[:,0,:] # Use the [CLS] token embedding as the label representation
      similarity = torch.cosine_similarity(embeddings, embeddings[0], dim=-1) # Compute the cosine similarity between the label and itself
      if similarity > 0.9: # A high similarity indicates that the label is mentioned in the text
        # Keep the box and the label
        filtered_boxes.append(box[:-1]) # Remove the confidence score from the box
        filtered_labels.append(label)
  return filtered_boxes, filtered_labels

# Load the human-annotated grounding data
# Use Flickr30K Entities [46] and ReferItGame [29] as examples
human_annotated_grounding_data = []
for dataset in ['Flickr30K', 'ReferItGame']:
  for image, text, boxes, labels in dataset:
    # Convert the image to a tensor
    image = torchvision.transforms.ToTensor()(image)
    # Convert the text to a tensor of token ids
    text = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)['input_ids']
    # Convert the boxes to a tensor of normalized coordinates [x1,y1,x2,y2]
    boxes = torch.tensor(boxes) / torch.tensor([image.width, image.height] * 2)
    # Convert the labels to a tensor of one-hot vectors
    labels = torch.nn.functional.one_hot(torch.tensor(labels), num_classes=num_classes)
    # Add the image, text, boxes, and labels to the human-annotated grounding data
    human_annotated_grounding_data.append((image, text, boxes, labels))

# Load the web-crawled image-text pairs
# Use Conceptual Captions [56] as an example
image_text_pairs = []
for image_url, text in conceptual_captions:
  # Download the image from the url
  image = torchvision.io.read_image(image_url)
  # Convert the text to a tensor of token ids
  text = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_length)['input_ids']
  # Add the image and text to the image-text pairs
  image_text_pairs.append((image, text))

# Generate grounding data from image-text pairs
grounding_data = []
for image, text in image_text_pairs:
  # Use an object detector to generate bounding boxes and labels for objects in images
  boxes, labels = object_detector(image)
  # Filter out noisy boxes using a confidence threshold and a language model
  boxes, labels = filter(boxes, labels, text, confidence_threshold, language_model)
  # Convert the boxes