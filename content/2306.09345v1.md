---
title: 2306.09345v1 Evaluating Data Attribution for Text-to-Image Models
date: 2023-06-10
---

# [Evaluating Data Attribution for Text-to-Image Models](http://arxiv.org/abs/2306.09345v1)

authors: Sheng-Yu Wang, Alexei A. Efros, Jun-Yan Zhu, Richard Zhang


## What, Why and How

[1]: https://arxiv.org/abs/2306.09345 "[2306.09345] Evaluating Data Attribution for Text-to-Image Models"
[2]: https://arxiv.org/pdf/2303.09345v1.pdf "arXiv:2303.09345v1 [math.RA] 16 Mar 2023"
[3]: http://export.arxiv.org/abs/2306.09345 "[2306.09345] Evaluating Data Attribution for Text-to-Image Models"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new method to evaluate data attribution for text-to-image models, which are models that can generate images from natural language descriptions.
- **Why**: Data attribution is the problem of identifying which images in the training set are most responsible for the appearance of a given generated image. This is important for understanding how text-to-image models work, how they reflect the training data, and how they can be improved or customized.
- **How**: The paper evaluates data attribution through "customization" methods, which tune an existing large-scale model toward a given exemplar object or style. The paper creates a new dataset of exemplar-influenced images and uses it to compare various data attribution algorithms and feature spaces. The paper also trains standard models, such as DINO, CLIP, and ViT, on the new dataset to adapt them to the data attribution problem. The paper shows that the proposed method can assign soft attribution scores over a set of training images and handle uncertainty in the problem.



## Main Contributions

[1]: https://arxiv.org/abs/2306.09345 "[2306.09345] Evaluating Data Attribution for Text-to-Image Models"
[2]: https://arxiv.org/pdf/2303.09345v1.pdf "arXiv:2303.09345v1 [math.RA] 16 Mar 2023"
[3]: http://export.arxiv.org/abs/2306.09345 "[2306.09345] Evaluating Data Attribution for Text-to-Image Models"

The paper at [^1^][1] claims the following contributions:

- A new method to evaluate data attribution for text-to-image models through customization methods, which can create synthetic images that are influenced by a given exemplar image.
- A new dataset of exemplar-influenced images, which can be used to benchmark different data attribution algorithms and feature spaces.
- A new way to train standard models, such as DINO, CLIP, and ViT, on the new dataset to adapt them to the data attribution problem and improve their performance.
- A new way to assign soft attribution scores over a set of training images and handle uncertainty in the data attribution problem.


## Method Summary

[1]: https://arxiv.org/abs/2306.09345 "[2306.09345] Evaluating Data Attribution for Text-to-Image Models"
[2]: https://arxiv.org/abs/2206.09345 "[2206.09345] Finding Diverse and Predictable Subgraphs for ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.09345 "[2306.09345] Evaluating Data Attribution for Text-to-Image Models"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces a new method to evaluate data attribution for text-to-image models through customization methods, which can create synthetic images that are influenced by a given exemplar image.
- The paper defines customization as a process of tuning an existing text-to-image model toward a given exemplar image, such that the generated images are more similar to the exemplar in terms of object or style.
- The paper proposes two types of customization methods: object customization and style customization. Object customization aims to generate images that contain the same object as the exemplar, while style customization aims to generate images that have the same style as the exemplar.
- The paper creates a new dataset of exemplar-influenced images by applying the customization methods to a large-scale text-to-image model (DALL-E) and a set of exemplar images from ImageNet. The dataset contains 10,000 images for each type of customization and each exemplar image.
- The paper uses the new dataset to evaluate different data attribution algorithms and feature spaces. The paper compares four data attribution algorithms: nearest neighbor, gradient-based, activation maximization, and contrastive learning. The paper also compares four feature spaces: pixel space, VGG space, CLIP space, and DINO space.
- The paper trains standard models, such as DINO, CLIP, and ViT, on the new dataset to adapt them to the data attribution problem and improve their performance. The paper shows that these models can learn to assign soft attribution scores over a set of training images and handle uncertainty in the problem.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# Input: a text-to-image model M, a set of exemplar images E, a text description T
# Output: a generated image I, a set of attribution scores S

# Define customization methods
def object_customization(M, e, T):
  # Modify the text description to include the object label of e
  T' = T + " that looks like " + object_label(e)
  # Generate an image using M and T'
  I = M(T')
  # Return the generated image
  return I

def style_customization(M, e, T):
  # Modify the text description to include the style label of e
  T' = T + " in the style of " + style_label(e)
  # Generate an image using M and T'
  I = M(T')
  # Return the generated image
  return I

# Create a new dataset of exemplar-influenced images
D = []
for e in E:
  for T in random_text_descriptions():
    # Apply object customization and style customization to e and T
    I_obj = object_customization(M, e, T)
    I_sty = style_customization(M, e, T)
    # Add the generated images and the exemplar image to the dataset
    D.append((I_obj, e))
    D.append((I_sty, e))

# Evaluate data attribution algorithms and feature spaces
for A in [nearest_neighbor, gradient_based, activation_maximization, contrastive_learning]:
  for F in [pixel_space, VGG_space, CLIP_space, DINO_space]:
    # Compute the attribution scores for each image-exemplar pair in D using A and F
    S = A(F(D))
    # Evaluate the performance of A and F using some metrics
    evaluate(A, F, S)

# Train standard models on the new dataset to adapt them to the data attribution problem
for M in [DINO, CLIP, ViT]:
  # Train M on D using some loss function
  train(M, D)
  # Compute the attribution scores for each image-exemplar pair in D using M
  S = M(D)
  # Evaluate the performance of M using some metrics
  evaluate(M, S)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing
import transformers # for text processing
import dall_e # for text-to-image model
import clip # for CLIP model
import vgg # for VGG model
import dino # for DINO model
import numpy as np # for numerical operations
import sklearn # for evaluation metrics

# Define some constants
NUM_EXEMPLARS = 1000 # the number of exemplar images to use
NUM_TEXTS = 10 # the number of text descriptions to use per exemplar image
IMAGE_SIZE = 256 # the size of the generated images
TEXT_LENGTH = 32 # the maximum length of the text descriptions
BATCH_SIZE = 64 # the batch size for training and evaluation
EPOCHS = 10 # the number of epochs for training

# Load the text-to-image model M
M = dall_e.load_model()

# Load the exemplar images E from ImageNet
E = torchvision.datasets.ImageNet(root='data', split='train')
E = E[:NUM_EXEMPLARS] # select a subset of exemplar images

# Load a tokenizer for text processing
tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')

# Define a function to get the object label and style label of an image
def get_labels(image):
  # Use a pre-trained classifier to get the object label of the image
  classifier = torchvision.models.resnet18(pretrained=True)
  classifier.eval()
  with torch.no_grad():
    output = classifier(image)
    object_label = output.argmax(dim=1)
  
  # Use a pre-trained style transfer model to get the style label of the image
  style_model = torchvision.models.vgg19(pretrained=True)
  style_model.eval()
  with torch.no_grad():
    output = style_model(image)
    style_label = output.argmax(dim=1)
  
  # Return the labels as strings
  return object_label.item(), style_label.item()

# Define customization methods
def object_customization(M, e, T):
  # Modify the text description to include the object label of e
  object_label, _ = get_labels(e)
  T' = T + " that looks like " + object_label
  # Tokenize and encode the text description
  tokens = tokenizer(T', return_tensors='pt', padding=True, truncation=True, max_length=TEXT_LENGTH)
  # Generate an image using M and T'
  I = M.generate(tokens, image_size=IMAGE_SIZE)
  # Return the generated image
  return I

def style_customization(M, e, T):
  # Modify the text description to include the style label of e
  _, style_label = get_labels(e)
  T' = T + " in the style of " + style_label
  # Tokenize and encode the text description
  tokens = tokenizer(T', return_tensors='pt', padding=True, truncation=True, max_length=TEXT_LENGTH)
  # Generate an image using M and T'
  I = M.generate(tokens, image_size=IMAGE_SIZE)
  # Return the generated image
  return I

# Create a new dataset of exemplar-influenced images
D = []
for e in E:
  for T in random_text_descriptions():
    # Apply object customization and style customization to e and T
    I_obj = object_customization(M, e, T)
    I_sty = style_customization(M, e, T)
    # Add the generated images and the exemplar image to the dataset as tensors
    D.append((torch.tensor(I_obj), torch.tensor(e)))
    D.append((torch.tensor(I_sty), torch.tensor(e)))

# Define a data loader for the dataset D
loader = torch.utils.data.DataLoader(D, batch_size=BATCH_SIZE, shuffle=True)

# Define a function to compute the similarity between two images using cosine similarity
def similarity(image1, image2):
  return torch.nn.functional.cosine_similarity(image1.flatten(), image2.flatten(), dim=0)

# Define data attribution algorithms
def nearest_neighbor(feature_space, D):
  # Initialize an empty list of attribution scores S
  S = []
  # For each image-exemplar pair in D:
  for I, e in D:
    # Compute the feature vector of I and e using feature_space (a function that takes an image and returns a feature vector)
    f_I = feature_space(I)
    f_e = feature_space(e)
    # Find the nearest neighbor of f_e in the training set of feature_space using similarity as a distance metric (a function that takes two feature vectors and returns a scalar)
    nn = similarity(f_e, feature_space.train_set).argmax()
    # Assign the index of the nearest neighbor as the attribution score for I and e
    S.append(nn)
  # Return the list of attribution scores S
  return S

def gradient_based(feature_space, D):
  # Initialize an empty list of attribution scores S
  S = []
  # For each image-exemplar pair in D:
  for I, e in D:
    # Compute the feature vector of I and e using feature_space (a function that takes an image and returns a feature vector)
    f_I = feature_space(I)
    f_e = feature_space(e)
    # Compute the gradient of f_I with respect to the training set of feature_space
    grad = torch.autograd.grad(f_I, feature_space.train_set)[0]
    # Find the index of the training image that has the largest gradient magnitude
    idx = grad.norm(dim=1).argmax()
    # Assign the index as the attribution score for I and e
    S.append(idx)
  # Return the list of attribution scores S
  return S

def activation_maximization(feature_space, D):
  # Initialize an empty list of attribution scores S
  S = []
  # For each image-exemplar pair in D:
  for I, e in D:
    # Compute the feature vector of I and e using feature_space (a function that takes an image and returns a feature vector)
    f_I = feature_space(I)
    f_e = feature_space(e)
    # Find the index of the training image that has the largest dot product with f_e
    idx = torch.matmul(f_e, feature_space.train_set.T).argmax()
    # Assign the index as the attribution score for I and e
    S.append(idx)
  # Return the list of attribution scores S
  return S

def contrastive_learning(feature_space, D):
  # Initialize an empty list of attribution scores S
  S = []
  # For each image-exemplar pair in D:
  for I, e in D:
    # Compute the feature vector of I and e using feature_space (a function that takes an image and returns a feature vector)
    f_I = feature_space(I)
    f_e = feature_space(e)
    # Compute the contrastive loss between f_I and f_e with respect to the training set of feature_space using a temperature parameter tau
    loss = -torch.log(torch.exp(similarity(f_I, f_e) / tau) / torch.sum(torch.exp(similarity(f_I, feature_space.train_set) / tau)))
    # Compute the gradient of the loss with respect to the training set of feature_space
    grad = torch.autograd.grad(loss, feature_space.train_set)[0]
    # Find the index of the training image that has the largest gradient magnitude
    idx = grad.norm(dim=1).argmax()
    # Assign the index as the attribution score for I and e
    S.append(idx)
  # Return the list of attribution scores S
  return S

# Define feature spaces
def pixel_space(image):
  # Return the image as a pixel vector
  return image.flatten()

def VGG_space(image):
  # Load a pre-trained VGG model
  model = vgg.vgg19(pretrained=True)
  model.eval()
  # Extract the features from the last convolutional layer of the model
  features = model.features(image)
  # Return the features as a vector
  return features.flatten()

def CLIP_space(image):
  # Load a pre-trained CLIP model
  model = clip.load('ViT-B/32', jit=False)[0]
  model.eval()
  # Extract the features from the last layer of the model
  features = model.encode_image(image)
  # Return the features as a vector
  return features.flatten()

def DINO_space(image):
  # Load a pre-trained DINO model
  model = dino.vit_small(pretrained=True)
  model.eval()
  # Extract the features from the last layer of the model
  features = model.forward_selfattention(image)[-1]
  # Return the features as a vector
  return features.flatten()

# Evaluate data attribution algorithms and feature spaces
for A in [nearest_neighbor, gradient_based, activation_maximization, contrastive_learning]:
  for F in [pixel_space, VGG_space, CLIP_space, DINO_space]:
    # Compute the attribution scores for each image-exemplar pair in D using A and F
    S = A(F, D)
    # Evaluate the performance of A and F using some metrics, such as accuracy, precision, recall, etc.
    metrics = sklearn.metrics.classification_report(D.labels, S)
    print(A.__name__, F.__name__, metrics)

# Train standard models on the new dataset to adapt them to the data attribution problem
for M in [DINO, CL