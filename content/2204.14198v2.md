---
title: 2204.14198v2 Flamingo  a Visual Language Model for Few-Shot Learning
date: 2022-04-15
---

# [Flamingo: a Visual Language Model for Few-Shot Learning](http://arxiv.org/abs/2204.14198v2)

authors: Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan


## What, Why and How

[1]: https://arxiv.org/abs/2204.14198 "Flamingo: a Visual Language Model for Few-Shot Learning"
[2]: https://arxiv.org/pdf/2204.14198v2.pdf "arXiv.org"
[3]: https://arxiv-export3.library.cornell.edu/abs/2204.14198?context=cs.CV "[2204.14198] Flamingo: a Visual Language Model for Few-Shot Learning"

Here is a summary of the paper:

- **What**: The paper introduces Flamingo, a family of Visual Language Models (VLM) that can perform few-shot learning on various image and video tasks by using natural language prompts and examples[^1^][1].
- **Why**: The paper aims to address the challenge of building multimodal models that can rapidly adapt to novel tasks using only a handful of annotated examples, without requiring task-specific fine-tuning or architectures[^1^][1].
- **How**: The paper proposes key architectural innovations to bridge powerful pretrained vision-only and language-only models, handle sequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as inputs[^1^][1]. The paper also trains Flamingo models on large-scale multimodal web corpora containing text and images, which endows them with in-context few-shot learning capabilities[^1^][1]. The paper evaluates Flamingo models on a variety of image and video tasks, such as visual question-answering, captioning, and multiple-choice visual question-answering, and shows that they achieve state-of-the-art results with few-shot learning[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2204.14198 "Flamingo: a Visual Language Model for Few-Shot Learning"
[2]: https://arxiv.org/pdf/2204.14198v2.pdf "arXiv.org"
[3]: https://arxiv-export3.library.cornell.edu/abs/2204.14198?context=cs.CV "[2204.14198] Flamingo: a Visual Language Model for Few-Shot Learning"

The paper claims the following contributions[^1^][1]:

- **A novel VLM architecture** that can bridge powerful pretrained vision-only and language-only models, handle sequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as inputs.
- **A large-scale multimodal web corpus** that contains text and images in various formats and domains, which is used to train Flamingo models and endow them with in-context few-shot learning capabilities.
- **A comprehensive evaluation** of Flamingo models on a variety of image and video tasks, such as visual question-answering, captioning, and multiple-choice visual question-answering, showing that they achieve state-of-the-art results with few-shot learning.

## Method Summary

[1]: https://arxiv.org/abs/2204.14198 "Flamingo: a Visual Language Model for Few-Shot Learning"
[2]: https://arxiv.org/pdf/2204.14198v2.pdf "arXiv.org"
[3]: https://arxiv-export3.library.cornell.edu/abs/2204.14198?context=cs.CV "[2204.14198] Flamingo: a Visual Language Model for Few-Shot Learning"

Here is a summary of the method section of the paper:

- The paper describes the **Flamingo architecture**, which consists of three main components: a **vision encoder**, a **language encoder**, and a **cross-modal transformer**[^1^][1].
- The **vision encoder** is based on a pretrained vision-only model, such as CLIP or DALL-E, that can extract visual features from images or videos[^1^][1]. The paper introduces a novel **visual tokenization** scheme that can handle sequences of arbitrarily interleaved visual and textual data, and a **visual embedding** layer that can map visual features to the same embedding space as textual tokens[^1^][1].
- The **language encoder** is based on a pretrained language-only model, such as GPT-3 or GPT-J, that can process natural language inputs[^1^][1]. The paper uses a standard **textual tokenization** scheme that can split text into subword tokens, and a **textual embedding** layer that can map textual tokens to the same embedding space as visual tokens[^1^][1].
- The **cross-modal transformer** is a large-scale transformer model that can fuse visual and textual information and generate multimodal outputs[^1^][1]. The paper introduces a novel **cross-modal attention** mechanism that can attend to both visual and textual tokens, and a **cross-modal generation** mechanism that can generate either visual or textual tokens depending on the task[^1^][1].
- The paper also describes the **Flamingo training**, which involves pretraining Flamingo models on a large-scale multimodal web corpus that contains text and images in various formats and domains[^1^][1]. The paper uses a combination of **masked language modeling**, **masked image modeling**, and **contrastive learning** objectives to train Flamingo models in an end-to-end fashion[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Flamingo architecture
class Flamingo(nn.Module):
  def __init__(self, vision_encoder, language_encoder, cross_modal_transformer):
    self.vision_encoder = vision_encoder # a pretrained vision-only model
    self.language_encoder = language_encoder # a pretrained language-only model
    self.cross_modal_transformer = cross_modal_transformer # a large-scale transformer model

  def forward(self, inputs):
    # Inputs can be a sequence of text and images in any order
    # Tokenize the inputs into visual and textual tokens
    visual_tokens, textual_tokens = tokenize(inputs)
    # Embed the tokens into a common embedding space
    visual_embeddings = self.vision_encoder.embed(visual_tokens)
    textual_embeddings = self.language_encoder.embed(textual_tokens)
    # Concatenate the embeddings and add positional embeddings
    embeddings = torch.cat([visual_embeddings, textual_embeddings], dim=1)
    embeddings += positional_embeddings(embeddings)
    # Apply the cross-modal transformer to fuse visual and textual information
    outputs = self.cross_modal_transformer(embeddings)
    # Generate either visual or textual tokens depending on the task
    generated_tokens = generate(outputs)
    return generated_tokens

# Define the Flamingo training
def train_flamingo(flamingo, corpus):
  # Corpus is a large-scale multimodal web corpus that contains text and images
  for batch in corpus:
    # Apply masking to some of the visual and textual tokens in the batch
    masked_batch, mask_labels = mask(batch)
    # Forward pass the masked batch through Flamingo
    generated_tokens = flamingo(masked_batch)
    # Compute the loss using masked language modeling, masked image modeling, and contrastive learning objectives
    loss = compute_loss(generated_tokens, mask_labels)
    # Backpropagate the loss and update the Flamingo parameters
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision
import transformers

# Define the Flamingo architecture
class Flamingo(nn.Module):
  def __init__(self, vision_encoder_name, language_encoder_name, cross_modal_transformer_name):
    super(Flamingo, self).__init__()
    # Load the pretrained vision-only model from torchvision or DALL-E
    self.vision_encoder = torchvision.models.__dict__[vision_encoder_name](pretrained=True) if vision_encoder_name in torchvision.models.__dict__ else dalle_pytorch.DALLE.load_from_checkpoint(vision_encoder_name)
    # Load the pretrained language-only model from transformers or GPT-J
    self.language_encoder = transformers.AutoModelForCausalLM.from_pretrained(language_encoder_name) if language_encoder_name in transformers.AutoModelForCausalLM.pretrained_model_archive_map else gpt_j_6B.load_from_checkpoint(language_encoder_name)
    # Load the pretrained cross-modal transformer model from transformers or FlaxGPTNeo
    self.cross_modal_transformer = transformers.AutoModelForCausalLM.from_pretrained(cross_modal_transformer_name) if cross_modal_transformer_name in transformers.AutoModelForCausalLM.pretrained_model_archive_map else FlaxGPTNeoForCausalLM.from_pretrained(cross_modal_transformer_name)
    # Define the visual tokenization scheme
    self.visual_tokenizer = VisualTokenizer(self.vision_encoder)
    # Define the textual tokenization scheme
    self.textual_tokenizer = transformers.AutoTokenizer.from_pretrained(language_encoder_name)
    # Define the visual embedding layer
    self.visual_embedding = nn.Linear(self.vision_encoder.config.hidden_size, self.cross_modal_transformer.config.hidden_size)
    # Define the textual embedding layer
    self.textual_embedding = nn.Embedding(self.textual_tokenizer.vocab_size, self.cross_modal_transformer.config.hidden_size)
    # Define the positional embedding layer
    self.positional_embedding = nn.Embedding(self.cross_modal_transformer.config.max_position_embeddings, self.cross_modal_transformer.config.hidden_size)

  def forward(self, inputs):
    # Inputs can be a sequence of text and images in any order
    # Tokenize the inputs into visual and textual tokens
    visual_tokens, textual_tokens = self.visual_tokenizer(inputs), self.textual_tokenizer(inputs)
    # Embed the tokens into a common embedding space
    visual_embeddings = self.visual_embedding(visual_tokens)
    textual_embeddings = self.textual_embedding(textual_tokens)
    # Concatenate the embeddings and add positional embeddings
    embeddings = torch.cat([visual_embeddings, textual_embeddings], dim=1)
    position_ids = torch.arange(embeddings.size(1), device=embeddings.device)
    embeddings += self.positional_embedding(position_ids)
    # Apply the cross-modal transformer to fuse visual and textual information
    outputs = self.cross_modal_transformer(inputs_embeds=embeddings).last_hidden_state
    # Generate either visual or textual tokens depending on the task
    generated_tokens = generate(outputs)
    return generated_tokens

# Define the VisualTokenizer class
class VisualTokenizer(nn.Module):
  def __init__(self, vision_encoder):
    super(VisualTokenizer, self).__init__()
    # Initialize the vision encoder parameters
    self.vision_encoder = vision_encoder
    # Define the image size for resizing inputs
    self.image_size = vision_encoder.config.image_size if hasattr(vision_encoder.config, "image_size") else 224
    # Define the number of visual tokens for splitting outputs
    self.num_visual_tokens = vision_encoder.config.num_visual_tokens if hasattr(vision_encoder.config, "num_visual_tokens") else 256

  def forward(self, inputs):
    # Inputs can be a sequence of text and images in any order
    # Initialize an empty list for storing visual tokens
    visual_tokens = []
    # Loop over the inputs and process each image
    for input in inputs:
      if isinstance(input, Image):
        # Resize the image to the desired size
        input = input.resize((self.image_size, self.image_size))
        # Convert the image to a tensor and normalize it
        input = torchvision.transforms.ToTensor()(input)
        input = torchvision.transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])(input)
        # Forward pass the image through the vision encoder and get the visual features
        visual_features = self.vision_encoder(input.unsqueeze(0)).last_hidden_state.squeeze(0)
        # Split the visual features into a fixed number of visual tokens
        visual_tokens.append(visual_features.view(self.num_visual_tokens, -1))
    # Stack the visual tokens into a tensor
    visual_tokens = torch.stack(visual_tokens, dim=0)
    return visual_tokens

# Define the Flamingo training
def train_flamingo(flamingo, corpus, optimizer, scheduler, device):
  # Corpus is a large-scale multimodal web corpus that contains text and images
  # Move the Flamingo model to the device
  flamingo.to(device)
  # Set the Flamingo model to training mode
  flamingo.train()
  # Loop over the batches in the corpus
  for batch in corpus:
    # Move the batch to the device
    batch = batch.to(device)
    # Apply masking to some of the visual and textual tokens in the batch
    masked_batch, mask_labels = mask(batch)
    # Forward pass the masked batch through Flamingo
    generated_tokens = flamingo(masked_batch)
    # Compute the loss using masked language modeling, masked image modeling, and contrastive learning objectives
    loss = compute_loss(generated_tokens, mask_labels)
    # Backpropagate the loss and update the Flamingo parameters
    loss.backward()
    optimizer.step()
    scheduler.step()
    # Zero the gradients
    optimizer.zero_grad()

# Define the mask function
def mask(batch):
  # Batch is a sequence of text and images in any order
  # Initialize an empty list for storing masked tokens and labels
  masked_batch = []
  mask_labels = []
  # Loop over the batch and process each input
  for input in batch:
    if isinstance(input, Image):
      # Apply random masking to some of the pixels in the image
      input, label = mask_image(input)
      masked_batch.append(input)
      mask_labels.append(label)
    elif isinstance(input, str):
      # Apply random masking to some of the words in the text
      input, label = mask_text(input)
      masked_batch.append(input)
      mask_labels.append(label)
  return masked_batch, mask_labels

# Define the mask_image function
def mask_image(image):
  # Image is a PIL image object
  # Convert the image to a tensor and normalize it
  image = torchvision.transforms.ToTensor()(image)
  image = torchvision.transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])(image)
  # Get the shape of the image
  C, H, W = image.shape
  # Define the masking probability for each pixel
  p = 0.15
  # Generate a random mask for each pixel
  mask = torch.bernoulli(torch.full((H, W), p))
  # Apply the mask to the image and replace the masked pixels with zeros
  masked_image = image * (1 - mask).unsqueeze(0)
  # Return the masked image and the original image as label
  return masked_image, image

# Define the mask_text function
def mask_text(text):
  # Text is a string of natural language
  # Tokenize the text into subword tokens using Flamingo's textual tokenizer
  tokens = flamingo.textual_tokenizer.tokenize(text)
  # Get the number of tokens
  N = len(tokens)
  # Define the masking probability for each token
  p = 0.15
  # Generate a random mask for each token
  mask = torch.bernoulli(torch.full((N,), p))
  # Apply the mask to the tokens and replace the masked tokens with [MASK] token
  masked_tokens = [token if mask[i] == 0 else "[MASK]" for i, token in enumerate(tokens)]
  # Convert the tokens to ids using Flamingo's textual tokenizer
  token_ids = flamingo.textual_tokenizer.convert_tokens_to_ids(masked_tokens)
  label_ids = flamingo.textual_tokenizer.convert_tokens_to_ids(tokens)
  # Return the token ids and the label ids
  return token_ids, label_ids

# Define the compute_loss function
def compute_loss(generated_tokens, mask_labels):
  # Generated tokens is a tensor of visual or textual tokens generated by Flamingo
  # Mask labels is a tensor of visual or textual tokens that are masked in the inputs
  # Initialize an empty list for storing losses
  losses = []
  # Loop over the generated tokens and mask labels and process each pair
  for generated_token, mask_label in zip(generated_tokens, mask_labels):
    if isinstance(mask_label, Image):
      # Compute the mean squared error loss between the generated visual token and the masked image label 
      loss = nn.MSELoss()(generated_token, mask_label)
      losses.append(loss)
    elif isinstance(mask_label, str):
      # Compute the cross entropy loss between the