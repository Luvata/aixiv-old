---
title: 1811.11286v3 Patch-based Progressive 3D Point Set Upsampling
date: 2018-11-12
---

# [Patch-based Progressive 3D Point Set Upsampling](http://arxiv.org/abs/1811.11286v3)

authors: Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga Sorkine-Hornung


## What, Why and How

[1]: https://arxiv.org/abs/1811.11286 "[1811.11286] Patch-based Progressive 3D Point Set Upsampling - arXiv.org"
[2]: http://export.arxiv.org/abs/1811.11286 "[1811.11286] Patch-based Progressive 3D Point Set Upsampling"
[3]: https://arxiv.org/pdf/1811.11389v3.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper presents a detail-driven deep neural network for point set upsampling, which is the task of increasing the resolution and quality of a 3D point cloud[^1^][1].
- **Why**: Point set upsampling is essential for point-based rendering and surface reconstruction, which are important applications in computer graphics and vision. Existing methods either suffer from low-quality outputs or high computational costs[^1^][1].
- **How**: The paper proposes a patch-based progressive approach that trains a cascade of upsampling networks on different levels of detail end-to-end. The paper also introduces several architectural design contributions that improve the performance and efficiency of the network, such as patch-wise feature aggregation, residual learning, and multi-scale supervision[^1^][1].

The paper claims that its method significantly outperforms the state-of-the-art learning-based and optimization-based approaches, both in terms of handling low-resolution inputs and revealing high-fidelity details[^1^][1]. The paper also provides qualitative and quantitative experiments to support its claims[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/1811.11286.pdf "arXiv:1811.11286v3 [cs.CV] 21 Mar 2019"
[2]: https://arxiv.org/pdf/1811.11286v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1811.11286 "[1811.11286] Patch-based Progressive 3D Point Set Upsampling"

According to the paper[^1^][1], the main contributions are:

- A patch-based progressive upsampling network for point sets that learns different levels of detail in multiple steps, where each step focuses on a local patch from the output of the previous step.
- A patch-wise feature aggregation module that enhances the feature representation of each point by aggregating features from neighboring patches.
- A residual learning scheme that enables the network to learn the residual details between the input and output point sets, rather than directly predicting the output coordinates.
- A multi-scale supervision strategy that applies loss functions on different levels of detail to guide the network training.

## Method Summary

[1]: https://arxiv.org/pdf/1811.11286.pdf "arXiv:1811.11286v3 [cs.CV] 21 Mar 2019"
[2]: https://arxiv.org/abs/1811.11286 "[1811.11286] Patch-based Progressive 3D Point Set Upsampling - arXiv.org"
[3]: http://export.arxiv.org/abs/1811.11286 "[1811.11286] Patch-based Progressive 3D Point Set Upsampling"

Here is a summary of the method section of the paper:

- The paper proposes a patch-based progressive upsampling network for point sets, which consists of four subnets that each perform 2x upsampling on different levels of detail. The input point set is first divided into overlapping patches, and each patch is fed into the first subnet to generate a denser patch. The output patches are then merged and divided again for the next subnet, and so on until the final output point set is obtained[^1^][1].
- The paper introduces a patch-wise feature aggregation module that enhances the feature representation of each point by aggregating features from neighboring patches. This module consists of two steps: patch-wise feature extraction and patch-wise feature fusion. The former step applies PointNet++  to extract local features for each patch, and the latter step uses a shared MLP to fuse features from adjacent patches[^1^][1].
- The paper adopts a residual learning scheme that enables the network to learn the residual details between the input and output point sets, rather than directly predicting the output coordinates. This scheme reduces the difficulty of learning and improves the stability of training. The paper also applies a chamfer distance loss function on the residual details to measure the similarity between point sets[^1^][1].
- The paper employs a multi-scale supervision strategy that applies loss functions on different levels of detail to guide the network training. Specifically, the paper uses a combination of chamfer distance loss and edge length loss on each subnet output to preserve both global structure and local details. The paper also uses an adaptive weight decay regularization to prevent overfitting[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a sparse point set P with N points
# Output: a dense point set Q with M points (M >> N)

# Define four subnets S1, S2, S3, S4 that each perform 2x upsampling
# Define a patch-wise feature aggregation module F
# Define a chamfer distance loss function L_cd
# Define an edge length loss function L_el
# Define an adaptive weight decay regularization R

# Initialize Q = P
# For i = 1 to 4:
  # Divide Q into overlapping patches {P_j}
  # For each patch P_j:
    # Extract and fuse features from neighboring patches using F
    # Upsample P_j to Q_j using Si and residual learning
  # Merge {Q_j} to form Q
  # Compute L_cd and L_el between Q and the ground truth point set
  # Compute R on the network parameters
  # Optimize the network using the total loss L_cd + L_el + R
# Return Q as the final output
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a sparse point set P with N points of dimension 3
# Output: a dense point set Q with M points of dimension 3 (M >> N)

# Define the patch size K and the patch overlap ratio r
# Define the number of subnets L and the upsampling factor u for each subnet
# Define the number of feature channels C for each subnet
# Define the number of nearest neighbors for feature fusion d
# Define the network parameters W for each subnet and feature aggregation module

# Initialize Q = P
# For i = 1 to L:
  # Initialize an empty list Q_list
  # Divide Q into overlapping patches {P_j} using KNN with K points and r overlap ratio
  # For each patch P_j:
    # Initialize an empty tensor F_j of size K x C
    # Find the d nearest patches of P_j using KNN on patch centroids
    # For each nearest patch P_k:
      # Apply PointNet++ on P_k to extract local features F_k of size K x C/u^i
      # Concatenate F_k to F_j along the channel dimension
    # Apply a shared MLP on F_j to fuse features from adjacent patches and reduce the channel dimension to C/u^i
    # Concatenate F_j and P_j along the channel dimension to form a tensor X_j of size K x (C/u^i + 3)
    # Apply Si on X_j to generate a tensor Y_j of size uK x (C/u^(i+1) + 3) using residual learning
    # Split Y_j into features and coordinates and append the coordinates to Q_list
  # Merge Q_list to form Q by averaging the coordinates of overlapping points
  # Compute L_cd and L_el between Q and the ground truth point set G using Eq. (1) and (2) in the paper
  # Compute R on W using Eq. (3) in the paper with adaptive lambda
  # Optimize W using the total loss L_cd + L_el + R with Adam optimizer
# Return Q as the final output
```