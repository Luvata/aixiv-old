---
title: 2210.10362v3 CPL  Counterfactual Prompt Learning for Vision and Language Models
date: 2022-10-11
---

# [CPL: Counterfactual Prompt Learning for Vision and Language Models](http://arxiv.org/abs/2210.10362v3)

authors: Xuehai He, Diji Yang, Weixi Feng, Tsu-Jui Fu, Arjun Akula, Varun Jampani, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Eric Wang


## What, Why and How

[1]: https://arxiv.org/pdf/2210.10362v3 "CPL: Counterfactual Prompt Learning for Vision and Language Models"
[2]: https://arxiv.org/abs/2210.10362 "[2210.10362] CPL: Counterfactual Prompt Learning for Vision and ..."
[3]: https://arxiv-export-lb.library.cornell.edu/pdf/1709.10362 "SOME ANALYTIC ASPECTS OF AUTOMORPHIC FORMS ON OF MINIMAL TYPE arXiv ..."

Here is a summary of the paper:

- **What**: The paper presents a novel method called CPL (Counterfactual Prompt Learning) for vision and language models, which uses counterfactual generation and contrastive learning to learn more generalizable prompts from few examples.
- **Why**: The paper aims to address the limitations of existing prompt tuning methods, which tend to learn spurious or entangled representations that lead to poor generalization to unseen concepts.
- **How**: The paper proposes to construct counterfactuals by identifying minimal non-spurious feature change between semantically-similar positive and negative samples that causes concept change, and then learn prompt representation from both factual and counterfactual examples via contrastive learning. The paper evaluates CPL on different vision and language tasks such as image classification, image-text retrieval, and visual question answering, and shows that CPL outperforms previous prompt tuning methods on CLIP[^1^][1].

## Main Contributions

The paper claims the following contributions:

- It introduces a novel counterfactual prompt learning method for vision and language models, which leverages counterfactual generation and contrastive learning to learn non-spurious and efficient prompts from few examples.
- It provides theoretical analysis and empirical evidence to show that CPL can learn more generalizable and robust prompt representations than existing prompt tuning methods.
- It demonstrates the effectiveness and versatility of CPL on different vision and language tasks across various few-shot scenarios, achieving state-of-the-art results on unseen classes and test sets.

## Method Summary

[1]: https://arxiv.org/pdf/2210.10362v3 "CPL: Counterfactual Prompt Learning for Vision and Language Models"
[2]: https://arxiv.org/abs/2210.10362 "[2210.10362] CPL: Counterfactual Prompt Learning for Vision and ..."

Here is a summary of the method section of the paper:

- The paper proposes a novel method called CPL (Counterfactual Prompt Learning) for vision and language models, which consists of two components: counterfactual generation and contrastive learning.
- Counterfactual generation aims to construct counterfactual examples by identifying minimal non-spurious feature change between semantically-similar positive and negative samples that causes concept change. For example, adding or removing a barn from an image of a train can change the prompt from "a large long train on a steel track" to "a large long train on a steel track near a barn".
- Contrastive learning aims to learn prompt representation from both factual and counterfactual examples by maximizing the similarity between the prompt and the positive sample, and minimizing the similarity between the prompt and the negative sample. The paper uses a contrastive loss function that incorporates both cosine similarity and Euclidean distance.
- The paper also introduces a regularization term to encourage the prompt to be close to the zero-shot CLIP prompt, which is obtained by concatenating the class names with a separator token.
- The paper optimizes the prompt parameters using gradient descent, while keeping the vision and language model parameters fixed. The paper also applies data augmentation techniques such as random cropping, resizing, and flipping to increase the diversity of the training data.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained vision and language model (VLM), a few-shot dataset D, a zero-shot CLIP prompt P_0
# Output: a learned prompt P

# Initialize the prompt parameters randomly
P = random_init()

# Loop until convergence or maximum iterations
while not converged or max_iter:

  # Sample a mini-batch of data from D
  B = sample(D)

  # Construct counterfactual examples for each sample in B
  C = construct_counterfactuals(B)

  # Concatenate B and C to form a new batch
  B = B + C

  # Compute the contrastive loss for each sample in B using VLM and P
  L_contrastive = compute_contrastive_loss(B, VLM, P)

  # Compute the regularization loss for P using P_0
  L_regularization = compute_regularization_loss(P, P_0)

  # Compute the total loss as a weighted sum of the contrastive and regularization losses
  L_total = alpha * L_contrastive + beta * L_regularization

  # Update the prompt parameters using gradient descent
  P = P - lr * grad(L_total, P)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define some hyperparameters
batch_size = 32 # the size of the mini-batch
max_iter = 1000 # the maximum number of iterations
alpha = 0.5 # the weight for the contrastive loss
beta = 0.1 # the weight for the regularization loss
lr = 0.01 # the learning rate
epsilon = 1e-6 # a small constant to avoid division by zero

# Load the pre-trained vision and language model (VLM)
VLM = clip.load("ViT-B/32", device="cuda")

# Load the few-shot dataset D
D = torchvision.datasets.ImageFolder("path/to/dataset")

# Define the data augmentation transforms
transforms = torchvision.transforms.Compose([
  torchvision.transforms.RandomResizedCrop(224),
  torchvision.transforms.RandomHorizontalFlip(),
  torchvision.transforms.ToTensor(),
  torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
])

# Define the data loader
loader = torch.utils.data.DataLoader(D, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

# Define the zero-shot CLIP prompt P_0 by concatenating the class names with a separator token
P_0 = "a photo of a " + " or a ".join(D.classes) + "."

# Initialize the prompt parameters randomly
P = torch.randn(len(D.classes), VLM.text_projection.shape[1], device="cuda", requires_grad=True)

# Define the optimizer
optimizer = torch.optim.Adam([P], lr=lr)

# Define a function to compute the cosine similarity between two tensors
def cosine_similarity(x, y):
  return torch.sum(x * y, dim=-1) / (torch.norm(x, dim=-1) * torch.norm(y, dim=-1) + epsilon)

# Define a function to compute the Euclidean distance between two tensors
def euclidean_distance(x, y):
  return torch.sqrt(torch.sum((x - y) ** 2, dim=-1) + epsilon)

# Define a function to construct counterfactual examples for a batch of data
def construct_counterfactuals(batch):
  # Unpack the batch into images and labels
  images, labels = batch

  # Compute the image features using VLM
  image_features = VLM.encode_image(images)

  # Compute the prompt features using VLM and P
  prompt_features = VLM.encode_text(P)

  # Compute the similarity scores between image features and prompt features
  scores = cosine_similarity(image_features[:, None, :], prompt_features[None, :, :])

  # Find the most similar prompt for each image
  top_scores, top_indices = torch.topk(scores, k=1, dim=-1)

  # Find the second most similar prompt for each image
  second_scores, second_indices = torch.topk(scores, k=2, dim=-1)
  second_scores = second_scores[:, -1]
  second_indices = second_indices[:, -1]

  # Find the least similar prompt for each image
  bottom_scores, bottom_indices = torch.topk(scores, k=1, dim=-1, largest=False)

  # Construct counterfactual examples by swapping labels with second or least similar prompts
  counterfactual_labels = labels.clone()
  swap_mask = torch.rand(len(labels)) < 0.5 # randomly choose to swap with second or least similar prompts
  counterfactual_labels[swap_mask] = second_indices[swap_mask]
  counterfactual_labels[~swap_mask] = bottom_indices[~swap_mask]

  # Return the counterfactual examples as a new batch of data
  return images, counterfactual_labels

# Loop until convergence or maximum iterations
for i in range(max_iter):

  # Sample a mini-batch of data from D using the data loader and apply data augmentation transforms
  batch = next(iter(loader))
  batch = transforms(batch)

  # Construct counterfactual examples for each sample in B using the function defined above
  counterfactual_batch = construct_counterfactuals(batch)

  # Concatenate B and C to form a new batch of data
  batch = torch.cat([batch, counterfactual_batch], dim=0)

  # Unpack the batch into images and labels
  images, labels = batch

  # Compute the image features using VLM
  image_features = VLM.encode_image(images)

  # Compute the prompt features using VLM and P
  prompt_features = VLM.encode_text(P)

  # Compute the zero-shot prompt feature using VLM and P_0
  zero_shot_feature = VLM.encode_text(P_0)

  # Compute the contrastive loss for each sample in B using cosine similarity and Euclidean distance
  L_contrastive = - cosine_similarity(image_features, prompt_features[labels]) + euclidean_distance(image_features, prompt_features[labels])

  # Compute the regularization loss for P using cosine similarity and Euclidean distance
  L_regularization = - cosine_similarity(prompt_features, zero_shot_feature) + euclidean_distance(prompt_features, zero_shot_feature)

  # Compute the total loss as a weighted sum of the contrastive and regularization losses
  L_total = alpha * torch.mean(L_contrastive) + beta * torch.mean(L_regularization)

  # Update the prompt parameters using gradient descent
  optimizer.zero_grad()
  L_total.backward()
  optimizer.step()

  # Print the loss value every 100 iterations
  if i % 100 == 0:
    print(f"Iteration {i}, Loss {L_total.item()}")
```