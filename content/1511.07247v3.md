---
title: 1511.07247v3 NetVLAD  CNN architecture for weakly supervised place recognition
date: 2015-11-08
---

# [NetVLAD: CNN architecture for weakly supervised place recognition](http://arxiv.org/abs/1511.07247v3)

authors: Relja ArandjeloviÄ‡, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef Sivic


## What, Why and How

[1]: https://arxiv.org/abs/1511.07247 "[1511.07247] NetVLAD: CNN architecture for weakly ... - arXiv.org"
[2]: https://arxiv.org/pdf/1511.07247v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2107.07247v3 "[2107.07247v3] Theoretical analysis of the leptonic decays $B\\to \\ell ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new CNN architecture for weakly supervised place recognition, called NetVLAD, which is inspired by the VLAD image representation.
- **Why**: The paper aims to tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph, using a learnt image representation that is compact and discriminative.
- **How**: The paper develops a new generalized VLAD layer that can be plugged into any CNN architecture and trained end-to-end using a weakly supervised ranking loss. The paper also uses images depicting the same places over time downloaded from Google Street View Time Machine as training data. The paper evaluates the proposed architecture on two place recognition benchmarks and two image retrieval benchmarks, and shows that it outperforms non-learnt image representations and off-the-shelf CNN descriptors.

## Main Contributions

The paper lists the following three principal contributions:

- A new CNN architecture that is trainable end-to-end directly for the place recognition task, called NetVLAD, which is a new generalized VLAD layer inspired by the VLAD image representation.
- A new weakly supervised ranking loss to learn parameters of the architecture from images depicting the same places over time downloaded from Google Street View Time Machine.
- A significant improvement over current state-of-the-art compact image representations on standard image retrieval benchmarks and place recognition benchmarks.

## Method Summary

[1]: https://arxiv.org/abs/1511.07247 "[1511.07247] NetVLAD: CNN architecture for weakly ... - arXiv.org"
[2]: https://arxiv.org/pdf/1511.07247v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2107.07247v3 "[2107.07247v3] Theoretical analysis of the leptonic decays $B\\to \\ell ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a new generalized VLAD layer, called NetVLAD, which is a differentiable module that can be plugged into any CNN architecture and trained end-to-end. The layer takes as input a set of local descriptors (e.g., feature maps of a convolutional layer) and outputs a global image representation that is invariant to permutations of the input descriptors. The layer learns a set of cluster centers and assigns each input descriptor to one of them, then aggregates the residuals between the descriptors and their assigned centers into a vector. The output vector is then normalized using intra-normalization and L2-normalization.
- The paper also proposes a new weakly supervised ranking loss, called triplet ranking loss, to train the NetVLAD layer and the CNN architecture. The loss function takes as input a set of triplets of images, where each triplet consists of a query image, a positive image (depicting the same place as the query), and a negative image (depicting a different place from the query). The loss function aims to minimize the distance between the query and the positive image representations, while maximizing the distance between the query and the negative image representations. The paper uses hard negative mining to select the most informative negative images for each query.
- The paper uses images depicting the same places over time downloaded from Google Street View Time Machine as training data. The paper constructs triplets of images by matching images based on their GPS coordinates and timestamps. The paper also augments the training data by applying random crops, rotations, flips, and color jittering to the images.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the CNN architecture with a NetVLAD layer at the end
cnn = CNN()
netvlad = NetVLAD(num_clusters, dim)

# Define the triplet ranking loss function
def triplet_ranking_loss(query, positive, negative, margin):
  # Compute the distance between query and positive
  dist_pos = euclidean_distance(query, positive)
  # Compute the distance between query and negative
  dist_neg = euclidean_distance(query, negative)
  # Return the hinge loss with a margin
  return max(0, margin + dist_pos - dist_neg)

# Load the training data from Google Street View Time Machine
train_data = load_data()

# Loop over the training epochs
for epoch in range(num_epochs):
  # Shuffle the training data
  train_data.shuffle()
  # Loop over the batches of triplets
  for batch in train_data.batches():
    # Extract the query, positive, and negative images from the batch
    query_images, positive_images, negative_images = batch
    # Apply data augmentation to the images
    query_images = augment(query_images)
    positive_images = augment(positive_images)
    negative_images = augment(negative_images)
    # Feed the images to the CNN and get the NetVLAD outputs
    query_outputs = netvlad(cnn(query_images))
    positive_outputs = netvlad(cnn(positive_images))
    negative_outputs = netvlad(cnn(negative_images))
    # Compute the triplet ranking loss for the batch
    loss = triplet_ranking_loss(query_outputs, positive_outputs, negative_outputs, margin)
    # Update the parameters of the CNN and NetVLAD using backpropagation
    update_parameters(loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torchvision
import random

# Define the CNN architecture with a NetVLAD layer at the end
# The CNN can be any pre-trained model, such as VGG16 or ResNet50
cnn = torchvision.models.vgg16(pretrained=True)
# Remove the last fully connected layer of the CNN
cnn.classifier = cnn.classifier[:-1]
# Define the NetVLAD layer as a subclass of torch.nn.Module
class NetVLAD(torch.nn.Module):
  # Initialize the layer with the number of clusters and the dimension of the input descriptors
  def __init__(self, num_clusters, dim):
    # Call the super class constructor
    super(NetVLAD, self).__init__()
    # Define the cluster centers as a learnable parameter
    self.cluster_centers = torch.nn.Parameter(torch.randn(num_clusters, dim))
    # Define the intra-normalization factor as a learnable parameter
    self.alpha = torch.nn.Parameter(torch.log(torch.tensor(num_clusters, dtype=torch.float32)))
    # Initialize the cluster centers using k-means clustering on a subset of training data (optional)

  # Define the forward pass of the layer
  def forward(self, x):
    # x is a tensor of shape (batch_size, dim, height, width) containing the input descriptors
    # Reshape x to (batch_size * height * width, dim)
    x = x.view(-1, x.shape[1])
    # Compute the dot product between x and the cluster centers
    dot_product = torch.matmul(x, self.cluster_centers.t())
    # Apply softmax to get the assignment probabilities of each descriptor to each cluster
    assignment = torch.nn.functional.softmax(dot_product * self.alpha, dim=1)
    # Reshape assignment to (batch_size, height * width, num_clusters)
    assignment = assignment.view(-1, x.shape[0] // assignment.shape[0], assignment.shape[1])
    # Compute the residuals between x and the cluster centers weighted by the assignment probabilities
    residuals = x.unsqueeze(2) - self.cluster_centers.unsqueeze(0)
    residuals = residuals * assignment.unsqueeze(1)
    # Sum up the residuals along the height and width dimensions to get the output vector of shape (batch_size, num_clusters * dim)
    output = residuals.sum(dim=0)
    output = output.view(output.shape[0], -1)
    # Apply intra-normalization and L2-normalization to the output vector
    output = torch.nn.functional.normalize(output, p=2, dim=1)
    output = torch.nn.functional.normalize(output, p=2, dim=0)
    # Return the output vector
    return output

# Instantiate the NetVLAD layer with the desired number of clusters and dimension
netvlad = NetVLAD(num_clusters=64, dim=4096)

# Define the triplet ranking loss function
def triplet_ranking_loss(query, positive, negative, margin):
  # query, positive, and negative are tensors of shape (batch_size, num_clusters * dim) containing the NetVLAD outputs
  # margin is a scalar indicating the desired margin between positive and negative distances
  # Compute the L2 distance between query and positive along the feature dimension
  dist_pos = torch.norm(query - positive, p=2, dim=1)
  # Compute the L2 distance between query and negative along the feature dimension
  dist_neg = torch.norm(query - negative, p=2, dim=1)
  # Return the mean hinge loss with a margin over the batch dimension
  return torch.mean(torch.clamp(margin + dist_pos - dist_neg, min=0))

# Load the training data from Google Street View Time Machine
# The training data should be a list of triplets of images with their GPS coordinates and timestamps
train_data = load_data()

# Define some hyperparameters for training
num_epochs = 10 # Number of epochs to train for
batch_size = 32 # Batch size for training
margin = 0.1 # Margin for triplet ranking loss
learning_rate = 0.001 # Learning rate for optimizer

# Define an optimizer for updating the parameters of CNN and NetVLAD
optimizer = torch.optim.Adam(list(cnn.parameters()) + list(netvlad.parameters()), lr=learning_rate)

# Loop over the training epochs
for epoch in range(num_epochs):
  # Shuffle the training data
  random.shuffle(train_data)
  # Loop over the batches of triplets
  for i in range(0, len(train_data), batch_size):
    # Extract the query, positive, and negative images from the batch
    query_images = [triplet[0] for triplet in train_data[i:i+batch_size]]
    positive_images = [triplet[1] for triplet in train_data[i:i+batch_size]]
    negative_images = [triplet[2] for triplet in train_data[i:i+batch_size]]
    # Convert the images to tensors and move them to the device (CPU or GPU)
    query_images = torch.stack(query_images).to(device)
    positive_images = torch.stack(positive_images).to(device)
    negative_images = torch.stack(negative_images).to(device)
    # Apply data augmentation to the images
    query_images = augment(query_images)
    positive_images = augment(positive_images)
    negative_images = augment(negative_images)
    # Feed the images to the CNN and get the NetVLAD outputs
    query_outputs = netvlad(cnn(query_images))
    positive_outputs = netvlad(cnn(positive_images))
    negative_outputs = netvlad(cnn(negative_images))
    # Compute the triplet ranking loss for the batch
    loss = triplet_ranking_loss(query_outputs, positive_outputs, negative_outputs, margin)
    # Zero out the gradients of the optimizer
    optimizer.zero_grad()
    # Compute the gradients of the loss with respect to the parameters of CNN and NetVLAD
    loss.backward()
    # Update the parameters of CNN and NetVLAD using the optimizer
    optimizer.step()
```