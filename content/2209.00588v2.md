---
title: 2209.00588v2 Transformers are Sample-Efficient World Models
date: 2022-09-01
---

# [Transformers are Sample-Efficient World Models](http://arxiv.org/abs/2209.00588v2)

authors: Vincent Micheli, Eloi Alonso, Fran√ßois Fleuret


## What, Why and How

[1]: https://arxiv.org/abs/2209.00588 "[2209.00588] Transformers are Sample-Efficient World Models - arXiv.org"
[2]: https://arxiv.org/pdf/2209.00588v2.pdf "arXiv:2209.00588v2 [cs.LG] 1 Mar 2023"
[3]: http://export.arxiv.org/abs/2209.00588 "[2209.00588] Transformers are Sample-Efficient World Models"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer.
- **Why**: The paper aims to address the sample inefficiency problem of deep reinforcement learning agents, which limits their application to real-world problems.
- **How**: The paper leverages the success of Transformers in sequence modeling tasks to build a world model that can generate accurate and diverse imagined trajectories. The paper evaluates IRIS on the Atari 100k benchmark and shows that it outperforms existing methods without lookahead search.


## Main Contributions

[1]: https://arxiv.org/abs/2209.00588 "[2209.00588] Transformers are Sample-Efficient World Models - arXiv.org"
[2]: https://arxiv.org/pdf/2209.00588v2.pdf "arXiv:2209.00588v2 [cs.LG] 1 Mar 2023"
[3]: http://export.arxiv.org/abs/2209.00588 "[2209.00588] Transformers are Sample-Efficient World Models"

According to the paper[^1^][1], the main contributions are:

- **Introducing IRIS**, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer.
- **Demonstrating the effectiveness of Transformers** as world models for sample-efficient reinforcement learning, by showing that they can generate accurate and diverse imagined trajectories.
- **Setting a new state of the art** for methods without lookahead search on the Atari 100k benchmark, by achieving a mean human normalized score of 1.046, and outperforming humans on 10 out of 26 games.


## Method Summary

[1]: https://arxiv.org/abs/2209.00588 "[2209.00588] Transformers are Sample-Efficient World Models - arXiv.org"
[2]: https://arxiv.org/pdf/2209.00588v2.pdf "arXiv:2209.00588v2 [cs.LG] 1 Mar 2023"
[3]: http://export.arxiv.org/abs/2209.00588 "[2209.00588] Transformers are Sample-Efficient World Models"

Here is a summary of the method section of the paper[^1^][2]:

- The paper proposes a model-based reinforcement learning framework that consists of three components: a **world model**, an **actor**, and a **critic**.
- The **world model** is composed of a discrete autoencoder and an autoregressive Transformer. The discrete autoencoder compresses high-dimensional observations into discrete latent codes, which are then fed into the Transformer. The Transformer learns to predict future latent codes conditioned on past ones, using self-attention and causal masking. The world model can also generate diverse imagined trajectories by sampling from the latent space.
- The **actor** and the **critic** are trained inside the world model using policy gradients. The actor is a recurrent neural network that outputs actions given latent codes. The critic is another recurrent neural network that estimates the value function given latent codes and actions. Both networks share the same initial hidden state, which is computed by a linear layer from the first latent code of a trajectory.
- The paper uses two variants of IRIS: IRIS-RND and IRIS-IMAGINE. IRIS-RND adds intrinsic rewards based on prediction errors to encourage exploration. IRIS-IMAGINE leverages multiple imagined trajectories to reduce variance in policy gradients. Both variants use clipped importance sampling to correct for the distribution mismatch between the real and the imagined data.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```
# Initialize world model, actor and critic networks
world_model = DiscreteAutoencoder + Transformer
actor = RNN
critic = RNN

# Collect initial data from environment using random policy
data = collect_data(env, random_policy)

# Train world model on data using reconstruction and prediction losses
world_model.train(data)

# Repeat until convergence
for iteration in range(max_iterations):

  # Generate imagined trajectories from world model
  imagined_data = world_model.imagine()

  # Train actor and critic on imagined data using policy gradients
  actor.train(imagined_data, critic)
  critic.train(imagined_data, actor)

  # Collect new data from environment using actor policy
  new_data = collect_data(env, actor.policy)

  # Add intrinsic rewards to new data based on prediction errors (optional)
  new_data.add_rewards(world_model.errors)

  # Update data buffer with new data
  data.update(new_data)

  # Train world model on data using reconstruction and prediction losses
  world_model.train(data)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```
# Define hyperparameters
num_iterations = 1000 # number of training iterations
batch_size = 64 # batch size for training networks
num_trajectories = 16 # number of imagined trajectories per batch
trajectory_length = 50 # length of each trajectory
latent_size = 64 # size of the latent codes
action_size = env.action_space.n # size of the action space
hidden_size = 256 # size of the hidden states and outputs of RNNs
num_heads = 8 # number of attention heads in Transformer
num_layers = 6 # number of layers in Transformer
learning_rate = 1e-4 # learning rate for Adam optimizer
discount_factor = 0.99 # discount factor for future rewards
clip_ratio = 0.2 # clipping ratio for importance sampling
entropy_coef = 0.01 # entropy coefficient for policy regularization
rnd_coef = 0.1 # coefficient for intrinsic rewards

# Initialize world model, actor and critic networks
world_model = DiscreteAutoencoder(latent_size) + Transformer(latent_size, num_heads, num_layers)
actor = RNN(latent_size, hidden_size, action_size)
critic = RNN(latent_size + action_size, hidden_size, 1)

# Initialize target and predictor networks for RND (optional)
target_net = MLP(observation_size, hidden_size, latent_size)
predictor_net = MLP(observation_size, hidden_size, latent_size)

# Initialize optimizers
world_model_optimizer = Adam(world_model.parameters(), learning_rate)
actor_optimizer = Adam(actor.parameters(), learning_rate)
critic_optimizer = Adam(critic.parameters(), learning_rate)
predictor_optimizer = Adam(predictor_net.parameters(), learning_rate) # optional

# Initialize data buffer
data_buffer = ReplayBuffer()

# Collect initial data from environment using random policy
for i in range(batch_size * trajectory_length):
  observation = env.reset() if done else observation
  action = env.action_space.sample()
  next_observation, reward, done, info = env.step(action)
  data_buffer.add(observation, action, reward, next_observation, done)
  observation = next_observation

# Train world model on data buffer using reconstruction and prediction losses
for i in range(num_iterations):
  observations, actions, rewards, next_observations, dones = data_buffer.sample(batch_size * trajectory_length)
  latent_codes = world_model.encode(observations) # shape: (batch_size * trajectory_length, latent_size)
  reconstructions = world_model.decode(latent_codes) # shape: (batch_size * trajectory_length, observation_size)
  predictions = world_model.predict(latent_codes) # shape: (batch_size * trajectory_length - 1, latent_size)
  reconstruction_loss = MSE(reconstructions, observations)
  prediction_loss = MSE(predictions, latent_codes[1:])
  world_model_loss = reconstruction_loss + prediction_loss
  world_model_optimizer.zero_grad()
  world_model_loss.backward()
  world_model_optimizer.step()

# Repeat until convergence
for iteration in range(num_iterations):

  # Generate imagined trajectories from world model
  imagined_observations = [] # shape: (num_trajectories * batch_size, trajectory_length + 1, observation_size)
  imagined_actions = [] # shape: (num_trajectories * batch_size, trajectory_length + 1)
  imagined_rewards = [] # shape: (num_trajectories * batch_size, trajectory_length + 1)
  imagined_dones = [] # shape: (num_trajectories * batch_size, trajectory_length + 1)
  
  for i in range(num_trajectories):
    # Sample a batch of initial observations from data buffer
    initial_observations = data_buffer.sample_initial_observations(batch_size) # shape: (batch_size, observation_size)

    # Encode initial observations into latent codes
    initial_latent_codes = world_model.encode(initial_observations) # shape: (batch_size, latent_size)

    # Generate a batch of imagined trajectories from initial latent codes using world model and actor
    imagined_trajectory_observations = [initial_observations] # shape: (trajectory_length + 1, batch_size, observation_size)
    imagined_trajectory_actions = [] # shape: (trajectory_length + 1, batch_size)
    imagined_trajectory_rewards = [] # shape: (trajectory_length + 1, batch_size)
    imagined_trajectory_dones = [] # shape: (trajectory_length + 1, batch_size)

    actor_hidden_state = actor.init_hidden_state(initial_latent_codes) # shape: (batch_size, hidden_size)
    critic_hidden_state = critic.init_hidden_state(torch.cat([initial_latent_codes, actor_hidden_state], dim=-1)) # shape: (batch_size, hidden_size)
    world_model_hidden_state = world_model.init_hidden_state(initial_latent_codes) # shape: (batch_size, num_layers, num_heads, hidden_size)

    for t in range(trajectory_length):
      # Sample an action from actor given current latent code and hidden state
      action, actor_hidden_state = actor.sample_action(imagined_trajectory_observations[t], actor_hidden_state) # shape: (batch_size,), (batch_size, hidden_size)

      # Predict next latent code from world model given current latent code and hidden state
      next_latent_code, world_model_hidden_state = world_model.predict_next_latent_code(imagined_trajectory_observations[t], world_model_hidden_state) # shape: (batch_size, latent_size), (batch_size, num_layers, num_heads, hidden_size)

      # Decode next observation from next latent code
      next_observation = world_model.decode(next_latent_code) # shape: (batch_size, observation_size)

      # Compute reward from critic given current latent code, action and hidden state
      reward, critic_hidden_state = critic.compute_reward(torch.cat([imagined_trajectory_observations[t], action], dim=-1), critic_hidden_state) # shape: (batch_size,), (batch_size, hidden_size)

      # Compute done mask from next observation
      done = env.is_terminal(next_observation) # shape: (batch_size,)

      # Append to trajectory lists
      imagined_trajectory_observations.append(next_observation)
      imagined_trajectory_actions.append(action)
      imagined_trajectory_rewards.append(reward)
      imagined_trajectory_dones.append(done)

    # Concatenate trajectory lists along batch dimension and append to imagined data lists
    imagined_observations.append(torch.cat(imagined_trajectory_observations, dim=0)) # shape: (trajectory_length + 1, num_trajectories * batch_size, observation_size)
    imagined_actions.append(torch.cat(imagined_trajectory_actions, dim=0)) # shape: (trajectory_length + 1, num_trajectories * batch_size)
    imagined_rewards.append(torch.cat(imagined_trajectory_rewards, dim=0)) # shape: (trajectory_length + 1, num_trajectories * batch_size)
    imagined_dones.append(torch.cat(imagined_trajectory_dones, dim=0)) # shape: (trajectory_length + 1, num_trajectories * batch_size)

  # Stack imagined data lists along trajectory dimension
  imagined_observations = torch.stack(imagined_observations, dim=0) # shape: (num_trajectories, trajectory_length + 1, batch_size, observation_size)
  imagined_actions = torch.stack(imagined_actions, dim=0) # shape: (num_trajectories, trajectory_length + 1, batch_size)
  imagined_rewards = torch.stack(imagined_rewards, dim=0) # shape: (num_trajectories, trajectory_length + 1, batch_size)
  imagined_dones = torch.stack(imagined_dones, dim=0) # shape: (num_trajectories, trajectory_length + 1, batch_size)

  # Train actor and critic on imagined data using policy gradients
  actor_loss = 0
  critic_loss = 0

  for i in range(num_trajectories):
    # Compute advantages and returns from rewards and dones using critic network
    advantages = [] # shape: (trajectory_length + 1,)
    returns = [] # shape: (trajectory_length + 1,)
    advantage = 0
    return = 0

    for t in reversed(range(trajectory_length + 1)):
      delta = imagined_rewards[i][t] + discount_factor * critic.predict_value(imagined_observations[i][t+1], imagined_actions[i][t+1]) * (1 - imagined_dones[i][t+1]) - critic.predict_value(imagined_observations[i][t], imagined_actions[i][t])
      advantage = delta + discount_factor * entropy_coef * advantage
      return = imagined_rewards[i][t] + discount_factor * return
      advantages.insert(0, advantage)
      returns.insert(0, return)

    advantages = torch.stack(advantages) # shape: (trajectory_length + 1,)
    returns = torch.stack(returns) # shape: (trajectory_length + 1,)
    
    # Normalize advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    for t in range(trajectory_length + 1):
      # Compute actor loss using clipped importance sampling
      log_prob = actor.compute_log_prob(imagined_observations[i][t], imagined_actions[i][t])
      ratio = torch.exp(log_prob - old_log_prob)
      clipped_ratio = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio)
      actor_loss += -torch.min(ratio * advantages[t], clipped_ratio * advantages[t])

      # Compute critic loss using mean squared error