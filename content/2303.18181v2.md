---
title: 2303.18181v2 A Closer Look at Parameter-Efficient Tuning in Diffusion Models
date: 2023-03-19
---

# [A Closer Look at Parameter-Efficient Tuning in Diffusion Models](http://arxiv.org/abs/2303.18181v2)

authors: Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu


## What, Why and How

[1]: https://arxiv.org/abs/2303.18181 "A Closer Look at Parameter-Efficient Tuning in Diffusion Models"
[2]: https://arxiv.org/pdf/2303.18181.pdf "arXiv.org e-Print archive"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2303.18181 "[2303.18181] A Closer Look at Parameter-Efficient Tuning in Diffusion ..."

Here is a summary of the paper:

- **What**: The paper investigates parameter-efficient tuning in large diffusion models by inserting small learnable modules (termed adapters).
- **Why**: The paper aims to address the memory and time inefficiency of fine-tuning large diffusion models for various customized tasks.
- **How**: The paper decomposes the design space of adapters into orthogonal factors, performs Analysis of Variance (ANOVA) to identify the critical factor, and provides a recipe for parameter-efficient tuning that achieves comparable or superior performance to the fully fine-tuned baseline with only 0.75% extra parameters.

The paper is titled "A Closer Look at Parameter-Efficient Tuning in Diffusion Models" by Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu[^1^][1]. It was submitted to arXiv on 31 Mar 2023.

## Main Contributions

[1]: https://arxiv.org/abs/2303.18181 "A Closer Look at Parameter-Efficient Tuning in Diffusion Models"
[2]: https://arxiv.org/abs/2304.06648 "DiffFit: Unlocking Transferability of Large Diffusion Models via Simple ..."
[3]: http://export.arxiv.org/abs/2303.18181 "Title: A Closer Look at Parameter-Efficient Tuning in Diffusion Models"
[4]: https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html "Understanding Parameter-Efficient Finetuning of Large Language Models ..."

According to the paper[^1^][1], the main contributions are:

- They perform a systematic analysis of the design space of adapters for diffusion models, and identify the input position of adapters as the most influential factor for downstream performance.
- They propose a novel input position for adapters after the cross-attention block, which outperforms other positions and achieves better alignment between the condition and the latent variables.
- They provide a recipe for parameter-efficient tuning in diffusion models that achieves comparable or superior results to the fully fine-tuned baseline with only 0.75% extra parameters on various tasks, such as text-to-image generation, image-to-image translation, image restoration, and 3D synthesis.

## Method Summary

[1]: https://arxiv.org/abs/2303.18181 "A Closer Look at Parameter-Efficient Tuning in Diffusion Models"
[2]: http://export.arxiv.org/abs/2303.18181 "Title: A Closer Look at Parameter-Efficient Tuning in Diffusion Models"
[3]: https://deepai.org/publication/multi-head-adapter-routing-for-data-efficient-fine-tuning "Multi-Head Adapter Routing for Data-Efficient Fine-Tuning"

Here is a summary of the method section of the paper:

- The paper introduces the concept of adapters, which are small learnable modules inserted into the pre-trained diffusion model to enable parameter-efficient tuning.
- The paper decomposes the design space of adapters into three orthogonal factors: the input position, the output position, and the function form. The input position refers to where the adapter receives the input from the diffusion model, the output position refers to where the adapter sends the output to the diffusion model, and the function form refers to how the adapter transforms the input to the output.
- The paper performs Analysis of Variance (ANOVA), a statistical method to analyze the effect of discrete factors on continuous variables, to evaluate how each factor influences the performance of downstream tasks. The paper uses CLIP similarity as a proxy metric for downstream performance, which measures how well the generated images match the condition text or image.
- The paper finds that the input position is the most important factor, and that putting the input position after the cross-attention block leads to the best performance. The paper also provides some visualization analyses to explain why this position is optimal.
- The paper provides a recipe for parameter-efficient tuning in diffusion models based on their findings. The recipe consists of inserting adapters with a linear function form after each cross-attention block, and fine-tuning only these adapters while keeping the rest of the diffusion model fixed. The paper shows that this recipe achieves comparable or superior results to the fully fine-tuned baseline with only 0.75% extra parameters on various tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the adapter module
class Adapter(nn.Module):
  def __init__(self, input_dim, output_dim):
    super().__init__()
    self.linear = nn.Linear(input_dim, output_dim) # linear function form
    self.bias = nn.Parameter(torch.zeros(output_dim)) # learnable bias term

  def forward(self, x):
    return self.linear(x) + self.bias # add bias to the linear output

# Insert adapters after each cross-attention block in the diffusion model
for layer in diffusion_model.layers:
  if layer.is_cross_attention:
    layer.adapter = Adapter(layer.hidden_size, layer.hidden_size)

# Fine-tune only the adapters while keeping the diffusion model fixed
for epoch in epochs:
  for batch in data_loader:
    condition, target = batch # get condition and target from data
    diffusion_model.eval() # set diffusion model to evaluation mode
    for adapter in diffusion_model.adapters:
      adapter.train() # set adapters to training mode
    optimizer.zero_grad() # reset gradients
    output = diffusion_model(condition) # generate output from condition
    loss = clip_loss(output, target) # compute CLIP similarity loss
    loss.backward() # compute gradients
    optimizer.step() # update parameters
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.optim as optim
import clip # https://github.com/openai/CLIP
import stable_diffusion # https://github.com/openai/stable-baselines3

# Define the adapter module
class Adapter(nn.Module):
  def __init__(self, input_dim, output_dim):
    super().__init__()
    self.linear = nn.Linear(input_dim, output_dim) # linear function form
    self.bias = nn.Parameter(torch.zeros(output_dim)) # learnable bias term

  def forward(self, x):
    return self.linear(x) + self.bias # add bias to the linear output

# Load the pre-trained diffusion model and the CLIP model
diffusion_model = stable_diffusion.load_model("stable_diffusion") # load from https://github.com/openai/stable-baselines3
clip_model = clip.load_model("ViT-B/32") # load from https://github.com/openai/CLIP

# Insert adapters after each cross-attention block in the diffusion model
for layer in diffusion_model.layers:
  if layer.is_cross_attention:
    layer.adapter = Adapter(layer.hidden_size, layer.hidden_size)

# Freeze the diffusion model parameters and create an optimizer for the adapters
for param in diffusion_model.parameters():
  param.requires_grad = False # freeze diffusion model parameters
adapter_params = [adapter.parameters() for adapter in diffusion_model.adapters] # get adapter parameters
optimizer = optim.Adam(adapter_params, lr=1e-4) # create an optimizer for the adapters

# Define the CLIP similarity loss function
def clip_loss(output, target):
  output_embed = clip_model.encode_image(output) # encode output images with CLIP
  target_embed = clip_model.encode_text(target) # encode target texts or images with CLIP
  return -torch.cosine_similarity(output_embed, target_embed).mean() # compute negative cosine similarity and average

# Load the data loader for the downstream task
data_loader = load_data_loader(task) # load data loader for a specific task

# Fine-tune only the adapters while keeping the diffusion model fixed
for epoch in epochs:
  for batch in data_loader:
    condition, target = batch # get condition and target from data
    diffusion_model.eval() # set diffusion model to evaluation mode
    for adapter in diffusion_model.adapters:
      adapter.train() # set adapters to training mode
    optimizer.zero_grad() # reset gradients
    output = diffusion_model(condition) # generate output from condition
    loss = clip_loss(output, target) # compute CLIP similarity loss
    loss.backward() # compute gradients
    optimizer.step() # update parameters
```