---
title: 2102.02754v2 Only a Matter of Style  Age Transformation Using a Style-Based Regression Model
date: 2021-02-03
---

# [Only a Matter of Style: Age Transformation Using a Style-Based Regression Model](http://arxiv.org/abs/2102.02754v2)

authors: Yuval Alaluf, Or Patashnik, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2102.02754 "[2102.02754] Only a Matter of Style: Age Transformation Using a Style ..."
[2]: http://export.arxiv.org/abs/2102.02754v2 "[2102.02754v2] Only a Matter of Style: Age Transformation Using a Style ..."
[3]: https://arxiv.org/pdf/2012.02754v2.pdf "arXiv:2012.02754v2 [quant-ph] 8 Apr 2022"

Here is a summary of the paper:

- **What**: The paper presents a method for age transformation of facial images using a style-based regression model that encodes real images into the latent space of a pre-trained unconditional GAN (e.g., StyleGAN).
- **Why**: The paper aims to address the challenges of accurately modeling the complex transformation of facial appearance over time, while preserving the input identity and providing fine-grained control over the generated image.
- **How**: The paper employs a pre-trained age regression network to guide the encoder in generating the latent codes corresponding to the desired age. The paper also learns a more disentangled, non-linear path for age transformation in the latent space, and demonstrates that the end-to-end nature of the approach allows for further editing of the generated images. The paper evaluates the method qualitatively and quantitatively against state-of-the-art approaches[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2102.02754 "[2102.02754] Only a Matter of Style: Age Transformation Using a Style ..."
[2]: http://export.arxiv.org/abs/2102.02754v2 "[2102.02754v2] Only a Matter of Style: Age Transformation Using a Style ..."
[3]: https://arxiv.org/pdf/2012.02754v2.pdf "arXiv:2012.02754v2 [quant-ph] 8 Apr 2022"

According to the paper[^1^][1], the main contributions are:

- **A novel image-to-image translation method** that learns to directly encode real facial images into the latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a given aging shift.
- **A style-based regression model** that employs a pre-trained age regression network to explicitly guide the encoder in generating the latent codes corresponding to the desired age, and approaches the continuous aging process as a regression task between the input age and desired target age.
- **A more disentangled, non-linear path for age transformation** in the latent space, unlike approaches that operate solely in the latent space using a prior on the path controlling age.
- **An end-to-end approach that allows for further editing of the generated images**, coupled with the rich semantic latent space of StyleGAN, such as changing hair color, facial expression, etc.
- **Qualitative and quantitative evaluations** that show the advantages of the proposed method compared to state-of-the-art approaches on various datasets and metrics.

## Method Summary

[1]: https://arxiv.org/abs/2102.02754 "[2102.02754] Only a Matter of Style: Age Transformation Using a Style ..."
[2]: http://export.arxiv.org/abs/2102.02754v2 "[2102.02754v2] Only a Matter of Style: Age Transformation Using a Style ..."
[3]: https://arxiv.org/pdf/2012.02754v2.pdf "arXiv:2012.02754v2 [quant-ph] 8 Apr 2022"

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a **style-based regression model** that consists of an encoder network and a pre-trained StyleGAN generator. The encoder network takes a real facial image and a desired age as inputs, and outputs a latent code that is fed into the StyleGAN generator to produce an aged image. The encoder network is trained with a pre-trained age regression network that predicts the age of the generated image and guides the encoder to match it with the desired age. The paper also introduces a **style consistency loss** that encourages the encoder to preserve the identity and style of the input image across different ages.
- The paper also proposes a **latent path optimization** technique that learns a more disentangled, non-linear path for age transformation in the latent space, instead of relying on a prior on the path controlling age. The paper optimizes the latent codes along the path to minimize the age prediction error and the reconstruction error, while maximizing the perceptual quality and identity preservation of the generated images. The paper also introduces a **path regularization term** that encourages smoothness and diversity along the path.
- The paper also demonstrates that the **end-to-end nature** of the proposed method allows for further editing of the generated images, such as changing hair color, facial expression, etc., by manipulating the latent codes or applying style mixing. The paper shows that these edits can be applied consistently across different ages, thanks to the style consistency loss and the latent path optimization.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder network E that takes an image x and a desired age y as inputs and outputs a latent code w
E = Encoder()

# Load the pre-trained StyleGAN generator G that takes a latent code w as input and outputs an image x'
G = StyleGAN()

# Load the pre-trained age regression network R that takes an image x as input and outputs an age prediction y'
R = AgeRegression()

# Define the style consistency loss L_s that measures the similarity between the styles of two images
L_s = StyleLoss()

# Define the latent path optimization function O that takes a set of latent codes W and a set of desired ages Y as inputs and outputs a set of optimized latent codes W*
O = PathOptimization()

# Train the encoder network E with the following objectives:
# - Minimize the age prediction error: L_a = |y - R(G(E(x, y)))|
# - Minimize the reconstruction error: L_r = |x - G(E(x, y))|
# - Maximize the style consistency: L_s = L_s(x, G(E(x, y)))
# - Maximize the perceptual quality: L_p = PerceptualLoss(x, G(E(x, y)))
# - Maximize the identity preservation: L_i = IdentityLoss(x, G(E(x, y)))
E.train(minimize(L_a + L_r + L_s + L_p + L_i))

# Optimize the latent path W with the following objectives:
# - Minimize the age prediction error: L_a = |Y - R(G(W))|
# - Minimize the reconstruction error: L_r = |X - G(W)|
# - Maximize the perceptual quality: L_p = PerceptualLoss(X, G(W))
# - Maximize the identity preservation: L_i = IdentityLoss(X, G(W))
# - Regularize the path smoothness and diversity: L_pr = PathRegularization(W)
W* = O(W, Y, minimize(L_a + L_r + L_p + L_i + L_pr))

# Generate aged images for a given image x and a set of desired ages Y using the encoder network E or the optimized latent path W*
X' = G(E(x, Y)) # or X' = G(W*)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import stylegan2 # https://github.com/rosinality/stylegan2-pytorch
import agenet # https://github.com/yuval-alaluf/agenet

# Define the encoder network E that takes an image x and a desired age y as inputs and outputs a latent code w
# The encoder network consists of a convolutional backbone followed by a fully connected layer
class Encoder(torch.nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Define the convolutional backbone
    self.backbone = torchvision.models.resnet18(pretrained=True)
    # Remove the last layer of the backbone
    self.backbone = torch.nn.Sequential(*list(self.backbone.children())[:-1])
    # Define the fully connected layer
    self.fc = torch.nn.Linear(512 + 1, 512) # 512 for the backbone output, 1 for the desired age
  
  def forward(self, x, y):
    # Normalize the image x to [-1, 1] range
    x = (x - 0.5) * 2
    # Extract the features from the backbone
    f = self.backbone(x)
    # Flatten the features
    f = f.view(f.size(0), -1)
    # Concatenate the features and the desired age
    f = torch.cat([f, y], dim=1)
    # Apply the fully connected layer
    w = self.fc(f)
    return w

# Load the pre-trained StyleGAN generator G that takes a latent code w as input and outputs an image x'
# The generator is based on StyleGAN2 with FFHQ dataset
G = stylegan2.Generator(1024, 512, 8) # 1024 for image resolution, 512 for latent dimension, 8 for style depth
G.load_state_dict(torch.load('stylegan2-ffhq-config-f.pt')) # load the pre-trained weights

# Load the pre-trained age regression network R that takes an image x as input and outputs an age prediction y'
# The age regression network is based on AgeNet with IMDB-WIKI dataset
R = agenet.AgeNet()
R.load_state_dict(torch.load('agenet-imdb-wiki.pt')) # load the pre-trained weights

# Define the style consistency loss L_s that measures the similarity between the styles of two images
# The style consistency loss is based on the Gram matrix of the VGG19 features
class StyleLoss(torch.nn.Module):
  def __init__(self):
    super(StyleLoss, self).__init__()
    # Define the VGG19 network
    self.vgg = torchvision.models.vgg19(pretrained=True).features.eval()
    # Define the layers to extract the features
    self.layers = [0, 5, 10, 19, 28] # conv1_1, conv2_1, conv3_1, conv4_1, conv5_1
  
  def gram_matrix(self, x):
    # Compute the Gram matrix of a feature map x
    b, c, h, w = x.size()
    x = x.view(b, c, h * w)
    g = torch.bmm(x, x.transpose(1, 2))
    g = g / (c * h * w)
    return g
  
  def forward(self, x1, x2):
    # Normalize the images to [0, 1] range
    x1 = (x1 + 1) / 2
    x2 = (x2 + 1) / 2
    # Extract the features from the VGG network
    f1 = []
    f2 = []
    for i in range(max(self.layers) + 1):
      x1 = self.vgg[i](x1)
      x2 = self.vgg[i](x2)
      if i in self.layers:
        f1.append(x1)
        f2.append(x2)
    
    # Compute the Gram matrices of the features
    g1 = [self.gram_matrix(f) for f in f1]
    g2 = [self.gram_matrix(f) for f in f2]

    # Compute the style consistency loss as the mean squared error between the Gram matrices
    L_s = 0
    for i in range(len(self.layers)):
      L_s += torch.nn.functional.mse_loss(g1[i], g2[i])
    
    return L_s

# Define the latent path optimization function O that takes a set of latent codes W and a set of desired ages Y as inputs and outputs a set of optimized latent codes W*
# The latent path optimization function is based on gradient descent with Adam optimizer
def PathOptimization(W, Y, loss_fn, lr=0.01, steps=100):
  # Make a copy of the latent codes W
  W = W.clone().detach().requires_grad_(True)
  # Define the Adam optimizer
  optimizer = torch.optim.Adam([W], lr=lr)
  # Optimize the latent codes for the given number of steps
  for i in range(steps):
    # Zero the gradients
    optimizer.zero_grad()
    # Compute the loss function
    loss = loss_fn(W, Y)
    # Compute the gradients
    loss.backward()
    # Update the latent codes
    optimizer.step()
  # Return the optimized latent codes
  return W

# Train the encoder network E with the following objectives:
# - Minimize the age prediction error: L_a = |y - R(G(E(x, y)))|
# - Minimize the reconstruction error: L_r = |x - G(E(x, y))|
# - Maximize the style consistency: L_s = L_s(x, G(E(x, y)))
# - Maximize the perceptual quality: L_p = PerceptualLoss(x, G(E(x, y)))
# - Maximize the identity preservation: L_i = IdentityLoss(x, G(E(x, y)))

# Define the perceptual loss L_p that measures the similarity between the features of two images
L_p = torchvision.models.vgg16(pretrained=True).features.eval()

# Define the identity loss L_i that measures the similarity between the embeddings of two images
L_i = torchvision.models.resnet50(pretrained=True).eval()

# Define the training data loader that provides batches of images x and desired ages y
data_loader = DataLoader()

# Define the learning rate and the number of epochs for training
lr = 0.0001
epochs = 10

# Define the optimizer for training
optimizer = torch.optim.Adam(E.parameters(), lr=lr)

# Train the encoder network E for the given number of epochs
for epoch in range(epochs):
  # Loop over the batches of images and desired ages
  for x, y in data_loader:
    # Zero the gradients
    optimizer.zero_grad()
    # Encode the images and desired ages into latent codes
    w = E(x, y)
    # Generate aged images from the latent codes
    x' = G(w)
    # Predict the ages of the generated images
    y' = R(x')
    # Compute the age prediction error
    L_a = torch.nn.functional.l1_loss(y, y')
    # Compute the reconstruction error
    L_r = torch.nn.functional.l1_loss(x, x')
    # Compute the style consistency loss
    L_s = StyleLoss()(x, x')
    # Compute the perceptual loss
    L_p = torch.nn.functional.l1_loss(L_p(x), L_p(x'))
    # Compute the identity loss
    L_i = torch.nn.functional.l1_loss(L_i(x), L_i(x'))
    # Compute the total loss as a weighted sum of the individual losses
    loss = L_a + L_r + L_s + L_p + L_i
    # Compute the gradients
    loss.backward()
    # Update the encoder network parameters
    optimizer.step()
  
  # Print the epoch and the loss
  print(f'Epoch {epoch}, Loss {loss.item()}')

# Optimize the latent path W with the following objectives:
# - Minimize the age prediction error: L_a = |Y - R(G(W))|
# - Minimize the reconstruction error: L_r = |X - G(W)|
# - Maximize the perceptual quality: L_p = PerceptualLoss(X, G(W))
# - Maximize the identity preservation: L_i = IdentityLoss(X, G(W))
# - Regularize the path smoothness and diversity: L_pr = PathRegularization(W)

# Define a function that computes a smoothness term for a given path W
def smoothness(W):
  # Compute the pairwise differences between adjacent latent codes in W
  dW = W[1:] - W[:-1]
  # Compute the smoothness term as the mean squared norm of dW
  s = torch.mean(torch.norm(dW, dim=1) ** 2)
  return s

# Define a function that computes a diversity term for a given path W
def diversity(W):
  # Compute the pairwise distances between all latent codes in W
  D = torch.cdist(W, W)
  # Set the diagonal elements to infinity to avoid self-distances
  D.fill_diagonal_(float('inf'))
  # Compute the diversity term as the negative mean of D
  d = -torch.mean(D)
  return