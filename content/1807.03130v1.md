---
title: 1807.03130v1 Unsupervised Natural Image Patch Learning
date: 2018-07-04
---

# [Unsupervised Natural Image Patch Learning](http://arxiv.org/abs/1807.03130v1)

authors: Dov Danon, Hadar Averbuch-Elor, Ohad Fried, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1807.02811 "[1807.02811] A Tutorial on Bayesian Optimization - arXiv.org"
[2]: https://arxiv.org/abs/1807.06521 "[1807.06521] CBAM: Convolutional Block Attention Module - arXiv.org"
[3]: https://info.arxiv.org/help/cs/index.html "Computer Science archive - arXiv info"

Here is a summary of the paper:

- **What**: The paper proposes a novel attention module for convolutional neural networks (CNNs) called Convolutional Block Attention Module (CBAM). CBAM can adaptively refine the intermediate feature maps of CNNs by using both channel and spatial attention mechanisms.
- **Why**: The paper aims to improve the performance of CNNs on various vision tasks, such as image classification and object detection, by enhancing the representation power of feature maps. The paper also claims that CBAM is a simple, lightweight, and general module that can be easily integrated into any CNN architectures without much overhead.
- **How**: The paper introduces CBAM as a sequential combination of two sub-modules: channel attention and spatial attention. The channel attention module learns a weight vector for each channel of the feature map, which reflects the importance of each channel. The spatial attention module learns a 2D mask for each spatial location of the feature map, which reflects the importance of each region. The paper applies CBAM to several popular CNN models, such as ResNet, MobileNet, and DenseNet, and evaluates them on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. The paper shows that CBAM consistently improves the accuracy and mAP scores of the base models across different tasks and datasets.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel attention module for CNNs that can adaptively refine the feature maps along both channel and spatial dimensions.
- It provides a general framework for designing and implementing attention modules that can be easily plugged into any CNN architectures.
- It demonstrates the effectiveness and efficiency of CBAM on various vision tasks and datasets, and shows that it outperforms previous attention methods.

## Method Summary

The method section of the paper describes the details of CBAM and how it is applied to different CNN models. The section consists of three subsections:

- **Convolutional Block Attention Module**: This subsection introduces the general formulation of CBAM, which consists of two sub-modules: channel attention and spatial attention. The channel attention module takes a feature map as input and outputs a channel-wise weight vector that is multiplied to the input feature map. The spatial attention module takes the output of the channel attention module and outputs a 2D spatial mask that is multiplied to the input feature map. The paper also discusses how to implement these sub-modules using different pooling operations and convolution layers.
- **Integration with CNN architectures**: This subsection explains how to integrate CBAM into various CNN architectures, such as ResNet, MobileNet, and DenseNet. The paper proposes to insert CBAM after each convolutional block of the base models, and shows that this simple strategy can improve the performance without changing the overall structure or complexity of the models.
- **Implementation details**: This subsection provides some implementation details of CBAM, such as the hyperparameters, the initialization, and the optimization methods. The paper also reports some ablation studies to analyze the effects of different components and settings of CBAM.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define CBAM module
class CBAM(nn.Module):
  def __init__(self, in_channels, reduction_ratio, kernel_size):
    # Initialize channel attention sub-module
    self.channel_attention = ChannelAttention(in_channels, reduction_ratio)
    # Initialize spatial attention sub-module
    self.spatial_attention = SpatialAttention(kernel_size)

  def forward(self, x):
    # Apply channel attention to input feature map
    x = self.channel_attention(x) * x
    # Apply spatial attention to output of channel attention
    x = self.spatial_attention(x) * x
    # Return refined feature map
    return x

# Define channel attention sub-module
class ChannelAttention(nn.Module):
  def __init__(self, in_channels, reduction_ratio):
    # Initialize MLP with two linear layers and a ReLU activation
    self.mlp = nn.Sequential(
      nn.Linear(in_channels, in_channels // reduction_ratio),
      nn.ReLU(),
      nn.Linear(in_channels // reduction_ratio, in_channels)
    )

  def forward(self, x):
    # Compute max and average pooled feature maps along spatial dimensions
    max_pool = torch.max(x, dim=(2, 3), keepdim=True)[0]
    avg_pool = torch.mean(x, dim=(2, 3), keepdim=True)
    # Flatten pooled feature maps to vectors
    max_pool = max_pool.view(max_pool.size(0), -1)
    avg_pool = avg_pool.view(avg_pool.size(0), -1)
    # Apply MLP to pooled vectors
    max_out = self.mlp(max_pool)
    avg_out = self.mlp(avg_pool)
    # Compute channel attention weights by adding and applying sigmoid
    out = torch.sigmoid(max_out + avg_out)
    # Reshape weights to match input dimensions
    out = out.view(out.size(0), out.size(1), 1, 1)
    # Return channel attention weights
    return out

# Define spatial attention sub-module
class SpatialAttention(nn.Module):
  def __init__(self, kernel_size):
    # Initialize convolution layer with kernel size and padding
    self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)

  def forward(self, x):
    # Compute max and average pooled feature maps along channel dimension
    max_pool = torch.max(x, dim=1, keepdim=True)[0]
    avg_pool = torch.mean(x, dim=1, keepdim=True)
    # Concatenate pooled feature maps along channel dimension
    cat = torch.cat([max_pool, avg_pool], dim=1)
    # Apply convolution layer to concatenated feature map
    out = self.conv(cat)
    # Apply sigmoid to output feature map
    out = torch.sigmoid(out)
    # Return spatial attention mask
    return out

# Define base CNN model (e.g. ResNet)
base_model = ResNet()

# Insert CBAM after each convolutional block of base model
for block in base_model.blocks:
  block.add_module("cbam", CBAM(block.out_channels))

# Train and evaluate the model on vision tasks and datasets
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import PyTorch and other libraries
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Define CBAM module
class CBAM(nn.Module):
  def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):
    # Initialize the base class
    super(CBAM, self).__init__()
    # Initialize channel attention sub-module
    self.channel_attention = ChannelAttention(in_channels, reduction_ratio)
    # Initialize spatial attention sub-module
    self.spatial_attention = SpatialAttention(kernel_size)

  def forward(self, x):
    # Apply channel attention to input feature map
    x = self.channel_attention(x) * x
    # Apply spatial attention to output of channel attention
    x = self.spatial_attention(x) * x
    # Return refined feature map
    return x

# Define channel attention sub-module
class ChannelAttention(nn.Module):
  def __init__(self, in_channels, reduction_ratio):
    # Initialize the base class
    super(ChannelAttention, self).__init__()
    # Initialize MLP with two linear layers and a ReLU activation
    self.mlp = nn.Sequential(
      nn.Linear(in_channels, in_channels // reduction_ratio),
      nn.ReLU(),
      nn.Linear(in_channels // reduction_ratio, in_channels)
    )

  def forward(self, x):
    # Compute max and average pooled feature maps along spatial dimensions
    max_pool = torch.max(x, dim=(2, 3), keepdim=True)[0]
    avg_pool = torch.mean(x, dim=(2, 3), keepdim=True)
    # Flatten pooled feature maps to vectors
    max_pool = max_pool.view(max_pool.size(0), -1)
    avg_pool = avg_pool.view(avg_pool.size(0), -1)
    # Apply MLP to pooled vectors
    max_out = self.mlp(max_pool)
    avg_out = self.mlp(avg_pool)
    # Compute channel attention weights by adding and applying sigmoid
    out = torch.sigmoid(max_out + avg_out)
    # Reshape weights to match input dimensions
    out = out.view(out.size(0), out.size(1), 1, 1)
    # Return channel attention weights
    return out

# Define spatial attention sub-module
class SpatialAttention(nn.Module):
  def __init__(self, kernel_size):
    # Initialize the base class
    super(SpatialAttention, self).__init__()
    # Initialize convolution layer with kernel size and padding
    self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2)

  def forward(self, x):
    # Compute max and average pooled feature maps along channel dimension
    max_pool = torch.max(x, dim=1, keepdim=True)[0]
    avg_pool = torch.mean(x, dim=1, keepdim=True)
    # Concatenate pooled feature maps along channel dimension
    cat = torch.cat([max_pool, avg_pool], dim=1)
    # Apply convolution layer to concatenated feature map
    out = self.conv(cat)
    # Apply sigmoid to output feature map
    out = torch.sigmoid(out)
    # Return spatial attention mask
    return out

# Define hyperparameters and other constants
batch_size = 256
num_epochs = 100
learning_rate = 0.01
momentum = 0.9
weight_decay = 1e-4

# Define data transforms for training and validation sets
train_transform = transforms.Compose([
  transforms.RandomResizedCrop(224),
  transforms.RandomHorizontalFlip(),
  transforms.ToTensor(),
  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

val_transform = transforms.Compose([
  transforms.Resize(256),
  transforms.CenterCrop(224),
  transforms.ToTensor(),
  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# Load ImageNet-1K dataset for training and validation sets
train_set = datasets.ImageFolder(root="data/train", transform=train_transform)
val_set =