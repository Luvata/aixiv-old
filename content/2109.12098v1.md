---
title: 2109.12098v1 CLIPort  What and Where Pathways for Robotic Manipulation
date: 2021-09-13
---

# [CLIPort: What and Where Pathways for Robotic Manipulation](http://arxiv.org/abs/2109.12098v1)

authors: Mohit Shridhar, Lucas Manuelli, Dieter Fox


## What, Why and How

[1]: https://arxiv.org/abs/2109.12098v1 "CLIPort: What and Where Pathways for Robotic Manipulation"
[2]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"
[3]: https://arxiv.org/pdf/2109.12098v1.pdf "arXiv.org e-Print archive"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper presents CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding of CLIP with the spatial precision of Transporter for vision-based manipulation[^1^][1].
- **Why**: The paper aims to address the challenge of how to imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts[^1^][1]. Existing methods either lack generalization to new goals or quickly learn transferable concepts across tasks (end-to-end networks) or lack the spatial understanding necessary for fine-grained manipulation (large-scale internet data)[^1^][1].
- **How**: The paper proposes a two-stream architecture with semantic and spatial pathways for vision-based manipulation. The semantic pathway uses CLIP to encode language instructions and visual observations into a shared embedding space, while the spatial pathway uses Transporter to predict pixel-wise action maps from visual observations. The two pathways are fused by a simple dot product operation to produce a final action map that is executed by a robot arm[^1^][1]. The paper evaluates the approach on a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, both in simulation and in real-world settings. The paper shows that the approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. The paper also shows that the approach can learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- The introduction of CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding of CLIP with the spatial precision of Transporter for vision-based manipulation.
- The demonstration of CLIPort's ability to solve a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, both in simulation and in real-world settings.
- The analysis of CLIPort's data efficiency in few-shot settings and generalization to seen and unseen semantic concepts.
- The comparison of CLIPort with single-task policies and ablations on 10 simulated and 9 real-world tasks.

## Method Summary

Here is a summary of the method section of the paper:

- The paper describes the two-stream architecture of CLIPort, which consists of a semantic pathway and a spatial pathway. The semantic pathway uses CLIP to encode language instructions and visual observations into a shared embedding space, where the cosine similarity between them measures the semantic alignment. The spatial pathway uses Transporter to predict pixel-wise action maps from visual observations, where the action map indicates the probability of picking or placing at each pixel location. The two pathways are fused by a simple dot product operation to produce a final action map that is executed by a robot arm.
- The paper also describes the imitation learning framework for training CLIPort, which uses expert demonstrations to provide supervision. The paper assumes access to a dataset of expert demonstrations, each consisting of a language instruction, a sequence of RGB images, and a sequence of actions. The paper uses the cross-entropy loss to train the spatial pathway and the contrastive loss to train the semantic pathway. The paper also uses data augmentation techniques such as random cropping, color jittering, and random erasing to improve the robustness and generalization of CLIPort.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the semantic pathway using CLIP
semantic_pathway = CLIP()

# Define the spatial pathway using Transporter
spatial_pathway = Transporter()

# Define the action fusion module using dot product
action_fusion = DotProduct()

# Define the imitation learning dataset
dataset = ExpertDemonstrations()

# Define the data augmentation techniques
augmentations = [RandomCrop(), ColorJitter(), RandomErasing()]

# Define the loss functions
spatial_loss = CrossEntropyLoss()
semantic_loss = ContrastiveLoss()

# Define the optimizer
optimizer = Adam()

# Train CLIPort using imitation learning
for epoch in range(num_epochs):
  for instruction, images, actions in dataset:
    # Apply data augmentations to images
    images = augment(images)
    
    # Encode instruction and images using semantic pathway
    instruction_embedding = semantic_pathway.encode_text(instruction)
    image_embeddings = semantic_pathway.encode_images(images)
    
    # Predict action maps using spatial pathway
    action_maps = spatial_pathway.predict_actions(images)
    
    # Fuse action maps and image embeddings using action fusion
    fused_action_maps = action_fusion(action_maps, image_embeddings)
    
    # Compute spatial loss using actions and fused action maps
    spatial_loss_value = spatial_loss(actions, fused_action_maps)
    
    # Compute semantic loss using instruction embedding and image embeddings
    semantic_loss_value = semantic_loss(instruction_embedding, image_embeddings)
    
    # Compute total loss as a weighted sum of spatial and semantic losses
    total_loss_value = alpha * spatial_loss_value + beta * semantic_loss_value
    
    # Update parameters using optimizer
    optimizer.step(total_loss_value)

# Test CLIPort on new tasks
for instruction, images in new_tasks:
  # Encode instruction and images using semantic pathway
  instruction_embedding = semantic_pathway.encode_text(instruction)
  image_embeddings = semantic_pathway.encode_images(images)
  
  # Predict action maps using spatial pathway
  action_maps = spatial_pathway.predict_actions(images)
  
  # Fuse action maps and image embeddings using action fusion
  fused_action_maps = action_fusion(action_maps, image_embeddings)
  
  # Execute the most probable action using robot arm
  robot_arm.execute_action(fused_action_maps.argmax())
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define the semantic pathway using CLIP
class SemanticPathway(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Load the CLIP model and tokenizer
    self.model, self.tokenizer = clip.load("ViT-B/32")
  
  def encode_text(self, instruction):
    # Tokenize the instruction and convert to tensor
    tokens = self.tokenizer(instruction, return_tensors="pt")
    # Encode the instruction using CLIP text encoder
    instruction_embedding = self.model.encode_text(tokens)
    return instruction_embedding
  
  def encode_images(self, images):
    # Normalize the images using CLIP mean and std
    normalize = torchvision.transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
    images = normalize(images)
    # Encode the images using CLIP image encoder
    image_embeddings = self.model.encode_image(images)
    return image_embeddings

# Define the spatial pathway using Transporter
class SpatialPathway(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the convolutional layers for keypoint detection and action prediction
    self.keypoint_conv = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1)
    self.action_conv = torch.nn.Conv2d(in_channels=6, out_channels=2, kernel_size=1)
  
  def detect_keypoints(self, images):
    # Apply a convolutional layer to images to get keypoint heatmaps
    keypoint_heatmaps = self.keypoint_conv(images)
    # Apply a softmax function to keypoint heatmaps to get keypoint probabilities
    keypoint_probs = torch.nn.functional.softmax(keypoint_heatmaps, dim=-1)
    return keypoint_probs
  
  def predict_actions(self, images):
    # Detect keypoints from images
    keypoint_probs = self.detect_keypoints(images)
    # Compute the expected keypoint locations from keypoint probabilities
    keypoint_locs = torch.nn.functional.soft_argmax(keypoint_probs, dim=(-2,-1))
    # Compute the keypoint features by bilinear sampling from images
    keypoint_feats = torch.nn.functional.grid_sample(images, keypoint_locs.unsqueeze(1))
    # Concatenate the images and keypoint features along channel dimension
    image_keypoint_feats = torch.cat([images, keypoint_feats], dim=1)
    # Apply a convolutional layer to image_keypoint_feats to get action maps
    action_maps = self.action_conv(image_keypoint_feats)
    return action_maps

# Define the action fusion module using dot product
class ActionFusion(torch.nn.Module):
  def __init__(self):
    super().__init__()
  
  def forward(self, action_maps, image_embeddings):
    # Reshape the action maps and image embeddings to have compatible dimensions
    action_maps = action_maps.view(action_maps.shape[0], action_maps.shape[1], -1)
    image_embeddings = image_embeddings.unsqueeze(1).repeat(1, action_maps.shape[1], 1)
    
    # Compute the dot product between action maps and image embeddings
    fused_action_maps = torch.bmm(action_maps.transpose(1,2), image_embeddings).transpose(1,2)
    
    # Reshape the fused action maps to have the same spatial dimensions as action maps
    fused_action_maps = fused_action_maps.view(fused_action_maps.shape[0], fused_action_maps.shape[1], int(np.sqrt(fused_action_maps.shape[2])), int(np.sqrt(fused_action_maps.shape[2])))
    
    return fused_action_maps

# Define the imitation learning dataset
class ExpertDemonstrations(torch.utils.data.Dataset):
  def __init__(self, data_dir):
    super().__init__()
    
    # Load the expert demonstrations from data_dir
    self.data = load_data(data_dir)
  
  def __len__(self):
    return len(self.data)
  
  def __getitem__(self, index):
    
    # Get the instruction, images, and actions for the given index
    instruction = self.data[index]["instruction"]
    
     # Convert the images to tensors and resize them to 224x224 pixels
     transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Resize((224,224))])
     images = torch.stack([transform(image) for image in self.data[index]["images"]])
     
     # Convert the actions to tensors and one-hot encode them
     actions = torch.nn.functional.one_hot(torch.tensor(self.data[index]["actions"]), num_classes=2)
     
     return instruction, images, actions

# Define the data augmentation techniques
augmentations = torchvision.transforms.Compose([
  # Randomly crop the images to 200x200 pixels
  torchvision.transforms.RandomCrop((200,200)),
  # Randomly apply color jittering with given parameters
  torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
  # Randomly erase a rectangular region of the images with given parameters
  torchvision.transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0)
])

# Define the loss functions
spatial_loss = torch.nn.CrossEntropyLoss()
semantic_loss = torch.nn.CosineEmbeddingLoss()

# Define the optimizer
optimizer = torch.optim.Adam()

# Instantiate the semantic pathway, spatial pathway, and action fusion module
semantic_pathway = SemanticPathway()
spatial_pathway = SpatialPathway()
action_fusion = ActionFusion()

# Instantiate the imitation learning dataset
dataset = ExpertDemonstrations(data_dir)

# Define the number of epochs and the weight parameters for the loss functions
num_epochs = 10
alpha = 1.0
beta = 0.1

# Train CLIPort using imitation learning
for epoch in range(num_epochs):
  
  # Shuffle the dataset and create batches of size 32
  dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
  
  for instruction, images, actions in dataloader:
    
    # Apply data augmentations to images
    images = augment(images)
    
    # Encode instruction and images using semantic pathway
    instruction_embedding = semantic_pathway.encode_text(instruction)
    image_embeddings = semantic_pathway.encode_images(images)
    
    # Predict action maps using spatial pathway
    action_maps = spatial_pathway.predict_actions(images)
    
    # Fuse action maps and image embeddings using action fusion
    fused_action_maps = action_fusion(action_maps, image_embeddings)
    
    # Compute spatial loss using actions and fused action maps
    spatial_loss_value = spatial_loss(fused_action_maps.view(-1,2), actions.view(-1))
    
    # Compute semantic loss using instruction embedding and image embeddings
    semantic_loss_value = semantic_loss(image_embeddings, instruction_embedding.repeat(image_embeddings.shape[0],1), torch.ones(image_embeddings.shape[0]))
    
    # Compute total loss as a weighted sum of spatial and semantic losses
    total_loss_value = alpha * spatial_loss_value + beta * semantic_loss_value
    
    # Zero out the gradients of the optimizer
    optimizer.zero_grad()
    
    # Backpropagate the total loss and update parameters using optimizer
    total_loss_value.backward()
    optimizer.step()

# Test CLIPort on new tasks
for instruction, images in new_tasks:
  
  # Encode instruction and images using semantic pathway
  instruction_embedding = semantic_pathway.encode_text(instruction)
  image_embeddings = semantic_pathway.encode_images(images)
  
  # Predict action maps using spatial pathway
  action_maps = spatial_pathway.predict_actions(images)
  
  # Fuse action maps and image embeddings using action fusion
  fused_action_maps = action_fusion(action_maps, image_embeddings)
  
  # Execute the most probable action using robot arm
  robot_arm.execute_action(fused_action_maps.argmax())
```