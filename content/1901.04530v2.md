---
title: 1901.04530v2 CrossNet  Latent Cross-Consistency for Unpaired Image Translation
date: 2019-01-05
---

# [CrossNet: Latent Cross-Consistency for Unpaired Image Translation](http://arxiv.org/abs/1901.04530v2)

authors: Omry Sendik, Dani Lischinski, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/1901.04530v2.pdf "arXiv:1901.04530v2 [cs.LG] 26 May 2019"
[2]: https://arxiv.org/abs/1901.04530v2 "CrossNet: Latent Cross-Consistency for Unpaired Image Translation"
[3]: http://export.arxiv.org/abs/1909.04530v2 "[1909.04530v2] Non-linear diffusion of cosmic rays escaping from ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel architecture for unpaired image translation, called CrossNet, which consists of a pair of GANs and a pair of translators between their latent spaces.
- **Why**: The paper aims to overcome the under-constrained nature of the unpaired image translation problem, which requires learning an image translation operator from two image sets without explicit pairing between them.
- **How**: The paper introduces several regularizers enabled by the cross-translators, collectively referred to as latent cross-consistency, which enforce cycle-consistency, identity preservation, and latent distribution matching in the latent spaces of the GANs. The paper evaluates the proposed architecture and regularizers on a variety of image translation tasks and shows that they outperform the existing state-of-the-art methods.

## Main Contributions

The contributions of this paper are:

- A novel architecture for unpaired image translation, which comprises a pair of GANs and a pair of cross-translators between their latent spaces.
- A set of latent cross-consistency constraints, which regularize the unpaired image translation problem by enforcing cycle-consistency, identity preservation, and latent distribution matching in the latent spaces of the GANs.
- An extensive experimental evaluation of the proposed architecture and regularizers on a variety of image translation tasks, demonstrating superior performance over the existing state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/pdf/1901.04530v2.pdf "arXiv:1901.04530v2 [cs.LG] 26 May 2019"
[2]: https://arxiv.org/abs/1901.04530v2 "CrossNet: Latent Cross-Consistency for Unpaired Image Translation"
[3]: https://www.researchgate.net/publication/330409795_CrossNet_Latent_Cross-Consistency_for_Unpaired_Image_Translation "(PDF) CrossNet: Latent Cross-Consistency for Unpaired ... - ResearchGate"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper presents a novel architecture for unpaired image translation, which consists of a pair of GANs, **G_A** and **G_B**, and a pair of cross-translators, **T_AB** and **T_BA**, between their latent spaces.
- The cross-translators are trained to map the latent codes of one GAN to the latent codes of the other GAN, such that applying the cross-translator and then the corresponding GAN produces an image that is similar to the original image.
- The paper introduces several latent cross-consistency constraints, which are added to the objective function of the GANs and the cross-translators. These constraints are:
    - **Cycle-consistency**: The latent codes of an image should be preserved after applying a cross-translator and then its inverse cross-translator.
    - **Identity preservation**: The latent codes of an image should be preserved after applying a cross-translator and then the same GAN.
    - **Latent distribution matching**: The latent distributions of the two GANs should be similar after applying the cross-translators.
- The paper also introduces a novel discriminator architecture, called **D_AB**, which discriminates between real images from domain A and fake images generated by **G_A** from latent codes produced by **T_BA**. This discriminator helps to align the latent distributions of the two GANs and improve the quality of the generated images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the GANs G_A and G_B with their generators and discriminators
G_A = GAN(generator_A, discriminator_A)
G_B = GAN(generator_B, discriminator_B)

# Define the cross-translators T_AB and T_BA
T_AB = Translator(latent_size_A, latent_size_B)
T_BA = Translator(latent_size_B, latent_size_A)

# Define the cross-discriminator D_AB
D_AB = Discriminator(image_size_A)

# Define the loss functions for the GANs, the cross-translators and the cross-discriminator
L_GAN = ...
L_T = ...
L_D_AB = ...

# Define the hyperparameters for the training
epochs = ...
batch_size = ...
learning_rate = ...
lambda_1 = ... # weight for cycle-consistency loss
lambda_2 = ... # weight for identity preservation loss
lambda_3 = ... # weight for latent distribution matching loss

# Load the unpaired image sets A and B
A = load_images(domain_A)
B = load_images(domain_B)

# Train the model
for epoch in range(epochs):
  # Shuffle the image sets
  A.shuffle()
  B.shuffle()
  
  # Iterate over batches of images
  for i in range(0, min(len(A), len(B)), batch_size):
    # Get a batch of images from each set
    a_batch = A[i:i+batch_size]
    b_batch = B[i:i+batch_size]
    
    # Encode the images to latent codes using the GANs
    z_a_batch = G_A.encode(a_batch)
    z_b_batch = G_B.encode(b_batch)
    
    # Translate the latent codes using the cross-translators
    z_ab_batch = T_AB(z_a_batch)
    z_ba_batch = T_BA(z_b_batch)
    
    # Decode the latent codes to images using the GANs
    a_hat_batch = G_A.decode(z_ab_batch)
    b_hat_batch = G_B.decode(z_ba_batch)
    
    # Compute the losses for the GANs, the cross-translators and the cross-discriminator
    L_GAN_A = L_GAN(G_A, a_batch, b_hat_batch)
    L_GAN_B = L_GAN(G_B, b_batch, a_hat_batch)
    L_T_AB = L_T(T_AB, z_a_batch, z_ba_batch)
    L_T_BA = L_T(T_BA, z_b_batch, z_ab_batch)
    L_D_AB_A = L_D_AB(D_AB, a_batch, a_hat_batch)
    
    # Update the parameters of the GANs, the cross-translators and the cross-discriminator using gradient descent
    update_params(G_A, learning_rate * (L_GAN_A + lambda_1 * L_T_AB + lambda_2 * L_T_BA + lambda_3 * L_D_AB_A))
    update_params(G_B, learning_rate * (L_GAN_B + lambda_1 * L_T_BA + lambda_2 * L_T_AB + lambda_3 * L_D_AB_A))
    update_params(T_AB, learning_rate * (lambda_1 * L_T_AB + lambda_2 * L_T_BA))
    update_params(T_BA, learning_rate * (lambda_1 * L_T_BA + lambda_2 * L_T_AB))
    update_params(D_AB, learning_rate * L_D_AB_A)
    
  # Save the model and generate some samples at the end of each epoch
  save_model(G_A, G_B, T_AB, T_BA, D_AB)
  generate_samples(G_A, G_B, T_AB, T_BA)  
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the GAN class with its generator and discriminator
class GAN:
  def __init__(self, generator, discriminator):
    # Initialize the generator and discriminator networks
    self.generator = generator
    self.discriminator = discriminator
    
    # Initialize the optimizers for the generator and discriminator
    self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # Initialize the loss function for the GANs
    self.gan_loss = torch.nn.BCEWithLogitsLoss()
  
  def encode(self, x):
    # Encode an image to a latent code using the generator network
    return self.generator.encode(x)
  
  def decode(self, z):
    # Decode a latent code to an image using the generator network
    return self.generator.decode(z)
  
  def discriminate(self, x):
    # Discriminate between real and fake images using the discriminator network
    return self.discriminator(x)
  
  def generate(self, z):
    # Generate an image from a latent code using the generator network
    return self.generator(z)
  
  def update_generator(self, loss):
    # Update the parameters of the generator network using gradient descent
    self.generator_optimizer.zero_grad()
    loss.backward()
    self.generator_optimizer.step()
  
  def update_discriminator(self, loss):
    # Update the parameters of the discriminator network using gradient descent
    self.discriminator_optimizer.zero_grad()
    loss.backward()
    self.discriminator_optimizer.step()

# Define the Translator class with its encoder and decoder
class Translator:
  def __init__(self, input_size, output_size):
    # Initialize the encoder and decoder networks
    self.encoder = torch.nn.Linear(input_size, output_size)
    self.decoder = torch.nn.Linear(output_size, input_size)
    
    # Initialize the optimizer for the translator
    self.optimizer = torch.optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=0.0002, betas=(0.5, 0.999))
    
    # Initialize the loss function for the translator
    self.translation_loss = torch.nn.MSELoss()
  
  def translate(self, z):
    # Translate a latent code from one domain to another using the encoder network
    return self.encoder(z)
  
  def inverse_translate(self, z):
    # Translate a latent code from one domain to another using the decoder network
    return self.decoder(z)
  
  def update_translator(self, loss):
    # Update the parameters of the translator network using gradient descent
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

# Define the Discriminator class with its convolutional layers
class Discriminator:
  def __init__(self, image_size):
    # Initialize the convolutional layers of the discriminator network
    self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)
    self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
    self.conv3 = torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)
    self.conv4 = torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)
    
    # Initialize the final layer of the discriminator network
    self.fc = torch.nn.Linear(512 * (image_size // 16) * (image_size // 16), 1)
    
    # Initialize the optimizer for the discriminator
    self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # Initialize the loss function for the discriminator
    self.gan_loss = torch.nn.BCEWithLogitsLoss()
  
  def forward(self, x):
    # Apply a series of convolutional layers with leaky ReLU activation and batch normalization
    x = torch.nn.LeakyReLU(0.2)(self.conv1(x))
    x = torch.nn.BatchNorm2d(128)(torch.nn.LeakyReLU(0.2)(self.conv2(x)))
    x = torch.nn.BatchNorm2d(256)(torch.nn.LeakyReLU(0.2)(self.conv3(x)))
    x = torch.nn.BatchNorm2d(512)(torch.nn.LeakyReLU(0.2)(self.conv4(x)))
    
    # Flatten the output and apply the final layer
    x = x.view(x.size(0), -1)
    x = self.fc(x)
    
    # Return the output
    return x
  
  def update_discriminator(self, loss):
    # Update the parameters of the discriminator network using gradient descent
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

# Define the Generator class with its encoder and decoder
class Generator:
  def __init__(self, latent_size):
    # Initialize the encoder network with its convolutional layers
    self.encoder_conv1 = torch.nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)
    self.encoder_conv2 = torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
    self.encoder_conv3 = torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)
    self.encoder_conv4 = torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)
    
    # Initialize the encoder network with its final layer
    self.encoder_fc = torch.nn.Linear(512 * 8 * 8, latent_size)
    
    # Initialize the decoder network with its first layer
    self.decoder_fc = torch.nn.Linear(latent_size, 512 * 8 * 8)
    
    # Initialize the decoder network with its deconvolutional layers
    self.decoder_deconv1 = torch.nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)
    self.decoder_deconv2 = torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)
    self.decoder_deconv3 = torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)
    self.decoder_deconv4 = torch.nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)
  
  def encode(self, x):
    # Apply a series of convolutional layers with leaky ReLU activation and batch normalization
    x = torch.nn.LeakyReLU(0.2)(self.encoder_conv1(x))
    x = torch.nn.BatchNorm2d(128)(torch.nn.LeakyReLU(0.2)(self.encoder_conv2(x)))
    x = torch.nn.BatchNorm2d(256)(torch.nn.LeakyReLU(0.2)(self.encoder_conv3(x)))
    x = torch.nn.BatchNorm2d(512)(torch.nn.LeakyReLU(0.2)(self.encoder_conv4(x)))
    
    # Flatten the output and apply the final layer
    x = x.view(x.size(0), -1)
    x = self.encoder_fc(x)
    
    # Return the latent code
    return x
  
  def decode(self, z):
    # Apply the first layer and reshape the output
    z = self.decoder_fc(z)
    z = z.view(z.size(0), 512, 8, 8)
    
    # Apply a series of deconvolutional layers with ReLU activation and batch normalization
    z = torch.nn.ReLU()(torch.nn.BatchNorm2d(256)(self.decoder_deconv1(z)))
    z = torch.nn.ReLU()(torch.nn.BatchNorm2d(128)(self.decoder_deconv2(z)))
    z = torch.nn.ReLU()(torch.nn.BatchNorm2d(64)(self.decoder_deconv3(z)))
    
    # Apply the final deconvolutional layer with tanh activation
    z = torch.tanh(self.decoder_deconv4(z))
    
    # Return the generated image
    return z
  
  def forward(self, z):
    # Encode and decode a latent code using the generator network
    return self.decode(self.encode(z))

# Define the loss function for the GANs
def L_GAN(GAN_A, GAN_B, a_batch, b_batch):
  # Generate fake images from real images using the GANs
  a_fake_batch = GAN_A.generate(GAN_B.encode(b_batch))
  b_fake_batch = GAN_B.generate(GAN_A.encode(a_batch))
  
  # Compute the discriminator outputs for real and fake images
  d_a_real_output = GAN_A.discriminate(a_batch)
  d_a_fake_output = GAN_A.discriminate(a_fake_batch)
  d_b_real_output = GAN_B.discriminate(b_batch)
  d_b_fake_output = GAN_B.discriminate(b_fake_batch)
  
  # Compute the generator losses for both domains
  g_a_loss = GAN_A.gan_loss(d_a_fake_output, torch