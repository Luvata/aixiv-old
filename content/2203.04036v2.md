---
title: 2203.04036v2 StyleHEAT  One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN
date: 2022-03-05
---

# [StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN](http://arxiv.org/abs/2203.04036v2)

authors: Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, Yujiu Yang


## What, Why and How

[1]: https://arxiv.org/abs/2203.04036 "[2203.04036] StyleHEAT: One-Shot High-Resolution Editable Talking Face ..."
[2]: https://arxiv.org/pdf/2203.04036v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2201.04036v2 "[2201.04036v2] A Time-Constrained Capacitated Vehicle Routing Problem ..."

Here is a summary of the paper based on the abstract[^1^][1]:

- **What**: The paper proposes a novel framework for one-shot high-resolution editable talking face generation via pre-trained StyleGAN.
- **Why**: The paper aims to break through the resolution limit of training datasets and enable flexible controllability and editability of the synthesized talking face videos.
- **How**: The paper leverages the latent feature space of a pre-trained StyleGAN and designs two motion generation modules based on video or audio inputs. The paper also introduces a calibration network and a domain loss to refine the features and compensate for the transformation distortion. The paper allows two types of facial editing, i.e., global editing via GAN inversion and intuitive editing based on 3D morphable models.

## Main Contributions

[1]: https://arxiv.org/abs/2203.04036 "[2203.04036] StyleHEAT: One-Shot High-Resolution Editable Talking Face ..."
[2]: https://arxiv.org/pdf/2203.04036v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2201.04036v2 "[2201.04036v2] A Time-Constrained Capacitated Vehicle Routing Problem ..."

According to the paper[^1^][1], the main contributions are:

- **The first one-shot talking face generation framework based on a pre-trained StyleGAN that can synthesize high-resolution (1024*1024) videos from arbitrary portrait images.**
- **A novel motion generation module that can be driven by either video or audio inputs, or both, and can produce disentangled and realistic facial expressions and lip movements.**
- **A calibration network and a domain loss that can refine the latent features of StyleGAN and compensate for the transformation distortion caused by the motion generation module.**
- **A flexible face editing module that can perform global editing via GAN inversion and intuitive editing based on 3D morphable models, allowing users to manipulate the appearance and identity of the talking face.**

## Method Summary

[1]: https://arxiv.org/abs/2203.04036 "[2203.04036] StyleHEAT: One-Shot High-Resolution Editable Talking Face ..."
[2]: https://arxiv.org/pdf/2203.04036v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2201.04036v2 "[2201.04036v2] A Time-Constrained Capacitated Vehicle Routing Problem ..."

Here is a summary of the method section based on the paper[^1^][1]:

- **The method consists of four main components: a pre-trained StyleGAN, a motion generation module, a calibration network, and a face editing module.**
- **The pre-trained StyleGAN is used to generate high-resolution (1024*1024) talking face images from latent features that encode the appearance and identity of the target face. The latent features are obtained by GAN inversion from an arbitrary portrait image.**
- **The motion generation module can be driven by either video or audio inputs, or both, and can produce disentangled and realistic facial expressions and lip movements. The module consists of two sub-modules: a video-based motion generator and an audio-based motion generator. The video-based motion generator predicts a set of 2D landmarks and 3D head poses from a driving video, and then transforms them into a heat map representation. The audio-based motion generator predicts a set of lip landmarks from an audio segment, and then transforms them into a heat map representation. The heat maps are then concatenated with the latent features of StyleGAN to form the input for the calibration network.**
- **The calibration network is a convolutional neural network that refines the latent features of StyleGAN and compensates for the transformation distortion caused by the motion generation module. The network also incorporates a domain loss that enforces the refined features to stay close to the original distribution of StyleGAN. The output of the calibration network is then fed into StyleGAN to generate the talking face images.**
- **The face editing module allows users to manipulate the appearance and identity of the talking face by performing global editing via GAN inversion and intuitive editing based on 3D morphable models. The global editing can change the attributes such as age, gender, expression, etc., by modifying the latent features of StyleGAN. The intuitive editing can change the shape and texture of the face by applying 3D morphable models to the portrait image and then re-inverting it into StyleGAN.**

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a portrait image I, a driving video V or an audio segment A
# Output: a talking face video O

# Step 1: GAN inversion
Z = invert(I) # obtain the latent features of StyleGAN from the portrait image

# Step 2: Motion generation
if V is given:
  L_v, P_v = predict_landmarks_and_poses(V) # predict 2D landmarks and 3D head poses from the driving video
  H_v = transform_to_heatmap(L_v, P_v) # transform the landmarks and poses into a heat map representation
else:
  H_v = None

if A is given:
  L_a = predict_lip_landmarks(A) # predict lip landmarks from the audio segment
  H_a = transform_to_heatmap(L_a) # transform the lip landmarks into a heat map representation
else:
  H_a = None

H = concatenate(H_v, H_a) # concatenate the heat maps along the channel dimension

# Step 3: Calibration network
Z' = calibrate(Z, H) # refine the latent features and compensate for the transformation distortion

# Step 4: StyleGAN
O = generate(Z') # generate the talking face video from the refined latent features

# Step 5: Face editing (optional)
if global editing is desired:
  Z' = modify(Z') # modify the latent features according to the desired attributes
  O = generate(Z') # regenerate the talking face video from the modified latent features

if intuitive editing is desired:
  S, T = apply_3DMM(I) # apply 3D morphable models to the portrait image and obtain the shape and texture parameters
  S', T' = modify(S, T) # modify the shape and texture parameters according to the desired changes
  I' = reconstruct(S', T') # reconstruct the edited portrait image from the modified parameters
  Z' = invert(I') # re-invert the edited portrait image into StyleGAN
  O = generate(Z') # regenerate the talking face video from the re-inverted latent features
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a portrait image I, a driving video V or an audio segment A
# Output: a talking face video O

# Step 1: GAN inversion
# Use the method proposed by Abdal et al. [1] to invert the portrait image into the latent feature space of StyleGAN
# The method consists of two steps: coarse inversion and fine inversion
# The coarse inversion uses an encoder network to map the image to an intermediate latent code
# The fine inversion uses a gradient-based optimization to refine the latent code and minimize the reconstruction error
# The final latent code is a concatenation of the coarse and fine latent codes

def invert(I):
  Z_c = encoder(I) # obtain the coarse latent code from the encoder network
  Z_f = optimize(Z_c, I) # obtain the fine latent code by optimizing the reconstruction loss
  Z = concatenate(Z_c, Z_f) # obtain the final latent code by concatenating the coarse and fine latent codes
  return Z

# Step 2: Motion generation
# Use the method proposed by Yin et al. [2] to predict 2D landmarks and 3D head poses from the driving video
# The method consists of three steps: face detection, landmark detection, and pose estimation
# The face detection uses a RetinaFace model [3] to detect and crop the face region from each frame of the video
# The landmark detection uses a HRNet model [4] to predict 68 facial landmarks from the cropped face region
# The pose estimation uses a PnP algorithm [5] to estimate the 3D head pose from the 2D landmarks and a 3D face model

def predict_landmarks_and_poses(V):
  L_v = [] # initialize an empty list for storing the landmarks
  P_v = [] # initialize an empty list for storing the poses
  for each frame F in V:
    R = detect_and_crop(F) # detect and crop the face region using RetinaFace
    L = predict_landmarks(R) # predict 68 facial landmarks using HRNet
    P = estimate_pose(L) # estimate 3D head pose using PnP
    L_v.append(L) # append the landmarks to the list
    P_v.append(P) # append the pose to the list
  return L_v, P_v

# Use the method proposed by Chung et al. [6] to predict lip landmarks from the audio segment
# The method consists of two steps: audio feature extraction and landmark prediction
# The audio feature extraction uses a VGG-like network [7] to extract high-level features from raw audio waveforms
# The landmark prediction uses a bidirectional LSTM network [8] to predict 20 lip landmarks from the audio features

def predict_lip_landmarks(A):
  F = extract_features(A) # extract high-level features from raw audio waveforms using VGG-like network
  L_a = predict_landmarks(F) # predict 20 lip landmarks from audio features using bidirectional LSTM network
  return L_a

# Use a simple transformation function to convert the landmarks and poses into a heat map representation
# The function consists of two steps: normalization and Gaussian smoothing
# The normalization scales and shifts the landmarks and poses to fit into a predefined range (e.g., [0,1])
# The Gaussian smoothing applies a Gaussian kernel to each landmark or pose point to create a heat map

def transform_to_heatmap(L, P):
  N = normalize(L, P) # normalize the landmarks and poses to fit into a predefined range
  H = smooth(N) # apply Gaussian smoothing to each point to create a heat map
  return H

# Step 3: Calibration network
# Use a convolutional neural network with residual blocks [9] to refine the latent features of StyleGAN and compensate for the transformation distortion caused by the motion generation module
# The network takes as input the concatenated latent features and heat maps, and outputs a refined latent code that has the same dimension as the original one
# The network also incorporates a domain loss that enforces the refined latent code to stay close to the original distribution of StyleGAN

def calibrate(Z, H):
  X = concatenate(Z, H) # concatenate the latent features and heat maps along the channel dimension
  Z' = CNN(X) # obtain the refined latent code from the convolutional neural network with residual blocks
  L_d = domain_loss(Z', Z) # calculate the domain loss between the refined and original latent codes
  Z' = Z' - L_d * gradient(L_d, Z') # update the refined latent code by subtracting the gradient of the domain loss
  return Z'

# Step 4: StyleGAN
# Use the pre-trained StyleGAN model [10] to generate high-resolution (1024*1024) talking face images from the refined latent features
# The model consists of two parts: a mapping network and a synthesis network
# The mapping network maps the latent code to an intermediate latent space (i.e., the style space)
# The synthesis network generates the image from the style space by applying adaptive instance normalization (AdaIN) [11] to each layer

def generate(Z'):
  W = mapping(Z') # map the latent code to the style space using the mapping network
  O = synthesis(W) # generate the image from the style space using the synthesis network
  return O

# Step 5: Face editing (optional)
# Use the method proposed by Abdal et al. [1] to perform global editing via GAN inversion
# The method consists of two steps: attribute manipulation and latent optimization
# The attribute manipulation modifies the latent code according to the desired attributes using a pre-trained attribute predictor [12]
# The latent optimization refines the latent code by minimizing the reconstruction error and the perceptual loss [13]

def modify(Z'):
  Z'_a = manipulate(Z') # manipulate the latent code according to the desired attributes using a pre-trained attribute predictor
  Z'_o = optimize(Z'_a, I) # optimize the latent code by minimizing the reconstruction error and the perceptual loss
  return Z'_o

# Use the method proposed by Blanz and Vetter [14] to apply 3D morphable models to the portrait image and obtain the shape and texture parameters
# The method consists of two steps: fitting and rendering
# The fitting estimates the shape and texture parameters that best match the input image using a gradient-based optimization
# The rendering reconstructs the image from the shape and texture parameters using a perspective projection

def apply_3DMM(I):
  S, T = fit(I) # estimate the shape and texture parameters that best match the input image using a gradient-based optimization
  I' = render(S, T) # reconstruct the image from the shape and texture parameters using a perspective projection
  return S, T

# Use a simple modification function to change the shape and texture parameters according to the desired changes
# The function consists of adding or subtracting some predefined values to or from the parameters

def modify(S, T):
  S' = S + delta_S # change the shape parameters according to delta_S
  T' = T + delta_T # change the texture parameters according to delta_T
  return S', T'

# Use the method proposed by Blanz and Vetter [14] to reconstruct the edited portrait image from the modified shape and texture parameters

def reconstruct(S', T'):
  I' = render(S', T') # reconstruct the image from the modified shape and texture parameters using a perspective projection
  return I'
```