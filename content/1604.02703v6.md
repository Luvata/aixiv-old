---
title: 1604.02703v6 Synthesizing Training Images for Boosting Human 3D Pose Estimation
date: 2016-04-03
---

# [Synthesizing Training Images for Boosting Human 3D Pose Estimation](http://arxiv.org/abs/1604.02703v6)

authors: Wenzheng Chen, Huan Wang, Yangyan Li, Hao Su, Zhenhua Wang, Changhe Tu, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen


## What, Why and How

[1]: https://arxiv.org/pdf/1604.02703v6.pdf "Synthesizing Training Images for Boosting Human 3D Pose Estimation"
[2]: https://arxiv.org/abs/1604.02703v6 "[1604.02703v6] Synthesizing Training Images for Boosting Human 3D Pose ..."
[3]: https://academia.stackexchange.com/questions/55626/is-there-a-more-user-friendly-way-to-download-multiple-articles-from-arxiv "Is there a more user-friendly way to download multiple articles from arXiv?"

Here is a summary of the paper:

- **What**: The paper presents a method for synthesizing training images for boosting human 3D pose estimation from a single image using CNNs.
- **Why**: The paper addresses the issue of the lack of suitable annotated training images for direct estimation of 3D poses, which hinders the performance of end-to-end solutions.
- **How**: The paper proposes a fully automatic, scalable approach that samples the human pose space for guiding the synthesis procedure and extracts clothing textures from real images. The paper also explores domain adaptation for bridging the gap between synthetic and real images. The paper demonstrates that CNNs trained with synthetic images outperform those trained with real photos on 3D pose estimation tasks.

The summary is based on the abstract and introduction of the paper[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/1604.02703v6.pdf "Synthesizing Training Images for Boosting Human 3D Pose ... - arXiv.org"
[2]: https://arxiv.org/abs/1604.02703v6 "[1604.02703v6] Synthesizing Training Images for Boosting Human 3D Pose ..."
[3]: http://export.arxiv.org/abs/1604.01631 "[1604.01631] Analysis of the intermediate-state contributions to ..."

According to the paper[^1^][1], the main contributions are:

- A systematic study of the effectiveness of synthetic training data for human 3D pose estimation from a single image.
- A fully automatic, scalable approach for synthesizing training images with ground truth pose annotations, which covers the human pose space and extracts clothing textures from real images.
- An exploration of domain adaptation techniques for bridging the gap between synthetic and real images.
- An empirical evaluation of CNNs trained with synthetic images on 3D pose estimation tasks, showing that they outperform those trained with real photos.

## Method Summary

[1]: https://arxiv.org/pdf/1604.02703v6.pdf "Synthesizing Training Images for Boosting Human 3D Pose ... - arXiv.org"
[2]: https://arxiv.org/abs/1604.02703v6 "[1604.02703v6] Synthesizing Training Images for Boosting Human 3D Pose ..."
[3]: https://www.kaggle.com/datasets/Cornell-University/arxiv "arXiv Dataset | Kaggle"

Here is a summary of the method section of the paper:

- The paper describes a pipeline for generating synthetic training images with ground truth pose annotations, which consists of four steps: pose sampling, human model deformation, texture synthesis, and image composition[^1^][1].
- The paper uses a pose space sampling algorithm to generate diverse and realistic poses for human models, based on a set of predefined key poses and a Gaussian Mixture Model (GMM) for interpolation[^1^][1].
- The paper uses the SCAPE model [7] to deform human models according to the sampled poses, and applies a skinning algorithm to preserve the volume and smoothness of the models[^1^][1].
- The paper presents an automatic method for transferring clothing textures from real product images onto human models, using a texture atlas and a patch-based synthesis algorithm[^1^][1].
- The paper renders the deformed and textured human models using various viewpoints and light sources, and composites them over real image backgrounds using Poisson blending [44][^1^][1].
- The paper also applies domain adaptation techniques to reduce the domain gap between synthetic and real images, such as color jittering, random cropping, and adversarial training [25][^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a set of key poses and a set of product images
# Output: a set of synthetic training images with ground truth pose annotations

# Step 1: Pose sampling
poses = [] # an empty list of poses
for i in range(N): # N is the number of synthetic images to generate
  pose = sample_pose(key_poses, GMM) # sample a pose from the key poses and the GMM
  poses.append(pose) # add the pose to the list

# Step 2: Human model deformation
models = [] # an empty list of models
for pose in poses:
  model = deform_model(SCAPE, pose) # deform a SCAPE model according to the pose
  model = skin_model(model) # apply skinning to the model
  models.append(model) # add the model to the list

# Step 3: Texture synthesis
textures = [] # an empty list of textures
for product_image in product_images:
  texture = extract_texture(product_image) # extract a clothing texture from the product image
  textures.append(texture) # add the texture to the list

for model in models:
  texture = random_choice(textures) # randomly choose a texture from the list
  model = apply_texture(model, texture) # apply the texture to the model

# Step 4: Image composition
images = [] # an empty list of images
for model in models:
  viewpoint = random_viewpoint() # randomly choose a viewpoint
  light_source = random_light_source() # randomly choose a light source
  image = render_model(model, viewpoint, light_source) # render the model with the viewpoint and light source
  background = random_background() # randomly choose a background image
  image = composite_image(image, background) # composite the image over the background
  image = domain_adaptation(image) # apply domain adaptation techniques to the image
  images.append(image) # add the image to the list

return images
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a set of key poses and a set of product images
# Output: a set of synthetic training images with ground truth pose annotations

# Step 1: Pose sampling
poses = [] # an empty list of poses
GMM = fit_GMM(key_poses) # fit a GMM to the key poses using EM algorithm
for i in range(N): # N is the number of synthetic images to generate
  k = sample_component(GMM) # sample a component index from the GMM
  mean = GMM.means[k] # get the mean pose of the component
  cov = GMM.covs[k] # get the covariance matrix of the component
  pose = sample_gaussian(mean, cov) # sample a pose from the Gaussian distribution
  pose = clip_pose(pose) # clip the pose to the valid range
  poses.append(pose) # add the pose to the list

# Step 2: Human model deformation
models = [] # an empty list of models
SCAPE = load_SCAPE() # load the SCAPE model with shape and pose blendshapes
for pose in poses:
  model = SCAPE.base_shape + SCAPE.shape_blendshapes * SCAPE.shape_coeffs # compute the shape of the model
  model = model + SCAPE.pose_blendshapes * pose # compute the pose deformation of the model
  model = skin_model(model, SCAPE.skinning_weights, SCAPE.joint_transforms) # apply linear blend skinning to the model
  models.append(model) # add the model to the list

# Step 3: Texture synthesis
textures = [] # an empty list of textures
for product_image in product_images:
  texture = segment_texture(product_image) # segment the clothing texture from the product image using GrabCut [43]
  texture = warp_texture(texture, SCAPE.texture_atlas) # warp the texture to fit the SCAPE texture atlas using TPS [8]
  textures.append(texture) # add the texture to the list

for model in models:
  texture = random_choice(textures) # randomly choose a texture from the list
  model.texture = texture # assign the texture to the model

# Step 4: Image composition
images = [] # an empty list of images
for model in models:
  viewpoint = random_viewpoint() # randomly choose a viewpoint from a uniform distribution over a sphere
  light_source = random_light_source() # randomly choose a light source from a uniform distribution over a hemisphere
  image = render_model(model, viewpoint, light_source) # render the model with Phong shading and perspective projection
  background = random_background() # randomly choose a background image from a large dataset [9]
  mask = render_mask(model, viewpoint) # render a binary mask for the model silhouette
  image = composite_image(image, background, mask) # composite the image over the background using Poisson blending [44]
  image = domain_adaptation(image) # apply domain adaptation techniques such as color jittering, random cropping, and adversarial training [25]
  images.append(image) # add the image to the list

return images
```