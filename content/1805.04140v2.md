---
title: 1805.04140v2 Neural Best-Buddies  Sparse Cross-Domain Correspondence
date: 2018-05-05
---

# [Neural Best-Buddies: Sparse Cross-Domain Correspondence](http://arxiv.org/abs/1805.04140v2)

authors: Kfir Aberman, Jing Liao, Mingyi Shi, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/1805.04140v2.pdf "Neural Best-Buddies: Sparse Cross-Domain Correspondence - arXiv.org"
[2]: https://arxiv.org/abs/1805.04140 "Neural Best-Buddies: Sparse Cross-Domain Correspondence"
[3]: https://arxiv-export1.library.cornell.edu/abs/1805.04140v2 "[1805.04140v2] Neural Best-Buddies: Sparse Cross-Domain Correspondence"

Here is a summary of the paper:

- **What**: The paper presents a novel method for sparse cross-domain correspondence, which is the problem of finding matching points or regions between images that belong to different domains, such as animals and vehicles[^1^][1] [^2^][2] [^3^][3].
- **Why**: Sparse cross-domain correspondence is useful for a variety of graphics applications, such as cross-domain image alignment, creation of hybrid images, automatic image morphing, and more[^1^][1] [^2^][2] [^3^][3]. Existing methods for correspondence either rely on user input, assume similar appearance or shape, or fail to capture semantic or geometric similarities across domains[^1^][1].
- **How**: The method operates on hierarchies of deep features, extracted from the input images by a pre-trained CNN. Specifically, starting from the coarsest layer in both hierarchies, it searches for Neural Best Buddies (NBB): pairs of neurons that are mutual nearest neighbors. The key idea is then to percolate NBBs through the hierarchy, while narrowing down the search regions at each level and retaining only NBBs with significant activations. Furthermore, in order to overcome differences in appearance, each pair of search regions is transformed into a common appearance[^1^][1] [^2^][2] [^3^][3].

## Main Contributions

According to the paper, the main contributions are:

- A novel method for sparse cross-domain correspondence that does not require user input, can handle large shape and appearance variations, and can capture semantic and geometric similarities across domains.
- A coarse-to-fine algorithm that operates on hierarchies of deep features and searches for Neural Best Buddies (NBB) at each level, while narrowing down the search regions and transforming them into a common appearance.
- A user study that evaluates the quality of the correspondence results and compares them with alternative methods.
- A variety of graphics applications that demonstrate the usefulness of the method, such as cross-domain image alignment, creation of hybrid images, automatic image morphing, and more.

## Method Summary

[1]: https://arxiv.org/pdf/1805.04140v2.pdf "Neural Best-Buddies: Sparse Cross-Domain Correspondence - arXiv.org"
[2]: https://arxiv.org/abs/1805.04140 "Neural Best-Buddies: Sparse Cross-Domain Correspondence"
[3]: https://arxiv-export1.library.cornell.edu/abs/1805.04140v2 "[1805.04140v2] Neural Best-Buddies: Sparse Cross-Domain Correspondence"

Here is a summary of the method section of the paper:

- The method consists of three main steps: feature extraction, Neural Best Buddies (NBB) search, and appearance transformation[^1^][1] [^2^][2] [^3^][3].
- Feature extraction: The method uses a pre-trained CNN (VGG-19) to extract deep features from the input images at different levels of abstraction. The features are normalized and stored in hierarchies of feature maps[^1^][1] [^2^][2] [^3^][3].
- NBB search: The method searches for NBBs at each level of the feature hierarchies, starting from the coarsest one. NBBs are pairs of neurons that are mutual nearest neighbors in the feature space. The method uses a coarse-to-fine strategy to narrow down the search regions at each level, based on the NBBs found at the previous level. The method also filters out NBBs with low activations or low mutual distances[^1^][1] [^2^][2] [^3^][3].
- Appearance transformation: The method transforms each pair of search regions into a common appearance, using a histogram matching technique. This helps to overcome differences in appearance and illumination between the images and to find more accurate NBBs[^1^][1] [^2^][2] [^3^][3].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: two images I1 and I2
# Output: a set of NBBs between I1 and I2

# Extract deep features from I1 and I2 using a pre-trained CNN
F1 = extract_features(I1)
F2 = extract_features(I2)

# Initialize the set of NBBs to be empty
NBBs = []

# Loop over the feature hierarchies from coarse to fine
for l in range(L-1, -1, -1):
  # Initialize the search regions to be the entire feature maps
  R1 = F1[l]
  R2 = F2[l]

  # Find NBBs at the current level
  NBBs_l = find_NBBs(R1, R2)

  # Filter out NBBs with low activations or low mutual distances
  NBBs_l = filter_NBBs(NBBs_l)

  # Add the NBBs at the current level to the final set
  NBBs = NBBs + NBBs_l

  # If not at the finest level, narrow down the search regions for the next level
  if l > 0:
    # Transform the search regions into a common appearance
    R1, R2 = transform_appearance(R1, R2)

    # Narrow down the search regions based on the NBBs at the current level
    R1, R2 = narrow_down_regions(R1, R2, NBBs_l)

# Return the final set of NBBs
return NBBs
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: two images I1 and I2
# Output: a set of NBBs between I1 and I2

# Define the number of levels in the feature hierarchy
L = 5

# Define the activation threshold for filtering NBBs
T_act = 0.01

# Define the mutual distance threshold for filtering NBBs
T_dist = 0.1

# Define the histogram matching parameters
alpha = 0.5
beta = 0.5

# Load the pre-trained CNN model (VGG-19)
model = load_model("vgg19")

# Extract deep features from I1 and I2 using the CNN model
F1 = extract_features(I1, model)
F2 = extract_features(I2, model)

# Initialize the set of NBBs to be empty
NBBs = []

# Loop over the feature hierarchies from coarse to fine
for l in range(L-1, -1, -1):
  # Initialize the search regions to be the entire feature maps
  R1 = F1[l]
  R2 = F2[l]

  # Find NBBs at the current level by computing pairwise distances and finding mutual nearest neighbors
  D = compute_distances(R1, R2)
  NBBs_l = find_mutual_nn(D)

  # Filter out NBBs with low activations or low mutual distances
  NBBs_l = filter_NBBs(NBBs_l, T_act, T_dist)

  # Add the NBBs at the current level to the final set
  NBBs = NBBs + NBBs_l

  # If not at the finest level, narrow down the search regions for the next level
  if l > 0:
    # Transform the search regions into a common appearance by applying histogram matching to each channel
    R1, R2 = transform_appearance(R1, R2, alpha, beta)

    # Narrow down the search regions based on the NBBs at the current level by cropping a window around each NBB and scaling it to match the next level resolution
    R1, R2 = narrow_down_regions(R1, R2, NBBs_l)

# Return the final set of NBBs
return NBBs

# Helper functions

def extract_features(I, model):
  # Extract deep features from an image using a CNN model
  # Input: an image I and a CNN model
  # Output: a list of feature maps at different levels of abstraction

  # Initialize an empty list of feature maps
  F = []

  # Pass the image through the CNN model and get the output of each convolutional layer
  outputs = model(I)

  # Loop over the outputs and append them to the list of feature maps
  for output in outputs:
    F.append(output)

  # Return the list of feature maps
  return F

def compute_distances(R1, R2):
  # Compute pairwise distances between two feature maps
  # Input: two feature maps R1 and R2 of size H x W x C
  # Output: a distance matrix D of size H x W x H x W

  # Initialize an empty distance matrix D
  D = []

  # Loop over the rows and columns of R1
  for i in range(H):
    for j in range(W):
      # Get the feature vector at (i,j) in R1
      f1 = R1[i,j,:]

      # Initialize an empty row in D
      row = []

      # Loop over the rows and columns of R2
      for k in range(H):
        for l in range(W):
          # Get the feature vector at (k,l) in R2
          f2 = R2[k,l,:]

          # Compute the Euclidean distance between f1 and f2 and append it to the row
          d = euclidean_distance(f1,f2)
          row.append(d)

      # Append the row to D
      D.append(row)

  # Return the distance matrix D
  return D

def find_mutual_nn(D):
  # Find mutual nearest neighbors in a distance matrix
  # Input: a distance matrix D of size H x W x H x W
  # Output: a list of NBBs as tuples of coordinates

  # Initialize an empty list of NBBs
  NBBs = []

  # Loop over the rows and columns of D (corresponding to R1)
  for i in range(H):
    for j in range(W):
      # Get the row at (i,j) in D
      row = D[i,j,:]

      # Find the minimum value and its index in the row
      min_val = min(row)
      min_idx = argmin(row)

      # Convert the index to coordinates in R2
      k = min_idx // W
      l = min_idx % W

      # Get the column at (k,l) in D
      col = D[:,k,l]

      # Find the minimum value and its index in the column
      min_val2 = min(col)
      min_idx2 = argmin(col)

      # Convert the index to coordinates in R1
      i2 = min_idx2 // W
      j2 = min_idx2 % W

      # Check if the coordinates are mutual nearest neighbors
      if i == i2 and j == j2:
        # Add the coordinates as a tuple to the list of NBBs
        NBBs.append((i,j,k,l))

  # Return the list of NBBs
  return NBBs

def filter_NBBs(NBBs, T_act, T_dist):
  # Filter out NBBs with low activations or low mutual distances
  # Input: a list of NBBs as tuples of coordinates, an activation threshold T_act, and a mutual distance threshold T_dist
  # Output: a filtered list of NBBs

  # Initialize an empty list of filtered NBBs
  filtered_NBBs = []

  # Loop over the NBBs
  for NBB in NBBs:
    # Unpack the coordinates of the NBB
    i,j,k,l = NBB

    # Get the activations of the corresponding neurons in R1 and R2
    a1 = R1[i,j,:]
    a2 = R2[k,l,:]

    # Get the mutual distance of the corresponding neurons in D
    d = D[i,j,k,l]

    # Check if the activations and the distance are above the thresholds
    if a1 > T_act and a2 > T_act and d > T_dist:
      # Add the NBB to the filtered list
      filtered_NBBs.append(NBB)

  # Return the filtered list of NBBs
  return filtered_NBBs

def transform_appearance(R1, R2, alpha, beta):
  # Transform the search regions into a common appearance by applying histogram matching to each channel
  # Input: two search regions R1 and R2 of size H x W x C, and two parameters alpha and beta for histogram matching
  # Output: two transformed search regions R1' and R2' of size H x W x C

  # Initialize two empty transformed search regions R1' and R2'
  R1_prime = []
  R2_prime = []

  # Loop over the channels of R1 and R2
  for c in range(C):
    # Get the channel c of R1 and R2
    C1 = R1[:,:,c]
    C2 = R2[:,:,c]

    # Apply histogram matching to C1 and C2 using alpha and beta as weights
    C1_prime, C2_prime = histogram_matching(C1, C2, alpha, beta)

    # Append C1_prime and C2_prime to R1' and R2'
    R1_prime.append(C1_prime)
    R2_prime.append(C2_prime)

  # Return the transformed search regions R1' and R2'
  return R1_prime, R2_prime

def narrow_down_regions(R1, R2, NBBs_l):
  # Narrow down the search regions based on the NBBs at the current level by cropping a window around each NBB and scaling it to match the next level resolution
  # Input: two search regions R1 and R2 of size H x W x C, and a list of NBBs at the current level as tuples of coordinates
  # Output: two narrowed down search regions R1' and R2' of size H/4 x W/4 x C

  # Initialize two empty narrowed down search regions R1' and R2'
  R1_prime = []
  R2_prime = []

  # Loop over the NBBs at the current level
  for NBB in NBBs_l:
    # Unpack the coordinates of the NBB
    i,j,k,l = NBB

    # Crop a window of size H/4 x W/4 around (i,j) in R1
    W1 = crop_window(R1, i, j, H/4, W/4)

    # Crop a window of size H/4 x W/4 around (k,l) in R2
    W2 = crop_window(R2, k, l, H/4, W/4)

    # Scale W1 and W2 to