---
title: 2306.17115v1 Michelangelo  Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation
date: 2023-06-18
---

# [Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation](http://arxiv.org/abs/2306.17115v1)

authors: Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao


## What, Why and How

[1]: https://arxiv.org/abs/2306.17115v1 "[2306.17115v1] Michelangelo: Conditional 3D Shape ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.17115v1.pdf "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text ..."
[3]: http://export.arxiv.org/abs/2306.17115 "[2306.17115] Michelangelo: Conditional 3D Shape Generation based on ..."

Here is a summary of the paper:

```
What: The paper presents a novel approach to generate 3D shapes based on 2D images or texts, using a shape-image-text-aligned latent space and a diffusion-based generative model.

Why: The paper aims to overcome the challenges of existing conditional generative models, such as inconsistent results, domain gap, and diverse shape topologies.

How: The paper proposes a two-stage framework that consists of a Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE) and a conditional Aligned Shape Latent Diffusion Model (ASLDM). The SITA-VAE encodes 3D shapes into a latent space that is aligned with images and texts, and reconstructs 3D neural fields from shape embeddings. The ASLDM learns a probabilistic mapping function from images or texts to the shape latent space, and generates 3D shapes by sampling from the diffusion process.
```


## Main Contributions

[1]: https://arxiv.org/abs/2306.17115v1 "[2306.17115v1] Michelangelo: Conditional 3D Shape ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.17115v1.pdf "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text ..."
[3]: http://export.arxiv.org/abs/2306.17115 "[2306.17115] Michelangelo: Conditional 3D Shape Generation based on ..."

Here is a list of the contributions of the paper:

```
- The paper proposes a novel alignment-before-generation approach to generate 3D shapes based on 2D images or texts, which bridges the domain gap among the three modalities and facilitates multi-modal-conditioned 3D shape generation.
- The paper introduces a shape-image-text-aligned latent space that encodes 3D shapes in a way that is consistent with images and texts, and enables cross-modality retrieval and interpolation.
- The paper develops a conditional Aligned Shape Latent Diffusion Model (ASLDM) that learns a probabilistic mapping function from images or texts to the shape latent space, and generates 3D shapes by sampling from the diffusion process.
- The paper demonstrates that the proposed approach can generate higher-quality and more diverse 3D shapes that better semantically conform to the visual or textural conditional inputs, and outperforms existing methods on various metrics and human evaluations.
```

## Method Summary

[1]: https://arxiv.org/abs/2306.17115v1 "[2306.17115v1] Michelangelo: Conditional 3D Shape ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.17115v1.pdf "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text ..."
[3]: http://export.arxiv.org/abs/2306.17115 "[2306.17115] Michelangelo: Conditional 3D Shape Generation based on ..."

Here is a summary of the method section of the paper:

```
The method section of the paper consists of four subsections: 3.1 Shape-Image-Text-Aligned Space, 3.2 Shape-Image-Text-Aligned Variational Auto-Encoder, 3.3 Aligned Shape Latent Diffusion Model, and 3.4 Implementation Details.

In 3.1, the paper introduces the concept of shape-image-text-aligned space, which is a latent space that encodes 3D shapes in a way that is consistent with images and texts. The paper defines three alignment losses to measure the alignment between shape embeddings and image or text embeddings, and uses them to train a joint encoder network that maps 3D shapes, images, and texts to the same latent space.

In 3.2, the paper describes the architecture and training procedure of the Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE), which is a model that encodes 3D shapes into the shape-image-text-aligned space and reconstructs them as fine-grained 3D neural fields. The paper uses a transformer-based decoder to generate 3D neural fields from shape embeddings, and employs a reconstruction loss and a KL-divergence loss to optimize the SITA-VAE.

In 3.3, the paper presents the conditional Aligned Shape Latent Diffusion Model (ASLDM), which is a model that learns a probabilistic mapping function from images or texts to the shape-image-text-aligned space, and generates 3D shapes by sampling from the diffusion process. The paper adopts a score-based generative model to model the diffusion process, and uses a contrastive loss and an alignment loss to train the ASLDM.

In 3.4, the paper provides some implementation details of the proposed framework, such as the network architectures, the training data, the hyperparameters, and the evaluation metrics.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.17115v1 "[2306.17115v1] Michelangelo: Conditional 3D Shape ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.17115v1.pdf "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text ..."
[3]: http://export.arxiv.org/abs/2306.17115 "[2306.17115] Michelangelo: Conditional 3D Shape Generation based on ..."

Here is a high-level pseudo code for the paper:

```
# Define the shape-image-text-aligned latent space
shape_latent_space = latent_space(shape_dim)
image_latent_space = latent_space(image_dim)
text_latent_space = latent_space(text_dim)

# Define the alignment losses
shape_image_alignment_loss = cosine_similarity(shape_latent_space, image_latent_space)
shape_text_alignment_loss = cosine_similarity(shape_latent_space, text_latent_space)
image_text_alignment_loss = cosine_similarity(image_latent_space, text_latent_space)

# Define the joint encoder network
joint_encoder = encoder_network(shape_input, image_input, text_input)
shape_embedding = joint_encoder(shape_input)
image_embedding = joint_encoder(image_input)
text_embedding = joint_encoder(text_input)

# Train the joint encoder network with the alignment losses
joint_encoder.train(minimize(shape_image_alignment_loss + shape_text_alignment_loss + image_text_alignment_loss))

# Define the Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE)
sita_vae = vae_network(shape_embedding, shape_output)
shape_reconstruction = sita_vae(shape_embedding)

# Define the reconstruction loss and the KL-divergence loss
reconstruction_loss = l2_loss(shape_output, shape_reconstruction)
kl_divergence_loss = kl_divergence(shape_embedding, prior_distribution)

# Train the SITA-VAE with the reconstruction loss and the KL-divergence loss
sita_vae.train(minimize(reconstruction_loss + kl_divergence_loss))

# Define the conditional Aligned Shape Latent Diffusion Model (ASLDM)
asldm = diffusion_model(image_embedding, text_embedding, shape_embedding, shape_output)

# Define the contrastive loss and the alignment loss
contrastive_loss = contrastive_learning(asldm(image_embedding), asldm(text_embedding))
alignment_loss = cosine_similarity(asldm(image_embedding), asldm(text_embedding))

# Train the ASLDM with the contrastive loss and the alignment loss
asldm.train(minimize(contrastive_loss + alignment_loss))

# Generate 3D shapes from images or texts by sampling from the diffusion process
image_input = image_data
text_input = text_data
image_embedding = joint_encoder(image_input)
text_embedding = joint_encoder(text_input)
shape_embedding = asldm(image_embedding) or asldm(text_embedding)
shape_output = sita_vae(shape_embedding)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2306.17115v1 "[2306.17115v1] Michelangelo: Conditional 3D Shape ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.17115v1.pdf "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text ..."
[3]: http://export.arxiv.org/abs/2306.17115 "[2306.17115] Michelangelo: Conditional 3D Shape Generation based on ..."

Here is a detailed pseudo code for the paper:

```
# Define the shape-image-text-aligned latent space
shape_latent_space = latent_space(shape_dim)
image_latent_space = latent_space(image_dim)
text_latent_space = latent_space(text_dim)

# Define the alignment losses
shape_image_alignment_loss = cosine_similarity(shape_latent_space, image_latent_space)
shape_text_alignment_loss = cosine_similarity(shape_latent_space, text_latent_space)
image_text_alignment_loss = cosine_similarity(image_latent_space, text_latent_space)

# Define the joint encoder network
joint_encoder = encoder_network(shape_input, image_input, text_input)
shape_embedding = joint_encoder(shape_input)
image_embedding = joint_encoder(image_input)
text_embedding = joint_encoder(text_input)

# Train the joint encoder network with the alignment losses
joint_encoder.train(minimize(shape_image_alignment_loss + shape_text_alignment_loss + image_text_alignment_loss))

# Define the Shape-Image-Text-Aligned Variational Auto-Encoder (SITA-VAE)
sita_vae = vae_network(shape_embedding, shape_output)
shape_reconstruction = sita_vae(shape_embedding)

# Define the reconstruction loss and the KL-divergence loss
reconstruction_loss = l2_loss(shape_output, shape_reconstruction)
kl_divergence_loss = kl_divergence(shape_embedding, prior_distribution)

# Train the SITA-VAE with the reconstruction loss and the KL-divergence loss
sita_vae.train(minimize(reconstruction_loss + kl_divergence_loss))

# Define the conditional Aligned Shape Latent Diffusion Model (ASLDM)
asldm = diffusion_model(image_embedding, text_embedding, shape_embedding, shape_output)

# Define the contrastive loss and the alignment loss
contrastive_loss = contrastive_learning(asldm(image_embedding), asldm(text_embedding))
alignment_loss = cosine_similarity(asldm(image_embedding), asldm(text_embedding))

# Train the ASLDM with the contrastive loss and the alignment loss
asldm.train(minimize(contrastive_loss + alignment_loss))

# Generate 3D shapes from images or texts by sampling from the diffusion process
image_input = image_data
text_input = text_data
image_embedding = joint_encoder(image_input)
text_embedding = joint_encoder(text_input)
shape_embedding = asldm(image_embedding) or asldm(text_embedding)
shape_output = sita_vae(shape_embedding)

# Define some helper functions and modules

def latent_space(dim):
  # Create a latent space of given dimension
  return torch.randn(dim)

def cosine_similarity(x, y):
  # Compute the cosine similarity between two vectors x and y
  return torch.dot(x, y) / (torch.norm(x) * torch.norm(y))

def encoder_network(shape_input, image_input, text_input):
  # Create an encoder network that takes shape, image, and text inputs and outputs embeddings
  # Use a PointNet++ [40] to encode shape input as a point cloud
  # Use a ResNet-50 [16] to encode image input as an RGB image
  # Use a BERT [10] to encode text input as a natural language description
  # Concatenate the outputs of each encoder and pass them through a fully connected layer
  shape_encoder = PointNetPlusPlus(shape_input)
  image_encoder = ResNet50(image_input)
  text_encoder = BERT(text_input)
  output = torch.cat([shape_encoder, image_encoder, text_encoder], dim=-1)
  output = linear_layer(output)
  return output

def vae_network(shape_embedding, shape_output):
  # Create a variational auto-encoder network that takes shape embedding as input and outputs shape reconstruction
  # Use a fully connected layer to map shape embedding to mean and log variance vectors
  # Sample a latent vector from a normal distribution parameterized by mean and log variance vectors
  # Use a transformer-based decoder [49] to generate a fine-grained 3D neural field from latent vector
  mean, log_var = linear_layer(shape_embedding).chunk(2, dim=-1)
  latent_vector = mean + torch.exp(log_var / 2) * torch.randn_like(log_var)
  decoder = TransformerDecoder(latent_vector)
  shape_reconstruction = decoder()
  return shape_reconstruction

def l2_loss(x, y):
  # Compute the L2 loss between two tensors x and y
  return torch.mean((x - y) ** 2)

def kl_divergence(x, y):
  # Compute the KL-divergence between two distributions x and y
  return torch.distributions.kl_divergence(x, y).mean()

def diffusion_model(image_embedding, text_embedding, shape_embedding, shape_output):
  # Create a diffusion model that takes image or text embedding as input and outputs shape embedding
  # Use a score-based generative model [11] to model the diffusion process
  # Use a U-Net [41] to predict the score function from image or text embedding
  # Use a reverse diffusion process to sample shape embedding from image or text embedding
  score_model = ScoreModel(image_embedding, text_embedding)
  shape_embedding = reverse_diffusion(score_model, shape_output)
  return shape_embedding

def contrastive_learning(x, y):
  # Compute the contrastive learning loss between two embeddings x and y
  # Use the InfoNCE loss [43] with a temperature parameter
  logits = torch.matmul(x, y.t()) / temperature
  labels = torch.arange(x.size(0))
  loss = cross_entropy_loss(logits, labels)
  return loss

def cross_entropy_loss(x, y):
  # Compute the cross-entropy loss between logits x and labels y
  return torch.nn.functional.cross_entropy(x, y)

def reverse_diffusion(score_model, shape_output):
  # Sample shape embedding from shape output using a reverse diffusion process
  # Use the algorithm described in Algorithm 1 of [11]
  shape_embedding = shape_output
  for t in reversed(range(num_steps)):
    epsilon_t = torch.randn_like(shape_embedding)
    sigma_t = get_sigma(t)
    shape_embedding = (shape_embedding - sigma_t ** 2 * score_model(shape_embedding, t)) / math.sqrt(1 - sigma_t ** 2) + math.sqrt(sigma_t ** 2 / (1 - sigma_t ** 2)) * epsilon_t
  return shape_embedding

def get_sigma(t):
  # Get the noise level sigma at step t
  # Use the linear schedule described in Section A.1 of [11]
  return (t + 1) / num_steps * (sigma_end - sigma_start) + sigma_start

def ScoreModel(image_embedding, text_embedding):
  # Create a score model that takes image or text embedding as input and outputs the score function for the diffusion process
  # Use a U-Net [41] architecture with residual blocks [16]
  # Concatenate image or text embedding with shape embedding as input
  # Output a scalar value for each shape embedding dimension
  unet = UNet()
  score_function = lambda shape_embedding, t: unet(torch.cat([image_embedding, text_embedding, shape_embedding], dim=-1))
  return score_function

def TransformerDecoder(latent_vector):
  # Create a transformer-based decoder that takes latent vector as input and outputs a fine-grained 3D neural field
  # Use the architecture described in Section A.3 of [49]
  # Output a scalar occupancy value and a three-dimensional normal vector for each point in the neural field
  transformer_decoder = TransformerDecoder()
  neural_field = lambda point: transformer_decoder(torch.cat([latent_vector, point], dim=-1))
  return neural_field

def UNet():
  # Create a U-Net architecture with residual blocks
  # Use the architecture described in Section A.2 of [41]
  unet = UNet()
  return unet

def PointNetPlusPlus(shape_input):
  # Create a PointNet++ encoder that takes shape input as a point cloud and outputs an embedding
  # Use the architecture described in Section A.4 of [40]
  pointnetplusplus = PointNetPlusPlus()
  embedding = pointnetplusplus(shape_input)
  return embedding

def ResNet50(image_input):
  # Create a ResNet-50 encoder that takes image input as an RGB image and outputs an embedding
  # Use the architecture described in Section A.5 of [16]
  resnet50 = ResNet50()
  embedding = resnet50(image_input)
  return embedding

def BERT(text_input):
  # Create a BERT encoder that takes text input as a natural language description and outputs an embedding
  # Use the architecture described in Section A.6 of [10]
  bert = BERT()
  embedding = bert(text_input)
```