---
title: 2306.01708v1 Resolving Interference When Merging Models
date: 2023-06-02
---

# [Resolving Interference When Merging Models](http://arxiv.org/abs/2306.01708v1)

authors: Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, Mohit Bansal


## What, Why and How

[1]: https://arxiv.org/pdf/2306.01708v1.pdf "arXiv:2306.01708v1 [cs.LG] 2 Jun 2023"
[2]: https://arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new method called **TrIm, Elect Sign & Merge (TIES-Merging)** for merging multiple task-specific models into a single multitask model without additional training.
- **Why**: The paper aims to address the drawbacks of having separate fine-tuned models for each task, such as storage and deployment costs, and lack of cross-task benefits. The paper also identifies two major sources of interference that cause performance drops when merging models: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models.
- **How**: The paper introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. The paper evaluates the proposed method on various settings covering different modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings, and shows that it outperforms several existing methods. The paper also analyzes the impact of different types of interference on model parameters and highlights the importance of resolving sign interference.


## Main Contributions

[1]: https://arxiv.org/pdf/2306.01708v1.pdf "arXiv:2306.01708v1 [cs.LG] 2 Jun 2023"
[2]: https://arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models"

According to the paper at [^1^][1], the main contributions are:

- **A novel method for merging models**: The paper proposes **TrIm, Elect Sign & Merge (TIES-Merging)**, which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign.
- **An extensive evaluation of the proposed method**: The paper evaluates TIES-Merging on various settings covering different modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings, and shows that it outperforms several existing methods.
- **An analysis of the sources and impact of interference**: The paper identifies two major sources of interference that cause performance drops when merging models: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. The paper also analyzes the impact of different types of interference on model parameters and highlights the importance of resolving sign interference.


## Method Summary

[1]: https://arxiv.org/pdf/2306.01708v1.pdf "arXiv:2306.01708v1 [cs.LG] 2 Jun 2023"
[2]: https://arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models"

Here is a summary of the method section of the paper at [^1^][1]:

- **Model Merging**: The paper defines model merging as the process of combining multiple task-specific models into a single multitask model without additional training. The paper assumes that all models are derived from the same pre-trained model and have the same architecture and parameter size. The paper also assumes that each task has a unique identifier that can be used to select the appropriate model for inference.
- **Interference Analysis**: The paper analyzes the sources and impact of interference when merging models. The paper defines interference as the difference between the parameter values of two models for a given layer. The paper measures interference using two metrics: (a) absolute interference, which is the absolute value of the difference between two parameter values, and (b) sign interference, which is the fraction of parameters that have opposite signs across two models. The paper shows that both types of interference are correlated with performance drops when merging models, and that sign interference is especially detrimental for deeper layers.
- **TIES-Merging**: The paper proposes a new method for merging models, called TrIm, Elect Sign & Merge (TIES-Merging), which consists of three steps: (1) TrIm: resetting parameters that only changed a small amount during fine-tuning, (2) Elect Sign: resolving sign conflicts by electing a sign for each parameter based on a voting scheme, and (3) Merge: merging only the parameters that are in alignment with the final agreed-upon sign. The paper provides pseudocode and mathematical formulations for each step, and explains the intuition and rationale behind them.


## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2306.01708v1.pdf "arXiv:2306.01708v1 [cs.LG] 2 Jun 2023"
[2]: https://arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.01708 "[2306.01708] Resolving Interference When Merging Models"

Here is the high-level pseudo code for the paper at [^1^][1]:

```
# Input: A set of task-specific models M = {M_1, M_2, ..., M_T} derived from the same pre-trained model P
# Output: A single multitask model M*

# Step 1: TrIm
for each model M_i in M:
  for each layer l in M_i:
    get the parameter vector p_i^l of M_i at layer l
    get the parameter vector p^l of P at layer l
    compute the absolute difference d_i^l between p_i^l and p^l
    reset the parameters in p_i^l that have d_i^l below a threshold tau to p^l

# Step 2: Elect Sign
for each layer l:
  initialize an empty list S^l to store the sign vectors of each model at layer l
  for each model M_i in M:
    get the parameter vector p_i^l of M_i at layer l
    compute the sign vector s_i^l of p_i^l
    append s_i^l to S^l
  compute the sign matrix S^l by stacking the sign vectors in S^l
  compute the sign frequency vector f^l by counting the number of positive and negative signs for each parameter in S^l
  compute the elected sign vector e^l by selecting the sign with higher frequency for each parameter in f^l

# Step 3: Merge
initialize an empty list P* to store the merged parameter vectors for each layer
for each layer l:
  initialize an empty list P^l to store the parameter vectors of each model at layer l
  for each model M_i in M:
    get the parameter vector p_i^l of M_i at layer l
    append p_i^l to P^l
  compute the parameter matrix P^l by stacking the parameter vectors in P^l
  get the elected sign vector e^l from Step 2
  compute the merged parameter vector p*^l by averaging only the parameters in P^l that have the same sign as e^l
  append p*^l to P*
construct the multitask model M* by replacing the parameter vectors of P with P*
return M*
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Input: A set of task-specific models M = {M_1, M_2, ..., M_T} derived from the same pre-trained model P
# Output: A single multitask model M*

# Step 1: TrIm
# Define a function to reset the parameters that only changed a small amount during fine-tuning
def trim(model, pre_trained_model, tau):
  # Loop over the layers of the model
  for layer in model.layers:
    # Get the parameter vector of the model at the current layer
    model_params = layer.get_weights()
    # Get the parameter vector of the pre-trained model at the current layer
    pre_trained_params = pre_trained_model.layers[layer.name].get_weights()
    # Compute the absolute difference between the two parameter vectors
    diff = np.abs(model_params - pre_trained_params)
    # Find the indices of the parameters that have difference below the threshold tau
    indices = np.where(diff < tau)
    # Reset those parameters to the values of the pre-trained model
    model_params[indices] = pre_trained_params[indices]
    # Update the weights of the layer with the modified parameter vector
    layer.set_weights(model_params)
  # Return the trimmed model
  return model

# Loop over the models in M
for i in range(len(M)):
  # Get the i-th model
  model = M[i]
  # Apply the trim function to the model with P and tau as arguments
  model = trim(model, P, tau)
  # Update the i-th model in M with the trimmed model
  M[i] = model

# Step 2: Elect Sign
# Define a function to resolve sign conflicts by electing a sign for each parameter based on a voting scheme
def elect_sign(models):
  # Initialize an empty dictionary to store the elected sign vectors for each layer
  elected_signs = {}
  # Loop over the layers of the models
  for layer in models[0].layers:
    # Initialize an empty list to store the sign vectors of each model at the current layer
    sign_vectors = []
    # Loop over the models in models
    for model in models:
      # Get the parameter vector of the model at the current layer
      params = layer.get_weights()
      # Compute the sign vector of the parameter vector by applying np.sign function
      sign_vector = np.sign(params)
      # Append the sign vector to sign_vectors list
      sign_vectors.append(sign_vector)
    # Convert sign_vectors list into a numpy array and transpose it
    sign_matrix = np.array(sign_vectors).T
    # Compute the sign frequency vector by counting the number of positive and negative signs for each parameter in sign_matrix using np.bincount function with minlength=2 argument
    sign_freqs = np.apply_along_axis(lambda x: np.bincount(x, minlength=2), axis=1, arr=sign_matrix)
    # Compute the elected sign vector by selecting the sign with higher frequency for each parameter in sign_freqs using np.argmax function with axis=1 argument and mapping 0 to -1 and 1 to +1 using np.where function
    elected_sign_vector = np.where(np.argmax(sign_freqs, axis=1) == 0, -1, +1)
    # Store the elected sign vector for the current layer in elected_signs dictionary with layer name as key
    elected_signs[layer.name] = elected_sign_vector
  # Return elected_signs dictionary
  return elected_signs

# Apply elect_sign function to M and store the result in a variable called elected_signs
elected_signs = elect_sign(M)

# Step 3: Merge
# Define a function to merge only the parameters that are in alignment with the final agreed-upon sign
def merge(models, pre_trained_model, elected_signs):
  # Initialize an empty list to store the merged parameter vectors for each layer
  merged_params = []
  # Loop over the layers of the models
  for layer in models[0].layers:
    # Initialize an empty list to store the parameter vectors of each model at the current layer
    param_vectors = []
    # Loop over the models in models
    for model in models:
      # Get the parameter vector of the model at the current layer
      params = layer.get_weights()
      # Append it to param_vectors list
      param_vectors.append(params)
    # Convert param_vectors list into a numpy array and transpose it 
    param_matrix = np.array(param_vectors).T 
    # Get the elected sign vector for this layer from elected_signs dictionary using layer name as key
    elected_sign_vector = elected_signs[layer.name]
    # Compute a mask vector by checking which parameters in param_matrix have the same sign as elected_sign_vector using np.equal function
    mask = np.equal(param_matrix, elected_sign_vector)
    # Compute the merged parameter vector by averaging only the parameters in param_matrix that have the same sign as elected_sign_vector using np.mean function with axis=1 argument and mask argument
    merged_param_vector = np.mean(param_matrix, axis=1, mask=mask)
    # Append the merged parameter vector to merged_params list
    merged_params.append(merged_param_vector)
  # Construct the multitask model by cloning the pre-trained model using tf.keras.models.clone_model function
  multitask_model = tf.keras.models.clone_model(pre_trained_model)
  # Loop over the layers of the multitask model
  for i, layer in enumerate(multitask_model.layers):
    # Get the i-th merged parameter vector from merged_params list
    merged_param = merged_params[i]
    # Update the weights of the layer with the merged parameter vector using layer.set_weights function
    layer.set_weights(merged_param)
  # Return the multitask model
  return multitask_model

# Apply merge function to M, P and elected_signs and store the result in a variable called M*
M* = merge(M, P, elected_signs)

# Return M* as output
return M*
```