---
title: 2209.13284v2 Frame Interpolation for Dynamic Scenes with Implicit Flow Encoding
date: 2022-09-14
---

# [Frame Interpolation for Dynamic Scenes with Implicit Flow Encoding](http://arxiv.org/abs/2209.13284v2)

authors: Pedro FigueirÃªdo, Avinash Paliwal, Nima Khademi Kalantari


## What, Why and How

[1]: https://arxiv.org/pdf/2209.13284v2 "Frame Interpolation for Dynamic Scenes with Implicit Flow Encoding"
[2]: https://arxiv.org/abs/2209.13284 "[2209.13284] Frame Interpolation for Dynamic Scenes with Implicit Flow ..."
[3]: http://export.arxiv.org/abs/2303.13284v2 "[2303.13284v2] GETT-QA: Graph Embedding based T2T Transformer for ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes an algorithm to interpolate between a pair of images of a dynamic scene with brightness and illumination changes.
- **Why**: The paper aims to address the problem that existing frame interpolation algorithms are not able to handle images with lighting variations, which degrade the quality of the estimated flows and the final intermediate images.
- **How**: The paper leverages existing optical flow methods that are robust to illumination changes and encodes them into a coordinate-based network powered by a hypernetwork to obtain a continuous representation of the flow across time. The paper then uses the estimated flows within a blending network to obtain the final intermediate frame.

## Main Contributions

The paper claims the following contributions:

- A novel approach to interpolate between a pair of images of a dynamic scene with brightness and illumination changes by encoding the bidirectional flows into a coordinate-based network powered by a hypernetwork.
- A comprehensive evaluation of the proposed method on various datasets and scenarios, demonstrating its superiority over state-of-the-art frame interpolation algorithms in terms of visual quality and quantitative metrics.
- A user study to assess the perceptual preference of the proposed method over existing methods.

## Method Summary

[1]: https://arxiv.org/pdf/2209.13284v2 "Frame Interpolation for Dynamic Scenes with Implicit Flow Encoding"
[2]: https://arxiv.org/abs/2209.13284 "[2209.13284] Frame Interpolation for Dynamic Scenes with Implicit Flow ..."
[3]: http://export.arxiv.org/abs/2303.13284v2 "[2303.13284v2] GETT-QA: Graph Embedding based T2T Transformer for ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper first estimates the bidirectional optical flows between the two input images using a pre-trained flow network (RAFT [54]).
- The paper then encodes the bidirectional flows into a coordinate-based network (SIREN [12]) powered by a hypernetwork (HyperNet [22]) to obtain a continuous representation of the flow across time. The paper uses a linear interpolation of the input images' timestamps as the input to the hypernetwork to predict the flows from an intermediate frame to the two input images.
- The paper then warps the two input images and their features using the predicted flows and feeds them into a blending network (FILM [48]) to obtain the final intermediate image. The paper uses a multi-scale feature extraction and blending strategy similar to FILM [48].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: two images I1 and I2 with timestamps t1 and t2
# Output: an intermediate image Ii with timestamp ti

# Estimate the bidirectional optical flows F12 and F21 using RAFT
F12 = RAFT(I1, I2)
F21 = RAFT(I2, I1)

# Encode the bidirectional flows into a coordinate-based network powered by a hypernetwork
# The hypernetwork takes a linear interpolation of the timestamps as input
# The coordinate-based network takes the pixel coordinates as input
# The output is the predicted flows Fi1 and Fi2 from the intermediate frame to the input images
Fi1 = HyperNet(SIREN((x, y), F12), (1 - ti) * t1 + ti * t2)
Fi2 = HyperNet(SIREN((x, y), F21), (1 - ti) * t1 + ti * t2)

# Warp the input images and their features using the predicted flows
Ii1 = warp(I1, Fi1)
Ii2 = warp(I2, Fi2)
Fi1 = warp(Fi1, Fi1)
Fi2 = warp(Fi2, Fi2)

# Use a blending network to obtain the final intermediate image
Ii = FILM(Ii1, Ii2, Fi1, Fi2)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: two images I1 and I2 with timestamps t1 and t2
# Output: an intermediate image Ii with timestamp ti

# Define the hyperparameters
num_scales = 5 # number of scales for the feature pyramid
num_layers = 6 # number of layers for the hypernetwork
num_units = 256 # number of units for the hypernetwork
num_filters = 64 # number of filters for the coordinate-based network
num_harmonics = 5 # number of harmonics for the coordinate-based network

# Define the networks
RAFT = load_pretrained_model("RAFT") # load the pre-trained RAFT model
SIREN = SIREN(num_filters, num_harmonics) # initialize the SIREN model
HyperNet = HyperNet(num_layers, num_units) # initialize the HyperNet model
FILM = FILM(num_scales) # initialize the FILM model

# Estimate the bidirectional optical flows F12 and F21 using RAFT
F12 = RAFT(I1, I2)
F21 = RAFT(I2, I1)

# Encode the bidirectional flows into a coordinate-based network powered by a hypernetwork
# The hypernetwork takes a linear interpolation of the timestamps as input
# The coordinate-based network takes the pixel coordinates as input
# The output is the predicted flows Fi1 and Fi2 from the intermediate frame to the input images
t = (1 - ti) * t1 + ti * t2 # linear interpolation of the timestamps
w = HyperNet(t) # weights for the coordinate-based network
Fi1 = SIREN((x, y), F12, w) # predicted flow from Ii to I1
Fi2 = SIREN((x, y), F21, w) # predicted flow from Ii to I2

# Warp the input images and their features using the predicted flows
Ii1 = warp(I1, Fi1)
Ii2 = warp(I2, Fi2)
Fi1 = warp(Fi1, Fi1)
Fi2 = warp(Fi2, Fi2)

# Use a blending network to obtain the final intermediate image
Ii = FILM(Ii1, Ii2, Fi1, Fi2)
```