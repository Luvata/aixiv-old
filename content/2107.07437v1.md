---
title: 2107.07437v1 StyleFusion  A Generative Model for Disentangling Spatial Segments
date: 2021-07-08
---

# [StyleFusion: A Generative Model for Disentangling Spatial Segments](http://arxiv.org/abs/2107.07437v1)

authors: Omer Kafri, Or Patashnik, Yuval Alaluf, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2107.07437 "[2107.07437] StyleFusion: A Generative Model for ... - arXiv.org"
[2]: https://arxiv.org/abs/2107.07651 "[2107.07651] Align before Fuse: Vision and Language Representation ..."
[3]: http://export.arxiv.org/abs/2103.07437v1 "[2103.07437v1] Hyperspectral Image Denoising and Anomaly Detection ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper presents StyleFusion, a new mapping architecture for StyleGAN, which takes as input a number of latent codes and fuses them into a single style code. The resulting style code can be used to generate an image with fine-grained control over each semantic region.
- **Why**: The paper aims to address the limitations of existing methods for image synthesis and editing, which either require aligned regions or lack disentanglement between different image features. The paper claims that StyleFusion can achieve better flexibility and quality in image synthesis and editing by learning to disentangle spatial segments and fuse them into a harmonized representation.
- **How**: The paper proposes a hierarchical fusion network that operates on pairs of latent codes and learns to align and fuse them into a single style code. The fusion network is trained with a contrastive loss that encourages the fused style code to be similar to the input latent codes in their corresponding regions and dissimilar otherwise. The paper also introduces a special input latent code that provides global control over the generated image. The paper evaluates StyleFusion on various tasks such as cross-image mixing, region editing, and semantic-aware interpolation. The paper shows that StyleFusion outperforms existing methods in terms of visual quality, diversity, and user preference.

## Main Contributions

According to the paper, the main contributions are:

- A new mapping architecture for StyleGAN that can fuse multiple latent codes into a single style code and generate an image with fine-grained control over each semantic region.
- A contrastive loss function that enables the fusion network to learn to align and disentangle spatial segments from different latent codes.
- A special input latent code that allows global control over the generated image and improves the harmony and diversity of the synthesis results.
- Extensive experiments and user studies that demonstrate the effectiveness and superiority of StyleFusion over existing methods on various image synthesis and editing tasks.

## Method Summary

The method section of the paper consists of four subsections:

- **StyleFusion Mapping Architecture**: This subsection describes the proposed fusion network that takes as input a number of latent codes and outputs a single style code. The fusion network consists of several fusion blocks that operate on pairs of latent codes and learn to align and fuse them into a single latent code. The fusion blocks are arranged in a hierarchical manner, where each level is responsible for disentangling a pair of image regions. The output of the final fusion block is the fused style code that can be inserted into a pre-trained StyleGAN generator to produce an image.
- **Contrastive Loss for Spatial Disentanglement**: This subsection introduces the contrastive loss function that is used to train the fusion network. The contrastive loss encourages the fused style code to be similar to the input latent codes in their corresponding regions and dissimilar otherwise. The contrastive loss is computed by comparing the feature maps of the generated image and the reference images that are obtained by inserting each input latent code into the generator separately. The paper also explains how to select positive and negative pairs for the contrastive loss computation.
- **Special Input Latent Code for Global Control**: This subsection presents a special input latent code that is incorporated into the fusion network to provide global control over the generated image. The special input latent code is randomly sampled from a normal distribution and is fused with the other input latent codes at each level of the fusion network. The paper argues that the special input latent code can improve the harmony and diversity of the synthesis results by introducing randomness and variation into the fused style code.
- **Implementation Details**: This subsection provides the details of the network architecture, training procedure, and evaluation metrics used in the paper. The paper also discusses some implementation challenges and solutions, such as how to handle different numbers of input latent codes and how to avoid mode collapse.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the fusion network
fusion_network = FusionNetwork()

# Define the StyleGAN generator
generator = StyleGAN()

# Define the contrastive loss function
contrastive_loss = ContrastiveLoss()

# Define the optimizer
optimizer = Adam()

# Loop over the training data
for batch in data_loader:

  # Get a number of latent codes as input
  latent_codes = batch["latent_codes"]

  # Optionally add a special input latent code for global control
  if use_special_input:
    special_input = sample_normal()
    latent_codes.append(special_input)

  # Fuse the latent codes into a single style code using the fusion network
  fused_style_code = fusion_network(latent_codes)

  # Generate an image using the fused style code and the generator
  fused_image = generator(fused_style_code)

  # Generate reference images using each input latent code and the generator
  reference_images = []
  for latent_code in latent_codes:
    reference_image = generator(latent_code)
    reference_images.append(reference_image)

  # Compute the contrastive loss between the fused image and the reference images
  loss = contrastive_loss(fused_image, reference_images)

  # Update the fusion network parameters using the optimizer
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

# Define the fusion block class
class FusionBlock(nn.Module):
  def __init__(self, in_channels, out_channels):
    super(FusionBlock, self).__init__()

    # Define the convolutional layers
    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)

    # Define the batch normalization layers
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.bn2 = nn.BatchNorm2d(out_channels)

    # Define the activation function
    self.relu = nn.ReLU()

  def forward(self, x1, x2):
    # Concatenate the two input latent codes along the channel dimension
    x = torch.cat([x1, x2], dim=1)

    # Apply the first convolutional layer followed by batch normalization and activation
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)

    # Apply the second convolutional layer followed by batch normalization and activation
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)

    # Return the output latent code
    return x

# Define the fusion network class
class FusionNetwork(nn.Module):
  def __init__(self, num_levels, num_channels):
    super(FusionNetwork, self).__init__()

    # Define the number of levels and channels in the fusion network
    self.num_levels = num_levels
    self.num_channels = num_channels

    # Define the fusion blocks for each level
    self.fusion_blocks = nn.ModuleList()
    for i in range(num_levels):
      in_channels = num_channels * (i + 2)
      out_channels = num_channels * (i + 1)
      fusion_block = FusionBlock(in_channels, out_channels)
      self.fusion_blocks.append(fusion_block)

  def forward(self, latent_codes):
    # Check that the number of input latent codes is equal to the number of levels plus one
    assert len(latent_codes) == self.num_levels + 1

    # Loop over the levels of the fusion network
    for i in range(self.num_levels):
      # Get the two input latent codes for the current level
      x1 = latent_codes[i]
      x2 = latent_codes[i + 1]

      # Apply the fusion block to fuse them into a single latent code
      x = self.fusion_blocks[i](x1, x2)

      # Replace the second input latent code with the fused latent code in the list of latent codes
      latent_codes[i + 1] = x

    # Return the final fused latent code
    return latent_codes[-1]

# Define the StyleGAN generator class (simplified version)
class StyleGAN(nn.Module):
  def __init__(self):
    super(StyleGAN, self).__init__()

    # Define the constant input tensor
    self.constant_input = nn.Parameter(torch.randn(1, 512, 4, 4))

    # Define the mapping network that maps a style code to a style vector
    self.mapping_network = nn.Sequential(
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU(),
      nn.Linear(512, 512),
      nn.ReLU()
    )

    # Define the synthesis blocks that generate feature maps from style vectors
    self.synthesis_blocks = nn.ModuleList()
    
```python

# Continue defining the StyleGAN generator class

# Define a list of output resolutions for each synthesis block
resolutions = [8, 16, 32, 64]

# Loop over the resolutions and create a synthesis block for each one
for resolution in resolutions:
  
  # Compute the number of channels for the current resolution
  channels = int(512 / (resolution / 8))

  # Create a synthesis block that consists of two convolutional layers and an upsampling layer
  synthesis_block = nn.Sequential(
    
    # First convolutional layer with adaptive instance normalization and leaky ReLU activation
    nn.Conv2d(512, channels, kernel_size=3, stride=1, padding=1),
    nn.InstanceNorm2d(channels, affine=True),
    nn.LeakyReLU(0.2),

    # Second convolutional layer with adaptive instance normalization and leaky ReLU activation
    nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),
    nn.InstanceNorm2d(channels, affine=True),
    nn.LeakyReLU(0.2),

    # Upsampling layer that doubles the resolution
    nn.Upsample(scale_factor=2, mode="nearest")
  )

  # Add the synthesis block to the list of synthesis blocks
  self.synthesis_blocks.append(synthesis_block)

# Define the output convolutional layer that produces the final RGB image
self.output_conv = nn.Conv2d(channels, 3, kernel_size=1, stride=1, padding=0)

def forward(self, style_code):
  # Apply the mapping network to the style code to get a style vector
  style_vector = self.mapping_network(style_code)

  # Reshape the style vector to match the shape of the constant input tensor
  style_vector = style_vector.view(1, 512, 1, 1)

  # Multiply the constant input tensor with the style vector element-wise
  x = self.constant_input * style_vector

  # Loop over the synthesis blocks and apply them to the feature map
  for synthesis_block in self.synthesis_blocks:
    
    # Apply the synthesis block to the feature map
    x = synthesis_block(x)

    # Multiply the feature map with the style vector element-wise
    x = x * style_vector

  # Apply the output convolutional layer to get the RGB image
  x = self.output_conv(x)

  # Return the generated image
  return x

# Define the contrastive loss class
class ContrastiveLoss(nn.Module):
  def __init__(self, temperature):
    super(ContrastiveLoss, self).__init__()

    # Define the temperature parameter for scaling the logits
    self.temperature = temperature

  def forward(self, fused_image, reference_images):
    # Compute the feature maps of the fused image and the reference images using a pre-trained VGG network
    vgg = torchvision.models.vgg16(pretrained=True).features.eval()
    fused_features = vgg(fused_image)
    reference_features = [vgg(reference_image) for reference_image in reference_images]

    # Flatten and normalize the feature maps
    fused_features = F.normalize(fused_features.flatten(start_dim=1), dim=1)
    reference_features = [F.normalize(reference_feature.flatten(start_dim=1), dim=1) for reference_feature in reference_features]

    # Compute the cosine similarity matrix between the fused features and the reference features
    similarity_matrix = torch.matmul(fused_features, torch.cat(reference_features).t())

    # Scale the similarity matrix by the temperature parameter
    similarity_matrix = similarity_matrix / self.temperature

    # Compute the contrastive loss using cross entropy with hard negative mining
    loss = 0
    for i in range(len(reference_images)):
      # Get the positive and negative similarities for the current reference image
      positive_similarity = similarity_matrix[i::len(reference_images), i::len(reference_images)].diag()
      negative_similarity = similarity_matrix[i::len(reference_images), :].flatten(end_dim=1)

      # Remove the positive similarity from the negative similarity
      negative_similarity[i::len(reference_images)] = -float("inf")

      # Concatenate the positive and negative similarities and apply softmax
      logits = torch.cat([positive_similarity.view(-1, 1), negative_similarity.view(-1, len(reference_images) - 1)], dim=1)
      probs = F.softmax(logits, dim=1)

      # Compute the cross entropy loss with respect to the positive class
      target = torch.zeros(len(reference_images)).long().to(device)
      loss += F.cross_entropy(probs, target)

    # Return the average loss over all reference images
    return loss / len(reference_images)

# Define the optimizer class (simplified version)
class Adam(nn.Module):
  def __init__(self, params, lr):
    super(Adam, self).__init__()

    # Define the learning rate and other hyperparameters
    self.lr = lr
    self.beta1 = 0.9
    self.beta2 = 0.999
    self.eps = 1e-8

    # Initialize the parameters and their corresponding moments
    self.params = params
    self.moments_1 = [torch.zeros_like(param) for param in params]
    self.moments_2 = [torch.zeros_like(param) for param in params]

  def zero_grad(self):
    # Set all parameter gradients to zero
    for param in self.params:
      param.grad.zero_()

  def step