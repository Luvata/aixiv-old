---
title: 2203.04279v1 Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences
date: 2022-03-05
---

# [Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences](http://arxiv.org/abs/2203.04279v1)

authors: Prune Truong, Martin Danelljan, Fisher Yu, Luc Van Gool


## What, Why and How

[1]: https://arxiv.org/abs/2203.04279 "[2203.04279] Probabilistic Warp Consistency for Weakly-Supervised ..."
[2]: https://arxiv.org/pdf/2203.04279v1.pdf "f g@vision.ee.ethz.ch i@yf.io Abstract arXiv:2203.04279v1 [cs.CV] 8 Mar ..."
[3]: http://export.arxiv.org/abs/2103.04279v1 "[2103.04279v1] Hierarchical Self Attention Based Autoencoder for Open ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes **Probabilistic Warp Consistency**, a weakly-supervised learning objective for semantic matching. Semantic matching is the problem of finding pixel-wise correspondences between images depicting instances of the same semantic category of object or scene, such as 'cat' or 'bird'.
- **Why**: The paper aims to address the challenges of semantic matching, such as large intra-class appearance and shape variations, view-point changes, background clutter, and the lack of ground-truth annotations. The paper also aims to improve the performance of semantic matching models in both weakly-supervised and strongly-supervised regimes.
- **How**: The paper's approach directly supervises the dense matching scores predicted by the network, encoded as a conditional probability distribution. The paper first constructs an image triplet by applying a known warp to one of the images in a pair depicting different instances of the same object class. The paper then derives probabilistic learning objectives using the constraints arising from the resulting image triplet. The paper further accounts for occlusion and background clutter present in real image pairs by extending the probabilistic output space with a learnable unmatched state. To supervise it, the paper designs an objective between image pairs depicting different object classes. The paper validates its method by applying it to four recent semantic matching architectures and sets a new state-of-the-art on four challenging semantic matching benchmarks. The paper also demonstrates that its objective brings substantial improvements in the strongly-supervised regime, when combined with keypoint annotations.

## Main Contributions

The paper claims the following contributions:

- A novel weakly-supervised learning objective for semantic matching, based on probabilistic warp consistency.
- A principled way to handle occlusion and background clutter by introducing a learnable unmatched state in the probabilistic output space.
- A comprehensive evaluation of the proposed method on four semantic matching architectures and four benchmarks, showing state-of-the-art results in the weakly-supervised setting.
- An empirical analysis of the proposed method in the strongly-supervised setting, showing significant improvements over existing keypoint-based objectives.

## Method Summary

The method section of the paper consists of four subsections:

- **Problem formulation**: The paper defines the semantic matching problem as finding a dense correspondence function between two images depicting instances of the same object class. The paper also introduces the notation and terminology used throughout the paper.
- **Probabilistic warp consistency**: The paper presents its main idea of probabilistic warp consistency, which is based on the assumption that the conditional probability distribution of a pixel in one image given its corresponding pixel in another image should be invariant to a known warp applied to either image. The paper then derives two probabilistic objectives from this assumption, one based on the composition of two warps and one based on a direct warp. The paper also shows how to compute the gradients of these objectives with respect to the network parameters.
- **Handling unmatched pixels**: The paper extends its probabilistic output space with an additional unmatched state, which represents the probability that a pixel in one image has no corresponding pixel in another image. The paper then proposes a new objective to supervise this state, based on the idea that images of different object classes should have a high unmatched probability. The paper also discusses how to balance the different objectives and how to sample image pairs for training.
- **Implementation details**: The paper provides the details of its implementation, such as the network architectures, the loss functions, the optimization algorithm, the data augmentation, and the evaluation metrics. The paper also reports some ablation studies and qualitative results.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pair of images I and J depicting the same object class
# Output: a dense correspondence function f from I to J

# Generate a new image I0 by applying a random warp W to I
# Compute the network output S0 = F(I0, J), which is a conditional probability distribution over J for each pixel in I0
# Compute the network output S = F(I, J), which is a conditional probability distribution over J for each pixel in I

# Define the probabilistic warp consistency objectives
# Objective 1: enforce S0 to be equal to S composed with W
L1 = KL_divergence(S0, S o W)
# Objective 2: enforce S0 to be equal to W
L2 = KL_divergence(S0, W)

# Define the unmatched state objective
# Sample an image A depicting a different object class from I and J
# Compute the network output SA = F(I, A), which is a conditional probability distribution over A for each pixel in I
# Objective 3: enforce SA to have a high unmatched probability
L3 = cross_entropy(SA, unmatched)

# Define the total loss function as a weighted sum of the objectives
L = alpha * L1 + beta * L2 + gamma * L3

# Update the network parameters by minimizing the loss function using gradient descent
F = F - lr * grad(L, F)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import random

# Define the network architecture
# The paper uses four different architectures: UCN [11], IMW [12], NGF [35], and DHPF [36]
# Here we use UCN as an example
class UCN(torch.nn.Module):
  def __init__(self):
    super(UCN, self).__init__()
    # Define the feature extractor
    # The paper uses VGG16 [37] pretrained on ImageNet [38]
    self.feature_extractor = torchvision.models.vgg16(pretrained=True).features
    # Define the correlation layer
    # The paper uses a 4D correlation layer as in [39]
    self.correlation_layer = CorrelationLayer()
    # Define the matching network
    # The paper uses a 4-layer convolutional network with ReLU activations and batch normalization
    self.matching_network = torch.nn.Sequential(
      torch.nn.Conv2d(49, 128, kernel_size=3, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(128),
      torch.nn.Conv2d(128, 64, kernel_size=3, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(64),
      torch.nn.Conv2d(64, 32, kernel_size=3, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(32),
      torch.nn.Conv2d(32, 1, kernel_size=3, padding=1)
    )
  
  def forward(self, I, J):
    # Extract features from the input images
    FI = self.feature_extractor(I)
    FJ = self.feature_extractor(J)
    # Compute the correlation map between the features
    C = self.correlation_layer(FI, FJ)
    # Compute the matching score map from the correlation map
    M = self.matching_network(C)
    # Apply softmax to obtain the conditional probability distribution over J for each pixel in I
    S = torch.softmax(M, dim=-1)
    return S

# Define the warp function
# The paper uses affine transformations with random parameters sampled from uniform distributions
def warp(I):
  # Sample random parameters for the affine transformation
  theta = random.uniform(-15, 15) # rotation angle in degrees
  scale = random.uniform(0.8, 1.2) # scaling factor
  tx = random.uniform(-0.1, 0.1) # horizontal translation
  ty = random.uniform(-0.1, 0.1) # vertical translation

  # Convert the parameters to a transformation matrix
  theta = np.radians(theta)
  cos = np.cos(theta)
  sin = np.sin(theta)
  matrix = np.array([[scale * cos, -scale * sin, tx], [scale * sin, scale * cos, ty], [0, 0, 1]])

  # Apply the transformation to the image using bilinear interpolation and zero padding
  I0 = torchvision.transforms.functional.affine(I, matrix=matrix, interpolation='bilinear', fill=0)

  # Return the warped image and the inverse transformation matrix
  return I0, np.linalg.inv(matrix)

# Define the composition function
# The paper uses bilinear interpolation to compose two warps
def compose(S, W):
  # Get the height and width of the image
  H = S.shape[-2]
  W = S.shape[-1]

  # Create a grid of pixel coordinates in the image
  grid_x, grid_y = torch.meshgrid(torch.arange(H), torch.arange(W))
  grid = torch.stack([grid_x, grid_y], dim=-1).float()

  # Apply the warp to the grid using matrix multiplication
  warped_grid = torch.matmul(grid.reshape(-1, 2), W[:2,:2].T) + W[:2,-1]

  # Reshape the warped grid to match the shape of S
  warped_grid = warped_grid.reshape(H, W, -1)

  # Interpolate S at the warped grid locations using bilinear interpolation and zero padding
  S0 = torch.nn.functional.grid_sample(S.unsqueeze(0), warped_grid.unsqueeze(0), mode='bilinear', padding_mode='zeros').squeeze(0)

  # Return the composed warp
  return S0

# Define the KL divergence function
# The paper uses KL divergence to measure the difference between two probability distributions
def KL_divergence(P, Q):
  # Compute the KL divergence between P and Q
  # Note: we add a small epsilon to avoid numerical issues
  epsilon = 1e-8
  D = torch.sum(P * torch.log((P + epsilon) / (Q + epsilon)), dim=-1)

  # Return the average KL divergence over all pixels
  return torch.mean(D)

# Define the cross entropy function
# The paper uses cross entropy to measure the difference between a probability distribution and a target label
def cross_entropy(P, y):
  # Compute the cross entropy between P and y
  # Note: we add a small epsilon to avoid numerical issues
  epsilon = 1e-8
  E = -torch.log(P[y] + epsilon)

  # Return the average cross entropy over all pixels
  return torch.mean(E)

# Define the unmatched state
# The paper uses an additional state to represent the probability that a pixel has no match in another image
unmatched = 0 # the index of the unmatched state

# Define the hyperparameters
alpha = 1.0 # the weight for the first objective
beta = 1.0 # the weight for the second objective
gamma = 0.1 # the weight for the third objective
lr = 0.001 # the learning rate for gradient descent
epochs = 100 # the number of training epochs

# Initialize the network
F = UCN()

# Initialize the optimizer
optimizer = torch.optim.Adam(F.parameters(), lr=lr)

# Load the training data
# The paper uses PF-PASCAL [11], PF-WILLOW [12], TSS [35], and SPair-71k [36] datasets
# Here we use PF-PASCAL as an example
train_data = load_PF_PASCAL()

# Train the network
for epoch in range(epochs):
  # Shuffle the training data
  random.shuffle(train_data)

  # Loop over the training data in batches
  for batch in train_data:
    # Get a pair of images I and J depicting the same object class from the batch
    I, J = batch

    # Generate a new image I0 by applying a random warp W to I
    I0, W = warp(I)

    # Compute the network output S0 = F(I0, J)
    S0 = F(I0, J)

    # Compute the network output S = F(I, J)
    S = F(I, J)

    # Compute the first objective: enforce S0 to be equal to S composed with W
    L1 = KL_divergence(S0, compose(S, W))

    # Compute the second objective: enforce S0 to be equal to W
    L2 = KL_divergence(S0, W)

    # Sample an image A depicting a different object class from I and J
    A = sample_different_class(I, J)

    # Compute the network output SA = F(I, A)
    SA = F(I, A)

    # Compute the third objective: enforce SA to have a high unmatched probability
    L3 = cross_entropy(SA, unmatched)

    # Compute the total loss function as a weighted sum of the objectives
    L = alpha * L1 + beta * L2 + gamma * L3

    # Update the network parameters by minimizing the loss function using gradient descent
    optimizer.zero_grad()
    L.backward()
    optimizer.step()

  # Print the epoch and the loss value
  print(f"Epoch {epoch}, Loss {L.item()}")
```