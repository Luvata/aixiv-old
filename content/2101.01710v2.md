---
title: 2101.01710v2 Learning Accurate Dense Correspondences and When to Trust Them
date: 2021-01-02
---

# [Learning Accurate Dense Correspondences and When to Trust Them](http://arxiv.org/abs/2101.01710v2)

authors: Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte


## What, Why and How

[1]: https://arxiv.org/abs/2101.01710 "Learning Accurate Dense Correspondences and When to Trust Them"
[2]: https://arxiv.org/pdf/2102.01710v2.pdf "1 arXiv:2102.01710v2 [astro-ph.GA] 7 May 2021"
[3]: http://export.arxiv.org/abs/2302.01710v2 "[2302.01710v2] Geometric Programming Problems with Triangular and ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a method to estimate a dense flow field between two images and a pixel-wise confidence map indicating the reliability of the matches. The method uses a probabilistic approach that jointly learns the flow prediction and its uncertainty, and a self-supervised training strategy.
- **Why**: The paper aims to address the problem of inaccurate or unreliable dense flow estimation in the case of large displacements or homogeneous regions, which can affect many applications and tasks that rely on geometric matching or optical flow.
- **How**: The paper develops a flexible probabilistic model that parametrizes the predictive distribution as a constrained mixture model, which can better capture both accurate flow predictions and outliers. The paper also designs an architecture and a loss function that are tailored for robust and generalizable uncertainty prediction in the context of self-supervised training, where ground-truth flow labels are not available. The paper evaluates the method on multiple challenging datasets and shows that it obtains state-of-the-art results and useful confidence estimation.

## Main Contributions

According to the paper at , the main contributions are:

- A novel probabilistic model for dense flow estimation and confidence prediction that can handle both accurate matches and outliers.
- A self-supervised training strategy that leverages a novel loss function and an architecture that are designed for uncertainty estimation.
- A comprehensive evaluation of the proposed method on multiple datasets and tasks, demonstrating its effectiveness and usefulness.

## Method Summary

[1]: https://arxiv.org/abs/2101.01710 "Learning Accurate Dense Correspondences and When to Trust Them"
[2]: https://arxiv.org/abs/2011.01710v2 "[2011.01710v2] Single Shot Reversible GAN for BCG artifact removal in ..."
[3]: http://export.arxiv.org/abs/2302.01710v2 "[2302.01710v2] Geometric Programming Problems with Triangular and ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a **probabilistic model** for dense flow estimation and confidence prediction that parametrizes the predictive distribution as a **constrained mixture model**. The mixture model consists of two components: a **normal distribution** that models the accurate flow predictions and a **uniform distribution** that models the outliers. The mixture model is constrained by two assumptions: (1) the variance of the normal component is lower-bounded by a small positive value to avoid degenerate solutions; and (2) the uniform component covers the entire output space to account for any possible outlier. The paper derives the **maximum likelihood estimation** (MLE) of the mixture model parameters and shows that it can be computed efficiently by a closed-form solution.
- The paper also proposes a **self-supervised training strategy** that leverages a novel loss function and an architecture that are designed for uncertainty estimation. The loss function consists of two terms: a **data term** that measures the consistency between the predicted flow and the warped image, and a **regularization term** that penalizes large flow magnitudes and encourages smoothness. The data term is weighted by the predicted confidence map, which allows the model to focus on reliable regions and ignore outliers. The architecture consists of two sub-networks: a **flow network** that predicts the flow field and a **confidence network** that predicts the confidence map. The flow network is based on PWC-Net [^2^][2], while the confidence network is based on U-Net [^3^][3]. The two sub-networks share the same feature extractor to reduce computational cost and improve feature alignment.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pair of images I1 and I2
# Output: a dense flow field F and a confidence map C

# Define the flow network based on PWC-Net
flow_network = PWCNet()

# Define the confidence network based on U-Net
confidence_network = UNet()

# Extract features from the input images using a shared feature extractor
F1 = feature_extractor(I1)
F2 = feature_extractor(I2)

# Predict the flow field from the extracted features using the flow network
F = flow_network(F1, F2)

# Warp the second image to the first image using the predicted flow field
I2_warp = warp(I2, F)

# Predict the confidence map from the warped image and the first image using the confidence network
C = confidence_network(I2_warp, I1)

# Define the data term as the weighted L1 distance between the warped image and the first image
data_term = C * L1(I2_warp, I1)

# Define the regularization term as the sum of flow magnitude and smoothness penalties
regularization_term = lambda * (L1(F) + smoothness(F))

# Define the total loss as the sum of data term and regularization term
loss = data_term + regularization_term

# Optimize the loss using gradient descent
optimize(loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a pair of images I1 and I2
# Output: a dense flow field F and a confidence map C

# Define the flow network based on PWC-Net
flow_network = PWCNet()
# The flow network consists of six levels of feature pyramids and six levels of flow estimation modules
# Each flow estimation module consists of a correlation layer, a convolutional layer, a deconvolutional layer, and a refinement layer
# The correlation layer computes the cost volume between the feature maps of the two images at each level
# The convolutional layer processes the cost volume and outputs a coarse flow field
# The deconvolutional layer upsamples the coarse flow field to the next level
# The refinement layer refines the upsampled flow field using residual connections and skip connections

# Define the confidence network based on U-Net
confidence_network = UNet()
# The confidence network consists of an encoder-decoder architecture with skip connections
# The encoder consists of four blocks of convolutional layers with stride 2 and leaky ReLU activation
# The decoder consists of four blocks of deconvolutional layers with stride 2 and ReLU activation
# Each block in the decoder concatenates the output of the corresponding block in the encoder before applying the deconvolutional layer
# The final output of the decoder is a sigmoid activation to produce a confidence map in [0, 1]

# Extract features from the input images using a shared feature extractor
F1 = feature_extractor(I1)
F2 = feature_extractor(I2)
# The feature extractor consists of two convolutional layers with stride 2 and leaky ReLU activation
# The output is a six-level feature pyramid for each image

# Predict the flow field from the extracted features using the flow network
F = flow_network(F1, F2)
# The flow network takes the feature pyramids as input and outputs a six-level flow pyramid
# The final output is the finest level of the flow pyramid, which has the same resolution as the input images

# Warp the second image to the first image using the predicted flow field
I2_warp = warp(I2, F)
# The warp function applies a bilinear interpolation to sample the pixels from the second image according to the flow field

# Predict the confidence map from the warped image and the first image using the confidence network
C = confidence_network(I2_warp, I1)
# The confidence network takes the warped image and the first image as input and outputs a confidence map
# The confidence map has the same resolution as the input images and indicates the reliability of each pixel in the flow field

# Define the data term as the weighted L1 distance between the warped image and the first image
data_term = C * L1(I2_warp, I1)
# The data term measures the consistency between the warped image and the first image
# The data term is weighted by the confidence map, which allows the model to focus on reliable regions and ignore outliers

# Define the regularization term as the sum of flow magnitude and smoothness penalties
regularization_term = lambda * (L1(F) + smoothness(F))
# The regularization term penalizes large flow magnitudes and encourages smoothness in the flow field
# The smoothness penalty is computed as the L1 norm of the gradient of the flow field
# The regularization term is scaled by a hyperparameter lambda

# Define the total loss as the sum of data term and regularization term
loss = data_term + regularization_term
# The total loss is a combination of data fidelity and prior knowledge

# Optimize the loss using gradient descent
optimize(loss)
# The optimize function updates the parameters of both sub-networks using gradient descent with Adam optimizer
```