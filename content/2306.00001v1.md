---
title: 2306.00001v1 TinyissimoYOLO  A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers
date: 2023-06-01
---

# [TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers](http://arxiv.org/abs/2306.00001v1)

authors: Julian Moosmann, Marco Giordano, Christian Vogt, Michele Magno


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00001.pdf "TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object ..."
[2]: https://arxiv.org/abs/2306.00001 "[2306.00001] TinyissimoYOLO: A Quantized, Low-Memory ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00001 "[2306.00001] TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper introduces a new object detection network called **TinyissimoYOLO**, which is quantized, memory-efficient, and ultra-lightweight. It can run on microcontrollers with less than 0.5MB memory and achieve high frame-rate and low energy consumption[^1^][1] [^2^][2].
- **Why**: The paper aims to enable object detection on edge devices such as microcontrollers, which have the benefits of reducing detection latency, increasing energy efficiency, and enhancing privacy by avoiding data transmission[^1^][1].
- **How**: The paper proposes a quantized network architecture with 422k parameters, which is based on the idea of YOLO but significantly reduced in size and complexity. The network is trained using quantization-aware training and deployed with 8-bit quantization on different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and on the MAX78000's CNN accelerator. The paper evaluates the performance of the network on various metrics, such as accuracy, frame-rate, energy consumption, and inference efficiency[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- It introduces **TinyissimoYOLO**, a highly flexible, quantized, memory-efficient, and ultra-lightweight object detection network for microcontrollers.
- It demonstrates that TinyissimoYOLO can achieve real-time object detection on embedded microcontrollers with high frame-rate and low energy consumption, and can exploit CNN accelerators.
- It shows that TinyissimoYOLO can be trained for any multi-object detection with up to 3 classes, and can handle different input resolutions and aspect ratios.
- It compares TinyissimoYOLO with other state-of-the-art object detection networks on microcontrollers, such as TinyYOLOv3 and SqueezeDet, and shows that TinyissimoYOLO outperforms them in terms of accuracy, speed, and energy efficiency.

## Method Summary

The method section of the paper describes the design and implementation of TinyissimoYOLO. It consists of four subsections:

- **Network Architecture**: This subsection explains the structure and parameters of TinyissimoYOLO, which is composed of 10 convolutional layers and 2 fully connected layers. It also describes the quantization scheme and the output format of the network, which predicts bounding boxes and class probabilities for each grid cell in the input image.
- **Quantization-Aware Training**: This subsection describes the training process of TinyissimoYOLO, which uses a quantization-aware training (QAT) technique to simulate the effects of quantization during training and to minimize the accuracy loss due to quantization. It also explains the loss function and the hyperparameters used for training.
- **Deployment on Microcontrollers**: This subsection describes the deployment process of TinyissimoYOLO on different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and MAX78000. It also discusses the challenges and solutions for deploying quantized networks on microcontrollers, such as memory allocation, data conversion, and hardware acceleration.
- **Dataset and Evaluation Metrics**: This subsection describes the dataset and the evaluation metrics used for testing TinyissimoYOLO. The dataset is a subset of the COCO dataset, which contains images of three classes: person, car, and dog. The evaluation metrics include mean average precision (mAP), frame-rate (fps), energy consumption (J), and inference efficiency (MAC/Cycle).

## Pseudo Code

Here is the detailed pseudo code to implement this paper:

```python
# Define the network architecture
# Input: image of size 64x64x3
# Output: tensor of size 8x8x(5*B+C), where B is the number of bounding boxes per grid cell and C is the number of classes
# Quantization: 8-bit fixed-point arithmetic

# Convolutional layer 1: 3x3 kernel, 16 filters, stride 1, padding 1, ReLU activation
conv1 = conv2d(image, kernel1, stride=1, padding=1)
conv1 = relu(conv1)
conv1 = quantize(conv1)

# Convolutional layer 2: 3x3 kernel, 32 filters, stride 2, padding 1, ReLU activation
conv2 = conv2d(conv1, kernel2, stride=2, padding=1)
conv2 = relu(conv2)
conv2 = quantize(conv2)

# Convolutional layer 3: 3x3 kernel, 64 filters, stride 1, padding 1, ReLU activation
conv3 = conv2d(conv2, kernel3, stride=1, padding=1)
conv3 = relu(conv3)
conv3 = quantize(conv3)

# Convolutional layer 4: 3x3 kernel, 128 filters, stride 2, padding 1, ReLU activation
conv4 = conv2d(conv3, kernel4, stride=2, padding=1)
conv4 = relu(conv4)
conv4 = quantize(conv4)

# Convolutional layer 5: 3x3 kernel, 256 filters, stride 1, padding 1, ReLU activation
conv5 = conv2d(conv4, kernel5, stride=1, padding=1)
conv5 = relu(conv5)
conv5 = quantize(conv5)

# Convolutional layer 6: 3x3 kernel, 512 filters, stride 2, padding 1, ReLU activation
conv6 = conv2d(conv5, kernel6, stride=2, padding=1)
conv6 = relu(conv6)
conv6 = quantize(conv6)

# Convolutional layer 7: 3x3 kernel, 1024 filters, stride 1, padding 1, ReLU activation
conv7 = conv2d(conv6, kernel7, stride=1, padding=1)
conv7 = relu(conv7)
conv7 = quantize(conv7)

# Convolutional layer 8: 3x3 kernel, (5*B+C) filters, stride 1, padding 0
output = conv2d(conv7,kernel8,stride=1,padding=0)

# Fully connected layer 9: output size (8*8*(5*B+C))
output = flatten(output)

# Fully connected layer 10: output size (8*8*(5*B+C))
output = linear(output)

# Define the loss function
# Input: output tensor and ground truth tensor of size (8*8*(5*B+C))
# Output: scalar loss value

# Split the output tensor and the ground truth tensor into three parts: bounding box coordinates (x,y,w,h), objectness score (p), and class probabilities (c)
output_xywh = output[:,:,:,0:4*B]
output_p = output[:,:,:,4*B:5*B]
output_c = output[:,:,:,5*B:]

truth_xywh = truth[:,:,:,0:4*B]
truth_p = truth[:,:,:,4*B:5*B]
truth_c = truth[:,:,:,5*B:]

# Compute the coordinate loss using mean squared error
coord_loss = mse(output_xywh * truth_p , truth_xywh * truth_p)

# Compute the objectness loss using binary cross entropy
obj_loss = bce(output_p , truth_p)

# Compute the class loss using categorical cross entropy
class_loss = cce(output_c * truth_p , truth_c * truth_p)

# Compute the total loss as a weighted sum of the three losses
total_loss = lambda_coord * coord_loss + lambda_obj * obj_loss + lambda_class * class_loss

# Define the training process
# Input: training dataset of images and labels
# Output: trained network parameters

# Initialize the network parameters randomly
initialize(network)

# Loop over the training dataset for a fixed number of epochs
for epoch in range(epochs):

    # Shuffle the training dataset
    shuffle(dataset)

    # Loop over the batches of images and labels in the dataset
    for batch in dataset:

        # Forward pass: compute the network output and the loss
        output = network(batch.images)
        loss = loss_function(output, batch.labels)

        # Backward pass: compute the gradients of the network parameters with respect to the loss
        gradients = backward(loss, network)

        # Update the network parameters using stochastic gradient descent with a fixed learning rate
        network = network - learning_rate * gradients

        # Quantize the network parameters to 8-bit fixed-point values
        network = quantize(network)

# Define the deployment process
# Input: trained network parameters and test image
# Output: detected objects and their bounding boxes and class probabilities

# Load the trained network parameters from a file
network = load(file)

# Quantize the test image to 8-bit fixed-point values
image = quantize(image)

# Forward pass: compute the network output
output = network(image)

# Split the output tensor into three parts: bounding box coordinates (x,y,w,h), objectness score (p), and class probabilities (c)
output_xywh = output[:,:,:,0:4*B]
output_p = output[:,:,:,4*B:5*B]
output_c = output[:,:,:,5*B:]

# Apply a threshold to the objectness score to filter out low-confidence predictions
mask = output_p > threshold

# Apply non-maximum suppression to the bounding boxes to eliminate overlapping predictions
boxes = nms(output_xywh * mask)

# Apply softmax to the class probabilities to obtain normalized scores
scores = softmax(output_c * mask)

# Return the detected objects and their bounding boxes and class probabilities
return boxes, scores
```