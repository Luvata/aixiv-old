---
title: 1805.04487v1 Non-Stationary Texture Synthesis by Adversarial Expansion
date: 2018-05-05
---

# [Non-Stationary Texture Synthesis by Adversarial Expansion](http://arxiv.org/abs/1805.04487v1)

authors: Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, Hui Huang


## What, Why and How

[1]: https://arxiv.org/abs/1805.04487v1 "Non-Stationary Texture Synthesis by Adversarial Expansion"
[2]: https://arxiv.org/pdf/1805.04487v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1805.04487v1 "[1805.04487v1] Non-Stationary Texture Synthesis by Adversarial Expansion"

Here is a summary of the paper:

- **What**: The paper proposes a new approach for example-based non-stationary texture synthesis using a generative adversarial network (GAN).
- **Why**: The paper aims to cope with challenging textures that have large-scale structures, spatially variant and inhomogeneous attributes, which are common in the real world but difficult for existing methods to handle.
- **How**: The paper trains a GAN to double the spatial extent of texture blocks extracted from a specific texture exemplar. The trained generator can then expand the size of the entire exemplar or any of its sub-blocks, capturing the non-stationary characteristics of the input. The paper demonstrates the effectiveness of the approach on various examples and compares it with existing methods.

## Main Contributions

According to the paper, the main contributions are:

- A novel GAN-based framework for non-stationary texture synthesis that can handle textures with large-scale structures and spatially variant attributes.
- A simple yet effective training strategy that uses a single texture exemplar as both the source and the target for the GAN.
- A comprehensive evaluation of the proposed method on various challenging textures and a comparison with state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/abs/1805.04487 "Non-Stationary Texture Synthesis by Adversarial Expansion"
[2]: https://arxiv.org/pdf/1805.04487v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1805.04487v1 "[1805.04487v1] Non-Stationary Texture Synthesis by Adversarial Expansion"

Here is a summary of the method section of the paper:

- The paper presents a GAN-based framework for non-stationary texture synthesis that consists of a fully convolutional generator G and a patch-based discriminator D.
- The generator G takes a texture block of size W×W as input and outputs a texture block of size 2W×2W that preserves the non-stationary characteristics of the input. The generator is composed of several residual blocks followed by upsampling layers.
- The discriminator D takes a texture block of size 2W×2W as input and outputs a scalar value indicating whether the input is real or fake. The discriminator is composed of several convolutional layers followed by fully connected layers.
- The paper trains the GAN on a single texture exemplar by extracting texture blocks of size W×W and 2W×2W from it. The paper uses a combination of adversarial loss and feature matching loss to optimize the GAN.
- The paper uses a post-processing step to stitch the generated texture blocks into a seamless output texture. The paper uses Poisson blending to smooth the seams between the blocks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator G
G = FullyConvolutionalGenerator()

# Define the discriminator D
D = PatchBasedDiscriminator()

# Define the adversarial loss and feature matching loss
adv_loss = BinaryCrossEntropyLoss()
fm_loss = MeanSquaredErrorLoss()

# Load the texture exemplar
exemplar = LoadImage("exemplar.jpg")

# Train the GAN
for epoch in range(num_epochs):
  # Sample texture blocks of size WxW and 2Wx2W from the exemplar
  real_small = SampleBlocks(exemplar, W)
  real_large = SampleBlocks(exemplar, 2W)

  # Generate texture blocks of size 2Wx2W from the small blocks
  fake_large = G(real_small)

  # Compute the discriminator outputs for real and fake blocks
  real_score = D(real_large)
  fake_score = D(fake_large)

  # Compute the generator and discriminator losses
  g_loss = adv_loss(fake_score, 1) + fm_loss(fake_large, real_large)
  d_loss = adv_loss(real_score, 1) + adv_loss(fake_score, 0)

  # Update the generator and discriminator parameters
  UpdateParameters(G, g_loss)
  UpdateParameters(D, d_loss)

# Synthesize a large output texture
output = SynthesizeTexture(G, exemplar)

# Post-process the output texture to remove seams
output = PoissonBlending(output)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the generator G
class FullyConvolutionalGenerator(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the residual block
    self.res_block = torch.nn.Sequential(
      torch.nn.Conv2d(64, 64, 3, padding=1),
      torch.nn.BatchNorm2d(64),
      torch.nn.ReLU(),
      torch.nn.Conv2d(64, 64, 3, padding=1),
      torch.nn.BatchNorm2d(64)
    )
    # Define the upsampling block
    self.upsample_block = torch.nn.Sequential(
      torch.nn.Upsample(scale_factor=2, mode="nearest"),
      torch.nn.Conv2d(64, 64, 3, padding=1),
      torch.nn.BatchNorm2d(64),
      torch.nn.ReLU()
    )
    # Define the first and last convolutional layers
    self.conv1 = torch.nn.Conv2d(3, 64, 9, padding=4)
    self.conv2 = torch.nn.Conv2d(64, 3, 9, padding=4)
    # Define the activation function
    self.tanh = torch.nn.Tanh()

  def forward(self, x):
    # Apply the first convolutional layer
    x = self.conv1(x)
    # Save the output for skip connection
    skip = x
    # Apply four residual blocks
    for i in range(4):
      x = self.res_block(x) + x
    # Apply two upsampling blocks
    for i in range(2):
      x = self.upsample_block(x)
    # Add the skip connection
    x = x + skip
    # Apply the last convolutional layer
    x = self.conv2(x)
    # Apply the activation function
    x = self.tanh(x)
    return x

# Define the discriminator D
class PatchBasedDiscriminator(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the convolutional block with leaky ReLU activation
    def conv_block(in_channels, out_channels, kernel_size, stride):
      return torch.nn.Sequential(
        torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride),
        torch.nn.BatchNorm2d(out_channels),
        torch.nn.LeakyReLU(0.2)
      )
    # Define the convolutional layers
    self.conv1 = conv_block(3, 64, 3, 1)
    self.conv2 = conv_block(64, 128, 3, 2)
    self.conv3 = conv_block(128, 256, 3, 2)
    self.conv4 = conv_block(256, 512, 3, 2)
    # Define the fully connected layers
    self.fc1 = torch.nn.Linear(512 * 8 * 8, 1024)
    self.fc2 = torch.nn.Linear(1024, 1)
    # Define the sigmoid function
    self.sigmoid = torch.nn.Sigmoid()

  def forward(self, x):
    # Apply the convolutional layers
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    # Flatten the output
    x = x.view(-1, 512 * 8 * 8)
    # Apply the fully connected layers
    x = self.fc1(x)
    x = self.fc2(x)
    # Apply the sigmoid function
    x = self.sigmoid(x)
    return x

# Define the adversarial loss and feature matching loss functions
adv_loss = torch.nn.BCELoss()
fm_loss = torch.nn.MSELoss()

# Create the generator and discriminator models
G = FullyConvolutionalGenerator()
D = PatchBasedDiscriminator()

# Create the optimizer for the generator and discriminator
g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001)
d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0001)

# Load the texture exemplar and resize it to a multiple of WxW (e.g. W=32)
exemplar = cv2.imread("exemplar.jpg")
exemplar_height = exemplar.shape[0]
exemplar_width = exemplar.shape[1]
exemplar_height_new = (exemplar_height // W) * W
exemplar_width_new = (exemplar_width // W) * W
exemplar = cv2.resize(exemplar, (exemplar_width_new, exemplar_height_new))

# Convert the exemplar to a torch tensor and normalize it to [-1, 1]
exemplar = torch.from_numpy(exemplar).permute(2, 0, 1).float() / 127.5 - 1.0
exemplar = exemplar.unsqueeze(0)

# Train the GAN
for epoch in range(num_epochs):
  # Sample texture blocks of size WxW and 2Wx2W from the exemplar
  real_small = torchvision.transforms.RandomCrop(W)(exemplar)
  real_large = torchvision.transforms.RandomCrop(2 * W)(exemplar)

  # Generate texture blocks of size 2Wx2W from the small blocks
  fake_large = G(real_small)

  # Compute the discriminator outputs for real and fake blocks
  real_score = D(real_large)
  fake_score = D(fake_large)

  # Compute the generator and discriminator losses
  g_loss = adv_loss(fake_score, torch.ones(fake_score.size())) + fm_loss(fake_large, real_large)
  d_loss = adv_loss(real_score, torch.ones(real_score.size())) + adv_loss(fake_score, torch.zeros(fake_score.size()))

  # Update the generator and discriminator parameters
  g_optimizer.zero_grad()
  g_loss.backward()
  g_optimizer.step()

  d_optimizer.zero_grad()
  d_loss.backward()
  d_optimizer.step()

# Synthesize a large output texture by tiling the exemplar and expanding each tile
output_height = exemplar_height_new * 2
output_width = exemplar_width_new * 2
output = torch.zeros(3, output_height, output_width)
for i in range(0, output_height, W):
  for j in range(0, output_width, W):
    # Extract a tile of size WxW from the exemplar
    tile = exemplar[:, :, i:i+W, j:j+W]
    # Expand the tile to size 2Wx2W using the generator
    expanded_tile = G(tile)
    # Paste the expanded tile to the output texture
    output[:, i:i+2*W, j:j+2*W] = expanded_tile

# Post-process the output texture to remove seams using Poisson blending
output = output.squeeze(0).permute(1, 2, 0).numpy() * 127.5 + 127.5
output = output.astype(np.uint8)
output = cv2.seamlessClone(output, exemplar[0].permute(1, 2, 0).numpy() * 127.5 + 127.5, np.ones(output.shape[:2], np.uint8) * 255, (output_width // 2, output_height // 2), cv2.NORMAL_CLONE)
```