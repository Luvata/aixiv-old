---
title: 1907.06571v2 Adversarial Video Generation on Complex Datasets
date: 2019-07-07
---

# [Adversarial Video Generation on Complex Datasets](http://arxiv.org/abs/1907.06571v2)

authors: Aidan Clark, Jeff Donahue, Karen Simonyan


## What, Why and How

[1]: https://arxiv.org/pdf/1907.06571.pdf "Jeff Donahue jeffdonahue@google.com Karen Simonyan arXiv:1907.06571v2 ..."
[2]: https://arxiv.org/abs/1907.06571 "[1907.06571] Adversarial Video Generation on Complex Datasets - arXiv.org"
[3]: http://export.arxiv.org/pdf/1902.06571 "Hiroyuki Kitamoto Yoshihisa Kitazawa arXiv:1902.06571v2 [hep-th] 1 May 2019"

Here is a possible summary of the paper:

- **What**: The paper proposes a generative model of natural video called DVD-GAN, which uses a dual video discriminator to train on large and complex datasets such as Kinetics-600.
- **Why**: The paper aims to extend the success of generative image models to the video domain, and to produce high-quality video samples at high resolutions and lengths.
- **How**: The paper builds upon the BigGAN architecture and introduces a spatio-temporal decomposition of the discriminator, which consists of a frame-level discriminator and a video-level discriminator. The paper evaluates the model on the tasks of video synthesis and video prediction, and compares it with existing methods on various metrics such as Fréchet Inception Distance and Inception Score. The paper also shows some qualitative examples of generated videos with interesting behaviors[^1^][1].

## Main Contributions

According to the paper, the contributions are as follows:

- The paper proposes DVD-GAN – a scalable generative model of natural video which produces high-quality samples at resolutions up to 256 256 and lengths up to 48 frames.
- The paper introduces an efficient spatio-temporal decomposition of the discriminator, which allows the model to train on Kinetics-600 – a complex dataset of natural videos an order of magnitude larger than other commonly used datasets.
- The paper achieves new state-of-the-art Fréchet Inception Distance for prediction for Kinetics-600, as well as state-of-the-art Inception Score for synthesis on the UCF-101 dataset, alongside establishing a strong baseline for synthesis on Kinetics-600.

## Method Summary

[1]: https://arxiv.org/pdf/1907.06571.pdf "Jeff Donahue jeffdonahue@google.com Karen Simonyan arXiv:1907.06571v2 ..."
[2]: https://arxiv.org/abs/1907.06571 "[1907.06571] Adversarial Video Generation on Complex Datasets - arXiv.org"
[3]: http://export.arxiv.org/pdf/1902.06571 "Hiroyuki Kitamoto Yoshihisa Kitazawa arXiv:1902.06571v2 [hep-th] 1 May 2019"

Here is a possible summary of the method section of the paper:

- The paper adopts the BigGAN architecture (Brock et al., 2019) as the backbone of the generative model, which consists of a generator G and a discriminator D. The generator takes a latent vector z and a class label y as inputs and outputs a video x. The discriminator takes a video x and a class label y as inputs and outputs a scalar score indicating the likelihood of x being real or fake.
- The paper modifies the discriminator to have two components: a frame-level discriminator Df and a video-level discriminator Dv. The frame-level discriminator operates on individual frames of the video and computes a score for each frame. The video-level discriminator operates on the entire video and computes a single score for the video. The final score of the discriminator is the sum of the scores from both components.
- The paper uses hinge loss for both the generator and the discriminator, following (Brock et al., 2019). The paper also uses spectral normalization (Miyato et al., 2018) and self-attention (Zhang et al., 2019) for both components of the discriminator. The paper also uses orthogonal regularization (Brock et al., 2019) for the generator.
- The paper trains the model on Kinetics-600 (Carreira et al., 2018), a large-scale dataset of natural videos with 600 action classes. The paper uses videos of length 48 frames and resolutions of 64 64, 128 128, and 256 256. The paper uses Adam optimizer (Kingma & Ba, 2015) with learning rate of 0.0002 and batch size of 64 for all resolutions. The paper trains the model for up to one million iterations.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the generator G and the discriminator D
G = BigGAN(z_dim, y_dim, x_dim)
D = DualDiscriminator(y_dim, x_dim)

# Define the hinge loss function
def hinge_loss(real_score, fake_score):
  return max(0, 1 - real_score) + max(0, 1 + fake_score)

# Define the orthogonal regularization function
def ortho_reg(W):
  return (W.T @ W - I).norm()

# Define the hyperparameters
lr = 0.0002 # learning rate
beta1 = 0 # Adam beta1
beta2 = 0.999 # Adam beta2
lambda_ortho = 1e-4 # weight for orthogonal regularization
n_iter = 1000000 # number of iterations
n_critic = 5 # number of discriminator updates per generator update

# Initialize the optimizer
optimizer_G = Adam(G.parameters(), lr=lr, betas=(beta1, beta2))
optimizer_D = Adam(D.parameters(), lr=lr, betas=(beta1, beta2))

# Load the Kinetics-600 dataset
dataset = Kinetics600()

# Train the model
for i in range(n_iter):
  # Sample a batch of latent vectors and class labels
  z = sample_z(batch_size, z_dim)
  y = sample_y(batch_size, y_dim)

  # Generate a batch of fake videos
  x_fake = G(z, y)

  # Sample a batch of real videos and class labels
  x_real, y_real = dataset.sample(batch_size)

  # Update the discriminator
  for _ in range(n_critic):
    # Compute the scores for real and fake videos
    real_score = D(x_real, y_real)
    fake_score = D(x_fake.detach(), y)

    # Compute the hinge loss for the discriminator
    loss_D = hinge_loss(real_score, fake_score)

    # Backpropagate and update the discriminator parameters
    optimizer_D.zero_grad()
    loss_D.backward()
    optimizer_D.step()

  # Update the generator
  # Compute the score for fake videos
  fake_score = D(x_fake, y)

  # Compute the hinge loss for the generator
  loss_G = -fake_score.mean()

  # Add the orthogonal regularization for the generator
  for W in G.parameters():
    if W.dim() > 2:
      loss_G += lambda_ortho * ortho_reg(W)

  # Backpropagate and update the generator parameters
  optimizer_G.zero_grad()
  loss_G.backward()
  optimizer_G.step()
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the generator G based on the BigGAN architecture
class Generator(nn.Module):
  def __init__(self, z_dim, y_dim, x_dim):
    super(Generator, self).__init__()
    # Define the latent vector and class label embedding layers
    self.z_embed = nn.Linear(z_dim, 16 * 16 * 16)
    self.y_embed = nn.Embedding(y_dim, 128)

    # Define the generator blocks
    # Each block consists of upsample, convolution, batch norm, conditional batch norm, and activation layers
    # The conditional batch norm layer takes the class label embedding as an additional input and modulates the batch norm statistics
    self.block1 = nn.Sequential(
      nn.Upsample(scale_factor=2),
      nn.Conv3d(16, 8, kernel_size=3, padding=1),
      nn.BatchNorm3d(8),
      nn.utils.spectral_norm(nn.Conv1d(128, 8 * 2)),
      nn.ReLU()
    )
    self.block2 = nn.Sequential(
      nn.Upsample(scale_factor=2),
      nn.Conv3d(8, 4, kernel_size=3, padding=1),
      nn.BatchNorm3d(4),
      nn.utils.spectral_norm(nn.Conv1d(128, 4 * 2)),
      nn.ReLU()
    )
    self.block3 = nn.Sequential(
      nn.Upsample(scale_factor=2),
      nn.Conv3d(4, 2, kernel_size=3, padding=1),
      nn.BatchNorm3d(2),
      nn.utils.spectral_norm(nn.Conv1d(128, 2 * 2)),
      nn.ReLU()
    )
    self.block4 = nn.Sequential(
      nn.Upsample(scale_factor=2),
      nn.Conv3d(2, x_dim[0], kernel_size=3, padding=1),
      nn.Tanh()
    )

    # Define the self-attention layer
    # The self-attention layer computes the attention map between different spatial locations of the feature map
    # The attention map is used to weight the feature map and produce an output feature map of the same size
    self.attn = SelfAttention(x_dim[0])

  def forward(self, z, y):
    # Embed the latent vector and class label
    z = self.z_embed(z)
    y = self.y_embed(y)

    # Reshape the latent vector to a 5D tensor
    z = z.view(-1, 16, 16, 4, 4)

    # Apply the generator blocks
    x = self.block1(z, y)
    x = self.block2(x, y)
    x = self.block3(x, y)
    
    # Apply the self-attention layer
    x = self.attn(x)

    # Apply the final generator block
    x = self.block4(x)

    return x

# Define the dual discriminator D based on the BigGAN architecture
class DualDiscriminator(nn.Module):
  def __init__(self, y_dim, x_dim):
    super(DualDiscriminator, self).__init__()
    
    # Define the frame-level discriminator Df
    # The frame-level discriminator consists of convolutional layers that operate on individual frames of the video
    # The output is a feature map for each frame that is averaged across spatial dimensions to produce a score for each frame
    self.Df = nn.Sequential(
      nn.Conv3d(x_dim[0], 2, kernel_size=3, stride=2, padding=1),
      nn.LeakyReLU(0.2),
      SelfAttention(2),
      nn.Conv3d(2, 4, kernel_size=3, stride=2, padding=1),
      nn.BatchNorm3d(4),
      nn.LeakyReLU(0.2),
      SelfAttention(4),
      nn.Conv3d(4, 8, kernel_size=3, stride=2, padding=1),
      nn.BatchNorm3d(8),
      nn.LeakyReLU(0.2),
      SelfAttention(8),
      nn.Conv3d(8, 16, kernel_size=3, stride=2, padding=1),
      nn.BatchNorm3d(16),
      nn.LeakyReLU(0.2)
    )

    # Define the video-level discriminator Dv
    # The video-level discriminator consists of convolutional layers that operate on the entire video
    # The output is a feature map for the video that is averaged across spatial and temporal dimensions to produce a single score for the video
    self.Dv = nn.Sequential(
      nn.Conv3d(x_dim[0], 2, kernel_size=3, stride=2, padding=1),
      nn.LeakyReLU(0.2),
      SelfAttention(2),
      nn.Conv3d(2, 4, kernel_size=3, stride=2, padding=1),
      nn.BatchNorm3d(4),
      nn.LeakyReLU(0.2),
      SelfAttention(4),
      nn.Conv3d(4, 8, kernel_size=3, stride=2, padding=1),
      nn.BatchNorm3d(8),
      nn.LeakyReLU(0.2),
      SelfAttention(8),
      nn.Conv3d(8, 16, kernel_size=3, stride=2, padding=1),
      nn.BatchNorm3d(16),
      nn.LeakyReLU(0.2)
    )

    # Define the class label embedding layer
    self.y_embed = nn.Embedding(y_dim, 16 * 2)

    # Define the projection layer
    # The projection layer takes the class label embedding and the feature map as inputs and computes a dot product
    # The projection layer is used to condition the discriminator on the class label
    self.proj = nn.utils.spectral_norm(nn.Linear(16 * 2, 1))

  def forward(self, x, y):
    # Embed the class label
    y = self.y_embed(y)

    # Apply the frame-level discriminator
    x_f = self.Df(x)

    # Apply the video-level discriminator
    x_v = self.Dv(x)

    # Average the feature maps across spatial dimensions
    x_f = x_f.mean([2, 3, 4])
    x_v = x_v.mean([1, 2, 3, 4])

    # Apply the projection layer
    x_f = self.proj(x_f * y)
    x_v = self.proj(x_v * y)

    # Sum the scores from both discriminators
    x = x_f + x_v

    return x

# Define the self-attention layer
class SelfAttention(nn.Module):
  def __init__(self, in_channels):
    super(SelfAttention, self).__init__()
    
    # Define the query, key, and value convolutional layers
    # The query and key layers have half of the input channels as output channels
    # The value layer has the same number of input and output channels
    self.query = nn.utils.spectral_norm(nn.Conv3d(in_channels, in_channels // 2, kernel_size=1))
    self.key = nn.utils.spectral_norm(nn.Conv3d(in_channels, in_channels // 2, kernel_size=1))
    self.value = nn.utils.spectral_norm(nn.Conv3d(in_channels, in_channels, kernel_size=1))

    # Define the scaling factor for the attention map
    self.gamma = nn.Parameter(torch.zeros(1))

  def forward(self, x):
    # Get the batch size and the number of channels
    batch_size, channels, _, _, _ = x.size()

    # Apply the query layer and reshape the output to a 2D matrix
    query = self.query(x).view(batch_size, channels // 2, -1)

    # Apply the key layer and reshape the output to a 2D matrix
    key = self.key(x).view(batch_size, channels // 2, -1)

    # Compute the attention map by matrix multiplication and scaling
    attn = torch.bmm(query.permute(0, 2, 1), key)
    attn = F.softmax(attn / (channels ** 0.5), dim=-1)

    # Apply the value layer and reshape the output to a 2D matrix
    value = self.value(x).view(batch_size, channels, -1)

    # Compute the output feature map by matrix multiplication and reshaping
    out = torch.bmm(value, attn.permute(0 ,2 ,1))
    out = out.view(batch_size, channels, *x.shape[2:])

    # Add a residual connection and scale by gamma
    out = self.gamma * out + x

    return out

# Define the hinge loss function
def hinge_loss(real_score, fake_score):
  return torch.max(torch.zeros_like(real_score), 1 - real_score).mean() + torch.max(torch.zeros_like(fake_score), 1 + fake_score).mean()

# Define the orthogonal regularization function
def ortho_reg(W):
  return (W.T @ W - torch.eye(W.size(0))).norm()

# Define the hyperparameters
z_dim = 120 # latent vector dimension
y_dim = 600 # class label dimension