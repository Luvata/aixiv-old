---
title: 2212.12185v2 Implementation of a Blind navigation method in outdoors/indoors areas
date: 2022-12-13
---

# [Implementation of a Blind navigation method in outdoors/indoors areas](http://arxiv.org/abs/2212.12185v2)

authors: Mohammad Javadian Farzaneh, Hossein Mahvash Mohammadi


## What, Why and How

[1]: https://arxiv.org/abs/2212.12185 "[2212.12185] Implementation of a Blind navigation method in outdoors ..."
[2]: https://arxiv.org/pdf/2212.12185 "A Blind navigation method in indoors/outdoors areas - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2212.12185v2 "[2212.12185v2] Implementation of a Blind navigation method in outdoors ..."

Here is a summary of the paper:

- **What**: The paper proposes a navigation system for visually impaired people based on visual slam and Yolo algorithm using monocular cameras[^1^][1].
- **Why**: The paper aims to provide a safe and efficient navigation system that can create a map from a predefined route, detect obstacles, estimate obstacle distance, detect path deviation, and predict next step[^1^][1].
- **How**: The paper uses the ORB-SLAM algorithm to create a map from a predefined route that a visually impaired person most uses. It also uses the Yolo algorithm to detect obstacles and classify them into three categories: static, dynamic, and human. It then estimates the obstacle distance using a depth estimation network and a stereo matching algorithm. It also detects the path deviation using a line detection algorithm and predicts the next step using a recurrent neural network[^1^][1].

## Main Contributions

The paper claims to have the following contributions:

- It proposes a novel navigation system that combines visual slam and Yolo algorithm using monocular cameras for visually impaired people.
- It introduces a new obstacle detection method that can classify obstacles into three categories and estimate their distance using a depth estimation network and a stereo matching algorithm.
- It presents a new path-following method that can detect the path deviation and predict the next step using a line detection algorithm and a recurrent neural network.
- It evaluates the proposed system on various indoor and outdoor scenarios and shows its effectiveness and robustness.

## Method Summary

The method section of the paper describes the proposed navigation system in detail. It consists of three main parts: map creation, obstacle detection, and path-following.

- Map creation: The paper uses the ORB-SLAM algorithm to create a map from a predefined route that a visually impaired person most uses. The algorithm uses a monocular camera to extract and match features from consecutive frames and estimate the camera pose and the 3D map points. The algorithm also performs loop closure detection and global bundle adjustment to correct the drift and optimize the map.
- Obstacle detection: The paper uses the Yolo algorithm to detect obstacles in the camera frame and classify them into three categories: static, dynamic, and human. The algorithm uses a convolutional neural network to predict bounding boxes and class probabilities for each object. The paper also uses a depth estimation network and a stereo matching algorithm to estimate the distance of each obstacle from the camera. The depth estimation network takes a single image as input and outputs a depth map. The stereo matching algorithm takes two images from different viewpoints as input and outputs a disparity map. The paper then converts the depth map and the disparity map into distance values using calibration parameters.
- Path-following: The paper uses a line detection algorithm and a recurrent neural network to detect the path deviation and predict the next step for the visually impaired person. The line detection algorithm uses the Hough transform to find straight lines in the camera frame that represent the edges of the path. The algorithm then calculates the angle between the camera direction and the path direction and sends it to the recurrent neural network. The recurrent neural network takes the angle as input and outputs a voice command that tells the visually impaired person how to adjust their direction. The recurrent neural network is trained on a dataset of voice commands and angles collected from human volunteers.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Initialize the ORB-SLAM algorithm
orb_slam = ORB_SLAM()

# Initialize the Yolo algorithm
yolo = YOLO()

# Initialize the depth estimation network
depth_net = DepthNet()

# Initialize the stereo matching algorithm
stereo_match = StereoMatch()

# Initialize the line detection algorithm
line_detect = LineDetect()

# Initialize the recurrent neural network
rnn = RNN()

# Loop until the end of the route
while not end_of_route:

  # Get the current camera frame
  frame = get_camera_frame()

  # Perform visual slam and update the map
  orb_slam.update(frame)

  # Detect obstacles and classify them
  obstacles = yolo.detect(frame)

  # Estimate the distance of each obstacle
  for obstacle in obstacles:

    # Get the bounding box of the obstacle
    bbox = obstacle.bbox

    # Crop the frame according to the bounding box
    cropped_frame = crop(frame, bbox)

    # Estimate the depth map using the depth estimation network
    depth_map = depth_net.predict(cropped_frame)

    # Estimate the disparity map using the stereo matching algorithm
    disparity_map = stereo_match.predict(cropped_frame)

    # Convert the depth map and the disparity map into distance values
    distance = convert_to_distance(depth_map, disparity_map)

    # Assign the distance to the obstacle
    obstacle.distance = distance

  # Detect the path deviation using the line detection algorithm
  angle = line_detect.detect(frame)

  # Predict the next step using the recurrent neural network
  voice_command = rnn.predict(angle)

  # Output the voice command to the visually impaired person
  output(voice_command)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import cv2
import numpy as np
import torch
import tensorflow as tf

# Define the parameters for the ORB-SLAM algorithm
ORB_FEATURES = 1000 # Number of features to extract
ORB_SCALE = 1.2 # Scale factor between levels in the scale pyramid
ORB_LEVELS = 8 # Number of levels in the scale pyramid
ORB_MATCH_RATIO = 0.75 # Ratio test threshold for feature matching
ORB_LOOP_THRESHOLD = 0.7 # Similarity threshold for loop closure detection
ORB_BA_ITERATIONS = 20 # Number of iterations for global bundle adjustment

# Define the parameters for the Yolo algorithm
YOLO_ANCHORS = [[10,13], [16,30], [33,23], [30,61], [62,45], [59,119], [116,90], [156,198], [373,326]] # Anchor boxes for each scale
YOLO_MASKS = [[6,7,8], [3,4,5], [0,1,2]] # Masks for each scale
YOLO_CLASSES = 80 # Number of classes to predict
YOLO_THRESHOLDS = [0.6, 0.6, 0.6] # Confidence thresholds for each scale
YOLO_NMS_THRESHOLD = 0.5 # Non-maximum suppression threshold

# Define the parameters for the depth estimation network
DEPTH_INPUT_SIZE = (224, 224) # Input size for the network
DEPTH_OUTPUT_SIZE = (112, 112) # Output size for the network
DEPTH_MODEL_PATH = "depth_net.pth" # Path to the pretrained model

# Define the parameters for the stereo matching algorithm
STEREO_WINDOW_SIZE = 15 # Window size for block matching
STEREO_MIN_DISPARITY = 0 # Minimum possible disparity value
STEREO_NUM_DISPARITIES = 64 # Number of disparities to consider
STEREO_UNIQUENESS_RATIO = 15 # Margin in percentage by which the best cost function value should win the second best value to consider the found match correct

# Define the parameters for the line detection algorithm
LINE_RHO = 1 # Distance resolution of the accumulator in pixels
LINE_THETA = np.pi/180 # Angle resolution of the accumulator in radians
LINE_THRESHOLD = 50 # Accumulator threshold parameter. Only those lines are returned that get enough votes (> threshold)
LINE_MIN_LENGTH = 100 # Minimum length of line. Line segments shorter than that are rejected.
LINE_MAX_GAP = 10 # Maximum allowed gap between points on the same line to link them.

# Define the parameters for the recurrent neural network
RNN_INPUT_SIZE = 1 # Input size for the network (angle)
RNN_HIDDEN_SIZE = 32 # Hidden size for the network
RNN_NUM_LAYERS = 2 # Number of layers for the network
RNN_OUTPUT_SIZE = 5 # Output size for the network (voice command)
RNN_MODEL_PATH = "rnn.pth" # Path to the pretrained model

# Define the calibration parameters for converting depth and disparity to distance
FOCAL_LENGTH = 500 # Focal length of the camera in pixels
BASELINE = 50 # Baseline distance between two cameras in millimeters
PIXEL_SIZE = 0.01 # Pixel size in millimeters

# Define the voice commands for each output class of the recurrent neural network
VOICE_COMMANDS = ["Go straight", "Turn left", "Turn right", "Stop", "Slow down"]

# Define a class for ORB-SLAM algorithm
class ORB_SLAM:

  def __init__(self):

    # Initialize an ORB feature detector and descriptor extractor
    self.orb = cv2.ORB_create(nfeatures=ORB_FEATURES, scaleFactor=ORB_SCALE, nlevels=ORB_LEVELS)

    # Initialize a brute-force feature matcher with Hamming distance metric
    self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING)

    # Initialize a list to store the keyframes and their features and descriptors
    self.keyframes = []

    # Initialize a list to store the map points and their coordinates and observations
    self.map_points = []

    # Initialize a variable to store the current camera pose as a 4x4 transformation matrix
    self.pose = np.eye(4)

    # Initialize a variable to store the previous camera pose as a 4x4 transformation matrix
    self.prev_pose = np.eye(4)

    # Initialize a variable to store the loop closure candidate as an index of keyframes list
    self.loop_candidate = None

  def update(self, frame):

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Extract features and descriptors from the frame
    keypoints, descriptors = self.orb.detectAndCompute(gray, None)

    # If there are no keyframes, add the first frame as a keyframe
    if len(self.keyframes) == 0:

      # Add the frame and its features and descriptors to the keyframes list
      self.keyframes.append((frame, keypoints, descriptors))

      # Return without updating the map or the pose
      return

    # Otherwise, match the current frame with the last keyframe
    else:

      # Get the last keyframe and its features and descriptors
      last_keyframe, last_keypoints, last_descriptors = self.keyframes[-1]

      # Match the descriptors of the current frame and the last keyframe
      matches = self.matcher.knnMatch(descriptors, last_descriptors, k=2)

      # Apply the ratio test to filter out the good matches
      good_matches = []
      for m, n in matches:
        if m.distance < ORB_MATCH_RATIO * n.distance:
          good_matches.append(m)

      # If there are not enough good matches, return without updating the map or the pose
      if len(good_matches) < 10:
        return

      # Otherwise, estimate the relative pose of the current frame and the last keyframe using the good matches
      else:

        # Get the matched keypoints from both frames
        src_pts = np.float32([keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([last_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)

        # Estimate a homography matrix using RANSAC algorithm
        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)

        # Convert the homography matrix to a 4x4 transformation matrix
        T = np.eye(4)
        T[:2,:3] = H

        # Update the current pose by multiplying the relative pose with the previous pose
        self.pose = self.prev_pose @ T

        # Check if the current frame is a keyframe by measuring its distance from the last keyframe
        dist = np.linalg.norm(self.pose[:3,3] - self.prev_pose[:3,3])

        # If the distance is larger than a threshold, add the current frame as a keyframe
        if dist > 10:

          # Add the frame and its features and descriptors to the keyframes list
          self.keyframes.append((frame, keypoints, descriptors))

          # Update the map points by triangulating the matched keypoints from both frames
          self.update_map_points(src_pts, dst_pts, mask)

          # Perform loop closure detection and correction if possible
          self.loop_closure()

          # Perform global bundle adjustment to optimize the map and the poses
          self.global_bundle_adjustment()

          # Update the previous pose with the current pose
          self.prev_pose = self.pose

  def update_map_points(self, src_pts, dst_pts, mask):

    # Get the camera matrix from the focal length and principal point
    K = np.array([[FOCAL_LENGTH, 0, gray.shape[1]/2], [0, FOCAL_LENGTH, gray.shape[0]/2], [0, 0, 1]])

    # Get the rotation and translation vectors from the current pose
    rvec = cv2.Rodrigues(self.pose[:3,:3])[0]
    tvec = self.pose[:3,3]

    # Triangulate the matched keypoints from both frames using their poses and camera matrix
    points_4d = cv2.triangulatePoints(K @ np.eye(3,4), K @ np.hstack((rvec,tvec)), dst_pts[mask.ravel()==1], src_pts[mask.ravel()==1])

    # Convert the homogeneous coordinates to 3D coordinates
    points_3d = points_4d[:3] / points_4d[3]

    # Add the 3D points and their observations to the map points list
    for point in points_3d.T:
      self.map_points.append((point, len(self.keyframes)-2, len(self.keyframes)-1))

  def loop_closure(self):

    # If there are less than 10 keyframes, return without loop closure detection
    if len(self.keyframes) < 10:
      return

    # Otherwise, compare the current keyframe with all previous keyframes except the last one using feature matching
    else:

      # Get the current keyframe and its features and descriptors
      curr_keyframe, curr_keypoints, curr_descriptors = self.keyframes[