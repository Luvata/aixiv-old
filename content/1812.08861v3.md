---
title: 1812.08861v3 Animating Arbitrary Objects via Deep Motion Transfer
date: 2018-12-09
---

# [Animating Arbitrary Objects via Deep Motion Transfer](http://arxiv.org/abs/1812.08861v3)

authors: Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe


## What, Why and How

[1]: https://arxiv.org/abs/1812.08861 "[1812.08861] Animating Arbitrary Objects via Deep Motion Transfer"
[2]: https://arxiv.org/pdf/1812.08861v3.pdf "arXiv.org e-Print archive"
[3]: https://www.researcher-app.com/paper/3482916 "Animating Arbitrary Objects via Deep Motion Transfer. (arXiv:1812.08"

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces a novel deep learning framework for image animation. Given an input image with a target object and a driving video sequence depicting a moving object, the framework generates a video in which the target object is animated according to the driving sequence.
- **Why**: The paper aims to address the limitations of existing image animation and video generation methods, such as requiring paired data, being restricted to specific object categories, or producing unrealistic or distorted results.
- **How**: The framework consists of three main modules: (i) a Keypoint Detector unsupervisely trained to extract object keypoints, (ii) a Dense Motion prediction network for generating dense heatmaps from sparse keypoints, in order to better encode motion information and (iii) a Motion Transfer Network, which uses the motion heatmaps and appearance information extracted from the input image to synthesize the output frames. The paper evaluates the framework on several benchmark datasets and shows that it outperforms state-of-the-art methods in terms of visual quality and diversity.

## Main Contributions

According to the paper, the main contributions are:

- A novel deep learning framework for image animation that can handle arbitrary objects and does not require paired data or pre-defined object categories.
- A novel unsupervised keypoint detector that can learn to extract object keypoints without any supervision or prior knowledge.
- A novel dense motion prediction network that can generate dense motion heatmaps from sparse keypoints and capture complex motion patterns.
- A novel motion transfer network that can synthesize realistic and diverse output frames using motion heatmaps and appearance information.
- Extensive experiments on several benchmark datasets that demonstrate the effectiveness and superiority of the proposed framework over existing methods.

## Method Summary

The method section of the paper describes the three main modules of the proposed framework: the keypoint detector, the dense motion prediction network, and the motion transfer network.

- The keypoint detector is a convolutional neural network that takes an input image and outputs a set of keypoints that represent the salient parts of the object. The keypoint detector is trained in an unsupervised manner using a reconstruction loss and a diversity loss. The reconstruction loss ensures that the keypoints can be used to reconstruct the input image using a generator network. The diversity loss encourages the keypoints to be distinct and evenly distributed over the object.
- The dense motion prediction network is a convolutional neural network that takes two sets of keypoints (one from the source image and one from the driving video) and outputs a dense motion heatmap for each keypoint. The motion heatmap represents the displacement of each pixel in the source image relative to the corresponding keypoint. The dense motion prediction network is trained using a cycle-consistency loss and a perceptual loss. The cycle-consistency loss ensures that the motion heatmaps can be used to warp the source image to match the driving video and vice versa. The perceptual loss measures the similarity between the warped images and the original images in terms of high-level features.
- The motion transfer network is a convolutional neural network that takes the source image, the driving video, and the motion heatmaps as inputs and outputs a synthesized video. The motion transfer network uses an encoder-decoder architecture with skip connections and attention modules. The encoder extracts appearance features from the source image and motion features from the driving video and the motion heatmaps. The decoder combines the appearance and motion features to generate realistic and diverse output frames. The motion transfer network is trained using an adversarial loss and a perceptual loss. The adversarial loss ensures that the output frames are indistinguishable from real frames by a discriminator network. The perceptual loss measures the similarity between the output frames and the driving video in terms of high-level features.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: source image I_s, driving video V_d
# Output: synthesized video V_s

# Initialize keypoint detector K, dense motion prediction network M, motion transfer network T
# Train K, M, T using the losses described in the paper

# For each frame F_d in V_d:
  # Extract keypoints from I_s and F_d using K
  # K_s = K(I_s)
  # K_d = K(F_d)
  
  # Generate motion heatmaps from K_s and K_d using M
  # H = M(K_s, K_d)
  
  # Synthesize output frame F_s from I_s, F_d, and H using T
  # F_s = T(I_s, F_d, H)
  
  # Append F_s to V_s
  # V_s.append(F_s)

# Return V_s
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import models, transforms
from PIL import Image
import numpy as np
import cv2

# Define hyperparameters
num_keypoints = 10 # number of keypoints to extract
num_channels = 3 # number of channels in input images and videos
image_size = 256 # size of input images and videos
num_frames = 32 # number of frames in driving video
batch_size = 16 # size of mini-batch for training
num_epochs = 100 # number of epochs for training
lr = 0.0002 # learning rate for optimizer
beta1 = 0.5 # beta1 for optimizer
beta2 = 0.999 # beta2 for optimizer
lambda_rec = 10 # weight for reconstruction loss
lambda_div = 0.01 # weight for diversity loss
lambda_cyc = 10 # weight for cycle-consistency loss
lambda_adv = 1 # weight for adversarial loss

# Define keypoint detector network K
class KeypointDetector(nn.Module):
  def __init__(self):
    super(KeypointDetector, self).__init__()
    # Use a pre-trained VGG-16 model as feature extractor
    self.vgg = models.vgg16(pretrained=True).features[:23]
    # Freeze the parameters of the feature extractor
    for param in self.vgg.parameters():
      param.requires_grad = False
    # Define a convolutional layer to output keypoints as heatmaps
    self.conv = nn.Conv2d(512, num_keypoints, kernel_size=1)

  def forward(self, x):
    # x: input image of shape (batch_size, num_channels, image_size, image_size)
    # Extract features from input image using VGG-16 model
    features = self.vgg(x) # shape: (batch_size, 512, image_size/16, image_size/16)
    # Output keypoints as heatmaps using convolutional layer
    heatmaps = self.conv(features) # shape: (batch_size, num_keypoints, image_size/16, image_size/16)
    return heatmaps

# Define generator network G for reconstructing input image from keypoints
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define an encoder-decoder architecture with skip connections and upsampling layers
    self.encoder = nn.Sequential(
      nn.Conv2d(num_keypoints, 64, kernel_size=4, stride=2, padding=1),
      nn.LeakyReLU(0.2),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(128),
      nn.LeakyReLU(0.2),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(256),
      nn.LeakyReLU(0.2),
      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(512),
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm