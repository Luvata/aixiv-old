---
title: 2112.14683v4 StyleGAN-V  A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2
date: 2021-12-15
---

# [StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2](http://arxiv.org/abs/2112.14683v4)

authors: Ivan Skorokhodov, Sergey Tulyakov, Mohamed Elhoseiny


## What, Why and How

[1]: https://arxiv.org/pdf/2112.14683v4.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2112.14683 "[2112.14683] StyleGAN-V: A Continuous Video Generator with the Price ..."
[3]: https://arxiv.org/pdf/2112.14683.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: The paper proposes StyleGAN-V, a continuous video generator that extends StyleGAN2 to model videos as continuous signals in time.

Why: The paper aims to overcome the limitations of existing video synthesis frameworks that treat videos as discrete sequences of images, which are computationally expensive and fail to capture long-term temporal patterns.

How: The paper introduces continuous motion representations based on positional embeddings, a sparse training scheme that uses only 2 frames per clip, and a holistic discriminator that aggregates temporal information by concatenating frames' features. The paper evaluates StyleGAN-V on five video synthesis benchmarks and shows that it outperforms previous methods in terms of quality, diversity, and temporal consistency. The paper also demonstrates that StyleGAN-V can generate arbitrarily long videos at arbitrary high frame rate, and can perform spatial manipulations that are propagated in time.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2112.14683v4.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2112.14683 "[2112.14683] StyleGAN-V: A Continuous Video Generator with the Price ..."
[3]: https://arxiv.org/pdf/2112.14683.pdf "arXiv.org e-Print archive"

The paper claims the following contributions:

- A continuous video generator that models videos as continuous signals in time, based on StyleGAN2 architecture.
- A continuous motion representation based on positional embeddings that can capture long-term temporal patterns and appearance transformations.
- A sparse training scheme that can learn a good generator from only 2 frames per clip, reducing the data and computation requirements.
- A holistic discriminator that aggregates temporal information by concatenating frames' features, simplifying the training process and providing richer learning signal to the generator.
- A comprehensive evaluation of StyleGAN-V on five video synthesis benchmarks, showing its superiority over previous methods in terms of quality, diversity, and temporal consistency.
- A demonstration of StyleGAN-V's ability to generate arbitrarily long videos at arbitrary high frame rate, and to perform spatial manipulations that are propagated in time.

## Method Summary

[1]: https://arxiv.org/pdf/2112.14683v4.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2112.14683 "[2112.14683] StyleGAN-V: A Continuous Video Generator with the Price ..."
[3]: https://arxiv.org/pdf/2112.14683.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

```
The method section of the paper consists of four subsections: 3.1 Continuous Motion Representations, 3.2 Sparse Training, 3.3 Holistic Discriminator, and 3.4 Implementation Details.

3.1 Continuous Motion Representations: This subsection introduces the idea of modeling videos as continuous signals in time, and proposes to use positional embeddings to encode the temporal information. The authors define two types of positional embeddings: spatial and temporal. Spatial embeddings are learned by a convolutional network that maps each pixel location to a high-dimensional vector. Temporal embeddings are learned by a fully connected network that maps each time coordinate to a high-dimensional vector. The authors also propose to use a sinusoidal function to modulate the temporal embeddings, which allows them to capture periodic patterns and avoid entanglement with spatial embeddings.

3.2 Sparse Training: This subsection explores the question of how many frames are needed to train a good video generator. The authors argue that using only 2 frames per clip is sufficient, as long as they are sampled from different parts of the video. The authors show that this sparse training scheme reduces the data and computation requirements, and does not degrade the quality or diversity of the generated videos.

3.3 Holistic Discriminator: This subsection rethinks the design of the discriminator for video synthesis. The authors propose to use a holistic discriminator that aggregates temporal information by simply concatenating frames' features along the channel dimension. The authors show that this discriminator simplifies the training process and provides richer learning signal to the generator, as it can capture both frame-level and video-level details.

3.4 Implementation Details: This subsection provides the details of the network architecture, loss function, training procedure, and evaluation metrics used in the paper. The authors build their model on top of StyleGAN2, and modify it to incorporate continuous motion representations, sparse training, and holistic discriminator. The authors use hinge loss for both generator and discriminator, and train their model using Adam optimizer with a batch size of 16 on 8 NVIDIA V100 GPUs for about 10 days. The authors evaluate their model using Fréchet Video Distance (FVD) and Frame-level Fréchet Inception Distance (FID) on five video synthesis benchmarks: UCF-101, VoxCeleb2, LSUN-Church, LSUN-Car, and LSUN-Cat.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2112.14683v4.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2112.14683 "[2112.14683] StyleGAN-V: A Continuous Video Generator with the Price ..."
[3]: https://arxiv.org/pdf/2112.14683.pdf "arXiv.org e-Print archive"

Here is a high-level pseudo code for the paper:

```
# Define the generator network based on StyleGAN2
generator = StyleGAN2()

# Define the spatial and temporal embedding networks
spatial_embedding = ConvNet()
temporal_embedding = FCNet()

# Define the sinusoidal function to modulate the temporal embeddings
sinusoid = lambda x: sin(2 * pi * x)

# Define the holistic discriminator network that concatenates frames' features
discriminator = HolisticDiscriminator()

# Define the hinge loss function for both generator and discriminator
hinge_loss = HingeLoss()

# Define the optimizer for both generator and discriminator
optimizer = Adam()

# Define the batch size and number of training iterations
batch_size = 16
num_iterations = 1000000

# Loop over the training iterations
for i in range(num_iterations):

  # Sample a batch of latent vectors and time coordinates
  z = sample_latent_vectors(batch_size)
  t = sample_time_coordinates(batch_size)

  # Generate a batch of frames using the generator and the embeddings
  x = generator(z + spatial_embedding + sinusoid(temporal_embedding(t)))

  # Sample a batch of real frames from the dataset
  y = sample_real_frames(batch_size)

  # Compute the discriminator outputs for both real and fake frames
  d_real = discriminator(y)
  d_fake = discriminator(x)

  # Compute the generator and discriminator losses using hinge loss
  g_loss = hinge_loss(d_fake, 1)
  d_loss = hinge_loss(d_real, 1) + hinge_loss(d_fake, -1)

  # Update the generator and discriminator parameters using optimizer
  optimizer.step(g_loss, generator.parameters())
  optimizer.step(d_loss, discriminator.parameters())
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2112.14683v4.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2112.14683 "[2112.14683] StyleGAN-V: A Continuous Video Generator with the Price ..."
[3]: https://arxiv.org/pdf/2112.14683.pdf "arXiv.org e-Print archive"

Here is a detailed pseudo code for the paper:

```
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np

# Define the generator network based on StyleGAN2
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the mapping network that maps latent vectors to intermediate latent vectors
    self.mapping = MappingNetwork()
    # Define the synthesis network that generates images from intermediate latent vectors and embeddings
    self.synthesis = SynthesisNetwork()

  def forward(self, z, spatial_embedding, temporal_embedding):
    # Map the latent vectors to intermediate latent vectors using the mapping network
    w = self.mapping(z)
    # Generate images from intermediate latent vectors and embeddings using the synthesis network
    x = self.synthesis(w, spatial_embedding, temporal_embedding)
    return x

# Define the mapping network that maps latent vectors to intermediate latent vectors
class MappingNetwork(nn.Module):
  def __init__(self):
    super(MappingNetwork, self).__init__()
    # Define the number of layers and features in the mapping network
    self.num_layers = 8
    self.num_features = 512
    # Define a list of fully connected layers with LeakyReLU activation
    self.layers = nn.ModuleList()
    for i in range(self.num_layers):
      self.layers.append(nn.Linear(self.num_features, self.num_features))
      self.layers.append(nn.LeakyReLU(0.2))

  def forward(self, z):
    # Normalize the input latent vectors
    z = F.normalize(z, dim=1)
    # Apply the fully connected layers with LeakyReLU activation
    w = z
    for layer in self.layers:
      w = layer(w)
    return w

# Define the synthesis network that generates images from intermediate latent vectors and embeddings
class SynthesisNetwork(nn.Module):
  def __init__(self):
    super(SynthesisNetwork, self).__init__()
    # Define the number of resolution levels and features in the synthesis network
    self.num_levels = 9
    self.num_features = [512, 512, 512, 512, 256, 128, 64, 32, 16]
    # Define a list of synthesis blocks that generate images at different resolutions
    self.blocks = nn.ModuleList()
    for i in range(self.num_levels):
      self.blocks.append(SynthesisBlock(self.num_features[i]))
    # Define a list of upsampling layers that increase the resolution of the images
    self.upsamples = nn.ModuleList()
    for i in range(self.num_levels - 1):
      self.upsamples.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False))
    # Define a list of output convolution layers that produce RGB images at different resolutions
    self.outputs = nn.ModuleList()
    for i in range(self.num_levels):
      self.outputs.append(nn.Conv2d(self.num_features[i], 3, kernel_size=1))

  def forward(self, w, spatial_embedding, temporal_embedding):
    # Initialize the image tensor with zeros
    x = torch.zeros(w.shape[0], self.num_features[0], 4, 4).to(w.device)
    # Loop over the resolution levels
    for i in range(self.num_levels):
      # Generate images at the current resolution using the synthesis block
      x = self.blocks[i](x, w[:, i], spatial_embedding[:, i], temporal_embedding[:, i])
      # Produce RGB images at the current resolution using the output convolution layer
      y = self.outputs[i](x)
      # Upsample the image tensor if not at the highest resolution
      if i < self.num_levels - 1:
        x = self.upsamples[i](x)
      # Add the RGB images to the output list
      if i == 0:
        outputs = y
      else:
        outputs = torch.cat([outputs, y], dim=2)
    return outputs

# Define the synthesis block that generates images at a given resolution
class SynthesisBlock(nn.Module):
  def __init__(self, num_features):
    super(SynthesisBlock, self).__init__()
    # Define the number of features and channels in the synthesis block
    self.num_features = num_features
    self.num_channels = num_features * 2
    # Define two convolution layers with modulation and noise injection
    self.conv1 = ModulatedConv2d(self.num_channels, self.num_channels, kernel_size=3)
    self.conv2 = ModulatedConv2d(self.num_channels, self.num_features, kernel_size=3)
  
  def forward(self, x, w, spatial_embedding, temporal_embedding):
     # Add spatial and temporal embeddings to intermediate latent vectors
     w = w + spatial_embedding + temporal_embedding
     # Apply the first convolution layer with modulation and noise injection
     x = self.conv1(x, w)
     # Apply the second convolution layer with modulation and noise injection
     x = self.conv2(x, w)
     return x

# Define the modulated convolution layer with modulation and noise injection
class ModulatedConv2d(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size):
    super(ModulatedConv2d, self).__init__()
    # Define the number of channels and features in the modulated convolution layer
    self.in_channels = in_channels
    self.out_channels = out_channels
    self.num_features = in_channels // 2
    # Define the convolution layer with equalized learning rate
    self.conv = EqualizedConv2d(in_channels, out_channels, kernel_size, padding=kernel_size // 2)
    # Define the modulation layer that scales the input features by intermediate latent vectors
    self.modulation = ModulationLayer(self.num_features)
    # Define the noise injection layer that adds random noise to the input features
    self.noise_injection = NoiseInjectionLayer()

  def forward(self, x, w):
    # Reshape the input tensor to match the number of channels
    x = x.view(x.shape[0], self.in_channels, x.shape[2], x.shape[3])
    # Apply the modulation layer to scale the input features by intermediate latent vectors
    x = self.modulation(x, w)
    # Apply the convolution layer with equalized learning rate
    x = self.conv(x)
    # Apply the noise injection layer to add random noise to the input features
    x = self.noise_injection(x)
    # Apply the LeakyReLU activation function
    x = F.leaky_relu(x, 0.2)
    return x

# Define the equalized convolution layer that normalizes the weight by its fan-in
class EqualizedConv2d(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, padding=0):
    super(EqualizedConv2d, self).__init__()
    # Define the convolution layer with He initialization
    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)
    nn.init.kaiming_normal_(self.conv.weight)
    # Compute the scale factor based on the fan-in of the weight
    fan_in = np.prod(self.conv.weight.shape[1:])
    self.scale = np.sqrt(2 / fan_in)

  def forward(self, x):
    # Normalize the weight by its fan-in and multiply by the scale factor
    weight = self.conv.weight * self.scale
    # Apply the convolution operation with the normalized weight and zero bias
    x = F.conv2d(x, weight, bias=None, stride=self.conv.stride, padding=self.conv.padding)
    return x

# Define the modulation layer that scales the input features by intermediate latent vectors
class ModulationLayer(nn.Module):
  def __init__(self, num_features):
    super(ModulationLayer, self).__init__()
    # Define the number of features in the modulation layer
    self.num_features = num_features
    # Define a fully connected layer that maps intermediate latent vectors to scaling factors
    self.fc = nn.Linear(self.num_features, self.num_features)

  def forward(self, x, w):
    # Apply the fully connected layer to map intermediate latent vectors to scaling factors
    s = self.fc(w)
    # Add a small constant to avoid zero scaling
    s = s + 1e-8
    # Reshape the scaling factors to match the input features
    s = s.view(s.shape[0], s.shape[1], 1, 1)
    # Scale the input features by multiplying them with scaling factors
    x = x * s
    return x

# Define the noise injection layer that adds random noise to the input features
class NoiseInjectionLayer(nn.Module):
  def __init__(self):
    super(NoiseInjectionLayer, self).__init__()
    # Define a learnable noise strength parameter initialized with zeros
    self.noise_strength = nn.Parameter(torch.zeros(1))

  def forward(self, x):
     # Sample random noise from a normal distribution with the same shape as input features
     noise = torch.randn(x.shape[0], 1, x.shape[2], x.shape[3]).to(x.device)
     # Add noise to input features by multiplying them with noise strength parameter
     x = x + noise * self.noise_strength 
     return x

# Define the spatial embedding network that maps each pixel location to a high-dimensional vector
class SpatialEmbedding(nn.Module):
  def __init__(self):
     super(SpatialEmbedding, self).__init__()
     # Define the number of resolution levels and features in the spatial embedding network 
     self