---
title: 2104.09621v1 Engineering Sketch Generation for Computer-Aided Design
date: 2021-04-10
---

# [Engineering Sketch Generation for Computer-Aided Design](http://arxiv.org/abs/2104.09621v1)

authors: Karl D. D. Willis, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Hang Chu, Yewen Pu


## What, Why and How

[1]: https://arxiv.org/pdf/2104.09621v1.pdf "arXiv:2104.09621v1 [cs.LG] 19 Apr 2021"
[2]: https://arxiv.org/abs/2104.09621 "Engineering Sketch Generation for Computer-Aided Design"
[3]: http://export.arxiv.org/abs/2104.09621 "[2104.09621] Engineering Sketch Generation for Computer-Aided Design"

Here is a summary of the paper:

- **What**: The paper proposes two generative models, **CurveGen** and **TurtleGen**, for engineering sketch generation. Engineering sketches are 2D curves made of geometric primitives that can be used to create 3D parametric CAD models.
- **Why**: The paper tackles the problem of learning based engineering sketch generation as a first step towards synthesis and composition of parametric CAD models. This is a major enabling technology for the automatic generation of solid models with an editable parametric CAD history, which can be applied in various CAD workﬂows such as reverse engineering, auto-completion, and beautiﬁcation.
- **How**: The paper introduces two models that generate curve primitives without the need for a sketch constraint solver and explicitly consider topology for downstream use with constraints and 3D CAD modeling operations. **CurveGen** is a conditional variational autoencoder that generates sketches as sequences of curve primitives. **TurtleGen** is a recurrent neural network that generates sketches as sequences of turtle commands. The paper evaluates the models using human subjects and compares them with the current state-of-the-art for engineering sketch generation[^1^][1].

## Main Contributions

The paper claims the following contributions:

- The first generative models for engineering sketch generation that do not require a sketch constraint solver and explicitly consider topology for downstream use with constraints and 3D CAD modeling operations.
- A novel representation of engineering sketches as sequences of turtle commands that can be easily parsed and executed by a CAD system.
- A perceptual evaluation using human subjects that shows that both CurveGen and TurtleGen produce more realistic engineering sketches when compared with the current state-of-the-art for engineering sketch generation.

## Method Summary

[1]: https://arxiv.org/pdf/2104.09621v1.pdf "arXiv:2104.09621v1 [cs.LG] 19 Apr 2021"
[2]: https://arxiv.org/abs/2104.09621 "Engineering Sketch Generation for Computer-Aided Design"
[3]: http://export.arxiv.org/abs/2104.09621v1 "[2104.09621v1] Engineering Sketch Generation for Computer-Aided Design"

Here is a summary of the method section of the paper:

- The paper introduces two models for engineering sketch generation: **CurveGen** and **TurtleGen**. Both models take as input a sketch category (e.g. bracket, gear, etc.) and output a sequence of tokens that represent a sketch.
- **CurveGen** is a conditional variational autoencoder that generates sketches as sequences of curve primitives (e.g. line, arc, circle, etc.). The encoder consists of a convolutional neural network that encodes an input sketch image into a latent vector. The decoder consists of a recurrent neural network with attention that decodes the latent vector into a sequence of curve tokens. The model is trained using a combination of reconstruction loss and KL divergence loss.
- **TurtleGen** is a recurrent neural network that generates sketches as sequences of turtle commands (e.g. forward, turn, pen up, pen down, etc.). The model consists of an embedding layer, a long short-term memory (LSTM) layer, and a fully connected layer. The model is trained using cross-entropy loss.
- The paper also describes how to convert the output sequences of both models into engineering sketches that can be used by a CAD system. For CurveGen, the paper defines a grammar for parsing the curve tokens and generating curve primitives with parameters. For TurtleGen, the paper defines a set of turtle commands and their corresponding actions for drawing curves on a canvas.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the CurveGen model
class CurveGen(nn.Module):
  def __init__(self):
    # Initialize the encoder and decoder networks
    self.encoder = ConvEncoder()
    self.decoder = RNNDecoder()

  def forward(self, x, y):
    # Encode the input sketch image x into a latent vector z
    z = self.encoder(x)
    # Decode the latent vector z into a sequence of curve tokens y_hat
    y_hat = self.decoder(z)
    # Return the output sequence y_hat
    return y_hat

# Define the TurtleGen model
class TurtleGen(nn.Module):
  def __init__(self):
    # Initialize the embedding, LSTM, and linear layers
    self.embedding = nn.Embedding(num_categories, embedding_dim)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim)
    self.linear = nn.Linear(hidden_dim, num_commands)

  def forward(self, x):
    # Embed the input sketch category x into a vector e
    e = self.embedding(x)
    # Feed the vector e into the LSTM layer and get the hidden state h
    h, _ = self.lstm(e)
    # Feed the hidden state h into the linear layer and get the output logits o
    o = self.linear(h)
    # Return the output logits o
    return o

# Define the function to convert curve tokens into curve primitives
def parse_curve_tokens(tokens):
  # Initialize an empty list of curve primitives
  curves = []
  # Loop through the tokens
  for token in tokens:
    # If the token is a curve type (e.g. line, arc, circle), create a new curve primitive with that type
    if token in curve_types:
      curve = Curve(token)
      curves.append(curve)
    # If the token is a parameter (e.g. length, angle, radius), add it to the current curve primitive
    elif token in parameters:
      curve.add_parameter(token)
  # Return the list of curve primitives
  return curves

# Define the function to convert turtle commands into curves on a canvas
def execute_turtle_commands(commands):
  # Initialize a turtle object with a pen and a canvas
  turtle = Turtle()
  # Loop through the commands
  for command in commands:
    # If the command is forward, move the turtle forward by a given distance and draw a line on the canvas
    if command == "forward":
      turtle.forward(distance)
    # If the command is turn, rotate the turtle by a given angle
    elif command == "turn":
      turtle.turn(angle)
    # If the command is pen up, lift the pen from the canvas
    elif command == "pen up":
      turtle.pen_up()
    # If the command is pen down, put the pen on the canvas
    elif command == "pen down":
      turtle.pen_down()
  # Return the canvas with the drawn curves
  return turtle.canvas

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# Define the hyperparameters
num_categories = 10 # The number of sketch categories
num_tokens = 20 # The number of curve tokens
num_commands = 10 # The number of turtle commands
embedding_dim = 128 # The dimension of the category embedding
hidden_dim = 256 # The dimension of the LSTM hidden state
latent_dim = 64 # The dimension of the latent vector
batch_size = 32 # The batch size for training
num_epochs = 100 # The number of epochs for training
learning_rate = 0.001 # The learning rate for optimization

# Define the ConvEncoder network
class ConvEncoder(nn.Module):
  def __init__(self):
    super(ConvEncoder, self).__init__()
    # Define the convolutional layers with ReLU activation and max pooling
    self.conv1 = nn.Sequential(
      nn.Conv2d(1, 16, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2)
    )
    self.conv2 = nn.Sequential(
      nn.Conv2d(16, 32, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2)
    )
    self.conv3 = nn.Sequential(
      nn.Conv2d(32, 64, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2)
    )
    self.conv4 = nn.Sequential(
      nn.Conv2d(64, 128, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.MaxPool2d(2)
    )
    # Define the linear layers for the mean and log variance of the latent vector
    self.fc_mean = nn.Linear(128 * 8 * 8, latent_dim)
    self.fc_logvar = nn.Linear(128 * 8 * 8, latent_dim)

  def forward(self, x):
    # Apply the convolutional layers to the input sketch image x
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    # Flatten the output of the last convolutional layer
    x = x.view(-1, 128 * 8 * 8)
    # Compute the mean and log variance of the latent vector z
    mean = self.fc_mean(x)
    logvar = self.fc_logvar(x)
    # Return the mean and log variance of z
    return mean, logvar

# Define the RNNDecoder network
class RNNDecoder(nn.Module):
  def __init__(self):
    super(RNNDecoder, self).__init__()
    # Define the embedding layer for the curve tokens
    self.embedding = nn.Embedding(num_tokens, embedding_dim)
    # Define the LSTM layer with attention mechanism
    self.lstm = nn.LSTMCell(embedding_dim + latent_dim, hidden_dim)
    self.attention = nn.Linear(hidden_dim + latent_dim, num_tokens)
    # Define the linear layer for the output logits
    self.linear = nn.Linear(hidden_dim + latent_dim, num_tokens)

  def forward(self, z):
    # Initialize the hidden state and cell state of the LSTM with zeros
    h = torch.zeros(batch_size, hidden_dim)
    c = torch.zeros(batch_size, hidden_dim)
    # Initialize the output sequence y_hat with zeros
    y_hat = torch.zeros(batch_size, num_tokens)
    # Loop through the sequence length
    for t in range(num_tokens):
      # If t is zero, use a special start token as the input token
      if t == 0:
        token = torch.ones(batch_size).long() * start_token_id
      # Otherwise, use the previous output token as the input token
      else:
        token = y_hat[:, t-1].argmax(dim=1).long()
      # Embed the input token into a vector e
      e = self.embedding(token)
      # Concatenate the vector e and the latent vector z into a vector i
      i = torch.cat([e, z], dim=1)
      # Feed the vector i into the LSTM cell and get the new hidden state and cell state h and c
      h, c = self.lstm(i, (h, c))
      # Concatenate the hidden state h and the latent vector z into a vector o
      o = torch.cat([h, z], dim=1)
      # Compute the attention weights a using the vector o and the embedding matrix
      a = torch.softmax(self.attention(o), dim=1)
      # Compute the context vector c using the attention weights a and the embedding matrix
      c = torch.matmul(a, self.embedding.weight)
      # Concatenate the context vector c and the vector o into a vector s
      s = torch.cat([c, o], dim=1)
      # Feed the vector s into the linear layer and get the output logits y_t
      y_t = self.linear(s)
      # Store the output logits y_t in the output sequence y_hat
      y_hat[:, t] = y_t
    # Return the output sequence y_hat
    return y_hat

# Define the CurveGen model
class CurveGen(nn.Module):
  def __init__(self):
    super(CurveGen, self).__init__()
    # Initialize the encoder and decoder networks
    self.encoder = ConvEncoder()
    self.decoder = RNNDecoder()

  def forward(self, x, y):
    # Encode the input sketch image x into a latent vector z
    mean, logvar = self.encoder(x)
    # Sample z from a normal distribution with mean and logvar
    z = torch.randn_like(mean) * torch.exp(logvar / 2) + mean
    # Decode the latent vector z into a sequence of curve tokens y_hat
    y_hat = self.decoder(z)
    # Return the output sequence y_hat, mean, and logvar
    return y_hat, mean, logvar

# Define the TurtleGen model
class TurtleGen(nn.Module):
  def __init__(self):
    super(TurtleGen, self).__init__()
    # Initialize the embedding, LSTM, and linear layers
    self.embedding = nn.Embedding(num_categories, embedding_dim)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim)
    self.linear = nn.Linear(hidden_dim, num_commands)

  def forward(self, x):
    # Embed the input sketch category x into a vector e
    e = self.embedding(x)
    # Feed the vector e into the LSTM layer and get the hidden state h
    h, _ = self.lstm(e)
    # Feed the hidden state h into the linear layer and get the output logits o
    o = self.linear(h)
    # Return the output logits o
    return o

# Define the function to convert curve tokens into curve primitives
def parse_curve_tokens(tokens):
  # Initialize an empty list of curve primitives
  curves = []
  # Loop through the tokens
  for token in tokens:
    # If the token is a curve type (e.g. line, arc, circle), create a new curve primitive with that type
    if token in curve_types:
      curve = Curve(token)
      curves.append(curve)
    # If the token is a parameter (e.g. length, angle, radius), add it to the current curve primitive
    elif token in parameters:
      curve.add_parameter(token)
  # Return the list of curve primitives
  return curves

# Define the function to convert turtle commands into curves on a canvas
def execute_turtle_commands(commands):
  # Initialize a turtle object with a pen and a canvas
  turtle = Turtle()
  # Loop through the commands
  for command in commands:
    # If the command is forward, move the turtle forward by a given distance and draw a line on the canvas
    if command == "forward":
      turtle.forward(distance)
    # If the command is turn, rotate the turtle by a given angle
    elif command == "turn":
      turtle.turn(angle)
    # If the command is pen up, lift the pen from the canvas
    elif command == "pen up":
      turtle.pen_up()
    # If the command is pen down, put the pen on the canvas
    elif command == "pen down":
      turtle.pen_down()
  # Return the canvas with the drawn curves
  return turtle.canvas

# Define the loss functions for CurveGen and TurtleGen models

# For CurveGen, use a combination of reconstruction loss and KL divergence loss

def reconstruction_loss(y_hat, y):
  # Compute the cross-entropy loss between the output sequence y_hat and the target sequence y
  return nn.CrossEntropyLoss()(y_hat.view(-1, num_tokens), y.view(-1))

def kl_divergence_loss(mean, logvar):
  # Compute the KL divergence loss between a normal distribution with mean and logvar and a standard normal distribution
  return -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())

def curvegen_loss(y_hat, y, mean, logvar):
  # Compute the total loss as a weighted sum of reconstruction loss and KL divergence loss
  return reconstruction_loss(y_hat, y) + kl_divergence_loss(mean, logvar)

#