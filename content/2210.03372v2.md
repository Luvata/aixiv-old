---
title: 2210.03372v2 Pre-trained Adversarial Perturbations
date: 2022-10-04
---

# [Pre-trained Adversarial Perturbations](http://arxiv.org/abs/2210.03372v2)

authors: Yuanhao Ban, Yinpeng Dong


## What, Why and How

[1]: https://arxiv.org/abs/2210.03372v2 "[2210.03372v2] Pre-trained Adversarial Perturbations - arXiv.org"
[2]: https://arxiv.org/abs/2210.03372 "[2210.03372] Pre-trained Adversarial Perturbations - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2110.03372v2 "[2110.03372v2] Unifying Likelihood-free Inference with Black-box ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper introduces Pre-trained Adversarial Perturbations (PAPs), which are universal perturbations crafted for the pre-trained models to maintain the effectiveness when attacking fine-tuned ones without any knowledge of the downstream tasks.
- **Why**: The paper aims to improve the robustness of pre-trained models, which are widely used for various downstream tasks, but lack the robustness to adversarial examples, which can also invoke security issues to pre-trained models.
- **How**: The paper proposes a Low-Level Layer Lifting Attack (L4A) method to generate effective PAPs by lifting the neuron activations of low-level layers of the pre-trained models. The paper also uses an enhanced noise augmentation strategy to generate more transferable PAPs against fine-tuned models. The paper evaluates the proposed method on typical pre-trained vision models and ten downstream tasks and shows that it improves the attack success rate by a large margin compared with state-of-the-art methods.

## Main Contributions

According to the paper, the main contributions are:

- The paper is the first to study the robustness of pre-trained models by introducing Pre-trained Adversarial Perturbations (PAPs), which can attack fine-tuned models without any knowledge of the downstream tasks.
- The paper proposes a novel Low-Level Layer Lifting Attack (L4A) method to generate effective PAPs by lifting the neuron activations of low-level layers of the pre-trained models, which can exploit the inherent vulnerability of pre-trained models and enhance the transferability of PAPs.
- The paper also proposes an enhanced noise augmentation strategy to further improve the transferability of PAPs by adding random noise to the input images during the generation process, which can increase the diversity and robustness of PAPs.
- The paper conducts extensive experiments on typical pre-trained vision models and ten downstream tasks and demonstrates that the proposed method outperforms state-of-the-art methods by a large margin in terms of attack success rate.

## Method Summary

The method section of the paper consists of three subsections: Pre-trained Adversarial Perturbations (PAPs), Low-Level Layer Lifting Attack (L4A), and Enhanced Noise Augmentation. Here is a summary of each subsection:

- Pre-trained Adversarial Perturbations (PAPs): The paper defines PAPs as universal perturbations that can be added to any input image to fool a pre-trained model and its fine-tuned versions on different downstream tasks. The paper formulates the generation of PAPs as an optimization problem that maximizes the expected loss of the pre-trained model over a set of input images, subject to a norm constraint on the perturbation.
- Low-Level Layer Lifting Attack (L4A): The paper proposes a novel method to generate effective PAPs by lifting the neuron activations of low-level layers of the pre-trained model, such as the first convolutional layer. The paper argues that low-level layers contain more general and transferable features that are shared across different downstream tasks, and thus lifting their activations can exploit the inherent vulnerability of pre-trained models and enhance the transferability of PAPs. The paper uses a gradient-based method to lift the activations by maximizing their L2 norm, while minimizing the cosine similarity between the original and perturbed activations.
- Enhanced Noise Augmentation: The paper also proposes an enhanced noise augmentation strategy to further improve the transferability of PAPs by adding random noise to the input images during the generation process. The paper argues that adding noise can increase the diversity and robustness of PAPs by making them less sensitive to small variations in the input images. The paper uses Gaussian noise with zero mean and a small standard deviation, and adjusts the noise level according to the norm of the perturbation.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a pre-trained model f, a set of input images X, a perturbation norm bound epsilon, a noise level sigma, a low-level layer index l
# Output: a pre-trained adversarial perturbation v

# Initialize v to zero
v = 0

# Repeat until convergence
while not converged:

  # Sample a batch of images from X
  x_batch = sample(X)

  # Add random noise to the images
  x_batch = x_batch + np.random.normal(0, sigma, x_batch.shape)

  # Compute the loss of the pre-trained model on the perturbed images
  loss = f(x_batch + v)

  # Compute the gradient of the loss with respect to v
  grad_v = gradient(loss, v)

  # Update v using gradient ascent
  v = v + learning_rate * grad_v

  # Project v to the norm bound epsilon
  v = project(v, epsilon)

  # Lift the activations of the low-level layer l of the pre-trained model on the perturbed images
  activations = f(x_batch + v)[l]

  # Compute the gradient of the L2 norm of the activations with respect to v
  grad_v = gradient(np.linalg.norm(activations), v)

  # Update v using gradient ascent
  v = v + learning_rate * grad_v

  # Project v to the norm bound epsilon
  v = project(v, epsilon)

# Return v
return v
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torchvision

# Define the hyperparameters
epsilon = 10 # The perturbation norm bound
sigma = 0.01 # The noise level
l = 0 # The low-level layer index
batch_size = 32 # The batch size
learning_rate = 0.1 # The learning rate
max_iter = 1000 # The maximum number of iterations

# Load the pre-trained model and set it to evaluation mode
model = torchvision.models.resnet50(pretrained=True)
model.eval()

# Load the input images and normalize them to [0, 1]
X = load_images()
X = X / 255

# Initialize the perturbation to zero
v = torch.zeros_like(X[0])

# Repeat until convergence or maximum iterations
for i in range(max_iter):

  # Sample a batch of images from X
  x_batch = X[np.random.choice(len(X), batch_size)]

  # Add random noise to the images
  x_batch = x_batch + torch.randn_like(x_batch) * sigma

  # Compute the loss of the pre-trained model on the perturbed images
  logits = model(x_batch + v)
  loss = torch.nn.CrossEntropyLoss()(logits, torch.argmax(logits, dim=1))

  # Compute the gradient of the loss with respect to v
  grad_v = torch.autograd.grad(loss, v)[0]

  # Update v using gradient ascent
  v = v + learning_rate * grad_v

  # Project v to the norm bound epsilon
  v = torch.sign(v) * torch.min(torch.abs(v), epsilon)

  # Lift the activations of the low-level layer l of the pre-trained model on the perturbed images
  activations = list(model.children())[l](x_batch + v)

  # Compute the gradient of the L2 norm of the activations with respect to v
  grad_v = torch.autograd.grad(torch.norm(activations), v)[0]

  # Update v using gradient ascent
  v = v + learning_rate * grad_v

  # Project v to the norm bound epsilon
  v = torch.sign(v) * torch.min(torch.abs(v), epsilon)

# Return v
return v
```