---
title: 2303.17591v1 Forget-Me-Not  Learning to Forget in Text-to-Image Diffusion Models
date: 2023-03-18
---

# [Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.17591v1)

authors: Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, Humphrey Shi


## What, Why and How

[1]: https://arxiv.org/pdf/2303.17591 "arXiv:2303.17591v1 [cs.CV] 30 Mar 2023"
[2]: https://arxiv.org/abs/2303.17591 "[2303.17591] Forget-Me-Not: Learning to Forget in Text-to-Image ..."
[3]: http://export.arxiv.org/abs/2303.17591v1 "[2303.17591v1] Forget-Me-Not: Learning to Forget in Text-to-Image ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes Forget-Me-Not, a method to remove specified concepts (such as IDs, objects, or styles) from a text-to-image model without affecting its ability to generate other content.
- **Why**: The paper addresses the unlearning problem of deep learning models, which is a prevalent issue in the industry due to privacy, copyright, and safety concerns. The paper aims to develop a safe and inclusive generative model that can forget unwanted or harmful concepts on demand.
- **How**: The paper introduces the Memorization Score (M-Score) and ConceptBench to measure the models' capacity to generate general concepts. The paper also presents two practical extensions of Forget-Me-Not: concept correction and disentanglement, and a lightweight model patch for Stable Diffusion. The paper evaluates Forget-Me-Not on various text-to-image models and demonstrates its effectiveness and efficiency.

## Main Contributions

The paper claims to make the following contributions:

- A novel method to remove specified concepts from a text-to-image model in as little as 30 seconds, without impairing its ability to generate other content.
- A new metric (M-Score) and a new benchmark (ConceptBench) to measure the models' capacity to generate general concepts.
- Two practical extensions of the method: concept correction and disentanglement, and a lightweight model patch for Stable Diffusion.
- An extensive evaluation of the method on various text-to-image models and concepts, showing its effectiveness and efficiency.

## Method Summary

[1]: https://arxiv.org/pdf/2303.17591v1.pdf "arXiv:2303.17591v1 [cs.CV] 30 Mar 2023"
[2]: https://arxiv.org/abs/2303.17591v1 "[2303.17591v1] Forget-Me-Not: Learning to Forget in Text-to-Image ..."
[3]: http://export.arxiv.org/abs/2303.17591v1 "[2303.17591v1] Forget-Me-Not: Learning to Forget in Text-to-Image ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper presents Forget-Me-Not, a method that consists of two steps: concept identification and concept removal.
- Concept identification is the process of finding the most relevant tokens in the text prompt that correspond to the target concept to be removed. The paper uses a cross-attention mechanism to compute the attention scores between the text tokens and the image features, and selects the top-k tokens with the highest scores as the concept identifiers.
- Concept removal is the process of erasing the influence of the concept identifiers from the text-to-image model. The paper proposes two strategies for concept removal: masking and noise injection. Masking replaces the concept identifiers with a special token, while noise injection adds random noise to the embeddings of the concept identifiers. The paper compares the two strategies and shows that noise injection performs better in terms of preserving other concepts and image quality.
- The paper also introduces two extensions of Forget-Me-Not: concept correction and disentanglement, and a lightweight model patch for Stable Diffusion. Concept correction and disentanglement allows the user to replace or modify a concept in the text prompt with another one, while preserving other concepts. A lightweight model patch for Stable Diffusion enables the user to apply Forget-Me-Not on top of an existing text-to-image model without retraining or fine-tuning, and allows for multi-concept manipulation and convenient distribution.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a text prompt T and a target concept C to be removed
# Output: a generated image I without C

# Step 1: Concept identification
# Compute the cross-attention scores between T and I
scores = cross_attention(T, I)
# Select the top-k tokens in T that have the highest scores as concept identifiers
identifiers = select_top_k(scores, k)
# Step 2: Concept removal
# Choose a strategy for concept removal: masking or noise injection
strategy = choose_strategy()
# Apply the strategy to the concept identifiers
modified_T = apply_strategy(T, identifiers, strategy)
# Generate a new image I' from the modified text prompt
I' = generate_image(modified_T)
# Return I' as the output
return I'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a text prompt T and a target concept C to be removed
# Output: a generated image I without C

# Step 1: Concept identification
# Assume T is a sequence of tokens [t_1, t_2, ..., t_n]
# Assume I is a sequence of image features [i_1, i_2, ..., i_m]
# Compute the cross-attention scores between T and I using a transformer model
scores = transformer_cross_attention(T, I)
# scores is a matrix of shape (n, m) where scores[i][j] is the attention score between t_i and i_j
# Select the top-k tokens in T that have the highest scores as concept identifiers
identifiers = []
for i in range(k):
  # Find the index of the token with the maximum score
  max_index = argmax(scores)
  # Append the token to the identifiers list
  identifiers.append(T[max_index])
  # Set the scores of the token to zero to avoid selecting it again
  scores[max_index] = 0
# Step 2: Concept removal
# Choose a strategy for concept removal: masking or noise injection
strategy = choose_strategy()
# Apply the strategy to the concept identifiers
modified_T = []
for t in T:
  if t in identifiers:
    # If the token is a concept identifier, apply the strategy
    if strategy == "masking":
      # Replace the token with a special mask token [MASK]
      modified_T.append([MASK])
    elif strategy == "noise injection":
      # Add random noise to the token embedding
      noise = random_noise()
      modified_T.append(t + noise)
  else:
    # If the token is not a concept identifier, keep it as it is
    modified_T.append(t)
# Generate a new image I' from the modified text prompt using a text-to-image model
I' = text_to_image(modified_T)
# Return I' as the output
return I'
```