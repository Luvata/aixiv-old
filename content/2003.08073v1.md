---
title: 2003.08073v1 Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation
date: 2020-03-09
---

# [Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation](http://arxiv.org/abs/2003.08073v1)

authors: Moab Arar, Yiftach Ginger, Dov Danon, Ilya Leizerson, Amit Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/2003.08073v1.pdf "arXiv:2003.08073v1 [cs.CV] 18 Mar 2020"
[2]: https://arxiv.org/abs/2003.08073 "[2003.08073] Unsupervised Multi-Modal Image Registration via Geometry ..."
[3]: http://export.arxiv.org/abs/1910.08073v1 "[1910.08073v1] A modular ultra-high vacuum millikelvin scanning ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes an unsupervised method for multi-modal image registration using geometry preserving image-to-image translation.
- **Why**: Multi-modal image registration is a challenging task that requires finding spatial correspondence between images from different sensors. Existing methods rely on cross-modality similarity measures that are difficult to design and optimize. The paper aims to overcome this limitation by learning a translation function between the modalities that enables the use of simple and reliable mono-modality metrics.
- **How**: The paper uses two networks: a spatial transformation network that warps a source image to match a target image, and a translation network that maps between the two modalities. The translation network is trained to be geometry preserving, meaning that it preserves the spatial structure and features of the input image. The spatial transformation network is trained using the translated images and a mono-modality metric, such as mean squared error or structural similarity index. The paper evaluates the method on several datasets and modalities, such as RGB-NIR, RGB-thermal, and CT-MRI, and shows that it achieves accurate alignment and outperforms state-of-the-art methods.

## Main Contributions

[1]: https://arxiv.org/pdf/2003.08073v1.pdf "arXiv:2003.08073v1 [cs.CV] 18 Mar 2020"
[2]: https://arxiv.org/abs/2003.08073 "[2003.08073] Unsupervised Multi-Modal Image Registration via Geometry ..."
[3]: http://export.arxiv.org/abs/1910.08073v1 "[1910.08073v1] A modular ultra-high vacuum millikelvin scanning ..."

The paper[^1^][1] claims the following contributions:

- **A novel unsupervised method for multi-modal image registration** that does not require any aligned pairs of modalities for training and can be adapted to any pair of modalities.
- **A geometry preserving image-to-image translation network** that maps between the modalities while preserving the spatial structure and features of the input image.
- **A spatial transformation network trained using mono-modality metrics** that warps a source image to match a target image using the translated images and a simple and reliable similarity measure, such as mean squared error or structural similarity index.
- **A comprehensive evaluation on several datasets and modalities** that demonstrates the accuracy and robustness of the proposed method and its superiority over state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/pdf/2003.08073v1.pdf "arXiv:2003.08073v1 [cs.CV] 18 Mar 2020"
[2]: https://arxiv.org/abs/2003.08073 "[2003.08073] Unsupervised Multi-Modal Image Registration via Geometry ..."
[3]: http://export.arxiv.org/abs/1910.08073v1 "[1910.08073v1] A modular ultra-high vacuum millikelvin scanning ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper presents a two-stage framework for multi-modal image registration, consisting of a spatial transformation network and a translation network.
- The spatial transformation network takes a source image and a target image as inputs and outputs a warped source image that matches the target image. The network is based on a U-Net architecture with a thin bottleneck that encodes the spatial transformation parameters. The network is trained using a mono-modality metric that measures the similarity between the warped source image and the target image.
- The translation network takes an image from one modality and outputs an image in another modality. The network is based on a CycleGAN architecture with two generators and two discriminators. The network is trained using adversarial loss, cycle-consistency loss, and geometry-preserving loss. The geometry-preserving loss encourages the network to preserve the spatial structure and features of the input image, such as edges, corners, and gradients. This loss is computed using a pre-trained VGG-19 network that extracts high-level features from the input and output images.
- The paper proposes a joint training scheme for the two networks, where the translation network provides synthetic images for the spatial transformation network to train on. The paper also introduces a self-supervised fine-tuning strategy, where the spatial transformation network is fine-tuned on real images using an unsupervised cycle-consistency criterion. The paper shows that these techniques improve the performance and generalization of the method.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the spatial transformation network STN
STN = UNet()

# Define the translation network TN
TN = CycleGAN()

# Define the mono-modality metric M
M = MSE or SSIM

# Define the geometry-preserving loss L_G
L_G = VGG19_Feature_Loss

# Define the cycle-consistency criterion C
C = Cycle_Consistency_Loss

# Train the two networks jointly on synthetic images
for epoch in epochs:
  for batch in batches:
    # Get a source image S and a target image T from different modalities
    S, T = get_batch()
    
    # Translate S to T' using TN
    T' = TN(S)
    
    # Warp S to W using STN
    W = STN(S, T')
    
    # Compute the mono-modality metric between W and T'
    loss_M = M(W, T')
    
    # Update STN parameters to minimize loss_M
    STN.backward(loss_M)
    
    # Translate T to S' using TN
    S' = TN(T)
    
    # Compute the geometry-preserving loss between S and S'
    loss_G = L_G(S, S')
    
    # Update TN parameters to minimize loss_G
    TN.backward(loss_G)

# Fine-tune STN on real images using cycle-consistency criterion
for epoch in epochs:
  for batch in batches:
    # Get a source image S and a target image T from different modalities
    S, T = get_batch()
    
    # Warp S to W using STN
    W = STN(S, T)
    
    # Translate W to S' using TN
    S' = TN(W)
    
    # Compute the cycle-consistency criterion between S and S'
    loss_C = C(S, S')
    
    # Update STN parameters to minimize loss_C
    STN.backward(loss_C)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms

# Define the spatial transformation network STN
class STN(nn.Module):
  def __init__(self):
    super(STN, self).__init__()
    # Define the encoder part of the U-Net
    self.enc1 = nn.Sequential(
      nn.Conv2d(3, 64, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(64, 64, 3, padding=1),
      nn.ReLU()
    )
    self.pool1 = nn.MaxPool2d(2)
    self.enc2 = nn.Sequential(
      nn.Conv2d(64, 128, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(128, 128, 3, padding=1),
      nn.ReLU()
    )
    self.pool2 = nn.MaxPool2d(2)
    self.enc3 = nn.Sequential(
      nn.Conv2d(128, 256, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(256, 256, 3, padding=1),
      nn.ReLU()
    )
    self.pool3 = nn.MaxPool2d(2)
    self.enc4 = nn.Sequential(
      nn.Conv2d(256, 512, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(512, 512, 3, padding=1),
      nn.ReLU()
    )
    self.pool4 = nn.MaxPool2d(2)
    
    # Define the bottleneck part of the U-Net
    self.bottleneck = nn.Sequential(
      nn.Conv2d(512, 1024, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(1024, 1024, 3, padding=1),
      nn.ReLU()
    )
    
    # Define the decoder part of the U-Net
    self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
    self.dec4 = nn.Sequential(
      nn.Conv2d(1024, 512, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(512, 512, 3, padding=1),
      nn.ReLU()
    )
    self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
    self.dec3 = nn.Sequential(
      nn.Conv2d(512, 256, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(256, 256, 3, padding=1),
      nn.ReLU()
    )
    self.upconv2 = nn.ConvTranspose2d(256, 128, 2 ,stride=2)
    self.dec2 = nn.Sequential(
      nn.Conv2d(256 ,128 ,3 ,padding=1),
      nn.ReLU(),
      nn.Conv2d(128 ,128 ,3 ,padding=1),
      nn.ReLU()
    )
    self.upconv1 =nn. ConvTranspose2d(128 ,64 ,2 ,stride=2)
    self.dec1 =nn. Sequential(
      nn.Conv2d(128 ,64 ,3 ,padding=1),
      nn.ReLU(),
      nn.Conv2d(64 ,64 ,3 ,padding=1),
      nn.ReLU()
    )
    
    # Define the output layer of the U-Net
    self.out =nn. Conv2d(64 ,6 ,1) # Output six parameters for affine transformation
    
  
  def forward(self,x,y):
    # Encode x and y
    x1 =self. enc1(x)
    x =self. pool1(x1)
    x2 =self. enc2(x)
    x =self. pool2(x2)
    x3 =self. enc3(x)
    x =self. pool3(x3)
    x4 =self. enc4(x)
    
    y1 =self. enc1(y)
    y =self. pool1(y1)
    y2 =self. enc2(y)
    y =self. pool2(y2)
    y3 =self. enc3(y)
    
    # Concatenate x and y at the bottleneck
    z =torch.cat([x4,y], dim=1) # Concatenate along the channel dimension
    
    # Decode z
    z =self. bottleneck(z)
    z =self. upconv4(z)
    z =torch.cat([z,x4], dim=1)
    z =self. dec4(z)
    z =self. upconv3(z)
    z =torch.cat([z,x3], dim=1)
    z =self. dec3(z)
    z =self. upconv2(z)
    z =torch.cat([z,x2], dim=1)
    z =self. dec2(z)
    z =self. upconv1(z)
    z =torch.cat([z,x1], dim=1)
    z =self. dec1(z)
    
    # Output the transformation parameters
    theta =self. out(z) # Shape: (batch_size, 6, h, w)
    
    # Reshape theta to a 2x3 matrix
    theta =theta.view(-1,2,3) # Shape: (batch_size * h * w, 2, 3)
    
    # Apply the affine transformation to x
    grid_size =torch.Size([theta.size(0), 3, 256, 256]) # Define the output size
    grid =nn.functional.affine_grid(theta, grid_size) # Generate a sampling grid
    x =x.repeat(1,1,256,256) # Repeat x to match the output size
    x =x.view(-1,3,256,256) # Reshape x to a single image
    w =nn.functional.grid_sample(x, grid) # Sample x using the grid
    w =w.view(-1,256,3,256,256) # Reshape w to a batch of images
    w =w.mean(dim=1) # Average over the height dimension
    
    return w

# Define the translation network TN
class TN(nn.Module):
  def __init__(self):
    super(TN, self).__init__()
    
    # Define the generator G that translates from modality X to modality Y
    self.G = nn.Sequential(
      nn.Conv2d(3, 64, 7, padding=3),
      nn.InstanceNorm2d(64),
      nn.ReLU(),
      nn.Conv2d(64, 128, 3, stride=2, padding=1),
      nn.InstanceNorm2d(128),
      nn.ReLU(),
      nn.Conv2d(128, 256, 3, stride=2, padding=1),
      nn.InstanceNorm2d(256),
      nn.ReLU(),
      ResnetBlock(256), # Define a residual block with 256 channels
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      nn.ConvTranspose2d(256, 128, 3 ,stride=2 ,padding=1 ,output_padding=1),
      nn.InstanceNorm2d(128),
      nn.ReLU(),
      nn.ConvTranspose2d(128 ,64 ,3 ,stride=2 ,padding=1 ,output_padding=1),
      nn.InstanceNorm2d(64),
      nn.ReLU(),
      nn.Conv2d(64 ,3 ,7 ,padding=3),
      nn.Tanh()
    )
    
    # Define the generator F that translates from modality Y to modality X
    self.F = nn.Sequential(
      nn.Conv2d(3 ,64 ,7 ,padding=3),
      nn.InstanceNorm2d(64),
      nn.ReLU(),
      nn.Conv2d(64 ,128 ,3 ,stride=2 ,padding=1),
      nn.InstanceNorm2d(128),
      nn.ReLU(),
      nn.Conv2d(128 ,256 ,3 ,stride=2 ,padding=1),
      nn.InstanceNorm2d(256),
      nn.ReLU(),
      ResnetBlock(256), # Define a residual block with 256 channels
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      ResnetBlock(256),
      nn.ConvTranspose2d(256 ,128 ,3 ,stride=2 ,padding=1 ,output_padding=1),
      nn.InstanceNorm2d(128),
      nn.ReLU(),
      nn.ConvTranspose2d(128 ,64 ,3 ,stride=2 ,padding=1 ,output_padding=1),
      nn.InstanceNorm2d(64),
      nn.ReLU(),
      nn.Conv2d(64 ,3 ,7 ,padding=3),
      nn.Tanh()
    )
    
    # Define the discriminator D_X that discriminates between real and fake images in modality X
    self.D_X =nn.