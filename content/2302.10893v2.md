---
title: 2302.10893v2 Fair Diffusion  Instructing Text-to-Image Generation Models on Fairness
date: 2023-02-11
---

# [Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness](http://arxiv.org/abs/2302.10893v2)

authors: Felix Friedrich, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Patrick Schramowski, Sasha Luccioni, Kristian Kersting


## What, Why and How

[1]: https://arxiv.org/abs/2302.10893 "[2302.10893] Fair Diffusion: Instructing Text-to-Image Generation ..."
[2]: https://arxiv.org/pdf/2302.10893v2.pdf "arXiv:2302.10893v2 [cs.LG] 2 Jun 2023"
[3]: http://export.arxiv.org/abs/2201.10893v2 "[2201.10893v2] Enhancing the force sensitivity of squeezed light ..."

Here is a summary of the paper:

- **What**: The paper presents a novel strategy, called **Fair Diffusion**, to attenuate biases after the deployment of generative text-to-image models. It demonstrates shifting a bias, based on human instructions, in any direction yielding arbitrary proportions for, e.g., identity groups.
- **Why**: The paper aims to address the ethical concern that generative text-to-image models may perpetuate and even amplify existing biases, leading to unfair outcomes. It argues that existing approaches to mitigate biases require data filtering or additional training, which are costly and impractical.
- **How**: The paper proposes to use **human feedback** as a way to instruct generative text-to-image models on fairness. It leverages the **diffusion process** of the models to modify the latent variables that control the image generation. It introduces a **fairness loss function** that measures the deviation from the desired proportions of identity groups. It evaluates the proposed method on a large-scale dataset of images depicting occupations and shows that it can effectively reduce gender occupation bias.


## Main Contributions

[1]: https://arxiv.org/abs/2302.10893 "[2302.10893] Fair Diffusion: Instructing Text-to-Image Generation ..."
[2]: https://arxiv.org/pdf/2302.10893v2.pdf "arXiv:2302.10893v2 [cs.LG] 2 Jun 2023"
[3]: http://export.arxiv.org/abs/2201.10893v2 "[2201.10893v2] Enhancing the force sensitivity of squeezed light ..."

According to the paper[^1^][1], the main contributions are:

- It introduces **Fair Diffusion**, a novel strategy to instruct generative text-to-image models on fairness after deployment, without data filtering or additional training.
- It leverages the **diffusion process** of the models to modify the latent variables that control the image generation, based on human feedback and a fairness loss function.
- It evaluates the proposed method on a large-scale dataset of images depicting occupations and shows that it can effectively reduce gender occupation bias and generate images with arbitrary proportions of identity groups.

## Method Summary

[1]: https://arxiv.org/abs/2302.10893 "[2302.10893] Fair Diffusion: Instructing Text-to-Image Generation ..."
[2]: https://arxiv.org/pdf/2302.10893v2.pdf "arXiv:2302.10893v2 [cs.LG] 2 Jun 2023"
[3]: http://export.arxiv.org/abs/2201.10893v2 "[2201.10893v2] Enhancing the force sensitivity of squeezed light ..."

Here is a summary of the method section of the paper:

- The paper proposes to use **human feedback** as a way to instruct generative text-to-image models on fairness. It assumes that the human can provide a desired proportion of identity groups for a given text input, such as 50% female and 50% male for "a doctor".
- The paper leverages the **diffusion process** of the models to modify the latent variables that control the image generation. It uses a pre-trained text encoder and image encoder to obtain the initial latent variables from the text input and the generated image. It then applies a series of Gaussian noise additions and denoising steps to diffuse the latent variables towards a prior distribution.
- The paper introduces a **fairness loss function** that measures the deviation from the desired proportion of identity groups. It uses a pre-trained classifier to predict the identity group of each generated image. It then computes the KL divergence between the predicted distribution and the desired distribution of identity groups. It minimizes this loss by adjusting the latent variables using gradient descent.
- The paper evaluates the proposed method on a large-scale dataset of images depicting occupations and shows that it can effectively reduce gender occupation bias and generate images with arbitrary proportions of identity groups. It also compares the method with other approaches such as data filtering and retraining.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: text, desired proportion of identity groups
# Output: image that matches the text and the desired proportion

# Pre-trained models: text encoder, image encoder, diffusion model, identity classifier

# Encode the text into initial latent variables
z_0 = text_encoder(text)

# Generate an initial image from the latent variables
x_0 = diffusion_model.reverse(z_0)

# Encode the image into latent variables
z_T = image_encoder(x_0)

# Diffuse the latent variables towards a prior distribution
for t in range(T-1, -1, -1):
  # Add Gaussian noise to the latent variables
  z_t = sqrt(1 - beta_t) * z_t+1 + sqrt(beta_t) * epsilon_t
  
  # Denoise the latent variables using the diffusion model
  z_t = z_t - alpha_t * diffusion_model.predict(z_t, t)

# Adjust the latent variables to minimize the fairness loss
for i in range(num_iterations):
  # Generate an image from the latent variables
  x_0 = diffusion_model.reverse(z_0)
  
  # Predict the identity group of the image using the classifier
  y_pred = identity_classifier(x_0)
  
  # Compute the KL divergence between the predicted and desired distributions
  L_fair = KL(y_pred || y_desired)
  
  # Compute the gradient of the loss with respect to the latent variables
  grad_z_0 = backward(L_fair, z_0)
  
  # Update the latent variables using gradient descent
  z_0 = z_0 - learning_rate * grad_z_0

# Generate the final image from the latent variables
x_0 = diffusion_model.reverse(z_0)

# Return the final image
return x_0
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
T = 1000 # number of diffusion steps
beta_1 = 0.0001 # initial noise level
beta_T = 0.02 # final noise level
num_iterations = 10 # number of iterations for fairness adjustment
learning_rate = 0.01 # learning rate for gradient descent

# Load the pre-trained models
text_encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'gpt2')
image_encoder = torchvision.models.resnet50(pretrained=True)
diffusion_model = torch.hub.load('openai/guided-diffusion', 'model', 'imagenet-1024')
identity_classifier = torchvision.models.resnet18(pretrained=True)

# Define the function to compute the KL divergence
def KL(p, q):
  return torch.sum(p * torch.log(p / q), dim=-1)

# Define the function to generate an image from text and desired proportion
def fair_diffusion(text, y_desired):
  # Encode the text into initial latent variables
  z_0 = text_encoder(text)
  
  # Generate an initial image from the latent variables
  x_0 = diffusion_model.reverse(z_0)
  
  # Encode the image into latent variables
  z_T = image_encoder(x_0)
  
  # Compute the noise levels for each diffusion step
  betas = torch.exp(torch.linspace(np.log(beta_1), np.log(beta_T), T))
  alphas = 1 - betas
  alphas_cumprod = torch.cumprod(alphas, dim=0)
  
  # Diffuse the latent variables towards a prior distribution
  for t in range(T-1, -1, -1):
    # Add Gaussian noise to the latent variables
    epsilon_t = torch.randn_like(z_t+1)
    z_t = torch.sqrt(1 - betas[t]) * z_t+1 + torch.sqrt(betas[t]) * epsilon_t
    
    # Denoise the latent variables using the diffusion model
    z_t = z_t - alphas[t] * diffusion_model.predict(z_t, t)

  # Adjust the latent variables to minimize the fairness loss
  for i in range(num_iterations):
    # Generate an image from the latent variables
    x_0 = diffusion_model.reverse(z_0)
    
    # Predict the identity group of the image using the classifier
    y_pred = identity_classifier(x_0)
    
    # Compute the KL divergence between the predicted and desired distributions
    L_fair = KL(y_pred, y_desired)
    
    # Compute the gradient of the loss with respect to the latent variables
    grad_z_0 = torch.autograd.grad(L_fair, z_0)[0]
    
    # Update the latent variables using gradient descent
    z_0 = z_0 - learning_rate * grad_z_0

  # Generate the final image from the latent variables
  x_0 = diffusion_model.reverse(z_0)

  # Return the final image
  return x_0

```