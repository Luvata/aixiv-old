---
title: 2211.07825v1 Direct Inversion  Optimization-Free Text-Driven Real Image Editing with Diffusion Models
date: 2022-11-08
---

# [Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models](http://arxiv.org/abs/2211.07825v1)

authors: Adham Elarabawy, Harish Kamath, Samuel Denton


## What, Why and How

[1]: https://arxiv.org/abs/2211.07825 "[2211.07825] Direct Inversion: Optimization-Free Text-Driven Real Image ..."
[2]: https://arxiv.org/pdf/2211.07825v1 "sam.denton@scale.com arXiv:2211.07825v1 [cs.CV] 15 Nov 2022"
[3]: http://export.arxiv.org/abs/2211.07825 "[2211.07825] Direct Inversion: Optimization-Free Text-Driven Real Image ..."

Here is a summary of the paper:

- **What**: The paper proposes a method called **Direct Inversion** that can edit real images using text prompts without any optimization or fine-tuning.
- **Why**: The paper aims to overcome the limitations of existing methods that either require per-instance or per-task training, are domain-specific, need additional inputs, or cannot control the edit strength and structural similarity.
- **How**: The paper leverages pre-trained text-to-image diffusion models and a novel technique of combining the latent codes of the input image and the target text to generate realistic and diverse image edits. The paper also introduces several hyperparameters to adjust the edit quality and extent. The paper evaluates the method on various tasks and compares it with state-of-the-art methods.

## Main Contributions

[1]: https://arxiv.org/abs/2211.07825 "[2211.07825] Direct Inversion: Optimization-Free Text-Driven Real Image ..."
[2]: https://arxiv.org/pdf/2211.07825v1 "sam.denton@scale.com arXiv:2211.07825v1 [cs.CV] 15 Nov 2022"
[3]: http://export.arxiv.org/abs/2211.07825 "[2211.07825] Direct Inversion: Optimization-Free Text-Driven Real Image ..."

According to the paper[^1^][2], the main contributions are:

- **A novel optimization-free and zero fine-tuning framework** that can edit real images using text prompts with pre-trained text-to-image diffusion models.
- **A technique of combining the latent codes** of the input image and the target text to generate realistic and diverse image edits.
- **Several hyperparameters to adjust the edit quality and extent**, such as edit strength, structural similarity, and semantic coherence.
- **A comprehensive evaluation of the method** on various tasks such as pose, scene, background, style, color, and racial identity editing, and a comparison with state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/abs/2211.07825 "[2211.07825] Direct Inversion: Optimization-Free Text-Driven Real Image ..."
[2]: https://arxiv.org/pdf/2211.07825v1 "sam.denton@scale.com arXiv:2211.07825v1 [cs.CV] 15 Nov 2022"
[3]: http://export.arxiv.org/abs/2211.07825 "[2211.07825] Direct Inversion: Optimization-Free Text-Driven Real Image ..."

Here is a summary of the method section of the paper:

- The paper uses a pre-trained text-to-image diffusion model that can generate images from text prompts by sampling from a latent space.
- The paper introduces a technique of combining the latent codes of the input image and the target text to generate an edited image that preserves the identity of the input image and aligns with the text guidance.
- The paper defines a similarity metric between latent codes based on cosine distance and uses it to control the edit strength and structural similarity between the input and output images.
- The paper also introduces a semantic coherence loss that penalizes the output image if it deviates from the target text semantically, and uses it to balance the trade-off between edit quality and text fidelity.
- The paper provides several hyperparameters that can be tuned by the user to adjust the edit quality and extent, such as edit strength, structural similarity, semantic coherence, and diversity.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a real image x, a target text t, a pre-trained text-to-image diffusion model M
# Output: an edited image y that matches t and preserves x's identity

# Encode x and t into latent codes z_x and z_t using M
z_x = M.encode(x)
z_t = M.encode(t)

# Combine z_x and z_t using a similarity metric d and a hyperparameter alpha
z_y = alpha * z_x + (1 - alpha) * z_t
# Alternatively, sample z_y from a Gaussian distribution with mean and variance computed from z_x and z_t

# Decode z_y into an output image y using M
y = M.decode(z_y)

# Optionally, apply a semantic coherence loss L to y and update z_y using gradient descent
L = semantic_loss(y, t)
z_y = z_y - lr * grad(L, z_y)
y = M.decode(z_y)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a real image x, a target text t, a pre-trained text-to-image diffusion model M
# Output: an edited image y that matches t and preserves x's identity

# Define the hyperparameters
alpha = edit_strength # a value between 0 and 1 that controls the edit strength
beta = structural_similarity # a value between 0 and 1 that controls the structural similarity
gamma = semantic_coherence # a value between 0 and 1 that controls the semantic coherence
delta = diversity # a value between 0 and 1 that controls the diversity
num_steps = number of optimization steps # an integer that determines how many steps to update z_y
lr = learning rate # a float that determines the step size for gradient descent

# Define the similarity metric d based on cosine distance
def d(z1, z2):
  return 1 - torch.cosine_similarity(z1, z2, dim=-1)

# Define the semantic coherence loss L based on CLIP [21]
def L(y, t):
  clip_model = load_pretrained_clip_model()
  image_features = clip_model.encode_image(y)
  text_features = clip_model.encode_text(t)
  return -torch.dot(image_features, text_features) / torch.norm(image_features) / torch.norm(text_features)

# Encode x and t into latent codes z_x and z_t using M
z_x = M.encode(x)
z_t = M.encode(t)

# Initialize z_y as a linear combination of z_x and z_t weighted by alpha and beta
z_y = alpha * beta * z_x + (1 - alpha) * z_t

# Alternatively, initialize z_y as a sample from a Gaussian distribution with mean and variance computed from z_x and z_t weighted by alpha and beta
mean = alpha * beta * z_x + (1 - alpha) * z_t
var = alpha * (1 - beta) * torch.var(z_x) + (1 - alpha) * torch.var(z_t)
z_y = torch.normal(mean, var)

# Decode z_y into an output image y using M
y = M.decode(z_y)

# Optionally, apply a semantic coherence loss L to y and update z_y using gradient descent for num_steps times
for i in range(num_steps):
  loss = gamma * L(y, t)
  z_y = z_y - lr * grad(loss, z_y)
  y = M.decode(z_y)

# Optionally, add diversity to y by adding noise to z_y and decoding it again
noise = torch.normal(0, delta)
z_y = z_y + noise
y = M.decode(z_y)
```