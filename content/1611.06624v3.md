---
title: 1611.06624v3 Temporal Generative Adversarial Nets with Singular Value Clipping
date: 2016-11-07
---

# [Temporal Generative Adversarial Nets with Singular Value Clipping](http://arxiv.org/abs/1611.06624v3)

authors: Masaki Saito, Eiichi Matsumoto, Shunta Saito


## What, Why and How

[1]: https://arxiv.org/pdf/1611.06624v3.pdf "Temporal Generative Adversarial Nets with Singular Value Clipping"
[2]: https://arxiv.org/abs/1611.06624 "Temporal Generative Adversarial Nets with Singular Value Clipping"
[3]: https://arxiv.org/pdf/1611.06624v2 "PDF for 1611.06624v2 - arXiv.org"

Here is a summary of the paper:

```
What: The paper proposes a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos.

Why: The paper aims to tackle the problem of video generation without any clues of data or assumptions about the background, which is still a highly challenging problem.

How: The paper exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. The paper also adopts a recently proposed model, Wasserstein GAN, and proposes a novel method to train it stably in an end-to-end manner.
```


## Main Contributions

[1]: https://arxiv.org/abs/1611.06624 "Temporal Generative Adversarial Nets with Singular Value Clipping"
[2]: https://ieeexplore.ieee.org/document/8237570/ "Temporal Generative Adversarial Nets with Singular Value Clipping ..."
[3]: https://paperswithcode.com/method/singular-value-clipping "Singular Value Clipping Explained | Papers With Code"
[4]: https://github.com/proceduralia/tgan-pytorch "GitHub - proceduralia/tgan-pytorch: A PyTorch implementation of ..."

Some of the contributions of this paper are:

- Proposing a generative model, TGAN, that can learn a semantic representation of unlabeled videos and generate videos
- Exploiting two different types of generators: a temporal generator and an image generator
- Adopting Wasserstein GAN and proposing a novel method to train it stably in an end-to-end manner
- Proposing Singular Value Clipping, a technique to enforce the 1-Lipschitz constraint of the WGAN objective


## Method Summary

[1]: https://arxiv.org/abs/1611.06624 "Temporal Generative Adversarial Nets with Singular Value Clipping"
[2]: https://ieeexplore.ieee.org/document/8237570/ "Temporal Generative Adversarial Nets with Singular Value Clipping ..."
[3]: https://paperswithcode.com/method/singular-value-clipping "Singular Value Clipping Explained | Papers With Code"
[4]: https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Reviews.html "Reviews: Time-series Generative Adversarial Networks"
[5]: https://pfnet-research.github.io/tgan/ "Temporal Generative Adversarial Nets with Singular Value Clipping"

Here is a summary of the method section of the paper:

```
The method section of the paper consists of four subsections: Temporal Generative Adversarial Nets, Wasserstein GAN, Singular Value Clipping, and Implementation Details.

- Temporal Generative Adversarial Nets: This subsection describes the proposed model, TGAN, which consists of two generators and one discriminator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. The discriminator tries to distinguish between real and fake videos.

- Wasserstein GAN: This subsection explains the motivation for using Wasserstein GAN, which is a variant of GAN that minimizes the Wasserstein distance between the real and fake data distributions. Wasserstein GAN has several advantages over the original GAN, such as stability, meaningful loss function, and gradient penalty.

- Singular Value Clipping: This subsection introduces a novel technique to enforce the 1-Lipschitz constraint of the Wasserstein GAN objective. It is a constraint to all linear layers in the discriminator that satisfies the spectral norm of weight parameter W is equal or less than one. To implement this, the authors perform singular value decomposition after parameter update, replace all the singular values larger than one with one, and reconstruct the parameter with them.

- Implementation Details: This subsection provides the details of the network architectures, hyperparameters, datasets, and evaluation metrics used in the experiments.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/1611.06624 "Temporal Generative Adversarial Nets with Singular Value Clipping"
[2]: https://arxiv.org/pdf/1611.06624.pdf "Temporal Generative Adversarial Nets with Singular Value Clipping"
[3]: https://github.com/pfnet-research/tgan "Temporal Generative Adversarial Nets - GitHub"
[4]: https://paperswithcode.com/method/singular-value-clipping "Singular Value Clipping Explained | Papers With Code"
[5]: https://github.com/proceduralia/tgan-pytorch "GitHub - proceduralia/tgan-pytorch: A PyTorch implementation of ..."

Here is a high-level pseudo code for the paper:

```
# Define the temporal generator G_t, the image generator G_i, and the discriminator D
G_t = TemporalGenerator()
G_i = ImageGenerator()
D = Discriminator()

# Define the Wasserstein loss function
def WassersteinLoss(real, fake):
  return mean(real) - mean(fake)

# Define the Singular Value Clipping function
def SingularValueClipping(W):
  # Perform singular value decomposition on W
  U, S, V = SVD(W)
  # Replace all the singular values larger than one with one
  S = clip(S, max=1)
  # Reconstruct W with the clipped singular values
  W = U * S * V
  return W

# Define the hyperparameters
batch_size = ...
num_epochs = ...
num_critic = ...
learning_rate = ...
latent_dim = ...

# Define the optimizers
optimizer_G = Adam(G_t.parameters() + G_i.parameters(), lr=learning_rate)
optimizer_D = Adam(D.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
  for i in range(0, len(data), batch_size):
    # Get a batch of real videos
    real_videos = data[i:i+batch_size]

    # Train the discriminator for num_critic steps
    for _ in range(num_critic):
      # Sample a batch of latent variables
      z = sample_normal(batch_size, latent_dim)
      # Generate a batch of fake videos using G_t and G_i
      fake_videos = G_i(G_t(z))
      # Compute the discriminator outputs for real and fake videos
      real_outputs = D(real_videos)
      fake_outputs = D(fake_videos)
      # Compute the Wasserstein loss for the discriminator
      loss_D = WassersteinLoss(real_outputs, fake_outputs)
      # Update the discriminator parameters using gradient descent
      optimizer_D.zero_grad()
      loss_D.backward()
      optimizer_D.step()
      # Apply Singular Value Clipping to all linear layers in D
      for layer in D.linear_layers:
        layer.weight.data = SingularValueClipping(layer.weight.data)

    # Train the generator
    # Sample a batch of latent variables
    z = sample_normal(batch_size, latent_dim)
    # Generate a batch of fake videos using G_t and G_i
    fake_videos = G_i(G_t(z))
    # Compute the discriminator outputs for fake videos
    fake_outputs = D(fake_videos)
    # Compute the Wasserstein loss for the generator
    loss_G = -mean(fake_outputs)
    # Update the generator parameters using gradient ascent
    optimizer_G.zero_grad()
    loss_G.backward()
    optimizer_G.step()

  # Evaluate the model on some metrics and save the model checkpoint
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/1611.06624 "Temporal Generative Adversarial Nets with Singular Value Clipping"
[2]: https://arxiv.org/pdf/1611.06624.pdf "Temporal Generative Adversarial Nets with Singular Value Clipping"
[3]: https://github.com/pfnet-research/tgan "Temporal Generative Adversarial Nets - GitHub"
[4]: https://www.arxiv-vanity.com/papers/1611.06624/ "Temporal Generative Adversarial Nets with Singular Value Clipping ..."

Here is a detailed pseudo code for the paper:

```
# Define the temporal generator G_t, the image generator G_i, and the discriminator D
G_t = TemporalGenerator()
G_i = ImageGenerator()
D = Discriminator()

# Define the Wasserstein loss function
def WassersteinLoss(real, fake):
  return mean(real) - mean(fake)

# Define the Singular Value Clipping function
def SingularValueClipping(W):
  # Perform singular value decomposition on W
  U, S, V = SVD(W)
  # Replace all the singular values larger than one with one
  S = clip(S, max=1)
  # Reconstruct W with the clipped singular values
  W = U * S * V
  return W

# Define the hyperparameters
batch_size = ...
num_epochs = ...
num_critic = ...
learning_rate = ...
latent_dim = ...
video_length = ...

# Define the optimizers
optimizer_G = Adam(G_t.parameters() + G_i.parameters(), lr=learning_rate)
optimizer_D = Adam(D.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
  for i in range(0, len(data), batch_size):
    # Get a batch of real videos
    real_videos = data[i:i+batch_size]

    # Train the discriminator for num_critic steps
    for _ in range(num_critic):
      # Sample a batch of latent variables
      z = sample_normal(batch_size, latent_dim)
      # Generate a batch of latent variables for each frame using G_t
      z_frames = G_t(z)
      # Generate a batch of fake videos using G_i
      fake_videos = []
      for t in range(video_length):
        fake_videos.append(G_i(z_frames[:, t]))
      fake_videos = stack(fake_videos, dim=1)
      # Compute the discriminator outputs for real and fake videos
      real_outputs = D(real_videos)
      fake_outputs = D(fake_videos)
      # Compute the Wasserstein loss for the discriminator
      loss_D = WassersteinLoss(real_outputs, fake_outputs)
      # Update the discriminator parameters using gradient descent
      optimizer_D.zero_grad()
      loss_D.backward()
      optimizer_D.step()
      # Apply Singular Value Clipping to all linear layers in D
      for layer in D.linear_layers:
        layer.weight.data = SingularValueClipping(layer.weight.data)

    # Train the generator
    # Sample a batch of latent variables
    z = sample_normal(batch_size, latent_dim)
    # Generate a batch of latent variables for each frame using G_t
    z_frames = G_t(z)
    # Generate a batch of fake videos using G_i
    fake_videos = []
    for t in range(video_length):
      fake_videos.append(G_i(z_frames[:, t]))
    fake_videos = stack(fake_videos, dim=1)
    # Compute the discriminator outputs for fake videos
    fake_outputs = D(fake_videos)
    # Compute the Wasserstein loss for the generator
    loss_G = -mean(fake_outputs)
    # Update the generator parameters using gradient ascent
    optimizer_G.zero_grad()
    loss_G.backward()
    optimizer_G.step()

  # Evaluate the model on some metrics and save the model checkpoint

# Define the temporal generator network architecture
class TemporalGenerator(nn.Module):
  def __init__(self):
    super(TemporalGenerator, self).__init__()
    # Define the input layer that takes a latent variable z and outputs a hidden state h_0
    self.input_layer = nn.Linear(latent_dim, hidden_dim)
    # Define a recurrent layer that takes h_0 and outputs a sequence of hidden states h_1, ..., h_T
    self.recurrent_layer = nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers)
    # Define an output layer that takes h_1, ..., h_T and outputs a sequence of latent variables z_1, ..., z_T
    self.output_layer = nn.Linear(hidden_dim, latent_dim)

  def forward(self, z):
    # Reshape z to have shape (batch_size, 1, latent_dim)
    z = z.view(batch_size, 1, latent_dim)
    # Apply the input layer to z and get h_0 with shape (num_layers, batch_size, hidden_dim)
    h_0 = self.input_layer(z).transpose(0, 1).contiguous()
    # Apply the recurrent layer to h_0 and get h_1, ..., h_T with shape (video_length, batch_size, hidden_dim)
    h_seq, _ = self.recurrent_layer(h_0)
    # Apply the output layer to h_seq and get z_1, ..., z_T with shape (video_length, batch_size, latent_dim)
    z_seq = self.output_layer(h_seq)
    # Reshape z_seq to have shape (batch_size, video_length, latent_dim)
    z_seq = z_seq.transpose(0, 1).contiguous()
    return z_seq

# Define the image generator network architecture
class ImageGenerator(nn.Module):
  def __init__(self):
    super(ImageGenerator, self).__init__()
    # Define a series of deconvolutional layers that take a latent variable z_t and output an image frame x_t
    self.deconv_layers = nn.Sequential(
      # Deconvolutional layer 1: input shape (batch_size, latent_dim, 1, 1), output shape (batch_size, 512, 4, 4)
      nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),
      nn.BatchNorm2d(512),
      nn.ReLU(),
      # Deconvolutional layer 2: input shape (batch_size, 512, 4, 4), output shape (batch_size, 256, 8, 8)
      nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(256),
      nn.ReLU(),
      # Deconvolutional layer 3: input shape (batch_size, 256, 8, 8), output shape (batch_size, 128, 16, 16)
      nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(128),
      nn.ReLU(),
      # Deconvolutional layer 4: input shape (batch_size, 128, 16, 16), output shape (batch_size, 64, 32, 32)
      nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(64),
      nn.ReLU(),
      # Deconvolutional layer 5: input shape (batch_size, 64, 32, 32), output shape (batch_size, num_channels, image_height,
       image_width)
      nn.ConvTranspose2d(64, num_channels,
       kernel_size=4,
       stride=2,
       padding=1),
      nn.Tanh()
    )

  def forward(self, z_t):
    # Reshape z_t to have shape (batch_size,
     latent_dim,
     1,
     1)
    z_t = z_t.view(batch_size,
     latent_dim,
     1,
     1)
    # Apply the deconvolutional layers to z_t and get x_t with shape (batch_size,
     num_channels,
     image_height,
     image_width)
    x_t = self.deconv_layers(z_t)
    return x_t

# Define the discriminator network architecture
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator,
     self).__init__()
    # Define a series of convolutional layers that take a video x and output a feature map f
    self.conv_layers = nn.Sequential(
      # Convolutional layer
       input shape (batch_size,
       num_channels * video_length,
       image_height,
       image_width),
       output shape (batch_size,
       num_channels * video_length * filter_factor_1,
       image_height / pool_factor_1,
       image_width / pool_factor_1)
      nn.Conv2d(num_channels * video_length,
       num_channels * video_length * filter_factor_1,
       kernel_size=kernel_size_1,
       stride=stride_1),
      nn.LeakyReLU(negative_slope=negative_slope),
      nn.MaxPool2d(kernel_size=pool_kernel_1,
       stride=pool_stride_1),
      # Convolutional layer
       input shape (batch_size,
       num_channels * video_length * filter_factor_1,
       image_height / pool_factor_1,
       image_width / pool_factor_1),
       output shape (batch_size,
       num_channels * video_length * filter_factor_2,
       image_height / pool_factor_2,
       image_width / pool_factor_2)
      nn.Conv2d(num_channels * video_length * filter_factor_1,
       num_channels * video_length * filter_factor_2,
       kernel_size=kernel_size_2,
       stride=stride_2),
      nn.LeakyReLU(negative_slope=negative_slope),
      nn.MaxPool2d(kernel_size=pool_kernel_2,
       stride=pool_stride_2),
      # Convolutional layer
       input shape (batch_size,
       num_channels * video_length * filter_factor_2,
       image_height / pool_factor_2,
       image_width / pool_factor_2),
       output shape (batch_size,
       num_channels * video_length * filter_factor_3,
       image_height / pool_factor_