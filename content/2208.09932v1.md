---
title: 2208.09932v1 Improving GANs for Long-Tailed Data through Group Spectral Regularization
date: 2022-08-10
---

# [Improving GANs for Long-Tailed Data through Group Spectral Regularization](http://arxiv.org/abs/2208.09932v1)

authors: Harsh Rangwani, Naman Jaswani, Tejan Karmali, Varun Jampani, R. Venkatesh Babu


## What, Why and How

[1]: https://arxiv.org/abs/2208.09932 "[2208.09932] Improving GANs for Long-Tailed Data through ... - arXiv.org"
[2]: https://arxiv.org/pdf/2207.09932v1.pdf "Determining the volume fraction in 2-phase composites and ... - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2201.09932v1 "[2201.09932v1] Learning Optimal Fair Classification Trees"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a novel group Spectral Regularizer (gSR) that prevents the spectral explosion of the conditioning parameter matrix in conditional Generative Adversarial Networks (GANs) trained on long-tailed data. Long-tailed data are imbalanced distributions where most labels of the tail classes are associated with a few samples.
- **Why**: The paper aims to address the problem of class-specific mode collapse for tail classes, which leads to poor image generation performance on long-tailed data. The paper argues that existing augmentation and regularization techniques are not sufficient to overcome this problem, and that spectral explosion is a key factor that causes mode collapse.
- **How**: The paper introduces gSR, which is a regularization term added to the generator loss function that penalizes the spectral norm of each group of rows in the conditioning parameter matrix corresponding to each class. The paper shows that gSR effectively reduces the spectral norm of the matrix and improves the diversity and quality of image generation for tail classes. The paper also demonstrates that gSR can be combined with existing techniques such as class-balanced sampling, feature augmentation, and gradient penalty to achieve state-of-the-art results on long-tailed datasets with different degrees of imbalance.

## Main Contributions

The paper claims to make the following contributions:

- It identifies the spectral explosion of the conditioning parameter matrix as a major cause of mode collapse for tail classes in conditional GANs trained on long-tailed data.
- It proposes a novel group Spectral Regularizer (gSR) that prevents the spectral explosion and alleviates mode collapse, resulting in diverse and plausible image generation even for tail classes.
- It shows that gSR can be effectively combined with existing augmentation and regularization techniques to further improve the performance of conditional GANs on long-tailed data.
- It conducts extensive experiments on several long-tailed datasets with different degrees of imbalance and demonstrates the superiority of gSR over existing methods.

## Method Summary

The method section of the paper consists of four subsections:

- In the first subsection, the paper reviews the background of conditional GANs and introduces the notation and problem formulation. It also defines the long-tailed distribution and the class-specific mode collapse problem.
- In the second subsection, the paper analyzes the spectral properties of the conditioning parameter matrix in conditional GANs and shows that its spectral norm increases with the degree of imbalance in the data. It also provides theoretical and empirical evidence that spectral explosion leads to mode collapse for tail classes.
- In the third subsection, the paper proposes the group Spectral Regularizer (gSR), which is a regularization term added to the generator loss function that penalizes the spectral norm of each group of rows in the conditioning parameter matrix corresponding to each class. It also explains how to compute gSR efficiently using power iteration and group normalization.
- In the fourth subsection, the paper discusses how to combine gSR with existing augmentation and regularization techniques such as class-balanced sampling, feature augmentation, and gradient penalty. It also provides an algorithm for training conditional GANs with gSR and other techniques.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the conditional GAN model with a generator G and a discriminator D
# Define the conditioning parameter matrix W
# Define the hyperparameters lambda_gsr, lambda_gp, and n_iter
# Define the long-tailed dataset with K classes and N samples per class
# Define the class-balanced sampling strategy
# Define the feature augmentation function F

# Initialize G and D randomly
# Initialize W with small random values
# Initialize the spectral norm of each group of rows in W as sigma_k

# Repeat until convergence:
  # Sample a mini-batch of real images x and their labels y from the dataset using class-balanced sampling
  # Sample a mini-batch of random noise z from a prior distribution
  # Generate fake images x_hat = G(z, W[y]) using the conditioning parameter matrix W
  # Compute the discriminator outputs for real and fake images: D_real = D(x, W[y]), D_fake = D(x_hat, W[y])
  # Compute the feature augmented discriminator outputs for real and fake images: D_real_aug = D(F(x), W[y]), D_fake_aug = D(F(x_hat), W[y])
  # Compute the generator loss: L_G = - E[D_fake] - E[D_fake_aug] + lambda_gsr * gSR(W) # gSR(W) is the group Spectral Regularizer term
  # Compute the discriminator loss: L_D = E[D_fake] - E[D_real] + E[D_fake_aug] - E[D_real_aug] + lambda_gp * GP(D) # GP(D) is the gradient penalty term
  # Update G and D using gradient descent: G = G - alpha * grad(L_G, G), D = D - alpha * grad(L_D, D)
  # Update W using gradient descent: W = W - alpha * grad(L_G, W)
  # Update sigma_k using power iteration for each group of rows in W: sigma_k = max(eig(W[k])) # eig(W[k]) is the eigenvalue decomposition of W[k]
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torch.nn as nn # for neural network modules
import torch.optim as optim # for optimization algorithms
import torchvision # for image processing and datasets
import torchvision.transforms as transforms # for image transformations
import numpy as np # for numerical computations

# Define the conditional GAN model with a generator G and a discriminator D
# The generator and discriminator are both convolutional neural networks with residual blocks and spectral normalization
# The generator takes a noise vector z and a class embedding vector w as inputs and outputs a fake image x_hat
# The discriminator takes an image x and a class embedding vector w as inputs and outputs a scalar score D(x, w)
# The conditioning parameter matrix W is a learnable parameter of the generator that maps the class label y to the class embedding vector w
class Generator(nn.Module):
  def __init__(self, z_dim, w_dim, x_dim):
    super(Generator, self).__init__()
    self.z_dim = z_dim # the dimension of the noise vector z
    self.w_dim = w_dim # the dimension of the class embedding vector w
    self.x_dim = x_dim # the dimension of the image x
    self.W = nn.Parameter(torch.randn(K, w_dim) * 0.01) # the conditioning parameter matrix W with K rows and w_dim columns, initialized with small random values
    self.sigma = nn.Parameter(torch.ones(K)) # the spectral norm of each group of rows in W, initialized with ones
    self.linear = nn.Linear(z_dim + w_dim, 4 * 4 * 512) # a linear layer that maps the concatenated vector [z; w] to a feature map of size 4 x 4 x 512
    self.main = nn.Sequential( # a sequence of convolutional layers with residual blocks and spectral normalization
      nn.BatchNorm2d(512),
      nn.ReLU(),
      nn.utils.spectral_norm(nn.ConvTranspose2d(512, 256, 4, 2, 1)), # a transposed convolution layer that upsamples the feature map from 4 x 4 x 512 to 8 x 8 x 256
      ResBlock(256), # a residual block that preserves the feature map size of 8 x 8 x 256
      nn.BatchNorm2d(256),
      nn.ReLU(),
      nn.utils.spectral_norm(nn.ConvTranspose2d(256, 128, 4, 2, 1)), # a transposed convolution layer that upsamples the feature map from 8 x 8 x 256 to 16 x 16 x 128
      ResBlock(128), # a residual block that preserves the feature map size of 16 x 16 x 128
      nn.BatchNorm2d(128),
      nn.ReLU(),
      nn.utils.spectral_norm(nn.ConvTranspose2d(128, 64, 4, 2, 1)), # a transposed convolution layer that upsamples the feature map from 16 x 16 x 128 to 32 x 32 x 64
      ResBlock(64), # a residual block that preserves the feature map size of 32 x 32 x 64
      nn.BatchNorm2d(64),
      nn.ReLU(),
      nn.utils.spectral_norm(nn.ConvTranspose2d(64, x_dim, 4, 2, 1)), # a transposed convolution layer that upsamples the feature map from 32 x 32 x 64 to 64 x 64 x x_dim
      nn.Tanh() # a tanh activation function that maps the output to the range [-1, 1]
    )

  
class ResBlock(nn.Module):
    def __init__(self, dim):
        super(ResBlock,self).__init__()
        self.main = nn.Sequential(
            nn.utils.spectral_norm(nn.Conv2d(dim,dim,kernel_size=3,padding=1)),
            nn.BatchNorm2d(dim),
            nn.ReLU(inplace=True),
            nn.utils.spectral_norm(nn.Conv2d(dim,dim,kernel_size=3,padding=1)),
            nn.BatchNorm2d(dim)
        )
    def forward(self,x):
        return x + self.main(x)

class Discriminator(nn.Module):
    def __init__(self,x_dim,w_dim):
        super(Discriminator,self).__init__()
        self.x_dim = x_dim # the dimension of the image x
        self.w_dim = w_dim # the dimension of the class embedding vector w
        self.main = nn.Sequential(
            nn.utils.spectral_norm(nn.Conv2d(x_dim+1,64,kernel_size=4,stride=2,padding=1)), # a convolution layer that downsamples the concatenated tensor [x; w] from 65 x 64 x 64 to 64 x 32 x 32
            nn.LeakyReLU(0.2,inplace=True),
            nn.utils.spectral_norm(nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1)), # a convolution layer that downsamples the feature map from 64 x 32 x 32 to 128 x 16 x 16
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2,inplace=True),
            nn.utils.spectral_norm(nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1)), # a convolution layer that downsamples the feature map from 128 x 16 x 16 to 256 x 8 x 8
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2,inplace=True),
            nn.utils.spectral_norm(nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1)), # a convolution layer that downsamples the feature map from 256 x 8 x 8 to 512 x 4 x 4
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2,inplace=True),
        )
        self.linear = nn.Linear(512*4*4,1) # a linear layer that maps the flattened feature map of size 512*4*4 to a scalar score

    def forward(self,x,w):
        w = w.view(-1,1,1,1) # reshape the class embedding vector w to a tensor of size -1 x 1 x 1 x 1
        w = w.repeat(1,1,x.size(2),x.size(3)) # repeat the class embedding vector w along the spatial dimensions to match the size of the image x
        x = torch.cat([x,w],dim=1) # concatenate the image x and the class embedding vector w along the channel dimension to form a tensor of size -1 x (x_dim + 1) x 64 x 64
        out = self.main(x) # pass the concatenated tensor through the main module of the discriminator
        out = out.view(-1,512*4*4) # flatten the output feature map to a vector of size -1 x (512*4*4)
        out = self.linear(out) # pass the flattened vector through the linear layer of the discriminator
        return out

# Define the hyperparameters lambda_gsr, lambda_gp, and n_iter
lambda_gsr = 0.01 # the weight of the group Spectral Regularizer term in the generator loss function
lambda_gp = 10 # the weight of the gradient penalty term in the discriminator loss function
n_iter = 100000 # the number of iterations for training

# Define the long-tailed dataset with K classes and N samples per class
# We use CIFAR-10 as an example dataset, which has K = 10 classes and N = 5000 samples per class in total
# We create a long-tailed version of CIFAR-10 by sampling only n_k samples per class, where n_k follows a power-law distribution with an imbalance ratio r
# We use r = 100 as an example, which means that n_0 = N and n_k = n_0 / (k + 1)^r for k > 0
# We use torchvision.datasets.CIFAR10 to load the CIFAR-10 dataset and torchvision.transforms to apply some image transformations such as resizing, cropping, flipping, and normalization
K = 10 # the number of classes
N = 5000 # the number of samples per class in total
r = 100 # the imbalance ratio
n_k = [N // (k + 1)**r for k in range(K)] # the number of samples per class in the long-tailed version
transform_train = transforms.Compose([ # define a composition of image transformations for training data
    transforms.Resize(64), # resize the image to 64 x 64 pixels
    transforms.RandomCrop(64,padding=4), # randomly crop a patch of size 64 x 64 pixels with padding of size 4 pixels on each side
    transforms.RandomHorizontalFlip(), # randomly flip the image horizontally with probability 0.5
    transforms.ToTensor(), # convert the image to a torch tensor with values in range [0,1]
    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)), # normalize the image with mean and standard deviation for each channel
])
transform_test = transforms.Compose([ # define a composition of