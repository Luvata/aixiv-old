---
title: 2305.19599v2 Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards
date: 2023-05-20
---

# [Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards](http://arxiv.org/abs/2305.19599v2)

authors: Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Xiaodan Liang


## What, Why and How

[1]: https://arxiv.org/abs/2305.19599 "[2305.19599] Boosting Text-to-Image Diffusion Models with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2305.19599v2.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2305.19599v1 "[2305.19599v1] Boosting Text-to-Image Diffusion Models with Fine ..."

Here is a summary of the paper:

- **What**: The paper proposes FineRewards, a method to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward.
- **Why**: The paper aims to address the problem of modality discrepancy between text concepts and generated images, which leads to poor visual quality and semantic similarity in text-to-image synthesis.
- **How**: The paper introduces two novel rewards that measure the global and local semantic alignment between text and images. The caption reward generates a detailed caption for the synthetic image using a BLIP-2 model and compares it with the given prompt. The SAM reward segments the synthetic image into local parts with category labels and scores them based on their likelihood of appearing in the prompted scene using a Vicuna-7B model. The paper also adopts an assemble reward-ranked learning strategy to integrate multiple reward functions to jointly guide the model training. The paper evaluates the proposed method on the MS-COCO benchmark and shows that it outperforms other baseline reward functions on both visual quality and semantic similarity metrics.

## Main Contributions

The paper claims the following contributions:

- It proposes FineRewards, a novel method to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the SAM reward.
- It introduces the caption reward, which generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt.
- It introduces the SAM reward, which segments the generated images into local parts with category labels, and scores the segmented parts by measuring the likelihood of each category appearing in the prompted scene via a Vicuna-7B model.
- It adopts an assemble reward-ranked learning strategy to enable the integration of multiple reward functions to jointly guide the model training.
- It demonstrates the effectiveness of the proposed method on the MS-COCO benchmark and shows that it outperforms other baseline reward functions on both visual quality and semantic similarity metrics.

## Method Summary

Here is a summary of the method section of the paper:

- The paper adopts the text-to-image diffusion model proposed by Fang et al. (2022), which consists of a text encoder, an image encoder, and a diffusion model. The text encoder encodes the given prompt into a latent vector, which is then used to condition the diffusion model. The image encoder encodes the generated image into another latent vector, which is used to compute the image reward. The diffusion model generates the image by reversing a Markov chain that gradually adds noise to the image until it becomes a pure noise sample.
- The paper introduces two new fine-grained semantic rewards: the caption reward and the SAM reward. The caption reward generates a detailed caption for the synthetic image using a BLIP-2 model, which is a pre-trained vision-language model that can generate natural language descriptions for images. The caption reward then calculates the reward score by measuring the similarity between the generated caption and the given prompt using a Vicuna-7B model, which is a large language model that can measure semantic similarity between texts. The SAM reward segments the synthetic image into local parts with category labels using a SAM model, which is a pre-trained semantic segmentation model that can segment any object in an image. The SAM reward then scores the segmented parts by measuring the likelihood of each category appearing in the prompted scene using a Vicuna-7B model.
- The paper also adopts an assemble reward-ranked learning strategy to integrate multiple reward functions to jointly guide the model training. The strategy consists of two steps: (1) ranking the rewards according to their importance for each prompt-image pair, and (2) weighting the rewards according to their ranks and applying them to update the model parameters. The paper uses three criteria to rank the rewards: (1) whether the reward is positive or negative, (2) whether the reward is consistent or inconsistent with other rewards, and (3) whether the reward is informative or uninformative. The paper uses a linear weighting scheme to assign weights to the rewards based on their ranks. The paper combines the proposed semantic rewards with the existing image rewards, such as perceptual quality, diversity, and style consistency.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the text encoder, image encoder, diffusion model, BLIP-2 model, SAM model, and Vicuna-7B model
text_encoder = TextEncoder()
image_encoder = ImageEncoder()
diffusion_model = DiffusionModel()
blip2_model = BLIP2Model()
sam_model = SAMModel()
vicuna7b_model = Vicuna7BModel()

# Define the image rewards: perceptual quality, diversity, and style consistency
image_rewards = [PerceptualQualityReward(), DiversityReward(), StyleConsistencyReward()]

# Define the semantic rewards: caption reward and SAM reward
semantic_rewards = [CaptionReward(), SAMReward()]

# Define the assemble reward-ranked learning strategy
assemble_reward_ranked_learning = AssembleRewardRankedLearning()

# Define the training data: a set of text prompts and corresponding images
training_data = [(text_prompt_1, image_1), (text_prompt_2, image_2), ..., (text_prompt_N, image_N)]

# Train the text-to-image diffusion model with fine-grained semantic rewards
for epoch in range(num_epochs):
  for (text_prompt, image) in training_data:
    # Encode the text prompt into a latent vector
    text_latent = text_encoder(text_prompt)
    
    # Generate an image by reversing a Markov chain from a noise sample
    generated_image = diffusion_model(text_latent)
    
    # Encode the generated image into another latent vector
    image_latent = image_encoder(generated_image)
    
    # Compute the image rewards for the generated image
    image_reward_scores = [image_reward(image_latent) for image_reward in image_rewards]
    
    # Compute the caption reward for the generated image
    generated_caption = blip2_model(generated_image)
    caption_reward_score = CaptionReward(vicuna7b_model(text_prompt, generated_caption))
    
    # Compute the SAM reward for the generated image
    segmented_image = sam_model(generated_image)
    sam_reward_score = SAMReward(vicuna7b_model(text_prompt, segmented_image))
    
    # Combine the image rewards and the semantic rewards into a list
    reward_scores = image_reward_scores + [caption_reward_score, sam_reward_score]
    
    # Rank and weight the rewards using the assemble reward-ranked learning strategy
    ranked_rewards, weighted_rewards = assemble_reward_ranked_learning(reward_scores)
    
    # Update the model parameters using the weighted rewards
    diffusion_model.update_parameters(weighted_rewards)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import timm
import segmentation_models_pytorch

# Define the text encoder, which is a pre-trained CLIP model that encodes text into a 512-dimensional vector
text_encoder = transformers.CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

# Define the image encoder, which is a pre-trained CLIP model that encodes images into a 512-dimensional vector
image_encoder = transformers.CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

# Define the diffusion model, which is a U-Net based model that generates images by reversing a Markov chain from a noise sample
diffusion_model = timm.create_model("unet_v2", pretrained=True)

# Define the BLIP-2 model, which is a pre-trained vision-language model that can generate natural language descriptions for images
blip2_model = transformers.Pipeline("image-captioning", model="google/blip2-base-400")

# Define the SAM model, which is a pre-trained semantic segmentation model that can segment any object in an image
sam_model = segmentation_models_pytorch.PAN(pretrained=True)

# Define the Vicuna-7B model, which is a large language model that can measure semantic similarity between texts
vicuna7b_model = transformers.AutoModelForSequenceClassification.from_pretrained("vicuna/vicuna-7b")

# Define the perceptual quality reward, which measures the perceptual similarity between the generated image and the original image using a VGG-16 model
perceptual_quality_reward = torchvision.models.vgg16(pretrained=True)

# Define the diversity reward, which measures the diversity of the generated images using a Kullback-Leibler divergence metric
diversity_reward = torch.nn.KLDivLoss()

# Define the style consistency reward, which measures the style consistency between the generated image and the original image using a Gram matrix
style_consistency_reward = torch.nn.MSELoss()

# Define the caption reward, which measures the similarity between the generated caption and the given prompt using a Vicuna-7B model
caption_reward = torch.nn.CrossEntropyLoss()

# Define the SAM reward, which measures the likelihood of each category appearing in the prompted scene using a Vicuna-7B model
sam_reward = torch.nn.CrossEntropyLoss()

# Define the assemble reward-ranked learning strategy, which ranks and weights the rewards according to their importance for each prompt-image pair
assemble_reward_ranked_learning = AssembleRewardRankedLearning()

# Define some hyperparameters for training
batch_size = 32 # The number of prompt-image pairs in each batch
num_timesteps = 1000 # The number of timesteps in the Markov chain
beta_1 = 0.9 # The beta parameter for Adam optimizer
beta_2 = 0.999 # The beta parameter for Adam optimizer
learning_rate = 0.0001 # The learning rate for Adam optimizer

# Define an Adam optimizer for updating the diffusion model parameters
optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=learning_rate, betas=(beta_1, beta_2))

# Load the training data: a set of text prompts and corresponding images from MS-COCO dataset
training_data = load_mscoco_data()

# Train the text-to-image diffusion model with fine-grained semantic rewards
for epoch in range(num_epochs):
  for batch in training_data.batch(batch_size):
    # Extract the text prompts and images from the batch
    text_prompts = batch["text_prompts"]
    images = batch["images"]
    
    # Encode the text prompts into latent vectors using the text encoder
    text_latents = text_encoder(text_prompts).pooler_output
    
    # Initialize a list to store the generated images at each timestep
    generated_images = []
    
    # Initialize a noise sample with the same shape as the images
    noise_sample = torch.randn_like(images)
    
    # Generate an image by reversing a Markov chain from a noise sample using the diffusion model
    for timestep in range(num_timesteps):
      # Compute the noise level at this timestep using a cosine schedule
      noise_level = 0.5 * (1 + torch.cos(timestep / num_timesteps * math.pi))
      
      # Compute the noisy image at this timestep by adding noise to the original image
      noisy_image = noise_level * images + (1 - noise_level) * noise_sample
      
      # Predict the denoised image at this timestep using the diffusion model conditioned on the text latent vector
      denoised_image = diffusion_model(noisy_image, text_latents)
      
      # Compute the generated image at this timestep by subtracting noise from the denoised image
      generated_image = (denoised_image - noise_sample) / (1 - noise_level)
      
      # Append the generated image to the list
      generated_images.append(generated_image)
    
    # Encode the generated images into latent vectors using the image encoder
    image_latents = [image_encoder(generated_image).pooler_output for generated_image in generated_images]
    
    # Compute the image rewards for the generated images using the perceptual quality, diversity, and style consistency reward functions
    image_reward_scores = [perceptual_quality_reward(image_latent, image) + diversity_reward(image_latent) + style_consistency_reward(image_latent, image) for image_latent, image in zip(image_latents, images)]
    
    # Compute the caption reward for the generated images using the BLIP-2 model and the Vicuna-7B model
    generated_captions = [blip2_model(generated_image) for generated_image in generated_images]
    caption_reward_scores = [caption_reward(vicuna7b_model(text_prompt, generated_caption)) for text_prompt, generated_caption in zip(text_prompts, generated_captions)]
    
    # Compute the SAM reward for the generated images using the SAM model and the Vicuna-7B model
    segmented_images = [sam_model(generated_image) for generated_image in generated_images]
    sam_reward_scores = [sam_reward(vicuna7b_model(text_prompt, segmented_image)) for text_prompt, segmented_image in zip(text_prompts, segmented_images)]
    
    # Combine the image rewards and the semantic rewards into a list
    reward_scores = [image_reward_score + caption_reward_score + sam_reward_score for image_reward_score, caption_reward_score, sam_reward_score in zip(image_reward_scores, caption_reward_scores, sam_reward_scores)]
    
    # Rank and weight the rewards using the assemble reward-ranked learning strategy
    ranked_rewards, weighted_rewards = assemble_reward_ranked_learning(reward_scores)
    
    # Update the diffusion model parameters using the weighted rewards and the Adam optimizer
    optimizer.zero_grad()
    weighted_rewards.backward()
    optimizer.step()
```