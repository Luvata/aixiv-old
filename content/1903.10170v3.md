---
title: 1903.10170v3 LOGAN  Unpaired Shape Transform in Latent Overcomplete Space
date: 2019-03-11
---

# [LOGAN: Unpaired Shape Transform in Latent Overcomplete Space](http://arxiv.org/abs/1903.10170v3)

authors: Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, Hao Zhang


## What, Why and How

[1]: https://arxiv.org/pdf/1903.10170v3.pdf "LOGAN: Unpaired Shape Transform in Latent Overcomplete Space - arXiv.org"
[2]: https://arxiv.org/abs/1903.10170 "LOGAN: Unpaired Shape Transform in Latent Overcomplete Space"
[3]: https://arxiv-export3.library.cornell.edu/abs/2211.10170v3 "[2211.10170v3] A Banach space with an infinite dimensional reflexive ..."

Here is a summary of the paper:

- **What**: The paper introduces LOGAN, a deep neural network that can learn general-purpose shape transforms from unpaired domains, such as tables and chairs, letters and fonts, or cross-sectional profiles and surfaces.
- **Why**: The paper aims to address the challenge of learning shape transforms without any supervision or correspondence between shapes from different domains, which is a common scenario in real-world applications.
- **How**: The paper proposes a network that consists of an autoencoder to encode shapes into a common latent space with multi-scale shape features, and a generative adversarial network (GAN) to translate between domains in the latent space. The paper also introduces a feature preservation loss to ensure that the right shape features are preserved during shape translation. The paper evaluates the network on various examples and compares it with baselines and state-of-the-art approaches.

## Main Contributions

According to the paper, the main contributions are:

- A novel network architecture for unpaired shape transform in latent overcomplete space, which can learn what shape features to preserve depending on the input domains.
- A feature preservation loss that enforces the preservation of shape features during shape translation, which can handle both local and non-local features, as well as content and style features.
- A comprehensive evaluation of the network on various examples and comparisons with baselines and state-of-the-art approaches, demonstrating superior capabilities in unpaired shape transforms.

## Method Summary

Here is a summary of the method section:

- The method consists of three main components: an autoencoder, a translator, and a feature preservation loss.
- The autoencoder encodes shapes from two input domains into a common latent space, where the latent codes concatenate multi-scale shape features extracted by PointNet++ modules. The autoencoder also decodes the latent codes back to the original domains using a shared decoder.
- The translator is based on a GAN, which consists of a generator and a discriminator. The generator takes a latent code from one domain and translates it to the other domain by modifying the shape features. The discriminator tries to distinguish between real and fake latent codes in each domain.
- The feature preservation loss is defined as the distance between the original and translated latent codes in terms of their shape features. The loss ensures that the shape features that are not relevant for the translation are preserved, while the ones that are relevant are changed. The loss can handle both local and non-local features, as well as content and style features.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: two sets of shapes X and Y, each represented by a point cloud
# Output: a network that can transform a shape from X to Y and vice versa

# Define the autoencoder
encoder = PointNet++(input_shape) # encode a shape into a latent code
decoder = MLP(latent_code) # decode a latent code into a shape
autoencoder = encoder + decoder # combine the encoder and decoder

# Define the translator
generator = MLP(latent_code) # generate a translated latent code
discriminator = MLP(latent_code) # discriminate between real and fake latent codes
translator = generator + discriminator # combine the generator and discriminator

# Define the feature preservation loss
def feature_preservation_loss(original_code, translated_code):
  # compute the distance between the original and translated codes in terms of their shape features
  # use different distance metrics for different types of features (local, non-local, content, style)
  # return the weighted sum of the distances

# Train the network
for epoch in epochs:
  for batch in batches:
    # sample shapes from X and Y
    x = sample(X)
    y = sample(Y)

    # encode shapes into latent codes
    x_code = encoder(x)
    y_code = encoder(y)

    # translate latent codes between domains
    x2y_code = generator(x_code)
    y2x_code = generator(y_code)

    # decode latent codes back to shapes
    x2y = decoder(x2y_code)
    y2x = decoder(y2x_code)

    # compute the reconstruction loss for the autoencoder
    reconstruction_loss = mse_loss(x, decoder(x_code)) + mse_loss(y, decoder(y_code))

    # compute the adversarial loss for the translator
    adversarial_loss = bce_loss(discriminator(x_code), real) + bce_loss(discriminator(y_code), real) \
                     + bce_loss(discriminator(x2y_code), fake) + bce_loss(discriminator(y2x_code), fake)

    # compute the feature preservation loss for the translator
    feature_preservation_loss = feature_preservation_loss(x_code, x2y_code) + feature_preservation_loss(y_code, y2x_code)

    # compute the total loss for the network
    total_loss = reconstruction_loss + adversarial_loss + feature_preservation_loss

    # update the network parameters using gradient descent
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torch.nn as nn # for neural network modules
import torch.nn.functional as F # for activation functions
import torch.optim as optim # for optimization algorithms
import numpy as np # for numerical operations
import open3d as o3d # for point cloud processing

# Define the hyperparameters
batch_size = 32 # number of shapes per batch
num_points = 2048 # number of points per shape
latent_dim = 256 # dimension of the latent code
feature_dim = 128 # dimension of the shape feature
num_scales = 4 # number of scales for multi-scale encoding
num_epochs = 100 # number of training epochs
learning_rate = 0.0001 # learning rate for gradient descent
beta1 = 0.5 # beta1 parameter for Adam optimizer
beta2 = 0.999 # beta2 parameter for Adam optimizer
lambda_rec = 10 # weight for reconstruction loss
lambda_adv = 1 # weight for adversarial loss
lambda_fp = 10 # weight for feature preservation loss

# Define the PointNet++ module
class PointNetPP(nn.Module):
  def __init__(self, input_dim, output_dim, num_scales):
    super(PointNetPP, self).__init__()
    self.input_dim = input_dim # dimension of the input point cloud
    self.output_dim = output_dim # dimension of the output latent code
    self.num_scales = num_scales # number of scales for multi-scale encoding

    # Define the PointNet modules for each scale
    self.pointnets = nn.ModuleList()
    for i in range(num_scales):
      self.pointnets.append(PointNet(input_dim, feature_dim))

    # Define the fully connected layers to concatenate and compress the shape features
    self.fc1 = nn.Linear(num_scales * feature_dim, output_dim // 2)
    self.fc2 = nn.Linear(output_dim // 2, output_dim)

  def forward(self, x):
    # Input: x: a batch of point clouds of shape [batch_size, num_points, input_dim]
    # Output: y: a batch of latent codes of shape [batch_size, output_dim]

    # Initialize a list to store the shape features from each scale
    features = []

    # For each scale, apply PointNet to extract shape features and append them to the list
    for i in range(num_scales):
      f = self.pointnets[i](x) # f: a batch of shape features of shape [batch_size, feature_dim]
      features.append(f)

    # Concatenate the shape features from all scales along the last dimension
    y = torch.cat(features, dim=-1) # y: a batch of concatenated shape features of shape [batch_size, num_scales * feature_dim]

    # Apply fully connected layers to compress the concatenated shape features into a latent code
    y = F.relu(self.fc1(y)) # y: a batch of compressed shape features of shape [batch_size, output_dim // 2]
    y = self.fc2(y) # y: a batch of latent codes of shape [batch_size, output_dim]

    return y

# Define the PointNet module
class PointNet(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(PointNet, self).__init__()
    self.input_dim = input_dim # dimension of the input point cloud
    self.output_dim = output_dim # dimension of the output shape feature

    # Define the convolutional layers to process the point cloud
    self.conv1 = nn.Conv1d(input_dim, 64, 1)
    self.conv2 = nn.Conv1d(64, 128, 1)
    self.conv3 = nn.Conv1d(128, output_dim, 1)

  def forward(self, x):
    # Input: x: a batch of point clouds of shape [batch_size, num_points, input_dim]
    # Output: y: a batch of shape features of shape [batch_size, output_dim]

    # Transpose x to match the convolutional layer input format
    x = x.transpose(1, 2) # x: a batch of transposed point clouds of shape [batch_size, input_dim, num_points]

    # Apply convolutional layers to process the point cloud and extract local features
    x = F.relu(self.conv1(x)) # x: a batch of local features of shape [batch_size, 64, num_points]
    x = F.relu(self.conv2(x)) # x: a batch of local features of shape [batch_size, 128, num_points]
    x = self.conv3(x) # x: a batch of local features of shape [batch_size, output_dim, num_points]

    # Apply max pooling to aggregate the local features into a global shape feature
    y = F.max_pool1d(x, num_points) # y: a batch of global shape features of shape [batch_size, output_dim, 1]

    # Squeeze the last dimension to get the final shape feature
    y = y.squeeze(-1) # y: a batch of shape features of shape [batch_size, output_dim]

    return y

# Define the MLP module
class MLP(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(MLP, self).__init__()
    self.input_dim = input_dim # dimension of the input vector
    self.output_dim = output_dim # dimension of the output vector

    # Define the fully connected layers to process the input vector
    self.fc1 = nn.Linear(input_dim, input_dim // 2)
    self.fc2 = nn.Linear(input_dim // 2, output_dim)

  def forward(self, x):
    # Input: x: a batch of input vectors of shape [batch_size, input_dim]
    # Output: y: a batch of output vectors of shape [batch_size, output_dim]

    # Apply fully connected layers to process the input vector
    y = F.relu(self.fc1(x)) # y: a batch of intermediate vectors of shape [batch_size, input_dim // 2]
    y = self.fc2(y) # y: a batch of output vectors of shape [batch_size, output_dim]

    return y

# Define the encoder module
class Encoder(nn.Module):
  def __init__(self, input_dim, latent_dim, num_scales):
    super(Encoder, self).__init__()
    self.input_dim = input_dim # dimension of the input point cloud
    self.latent_dim = latent_dim # dimension of the latent code
    self.num_scales = num_scales # number of scales for multi-scale encoding

    # Define the PointNet++ module to encode the point cloud into a latent code
    self.pointnetpp = PointNetPP(input_dim, latent_dim, num_scales)

  def forward(self, x):
    # Input: x: a batch of point clouds of shape [batch_size, num_points, input_dim]
    # Output: y: a batch of latent codes of shape [batch_size, latent_dim]

    # Apply PointNet++ to encode the point cloud into a latent code
    y = self.pointnetpp(x) # y: a batch of latent codes of shape [batch_size, latent_dim]

    return y

# Define the decoder module
class Decoder(nn.Module):
  def __init__(self, latent_dim, output_dim):
    super(Decoder, self).__init__()
    self.latent_dim = latent_dim # dimension of the latent code
    self.output_dim = output_dim # dimension of the output point cloud

    # Define the fully connected layers to decode the latent code into a point cloud
    self.fc1 = nn.Linear(latent_dim, latent_dim * 2)
    self.fc2 = nn.Linear(latent_dim * 2, num_points * output_dim)

  def forward(self, x):
    # Input: x: a batch of latent codes of shape [batch_size, latent_dim]
    # Output: y: a batch of point clouds of shape [batch_size, num_points, output_dim]

    # Apply fully connected layers to decode the latent code into a point cloud
    x = F.relu(self.fc1(x)) # x: a batch of intermediate vectors of shape [batch_size, latent_dim * 2]
    x = self.fc2(x) # x: a batch of flattened point clouds of shape [batch_size, num_points * output_dim]

    # Reshape x to match the point cloud format
    y = x.view(batch_size, num_points, output_dim) # y: a batch of point clouds of shape [batch_size, num_points, output_dim]

    return y

# Define the generator module
class Generator(nn.Module):
  def __init__(self, latent_dim):
    super(Generator, self).__init__()
    self.latent_dim = latent_dim # dimension of the latent code

    # Define the MLP module to translate the latent code between domains
    self.mlp = MLP(latent_dim, latent_dim)

  def forward(self, x):
    # Input: x: a batch of latent codes from one domain of shape [batch_size, latent_dim]
    # Output: y: a batch of translated latent codes from another domain of shape [batch_size, latent_dim]

    # Apply MLP to translate the latent code between domains
    y = self.mlp(x) # y: a batch