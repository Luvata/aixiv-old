---
title: 2211.09800v2 InstructPix2Pix  Learning to Follow Image Editing Instructions
date: 2022-11-10
---

# [InstructPix2Pix: Learning to Follow Image Editing Instructions](http://arxiv.org/abs/2211.09800v2)

authors: Tim Brooks, Aleksander Holynski, Alexei A. Efros


## What, Why and How

[1]: https://arxiv.org/abs/2211.09800v2 "InstructPix2Pix: Learning to Follow Image Editing Instructions"
[2]: https://arxiv.org/pdf/2211.09800v2.pdf "InstructPix2Pix: Learning to Follow Image Editing Instructions - arXiv.org"
[3]: http://export.arxiv.org/abs/2211.09800 "[2211.09800] InstructPix2Pix: Learning to Follow Image Editing Instructions"

Here is a summary of the paper:

- **What**: The paper proposes a method for editing images from human instructions, using a conditional diffusion model called InstructPix2Pix[^1^][1].
- **Why**: The paper aims to enable intuitive and diverse image editing that can follow natural language instructions, without requiring full descriptions of the input/output images or per-example fine-tuning[^1^][1].
- **How**: The paper generates a large dataset of image editing examples by combining the knowledge of two large pretrained models: a language model (GPT-3) and a text-to-image model (Stable Diffusion). The paper then trains InstructPix2Pix on the generated data, and shows that it can generalize to real images and user-written instructions at inference time[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2211.09800v2 "InstructPix2Pix: Learning to Follow Image Editing Instructions"
[2]: https://arxiv.org/pdf/2211.09800v2.pdf "InstructPix2Pix: Learning to Follow Image Editing Instructions - arXiv.org"
[3]: http://export.arxiv.org/abs/2211.09800 "[2211.09800] InstructPix2Pix: Learning to Follow Image Editing Instructions"

According to the paper[^1^][1], the main contributions are:

- A method for generating a large dataset of image editing examples by combining the knowledge of two large pretrained models: a language model (GPT-3) and a text-to-image model (Stable Diffusion).
- A conditional diffusion model (InstructPix2Pix) that can edit images from human instructions, without requiring full descriptions of the input/output images or per-example fine-tuning.
- A demonstration of zero-shot generalization to real images and user-written instructions, and a diverse collection of image editing results.

## Method Summary

[1]: https://arxiv.org/abs/2211.09800v2 "InstructPix2Pix: Learning to Follow Image Editing Instructions"
[2]: https://arxiv.org/pdf/2211.09800v2.pdf "InstructPix2Pix: Learning to Follow Image Editing Instructions - arXiv.org"
[3]: http://export.arxiv.org/abs/2211.09800 "[2211.09800] InstructPix2Pix: Learning to Follow Image Editing Instructions"

Here is a summary of the method section of the paper:

- The paper describes how to generate a large dataset of image editing examples by using two large pretrained models: a language model (GPT-3) and a text-to-image model (Stable Diffusion). The paper first samples a random image from the text-to-image model, and then uses the language model to generate an instruction for how to edit it. The paper then samples another image from the text-to-image model conditioned on the instruction, and uses it as the target image. The paper repeats this process to obtain a large number of input-output pairs with corresponding instructions[^1^][1].
- The paper introduces InstructPix2Pix, a conditional diffusion model that can edit images from human instructions. The paper uses a modified version of Stable Diffusion that takes both an input image and a text instruction as conditioning. The paper trains InstructPix2Pix on the generated dataset using the standard diffusion loss[^1^][1].
- The paper evaluates InstructPix2Pix on real images and user-written instructions, and shows that it can perform diverse and realistic edits. The paper also compares InstructPix2Pix with other baselines, such as CLIP-guided diffusion and StyleCLIP, and shows that InstructPix2Pix can better follow natural language instructions and preserve the input image content[^1^][1].

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Generate a large dataset of image editing examples
dataset = []
for i in range(num_examples):
  # Sample a random image from the text-to-image model
  input_image = sample_image_from_text_to_image_model()
  # Generate an instruction for how to edit the image using the language model
  instruction = generate_instruction_from_language_model(input_image)
  # Sample another image from the text-to-image model conditioned on the instruction
  target_image = sample_image_from_text_to_image_model(instruction)
  # Add the input-output pair and the instruction to the dataset
  dataset.append((input_image, target_image, instruction))

# Train InstructPix2Pix on the generated dataset
model = InstructPix2Pix()
for epoch in range(num_epochs):
  for batch in dataset:
    # Get the input images, target images and instructions from the batch
    input_images, target_images, instructions = batch
    # Encode the instructions into embeddings using a pretrained transformer
    instruction_embeddings = encode_instructions(instructions)
    # Train the model using the diffusion loss
    loss = diffusion_loss(model, input_images, target_images, instruction_embeddings)
    # Update the model parameters using gradient descent
    update_parameters(model, loss)

# Evaluate InstructPix2Pix on real images and user-written instructions
for test_example in test_set:
  # Get the input image and the user-written instruction from the test example
  input_image, instruction = test_example
  # Encode the instruction into an embedding using a pretrained transformer
  instruction_embedding = encode_instruction(instruction)
  # Generate the edited image using InstructPix2Pix in the forward pass
  edited_image = generate_image(model, input_image, instruction_embedding)
  # Display the input image, the edited image and the instruction
  display(input_image, edited_image, instruction)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import diffusion

# Define the hyperparameters
num_examples = 1000000 # The number of image editing examples to generate
num_epochs = 100 # The number of epochs to train InstructPix2Pix
batch_size = 32 # The batch size for training and evaluation
image_size = 256 # The size of the input and output images
text_length = 32 # The maximum length of the instructions
hidden_size = 512 # The hidden size of the instruction encoder
num_heads = 8 # The number of attention heads of the instruction encoder
num_layers = 6 # The number of layers of the instruction encoder
vocab_size = 50257 # The vocabulary size of the instruction encoder
num_timesteps = 1000 # The number of diffusion timesteps for InstructPix2Pix

# Load the pretrained models
text_to_image_model = diffusion.StableDiffusion.load_from_checkpoint("stable_diffusion.pt") # Load the Stable Diffusion model pretrained on ImageNet
language_model = transformers.GPT3LMHeadModel.from_pretrained("gpt3-large") # Load the GPT-3 model with 1.3B parameters

# Generate a large dataset of image editing examples
dataset = []
for i in range(num_examples):
  # Sample a random image from the text-to-image model
  input_image = text_to_image_model.sample(image_size) # A tensor of shape [3, image_size, image_size]
  # Generate an instruction for how to edit the image using the language model
  instruction = language_model.generate(input_image, max_length=text_length) # A tensor of shape [text_length]
  # Sample another image from the text-to-image model conditioned on the instruction
  target_image = text_to_image_model.sample(image_size, instruction) # A tensor of shape [3, image_size, image_size]
  # Add the input-output pair and the instruction to the dataset
  dataset.append((input_image, target_image, instruction))

# Define InstructPix2Pix as a conditional diffusion model with an instruction encoder
class InstructPix2Pix(diffusion.Diffusion):
  def __init__(self):
    super().__init__()
    # Define the instruction encoder as a transformer encoder
    self.instruction_encoder = transformers.TransformerEncoder(transformers.TransformerEncoderLayer(hidden_size, num_heads), num_layers)
    # Define the embedding layer for the instruction tokens
    self.embedding = torch.nn.Embedding(vocab_size, hidden_size)
    # Define the projection layer for the instruction embedding
    self.projection = torch.nn.Linear(hidden_size, hidden_size)
    # Define the UNet model for the image encoder and decoder
    self.unet = torchvision.models.segmentation.fcn_resnet50(pretrained=True)
  
  def encode(self, x):
    # Encode an image x into a hidden representation h
    h = self.unet.backbone(x) # A tensor of shape [batch_size, hidden_size, image_size/32, image_size/32]
    return h
  
  def decode(self, h, c):
    # Decode a hidden representation h and a conditioning vector c into an image x_mean
    h_c = torch.cat([h, c.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, h.shape[-2], h.shape[-1])], dim=1) # A tensor of shape [batch_size, hidden_size*2, image_size/32, image_size/32]
    x_mean = self.unet.classifier(h_c) # A tensor of shape [batch_size, 3, image_size/32, image_size/32]
    x_mean = torch.nn.functional.interpolate(x_mean, size=image_size) # A tensor of shape [batch_size, 3, image_size, image_size]
    return x_mean
  
  def encode_instruction(self, y):
    # Encode an instruction y into a conditioning vector c
    y_embed = self.embedding(y) # A tensor of shape [batch_size, text_length, hidden_size]
    y_mask = (y != 0).unsqueeze(1).unsqueeze(2) # A tensor of shape [batch_size, 1, 1, text_length]
    y_encoded = self.instruction_encoder(y_embed, src_key_padding_mask=~y_mask) # A tensor of shape [batch_size, text_length, hidden_size]
    y_pooled = torch.mean(y_encoded * y_mask.float(), dim=1) # A tensor of shape [batch_size, hidden_size]
    c = self.projection(y_pooled) # A tensor of shape [batch_size, hidden_size]
    return c

# Train InstructPix2Pix on the generated dataset
model = InstructPix2Pix()
optimizer = torch.optim.Adam(model.parameters())
for epoch in range(num_epochs):
  for batch in torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True):
    # Get the input images, target images and instructions from the batch
    input_images, target_images, instructions = batch
    # Encode the instructions into embeddings using the instruction encoder
    instruction_embeddings = model.encode_instruction(instructions)
    # Train the model using the diffusion loss
    loss = model.diffusion_loss(input_images, target_images, instruction_embeddings)
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Evaluate InstructPix2Pix on real images and user-written instructions
for test_example in test_set:
  # Get the input image and the user-written instruction from the test example
  input_image, instruction = test_example
  # Encode the instruction into an embedding using the instruction encoder
  instruction_embedding = model.encode_instruction(instruction)
  # Generate the edited image using InstructPix2Pix in the forward pass
  edited_image = model.generate(input_image, instruction_embedding, num_timesteps)
  # Display the input image, the edited image and the instruction
  display(input_image, edited_image, instruction)
```