---
title: 2104.14754v2 Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing
date: 2021-04-15
---

# [Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing](http://arxiv.org/abs/2104.14754v2)

authors: Hyunsu Kim, Yunjey Choi, Junho Kim, Sungjoo Yoo, Youngjung Uh


## What, Why and How

[1]: https://arxiv.org/pdf/2104.14754v2 "Reconstruction - arXiv.org"
[2]: https://arxiv.org/abs/2104.14754 "[2104.14754] Exploiting Spatial Dimensions of Latent in GAN for Real ..."
[3]: http://export.arxiv.org/abs/2304.14754v2 "[2304.14754v2] Shadows and quasinormal modes of the Bardeen black hole ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a novel method for real-time image editing using generative adversarial networks (GANs). The method, called StyleMapGAN, exploits the spatial dimensions of the latent space and replaces the adaptive instance normalization (AdaIN) with a spatially variant modulation.
- **Why**: The paper aims to overcome the limitations of existing methods for manipulating real images with GANs, such as time-consuming optimization, inaccurate embedding, or lack of user controllability.
- **How**: The paper introduces a new intermediate latent space that has spatial dimensions and preserves the spatial information of the input image. The paper also proposes a spatially variant modulation that modulates the feature maps according to the spatial location of the latent code. The paper shows that these modifications enable accurate and flexible image editing on various datasets and tasks, such as local editing, image interpolation, and semantic manipulation. The paper also demonstrates that conventional editing methods on GANs are still applicable on StyleMapGAN.

## Main Contributions

[1]: https://arxiv.org/pdf/2104.14754v2 "Reconstruction - arXiv.org"
[2]: https://arxiv.org/abs/2104.14754 "[2104.14754] Exploiting Spatial Dimensions of Latent in GAN for Real ..."
[3]: http://export.arxiv.org/abs/2304.14754v2 "[2304.14754v2] Shadows and quasinormal modes of the Bardeen black hole ..."

According to the paper[^1^][1], the main contributions are:

- **StyleMapGAN**: A novel method for real-time image editing using GANs that exploits the spatial dimensions of the latent space and replaces AdaIN with a spatially variant modulation.
- **Accurate embedding**: A demonstration that StyleMapGAN can project real images into the latent space more accurately than existing optimization-based methods while maintaining the properties of GANs.
- **Flexible manipulation**: A validation that StyleMapGAN can perform various image manipulation tasks such as local editing, image interpolation, and semantic manipulation on multiple datasets with high fidelity and user controllability.
- **Compatibility**: A confirmation that conventional editing methods on GANs are still valid on StyleMapGAN and can be combined with the proposed method for further improvement.

## Method Summary

[1]: https://arxiv.org/pdf/2104.14754v2 "Reconstruction - arXiv.org"
[2]: https://arxiv.org/abs/2104.14754 "[2104.14754] Exploiting Spatial Dimensions of Latent in GAN for Real ..."
[3]: http://export.arxiv.org/abs/2304.14754v2 "[2304.14754v2] Shadows and quasinormal modes of the Bardeen black hole ..."

Here is a summary of the method section of the paper[^1^][1]:

- **StyleMapGAN**: The paper introduces a new intermediate latent space that has spatial dimensions and preserves the spatial information of the input image. The paper also proposes a spatially variant modulation that modulates the feature maps according to the spatial location of the latent code. The paper shows that these modifications enable accurate and flexible image editing on various datasets and tasks, such as local editing, image interpolation, and semantic manipulation. The paper also demonstrates that conventional editing methods on GANs are still applicable on StyleMapGAN and can be combined with the proposed method for further improvement.
- **Network architecture**: The paper describes the network architecture of StyleMapGAN, which consists of an encoder, a generator, and a discriminator. The encoder maps an input image to an intermediate latent code with spatial dimensions. The generator synthesizes an output image from the latent code using a spatially variant modulation. The discriminator distinguishes between real and fake images. The paper also explains how to train StyleMapGAN using a combination of reconstruction loss, adversarial loss, perceptual loss, and style loss.
- **Image editing**: The paper presents various image editing methods using StyleMapGAN, such as local editing, image interpolation, and semantic manipulation. The paper also explains how to perform conventional editing methods on GANs, such as style mixing, inversion, and interpolation, on StyleMapGAN. The paper provides examples and comparisons of these methods on different datasets and tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder network
encoder = Encoder()

# Define the generator network
generator = Generator()

# Define the discriminator network
discriminator = Discriminator()

# Define the losses
reconstruction_loss = L1Loss()
adversarial_loss = HingeLoss()
perceptual_loss = VGGPerceptualLoss()
style_loss = StyleLoss()

# Define the optimizer
optimizer = Adam(lr=0.0002, beta1=0.5, beta2=0.999)

# Train the StyleMapGAN
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the input image and the reference image
    input_image = batch["input"]
    reference_image = batch["reference"]

    # Encode the input image and the reference image to the latent space
    input_latent = encoder(input_image)
    reference_latent = encoder(reference_image)

    # Generate the output image from the input latent code
    output_image = generator(input_latent)

    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(output_image, input_image)

    # Compute the adversarial loss for the generator and the discriminator
    fake_logits = discriminator(output_image)
    real_logits = discriminator(input_image)
    gen_adv_loss = adversarial_loss(fake_logits, True)
    dis_adv_loss = adversarial_loss(fake_logits, False) + adversarial_loss(real_logits, True)

    # Compute the perceptual loss and the style loss
    per_loss = perceptual_loss(output_image, input_image)
    sty_loss = style_loss(output_image, input_image)

    # Compute the total generator loss and the total discriminator loss
    gen_loss = rec_loss + gen_adv_loss + per_loss + sty_loss
    dis_loss = dis_adv_loss

    # Update the generator and the discriminator parameters
    optimizer.zero_grad()
    gen_loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    dis_loss.backward()
    optimizer.step()

# Image editing using StyleMapGAN
# Local editing: mix multiple parts of reference images with the input image
def local_editing(input_image, reference_images, masks):
  # Encode the input image and the reference images to the latent space
  input_latent = encoder(input_image)
  reference_latents = [encoder(reference_image) for reference_image in reference_images]

  # Apply masks to the reference latents to select the desired parts
  masked_latents = [reference_latent * mask for reference_latent, mask in zip(reference_latents, masks)]

  # Add up the masked latents to get the mixed latent code
  mixed_latent = input_latent + sum(masked_latents)

  # Generate the output image from the mixed latent code
  output_image = generator(mixed_latent)

  return output_image

# Image interpolation: generate smooth transitions between two images
def image_interpolation(image1, image2, num_steps):
  # Encode the two images to the latent space
  latent1 = encoder(image1)
  latent2 = encoder(image2)

  # Interpolate between the two latents with linear interpolation
  interpolated_latents = [latent1 + (latent2 - latent1) * alpha for alpha in np.linspace(0, 1, num_steps)]

  # Generate the output images from the interpolated latents
  output_images = [generator(interpolated_latent) for interpolated_latent in interpolated_latents]

  return output_images

# Semantic manipulation: change specific attributes of an image using a semantic map
def semantic_manipulation(input_image, semantic_map):
  # Encode the input image to the latent space
  input_latent = encoder(input_image)

  # Apply semantic map to modify specific locations of the latent code
  modified_latent = input_latent * semantic_map

  # Generate the output image from the modified latent code
  output_image = generator(modified_latent)

  return output_image

# Conventional editing methods on GANs using StyleMapGAN

# Style mixing: mix two styles at different layers of the generator
def style_mixing(style1, style2, layer):
  # Generate two random latent vectors from a normal distribution
  z1 = torch.randn(1, latent_dim)
  z2 = torch.randn(1, latent_dim)

  # Map them to two intermediate latent codes using a mapping network (same as StyleGAN)
  w1 = mapping_network(z1)
  w2 = mapping_network(z2)

  # Convert them to two spatially variant latent codes using a broadcast network (same as StyleMapGAN)
  style1 = broadcast_network(w1)
  style2 = broadcast_network(w2)

  # Mix the two styles at a given layer of the generator
  mixed_style = style1.clone()
  mixed_style[:, :, layer:, :] = style2[:, :, layer:, :]

  # Generate the output image from the mixed style
  output_image = generator(mixed_style)

  return output_image

# Inversion: find the latent code that best reconstructs a given image
def inversion(image, num_iterations):
  # Initialize a random latent vector from a normal distribution
  z = torch.randn(1, latent_dim, requires_grad=True)

  # Map it to an intermediate latent code using a mapping network (same as StyleGAN)
  w = mapping_network(z)

  # Convert it to a spatially variant latent code using a broadcast network (same as StyleMapGAN)
  style = broadcast_network(w)

  # Optimize the latent vector to minimize the reconstruction loss
  optimizer = Adam([z], lr=0.01)
  for i in range(num_iterations):
    # Generate the output image from the style
    output_image = generator(style)

    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(output_image, image)

    # Update the latent vector
    optimizer.zero_grad()
    rec_loss.backward()
    optimizer.step()

    # Update the style
    w = mapping_network(z)
    style = broadcast_network(w)

  return style

# Interpolation: generate smooth transitions between two styles
def interpolation(style1, style2, num_steps):
  # Interpolate between the two styles with linear interpolation
  interpolated_styles = [style1 + (style2 - style1) * alpha for alpha in np.linspace(0, 1, num_steps)]

  # Generate the output images from the interpolated styles
  output_images = [generator(interpolated_style) for interpolated_style in interpolated_styles]

  return output_images
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# Define the hyperparameters
latent_dim = 512 # The dimension of the latent vector
num_layers = 18 # The number of layers in the generator
num_channels = 3 # The number of channels in the image
image_size = 256 # The size of the image
num_filters = 32 # The number of filters in the first layer of the generator and the discriminator
kernel_size = 3 # The kernel size for convolutional layers
padding = 1 # The padding for convolutional layers
stride = 2 # The stride for convolutional and deconvolutional layers
upsample_factor = 2 # The upsample factor for deconvolutional layers
leaky_relu_slope = 0.2 # The slope for leaky ReLU activation
num_classes = 10 # The number of classes for conditional GANs

# Define the encoder network
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()

    # Define the convolutional layers with leaky ReLU activation and instance normalization
    self.conv1 = nn.Conv2d(num_channels, num_filters, kernel_size, stride, padding)
    self.in1 = nn.InstanceNorm2d(num_filters)
    self.lrelu1 = nn.LeakyReLU(leaky_relu_slope)

    self.conv2 = nn.Conv2d(num_filters, num_filters * 2, kernel_size, stride, padding)
    self.in2 = nn.InstanceNorm2d(num_filters * 2)
    self.lrelu2 = nn.LeakyReLU(leaky_relu_slope)

    self.conv3 = nn.Conv2d(num_filters * 2, num_filters * 4, kernel_size, stride, padding)
    self.in3 = nn.InstanceNorm2d(num_filters * 4)
    self.lrelu3 = nn.LeakyReLU(leaky_relu_slope)

    self.conv4 = nn.Conv2d(num_filters * 4, num_filters * 8, kernel_size, stride, padding)
    self.in4 = nn.InstanceNorm2d(num_filters * 8)
    self.lrelu4 = nn.LeakyReLU(leaky_relu_slope)

    self.conv5 = nn.Conv2d(num_filters * 8, num_filters * 16, kernel_size, stride, padding)
    self.in5 = nn.InstanceNorm2d(num_filters * 16)
    self.lrelu5 = nn.LeakyReLU(leaky_relu_slope)

    # Define the fully connected layer to output the latent code with spatial dimensions
    self.fc = nn.Linear(num_filters * 16 * (image_size // (stride ** 5)) ** 2, latent_dim * num_layers)

  def forward(self, x):
    # Apply the convolutional layers to the input image
    x = self.lrelu1(self.in1(self.conv1(x)))
    x = self.lrelu2(self.in2(self.conv2(x)))
    x = self.lrelu3(self.in3(self.conv3(x)))
    x = self.lrelu4(self.in4(self.conv4(x)))
    x = self.lrelu5(self.in5(self.conv5(x)))

    # Flatten the feature maps
    x = x.view(x.size(0), -1)

    # Apply the fully connected layer to get the latent code
    x = self.fc(x)

    # Reshape the latent code to have spatial dimensions
    x = x.view(x.size(0), latent_dim, num_layers, 1)

    return x

# Define the generator network
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()

    # Define the constant input vector for each layer of the generator
    self.constant_input = nn.Parameter(torch.randn(1, num_filters * 16, image_size // (stride ** 5), image_size // (stride ** 5)))

    # Define the deconvolutional layers with leaky ReLU activation and instance normalization
    self.deconv1 = nn.ConvTranspose2d(num_filters * 16, num_filters * 8, kernel_size, stride, padding)
    self.in1 = nn.InstanceNorm2d(num_filters * 8)
    self.lrelu1 = nn.LeakyReLU(leaky_relu_slope)

    self.deconv2 = nn.ConvTranspose2d(num_filters * 8 + num_classes, num_filters * 4, kernel_size, stride, padding)
    self.in2 = nn.InstanceNorm2d(num_filters * 4)
    self.lrelu2 = nn.LeakyReLU(leaky_relu_slope)

    self.deconv3 = nn.ConvTranspose2d(num_filters * 4 + num_classes, num_filters * 2, kernel_size, stride, padding)
    self.in3 = nn.InstanceNorm2d(num_filters * 2)
    self.lrelu3 = nn.LeakyReLU(leaky_relu_slope)

    self.deconv4 = nn.ConvTranspose2d(num_filters * 2 + num_classes, num_filters, kernel_size, stride, padding)
    self.in4 = nn.InstanceNorm2d(num_filters)
    self.lrelu4 = nn.LeakyReLU(leaky_relu_slope)

    self.deconv5 = nn.ConvTranspose2d(num_filters + num_classes, num_channels, kernel_size, stride, padding)
    self.tanh = nn.Tanh()

    # Define the spatially variant modulation layers for each layer of the generator
    self.svm1 = SpatiallyVariantModulation(num_filters * 16)
    self.svm2 = SpatiallyVariantModulation(num_filters * 8)
    self.svm3 = SpatiallyVariantModulation(num_filters * 4)
    self.svm4 = SpatiallyVariantModulation(num_filters * 2)
    self.svm5 = SpatiallyVariantModulation(num_filters)

  def forward(self, x, y):
    # Get the batch size and the number of classes
    batch_size = x.size(0)
    num_classes = y.size(1)

    # Repeat the constant input vector for each batch
    x = self.constant_input.repeat(batch_size, 1, 1, 1)

    # Apply the spatially variant modulation to the first layer
    x = self.svm1(x, x[:, :, 0])

    # Apply the deconvolutional layer to the first layer
    x = self.lrelu1(self.in1(self.deconv1(x)))

    # Concatenate the class label to the feature maps
    y = y.view(batch_size, num_classes, 1, 1)
    y = y.repeat(1, 1, x.size(2), x.size(3))
    x = torch.cat([x, y], dim=1)

    # Apply the spatially variant modulation to the second layer
    x = self.svm2(x, x[:, :, 1])

    # Apply the deconvolutional layer to the second layer
    x = self.lrelu2(self.in2(self.deconv2(x)))

    # Concatenate the class label to the feature maps
    y = y.repeat(1, 1, upsample_factor, upsample_factor)
    x = torch.cat([x, y], dim=1)

    # Apply the spatially variant modulation to the third layer
    x = self.svm3(x, x[:, :, 2])

    # Apply the deconvolutional layer to the third layer
    x = self.lrelu3(self.in3(self.deconv3(x)))

    # Concatenate the class label to the feature maps
    y = y.repeat(1, 1, upsample_factor, upsample_factor)
    x = torch.cat([x, y], dim=1)

    # Apply the spatially variant modulation to the fourth layer
    x = self.svm4(x, x[:, :, 3])

    # Apply the deconvolutional layer to the fourth layer
    x = self.lrelu4(self.in4(self.deconv4(x)))

    # Concatenate the class label to the feature maps
    y = y.repeat(1, 1, upsample_factor, upsample_factor)
    x = torch.cat([x, y], dim=1)

    # Apply the spatially variant modulation to the fifth layer
    x = self.svm5(x, x[:, :, 4])

    # Apply the deconvolutional layer to the fifth layer
    x = self.tanh(self.deconv5(x))

    return x

# Define the spatially variant modulation layer
class SpatiallyVariantModulation(nn.Module):
  def __init__(self, num_features):
    super(SpatiallyVariantModulation, self).__init__()

    # Define two fully connected layers to output the scale and bias parameters for each location of the latent code
    self.scale_fc = nn.Linear(latent_dim, num_features)
    self.bias_fc = nn.Linear(latent_dim, num_features)

  def forward(self, x, w):
    # Get the batch size and the number of features
    batch_size = x.size(0)
    num_features = x.size(1)

    # Apply the fully connected layers to get the scale and bias parameters
    scale = self.scale_fc(w).view(batch_size, num_features, 1) + 1.0
    bias = self.bias_fc(w).view(batch_size, num_features)

    # Modulate the feature maps by multiplying with scale and