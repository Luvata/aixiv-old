---
title: 1901.04544v2 PointWise  An Unsupervised Point-wise Feature Learning Network
date: 2019-01-05
---

# [PointWise: An Unsupervised Point-wise Feature Learning Network](http://arxiv.org/abs/1901.04544v2)

authors: Matan Shoef, Sharon Fogel, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1901.04544v2 "PointWise: An Unsupervised Point-wise Feature Learning Network"
[2]: https://arxiv.org/abs/1901.04544 "PointWise: An Unsupervised Point-wise Feature Learning Network"
[3]: https://arxiv-export3.library.cornell.edu/abs/2201.04544v2 "[2201.04544v2] Rapid generation of all-optical $^{39}$K Bose-Einstein ..."

Here is a summary of the paper:

- **What**: The paper presents a novel approach to learning a point-wise, meaningful embedding for point-clouds in an unsupervised manner, through the use of neural-networks. Point-clouds are sets of points that represent the shape of an object in 3D space.
- **Why**: The paper aims to find ways of characterizing point-cloud data, which is abundant and useful for various applications, such as computer graphics, computer vision, and robotics. However, existing methods for unsupervised learning are not well adapted to the point-cloud representation, which is sparse, unordered, and irregular.
- **How**: The paper proposes a deep learning framework that leverages self-supervision to define a relevant loss function to learn rich per-point features. The framework uses local structures of point-clouds to incorporate geometric information into each point's latent representation. It also encourages adjacent points to have similar representations and vice-versa, creating a smoother, more descriptive representation. The paper demonstrates the ability of the method to capture meaningful point-wise features through three applications: unsupervised part-segmentation, semantic point-analogies, and point-correspondence.

The summary is based on the abstract and introduction of the paper[^1^][1] [^2^][2].


## Main Contributions

[1]: https://arxiv.org/abs/1901.04544v2 "PointWise: An Unsupervised Point-wise Feature Learning Network"
[2]: https://arxiv.org/abs/1901.04544 "PointWise: An Unsupervised Point-wise Feature Learning Network"
[3]: https://arxiv-export3.library.cornell.edu/abs/2201.04544v2 "[2201.04544v2] Rapid generation of all-optical $^{39}$K Bose-Einstein ..."

According to the paper, the main contributions are:

- **A novel unsupervised point-wise feature learning network** that learns meaningful embeddings for point-clouds without any supervision or annotation.
- **A self-supervised loss function** that uses local structures and adjacency information to guide the learning process and create smooth and descriptive representations.
- **Three applications** that demonstrate the effectiveness and usefulness of the learned point-wise features: unsupervised part-segmentation, semantic point-analogies, and point-correspondence.

The contributions are stated in the abstract and section 1.1 of the paper[^1^][1] [^2^][2].


## Method Summary

[1]: https://arxiv.org/abs/1901.04544v2 "PointWise: An Unsupervised Point-wise Feature Learning Network"
[2]: https://arxiv.org/abs/1901.04544 "PointWise: An Unsupervised Point-wise Feature Learning Network"
[3]: https://arxiv-export3.library.cornell.edu/abs/2201.04544v2 "[2201.04544v2] Rapid generation of all-optical $^{39}$K Bose-Einstein ..."

Here is a summary of the method section of the paper:

- The paper proposes a neural network architecture that consists of three main components: a **feature extractor**, a **local structure encoder**, and a **point-wise embedding layer**.
- The feature extractor takes as input a point-cloud and outputs a set of per-point features that capture the local geometry and context of each point. The feature extractor is based on PointNet , a state-of-the-art network for point-cloud processing.
- The local structure encoder takes as input the per-point features and outputs a set of local structures that represent the neighborhoods of each point. The local structure encoder is based on PointNet++ , an extension of PointNet that hierarchically groups points into local regions.
- The point-wise embedding layer takes as input the local structures and outputs a set of point-wise embeddings that encode the semantic and geometric information of each point. The point-wise embedding layer is based on a fully connected network with batch normalization and ReLU activation.
- The paper defines a self-supervised loss function that consists of two terms: a **local structure loss** and an **adjacency loss**. The local structure loss encourages points that belong to the same local structure to have similar embeddings, while the adjacency loss encourages points that are adjacent in the point-cloud to have similar embeddings and vice-versa. The loss function is minimized using stochastic gradient descent with Adam optimizer.

The summary is based on section 3 of the paper[^1^][1] [^2^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a point-cloud P with N points
# Output: a point-wise embedding E with N vectors

# Define the feature extractor F based on PointNet
# Define the local structure encoder L based on PointNet++
# Define the point-wise embedding layer E based on a fully connected network

# Initialize the network parameters randomly
# Define the local structure loss Ls and the adjacency loss La
# Define the total loss L = Ls + La

# Repeat until convergence:
  # Sample a batch of point-clouds from the dataset
  # For each point-cloud in the batch:
    # Apply F to get per-point features
    # Apply L to get local structures
    # Apply E to get point-wise embeddings
    # Compute Ls and La using the embeddings and the point-cloud
    # Compute L as the sum of Ls and La
  # Update the network parameters using gradient descent with Adam optimizer

# Return E as the final point-wise embedding
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a point-cloud P with N points, each with D dimensions
# Output: a point-wise embedding E with N vectors, each with K dimensions

# Define the feature extractor F based on PointNet
# F consists of a series of convolutional layers followed by a max-pooling layer
# F takes as input a point-cloud P and outputs a per-point feature matrix Fp with N rows and M columns

# Define the local structure encoder L based on PointNet++
# L consists of a series of set abstraction layers that hierarchically group points into local regions
# L takes as input a per-point feature matrix Fp and outputs a local structure matrix Ls with N rows and R columns

# Define the point-wise embedding layer E based on a fully connected network
# E consists of a series of linear layers with batch normalization and ReLU activation
# E takes as input a local structure matrix Ls and outputs a point-wise embedding matrix E with N rows and K columns

# Initialize the network parameters randomly
# Define the local structure loss Ls and the adjacency loss La
# Ls is computed as the mean squared error between the embeddings of points that belong to the same local structure
# La is computed as the mean squared error between the embeddings of points that are adjacent in the point-cloud and their inverse
# Define the total loss L = Ls + La

# Repeat until convergence:
  # Sample a batch of point-clouds from the dataset
  # For each point-cloud in the batch:
    # Normalize the point-cloud to have zero mean and unit variance
    # Apply F to get per-point features Fp
    # Apply L to get local structures Ls
    # Apply E to get point-wise embeddings E
    # Compute Ls and La using the embeddings and the point-cloud
    # Compute L as the sum of Ls and La
  # Update the network parameters using gradient descent with Adam optimizer

# Return E as the final point-wise embedding
```