---
title: 2305.06710v2 Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator
date: 2023-05-07
---

# [Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator](http://arxiv.org/abs/2305.06710v2)

authors: Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang


## What, Why and How

[1]: https://arxiv.org/abs/2305.06710 "[2305.06710] Null-text Guidance in Diffusion Models is Secretly a ..."
[2]: https://arxiv.org/pdf/2305.06710 "arXiv.org"
[3]: http://export.arxiv.org/abs/2303.06710v2 "[2303.06710v2] Decision Making for Human-in-the-loop Robotic Agents via ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes two methods to generate cartoon-style images from real images using diffusion models with null-text guidance.
- **Why**: The paper aims to show that null-text guidance in diffusion models is secretly a cartoon-style creator, and to explore the principle of noise disturbing for null-text.
- **How**: The paper introduces Rollback disturbance (Back-D) and Image disturbance (Image-D) as two ways to construct misalignment between the noisy images used for predicting null-text guidance and text guidance. Back-D achieves cartoonization by altering the noise level of null-text noisy image, while Image-D produces high-fidelity cartoons by defining null-text noisy image as a clean input image. The paper evaluates the proposed methods on various datasets and compares them with existing methods.



## Main Contributions

[1]: https://arxiv.org/abs/2305.06710 "[2305.06710] Null-text Guidance in Diffusion Models is Secretly a ..."
[2]: https://arxiv.org/pdf/2305.06710 "arXiv.org"
[3]: http://export.arxiv.org/abs/2303.06710v2 "[2303.06710v2] Decision Making for Human-in-the-loop Robotic Agents via ..."

The paper[^1^][1] claims the following contributions:

- It reveals that null-text guidance in diffusion models is secretly a cartoon-style creator, and provides insights into the principle of noise disturbing for null-text.
- It proposes two novel disturbance methods, Back-D and Image-D, to achieve cartoonization by constructing misalignment between the noisy images used for predicting null-text guidance and text guidance.
- It demonstrates that the proposed methods can generate high-quality and diverse cartoon images from real images without any additional training or supervision, and outperform existing methods on various datasets.


## Method Summary

[1]: https://arxiv.org/abs/2305.06710 "[2305.06710] Null-text Guidance in Diffusion Models is Secretly a ..."
[2]: https://arxiv.org/pdf/2305.06710 "arXiv.org"
[3]: http://export.arxiv.org/abs/2303.06710v2 "[2303.06710v2] Decision Making for Human-in-the-loop Robotic Agents via ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper first reviews the background of diffusion models and classifier-free guidance, and introduces the notation and formulation of the problem.
- The paper then presents the proposed disturbance methods, Back-D and Image-D, which aim to create misalignment between the null-text noisy image and the text noisy image during sampling. The paper explains the intuition and implementation of each method, and analyzes their effects on cartoonization.
- The paper also discusses the principle of noise disturbing for null-text, and derives a theoretical upper bound for the correlation between the null-text noisy image and the source image. The paper shows that the correlation decreases as the noise level increases, which implies that higher noise levels can lead to better cartoonization effects.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a source image x, a text guidance t, a diffusion model p(x|y), a null-text guidance classifier c(y), a noise schedule beta
# Output: a cartoon-style image x_tilde

# Define the null-text noisy image y_0 and the text noisy image y_T
y_0 = x + sqrt(beta_0) * epsilon_0 # epsilon_0 is Gaussian noise
y_T = sqrt(beta_T) * epsilon_T # epsilon_T is Gaussian noise

# Choose a disturbance method: Back-D or Image-D
if Back-D:
  # Replace beta_0 with beta_T in y_0
  y_0 = x + sqrt(beta_T) * epsilon_0
elif Image-D:
  # Replace y_0 with a clean input image z
  y_0 = z

# Sample x_tilde from the diffusion model with null-text guidance and text guidance
for i in range(T-1, -1, -1):
  # Compute the null-text guidance score s_i
  s_i = c(y_i)
  # Compute the text guidance score r_i
  r_i = p(t|y_i)
  # Compute the sampling distribution q_i
  q_i = p(y_i|x) * exp(s_i + r_i)
  # Sample x_i from q_i
  x_i = sample(q_i)
  # Compute the next noisy image y_i-1
  y_i-1 = (y_i - sqrt(beta_i) * epsilon_i) / sqrt(1 - beta_i)
# Return the final sample x_tilde
x_tilde = x_0
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt

# Load the pretrained diffusion model and the null-text guidance classifier
diffusion_model = torch.hub.load('openai/guided-diffusion', 'cifar10')
null_text_classifier = torch.hub.load('openai/CLIP', 'ViT-B/32', jit=False)

# Define the text guidance
text_guidance = "a cute cat"

# Define the noise schedule
beta = diffusion_model.beta_schedule

# Define the number of sampling steps
T = len(beta)

# Define a function to sample from a Gaussian distribution
def sample_gaussian(mean, std):
  return mean + std * torch.randn_like(mean)

# Define a function to sample from the diffusion model with null-text guidance and text guidance
def sample_cartoon(x, t, p, c, beta, method):
  # Convert the text guidance to a CLIP embedding
  t = c.encode_text(t).detach()
  # Normalize the source image to [0, 1]
  x = x / 255.0
  # Add noise to the source image to get the null-text noisy image y_0
  y_0 = x + torch.sqrt(beta[0]) * sample_gaussian(0, 1)
  # Choose a disturbance method: Back-D or Image-D
  if method == "Back-D":
    # Replace beta_0 with beta_T in y_0
    y_0 = x + torch.sqrt(beta[-1]) * sample_gaussian(0, 1)
  elif method == "Image-D":
    # Replace y_0 with a clean input image z
    y_0 = z / 255.0
  else:
    raise ValueError("Invalid method")
  # Initialize the text noisy image y_T with Gaussian noise
  y_T = torch.sqrt(beta[-1]) * sample_gaussian(0, 1)
  # Initialize the list of samples
  samples = []
  # Loop over the sampling steps from T-1 to 0
  for i in reversed(range(T)):
    # Compute the null-text guidance score s_i by passing y_i through the classifier and computing the cosine similarity with t
    s_i = torch.cosine_similarity(c.encode_image(y_i), t, dim=-1)
    # Compute the text guidance score r_i by passing y_i through the diffusion model and computing the log probability of t
    r_i = p.log_prob(t, y_i)
    # Compute the mean and standard deviation of the sampling distribution q_i using the diffusion model
    mean_i, std_i = p.predictor(y_i)
    # Sample x_i from q_i using reparameterization trick and adding s_i and r_i as logits
    logits_i = (x_i - mean_i) / std_i + s_i + r_i
    x_i = mean_i + std_i * torch.softmax(logits_i, dim=-1)
    # Append x_i to the list of samples
    samples.append(x_i)
    # Compute the next noisy image y_i-1 using the reverse diffusion process
    y_i_1 = (y_i - torch.sqrt(beta[i]) * sample_gaussian(0, 1)) / torch.sqrt(1 - beta[i])
  # Return the final sample x_tilde and the list of samples
  x_tilde = samples[0]
  return x_tilde, samples

# Load a source image from CIFAR-10 dataset
dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True)
x, _ = dataset[0]
x = torch.tensor(x).permute(2, 0, 1).unsqueeze(0)

# Sample a cartoon-style image using Back-D method
x_tilde_backd, samples_backd = sample_cartoon(x, text_guidance, diffusion_model, null_text_classifier, beta, "Back-D")

# Sample a cartoon-style image using Image-D method with a random clean image z from CIFAR-10 dataset
z, _ = dataset[np.random.randint(len(dataset))]
z = torch.tensor(z).permute(2, 0, 1).unsqueeze(0)
x_tilde_imaged, samples_imaged = sample_cartoon(x, text_guidance, diffusion_model, null_text_classifier, beta, "Image-D")

# Plot the source image and the cartoon-style images
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.imshow(x.squeeze().permute(1, 2, 0))
plt.title("Source image")
plt.subplot(1, 3, 2)
plt.imshow(x_tilde_backd.squeeze().permute(1, 2, 0))
plt.title("Cartoon image (Back-D)")
plt.subplot(1, 3, 3)
plt.imshow(x_tilde_imaged.squeeze().permute(1, 2, 0))
plt.title("Cartoon image (Image-D)")
plt.show()
```