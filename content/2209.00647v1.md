---
title: 2209.00647v1 Visual Prompting via Image Inpainting
date: 2022-09-01
---

# [Visual Prompting via Image Inpainting](http://arxiv.org/abs/2209.00647v1)

authors: Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, Alexei A. Efros


## What, Why and How

[1]: https://arxiv.org/abs/2209.00647v1 "[2209.00647v1] Visual Prompting via Image Inpainting - arXiv.org"
[2]: http://export.arxiv.org/abs/2209.00647 "[2209.00647] Visual Prompting via Image Inpainting"
[3]: https://arxiv.org/pdf/2209.00647.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a method called **visual prompting** that can adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification. The method uses image inpainting to fill in a hole in a concatenated visual prompt image that contains input-output image example(s) of a new task and a new input image.
- **Why**: The paper is inspired by prompting in NLP, which has shown that pre-trained language models can perform various tasks by using natural language prompts. The paper aims to achieve similar flexibility and generality for visual models, which are usually trained for specific tasks and require finetuning or architecture changes for new tasks.
- **How**: The paper trains masked auto-encoders on a new dataset of 88k unlabeled figures from academic papers sources on Arxiv. The paper applies visual prompting to these pretrained models and demonstrates results on various downstream image-to-image tasks, such as foreground segmentation, single object detection, colorization, edge detection, etc. The paper shows that visual prompting can achieve competitive or even superior performance compared to task-specific models on some tasks, and can also handle tasks that are not well-defined or have multiple possible outputs.

## Main Contributions

[1]: https://arxiv.org/abs/2209.00647v1 "[2209.00647v1] Visual Prompting via Image Inpainting - arXiv.org"
[2]: http://export.arxiv.org/abs/2209.00647 "[2209.00647] Visual Prompting via Image Inpainting"
[3]: https://arxiv.org/pdf/2209.00647.pdf "arXiv.org e-Print archive"

The paper claims the following contributions[^1^][1]:

- **A new method for visual prompting** that can adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification, by using image inpainting to fill in a hole in a concatenated visual prompt image that contains input-output image example(s) of a new task and a new input image.
- **A new dataset of 88k unlabeled figures from academic papers sources on Arxiv** that is used to train masked auto-encoders for image inpainting. The paper argues that this dataset is more diverse and challenging than existing inpainting datasets, and that it enables visual prompting for a wide range of tasks.
- **An extensive evaluation of visual prompting on various downstream image-to-image tasks**, such as foreground segmentation, single object detection, colorization, edge detection, etc. The paper shows that visual prompting can achieve competitive or even superior performance compared to task-specific models on some tasks, and can also handle tasks that are not well-defined or have multiple possible outputs. The paper also provides qualitative and quantitative analysis of the factors that affect the performance of visual prompting, such as the number and quality of examples, the size and location of the hole, and the choice of inpainting model.

## Method Summary

[1]: https://arxiv.org/abs/2209.00647v1 "[2209.00647v1] Visual Prompting via Image Inpainting - arXiv.org"
[2]: http://export.arxiv.org/abs/2209.00647 "[2209.00647] Visual Prompting via Image Inpainting"
[3]: https://arxiv.org/pdf/2209.00647.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][1]:

- The paper defines the problem of visual prompting as follows: given a set of input-output image example(s) of a new task (x e,y e) and a new input image x q, the goal is to produce the output image y q that is consistent with the given example(s).
- The paper proposes to use image inpainting as a way to solve visual prompting. Image inpainting is the task of filling in a missing region in an image given the surrounding context. The paper constructs a single image called a visual prompt x vp by concatenating the input-output image example(s) and the new input image, and masking out the region corresponding to the desired output image y q. The paper then uses an inpainting model to predict the masked region, which is taken as the output image y q.
- The paper trains masked auto-encoders (MAEs) as inpainting models on a new dataset of 88k unlabeled figures from academic papers sources on Arxiv. The paper argues that this dataset is more diverse and challenging than existing inpainting datasets, and that it enables visual prompting for a wide range of tasks. The paper also compares different MAE architectures and loss functions, and finds that using skip connections and perceptual loss improves inpainting quality.
- The paper applies visual prompting to various downstream image-to-image tasks, such as foreground segmentation, single object detection, colorization, edge detection, etc. The paper shows how to construct visual prompts for different tasks using different numbers and qualities of examples, different sizes and locations of holes, and different inpainting models. The paper also provides qualitative and quantitative results on these tasks, and compares them with task-specific models or baselines.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a function for visual prompting
def visual_prompting(input_output_examples, new_input_image, inpainting_model):
  # Concatenate the input-output image example(s) and the new input image
  visual_prompt = concatenate(input_output_examples, new_input_image)
  # Mask out the region corresponding to the desired output image
  visual_prompt = mask(visual_prompt, output_region)
  # Use the inpainting model to fill in the masked region
  output_image = inpainting_model(visual_prompt)
  # Return the output image
  return output_image

# Train a masked auto-encoder (MAE) as an inpainting model on a new dataset of figures from academic papers
mae = train_mae(arxiv_figures_dataset)

# Apply visual prompting to various downstream image-to-image tasks
for task in tasks:
  # Get the input-output image example(s) and the new input image for the task
  input_output_examples, new_input_image = get_data(task)
  # Use visual prompting to produce the output image
  output_image = visual_prompting(input_output_examples, new_input_image, mae)
  # Evaluate the output image on the task
  evaluate(output_image, task)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define a function for visual prompting
def visual_prompting(input_output_examples, new_input_image, inpainting_model):
  # Concatenate the input-output image example(s) and the new input image along the horizontal axis
  visual_prompt = np.hstack(input_output_examples + [new_input_image])
  # Get the height and width of the visual prompt
  height, width = visual_prompt.shape[:2]
  # Define the coordinates of the output region as a fraction of the width
  output_x1 = 0.8 * width
  output_x2 = width
  output_y1 = 0
  output_y2 = height
  # Mask out the output region with a black rectangle
  visual_prompt[output_y1:output_y2, output_x1:output_x2] = 0
  # Convert the visual prompt to a tensor and normalize it
  visual_prompt = torch.from_numpy(visual_prompt).permute(2, 0, 1).float() / 255.0
  # Add a batch dimension to the visual prompt
  visual_prompt = visual_prompt.unsqueeze(0)
  # Use the inpainting model to fill in the masked region
  output_image = inpainting_model(visual_prompt)
  # Remove the batch dimension from the output image and convert it to a numpy array
  output_image = output_image.squeeze(0).permute(1, 2, 0).numpy()
  # Return the output image
  return output_image

# Define a masked auto-encoder (MAE) class as an inpainting model
class MAE(torch.nn.Module):
  def __init__(self):
    super(MAE, self).__init__()
    # Define the encoder layers
    self.encoder_conv1 = torch.nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)
    self.encoder_conv2 = torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
    self.encoder_conv3 = torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)
    self.encoder_conv4 = torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)
    self.encoder_conv5 = torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)
    self.encoder_conv6 = torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)
    self.encoder_conv7 = torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)
    self.encoder_conv8 = torch.nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)
    # Define the decoder layers with skip connections
    self.decoder_conv1 = torch.nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1)
    self.decoder_conv2 = torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)
    self.decoder_conv3 = torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)
    self.decoder_conv4 = torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)
    self.decoder_conv5 = torch.nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1)
    self.decoder_conv6 = torch.nn.ConvTranspose2d(512 ,128 ,kernel_size=4 ,stride=2 ,padding=1)
    self.decoder_conv7 = torch.nn.ConvTranspose2d(256 ,64 ,kernel_size=4 ,stride=2 ,padding=1)
    self.decoder_conv8 = torch.nn.ConvTranspose2d(128 ,3 ,kernel_size=4 ,stride=2 ,padding=1)
    # Define the activation functions
    self.leaky_relu = torch.nn.LeakyReLU(negative_slope=0.2)
    self.relu = torch.nn.ReLU()
    self.tanh = torch.nn.Tanh()

  def forward(self,x):
    # Encode the input image x into a latent representation z
    z1 = self.leaky_relu(self.encoder_conv1(x))
    z2 = self.leaky_relu(self.encoder_conv2(z1))
    z3 = self.leaky_relu(self.encoder_conv3(z2))
    z4 = self.leaky_relu(self.encoder_conv4(z3))
    z5 = self.leaky_relu(self.encoder_conv5(z4))
    z6 = self.leaky_relu(self.encoder_conv6(z5))
    z7 = self.leaky_relu(self.encoder_conv7(z6))
    z8 = self.leaky_relu(self.encoder_conv8(z7))
    # Decode the latent representation z into an output image y
    y1 = self.relu(self.decoder_conv1(z8))
    y2 = self.relu(self.decoder_conv2(torch.cat([y1, z7], dim=1)))
    y3 = self.relu(self.decoder_conv3(torch.cat([y2, z6], dim=1)))
    y4 = self.relu(self.decoder_conv4(torch.cat([y3, z5], dim=1)))
    y5 = self.relu(self.decoder_conv5(torch.cat([y4, z4], dim=1)))
    y6 = self.relu(self.decoder_conv6(torch.cat([y5, z3], dim=1)))
    y7 = self.relu(self.decoder_conv7(torch.cat([y6, z2], dim=1)))
    y8 = self.tanh(self.decoder_conv8(torch.cat([y7, z1], dim=1)))
    # Return the output image y
    return y8

# Train a masked auto-encoder (MAE) as an inpainting model on a new dataset of figures from academic papers
# Load the dataset of figures from academic papers
dataset = torchvision.datasets.ImageFolder(root='arxiv_figures_dataset', transform=torchvision.transforms.ToTensor())
# Create a data loader for the dataset
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
# Create an instance of the MAE class
mae = MAE()
# Move the MAE to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
mae.to(device)
# Define the optimizer and the loss function
optimizer = torch.optim.Adam(mae.parameters(), lr=0.0002, betas=(0.5, 0.999))
loss_function = torch.nn.MSELoss()
# Define the number of epochs to train
epochs = 100
# Loop over the epochs
for epoch in range(epochs):
  # Loop over the batches
  for batch in dataloader:
    # Get the input images from the batch
    input_images = batch[0].to(device)
    # Randomly mask out a region in the input images
    masked_images, masks = random_mask(input_images)
    # Use the MAE to inpaint the masked images
    output_images = mae(masked_images)
    # Compute the loss between the output images and the original images
    loss = loss_function(output_images * masks, input_images * masks)
    # Backpropagate the loss and update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # Print the epoch and the loss
  print(f'Epoch {epoch}, Loss {loss.item()}')

# Define a function for randomly masking out a region in an image
def random_mask(image):
  # Get the batch size, channels, height and width of the image
  b, c, h, w = image.shape
  # Define the minimum and maximum size of the hole as a fraction of the width or height
  min_size = 0.1
  max_size = 0.4
  # Sample a random size for the hole
  hole_h = np.random.randint(min_size * h, max_size * h)
  hole_w = np.random.randint(min_size * w, max_size * w)
  # Sample a random location for the hole
  hole_y1 = np.random.randint(0, h - hole_h)
  hole_x1 = np.random.randint(0, w - hole_w)
  hole_y2 = hole_y1 + hole_h
  hole_x2 = hole_x1 + hole_w
  # Create a mask with a hole
  mask = torch.ones(b, c, h, w).to(image.device)
  mask[:, :, hole_y1:hole_y2, hole_x1:hole_x2] = 0
  # Apply the mask to the image
  masked_image = image * mask
  # Return the masked image and the mask
  return masked_image, mask

# Apply visual prompting to various downstream image-to-image tasks
for task in tasks:
  # Get the input-output image example(s) and the new input image for the task from some source (e.g., a folder or a URL)
  input_output_examples, new_input_image = get_data(task)
  # Convert the input-output image example