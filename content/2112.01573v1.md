---
title: 2112.01573v1 FuseDream  Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization
date: 2021-12-02
---

# [FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization](http://arxiv.org/abs/2112.01573v1)

authors: Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, Qiang Liu


## What, Why and How

[1]: https://arxiv.org/abs/2112.01573 "[2112.01573] FuseDream: Training-Free Text-to-Image Generation with ..."
[2]: https://arxiv.org/pdf/2112.01573v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2302.01573v1 "[2302.01573v1] $p$-Hyperbolic Zolotarev Functions in Boundary Value ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a training-free text-to-image generation method called FuseDream, which combines a retrained CLIP representation with an off-the-shelf image generator (GANs), and improves the optimization process with three key techniques: AugCLIP score, novel initialization and over-parameterization, and composed generation.
- **Why**: The paper aims to overcome the limitations of traditional text-to-image generation methods that require training generative models from scratch, which are costly, data-hungry, and hard to customize. The paper also addresses the challenges of optimizing CLIP score in the GAN space, which is highly non-convex and biased.
- **How**: The paper introduces three techniques to enhance the CLIP+GAN approach: 1) AugCLIP score, which adds random augmentations to the image to make the CLIP objective more robust and diverse. 2) Novel initialization and over-parameterization, which uses a large number of latent codes and a learned linear mapping to initialize and optimize the image generation. 3) Composed generation, which leverages a bi-level optimization formulation to compose multiple images from different latent codes to extend the GAN space and overcome the data-bias. The paper evaluates FuseDream on MS COCO dataset and shows that it can generate high-quality images with varying objects, backgrounds, artistic styles, and novel concepts.

## Main Contributions

The paper claims the following contributions:

- It proposes a training-free text-to-image generation method that can leverage any off-the-shelf image generator and retrained CLIP representation.
- It introduces three techniques to improve the optimization process of CLIP+GAN: AugCLIP score, novel initialization and over-parameterization, and composed generation.
- It demonstrates that FuseDream can generate high-quality images with varying objects, backgrounds, artistic styles, and novel concepts that do not appear in the training data of the GAN. It also achieves top-level Inception score and FID score on MS COCO dataset.

## Method Summary

[1]: https://arxiv.org/abs/2112.01573 "[2112.01573] FuseDream: Training-Free Text-to-Image Generation with ..."
[2]: https://arxiv.org/pdf/2112.01573v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2305.01573v1 "[2305.01573v1] NELoRa-Bench: A Benchmark for Neural-enhanced LoRa ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first introduces the CLIP+GAN approach, which uses a retrained CLIP model to measure the similarity between text and image, and an off-the-shelf GAN model to generate images from latent codes. The paper formulates the text-to-image generation as an optimization problem that maximizes the CLIP score in the GAN space.
- The paper then proposes three techniques to improve the CLIP+GAN optimization: 1) AugCLIP score, which applies random augmentations to the generated image and computes the average CLIP score over multiple augmented images. This technique aims to make the objective more robust and diverse. 2) Novel initialization and over-parameterization, which uses a large number of latent codes (e.g., 256) and a learned linear mapping to initialize and optimize the image generation. This technique aims to increase the expressiveness and flexibility of the GAN space. 3) Composed generation, which leverages a bi-level optimization formulation to compose multiple images from different latent codes. This technique aims to extend the GAN space and overcome the data-bias of the GAN model.
- The paper also describes some implementation details, such as the choice of GAN models, CLIP models, optimizers, hyperparameters, and evaluation metrics. The paper uses StyleGAN2-ADA for image generation, CLIP-RN50x16 for text-image similarity, AdamW for optimization, and Inception score and FID score for evaluation.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: text description
# Output: generated image

# Load retrained CLIP model and off-the-shelf GAN model
clip_model = load_clip_model()
gan_model = load_gan_model()

# Initialize latent codes and linear mapping
latent_codes = random_init(num_codes)
linear_mapping = random_init(num_codes, image_size)

# Define AugCLIP score function
def augclip_score(text, image):
  # Apply random augmentations to image
  augmented_images = augment(image, num_augments)
  # Compute CLIP score for each augmented image
  clip_scores = clip_model(text, augmented_images)
  # Return the average CLIP score
  return mean(clip_scores)

# Define composed generation function
def compose_images(latent_codes):
  # Split latent codes into two groups
  latent_codes_1, latent_codes_2 = split(latent_codes)
  # Generate two images from each group
  image_1 = gan_model(latent_codes_1)
  image_2 = gan_model(latent_codes_2)
  # Use bi-level optimization to find the best composition mask
  mask = optimize_mask(text, image_1, image_2, clip_model)
  # Compose the two images with the mask
  composed_image = mask * image_1 + (1 - mask) * image_2
  # Return the composed image
  return composed_image

# Optimize latent codes and linear mapping to maximize AugCLIP score
for iteration in range(max_iterations):
  # Generate image from latent codes and linear mapping
  image = linear_mapping * gan_model(latent_codes)
  # Compute AugCLIP score for text and image
  score = augclip_score(text, image)
  # Update latent codes and linear mapping with gradient ascent
  latent_codes, linear_mapping = update_with_gradient_ascent(score, latent_codes, linear_mapping)

# Generate final image with composed generation
final_image = compose_images(latent_codes)

# Return final image
return final_image
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: text description
# Output: generated image

# Import libraries
import torch
import torchvision
import clip
import stylegan2_ada_pytorch as stylegan

# Load retrained CLIP model and off-the-shelf GAN model
clip_model = clip.load("RN50x16", device="cuda")
gan_model = stylegan.load_pretrained("ffhq", device="cuda")

# Initialize latent codes and linear mapping
num_codes = 256 # number of latent codes
image_size = 256 # size of generated image
latent_dim = 512 # dimension of latent space
latent_codes = torch.randn(num_codes, latent_dim, device="cuda") # random initialization
linear_mapping = torch.randn(num_codes, image_size * image_size * 3, device="cuda") # random initialization

# Define AugCLIP score function
def augclip_score(text, image):
  # Apply random augmentations to image
  num_augments = 8 # number of augmentations
  transforms = torchvision.transforms.Compose([
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2)),
    torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),
    torchvision.transforms.GaussianBlur(3),
    torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))
  ])
  augmented_images = torch.stack([transforms(image) for _ in range(num_augments)]) # apply transforms to image and stack them
  # Compute CLIP score for each augmented image
  text_features = clip_model.encode_text(text) # encode text with CLIP model
  image_features = clip_model.encode_image(augmented_images) # encode augmented images with CLIP model
  clip_scores = torch.matmul(text_features, image_features.T) # compute dot product between text and image features
  # Return the average CLIP score
  return clip_scores.mean()

# Define composed generation function
def compose_images(latent_codes):
  # Split latent codes into two groups
  latent_codes_1, latent_codes_2 = torch.chunk(latent_codes, chunks=2, dim=0) # split along the first dimension
  # Generate two images from each group
  image_1 = gan_model.synthesis(latent_codes_1) # synthesize image from latent codes with GAN model
  image_2 = gan_model.synthesis(latent_codes_2) # synthesize image from latent codes with GAN model
  # Use bi-level optimization to find the best composition mask
  mask = torch.ones(1, 1, image_size, image_size, device="cuda") # initialize mask with ones
  mask_optimizer = torch.optim.Adam([mask], lr=0.01) # use Adam optimizer for mask optimization
  mask_iterations = 100 # number of iterations for mask optimization
  for _ in range(mask_iterations):
    # Compose the two images with the mask
    composed_image = mask * image_1 + (1 - mask) * image_2 # element-wise multiplication and addition
    # Compute AugCLIP score for text and composed image
    score = augclip_score(text, composed_image)
    # Update mask with gradient ascent
    mask_optimizer.zero_grad() # clear previous gradients
    score.backward() # compute gradients with respect to score
    mask_optimizer.step() # update mask parameters with gradients

  # Return the composed image
  return composed_image

# Optimize latent codes and linear mapping to maximize AugCLIP score
latent_optimizer = torch.optim.AdamW([latent_codes], lr=0.01) # use AdamW optimizer for latent codes optimization
linear_optimizer = torch.optim.AdamW([linear_mapping], lr=0.01) # use AdamW optimizer for linear mapping optimization
max_iterations = 1000 # number of iterations for optimization
for iteration in range(max_iterations):
  # Generate image from latent codes and linear mapping
  gan_image = gan_model.synthesis(latent_codes) # synthesize image from latent codes with GAN model
  gan_image = gan_image.reshape(num_codes, -1) # flatten the image along the channel and spatial dimensions 
  image = linear_mapping @ gan_image.T # matrix multiplication between linear mapping and gan image 
  image = image.reshape(1, num_codes, 3, image_size, image_size) # reshape the image to the original shape
  image = (image - image.min()) / (image.max() - image.min()) # normalize the image to [0, 1] range
  # Compute AugCLIP score for text and image
  score = augclip_score(text, image)
  # Update latent codes and linear mapping with gradient ascent
  latent_optimizer.zero_grad() # clear previous gradients
  linear_optimizer.zero_grad() # clear previous gradients
  score.backward() # compute gradients with respect to score
  latent_optimizer.step() # update latent codes with gradients
  linear_optimizer.step() # update linear mapping with gradients

# Generate final image with composed generation
final_image = compose_images(latent_codes)

# Return final image
return final_image
```