---
title: 2207.01696v2 TM2T  Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts
date: 2022-07-02
---

# [TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts](http://arxiv.org/abs/2207.01696v2)

authors: Chuan Guo, Xinxin Zuo, Sen Wang, Li Cheng


## What, Why and How

[1]: https://arxiv.org/abs/2207.01696v2 "[2207.01696v2] TM2T: Stochastic and Tokenized Modeling for the ..."
[2]: https://arxiv.org/abs/2207.01696 "[2207.01696] TM2T: Stochastic and Tokenized Modeling for ... - arXiv.org"
[3]: https://arxiv.org/pdf/2205.01696v2.pdf "arXiv:2205.01696v2 [astro-ph.CO] 22 Sep 2022"

Here is a summary of the paper:

- **What**: The paper proposes a novel method for generating 3D human motions from texts and vice versa, using motion tokens and neural machine translation models.
- **Why**: The paper aims to explore the strong ties between vision and language, the two intimate human sensing and communication modalities, and to enable the generation of multiple distinct motions from the same text, as well as to avoid the production of trivial motionless pose sequences.
- **How**: The paper introduces motion token, a discrete and compact motion representation that provides a level playing ground for both motions and text signals. The paper also adapts the neural model for machine translation (NMT) to model the distribution over discrete motion tokens and to facilitate the mappings between the two modalities. The paper integrates the motion2text module into the inverse alignment process of the text2motion training pipeline, where a large training loss is imposed if the synthesized text deviates significantly from the input text. The paper evaluates the proposed method on two benchmark datasets and shows superior performance over state-of-the-art methods on both tasks[^1^][1].

## Main Contributions

The contributions of this paper are:

- It proposes a novel method for the reciprocal generation of 3D human motions and texts, shorthanded for text2motion and motion2text, respectively.
- It introduces motion token, a discrete and compact motion representation that enables non-deterministic production of pose sequences of variable lengths from an input text.
- It adapts the neural model for machine translation (NMT) to model the distribution over discrete motion tokens and to facilitate the mappings between the two modalities.
- It integrates the motion2text module into the inverse alignment process of the text2motion training pipeline, where a large training loss is imposed if the synthesized text deviates significantly from the input text.
- It evaluates the proposed method on two benchmark datasets and shows superior performance over state-of-the-art methods on both tasks.

## Method Summary

[1]: https://arxiv.org/abs/2207.01696v2 "[2207.01696v2] TM2T: Stochastic and Tokenized Modeling for the ..."
[2]: https://arxiv.org/pdf/2210.01696v2 "arXiv:2210.01696v2 [eess.IV] 7 Oct 2022"
[3]: http://export.arxiv.org/abs/2210.01696v2 "[2210.01696v2] Simultaneous self-supervised reconstruction and ..."

Here is a summary of the method section of the paper:

- The paper proposes a novel method for the reciprocal generation of 3D human motions and texts, shorthanded for text2motion and motion2text, respectively.
- The paper introduces motion token, a discrete and compact motion representation that enables non-deterministic production of pose sequences of variable lengths from an input text.
- The paper adapts the neural model for machine translation (NMT) to model the distribution over discrete motion tokens and to facilitate the mappings between the two modalities.
- The paper integrates the motion2text module into the inverse alignment process of the text2motion training pipeline, where a large training loss is imposed if the synthesized text deviates significantly from the input text.
- The paper describes the network architecture, the loss function, the training procedure, and the evaluation metrics for both text2motion and motion2text tasks.
- The paper also discusses some implementation details, such as motion tokenization, data augmentation, and inference strategies.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the network architecture
encoder = TransformerEncoder()
decoder = TransformerDecoder()

# Define the loss function
loss = L1Loss() + CrossEntropyLoss()

# Define the training procedure
for epoch in range(num_epochs):
  # Shuffle the training data
  shuffle(data)
  # Loop over the training batches
  for batch in data:
    # Get the input and output tokens
    input_tokens = batch["input_tokens"]
    output_tokens = batch["output_tokens"]
    # Add noise to the input tokens
    noisy_input_tokens = add_noise(input_tokens)
    # Forward pass
    encoder_output = encoder(noisy_input_tokens)
    decoder_output = decoder(encoder_output)
    # Compute the loss
    batch_loss = loss(decoder_output, output_tokens)
    # Backward pass and update parameters
    optimizer.zero_grad()
    batch_loss.backward()
    optimizer.step()
  # Evaluate on the validation data
  evaluate(encoder, decoder, val_data)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import random

# Define some hyperparameters
num_epochs = 100
batch_size = 32
learning_rate = 0.001
num_heads = 8
num_layers = 6
hidden_size = 512
dropout = 0.1
max_length = 128
vocab_size = 1000

# Define the network architecture
class TransformerEncoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Initialize the embedding layer
    self.embedding = torch.nn.Embedding(vocab_size, hidden_size)
    # Initialize the positional encoding layer
    self.positional_encoding = PositionalEncoding(hidden_size, max_length)
    # Initialize the encoder layers
    self.encoder_layers = torch.nn.ModuleList([EncoderLayer(hidden_size, num_heads, dropout) for _ in range(num_layers)])
  
  def forward(self, input_tokens):
    # Embed the input tokens
    embedded_tokens = self.embedding(input_tokens)
    # Add the positional encoding
    encoded_tokens = self.positional_encoding(embedded_tokens)
    # Pass through the encoder layers
    for encoder_layer in self.encoder_layers:
      encoded_tokens = encoder_layer(encoded_tokens)
    # Return the encoder output
    return encoded_tokens

class TransformerDecoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Initialize the embedding layer
    self.embedding = torch.nn.Embedding(vocab_size, hidden_size)
    # Initialize the positional encoding layer
    self.positional_encoding = PositionalEncoding(hidden_size, max_length)
    # Initialize the decoder layers
    self.decoder_layers = torch.nn.ModuleList([DecoderLayer(hidden_size, num_heads, dropout) for _ in range(num_layers)])
    # Initialize the output layer
    self.output_layer = torch.nn.Linear(hidden_size, vocab_size)
  
  def forward(self, encoder_output):
    # Generate the output tokens using teacher forcing
    output_tokens = torch.zeros(batch_size, max_length).long()
    output_tokens[:, 0] = torch.ones(batch_size).long() # Start token
    for i in range(1, max_length):
      # Embed the output tokens so far
      embedded_tokens = self.embedding(output_tokens[:, :i])
      # Add the positional encoding
      decoded_tokens = self.positional_encoding(embedded_tokens)
      # Pass through the decoder layers with encoder output as memory
      for decoder_layer in self.decoder_layers:
        decoded_tokens = decoder_layer(decoded_tokens, encoder_output)
      # Predict the next token using the output layer
      logits = self.output_layer(decoded_tokens[:, -1, :])
      predictions = torch.argmax(logits, dim=-1)
      # Update the output tokens with the predictions
      output_tokens[:, i] = predictions
    # Return the output tokens
    return output_tokens

class PositionalEncoding(torch.nn.Module):
  def __init__(self, hidden_size, max_length):
    super().__init__()
    # Compute the positional encodings once and store them in a buffer
    positional_encodings = torch.zeros(max_length, hidden_size)
    positions = torch.arange(0, max_length).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, hidden_size, 2) * (-np.log(10000.0) / hidden_size))
    positional_encodings[:, 0::2] = torch.sin(positions * div_term)
    positional_encodings[:, 1::2] = torch.cos(positions * div_term)
    positional_encodings = positional_encodings.unsqueeze(0).transpose(0, 1)
    self.register_buffer('positional_encodings', positional_encodings)

  def forward(self, x):
    # Add the positional encodings to the input embeddings
    x = x + self.positional_encodings[:x.size(0), :]
    return x

class EncoderLayer(torch.nn.Module):
  def __init__(self, hidden_size, num_heads, dropout):
    super().__init__()
    # Initialize the multi-head attention layer
    self.attention_layer = MultiHeadAttention(hidden_size, num_heads)
    # Initialize the feed-forward layer
    self.feed_forward_layer = FeedForwardLayer(hidden_size)
    # Initialize the layer normalization layers
    self.layer_norm_1 = torch.nn.LayerNorm(hidden_size)
    self.layer_norm_2 = torch.nn.LayerNorm(hidden_size)
  
  def forward(self, x):