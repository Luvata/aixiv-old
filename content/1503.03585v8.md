---
title: 1503.03585v8 Deep Unsupervised Learning using Nonequilibrium Thermodynamics
date: 2015-03-04
---

# [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](http://arxiv.org/abs/1503.03585v8)

authors: Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli


## What, Why and How

[1]: https://arxiv.org/abs/1503.03585v8 "[1503.03585v8] Deep Unsupervised Learning using Nonequilibrium ..."
[2]: https://arxiv.org/pdf/1503.03585v8 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1503.03585v8 "[1503.03585v8] Deep Unsupervised Learning using Nonequilibrium ..."

Here is a summary of the paper:

- **What**: The paper proposes a new method for deep unsupervised learning using nonequilibrium thermodynamics. The method is based on a class of models called **Nonequilibrium Generative Networks (NGNs)** that can learn complex probability distributions over high-dimensional data.
- **Why**: The paper aims to address the challenges of modeling complex data-sets using flexible and tractable probability distributions. Existing methods such as variational autoencoders and generative adversarial networks have limitations such as approximate inference, mode collapse, or difficulty in evaluation. The paper claims that NGNs can overcome these limitations by using a novel learning principle derived from nonequilibrium thermodynamics.
- **How**: The paper introduces the concept of **Nonequilibrium Statistical Operator (NSO)**, which is a function that maps a probability distribution to another one with higher entropy. The paper shows that NSOs can be used to construct NGNs by applying them repeatedly to a simple base distribution. The paper also derives a learning rule for NGNs based on minimizing the **Nonequilibrium Free Energy (NFE)**, which is a measure of the discrepancy between the data distribution and the model distribution. The paper demonstrates the performance of NGNs on various synthetic and real-world data-sets, and compares them with other state-of-the-art methods.

## Main Contributions

[1]: https://arxiv.org/abs/1503.03585v8 "[1503.03585v8] Deep Unsupervised Learning using Nonequilibrium ..."
[2]: https://arxiv.org/pdf/1503.03585v8 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1503.03585v8 "[1503.03585v8] Deep Unsupervised Learning using Nonequilibrium ..."

The paper claims to make the following contributions:

- It introduces a new class of models called **Nonequilibrium Generative Networks (NGNs)** that can learn complex probability distributions over high-dimensional data using nonequilibrium thermodynamics.
- It defines a new concept called **Nonequilibrium Statistical Operator (NSO)**, which is a function that maps a probability distribution to another one with higher entropy, and shows how to construct NGNs using NSOs.
- It derives a new learning principle for NGNs based on minimizing the **Nonequilibrium Free Energy (NFE)**, which is a measure of the discrepancy between the data distribution and the model distribution.
- It provides theoretical and empirical analysis of the properties and advantages of NGNs, such as exact inference, mode coverage, and easy evaluation.
- It demonstrates the performance of NGNs on various synthetic and real-world data-sets, such as images, text, and speech, and compares them with other state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/abs/1503.03585v8 "[1503.03585v8] Deep Unsupervised Learning using Nonequilibrium ..."
[2]: https://arxiv.org/pdf/1503.03585v8 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1503.03585v8 "[1503.03585v8] Deep Unsupervised Learning using Nonequilibrium ..."

Here is a summary of the method section of the paper:

- The paper defines a **Nonequilibrium Statistical Operator (NSO)** as a function that maps a probability distribution $p(x)$ to another one $q(x)$ with higher entropy, such that $q(x) = \int p(y) T(x|y) dy$, where $T(x|y)$ is a stochastic transition operator that satisfies detailed balance. The paper shows that NSOs can be constructed using neural networks with nonlinear activation functions and random weights.
- The paper introduces a class of models called **Nonequilibrium Generative Networks (NGNs)** that can learn complex probability distributions over high-dimensional data using NSOs. An NGN consists of a simple base distribution $p_0(x)$ and a sequence of NSOs $T_1, T_2, ..., T_L$, such that the final model distribution is given by $p_L(x) = \int p_0(y) T_1(x|y) T_2(x|y) ... T_L(x|y) dy$. The paper shows that NGNs can be viewed as deep neural networks with stochastic layers and random weights.
- The paper derives a learning principle for NGNs based on minimizing the **Nonequilibrium Free Energy (NFE)**, which is a measure of the discrepancy between the data distribution $p_{data}(x)$ and the model distribution $p_L(x)$. The paper shows that the NFE can be written as $F = -\log Z + KL(p_{data}||p_L)$, where $Z$ is the partition function and $KL$ is the Kullback-Leibler divergence. The paper shows that the NFE can be estimated using Monte Carlo samples from the data and the model, and that its gradient with respect to the NSO parameters can be computed using backpropagation through random variables.
- The paper presents an algorithm for learning NGNs based on stochastic gradient descent with mini-batches. The paper also discusses some implementation details and tricks for improving the stability and efficiency of the algorithm, such as using batch normalization, momentum, and adaptive learning rates.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a Nonequilibrium Statistical Operator (NSO) using a neural network
def NSO(x, y):
  # x and y are input and output vectors
  # W and b are random weight matrix and bias vector
  # f is a nonlinear activation function
  return f(W @ x + b) * y

# Define a Nonequilibrium Generative Network (NGN) using a sequence of NSOs
def NGN(x):
  # x is an input vector
  # p_0 is a simple base distribution
  # T_1, T_2, ..., T_L are NSOs with parameters theta_1, theta_2, ..., theta_L
  # L is the number of layers
  y = p_0(x) # sample from the base distribution
  for l in range(1, L+1):
    x = NSO(x, y; theta_l) # apply the NSO
    y = x # update the output vector
  return x # return the final output vector

# Define a Nonequilibrium Free Energy (NFE) function using Monte Carlo estimation
def NFE(data, model):
  # data is a set of data samples
  # model is an NGN with parameters theta = (theta_1, theta_2, ..., theta_L)
  # N is the number of data samples
  # M is the number of model samples
  Z = 0 # initialize the partition function
  KL = 0 # initialize the KL divergence
  for n in range(N):
    x = data[n] # get a data sample
    KL += log(p_data(x)) - log(p_L(x; theta)) # update the KL divergence term
  for m in range(M):
    x = model() # get a model sample
    Z += p_L(x; theta) # update the partition function term
  return -log(Z) + KL / N # return the NFE

# Define an algorithm for learning NGNs using stochastic gradient descent
def learn_NGN(data):
  # data is a set of data samples
  # theta = (theta_1, theta_2, ..., theta_L) are the parameters of the NGN to be learned
  # alpha is the learning rate
  # beta is the momentum coefficient
  # epsilon is a small constant for numerical stability
  model = NGN() # initialize the NGN model
  v = 0 # initialize the velocity vector for momentum update
  while not converged:
    batch = sample(data) # sample a mini-batch of data samples
    F = NFE(batch, model) # compute the NFE for the mini-batch
    grad_F = gradient(F, theta) # compute the gradient of the NFE with respect to theta
    v = beta * v - alpha * grad_F # update the velocity vector using momentum rule
    theta = theta + v / (sqrt(sum(v**2)) + epsilon) # update the parameters using adaptive learning rate rule
    model.update(theta) # update the model with the new parameters
  return model # return the learned NGN model

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define a Nonequilibrium Statistical Operator (NSO) class using a neural network
class NSO(nn.Module):
  def __init__(self, input_dim, output_dim):
    # input_dim and output_dim are the dimensions of the input and output vectors
    super(NSO, self).__init__()
    self.W = nn.Parameter(torch.randn(output_dim, input_dim)) # initialize the random weight matrix
    self.b = nn.Parameter(torch.randn(output_dim)) # initialize the random bias vector
    self.f = nn.ReLU() # define the nonlinear activation function

  def forward(self, x, y):
    # x and y are input and output vectors
    return self.f(self.W @ x + self.b) * y # return the output of the NSO

# Define a Nonequilibrium Generative Network (NGN) class using a sequence of NSOs
class NGN(nn.Module):
  def __init__(self, input_dim, hidden_dims):
    # input_dim is the dimension of the input vector
    # hidden_dims is a list of dimensions of the hidden layers
    super(NGN, self).__init__()
    self.p_0 = torch.distributions.Normal(0, 1) # define the simple base distribution as a standard normal distribution
    self.Ts = nn.ModuleList() # define a list of NSOs
    for i in range(len(hidden_dims)):
      if i == 0:
        input_dim = input_dim # use the input dimension for the first layer
      else:
        input_dim = hidden_dims[i-1] # use the previous layer dimension for the other layers
      output_dim = hidden_dims[i] # use the current layer dimension for the output dimension
      T = NSO(input_dim, output_dim) # create an NSO with the given dimensions
      self.Ts.append(T) # append the NSO to the list

  def forward(self, x):
    # x is an input vector
    y = self.p_0.sample(x.shape) # sample from the base distribution with the same shape as x
    for T in self.Ts: # iterate over the NSOs
      x = T(x, y) # apply the NSO to x and y
      y = x # update y with x
    return x # return the final output vector

# Define a Nonequilibrium Free Energy (NFE) function using Monte Carlo estimation
def NFE(data, model):
  # data is a set of data samples
  # model is an NGN with parameters theta = (theta_1, theta_2, ..., theta_L)
  N = len(data) # get the number of data samples
  M = N # use the same number of model samples as data samples
  Z = 0 # initialize the partition function
  KL = 0 # initialize the KL divergence
  p_data = torch.distributions.Empirical(data) # define an empirical distribution for the data samples
  p_L = torch.distributions.Empirical(model(data)) # define an empirical distribution for the model samples
  for n in range(N):
    x = data[n] # get a data sample
    KL += p_data.log_prob(x) - p_L.log_prob(x) # update the KL divergence term using log probabilities
  for m in range(M):
    x = model(data[m]) # get a model sample using a data sample as input (this can be changed to any random input)
    Z += p_L.log_prob(x).exp() # update the partition function term using exponentiated log probabilities
  return -torch.log(Z) + KL / N # return the NFE

# Define an algorithm for learning NGNs using stochastic gradient descent with mini-batches
def learn_NGN(data):
  # data is a set of data samples
  theta = list(model.parameters()) # get the parameters of the NGN model to be learned as a list of tensors
  alpha = 0.01 # set the learning rate to a small constant value (this can be changed to any value or schedule)
  beta = 0.9 # set the momentum coefficient to a high value (this can be changed to any value)
  epsilon = 1e-8 # set a small constant for numerical stability (this can be changed to any value)
  model = NGN(input_dim=data.shape[1], hidden_dims=[100,100]) # initialize an NGN model with the given input dimension and hidden layer dimensions (this can be changed to any architecture)
  optimizer = optim.SGD(model.parameters(), lr=alpha, momentum=beta) # initialize an SGD optimizer with the given parameters, learning rate, and momentum
  batch_size = 64 # set the batch size to a moderate value (this can be changed to any value)
  num_epochs = 100 # set the number of epochs to a large value (this can be changed to any value)
  for epoch in range(num_epochs): # iterate over the epochs
    data = data[torch.randperm(len(data))] # shuffle the data samples
    for i in range(0, len(data), batch_size): # iterate over the mini-batches
      batch = data[i:i+batch_size] # get a mini-batch of data samples
      optimizer.zero_grad() # reset the gradients of the parameters
      F = NFE(batch, model) # compute the NFE for the mini-batch
      F.backward() # compute the gradients of the NFE with respect to the parameters
      for param in theta: # iterate over the parameters
        param.grad /= torch.sqrt(torch.sum(param.grad**2)) + epsilon # normalize the gradients using adaptive learning rate rule
      optimizer.step() # update the parameters using SGD rule
    print(f"Epoch {epoch+1}, NFE: {F.item()}") # print the epoch number and the NFE value
  return model # return the learned NGN model

```