---
title: 2004.02222v3 Structural-analogy from a Single Image Pair
date: 2020-04-03
---

# [Structural-analogy from a Single Image Pair](http://arxiv.org/abs/2004.02222v3)

authors: Sagie Benaim, Ron Mokady, Amit Bermano, Daniel Cohen-Or, Lior Wolf


## What, Why and How

[1]: https://arxiv.org/pdf/2004.02222v3 "Facebook AI Research arXiv:2004.02222v3 [cs.CV] 6 Jan 2021"
[2]: https://arxiv.org/abs/2004.02222v1 "[2004.02222v1] Structural-analogy from a Single Image Pair - arXiv.org"
[3]: https://www.researchgate.net/publication/340474696_Structural-analogy_from_a_Single_Image_Pair "(PDF) Structural-analogy from a Single Image Pair - ResearchGate"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method for structural analogy from a single image pair, which means generating an image that keeps the appearance and style of one image, but has a structural arrangement that corresponds to another image.
- **Why**: The paper aims to explore the capabilities of neural networks to understand image structure given only a single pair of images, which is a challenging and novel task in unsupervised image-to-image translation.
- **How**: The paper uses a patch-based approach that maps between image patches at different scales, which enables controlling the granularity at which analogies are produced. The paper also introduces a novel loss function that encourages structural alignment and style preservation. The paper demonstrates the effectiveness of the method on various conditional generation tasks such as guided image synthesis, style and texture transfer, text translation and video translation.

## Main Contributions

[1]: https://arxiv.org/pdf/2004.02222v3 "Facebook AI Research arXiv:2004.02222v3 [cs.CV] 6 Jan 2021"
[2]: https://arxiv.org/abs/2004.02222v1 "[2004.02222v1] Structural-analogy from a Single Image Pair - arXiv.org"
[3]: https://www.researchgate.net/publication/340474696_Structural-analogy_from_a_Single_Image_Pair "(PDF) Structural-analogy from a Single Image Pair - ResearchGate"

According to the paper[^1^][1], the main contributions are:

- **A novel method for structural analogy from a single image pair**, which can generate images that are structurally aligned to one image and stylistically aligned to another image, without requiring large collections of images or paired examples.
- **A patch-based approach that maps between image patches at different scales**, which enables controlling the granularity at which analogies are produced and determines the conceptual distinction between style and content.
- **A novel loss function that encourages structural alignment and style preservation**, which combines a patch-based cycle-consistency loss, a patch-based identity loss, and a patch-based style loss.
- **Extensive experiments on various conditional generation tasks**, such as guided image synthesis, style and texture transfer, text translation and video translation, demonstrating the effectiveness and versatility of the proposed method.

## Method Summary

[1]: https://arxiv.org/pdf/2004.02222v3 "Facebook AI Research arXiv:2004.02222v3 [cs.CV] 6 Jan 2021"
[2]: https://arxiv.org/abs/2004.02222v1 "[2004.02222v1] Structural-analogy from a Single Image Pair - arXiv.org"
[3]: https://www.researchgate.net/publication/340474696_Structural-analogy_from_a_Single_Image_Pair "(PDF) Structural-analogy from a Single Image Pair - ResearchGate"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a **patch-based approach** that maps between image patches at different scales, using a **multi-scale patch discriminator** and a **multi-scale patch generator**. The discriminator learns to distinguish between real and fake patches at each scale, while the generator learns to produce realistic patches at each scale that are consistent with the input images.
- The paper also introduces a **novel loss function** that encourages structural alignment and style preservation, which consists of three components: a **patch-based cycle-consistency loss**, a **patch-based identity loss**, and a **patch-based style loss**. The cycle-consistency loss ensures that the generated images can be mapped back to the original images, the identity loss ensures that the generated images do not change the content of the input images, and the style loss ensures that the generated images match the style of the input images.
- The paper applies the proposed method to various conditional generation tasks, such as guided image synthesis, style and texture transfer, text translation and video translation. The paper shows that the method can generate high quality images that are structurally aligned to one image and stylistically aligned to another image, given only a single pair of images as input. The paper also compares the method with existing methods and shows that it outperforms them in terms of visual quality and diversity.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: two images A and B
# Output: an image C that is structurally aligned to A and stylistically aligned to B

# Define the multi-scale patch generator G and the multi-scale patch discriminator D
G = MultiScalePatchGenerator()
D = MultiScalePatchDiscriminator()

# Define the patch-based cycle-consistency loss L_cycle, the patch-based identity loss L_idt, and the patch-based style loss L_style
L_cycle = PatchBasedCycleConsistencyLoss()
L_idt = PatchBasedIdentityLoss()
L_style = PatchBasedStyleLoss()

# Define the total loss L_total as a weighted combination of the three losses
L_total = lambda_cycle * L_cycle + lambda_idt * L_idt + lambda_style * L_style

# Train G and D using an adversarial learning scheme
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get a pair of images A and B from the batch
    A, B = batch

    # Generate an image C that is structurally aligned to A and stylistically aligned to B using G
    C = G(A, B)

    # Generate an image D that is structurally aligned to B and stylistically aligned to A using G
    D = G(B, A)

    # Compute the discriminator outputs for real and fake patches at each scale
    D_A_real = D(A)
    D_B_real = D(B)
    D_C_fake = D(C)
    D_D_fake = D(D)

    # Compute the generator outputs for reconstructed images at each scale
    A_rec = G(D, A)
    B_rec = G(C, B)

    # Compute the generator loss as the sum of adversarial loss and total loss
    G_loss = AdversarialLoss(D_C_fake) + AdversarialLoss(D_D_fake) + L_total(A, B, C, D, A_rec, B_rec)

    # Compute the discriminator loss as the sum of adversarial loss for real and fake patches
    D_loss = AdversarialLoss(D_A_real) + AdversarialLoss(D_B_real) + AdversarialLoss(D_C_fake) + AdversarialLoss(D_D_fake)

    # Update the parameters of G and D using gradient descent
    G_optimizer.zero_grad()
    G_loss.backward()
    G_optimizer.step()

    D_optimizer.zero_grad()
    D_loss.backward()
    D_optimizer.step()

# Return the generated image C
return C
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.utils as utils

# Define the hyperparameters
num_epochs = 100 # number of training epochs
batch_size = 4 # batch size
image_size = 256 # image size
num_scales = 3 # number of scales for multi-scale patch discriminator and generator
lambda_cycle = 10 # weight for cycle-consistency loss
lambda_idt = 0.5 # weight for identity loss
lambda_style = 1 # weight for style loss
lr = 0.0002 # learning rate
beta1 = 0.5 # beta1 for Adam optimizer
beta2 = 0.999 # beta2 for Adam optimizer

# Define the device (CPU or GPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Define the image transformation
transform = transforms.Compose([
  transforms.Resize(image_size), # resize the image to the desired size
  transforms.CenterCrop(image_size), # crop the image at the center
  transforms.ToTensor(), # convert the image to a tensor
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalize the image to [-1, 1]
])

# Load the dataset of image pairs (A and B)
dataset = datasets.ImageFolder(root="data", transform=transform)

# Create a data loader for the dataset
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the multi-scale patch generator class
class MultiScalePatchGenerator(nn.Module):
  def __init__(self):
    super(MultiScalePatchGenerator, self).__init__()

    # Define the encoder-decoder network for each scale
    self.encoders = nn.ModuleList()
    self.decoders = nn.ModuleList()
    for i in range(num_scales):
      self.encoders.append(Encoder())
      self.decoders.append(Decoder())

    # Define the down-sampling and up-sampling layers for each scale
    self.downs = nn.ModuleList()
    self.ups = nn.ModuleList()
    for i in range(num_scales - 1):
      self.downs.append(nn.AvgPool2d(3, stride=2, padding=1))
      self.ups.append(nn.Upsample(scale_factor=2))

  def forward(self, x, y):
    # Input: two images x and y of shape (batch_size, 3, image_size, image_size)
    # Output: a list of generated images z at each scale of shape (batch_size, 3, image_size / (2 ** i), image_size / (2 ** i))

    # Initialize the list of generated images z
    z = []

    # For each scale from coarse to fine
    for i in range(num_scales):

      # Encode x and y using the encoder network at the current scale
      x_enc = self.encoders[i](x)
      y_enc = self.encoders[i](y)

      # Concatenate x_enc and y_enc along the channel dimension
      xy_enc = torch.cat([x_enc, y_enc], dim=1)

      # Decode xy_enc using the decoder network at the current scale
      z_i = self.decoders[i](xy_enc)

      # Append z_i to the list of generated images z
      z.append(z_i)

      # If not at the finest scale, down-sample x and y for the next scale
      if i < num_scales - 1:
        x = self.downs[i](x)
        y = self.downs[i](y)

    # Return the list of generated images z
    return z

# Define the encoder network class
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()

    # Define the convolutional layers with instance normalization and leaky ReLU activation
    self.conv1 = nn.Sequential(
      nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3),
      nn.InstanceNorm2d(64),
      nn.LeakyReLU(0.2)
    )
    self.conv2 = nn.Sequential(
      nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
      nn.InstanceNorm2d(128),
      nn.LeakyReLU(0.2)
    )
    self.conv3 = nn.Sequential(
      nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
      nn.InstanceNorm2d(256),
      nn.LeakyReLU(0.2)
    )

  def forward(self, x):
    # Input: an image x of shape (batch_size, 3, image_size, image_size)
    # Output: an encoded feature map of shape (batch_size, 256, image_size / 4, image_size / 4)

    # Apply the convolutional layers
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)

    # Return the encoded feature map
    return x

# Define the decoder network class
class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()

    # Define the convolutional layers with instance normalization and ReLU activation
    self.conv1 = nn.Sequential(
      nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),
      nn.InstanceNorm2d(256),
      nn.ReLU()
    )
    self.conv2 = nn.Sequential(
      nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),
      nn.InstanceNorm2d(128),
      nn.ReLU()
    )
    self.conv3 = nn.Sequential(
      nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),
      nn.InstanceNorm2d(64),
      nn.ReLU()
    )

    # Define the up-sampling layer
    self.up = nn.Upsample(scale_factor=2)

    # Define the output layer with tanh activation
    self.out = nn.Sequential(
      nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3),
      nn.Tanh()
    )

  def forward(self, x):
    # Input: an encoded feature map x of shape (batch_size, 512, image_size / 4, image_size / 4)
    # Output: a decoded image of shape (batch_size, 3, image_size / (2 ** i), image_size / (2 ** i))

    # Apply the convolutional layers
    x = self.conv1(x)
    x = self.up(x)
    x = self.conv2(x)
    x = self.up(x)
    x = self.conv3(x)

    # Apply the output layer
    x = self.out(x)

    # Return the decoded image
    return x

# Define the multi-scale patch discriminator class
class MultiScalePatchDiscriminator(nn.Module):
  def __init__(self):
    super(MultiScalePatchDiscriminator, self).__init__()

    # Define the patch discriminator network for each scale
    self.discriminators = nn.ModuleList()
    for i in range(num_scales):
      self.discriminators.append(PatchDiscriminator())

    # Define the down-sampling layer for each scale
    self.downs = nn.ModuleList()
    for i in range(num_scales - 1):
      self.downs.append(nn.AvgPool2d(3, stride=2, padding=1))

  def forward(self, x):
    # Input: an image x of shape (batch_size, 3, image_size / (2 ** i), image_size / (2 ** i))
    # Output: a list of discriminator outputs at each scale of shape (batch_size, 1)

    # Initialize the list of discriminator outputs y
    y = []

    # For each scale from coarse to fine
    for i in range(num_scales):

      # Apply the patch discriminator network at the current scale
      y_i = self.discriminators[i](x)

      # Append y_i to the list of discriminator outputs y
      y.append(y_i)

      # If not at the finest scale, down-sample x for the next scale
      if i < num_scales - 1:
        x = self.downs[i](x)

    # Return the list of discriminator outputs y
    return y

# Define the patch discriminator network class
class PatchDiscriminator(nn.Module):
  def __init__(self):
    super(PatchDiscriminator, self).__init__()

    # Define the convolutional layers with instance normalization and leaky ReLU activation
    self.conv1 = nn.Sequential(
      nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),
      nn.LeakyReLU(0.2)
    )
    self.conv2 = nn.Sequential(
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
      nn.InstanceNorm2d(128),
      nn.LeakyReLU(0.2)
    )