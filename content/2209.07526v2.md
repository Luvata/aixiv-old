---
title: 2209.07526v2 OmniVL One Foundation Model for Image-Language and Video-Language Tasks
date: 2022-09-08
---

# [OmniVL:One Foundation Model for Image-Language and Video-Language Tasks](http://arxiv.org/abs/2209.07526v2)

authors: Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, Lu Yuan


## What, Why and How

[1]: https://arxiv.org/abs/2209.07526v2 "[2209.07526v2] OmniVL:One Foundation Model for Image-Language and Video ..."
[2]: https://arxiv.org/pdf/2209.07526v2 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2209.07526v2 "[2209.07526v2] OmniVL:One Foundation Model for Image-Language and Video ..."

Here is a summary of the paper:

- **What**: The paper presents OmniVL, a new foundation model that can support both image-language and video-language tasks using one universal architecture.
- **Why**: The paper aims to design a model that can benefit both image and video tasks, as opposed to the conventional one-directional transfer (e.g., use image-language to help video-language), and that can leverage both supervised and noisily supervised pretraining data as much as possible.
- **How**: The paper proposes a decoupled joint pretraining of image-language and video-language to effectively decompose the vision-language modeling into spatial and temporal dimensions, and introduces a novel unified vision-language contrastive (UniVLC) loss to leverage image-text, video-text, image-label, and video-label data together. The paper also evaluates OmniVL on a wide range of downstream tasks and achieves state-of-the-art or competitive results with similar model size and data scale[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- They propose OmniVL, a new foundation model that can support both image-language and video-language tasks using one universal architecture, without incurring extra task-specific adaptors.
- They propose a decoupled joint pretraining of image-language and video-language to effectively decompose the vision-language modeling into spatial and temporal dimensions and obtain performance boost on both image and video tasks.
- They introduce a novel unified vision-language contrastive (UniVLC) loss to leverage image-text, video-text, image-label, and video-label data together, so that both supervised and noisily supervised pretraining data are utilized as much as possible.
- They evaluate OmniVL on a wide range of downstream tasks and achieve state-of-the-art or competitive results with similar model size and data scale.

## Method Summary

[1]: https://arxiv.org/pdf/2209.07526v2 "Abstract - arXiv.org"
[2]: http://export.arxiv.org/abs/2209.07526v2 "[2209.07526v2] OmniVL:One Foundation Model for Image-Language and Video ..."
[3]: http://export.arxiv.org/abs/2109.07526v2 "[2109.07526v2] Fast Magnetic Reconnection induced by Resistivity ..."

Here is a summary of the method section of the paper:

- The paper proposes a **universal architecture** for both image-language and video-language tasks, which consists of a **unified visual encoder** and a **shared text encoder**. The visual encoder is a transformer-based model that can process both image and video inputs by using different input embeddings and attention masks. The text encoder is also a transformer-based model that can encode text inputs such as captions or questions.
- The paper proposes a **decoupled joint pretraining** of image-language and video-language, which consists of two stages: **spatial pretraining** and **temporal pretraining**. In the spatial pretraining stage, the model is pretrained on image-text pairs using masked language modeling (MLM) and masked region modeling (MRM) objectives. In the temporal pretraining stage, the model is further pretrained on video-text pairs using masked frame modeling (MFM) and masked clip modeling (MCM) objectives. The paper argues that this decoupled pretraining can effectively decompose the vision-language modeling into spatial and temporal dimensions and obtain performance boost on both image and video tasks.
- The paper introduces a novel **unified vision-language contrastive (UniVLC) loss** to leverage image-text, video-text, image-label, and video-label data together, so that both supervised and noisily supervised pretraining data are utilized as much as possible. The UniVLC loss is based on the contrastive learning framework, where the model learns to maximize the similarity between positive pairs (e.g., image-text or video-text pairs that are aligned) and minimize the similarity between negative pairs (e.g., image-text or video-text pairs that are randomly sampled). The paper also proposes a **multi-modal contrastive sampling strategy** to sample negative pairs from different modalities and domains, which can enhance the robustness and generalization of the model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the universal architecture for OmniVL
def OmniVL():
  # Define the unified visual encoder
  def visual_encoder():
    # Use different input embeddings for image and video inputs
    if input is image:
      input_embedding = image_embedding(input)
    elif input is video:
      input_embedding = video_embedding(input)
    # Use a transformer-based model to encode the visual input
    visual_output = transformer(input_embedding)
    return visual_output
  
  # Define the shared text encoder
  def text_encoder():
    # Use a standard text embedding for text inputs
    input_embedding = text_embedding(input)
    # Use a transformer-based model to encode the text input
    text_output = transformer(input_embedding)
    return text_output
  
  # Return the visual encoder and the text encoder
  return visual_encoder, text_encoder

# Define the decoupled joint pretraining of image-language and video-language
def decoupled_joint_pretraining():
  # Initialize the visual encoder and the text encoder
  visual_encoder, text_encoder = OmniVL()
  
  # Define the spatial pretraining stage
  def spatial_pretraining():
    # Load image-text pairs from pretraining data
    for image, text in image_text_data:
      # Encode the image and the text using the visual encoder and the text encoder
      image_output = visual_encoder(image)
      text_output = text_encoder(text)
      # Compute the MLM and MRM losses using the image output and the text output
      mlm_loss = compute_mlm_loss(text_output)
      mrm_loss = compute_mrm_loss(image_output)
      # Update the model parameters using the MLM and MRM losses
      update_model(mlm_loss + mrm_loss)
  
  # Define the temporal pretraining stage
  def temporal_pretraining():
    # Load video-text pairs from pretraining data
    for video, text in video_text_data:
      # Encode the video and the text using the visual encoder and the text encoder
      video_output = visual_encoder(video)
      text_output = text_encoder(text)
      # Compute the MFM and MCM losses using the video output and the text output
      mfm_loss = compute_mfm_loss(video_output)
      mcm_loss = compute_mcm_loss(video_output)
      # Update the model parameters using the MFM and MCM losses
      update_model(mfm_loss + mcm_loss)
  
  # Perform spatial pretraining followed by temporal pretraining
  spatial_pretraining()
  temporal_pretraining()

# Define the unified vision-language contrastive (UniVLC) loss
def UniVLC():
  # Initialize the visual encoder and the text encoder
  visual_encoder, text_encoder = OmniVL()
  
  # Load image-text, video-text, image-label, and video-label pairs from pretraining data
  for (image, text), (video, text), (image, label), (video, label) in pretraining_data:
    # Encode the image, video, text, label using the visual encoder and the text encoder
    image_output = visual_encoder(image)
    video_output = visual_encoder(video)
    text_output = text_encoder(text)
    label_output = text_encoder(label)
    
    # Compute the similarity scores between positive and negative pairs using dot product
    positive_score_image_text = dot_product(image_output, text_output)
    positive_score_video_text = dot_product(video_output, text_output)
    positive_score_image_label = dot_product(image_output, label_output)
    positive_score_video_label = dot_product(video_output, label_output)
    
    negative_score_image_text = dot_product(image_output, sample_negative(text))
    negative_score_video_text = dot_product(video_output, sample_negative(text))
    negative_score_image_label = dot_product(image_output, sample_negative(label))
    negative_score_video_label = dot_product(video_output, sample_negative(label))
    
    # Compute the UniVLC loss using a softmax function over positive and negative scores
    univlc_loss_image_text = softmax(positive_score_image_text - negative_score_image_text)
    univlc_loss_video_text = softmax(positive_score_video_text - negative_score_video_text)
    univlc_loss_image_label = softmax(positive_score_image_label - negative_score_image_label)
    univlc_loss_video_label = softmax(positive_score_video_label - negative_score_video_label)

    # Update the model parameters using the UniVLC loss
    update_model(univlc_loss_image_text + univlc_loss_video_text + univlc_loss_image_label + univlc_loss_video_label)

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
image_size = 224 # The size of the input image
video_size = 112 # The size of the input video frame
video_length = 32 # The length of the input video clip
hidden_size = 768 # The hidden size of the transformer model
num_heads = 12 # The number of attention heads in the transformer model
num_layers = 12 # The number of layers in the transformer model
vocab_size = 30522 # The size of the text vocabulary
max_length = 128 # The maximum length of the text input
mlm_prob = 0.15 # The probability of masking tokens for MLM objective
mrm_prob = 0.15 # The probability of masking regions for MRM objective
mfm_prob = 0.15 # The probability of masking frames for MFM objective
mcm_prob = 0.15 # The probability of masking clips for MCM objective
temperature = 0.07 # The temperature parameter for softmax function
batch_size = 256 # The batch size for pretraining
learning_rate = 1e-4 # The learning rate for pretraining
num_epochs = 100 # The number of epochs for pretraining

# Define the universal architecture for OmniVL
def OmniVL():
  # Define the unified visual encoder
  def visual_encoder():
    # Use different input embeddings for image and video inputs
    if input is image:
      # Resize the image to a fixed size and normalize it
      input = torchvision.transforms.Resize(image_size)(input)
      input = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(input)
      # Use a convolutional layer to project the image to a hidden size
      input_embedding = torch.nn.Conv2d(3, hidden_size, kernel_size=1)(input)
    elif input is video:
      # Resize the video frames to a fixed size and normalize them
      input = torchvision.transforms.Resize(video_size)(input)
      input = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(input)
      # Use a convolutional layer to project the video frames to a hidden size
      input_embedding = torch.nn.Conv3d(3, hidden_size, kernel_size=1)(input)
    # Use a transformer-based model to encode the visual input
    visual_output = transformers.TransformerEncoder(hidden_size, num_heads, num_layers)(input_embedding)
    return visual_output
  
  # Define the shared text encoder
  def text_encoder():
    # Use a standard text embedding for text inputs
    input_embedding = transformers.BertEmbeddings(vocab_size)(input)
    # Use a transformer-based model to encode the text input
    text_output = transformers.TransformerEncoder(hidden_size, num_heads, num_layers)(input_embedding)
    return text_output
  
  # Return the visual encoder and the text encoder
  return visual_encoder, text_encoder

# Define the decoupled joint pretraining of image-language and video-language
def decoupled_joint_pretraining():
  # Initialize the visual encoder and the text encoder
  visual_encoder, text_encoder = OmniVL()
  
  # Define the spatial pretraining stage
  def spatial_pretraining():
    # Load image-text pairs from pretraining data
    for image, text in image_text_data:
      # Randomly mask some tokens in the text for MLM objective
      masked_text, mlm_labels = mask_tokens(text, mlm_prob)
      # Randomly mask some regions in the image for MRM objective
      masked_image, mrm_labels = mask_regions(image, mrm_prob)
      # Encode the masked image and the masked text using the visual encoder and the text encoder
      image_output = visual_encoder(masked_image)
      text_output = text_encoder(masked_text)
      # Compute the MLM and MRM losses using the image output and the text output
      mlm_loss = compute_mlm_loss(text_output, mlm_labels)
      mrm_loss = compute_mrm_loss(image_output, mrm_labels)
      # Update the model parameters using the MLM and MRM losses
      update_model(mlm_loss + mrm_loss)
  
  # Define the temporal pretraining stage
  def temporal_pretraining():
    # Load video-text pairs from pretraining data
    for video, text in video_text_data:
      # Randomly mask some tokens in the text for MLM objective
      masked_text, mlm_labels = mask_tokens(text, mlm_prob)
      # Randomly mask some frames in the video for MFM objective
      masked_video, mfm_labels = mask_frames(video, mfm_prob)
      # Randomly mask some clips in the video for MCM objective
      masked_video, mcm_labels = mask_clips(video, mcm_prob)
      # Encode the masked video and the masked text using the visual encoder and the text encoder
      video_output = visual_encoder(masked_video)
      text_output = text_encoder(masked_text)
      # Compute the MFM and MCM losses using the video output and the text output
      mfm_loss = compute_mfm_loss(video_output, mfm_labels)
      mcm_loss = compute_mcm_loss(video_output, mcm_labels)
      # Update the model parameters using the MFM and MCM losses
      update_model(mfm_loss + mcm_loss)
  
  # Perform spatial pretraining followed by temporal pretraining
  spatial_pretraining()
  temporal_pretraining()

# Define the unified vision-language contrastive (UniVLC) loss
def UniVLC():
  # Initialize the visual encoder and the text encoder
  visual_encoder, text_encoder = OmniVL()
  
  # Load image-text, video-text, image-label, and video-label pairs from pretraining data
  for (image, text), (video, text), (image, label), (video, label) in pretraining_data:
    # Encode the image, video, text, label using the visual encoder and the text encoder
    image_output = visual_encoder(image)
    video_output = visual_encoder(video)
    text_output = text_encoder(text)
    label_output = text_encoder(label)
    
    # Compute the similarity scores between positive and negative pairs using dot product
    positive_score_image_text = dot_product(image_output, text_output)
    positive_score_video_text = dot_product(video_output, text_output)
    positive_score_image_label = dot_product(image_output, label_output)
    positive_score_video_label = dot_product(video_output, label_output)
    
    negative_score_image_text = dot_product(image_output, sample_negative(text))
    negative_score_video_text = dot_product(video_output, sample_negative(text))
    negative_score_image_label = dot_product(image_output, sample_negative(label))
    negative_score_video_label = dot_product(video_output, sample_negative(label))
    
    # Compute the UniVLC loss using a softmax function over positive and negative scores
    univlc_loss_image_text = softmax(positive_score_image_text - negative_score_image_text) / temperature
    univlc_loss_video_text = softmax(positive_score_video_text - negative_score_video_text) / temperature
    univlc_loss_image_label = softmax(positive_score_image_label - negative_score_image_label) / temperature
    univlc_loss_video_label = softmax(positive_score_video_label - negative_score_video_label) / temperature

    # Update the model parameters using the UniVLC loss
    update_model(univlc_loss_image_text + univlc_loss_video_text + univlc_loss_image_label + univlc_loss_video_label)

# Define some helper functions for masking tokens, regions, frames, clips and sampling negatives

def mask_tokens(text, prob):
  # Convert the text to a list of token ids
  token_ids = tokenizer.encode(text)
  # Initialize a list of labels with -100 (ignore index)
  labels = [-100] * len(token_ids)
  # Randomly select some token ids to mask based on the probability
  masked_indices = np.random.choice(len(token_ids), int(prob * len(token_ids)), replace=False)
  # Replace the selected token ids with [MASK] token id and update the labels accordingly
  for index in masked_indices:
    token_ids[index] = tokenizer.mask_token_id
    labels[index] = token_ids[index]
  # Return the masked token ids and labels as tensors
  return torch.tensor(token_ids), torch.tensor(labels)

def mask_regions(image, prob):
  # Convert the image to a tensor of shape (3, H, W)
  image_tensor = torchvision.transforms.ToTensor()(image)
  # Initialize a tensor of labels with -100 (ignore index) of shape (hidden_size,)
  labels = torch.full((hidden_size,), -100)
  # Randomly select some regions to mask based on the probability
  masked_indices = np.random.choice(hidden_size, int(prob * hidden_size), replace=False)
  # Replace the selected regions with zero values and update the labels accordingly
  for index in masked_indices:
    image_tensor[:, index // image_size, index % image_size] = 0
    labels[index] = image_tensor[:, index // image_size, index % image_size]
  # Return the masked image tensor and labels
  return image_tensor, labels

def mask_frames(video, prob):
  # Convert