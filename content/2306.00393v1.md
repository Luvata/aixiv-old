---
title: 2306.00393v1 Teacher Agent  A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning
date: 2023-06-01
---

# [Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning](http://arxiv.org/abs/2306.00393v1)

authors: Shengqin Jiang, Yaoyu Fang, Haokui Zhang, Peng Wang, Yuankai Qi, Qingshan Liu


## What, Why and How

[1]: https://arxiv.org/abs/2306.00393 "[2306.00393] Teacher Agent: A Non-Knowledge Distillation Method for ..."
[2]: https://arxiv.org/pdf/2306.00393v1.pdf "JOURNAL OF LA Teacher Agent: A Non-Knowledge Distillation ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00393 "[2306.00393] Teacher Agent: A Non-Knowledge Distillation Method for ..."

Here is a summary of the paper based on the search results:

- **What**: The paper proposes a novel method for rehearsal-based video incremental learning, which is a task of learning new video categories without forgetting previous ones. The method uses a teacher agent to generate soft labels for memory exemplars instead of using a teacher model for knowledge distillation.
- **Why**: The paper aims to address the problem of catastrophic forgetting and knowledge misleading in video incremental learning. Catastrophic forgetting occurs when the network forgets previously learned data while learning new categories. Knowledge misleading happens when the teacher model makes inaccurate predictions for some memory exemplars, which limits the student model's performance.
- **How**: The paper introduces a teacher agent that can generate stable and accurate soft labels for memory exemplars by learning from the teacher model's output and the ground truth labels. The teacher agent is trained with a triplet loss and a cross-entropy loss to capture the similarities among different categories and to match the ground truth labels. The teacher agent replaces the teacher model in the knowledge distillation process, which avoids the computational overhead of loading the teacher model and circumvents the problem of knowledge misleading. The paper evaluates the proposed method on three video action recognition datasets and compares it with several state-of-the-art methods. The paper shows that the proposed method achieves significant performance improvements and surpasses the performance of joint training when using more samples in episodic memory.

## Main Contributions

According to the paper, the main contributions are:

- The paper proposes a novel teacher agent that can generate stable and accurate soft labels for memory exemplars in rehearsal-based video incremental learning.
- The paper introduces a new training strategy for the teacher agent that involves learning from both the teacher model's output and the ground truth labels.
- The paper demonstrates the advantages of the proposed method over existing methods in terms of performance, memory efficiency, and computational cost.

## Method Summary

The method section of the paper consists of four subsections:

- **Problem formulation**: The paper defines the problem of video incremental learning as learning new video categories without forgetting previous ones. The paper assumes that the video data are divided into several batches, each containing a subset of categories. The paper also introduces some notations and definitions related to the problem.
- **Rehearsal-based incremental learning**: The paper reviews the existing rehearsal-based incremental learning framework, which involves storing a small number of samples from previous batches in an episodic memory and using them to prevent catastrophic forgetting. The paper also describes the knowledge distillation technique, which is commonly used to transfer knowledge from a teacher model to a student model in this framework.
- **Teacher agent**: The paper proposes a teacher agent that can generate soft labels for memory exemplars instead of using a teacher model. The paper explains the motivation and the architecture of the teacher agent, which consists of two branches: a feature extractor and a classifier. The paper also introduces a new training strategy for the teacher agent, which involves learning from both the teacher model's output and the ground truth labels. The paper details the loss functions and the optimization algorithm used for training the teacher agent.
- **Incremental learning with teacher agent**: The paper describes how to use the teacher agent in the incremental learning process. The paper explains how to update the episodic memory, how to generate soft labels for memory exemplars using the teacher agent, and how to train the student model with knowledge distillation using the soft labels. The paper also discusses some implementation details and hyperparameters of the proposed method.

## Pseudo Code

Here is a possible pseudo code to implement the paper:

```python
# Define the teacher model, the student model, and the teacher agent
teacher_model = ResNet3D()
student_model = ResNet3D()
teacher_agent = TeacherAgent()

# Define the loss functions and the optimizers
kd_loss = KLDivLoss() # knowledge distillation loss
ce_loss = CrossEntropyLoss() # cross-entropy loss
tr_loss = TripletLoss() # triplet loss
student_optimizer = SGD(student_model.parameters())
teacher_agent_optimizer = SGD(teacher_agent.parameters())

# Initialize the episodic memory
memory = []

# Loop over the batches of video data
for batch in batches:
  # Get the video clips and the labels from the batch
  clips, labels = batch

  # Train the teacher model on the current batch
  teacher_model.train(clips, labels)

  # Update the episodic memory with samples from the current batch
  memory = update_memory(memory, clips, labels)

  # Train the teacher agent on the episodic memory
  for epoch in epochs:
    # Shuffle the episodic memory
    shuffle(memory)

    # Loop over the mini-batches of memory exemplars
    for mini_batch in memory:
      # Get the memory clips and the labels from the mini-batch
      memory_clips, memory_labels = mini_batch

      # Forward pass the memory clips through the teacher model and get the logits
      teacher_logits = teacher_model(memory_clips)

      # Forward pass the memory clips through the teacher agent and get the logits and the features
      agent_logits, agent_features = teacher_agent(memory_clips)

      # Compute the cross-entropy loss between the agent logits and the ground truth labels
      ce_loss_value = ce_loss(agent_logits, memory_labels)

      # Compute the triplet loss between the agent features and the teacher logits
      tr_loss_value = tr_loss(agent_features, teacher_logits)

      # Compute the total loss as a weighted sum of the cross-entropy loss and the triplet loss
      total_loss_value = alpha * ce_loss_value + beta * tr_loss_value

      # Backpropagate the gradients and update the teacher agent parameters
      teacher_agent_optimizer.zero_grad()
      total_loss_value.backward()
      teacher_agent_optimizer.step()

  # Train the student model on the current batch and the episodic memory with knowledge distillation
  for epoch in epochs:
    # Shuffle the current batch and the episodic memory
    shuffle(batch)
    shuffle(memory)

    # Loop over the mini-batches of video clips from both sources
    for mini_batch in batch + memory:
      # Get the video clips and the labels from the mini-batch
      clips, labels = mini_batch

      # Forward pass the clips through the student model and get the logits
      student_logits = student_model(clips)

      # Forward pass the clips through the teacher agent and get the soft labels
      soft_labels = teacher_agent(clips)

      # Compute the knowledge distillation loss between the student logits and the soft labels
      kd_loss_value = kd_loss(student_logits, soft_labels)

      # Backpropagate the gradients and update the student model parameters
      student_optimizer.zero_grad()
      kd_loss_value.backward()
      student_optimizer.step()
```