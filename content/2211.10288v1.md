---
title: 2211.10288v1 Just a Matter of Scale? Reevaluating Scale Equivariance in Convolutional Neural Networks
date: 2022-11-11
---

# [Just a Matter of Scale? Reevaluating Scale Equivariance in Convolutional Neural Networks](http://arxiv.org/abs/2211.10288v1)

authors: Thomas Altstidl, An Nguyen, Leo Schwinn, Franz Köferl, Christopher Mutschler, Björn Eskofier, Dario Zanca


## What, Why and How

[1]: https://arxiv.org/abs/2211.10288 "Just a Matter of Scale? Reevaluating Scale Equivariance in ..."
[2]: https://arxiv.org/pdf/2211.10288 "Abstract - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2111.10288v1 "[2111.10288v1] On the applicability of single-spacecraft interferometry ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper evaluates the performance of different convolutional neural network models on a novel benchmark that tests their ability to generalize to images with different scales and translations.
- **Why**: The paper aims to address the gap in understanding how scale equivariance can improve generalization and robustness of convolutional neural networks, which are widely used for image recognition tasks but are not equivariant to variations in scale.
- **How**: The paper proposes the Scaled and Translated Image Recognition (STIR) benchmark based on four different domains: MNIST, CIFAR-10, ImageNet, and COCO. The paper also introduces a new family of models that applies many re-scaled kernels with shared weights in parallel and then selects the most appropriate one. The paper compares the existing and proposed models on STIR and measures their accuracy, scale equivariance, and kernel selection consistency.

## Main Contributions

[1]: https://arxiv.org/abs/2211.10288 "Just a Matter of Scale? Reevaluating Scale Equivariance in ..."
[2]: https://arxiv.org/pdf/2211.10288 "Abstract - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2111.10288v1 "[2111.10288v1] On the applicability of single-spacecraft interferometry ..."

The paper claims to make the following contributions[^1^][1]:

- **A novel benchmark** for evaluating the generalization and robustness of convolutional neural network models to images with different scales and translations, called STIR, based on four different domains: MNIST, CIFAR-10, ImageNet, and COCO.
- **A new family of models** that applies many re-scaled kernels with shared weights in parallel and then selects the most appropriate one, called Scale-Adaptive Convolutional Neural Networks (SACNNs).
- **An extensive experimental evaluation** of existing and proposed models on STIR, showing that SACNNs can improve generalization across scales, scale equivariance, and kernel selection consistency compared to standard convolutions and other methods.

## Method Summary

[1]: https://arxiv.org/abs/2211.10288 "Just a Matter of Scale? Reevaluating Scale Equivariance in ..."
[2]: https://arxiv.org/pdf/2211.10288 "Abstract - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2111.10288v1 "[2111.10288v1] On the applicability of single-spacecraft interferometry ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the **STIR benchmark**, which consists of four datasets: MNIST, CIFAR-10, ImageNet, and COCO. Each dataset is split into three subsets: train, test, and test-scaled. The train and test subsets are the same as the original datasets, while the test-scaled subset contains images that are randomly scaled and translated by different factors. The paper defines two evaluation metrics: accuracy and scale equivariance. Accuracy measures the percentage of correctly classified images, while scale equivariance measures the percentage of images that are classified consistently across different scales.
- The paper introduces the **SACNNs**, which are a family of models that apply many re-scaled kernels with shared weights in parallel and then select the most appropriate one. The paper defines two types of SACNNs: SACNN-S and SACNN-M. SACNN-S applies a single kernel with different scales to each input channel, while SACNN-M applies multiple kernels with different scales to each input channel. The paper also defines two methods for kernel selection: max-pooling and attention. Max-pooling selects the kernel with the highest activation value, while attention computes a weighted average of all kernels based on their activation values.
- The paper conducts **experiments** on the STIR benchmark using different models: standard convolutions (Conv), group equivariant convolutions (G-CNN) [8], scale equivariant convolutions (SE-CNN) [9], scale-steerable filters (SSF) [10], SACNN-S with max-pooling (SACNN-S-MP), SACNN-S with attention (SACNN-S-AT), SACNN-M with max-pooling (SACNN-M-MP), and SACNN-M with attention (SACNN-M-AT). The paper compares the models on accuracy, scale equivariance, and kernel selection consistency. Kernel selection consistency measures the percentage of images that select the same kernel across different scales.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the SACNN model
class SACNN(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, scales, mode):
    # Initialize the model parameters
    self.in_channels = in_channels
    self.out_channels = out_channels
    self.kernel_size = kernel_size
    self.scales = scales # a list of scale factors
    self.mode = mode # either "S" or "M"
    self.num_kernels = len(scales) # the number of re-scaled kernels

    # Create the shared weight matrix
    self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))

    # Create the attention mechanism if needed
    if mode == "S":
      self.attention = nn.Linear(in_channels * kernel_size * kernel_size, num_kernels)
    elif mode == "M":
      self.attention = nn.Linear(in_channels * num_kernels * kernel_size * kernel_size, num_kernels)

  def forward(self, x):
    # Input: x is a tensor of shape (batch_size, in_channels, height, width)
    # Output: y is a tensor of shape (batch_size, out_channels, height, width)

    # Apply each re-scaled kernel to the input and store the results in a list
    outputs = []
    for scale in self.scales:
      # Resize the weight matrix by the scale factor
      resized_weight = F.interpolate(self.weight, scale_factor=scale, mode="bilinear", align_corners=False)

      # Apply the convolution operation
      output = F.conv2d(x, resized_weight, padding=self.kernel_size // 2)

      # Append the output to the list
      outputs.append(output)

    # Concatenate the outputs along the channel dimension
    outputs = torch.cat(outputs, dim=1) # shape: (batch_size, out_channels * num_kernels, height, width)

    # Apply the attention mechanism to select the most appropriate kernel for each output channel
    if self.mode == "S":
      # Reshape the outputs to match the attention input
      outputs = outputs.view(batch_size, out_channels * num_kernels * in_channels * kernel_size * kernel_size)

      # Compute the attention scores
      scores = self.attention(outputs) # shape: (batch_size, num_kernels)

      # Apply softmax to get the attention weights
      weights = F.softmax(scores, dim=1) # shape: (batch_size, num_kernels)

      # Reshape the weights to match the output shape
      weights = weights.view(batch_size, num_kernels, 1, 1, 1)

      # Reshape the outputs to match the weight shape
      outputs = outputs.view(batch_size, out_channels, num_kernels, height, width)

      # Apply the weighted average of the outputs
      y = torch.sum(outputs * weights, dim=2) # shape: (batch_size, out_channels, height, width)
    
    elif self.mode == "M":
      # Reshape the outputs to match the attention input
      outputs = outputs.view(batch_size, out_channels * num_kernels * in_channels * num_kernels * kernel_size * kernel_size)

      # Compute the attention scores
      scores = self.attention(outputs) # shape: (batch_size, num_kernels)

      # Apply softmax to get the attention weights
      weights = F.softmax(scores, dim=1) # shape: (batch_size, num_kernels)

      # Reshape the weights to match the output shape
      weights = weights.view(batch_size, 1, num_kernels, 1, 1)

      # Reshape the outputs to match the weight shape
      outputs = outputs.view(batch_size, out_channels * num_kernels , num_kernels , height , width )

      # Apply the weighted average of the outputs
      y = torch.sum(outputs * weights , dim=2) # shape: (batch_size , out_channels , height , width )

    return y

# Define the STIR benchmark
class STIR(Dataset):
  def __init__(self , dataset_name , subset_name ):
    # Initialize the dataset parameters
    self.dataset_name = dataset_name # either "MNIST" , "CIFAR-10" , "ImageNet" , or "COCO"
    self.subset_name = subset_name   # either "train" , "test" , or "test-scaled"

    # Load the original dataset and split it into train and test subsets
    if dataset_name == "MNIST":
      self.dataset = torchvision.datasets.MNIST(root="./data" , train=(subset_name == "train") , download=True)
    elif dataset_name == "CIFAR-10":
      self.dataset = torchvision.datasets.CIFAR10(root="./data" , train=(subset_name == "train") , download=True)
    elif dataset_name == "ImageNet":
      self.dataset = torchvision.datasets.ImageNet(root="./data" , split=subset_name , download=True)
    elif dataset_name == "COCO":
      self.dataset = torchvision.datasets.CocoDetection(root="./data" , annFile="./data/annotations/instances_" + subset_name + ".json" , download=True)

    # Define the scale and translation factors for the test-scaled subset
    if subset_name == "test-scaled":
      self.scale_factors = [0.5 , 0.75 , 1.25 , 1.5] # a list of scale factors
      self.translation_factors = [-0.25 , -0.125 , 0.125 , 0.25] # a list of translation factors

  def __len__(self):
    # Return the length of the dataset
    return len(self.dataset)

  def __getitem__(self , index):
    # Input: index is an integer between 0 and len(self.dataset) - 1
    # Output: a tuple of (image , label) where image is a tensor of shape (3 , height , width) and label is an integer

    # Get the original image and label from the dataset
    image , label = self.dataset[index]

    # Convert the image to a tensor and normalize it
    image = torchvision.transforms.ToTensor()(image)
    image = torchvision.transforms.Normalize(mean=[0.485 , 0.456 , 0.406] , std=[0.229 , 0.224 , 0.225])(image)

    # If the subset is test-scaled, apply a random scale and translation to the image
    if self.subset_name == "test-scaled":
      # Choose a random scale factor from the list
      scale_factor = random.choice(self.scale_factors)

      # Resize the image by the scale factor
      image = F.interpolate(image.unsqueeze(0) , scale_factor=scale_factor , mode="bilinear" , align_corners=False).squeeze(0)

      # Choose a random translation factor from the list
      translation_factor = random.choice(self.translation_factors)

      # Translate the image by the translation factor
      image = torchvision.transforms.functional.affine(image=image.unsqueeze(0) , angle=0 , translate=(int(translation_factor * image.shape[1]) , int(translation_factor * image.shape[2])) , scale=1 , shear=0).squeeze(0)

      # Pad or crop the image to match the original size
      if scale_factor < 1:
        # Pad the image with zeros
        padding = (image.shape[1] - self.dataset[0][0].size[0]) // 2
        image = F.pad(image=image.unsqueeze(0) , pad=(padding, padding, padding, padding) , mode="constant" , value=0).squeeze(0)
      elif scale_factor > 1:
        # Crop the image from the center
        crop = (image.shape[1] - self.dataset[0][0].size[0]) // 2
        image = torchvision.transforms.CenterCrop(size=self.dataset[0][0].size)(image.unsqueeze(0)).squeeze(0)

    return (image, label)

# Define the evaluation metrics
def accuracy(model, dataloader):
  # Input: model is a SACNN model, dataloader is a torch.utils.data.DataLoader object for STIR
  # Output: acc is a float between 0 and 100

  # Initialize the number of correct predictions and the total number of predictions
  correct = 0
  total = 0

  # Loop over the dataloader
  for images, labels in dataloader:
    # Move the images and labels to the device (CPU or GPU)
    images = images.to(device)
    labels = labels.to(device)

    # Forward pass the images through the model and get the predictions
    outputs = model(images)
    predictions = torch.argmax(outputs, dim=1)

    # Update the number of correct predictions and the total number of predictions
    correct += torch.sum(predictions == labels).item()
    total += len(labels)

  # Compute the accuracy as the percentage of correct predictions over the total number of predictions
  acc = correct / total * 100

  return acc

def scale_equivariance(model, dataloader):
  # Input: model is a SACNN model, dataloader is a torch.utils.data.DataLoader object for STIR test-scaled subset
  # Output: seq is a float between 0 and 100

  # Initialize the number of

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import random

# Define the device (CPU or GPU) to use
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the SACNN model
class SACNN(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, scales, mode):
    # Initialize the model parameters
    super(SACNN, self).__init__()
    self.in_channels = in_channels
    self.out_channels = out_channels
    self.kernel_size = kernel_size
    self.scales = scales # a list of scale factors
    self.mode = mode # either "S" or "M"
    self.num_kernels = len(scales) # the number of re-scaled kernels

    # Create the shared weight matrix
    self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))

    # Create the attention mechanism if needed
    if mode == "S":
      self.attention = nn.Linear(in_channels * kernel_size * kernel_size, num_kernels)
    elif mode == "M":
      self.attention = nn.Linear(in_channels * num_kernels * kernel_size * kernel_size, num_kernels)

  def forward(self, x):
    # Input: x is a tensor of shape (batch_size, in_channels, height, width)
    # Output: y is a tensor of shape (batch_size, out_channels, height, width)

    # Apply each re-scaled kernel to the input and store the results in a list
    outputs = []
    for scale in self.scales:
      # Resize the weight matrix by the scale factor
      resized_weight = F.interpolate(self.weight, scale_factor=scale, mode="bilinear", align_corners=False)

      # Apply the convolution operation
      output = F.conv2d(x, resized_weight, padding=self.kernel_size // 2)

      # Append the output to the list
      outputs.append(output)

    # Concatenate the outputs along the channel dimension
    outputs = torch.cat(outputs, dim=1) # shape: (batch_size, out_channels * num_kernels, height, width)

    # Apply the attention mechanism to select the most appropriate kernel for each output channel
    if self.mode == "S":
      # Reshape the outputs to match the attention input
      outputs = outputs.view(batch_size, out_channels * num_kernels * in_channels * kernel_size * kernel_size)

      # Compute the attention scores
      scores = self.attention(outputs) # shape: (batch_size, num_kernels)

      # Apply softmax to get the attention weights
      weights = F.softmax(scores, dim=1) # shape: (batch_size, num_kernels)

      # Reshape the weights to match the output shape
      weights = weights.view(batch_size, num_kernels, 1, 1, 1)

      # Reshape the outputs to match the weight shape
      outputs = outputs.view(batch_size, out_channels, num_kernels, height, width)

      # Apply the weighted average of the outputs
      y = torch.sum(outputs * weights, dim=2) # shape: (batch_size, out_channels, height, width)
    
    elif self.mode == "M":
      # Reshape the outputs to match the attention input
      outputs = outputs.view(batch_size, out_channels * num_kernels * in_channels * num_kernels * kernel_size * kernel_size)

      # Compute the attention scores
      scores = self.attention(outputs) # shape: (batch_size, num_kernels)

      # Apply softmax to get the attention weights
      weights = F.softmax(scores, dim=1) # shape: (batch_size, num_kernels)

      # Reshape the weights to match the output shape
      weights = weights.view(batch_size, 1 , num_kernels , 1 , 1 )

      # Reshape the outputs to match the weight shape
      outputs = outputs.view(batch_size , out_channels * num_kernels , num_kernels , height , width )

      # Apply the weighted average of the outputs
      y = torch.sum(outputs * weights , dim=2) # shape: (batch_size , out_channels , height , width )

    return y

# Define a function to create a SACNN-based model with multiple layers and a classifier head
def create_sacnn_model(num_classes , scales , mode ):
  # Input: num_classes is an integer for the number of classes in the dataset , scales is a list of scale factors , mode is either "S" or "M"
  # Output: model is a SACNN-based model

  # Create a list of SACNN layers with different parameters
  sacnn_layers = [
    SACNN(in_channels=3 , out_channels=64 , kernel_size=3 , scales=scales , mode=mode ) , # first layer
    nn.ReLU() , # activation function
    nn.MaxPool2d(kernel_size=2) , # pooling layer
    SACNN(in_channels=64 , out_channels=128 , kernel_size=3 , scales=scales , mode=mode ) , # second layer
    nn.ReLU() , # activation function
    nn.MaxPool2d(kernel_size=2) , # pooling layer
    SACNN(in_channels=128 , out_channels=256 , kernel_size=3 , scales=scales , mode=mode ) , # third layer
    nn.ReLU() , # activation function
    nn.MaxPool2d(kernel_size=2) # pooling layer
  ]

  # Create a classifier head with a linear layer and a softmax function
  classifier_head = [
    nn.Flatten() , # flatten the output of the last SACNN layer
    nn.Linear(in_features=256 * 4 * 4 , out_features=num_classes) , # linear layer
    nn.Softmax(dim=1) # softmax function
  ]

  # Concatenate the SACNN layers and the classifier head
  model = nn.Sequential(*sacnn_layers + classifier_head)

  return model

# Define the STIR benchmark
class STIR(Dataset):
  def __init__(self, dataset_name, subset_name):
    # Initialize the dataset parameters
    self.dataset_name = dataset_name # either "MNIST", "CIFAR-10", "ImageNet", or "COCO"
    self.subset_name = subset_name   # either "train", "test", or "test-scaled"

    # Load the original dataset and split it into train and test subsets
    if dataset_name == "MNIST":
      self.dataset = torchvision.datasets.MNIST(root="./data", train=(subset_name == "train"), download=True)
    elif dataset_name == "CIFAR-10":
      self.dataset = torchvision.datasets.CIFAR10(root="./data", train=(subset_name == "train"), download=True)
    elif dataset_name == "ImageNet":
      self.dataset = torchvision.datasets.ImageNet(root="./data", split=subset_name, download=True)
    elif dataset_name == "COCO":
      self.dataset = torchvision.datasets.CocoDetection(root="./data", annFile="./data/annotations/instances_" + subset_name + ".json", download=True)

    # Define the scale and translation factors for the test-scaled subset
    if subset_name == "test-scaled":
      self.scale_factors = [0.5, 0.75, 1.25, 1.5] # a list of scale factors
      self.translation_factors = [-0.25, -0.125, 0.125, 0.25] # a list of translation factors

  def __len__(self):
    # Return the length of the dataset
    return len(self.dataset)

  def __getitem__(self, index):
    # Input: index is an integer between 0 and len(self.dataset) - 1
    # Output: a tuple of (image, label) where image is a tensor of shape (3, height, width) and label is an integer

    # Get the original image and label from the dataset
    image, label = self.dataset[index]

    # Convert the image to a tensor and normalize it
    image = torchvision.transforms.ToTensor()(image)
    image = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)

    # If the subset is test-scaled, apply a random scale and translation to the image
    if self.subset_name == "test-scaled":
      # Choose a random scale factor from the list
      scale_factor = random.choice(self.scale_factors)

      # Resize the image by the scale factor
      image = F.interpolate(image.unsqueeze(0), scale_factor=scale_factor, mode="bilinear", align_corners=False).squeeze(0)

      # Choose a random translation factor from the list
      translation_factor = random.choice(self.translation_factors)

      # Translate the image by the translation factor
      image = torchvision.transforms.functional.affine(image=image.unsqueeze(0), angle=0, translate=(int(translation_factor * image.shape[1]), int(translation_factor * image.shape[2])), scale=1, shear=0).squeeze(0)

      # Pad or crop the image to match the original size
      if scale_factor < 1:
        # Pad the image with

```