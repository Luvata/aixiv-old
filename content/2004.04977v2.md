---
title: 2004.04977v2 SESAME  Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects
date: 2020-04-05
---

# [SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects](http://arxiv.org/abs/2004.04977v2)

authors: Evangelos Ntavelis, Andr√©s Romero, Iason Kastanis, Luc Van Gool, Radu Timofte


## What, Why and How

[1]: https://arxiv.org/abs/2004.04977v2 "[2004.04977v2] SESAME: Semantic Editing of Scenes by Adding ... - arXiv.org"
[2]: https://arxiv.org/pdf/2004.04977v2.pdf "Robotics and Machine Learning, CSEM SA, Switzerland arXiv:2004.04977v2 ..."
[3]: http://export.arxiv.org/abs/1603.04977v2 "[1603.04977v2] On the Sign Changes of a Weighted Divisor Problem"

Here is a summary of the paper:

- **What**: The paper proposes SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects[^1^][1].
- **Why**: The paper aims to address the limitations of existing approaches for semantic image editing, which can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts[^1^][1].
- **How**: The paper introduces a new discriminator architecture that is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. The paper evaluates the model on a diverse set of datasets and reports state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2004.04977v2.pdf "Robotics and Machine Learning, CSEM SA, Switzerland arXiv:2004.04977v2 ..."
[2]: https://arxiv.org/abs/2004.04977v2 "[2004.04977v2] SESAME: Semantic Editing of Scenes by Adding ... - arXiv.org"
[3]: http://export.arxiv.org/abs/1603.04977v2 "[1603.04977v2] On the Sign Changes of a Weighted Divisor Problem"

The paper claims the following contributions:

- **A novel generator-discriminator pair for semantic image editing** that can handle the complete set of editing operations, that is addition, manipulation or removal of semantic concepts[^1^][1].
- **A new discriminator architecture** that is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former[^1^][1].
- **State-of-the-art performance on two tasks**: (a) image manipulation and (b) image generation conditioned on semantic labels, evaluated on a diverse set of datasets[^1^][1].

## Method Summary

[1]: https://arxiv.org/pdf/2004.04977v2.pdf "Robotics and Machine Learning, CSEM SA, Switzerland arXiv:2004.04977v2 ..."
[2]: https://arxiv.org/abs/2004.04977v2 "[2004.04977v2] SESAME: Semantic Editing of Scenes by Adding ... - arXiv.org"
[3]: http://export.arxiv.org/abs/1603.04977v2 "[1603.04977v2] On the Sign Changes of a Weighted Divisor Problem"

Here is a summary of the method section of the paper:

- The paper introduces a novel generator-discriminator pair for semantic image editing, called SESAME, which stands for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects[^1^][1].
- The generator is based on a U-Net architecture [32] with skip connections and residual blocks [13]. It takes as input an RGB image and a semantic map that indicates the areas to be edited. It outputs a modified image that reflects the desired changes[^1^][1].
- The discriminator is composed of two input streams: one for the image and one for the semantics. The image stream consists of several convolutional layers that extract features from the image. The semantic stream consists of a single convolutional layer that maps the semantic map to a feature map with the same spatial resolution as the image stream. The two streams are then combined by element-wise multiplication, which allows the discriminator to focus on the regions that are relevant for the editing task. The final output of the discriminator is a scalar value that indicates whether the input image is real or fake[^1^][1].
- The paper uses a hinge loss [28] for both the generator and the discriminator, as well as a feature matching loss [37] and a perceptual loss [16] for the generator. The paper also employs a multi-scale discriminator [19] to capture features at different resolutions[^1^][1].
- The paper trains SESAME on four datasets: Cityscapes [6], ADE20K [48], Facades [27], and CelebAMask-HQ [18]. The paper uses data augmentation techniques such as random cropping, flipping, and color jittering[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator and discriminator networks
generator = UNet(input_channels=3+num_classes, output_channels=3)
discriminator = DualStreamDiscriminator(input_channels=3+num_classes)

# Define the loss functions
hinge_loss = HingeLoss()
feature_matching_loss = FeatureMatchingLoss()
perceptual_loss = PerceptualLoss()

# Define the optimizers
optimizer_G = Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

# Loop over the epochs
for epoch in range(num_epochs):

  # Loop over the batches
  for batch in dataloader:

    # Get the real images and semantic maps from the batch
    real_images = batch["image"]
    semantic_maps = batch["semantic"]

    # Generate fake images using the generator
    fake_images = generator(real_images, semantic_maps)

    # Compute the discriminator outputs for real and fake images
    real_outputs = discriminator(real_images, semantic_maps)
    fake_outputs = discriminator(fake_images.detach(), semantic_maps)

    # Compute the discriminator loss
    loss_D = hinge_loss(real_outputs, fake_outputs)

    # Update the discriminator parameters
    optimizer_D.zero_grad()
    loss_D.backward()
    optimizer_D.step()

    # Compute the generator outputs for fake images
    fake_outputs = discriminator(fake_images, semantic_maps)

    # Compute the generator loss
    loss_G = hinge_loss(fake_outputs) + feature_matching_loss(real_outputs, fake_outputs) + perceptual_loss(real_images, fake_images)

    # Update the generator parameters
    optimizer_G.zero_grad()
    loss_G.backward()
    optimizer_G.step()

  # Save the model and generate some samples
  save_model(generator, discriminator, epoch)
  generate_samples(generator, epoch)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import random

# Define some hyperparameters
num_epochs = 200 # number of training epochs
batch_size = 16 # size of mini-batches
num_classes = 150 # number of semantic classes
image_size = 256 # size of input images
lambda_fm = 10 # weight for feature matching loss
lambda_p = 10 # weight for perceptual loss

# Define the U-Net generator network
class UNet(nn.Module):

  def __init__(self, input_channels, output_channels):
    super(UNet, self).__init__()

    # Define the encoder blocks
    self.enc1 = self.encoder_block(input_channels, 64, kernel_size=4, stride=2, padding=1)
    self.enc2 = self.encoder_block(64, 128, kernel_size=4, stride=2, padding=1)
    self.enc3 = self.encoder_block(128, 256, kernel_size=4, stride=2, padding=1)
    self.enc4 = self.encoder_block(256, 512, kernel_size=4, stride=2, padding=1)
    self.enc5 = self.encoder_block(512, 512, kernel_size=4, stride=2, padding=1)
    self.enc6 = self.encoder_block(512, 512, kernel_size=4, stride=2, padding=1)
    self.enc7 = self.encoder_block(512, 512, kernel_size=4, stride=2, padding=1)
    self.enc8 = self.encoder_block(512, 512, kernel_size=4, stride=2, padding=1)

    # Define the decoder blocks
    self.dec1 = self.decoder_block(512, 512, kernel_size=4, stride=2, padding=1)
    self.dec2 = self.decoder_block(1024, 512, kernel_size=4, stride=2, padding=1)
    self.dec3 = self.decoder_block(1024, 512, kernel_size=4, stride=2, padding=1)
    self.dec4 = self.decoder_block(1024, 512, kernel_size=4, stride=2, padding=1)
    self.dec5 = self.decoder_block(1024, 256, kernel_size=4, stride=2, padding=1)
    self.dec6 = self.decoder_block(512 ,128 ,kernel_size=4 ,stride=2 ,padding=1)
    self.dec7 = self.decoder_block(256 ,64 ,kernel_size=4 ,stride=2 ,padding=1)
    self.dec8 = nn.Sequential(
      nn.ConvTranspose2d(128 ,output_channels ,kernel_size=4 ,stride=2 ,padding=1),
      nn.Tanh()
    )

    # Define the residual blocks
    resblock_list = []
    for _ in range(9):
      resblock_list.append(self.residual_block(512))
    self.resblocks = nn.Sequential(*resblock_list)

  def encoder_block(self ,in_channels ,out_channels ,kernel_size ,stride ,padding):
    # Define a convolutional block with leaky ReLU activation and instance normalization
    block = nn.Sequential(
      nn.Conv2d(in_channels ,out_channels ,kernel_size ,stride ,padding),
      nn.LeakyReLU(0.2),
      nn.InstanceNorm2d(out_channels)
    )
    return block

  def decoder_block(self ,in_channels ,out_channels ,kernel_size ,stride ,padding):
    # Define a transposed convolutional block with ReLU activation and instance normalization
    block = nn.Sequential(
      nn.ConvTranspose2d(in_channels ,out_channels ,kernel_size ,stride ,padding),
      nn.ReLU(),
      nn.InstanceNorm2d(out_channels),
      nn.Dropout(0.5)
    )
    return block

  def residual_block(self ,channels):
    # Define a residual block with two convolutional layers and a skip connection
    block = nn.Sequential(
      nn.ReflectionPad2d(1),
      nn.Conv2d(channels ,channels ,kernel_size=3),
      nn.InstanceNorm2d(channels),
      nn.ReLU(),
      nn.ReflectionPad2d(1),
      nn.Conv2d(channels ,channels ,kernel_size=3),
      nn.InstanceNorm2d(channels)
    )
    return block

  def forward(self ,x):
    # Forward pass of the generator network
    # x: input tensor of shape (batch_size ,input_channels ,image_size ,image_size)

    # Encode the input
    e1 = self.enc1(x) # shape: (batch_size ,64 ,image_size/2 ,image_size/2)
    e2 = self.enc2(e1) # shape: (batch_size ,128 ,image_size/4 ,image_size/4)
    e3 = self.enc3(e2) # shape: (batch_size ,256 ,image_size/8 ,image_size/8)
    e4 = self.enc4(e3) # shape: (batch_size ,512 ,image_size/16 ,image_size/16)
    e5 = self.enc5(e4) # shape: (batch_size ,512 ,image_size/32 ,image_size/32)
    e6 = self.enc6(e5) # shape: (batch_size ,512 ,image_size/64 ,image_size/64)
    e7 = self.enc7(e6) # shape: (batch_size ,512 ,image_size/128 ,image_size/128)
    e8 = self.enc8(e7) # shape: (batch_size ,512 ,image_size/256 ,image_size/256)

    # Apply the residual blocks
    r = self.resblocks(e8) # shape: (batch_size ,512 ,image_size/256 ,image_size/256)

    # Decode the output
    d1 = self.dec1(r) # shape: (batch_size ,512 ,image_size/128 ,image_size/128)
    d1 = torch.cat([d1, e7], dim=1) # skip connection, shape: (batch_size, 1024, image_size/128, image_size/128)
    d2 = self.dec2(d1) # shape: (batch_size, 512, image_size/64, image_size/64)
    d2 = torch.cat([d2, e6], dim=1) # skip connection, shape: (batch_size, 1024, image_size/64, image_size/64)
    d3 = self.dec3(d2) # shape: (batch_size, 512, image_size/32, image_size/32)
    d3 = torch.cat([d3, e5], dim=1) # skip connection, shape: (batch_size, 1024, image_size/32, image_size/32)
    d4 = self.dec4(d3) # shape: (batch_size, 512, image_size/16, image_size/16)
    d4 = torch.cat([d4, e4], dim=1) # skip connection, shape: (batch_size, 1024, image_size/16, image_size/16)
    d5 = self.dec5(d4) # shape: (batch_size, 256, image_size/8, image_size/8)
    d5 = torch.cat([d5, e3], dim=1) # skip connection, shape: (batch_size, 512, image_siz
e /8,image_siz
e /8)
d6=self.dec6(d5)#shape:(batch_siz
e
,
128
,
image_siz
e /4,image_siz
e /4)
d6=torch.cat([d6,e2],dim=1)#skip connection,shape:(batch_siz
e
,
256
,
image_siz
e /4,image_siz
e /4)
d7=self.dec7(d6)#shape:(batch_siz
e
,
64
,
image_siz
e /2,image_siz
e /2)
d7=torch.cat([d7,e1],dim=1)#skip connection,shape:(batch_siz
e
,
128
,
image_siz
e /2,image_siz
e /2)
d8=self.dec8(d7)#shape:(batch_siz
e
,
output_channels
,
image_siz
e
,
image_siz
e)

#Return the generated image
return d8

#Define the dual-stream discriminator network

class DualStreamDiscriminator(nn.Module):

def __init__(self,input_channels):
super(DualStreamDiscriminator,self).__init__()

#Define the image stream blocks

self.img_block1=self.discriminator_block(input_channels//2,kernel_siz

e

=
4,strid

e

=
2,padding=1,bias=False,norm=False)#no bias and normalization for the first layer

self.img_block2=self.discriminator_block(64,kernel_siz

e

=
4,strid

e

=
2,padding=1,bias=False,norm=True)

self.img_block3=self.discriminator_block(128,kernel_siz

e

=
4,strid

e

=
2,padding=1,bias=False,norm=True