---
title: 2106.00178v3 Language-Driven Image Style Transfer
date: 2021-06-01
---

# [Language-Driven Image Style Transfer](http://arxiv.org/abs/2106.00178v3)

authors: Tsu-Jui Fu, Xin Eric Wang, William Yang Wang


## What, Why and How

[1]: https://arxiv.org/abs/2106.00178 "[2106.00178] Language-Driven Image Style Transfer - arXiv.org"
[2]: http://export.arxiv.org/abs/2106.00178 "[2106.00178] Language-Driven Image Style Transfer"
[3]: https://arxiv.org/pdf/2106.00178v3.pdf "arXiv.org"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper introduces a new task, language-driven artistic style transfer (LDAST), to manipulate the style of a content image, guided by a text. It also proposes a model, contrastive language visual artist (CLVA), that learns to extract visual semantics from style instructions and accomplish LDAST by the patch-wise style discriminator.
- **Why**: The paper aims to overcome the limitations of existing style transfer methods, which require preparing style images in advance and may result in lack of creativity and accessibility. The paper also argues that following human instruction is the most natural way to perform artistic style transfer that can significantly improve controllability for visual effect applications.
- **How**: The paper designs CLVA as a two-stage model that consists of a language-driven style transfer network and a patch-wise style discriminator. The former takes a content image and a style instruction as inputs and generates a transferred image that preserves the content structure and reflects the style semantics. The latter considers the correlation between language and patches of style images or transferred results to jointly embed style instructions. CLVA further compares contrastive pairs of content images and style instructions to improve the mutual relativeness. The paper evaluates CLVA on a newly collected dataset of content images and style instructions and shows that it outperforms several baselines on both quantitative and qualitative metrics.

## Main Contributions

According to the paper, the main contributions are:

- They introduce a new task, language-driven artistic style transfer (LDAST), to manipulate the style of a content image, guided by a text.
- They propose a novel model, contrastive language visual artist (CLVA), that learns to extract visual semantics from style instructions and accomplish LDAST by the patch-wise style discriminator.
- They collect a new dataset of content images and style instructions for LDAST and release it for future research.
- They conduct extensive experiments and ablation studies to demonstrate the effectiveness and superiority of CLVA over several baselines on LDAST.

## Method Summary

The method section of the paper describes the proposed model, contrastive language visual artist (CLVA), in detail. It consists of two main components: a language-driven style transfer network and a patch-wise style discriminator. The former is composed of a content encoder, a style encoder, and a decoder. The content encoder extracts features from the content image, the style encoder encodes the style instruction into a latent vector, and the decoder generates the transferred image by combining the content features and the style vector. The latter is a convolutional neural network that takes patches of style images or transferred results and style instructions as inputs and outputs a score indicating how well they match. The paper also introduces a contrastive learning objective that encourages CLVA to generate transferred images that are consistent with the style instructions and distinguishable from other irrelevant ones. The paper provides the architecture details, the loss functions, and the training procedure of CLVA in this section.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the content encoder, style encoder, decoder, and discriminator
content_encoder = ContentEncoder()
style_encoder = StyleEncoder()
decoder = Decoder()
discriminator = Discriminator()

# Define the loss functions
content_loss = ContentLoss()
style_loss = StyleLoss()
adversarial_loss = AdversarialLoss()
contrastive_loss = ContrastiveLoss()

# Define the optimizer
optimizer = Optimizer()

# Define the dataset of content images and style instructions
dataset = Dataset()

# Train the model
for epoch in range(num_epochs):
  for batch in dataset:
    # Get the content images, style instructions, and style images
    content_images = batch["content_images"]
    style_instructions = batch["style_instructions"]
    style_images = batch["style_images"]

    # Encode the content images and style instructions
    content_features = content_encoder(content_images)
    style_vectors = style_encoder(style_instructions)

    # Generate the transferred images
    transferred_images = decoder(content_features, style_vectors)

    # Compute the content loss and style loss
    L_content = content_loss(content_features, transferred_images)
    L_style = style_loss(style_vectors, transferred_images)

    # Compute the adversarial loss
    L_adv = adversarial_loss(discriminator(transferred_images, style_instructions))

    # Compute the contrastive loss
    L_con = contrastive_loss(discriminator(transferred_images, style_instructions), discriminator(style_images, style_instructions))

    # Compute the total loss
    L_total = L_content + L_style + L_adv + L_con

    # Update the model parameters
    optimizer.zero_grad()
    L_total.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import nltk
import numpy as np

# Define the hyperparameters
batch_size = 16 # The batch size for training and testing
num_epochs = 100 # The number of epochs for training
learning_rate = 0.0001 # The learning rate for the optimizer
beta1 = 0.5 # The beta1 parameter for the Adam optimizer
beta2 = 0.999 # The beta2 parameter for the Adam optimizer
lambda_content = 1.0 # The weight for the content loss
lambda_style = 10.0 # The weight for the style loss
lambda_adv = 1.0 # The weight for the adversarial loss
lambda_con = 0.1 # The weight for the contrastive loss
image_size = 256 # The size of the input and output images
style_dim = 128 # The dimension of the style vector
patch_size = 32 # The size of the patches for the discriminator

# Define the content encoder network
class ContentEncoder(nn.Module):
  def __init__(self):
    super(ContentEncoder, self).__init__()
    # Define the convolutional layers with batch normalization and ReLU activation
    self.conv1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3), nn.BatchNorm2d(64), nn.ReLU())
    self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU())
    self.conv3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(256), nn.ReLU())
    self.conv4 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(512), nn.ReLU())
  
  def forward(self, x):
    # Encode the content image into a feature map
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    return x

# Define the style encoder network
class StyleEncoder(nn.Module):
  def __init__(self):
    super(StyleEncoder, self).__init__()
    # Define the embedding layer to convert words to vectors
    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=style_dim)
    # Define the LSTM layer to encode the style instruction into a vector
    self.lstm = nn.LSTM(input_size=style_dim, hidden_size=style_dim, num_layers=1, batch_first=True)
  
  def forward(self, x):
    # Encode the style instruction into a vector
    x = self.embedding(x) # x is a tensor of shape (batch_size, max_length, style_dim)
    _, (h_n, _) = self.lstm(x) # h_n is a tensor of shape (1, batch_size, style_dim)
    h_n = h_n.squeeze(0) # h_n is a tensor of shape (batch_size, style_dim)
    return h_n

# Define the decoder network
class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    # Define the adaptive instance normalization layer to combine content features and style vector
    self.adain = AdaIN()
    # Define the deconvolutional layers with batch normalization and ReLU activation
    self.deconv1 = nn.Sequential(nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(256), nn.ReLU())
    self.deconv2 = nn.Sequential(nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(128), nn.ReLU())
    self.deconv3 = nn.Sequential(nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(64), nn.ReLU())
    self.deconv4 = nn.Sequential(nn.ConvTranspose2d(64, 3, kernel_size=7, stride=1, padding=3), nn.Tanh())
  
  def forward(self, x_c, x_s):
    # Decode the content features and style vector into a transferred image
    x = self.adain(x_c, x_s) # x is a tensor of shape (batch_size, 512, image_size/8, image_size/8)
    x = self.deconv1(x) # x is a tensor of shape (batch_size, 256, image_size/4, image_size/4)
    x = self.deconv2(x) # x is a tensor of shape (batch_size, 128, image_size/2, image_size/2)
    x = self.deconv3(x) # x is a tensor of shape (batch_size, 64, image_size, image_size)
    x = self.deconv4(x) # x is a tensor of shape (batch_size, 3, image_size, image_size)
    return x

# Define the adaptive instance normalization layer
class AdaIN(nn.Module):
  def __init__(self):
    super(AdaIN, self).__init__()
  
  def forward(self, x_c, x_s):
    # Compute the mean and standard deviation of the content features and style vector
    mean_c = torch.mean(x_c, dim=(2, 3), keepdim=True) # mean_c is a tensor of shape (batch_size, 512, 1, 1)
    std_c = torch.std(x_c, dim=(2, 3), keepdim=True) # std_c is a tensor of shape (batch_size, 512, 1, 1)
    mean_s = torch.mean(x_s, dim=1, keepdim=True) # mean_s is a tensor of shape (batch_size, 1)
    std_s = torch.std(x_s, dim=1, keepdim=True) # std_s is a tensor of shape (batch_size, 1)

    # Normalize the content features and scale them with the style vector
    x_c = (x_c - mean_c) / std_c # x_c is a tensor of shape (batch_size, 512, image_size/8, image_size/8)
    x_s = x_s.unsqueeze(2).unsqueeze(3) # x_s is a tensor of shape (batch_size, style_dim, 1, 1)
    x_s = (x_s - mean_s) / std_s # x_s is a tensor of shape (batch_size, style_dim, 1, 1)
    x = x_c * x_s # x is a tensor of shape (batch_size, style_dim, image_size/8, image_size/8)

    return x

# Define the discriminator network
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Define the convolutional layers with batch normalization and LeakyReLU activation
    self.conv1 = nn.Sequential(nn.Conv2d(3 + style_dim + patch_size * patch_size * 3 + patch_size * patch_size * style_dim , 64 , kernel_size=4 , stride=2 , padding=1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2))
    self.conv2 = nn.Sequential(nn.Conv2d(64 , 128 , kernel_size=4 , stride=2 , padding=1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2))
    self.conv3 = nn.Sequential(nn.Conv2d(128 , 256 , kernel_size=4 , stride=2 , padding=1), nn.BatchNorm2d(256), nn.LeakyReLU(0.2))
    self.conv4 = nn.Sequential(nn.Conv2d(256 , 512 , kernel_size=4 , stride=2 , padding=1), nn.BatchNorm2d(512), nn.LeakyReLU(0.2))
    self.conv5 = nn.Sequential(nn.Conv2d(512 , 1 , kernel_size=4 , stride=1 , padding=0), nn.Sigmoid())
  
  def forward(self, x_i, x_t):
    # Extract patches from the input image
    patches_i = extract_patches(x_i) # patches_i is a tensor of shape (batch_size * patch_num * patch_num , patch_size * patch_size * 3)

    # Encode the style instruction into a vector
    vector_t = style_encoder(x_t) # vector_t is a tensor of shape (batch_size , style_dim)

    # Repeat the style vector for each patch
    vector_t = vector_t.unsqueeze(1).unsqueeze(1).repeat(1 , patch_num * patch_num , patch_num * patch_num).view(-1 , style_dim) # vector_t is a tensor of shape (batch_size * patch_num * patch_num , style_dim)

    # Concatenate the input image patches and the style vector along the channel dimension
    input = torch.cat([patches_i , vector_t] , dim=1) # input is