---
title: 2206.02338v2 OrdinalCLIP  Learning Rank Prompts for Language-Guided Ordinal Regression
date: 2022-06-03
---

# [OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression](http://arxiv.org/abs/2206.02338v2)

authors: Wanhua Li, Xiaoke Huang, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, Jiwen Lu


## What, Why and How

[1]: https://arxiv.org/abs/2206.02338 "[2206.02338] OrdinalCLIP: Learning Rank Prompts for Language-Guided ..."
[2]: https://arxiv.org/pdf/2206.02338 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2206.02338v2 "[2206.02338v2] OrdinalCLIP: Learning Rank Prompts for Language-Guided ..."

Here is a summary of the paper:

- **What**: The paper proposes a language-powered paradigm for ordinal regression, which is a task of predicting a rank number or a continuous value for a given data. The paper introduces OrdinalCLIP, a method that leverages the pre-trained vision-language model CLIP to learn the rank concepts from natural language and adapts it for ordinal regression with a differentiable prompting mechanism.
- **Why**: The paper aims to address the limitations of existing methods for ordinal regression, which usually treat each rank as a category and employ a set of weights to learn these concepts. These methods are prone to overfitting and unsatisfactory performance as the learned concepts are mainly derived from the training set. The paper argues that natural language can provide rich semantic priors for learning the rank concepts, and that CLIP can offer a powerful latent space for image-language matching.
- **How**: The paper reformulates ordinal regression as an image-language matching problem with a contrastive objective, which regards labels as text and obtains a language prototype from a text encoder for each rank. The paper then proposes OrdinalCLIP, which consists of learnable context tokens and learnable rank embeddings. The context tokens are used to generate natural language prompts for each rank, and the rank embeddings are constructed by explicitly modeling numerical continuity, resulting in well-ordered, compact language prototypes in the CLIP space. The paper shows that OrdinalCLIP can achieve competitive performance in general ordinal regression tasks, and gains improvements in few-shot and distribution shift settings for age estimation. The paper also demonstrates that OrdinalCLIP can only save the language prototypes and discard the huge language model, resulting in zero additional computational overhead compared with the linear head counterpart.

The summary is based on the abstract[^1^][1] and Figure 1[^2^][2] of the paper.

## Main Contributions

[1]: https://arxiv.org/abs/2206.02338 "[2206.02338] OrdinalCLIP: Learning Rank Prompts for Language-Guided ..."
[2]: https://arxiv.org/pdf/2206.02338 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2206.02338v2 "[2206.02338v2] OrdinalCLIP: Learning Rank Prompts for Language-Guided ..."

According to the paper[^1^][1], the main contributions are:

- The paper proposes a **language-powered paradigm** for ordinal regression, which leverages the rich semantic CLIP latent space to learn the rank concepts from natural language.
- The paper introduces **OrdinalCLIP**, a **differentiable prompting method** for adapting CLIP for ordinal regression, which consists of learnable context tokens and learnable rank embeddings.
- The paper shows that OrdinalCLIP can achieve **competitive performance** in general ordinal regression tasks, and gains **improvements** in few-shot and distribution shift settings for age estimation.
- The paper demonstrates that OrdinalCLIP can **only save the language prototypes** and discard the huge language model, resulting in **zero additional computational overhead** compared with the linear head counterpart.

## Method Summary

[1]: https://arxiv.org/abs/2206.02338 "[2206.02338] OrdinalCLIP: Learning Rank Prompts for Language-Guided ..."
[2]: https://arxiv.org/pdf/2206.02338 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2206.02338v2 "[2206.02338v2] OrdinalCLIP: Learning Rank Prompts for Language-Guided ..."

Here is a summary of the method section of the paper:

- The paper first introduces the **preliminaries** of ordinal regression and CLIP, which are the background knowledge for the proposed method.
- The paper then presents the **language-powered paradigm** for ordinal regression, which reformulates the task as an image-language matching problem with a contrastive objective. The paper explains how to obtain a language prototype for each rank by treating labels as text and using a text encoder from CLIP. The paper also describes how to compute the similarity score between an image and a language prototype, and how to optimize the model with a contrastive loss function.
- The paper then proposes **OrdinalCLIP**, which is a **differentiable prompting method** for adapting CLIP for ordinal regression. The paper introduces two components of OrdinalCLIP: learnable context tokens and learnable rank embeddings. The paper explains how to generate natural language prompts for each rank by concatenating the context tokens and the rank embeddings, and how to construct the rank embeddings by explicitly modeling numerical continuity. The paper also shows how to learn OrdinalCLIP with gradient descent, and how to only save the language prototypes and discard the huge language model after training.
- The paper finally provides some **implementation details** of OrdinalCLIP, such as the choice of text encoder, image encoder, context tokens, rank embeddings, contrastive loss function, optimizer, learning rate, batch size, etc.

The summary is based on Section 3[^1^][2] of the paper.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the text encoder and image encoder from CLIP
text_encoder = CLIPTextEncoder()
image_encoder = CLIPImageEncoder()

# Define the learnable context tokens and rank embeddings
context_tokens = LearnableTokens(num_tokens, token_dim)
rank_embeddings = LearnableRankEmbeddings(num_ranks, rank_dim)

# Define the contrastive loss function
contrastive_loss = InfoNCELoss(temperature)

# Define the optimizer
optimizer = Adam(learning_rate)

# Loop over the training data
for image, label in train_data:

  # Generate the natural language prompt for the label
  prompt = concatenate(context_tokens, rank_embeddings[label])

  # Obtain the language prototype from the text encoder
  prototype = text_encoder(prompt)

  # Obtain the image feature from the image encoder
  feature = image_encoder(image)

  # Compute the similarity score between the image and the prototype
  score = dot_product(feature, prototype) / norm(feature) / norm(prototype)

  # Compute the contrastive loss with respect to other prototypes
  loss = contrastive_loss(score, prototypes)

  # Update the model parameters with gradient descent
  optimizer.step(loss)

# Save the language prototypes and discard the text encoder
language_prototypes = text_encoder(concatenate(context_tokens, rank_embeddings))
save(language_prototypes)
delete(text_encoder)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define some hyperparameters
num_tokens = 4 # the number of context tokens
token_dim = 512 # the dimension of context tokens
num_ranks = 100 # the number of rank categories
rank_dim = 512 # the dimension of rank embeddings
temperature = 0.07 # the temperature for InfoNCE loss
learning_rate = 3e-4 # the learning rate for Adam optimizer
batch_size = 256 # the batch size for training
num_epochs = 100 # the number of epochs for training

# Load the pre-trained CLIP model
clip_model, preprocess = clip.load("ViT-B/32", device="cuda")

# Extract the text encoder and image encoder from CLIP model
text_encoder = clip_model.text_projection.to("cuda")
image_encoder = clip_model.visual.to("cuda")

# Freeze the text encoder and image encoder parameters
for param in text_encoder.parameters():
  param.requires_grad = False

for param in image_encoder.parameters():
  param.requires_grad = False

# Define the learnable context tokens and rank embeddings
context_tokens = torch.nn.Embedding(num_tokens, token_dim).to("cuda")
rank_embeddings = torch.nn.Embedding(num_ranks, rank_dim).to("cuda")

# Initialize the context tokens and rank embeddings with normal distribution
torch.nn.init.normal_(context_tokens.weight, std=0.02)
torch.nn.init.normal_(rank_embeddings.weight, std=0.02)

# Define the contrastive loss function as InfoNCE loss
def contrastive_loss(score, prototypes):
  # score: a scalar tensor representing the similarity score between an image and a prototype
  # prototypes: a tensor of shape (num_ranks, token_dim) representing the language prototypes for all ranks

  # Compute the logits by multiplying the score with the temperature
  logits = score / temperature

  # Compute the labels by finding the index of the maximum score among all prototypes
  labels = torch.argmax(torch.matmul(prototypes, score.unsqueeze(-1)), dim=0)

  # Compute the cross entropy loss between the logits and the labels
  loss = torch.nn.functional.cross_entropy(logits.unsqueeze(0), labels.unsqueeze(0))

  return loss

# Define the optimizer as Adam optimizer
optimizer = torch.optim.Adam([context_tokens.weight, rank_embeddings.weight], lr=learning_rate)

# Load the training data as a PyTorch dataset
train_data = torchvision.datasets.ImageFolder("path/to/train_data", transform=preprocess)

# Create a data loader for the training data
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)

# Loop over the number of epochs
for epoch in range(num_epochs):

  # Loop over the batches of training data
  for batch in train_loader:

    # Extract the images and labels from the batch
    images, labels = batch

    # Move the images and labels to GPU
    images = images.to("cuda")
    labels = labels.to("cuda")

    # Generate the natural language prompts for the labels by concatenating the context tokens and the rank embeddings
    prompts = torch.cat([context_tokens.weight.repeat(num_ranks, 1), rank_embeddings.weight], dim=1)

    # Obtain the language prototypes from the text encoder by encoding the prompts
    prototypes = text_encoder(prompts)

    # Obtain the image features from the image encoder by encoding the images
    features = image_encoder(images)

    # Compute the similarity scores between each image and each prototype by dot product and normalization
    scores = torch.matmul(features, prototypes.t()) / features.norm(dim=-1)[:, None] / prototypes.norm(dim=-1)[None, :]

    # Compute the contrastive loss for each image with respect to all prototypes
    losses = contrastive_loss(scores, prototypes)

    # Compute the average loss over the batch
    loss = losses.mean()

    # Print the loss value every 100 steps
    if step % 100 == 0:
      print(f"Epoch {epoch}, Step {step}, Loss {loss.item()}")

    # Zero out the gradients of the optimizer
    optimizer.zero_grad()

    # Backpropagate the loss and update the model parameters
    loss.backward()
    optimizer.step()

# Save the language prototypes and discard the text encoder
language_prototypes = text_encoder(torch.cat([context_tokens.weight.repeat(num_ranks, 1), rank_embeddings.weight], dim=1))
torch.save(language_prototypes, "language_prototypes.pth")
del text_encoder

```