---
title: 2112.02300v2 Unsupervised Domain Generalization by Learning a Bridge Across Domains
date: 2021-12-03
---

# [Unsupervised Domain Generalization by Learning a Bridge Across Domains](http://arxiv.org/abs/2112.02300v2)

authors: Sivan Harary, Eli Schwartz, Assaf Arbelle, Peter Staar, Shady Abu-Hussein, Elad Amrani, Roei Herzig, Amit Alfassy, Raja Giryes, Hilde Kuehne, Dina Katabi, Kate Saenko, Rogerio Feris, Leonid Karlinsky


## What, Why and How

[1]: https://arxiv.org/abs/2112.02300 "[2112.02300] Unsupervised Domain Generalization by Learning a Bridge ..."
[2]: https://arxiv.org/pdf/2112.02300v2.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2302.02300v2 "[2302.02300v2] Run-Off Election: Improved Provable Defense against Data ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new approach for Unsupervised Domain Generalization (UDG), which is the problem of learning a model that can generalize to unseen domains without any supervision in the source or target domains.
- **Why**: The paper argues that UDG is a very practical and challenging problem that requires the model to learn domain-invariant representations that can handle significant visual variations across domains, such as between real photos, clipart, paintings, and sketches.
- **How**: The paper introduces a novel method that learns a Bridge Across Domains (BrAD) - an auxiliary domain that is semantically aligned with each of the training domains through image-to-image mappings. The paper also learns a contrastive self-supervised representation model that uses the BrAD-projections of the images to enforce semantic consistency across domains. The paper shows that this method achieves state-of-the-art results on several UDG benchmarks and tasks.

## Main Contributions

[1]: https://arxiv.org/abs/2112.02300 "[2112.02300] Unsupervised Domain Generalization by Learning a Bridge ..."
[2]: https://arxiv.org/pdf/2112.02300v2.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2302.02300v2 "[2302.02300v2] Run-Off Election: Improved Provable Defense against Data ..."

According to the paper[^1^][1], the main contributions are:

- **A novel method for UDG** that learns a Bridge Across Domains (BrAD) and a contrastive self-supervised representation model that uses the BrAD-projections of the images to enforce semantic consistency across domains.
- **An edge regularization technique** that improves the quality of the BrAD and the mappings to it by preserving the edge information of the images.
- **Extensive experiments** on several UDG benchmarks and tasks, such as Office-Home, PACS, DomainNet, VLCS, and Few-shot UDA, showing that the proposed method outperforms existing methods by a large margin and achieves state-of-the-art results.

## Method Summary

[1]: https://arxiv.org/abs/2112.02300 "[2112.02300] Unsupervised Domain Generalization by Learning a Bridge ..."
[2]: https://arxiv.org/pdf/2112.02300v2.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2302.02300v2 "[2302.02300v2] Run-Off Election: Improved Provable Defense against Data ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper presents a framework for learning a Bridge Across Domains (BrAD) and a contrastive self-supervised representation model that uses the BrAD-projections of the images to enforce semantic consistency across domains.
- The paper defines the BrAD as an auxiliary domain that is learned from scratch by a generative adversarial network (GAN) that consists of a generator and a discriminator. The generator takes as input a random noise vector and produces an image in the BrAD domain. The discriminator tries to distinguish between real and fake BrAD images.
- The paper also learns a set of image-to-image mappings from each of the training domains to the BrAD domain. These mappings are also implemented by GANs that consist of encoders and decoders. The encoders take as input an image from a source domain and produce a latent code that is fed to the generator of the BrAD GAN. The decoders take as input the output of the generator and try to reconstruct the original image in the source domain. The decoders are trained with reconstruction loss and adversarial loss.
- The paper further learns a contrastive self-supervised representation model that uses the BrAD-projections of the images to enforce semantic consistency across domains. The representation model consists of an encoder and a projection head. The encoder takes as input an image from any domain and produces a feature vector that is fed to the projection head. The projection head maps the feature vector to a unit sphere in a lower-dimensional space. The representation model is trained with contrastive loss that maximizes the similarity between the projections of images that belong to the same class and minimizes the similarity between the projections of images that belong to different classes.
- The paper also introduces an edge regularization technique that improves the quality of the BrAD and the mappings to it by preserving the edge information of the images. The paper applies an edge detector to both the source images and the BrAD images and computes an edge loss that penalizes the difference between them. The paper shows that this technique helps to generate more realistic and diverse BrAD images and improves the performance of the representation model.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Initialize the BrAD GAN, the image-to-image mappings, and the representation model
brad_gan = GAN()
mappings = [GAN() for domain in domains]
representation_model = EncoderProjection()

# Train the models with self-supervised learning
for epoch in epochs:
  # Sample a batch of images from each domain
  images = [sample_batch(domain) for domain in domains]
  
  # Generate BrAD images from random noise vectors
  noise = sample_noise()
  brad_images = brad_gan.generator(noise)
  
  # Compute the BrAD GAN loss and update the generator and discriminator
  brad_gan_loss = compute_gan_loss(brad_images, brad_gan.discriminator)
  update(brad_gan.generator, brad_gan_loss)
  update(brad_gan.discriminator, brad_gan_loss)
  
  # Map the images from each domain to the BrAD domain and reconstruct them
  brad_projections = []
  reconstructions = []
  for i in range(len(domains)):
    latent_code = mappings[i].encoder(images[i])
    brad_projection = brad_gan.generator(latent_code)
    reconstruction = mappings[i].decoder(brad_projection)
    brad_projections.append(brad_projection)
    reconstructions.append(reconstruction)
  
  # Compute the image-to-image mapping loss and update the encoders and decoders
  mapping_loss = 0
  for i in range(len(domains)):
    reconstruction_loss = compute_reconstruction_loss(images[i], reconstructions[i])
    adversarial_loss = compute_adversarial_loss(reconstructions[i], mappings[i].discriminator)
    edge_loss = compute_edge_loss(images[i], brad_projections[i])
    mapping_loss += reconstruction_loss + adversarial_loss + edge_loss
    update(mappings[i].encoder, mapping_loss)
    update(mappings[i].decoder, mapping_loss)
  
  # Encode the images and the BrAD projections with the representation model
  features = []
  projections = []
  for i in range(len(domains)):
    feature = representation_model.encoder(images[i])
    projection = representation_model.projection_head(feature)
    features.append(feature)
    projections.append(projection)
  
  # Compute the contrastive loss and update the encoder and projection head
  contrastive_loss = compute_contrastive_loss(projections, labels)
  update(representation_model.encoder, contrastive_loss)
  update(representation_model.projection_head, contrastive_loss)

# Evaluate the representation model on unseen domains and tasks
evaluate(representation_model, unseen_domains, tasks)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import random

# Define the hyperparameters
batch_size = 64 # The number of images per batch
num_domains = 4 # The number of domains in the training data
num_classes = 65 # The number of classes in the training data
latent_dim = 128 # The dimension of the latent code for the BrAD GAN and the image-to-image mappings
feature_dim = 512 # The dimension of the feature vector for the representation model
projection_dim = 128 # The dimension of the projection vector for the representation model
temperature = 0.1 # The temperature parameter for the contrastive loss
alpha = 0.5 # The weight for the reconstruction loss
beta = 0.5 # The weight for the adversarial loss
gamma = 0.01 # The weight for the edge loss
lr = 0.0002 # The learning rate for the optimizers
beta1 = 0.5 # The beta1 parameter for the Adam optimizers
beta2 = 0.999 # The beta2 parameter for the Adam optimizers

# Define the device to run the models on (CPU or GPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Define the BrAD GAN generator as a convolutional neural network (CNN)
class BrADGenerator(torch.nn.Module):
  def __init__(self):
    super(BrADGenerator, self).__init__()
    # Define the layers of the generator
    self.fc = torch.nn.Linear(latent_dim, 4 * 4 * 1024) # A fully connected layer that maps the latent code to a feature map of size 4 x 4 x 1024
    self.bn1 = torch.nn.BatchNorm1d(4 * 4 * 1024) # A batch normalization layer to stabilize the training
    self.relu1 = torch.nn.ReLU() # A ReLU activation function to introduce non-linearity
    self.deconv1 = torch.nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1) # A transposed convolution layer that upsamples the feature map to size 8 x 8 x 512
    self.bn2 = torch.nn.BatchNorm2d(512) # A batch normalization layer to stabilize the training
    self.relu2 = torch.nn.ReLU() # A ReLU activation function to introduce non-linearity
    self.deconv2 = torch.nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1) # A transposed convolution layer that upsamples the feature map to size 16 x 16 x 256
    self.bn3 = torch.nn.BatchNorm2d(256) # A batch normalization layer to stabilize the training
    self.relu3 = torch.nn.ReLU() # A ReLU activation function to introduce non-linearity
    self.deconv3 = torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1) # A transposed convolution layer that upsamples the feature map to size 32 x 32 x 128
    self.bn4 = torch.nn.BatchNorm2d(128) # A batch normalization layer to stabilize the training
    self.relu4 = torch.nn.ReLU() # A ReLU activation function to introduce non-linearity
    self.deconv4 = torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1) # A transposed convolution layer that upsamples the feature map to size 64 x 64 x 64
    self.bn5 = torch.nn.BatchNorm2d(64) # A batch normalization layer to stabilize the training
    self.relu5 = torch.nn.ReLU() # A ReLU activation function to introduce non-linearity
    self.deconv5 = torch.nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1) # A transposed convolution layer that upsamples the feature map to size 128 x 128 x 3 (the size of a BrAD image)
    self.tanh = torch.nn.Tanh() # A tanh activation function to scale the output to [-1, 1]

  
  def forward(self, x):
    # Define the forward pass of the generator
    x = self.fc(x) # Map the latent code to a feature map of size 4 x 4 x 1024
    x = x.view(-1, 1024, 4, 4) # Reshape the feature map to a 4D tensor
    x = self.bn1(x) # Apply batch normalization
    x = self.relu1(x) # Apply ReLU activation
    x = self.deconv1(x) # Upsample the feature map to size 8 x 8 x 512
    x = self.bn2(x) # Apply batch normalization
    x = self.relu2(x) # Apply ReLU activation
    x = self.deconv2(x) # Upsample the feature map to size 16 x 16 x 256
    x = self.bn3(x) # Apply batch normalization
    x = self.relu3(x) # Apply ReLU activation
    x = self.deconv3(x) # Upsample the feature map to size 32 x 32 x 128
    x = self.bn4(x) # Apply batch normalization
    x = self.relu4(x) # Apply ReLU activation
    x = self.deconv4(x) # Upsample the feature map to size 64 x 64 x 64
    x = self.bn5(x) # Apply batch normalization
    x = self.relu5(x) # Apply ReLU activation
    x = self.deconv5(x) # Upsample the feature map to size 128 x 128 x 3
    x = self.tanh(x) # Apply tanh activation and scale the output to [-1, 1]
    return x

# Define the BrAD GAN discriminator as a convolutional neural network (CNN)
class BrADDiscriminator(torch.nn.Module):
  def __init__(self):
    super(BrADDiscriminator, self).__init__()
    # Define the layers of the discriminator
    self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1) # A convolution layer that downsamples the input image of size 128 x 128 x 3 to a feature map of size 64 x 64 x 64
    self.lrelu1 = torch.nn.LeakyReLU(0.2) # A leaky ReLU activation function with negative slope of 0.2 to introduce non-linearity and avoid sparse gradients
    self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1) # A convolution layer that downsamples the feature map of size 64 x 64 x 64 to a feature map of size 32 x 32 x 128
    self.bn2 = torch.nn.BatchNorm2d(128) # A batch normalization layer to stabilize the training
    self.lrelu2 = torch.nn.LeakyReLU(0.2) # A leaky ReLU activation function with negative slope of 0.2 to introduce non-linearity and avoid sparse gradients
    self.conv3 = torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1) # A convolution layer that downsamples the feature map of size 32 x 32 x 128 to a feature map of size 16 x 16 x 256
    self.bn3 = torch.nn.BatchNorm2d(256) # A batch normalization layer to stabilize the training
    self.lrelu3 = torch.nn.LeakyReLU(0.2) # A leaky ReLU activation function with negative slope of 0.2 to introduce non-linearity and avoid sparse gradients
    self.conv4 = torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1) # A convolution layer that downsamples the feature map of size 16 x 16 x 256 to a feature map of size 8 x 8 x 512
    self.bn4 = torch.nn.BatchNorm2d(512) # A batch normalization layer to stabilize the training
    self.lrelu4 = torch.nn.LeakyReLU(0.2) # A leaky ReLU activation function with negative slope of 0.2 to introduce non-linearity and avoid sparse gradients
    self.conv5 = torch.nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=1) # A convolution layer that downsamples the feature map of size 8 x 8 x 512 to a single value that represents the probability of the input image being real or fake

  
 
 def forward(self, x):
   # Define the forward pass of the discriminator 
   x = self.conv1(x) # Downsample the input image to a feature map of size 
   
```