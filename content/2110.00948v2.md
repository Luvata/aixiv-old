---
title: 2110.00948v2 Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT scans
date: 2021-10-01
---

# [Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT scans](http://arxiv.org/abs/2110.00948v2)

authors: Michelle Xiao-Lin Foo, Seong Tae Kim, Magdalini Paschali, Leili Goli, Egon Burian, Marcus Makowski, Rickmer Braren, Nassir Navab, Thomas Wendler


## What, Why and How

[1]: https://arxiv.org/pdf/2110.00948v2.pdf "Interactive Segmentation for COVID-19 Infection ... - arXiv.org"
[2]: https://arxiv.org/pdf/2110.09482 "Self-Supervised Monocular Depth Estimation with Internal ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/abs/2110.00948 "[2110.00948] Interactive Segmentation for COVID-19 Infection ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new single network model for interactive segmentation of COVID-19 infection on longitudinal CT scans, which uses past information from previous time points and user feedback to refine the segmentation of follow-up scans.
- **Why**: The paper aims to address the challenges of consistent segmentation of COVID-19 patient's CT scans across multiple time points, which is essential for accurate assessment of disease progression and response to therapy. Existing models only use data from a single time point (static) and often produce results that need further editing for clinical use.
- **How**: The paper introduces a novel interactive segmentation model that fully utilizes all available past information to segment the target scan. The model takes 3D volumes of medical images from two-time points (target and reference) as concatenated slices with the additional reference time point segmentation as a guide. In subsequent refinement rounds, user feedback in the form of scribbles that correct the segmentation and the target's previous segmentation results are additionally fed into the model. The paper evaluates the proposed model on an in-house multiclass longitudinal COVID-19 dataset and shows that it outperforms its static version and can assist in localizing COVID-19 infections in patient's follow-up scans.

## Main Contributions

[1]: https://arxiv.org/pdf/2110.00948v2.pdf "Interactive Segmentation for COVID-19 Infection ... - arXiv.org"
[2]: https://arxiv.org/pdf/2110.09482 "Self-Supervised Monocular Depth Estimation with Internal ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/abs/2110.00948 "[2110.00948] Interactive Segmentation for COVID-19 Infection ..."

The paper claims the following contributions[^1^][1]:

- A new single network model for interactive segmentation of COVID-19 infection on longitudinal CT scans that fully utilizes all available past information to refine the segmentation of follow-up scans.
- A novel way of concatenating 3D volumes of medical images from two-time points with the additional reference time point segmentation as a guide to segment the target scan.
- An effective use of user feedback in the form of scribbles and the target's previous segmentation results to retain the segmentation information from previous refinement rounds.
- An in-house multiclass longitudinal COVID-19 dataset that contains 3D CT scans of 20 patients with COVID-19 infection at different time points and corresponding manual segmentations.
- Experimental results that show the superiority of the proposed model over its static version and its ability to assist in localizing COVID-19 infections in patient's follow-up scans.

## Method Summary

[1]: https://arxiv.org/pdf/2110.00948v2.pdf "Interactive Segmentation for COVID-19 Infection ... - arXiv.org"
[2]: https://arxiv.org/pdf/2110.09482 "Self-Supervised Monocular Depth Estimation with Internal ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/abs/2110.00948 "[2110.00948] Interactive Segmentation for COVID-19 Infection ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the proposed interactive segmentation model, which consists of three main components: a backbone network, a segmentation head, and a refinement head.
- The backbone network is based on a modified U-Net architecture that takes as input concatenated slices from two-time points (target and reference) along with the reference time point segmentation as a guide. The backbone network extracts features from both time points and fuses them using attention mechanisms.
- The segmentation head is a convolutional layer that produces an initial segmentation map for the target time point based on the fused features from the backbone network.
- The refinement head is another convolutional layer that takes as input the initial segmentation map, the user feedback in the form of scribbles, and the target's previous segmentation results. The refinement head produces a refined segmentation map that incorporates the user feedback and retains the segmentation information from previous refinement rounds.
- The paper also describes the loss function and the training procedure of the proposed model, which involves alternating between static and dynamic training modes. In static mode, the model is trained on pairs of time points without user feedback. In dynamic mode, the model is trained on sequences of time points with simulated user feedback.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the backbone network, the segmentation head, and the refinement head
backbone = UNet(input_channels=4, output_channels=64)
segmentation_head = Conv2D(in_channels=64, out_channels=3)
refinement_head = Conv2D(in_channels=7, out_channels=3)

# Define the loss function
loss_function = DiceLoss()

# Define the optimizer
optimizer = Adam()

# Define the training procedure
def train(model, data_loader, epochs, mode):
  # Loop over the epochs
  for epoch in range(epochs):
    # Loop over the batches
    for batch in data_loader:
      # Get the input and output data
      if mode == "static":
        # Static mode: use pairs of time points without user feedback
        target_image, reference_image, reference_segmentation, target_segmentation = batch
        user_feedback = None
        previous_segmentation = None
      elif mode == "dynamic":
        # Dynamic mode: use sequences of time points with simulated user feedback
        target_image, reference_image, reference_segmentation, target_segmentation, user_feedback, previous_segmentation = batch
      
      # Concatenate the input slices and the reference segmentation as a guide
      input = torch.cat([target_image, reference_image, reference_segmentation], dim=1)

      # Forward pass through the model
      fused_features = backbone(input)
      initial_segmentation = segmentation_head(fused_features)

      # If user feedback and previous segmentation are available, use them for refinement
      if user_feedback is not None and previous_segmentation is not None:
        refinement_input = torch.cat([initial_segmentation, user_feedback, previous_segmentation], dim=1)
        refined_segmentation = refinement_head(refinement_input)
      else:
        refined_segmentation = initial_segmentation
      
      # Compute the loss
      loss = loss_function(refined_segmentation, target_segmentation)

      # Backward pass and update the model parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

# Define the U-Net block
class UNetBlock(nn.Module):
  def __init__(self, in_channels, out_channels):
    super(UNetBlock, self).__init__()
    # Define two convolutional layers with batch normalization and ReLU activation
    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
    self.bn1 = nn.BatchNorm2d(out_channels)
    self.relu1 = nn.ReLU()
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
    self.bn2 = nn.BatchNorm2d(out_channels)
    self.relu2 = nn.ReLU()
  
  def forward(self, x):
    # Apply the first convolutional layer
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu1(x)
    # Apply the second convolutional layer
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu2(x)
    return x

# Define the attention block
class AttentionBlock(nn.Module):
  def __init__(self, F_g, F_l, F_int):
    super(AttentionBlock, self).__init__()
    # Define two convolutional layers with batch normalization and ReLU activation for the gating signal and the local features
    self.W_g = nn.Sequential(
      nn.Conv2d(F_g, F_int, kernel_size=1),
      nn.BatchNorm2d(F_int),
      nn.ReLU()
    )
    self.W_x = nn.Sequential(
      nn.Conv2d(F_l, F_int, kernel_size=1),
      nn.BatchNorm2d(F_int),
      nn.ReLU()
    )
    # Define a convolutional layer with sigmoid activation for the attention coefficients
    self.psi = nn.Sequential(
      nn.Conv2d(F_int, 1, kernel_size=1),
      nn.Sigmoid()
    )
  
  def forward(self, g, x):
    # Apply the convolutional layers to the gating signal and the local features
    g1 = self.W_g(g)
    x1 = self.W_x(x)
    # Add them and apply the ReLU activation
    psi = F.relu(g1 + x1)
    # Apply the convolutional layer to get the attention coefficients
    psi = self.psi(psi)
    # Multiply the local features by the attention coefficients
    out = x * psi
    return out

# Define the backbone network based on a modified U-Net architecture
class UNet(nn.Module):
  def __init__(self, input_channels, output_channels):
    super(UNet, self).__init__()
    
    # Define the number of channels for each block
    filters = [64, 128, 256]

    # Define the encoder blocks
    self.encoder_blocks = nn.ModuleList()
    for i in range(len(filters)):
      if i == 0:
        # The first encoder block takes the input channels as input
        encoder_block = UNetBlock(input_channels, filters[i])
      else:
        # The subsequent encoder blocks take the previous output channels as input
        encoder_block = UNetBlock(filters[i-1], filters[i])
      self.encoder_blocks.append(encoder_block)

    # Define the decoder blocks
    self.decoder_blocks = nn.ModuleList()
    for i in range(len(filters)-1):
      # The decoder blocks take the current and previous output channels as input
      decoder_block = UNetBlock(filters[-(i+1)] + filters[-(i+2)], filters[-(i+2)])
      self.decoder_blocks.append(decoder_block)

    # Define the attention blocks for feature fusion
    self.attention_blocks = nn.ModuleList()
    for i in range(len(filters)-1):
      # The attention blocks take the current output channels of the encoder and decoder as input
      attention_block = AttentionBlock(filters[-(i+1)], filters[-(i+2)], filters[-(i+2)] // 2)
      self.attention_blocks.append(attention_block)

    # Define the final convolutional layer that produces the output channels
    self.final_conv = nn.Conv2d(filters[0], output_channels, kernel_size=1)

  def forward(self, x):