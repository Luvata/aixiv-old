---
title: 2206.01843v2 Visual Clues  Bridging Vision and Language Foundations for Image Paragraph Captioning
date: 2022-06-02
---

# [Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning](http://arxiv.org/abs/2206.01843v2)

authors: Yujia Xie, Luowei Zhou, Xiyang Dai, Lu Yuan, Nguyen Bach, Ce Liu, Michael Zeng


## What, Why and How

[1]: https://arxiv.org/pdf/2206.01843v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2206.01843 "[2206.01843] Visual Clues: Bridging Vision and Language Foundations for ..."
[3]: http://export.arxiv.org/abs/2207.01843v2 "[2207.01843v2] Twelve Scientific Challenges for 6G: Rethinking the ..."

Here is a summary of the paper:

- **What**: The paper proposes a method for generating image paragraph captions using visual clues, which are structured textual prompts derived from a vision foundation model. The visual clues are fed to a language model, which produces multiple candidates for the paragraph. The candidates are then verified by the vision model again to select the best one.
- **Why**: The paper argues that textual representation is sufficient for describing visual content holistically, and that using visual clues can bridge the gap between vision and language foundation models without any extra cross-modal training. The paper also claims that visual clues can improve the quality and diversity of the generated paragraphs, as well as enable zero-shot captioning for unseen domains.
- **How**: The paper uses a vision foundation model called CLIP (Contrastive Language-Image Pre-training) to extract visual clues from an image, such as image tags, object attributes, locations, and captions. The visual clues are formatted as a structured text with bullet points and separators. The text is then input to a language model called GPT-3 (Generative Pre-trained Transformer 3), which generates multiple paragraph candidates based on the visual clues. The candidates are ranked by their log-likelihood and diversity scores, and then re-scored by CLIP based on their semantic similarity to the image. The highest-scoring candidate is selected as the final output. The paper evaluates the method on two datasets: COCO (Common Objects in Context) and Conceptual Captions, and compares it with several baselines. The paper reports that the method outperforms the baselines on various metrics, such as BLEU (Bilingual Evaluation Understudy), ROUGE (Recall-Oriented Understudy for Gisting Evaluation), CIDEr (Consensus-based Image Description Evaluation), SPICE (Semantic Propositional Image Caption Evaluation), and human evaluation. The paper also shows some qualitative examples of the generated paragraphs and visual clues.

## Main Contributions

[1]: https://arxiv.org/pdf/2206.01843v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2206.01843 "[2206.01843] Visual Clues: Bridging Vision and Language Foundations for ..."
[3]: http://export.arxiv.org/abs/2207.01843v2 "[2207.01843v2] Twelve Scientific Challenges for 6G: Rethinking the ..."

The paper lists the following contributions:

- **A novel method for image paragraph captioning using visual clues**, which are structured textual prompts derived from a vision foundation model. The visual clues capture various aspects of the image, such as image tags, object attributes, locations, and captions.
- **A zero-shot captioning framework that leverages large vision and language foundation models without any extra cross-modal training**. The paper uses CLIP as the vision model and GPT-3 as the language model, and shows that they can generate high-quality paragraphs for images from unseen domains by using visual clues as bridges.
- **A comprehensive evaluation of the proposed method on two datasets and several metrics**, as well as a human evaluation. The paper reports that the method outperforms several baselines on both COCO and Conceptual Captions datasets, and achieves high scores on metrics such as BLEU, ROUGE, CIDEr, and SPICE. The paper also shows that the human evaluators prefer the paragraphs generated by the proposed method over the baselines.

## Method Summary

[1]: https://arxiv.org/pdf/2206.01843v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2206.01843 "[2206.01843] Visual Clues: Bridging Vision and Language Foundations for ..."
[3]: http://export.arxiv.org/abs/2207.01843v2 "[2207.01843v2] Twelve Scientific Challenges for 6G: Rethinking the ..."

Here is a summary of the method section of the paper:

- The method consists of three main steps: **visual clue extraction**, **paragraph generation**, and **paragraph verification**.
- **Visual clue extraction**: The paper uses CLIP as the vision foundation model to extract visual clues from an image. The visual clues are structured textual prompts that capture various aspects of the image, such as image tags, object attributes, locations, and captions. The paper uses different modules of CLIP to obtain different types of visual clues, such as the text encoder, the image encoder, and the contrastive loss. The paper also introduces a novel module called CLIP-Attr to extract object attributes using a pre-trained attribute classifier. The paper formats the visual clues as a structured text with bullet points and separators, and uses them as input to the language model.
- **Paragraph generation**: The paper uses GPT-3 as the language foundation model to generate multiple paragraph candidates based on the visual clues. The paper feeds the visual clues to GPT-3 as a prompt, and uses a special token to indicate the end of the paragraph. The paper generates multiple candidates by sampling from different temperature values and beam sizes. The paper ranks the candidates by their log-likelihood and diversity scores, and selects the top-K candidates for the next step.
- **Paragraph verification**: The paper uses CLIP again to verify the quality and relevance of the paragraph candidates. The paper computes the semantic similarity between each candidate and the image using CLIP's text-image similarity score. The paper re-scores the candidates by multiplying their log-likelihood score with their similarity score, and selects the highest-scoring candidate as the final output.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image I
# Output: a paragraph P describing the image

# Step 1: Visual clue extraction
VC = [] # a list of visual clues
VC.append(CLIP.text_encoder(I)) # image tags
VC.append(CLIP.image_encoder(I)) # object locations
VC.append(CLIP.contrastive_loss(I)) # captions
VC.append(CLIP-Attr(I)) # object attributes
VC = format(VC) # format the visual clues as a structured text

# Step 2: Paragraph generation
C = [] # a list of paragraph candidates
for t in temperatures: # different temperature values for sampling
  for b in beams: # different beam sizes for beam search
    c = GPT-3(VC + "<END>") # generate a paragraph candidate using GPT-3
    C.append(c) # add the candidate to the list
C = rank(C) # rank the candidates by their log-likelihood and diversity scores
C = select_top_K(C) # select the top-K candidates for verification

# Step 3: Paragraph verification
for c in C: # for each candidate
  s = CLIP.similarity(c, I) # compute the semantic similarity to the image using CLIP
  c.score = c.score * s # re-score the candidate by multiplying with the similarity score
P = max(C) # select the highest-scoring candidate as the final output

return P # return the paragraph
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import clip # for vision foundation model
import transformers # for language foundation model
import nltk # for natural language processing
import numpy as np # for numerical operations

# Load the pre-trained models
clip_model, clip_preprocess = clip.load("ViT-B/32", device="cuda") # load CLIP model and preprocessing function
gpt3_model = transformers.AutoModelForCausalLM.from_pretrained("gpt3-large", device="cuda") # load GPT-3 model
gpt3_tokenizer = transformers.AutoTokenizer.from_pretrained("gpt3-large") # load GPT-3 tokenizer
attr_model = torch.load("attr_model.pth", device="cuda") # load attribute classifier model
attr_vocab = torch.load("attr_vocab.pth") # load attribute vocabulary

# Define some hyperparameters
K = 10 # number of candidates to select for verification
temperatures = [0.7, 0.9, 1.1] # temperature values for sampling
beams = [1, 3, 5] # beam sizes for beam search
alpha = 0.5 # weight for diversity score
beta = 0.5 # weight for similarity score

# Define some helper functions
def format(VC):
  # Format the visual clues as a structured text with bullet points and separators
  VC_text = ""
  VC_text += "Image tags:\n"
  VC_text += "- " + "\n- ".join(VC[0]) + "\n"
  VC_text += "Object locations:\n"
  VC_text += "- " + "\n- ".join(VC[1]) + "\n"
  VC_text += "Captions:\n"
  VC_text += "- " + "\n- ".join(VC[2]) + "\n"
  VC_text += "Object attributes:\n"
  VC_text += "- " + "\n- ".join(VC[3]) + "\n"
  return VC_text

def rank(C):
  # Rank the candidates by their log-likelihood and diversity scores
  C_scores = []
  for c in C:
    log_prob = c.log_prob # get the log-likelihood score from GPT-3 output
    diversity = len(set(nltk.word_tokenize(c.text))) / len(nltk.word_tokenize(c.text)) # compute the diversity score as the ratio of unique words to total words
    score = log_prob + alpha * diversity # combine the scores with a weight alpha
    C_scores.append(score)
  C_sorted = [c for _, c in sorted(zip(C_scores, C), reverse=True)] # sort the candidates by their scores in descending order
  return C_sorted

def select_top_K(C):
  # Select the top-K candidates for verification
  return C[:K]

def similarity(c, I):
  # Compute the semantic similarity between a candidate and an image using CLIP
  c_tensor = clip.tokenize(c.text).to("cuda") # tokenize the candidate text and convert to tensor
  I_tensor = clip_model.encode_image(I) # encode the image using CLIP image encoder
  c_tensor = clip_model.encode_text(c_tensor) # encode the candidate text using CLIP text encoder
  sim = torch.cosine_similarity(c_tensor, I_tensor) # compute the cosine similarity between the text and image embeddings
  return sim.item() # return the similarity score as a scalar

# Input: an image I
# Output: a paragraph P describing the image

# Step 1: Visual clue extraction
VC = [] # a list of visual clues

# Image tags: use CLIP text encoder to get the most probable labels for the image
image = clip_preprocess(I).unsqueeze(0).to("cuda") # preprocess the image and convert to tensor
text = clip.tokenize(["a photo of a %s" % concept for concept in clip.available_concepts]).to("cuda") # tokenize the possible concepts from CLIP vocabulary
logits_per_image, logits_per_text = clip_model(image, text) # get the logits from CLIP model
probs = logits_per_image.softmax(dim=-1).cpu().numpy() # get the probabilities from the logits
top_probs, top_labels = probs[0].argsort()[-5:][::-1], text[0].argsort()[-5:][::-1] # get the top 5 probabilities and labels
image_tags = [clip.available_concepts[i] for i in top_labels] # get the image tags from the labels
VC.append(image_tags) # add the image tags to the visual clues

# Object locations: use CLIP image encoder to get the bounding boxes and labels for the objects in the image
image_features = clip_model.encode_image(image) # encode the image using CLIP image encoder
boxes = clip_model.visual.bbox_transform(image_features) # get the bounding boxes from the image features
labels = clip_model.visual.class_embed.weight.argmax(dim=1) # get the labels from the class embeddings
object_locations = [] # a list of object locations
for i in range(len(boxes)):
  box = boxes[i].cpu().numpy() # get the coordinates of the box
  label = clip.available_concepts[labels[i]] # get the label of the box
  location = "%s at (%.2f, %.2f, %.2f, %.2f)" % (label, box[0], box[1], box[2], box[3]) # format the location as a string
  object_locations.append(location) # add the location to the list
VC.append(object_locations) # add the object locations to the visual clues

# Captions: use CLIP contrastive loss to get the most relevant captions for the image
text = clip.tokenize(["a photo of a %s" % caption for caption in captions]).to("cuda") # tokenize the possible captions from a pre-defined list
logits_per_image, logits_per_text = clip_model(image, text) # get the logits from CLIP model
probs = logits_per_image.softmax(dim=-1).cpu().numpy() # get the probabilities from the logits
top_probs, top_captions = probs[0].argsort()[-3:][::-1], text[0].argsort()[-3:][::-1] # get the top 3 probabilities and captions
image_captions = [captions[i] for i in top_captions] # get the image captions from the captions list
VC.append(image_captions) # add the image captions to the visual clues

# Object attributes: use CLIP-Attr to get the most probable attributes for each object in the image
object_attributes = [] # a list of object attributes
for i in range(len(boxes)):
  box = boxes[i].cpu().numpy() # get the coordinates of the box
  label = clip.available_concepts[labels[i]] # get the label of the box
  crop = I.crop(box) # crop the image according to the box
  crop_tensor = attr_preprocess(crop).unsqueeze(0).to("cuda") # preprocess the cropped image and convert to tensor
  logits = attr_model(crop_tensor) # get the logits from attribute classifier model
  probs = logits.softmax(dim=-1).cpu().numpy() # get the probabilities from the logits
  top_probs, top_attrs = probs[0].argsort()[-3:][::-1], attr_vocab[0].argsort()[-3:][::-1] # get the top 3 probabilities and attributes
  attr_list = [attr_vocab[i] for i in top_attrs] # get the attribute list from the attribute vocabulary
  attributes = "%s is %s" % (label, ", ".join(attr_list)) # format the attributes as a string
  object_attributes.append(attributes) # add the attributes to the list
VC.append(object_attributes) # add the object attributes to the visual clues

VC = format(VC) # format the visual clues as a structured text

# Step 2: Paragraph generation
C = [] # a list of paragraph candidates
for t in temperatures: # different temperature values for sampling
  for b in beams: # different beam sizes for beam search
    input_ids = gpt3_tokenizer(VC, return_tensors="pt").input_ids.to("cuda") # tokenize and convert visual clues to input ids 
    output_ids = gpt3_model.generate(input_ids, do_sample=True, max_length=256, temperature=t, num_beams=b, eos_token_id=gpt3_tokenizer.eos_token_id) # generate output ids using GPT-3 model with sampling and beam search parameters 
    output_text = gpt3_tokenizer.decode(output_ids[0], skip_special_tokens=True) # decode output ids to output text 
    output_log_prob = torch.log_softmax(gpt3_model(input_ids, labels=output_ids).logits, dim=-1).sum().item() # compute log-likelihood score of output text 
    c = {"text": output_text, "log_prob": output_log_prob} # create a candidate dictionary with text and log-likelihood score 
    C.append(c) # add candidate to candidate list 
C = rank(C) # rank candidates by their log-likelihood and diversity scores 
C = select_top_K(C) # select top-K candidates for verification 

# Step 3: Paragraph verification 
for c in C: # for each candidate