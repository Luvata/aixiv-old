---
title: 2008.00951v2 Encoding in Style  a StyleGAN Encoder for Image-to-Image Translation
date: 2020-08-01
---

# [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](http://arxiv.org/abs/2008.00951v2)

authors: Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2008.00951 "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation"
[2]: https://arxiv.org/pdf/2008.00951v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1501.00951v2 "[1501.00951v2] Normal subgroups of SimpHAtic groups"

Here is a summary of the paper[^1^][1]:

- **What**: The paper presents a generic image-to-image translation framework, pixel2style2pixel (pSp), that uses a novel encoder network to generate style vectors for a pretrained StyleGAN generator.
- **Why**: The paper aims to solve image-to-image translation tasks by encoding images into the latent space of StyleGAN, which simplifies the training process, supports multi-modal synthesis, and handles tasks without pixel-to-pixel correspondence.
- **How**: The paper proposes an encoder network that directly maps real images into the extended W+ latent space of StyleGAN, without additional optimization. The paper then defines image-to-image translation tasks as encoding problems from some input domain into the latent domain. The paper demonstrates the potential of the framework on various facial image-to-image translation tasks and shows that it can be extended beyond the human facial domain.

## Main Contributions

According to the paper, the main contributions are:

- A novel encoder network that directly embeds real images into the extended W+ latent space of StyleGAN, with no additional optimization.
- A generic image-to-image translation framework, pixel2style2pixel (pSp), that leverages the encoder network and a pretrained StyleGAN generator to solve various translation tasks by encoding images into the latent space.
- An extensive evaluation of the framework on a variety of facial image-to-image translation tasks, showing that it outperforms or matches state-of-the-art methods designed specifically for each task.
- A demonstration that the framework can be extended beyond the human facial domain by applying it to animal faces and cars.

## Method Summary

[1]: https://arxiv.org/abs/2008.00951 "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation"
[2]: https://arxiv.org/pdf/2008.00951v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1501.00951v2 "[1501.00951v2] Normal subgroups of SimpHAtic groups"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a novel encoder network that directly maps real images into the extended W+ latent space of StyleGAN, which consists of a series of style vectors corresponding to different layers of the generator. The encoder network is composed of a ResNet backbone followed by a fully connected layer for each style vector. The encoder network is trained with a pixel-wise reconstruction loss and a perceptual loss between the input image and the generated image.
- The paper defines image-to-image translation tasks as encoding problems from some input domain into the latent domain. The paper assumes that there exists a pretrained StyleGAN generator for the target domain, and that the encoder network can be adapted to different input domains by fine-tuning. The paper then proposes two variants of the pixel2style2pixel (pSp) framework: pSp-Global and pSp-Local.
- pSp-Global is designed for tasks where the input and output images have similar global structure, such as face frontalization or super-resolution. In this variant, the encoder network directly outputs a single style vector that is fed into all layers of the generator. The encoder network is trained with an additional identity loss that preserves the identity of the input image in the output image.
- pSp-Local is designed for tasks where the input and output images have different global structure, such as face editing or sketch-to-image. In this variant, the encoder network outputs a series of style vectors that are fed into different layers of the generator. The encoder network is trained with an additional mixing regularization loss that encourages diversity in the output images by randomly mixing styles from different images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder network
encoder = ResNet(backbone) + FC(layers)

# Define the StyleGAN generator
generator = StyleGAN(pretrained)

# Define the losses
reconstruction_loss = L1(input, output)
perceptual_loss = LPIPS(input, output)
identity_loss = L1(input, output) * mask
mixing_regularization_loss = L1(output1, output2)

# Choose the pSp variant
if task == "global":
  # Use a single style vector for all layers
  style = encoder(input)
  output = generator(style)
  loss = reconstruction_loss + perceptual_loss + identity_loss
elif task == "local":
  # Use a series of style vectors for different layers
  styles = encoder(input)
  output = generator(styles)
  # Randomly mix styles from different images
  styles_mixed = mix(styles, styles_other)
  output_mixed = generator(styles_mixed)
  loss = reconstruction_loss + perceptual_loss + mixing_regularization_loss

# Train the encoder network
optimizer = Adam(lr)
optimizer.minimize(loss, encoder.parameters)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the libraries
import torch
import torchvision
import lpips
import stylegan2

# Define the encoder network
class Encoder(torch.nn.Module):
  def __init__(self, backbone, layers):
    super().__init__()
    # Use a ResNet backbone with a fixed number of output channels
    self.backbone = torchvision.models.resnet50(pretrained=True)
    self.backbone.fc = torch.nn.Identity()
    self.backbone.eval()
    # Use a fully connected layer for each style vector
    self.fc = torch.nn.ModuleList([torch.nn.Linear(2048, 512) for _ in range(layers)])

  def forward(self, x):
    # Extract features from the backbone
    features = self.backbone(x)
    # Generate style vectors from the features
    styles = [fc(features) for fc in self.fc]
    return styles

# Define the StyleGAN generator
generator = stylegan2.StyleGAN2Generator(resolution=1024, w_avg_beta=0.995)

# Load the pretrained weights for the generator
generator.load_state_dict(torch.load("stylegan2-ffhq-config-f.pt"))

# Freeze the generator parameters
generator.eval()

# Define the losses
reconstruction_loss = torch.nn.L1Loss()
perceptual_loss = lpips.LPIPS(net="vgg")
identity_loss = torch.nn.L1Loss()
mixing_regularization_loss = torch.nn.L1Loss()

# Choose the pSp variant
task = "global" # or "local"

# Create the encoder network
encoder = Encoder(backbone="resnet50", layers=18)

# Create the optimizer for the encoder network
optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)

# Create the data loader for the input domain
data_loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)

# Train the encoder network
for epoch in range(100):
  for input in data_loader:
    # Zero the gradients
    optimizer.zero_grad()
    # Generate style vectors from the input images
    styles = encoder(input)
    if task == "global":
      # Use a single style vector for all layers
      style = styles[0].unsqueeze(1).repeat(1, 18, 1)
      output = generator(style)
      # Compute the identity loss with a face mask
      mask = get_face_mask(input)
      id_loss = identity_loss(input * mask, output * mask)
      # Compute the total loss
      loss = reconstruction_loss(input, output) + perceptual_loss(input, output) + id_loss
    elif task == "local":
      # Use a series of style vectors for different layers
      output = generator(styles)
      # Randomly mix styles from different images
      styles_mixed = mix(styles, styles_other)
      output_mixed = generator(styles_mixed)
      # Compute the mixing regularization loss
      mix_loss = mixing_regularization_loss(output, output_mixed)
      # Compute the total loss
      loss = reconstruction_loss(input, output) + perceptual_loss(input, output) + mix_loss
    
    # Backpropagate the loss and update the encoder parameters
    loss.backward()
    optimizer.step()

    # Print the loss value
    print(f"Epoch {epoch}, Loss {loss.item()}")
```