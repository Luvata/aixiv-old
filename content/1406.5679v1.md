---
title: 1406.5679v1 Deep Fragment Embeddings for Bidirectional Image Sentence Mapping
date: 2014-06-31
---

# [Deep Fragment Embeddings for Bidirectional Image Sentence Mapping](http://arxiv.org/abs/1406.5679v1)

authors: Andrej Karpathy, Armand Joulin, Li Fei-Fei


## What, Why and How

[1]: https://arxiv.org/abs/1406.5679 "[1406.5679] Deep Fragment Embeddings for Bidirectional ... - arXiv.org"
[2]: https://arxiv.org/pdf/1406.5679v1.pdf "arXiv:1406.5679v1 [cs.CV] 22 Jun 2014"
[3]: http://export.arxiv.org/abs/1302.5679v1 "[1302.5679v1] Memory Aware Load Balance Strategy on a Parallel Branch ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data.
- **Why**: The paper aims to improve the performance and interpretability of image-sentence retrieval tasks by reasoning on both the global level of images and sentences and the finer level of their respective fragments.
- **How**: The paper proposes a deep neural network that embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space and learns to align them across modalities. The paper also introduces a new fragment alignment objective that complements a traditional sentence-image ranking objective. The paper evaluates the model on three datasets: Pascal1K, Flickr8K and Flickr30K.

## Main Contributions

The paper claims to make the following contributions:

- A novel model for bidirectional image-sentence retrieval that embeds fragments of images and sentences into a common space and learns to align them across modalities.
- A new fragment alignment objective that encourages the model to associate relevant fragments in images and sentences.
- A comprehensive experimental evaluation that shows significant improvements over state of the art methods on three datasets and provides interpretable predictions.

## Method Summary

[1]: https://arxiv.org/pdf/1406.5679v1.pdf "arXiv:1406.5679v1 [cs.CV] 22 Jun 2014"
[2]: https://arxiv.org/abs/1406.5679 "[1406.5679] Deep Fragment Embeddings for Bidirectional ... - arXiv.org"
[3]: http://export.arxiv.org/abs/0909.5679v1 "[0909.5679v1] Diboson physics at CDF - export.arxiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the model architecture, which consists of three components: an image encoder, a sentence encoder and a fragment alignment layer.
- The image encoder takes an image as input and extracts features from a convolutional neural network (CNN). It also detects objects in the image using a region proposal network (RPN) and encodes them into vectors using a fully connected layer.
- The sentence encoder takes a sentence as input and parses it into a dependency tree. It then encodes each dependency relation into a vector using a recurrent neural network (RNN) with long short-term memory (LSTM) cells.
- The fragment alignment layer takes the image and sentence embeddings and computes their similarity scores using a dot product. It also learns to align the fragments across modalities using a structured hinge loss that encourages matching fragments to have higher scores than non-matching ones.
- The paper also defines a ranking loss that penalizes the model for ranking incorrect sentences higher than correct ones for a given image, and vice versa. The paper combines the ranking loss and the fragment alignment loss into a single objective function that is optimized using stochastic gradient descent (SGD).

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the model parameters
D = # dimension of the common embedding space
L = # number of objects to detect in each image
M = # number of dependency relations to extract from each sentence
margin = # margin for the hinge loss

# Initialize the model components
CNN = # convolutional neural network for image feature extraction
RPN = # region proposal network for object detection
FC = # fully connected layer for object encoding
RNN = # recurrent neural network with LSTM cells for sentence encoding
W = # weight matrix for fragment alignment

# Define the loss functions
def ranking_loss(image, sentence, negative_image, negative_sentence):
  # Compute the similarity scores between image and sentence embeddings
  score_pos = dot_product(image, sentence)
  score_neg1 = dot_product(negative_image, sentence)
  score_neg2 = dot_product(image, negative_sentence)
  
  # Compute the ranking loss as the sum of hinge losses
  loss1 = max(0, margin - score_pos + score_neg1)
  loss2 = max(0, margin - score_pos + score_neg2)
  return loss1 + loss2

def fragment_alignment_loss(image_fragments, sentence_fragments):
  # Compute the similarity scores between image and sentence fragments
  scores = dot_product(W * image_fragments, sentence_fragments)
  
  # Find the best matching fragments for each modality
  best_image_match = argmax(scores, axis=1)
  best_sentence_match = argmax(scores, axis=0)
  
  # Compute the fragment alignment loss as the sum of hinge losses
  loss = 0
  for i in range(L):
    for j in range(M):
      if i == best_image_match[j] or j == best_sentence_match[i]:
        # Positive pair
        loss += max(0, margin - scores[i,j] + max(scores[i,:]) + max(scores[:,j]))
      else:
        # Negative pair
        loss += max(0, margin + scores[i,j] - max(scores[i,:]) - max(scores[:,j]))
  return loss

# Define the objective function as a weighted sum of the losses
def objective_function(image, sentence, negative_image, negative_sentence):
  # Encode the image and sentence into vectors
  image_embedding = CNN(image)
  sentence_embedding = RNN(sentence)
  
  # Detect and encode the image and sentence fragments into vectors
  image_fragments = FC(RPN(image))
  sentence_fragments = RNN(dependency_parse(sentence))
  
  # Compute the ranking loss and the fragment alignment loss
  rank_loss = ranking_loss(image_embedding, sentence_embedding, negative_image, negative_sentence)
  frag_loss = fragment_alignment_loss(image_fragments, sentence_fragments)
  
  # Return the weighted sum of the losses
  return alpha * rank_loss + beta * frag_loss

# Train the model using stochastic gradient descent
for epoch in range(num_epochs):
  for batch in data_loader:
    # Sample a batch of images and sentences and their negatives
    image_batch, sentence_batch, negative_image_batch, negative_sentence_batch = batch
    
    # Compute the gradients of the objective function with respect to the model parameters
    gradients = compute_gradients(objective_function, image_batch, sentence_batch, negative_image_batch, negative_sentence_batch)
    
    # Update the model parameters using a learning rate schedule
    update_parameters(gradients, learning_rate_schedule(epoch))
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the required libraries
import torch # for tensor operations and neural network modules
import torchvision # for image processing and pre-trained CNN models
import spacy # for natural language processing and dependency parsing
import numpy as np # for numerical computations

# Define the model parameters
D = 1024 # dimension of the common embedding space
L = 19 # number of objects to detect in each image
M = 19 # number of dependency relations to extract from each sentence
margin = 0.1 # margin for the hinge loss
alpha = 0.5 # weight for the ranking loss
beta = 0.5 # weight for the fragment alignment loss
num_epochs = 100 # number of training epochs
batch_size = 32 # size of each training batch
learning_rate = 0.01 # initial learning rate

# Initialize the model components
CNN = torchvision.models.resnet50(pretrained=True) # pre-trained ResNet-50 model for image feature extraction
RPN = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # pre-trained Faster R-CNN model for object detection
FC = torch.nn.Linear(2048, D) # fully connected layer for object encoding
RNN = torch.nn.LSTM(300, D) # recurrent neural network with LSTM cells for sentence encoding
W = torch.nn.Parameter(torch.randn(D, D)) # weight matrix for fragment alignment

# Define the loss functions
def ranking_loss(image, sentence, negative_image, negative_sentence):
  # Compute the similarity scores between image and sentence embeddings
  score_pos = torch.dot(image, sentence)
  score_neg1 = torch.dot(negative_image, sentence)
  score_neg2 = torch.dot(image, negative_sentence)
  
  # Compute the ranking loss as the sum of hinge losses
  loss1 = torch.relu(margin - score_pos + score_neg1)
  loss2 = torch.relu(margin - score_pos + score_neg2)
  return loss1 + loss2

def fragment_alignment_loss(image_fragments, sentence_fragments):
  # Compute the similarity scores between image and sentence fragments
  scores = torch.matmul(W * image_fragments, sentence_fragments.t())
  
  # Find the best matching fragments for each modality
  best_image_match = torch.argmax(scores, dim=1)
  best_sentence_match = torch.argmax(scores, dim=0)
  
  # Compute the fragment alignment loss as the sum of hinge losses
  loss = 0
  for i in range(L):
    for j in range(M):
      if i == best_image_match[j] or j == best_sentence_match[i]:
        # Positive pair
        loss += torch.relu(margin - scores[i,j] + torch.max(scores[i,:]) + torch.max(scores[:,j]))
      else:
        # Negative pair
        loss += torch.relu(margin + scores[i,j] - torch.max(scores[i,:]) - torch.max(scores[:,j]))
  return loss

# Define the objective function as a weighted sum of the losses
def objective_function(image, sentence, negative_image, negative_sentence):
  # Encode the image and sentence into vectors
  image_embedding = CNN(image).squeeze()
  sentence_embedding = RNN(sentence)[0][-1]
  
  # Detect and encode the image and sentence fragments into vectors
  image_fragments = FC(RPN(image)[0]['boxes']).squeeze()
  sentence_fragments = RNN(dependency_parse(sentence))[0][-1]
  
  # Compute the ranking loss and the fragment alignment loss
  rank_loss = ranking_loss(image_embedding, sentence_embedding, negative_image, negative_sentence)
  frag_loss = fragment_alignment_loss(image_fragments, sentence_fragments)
  
  # Return the weighted sum of the losses
  return alpha * rank_loss + beta * frag_loss

# Define a function to parse a sentence into a dependency tree and encode each relation into a vector
def dependency_parse(sentence):
  # Load a pre-trained spaCy model for English language
  nlp = spacy.load('en_core_web_lg')
  
  # Parse the sentence into a dependency tree using spaCy
  doc = nlp(sentence)
  
  # Initialize an empty list to store the relation vectors
  relations = []
  
  # Iterate over each token in the parsed document
  for token in doc:
    # Extract the head token, the dependency label and the child token of each relation
    head = token.head.text.lower()
    label = token.dep_.lower()
    child = token.text.lower()
    
    # Concatenate the word vectors of the head, label and child tokens using spaCy
    relation = np.concatenate([nlp.vocab[head].vector, nlp.vocab[label].vector, nlp.vocab[child].vector])
    
    # Append the relation vector to the list
    relations.append(relation)
  
  # Convert the list of relation vectors into a torch tensor
  relations = torch.tensor(relations, dtype=torch.float32)
  
  # Return the tensor of relation vectors
  return relations

# Define a function to load the data and create batches
def data_loader():
  # Load the images and sentences from the dataset
  images, sentences = load_data()
  
  # Shuffle the data
  indices = np.random.permutation(len(images))
  images = images[indices]
  sentences = sentences[indices]
  
  # Create batches of images and sentences and their negatives
  for i in range(0, len(images), batch_size):
    # Get a batch of images and sentences
    image_batch = images[i:i+batch_size]
    sentence_batch = sentences[i:i+batch_size]
    
    # Get a batch of negative images and sentences by randomly sampling from the data
    negative_image_batch = images[np.random.choice(len(images), size=batch_size)]
    negative_sentence_batch = sentences[np.random.choice(len(sentences), size=batch_size)]
    
    # Yield the batch of images and sentences and their negatives
    yield image_batch, sentence_batch, negative_image_batch, negative_sentence_batch

# Train the model using stochastic gradient descent
for epoch in range(num_epochs):
  for batch in data_loader():
    # Sample a batch of images and sentences and their negatives
    image_batch, sentence_batch, negative_image_batch, negative_sentence_batch = batch
    
    # Compute the gradients of the objective function with respect to the model parameters
    gradients = torch.autograd.grad(objective_function(image_batch, sentence_batch, negative_image_batch, negative_sentence_batch), [CNN.parameters(), RPN.parameters(), FC.parameters(), RNN.parameters(), W])
    
    # Update the model parameters using a learning rate schedule
    for param, grad in zip([CNN.parameters(), RPN.parameters(), FC.parameters(), RNN.parameters(), W], gradients):
      param -= learning_rate * grad
    
    # Decay the learning rate by a factor of 0.1 every 10 epochs
    if epoch % 10 == 0:
      learning_rate *= 0.1
```