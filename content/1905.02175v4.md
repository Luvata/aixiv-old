---
title: 1905.02175v4 Adversarial Examples Are Not Bugs, They Are Features
date: 2019-05-03
---

# [Adversarial Examples Are Not Bugs, They Are Features](http://arxiv.org/abs/1905.02175v4)

authors: Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry


## What, Why and How

[1]: https://arxiv.org/pdf/1905.02175v4.pdf "Adversarial Examples Are Not Bugs, They Are Features - arXiv.org"
[2]: https://arxiv.org/abs/1905.02175 "Title: Adversarial Examples Are Not Bugs, They Are Features - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/1905.02175 "[1905.02175] Adversarial Examples Are Not Bugs, They Are Features"

Here is a summary of the paper:

- **What**: The paper investigates the phenomenon of adversarial examples, which are inputs that fool machine learning models but are imperceptible to humans. The paper claims that adversarial examples are not bugs, but features that are learned by the models because they are highly predictive of the labels.
- **Why**: The paper aims to provide a new perspective on adversarial examples and explain why they exist and persist in standard datasets and models. The paper also challenges the common view that adversarial robustness can be achieved independently of accuracy by using regularization or preprocessing methods.
- **How**: The paper introduces a theoretical framework to capture the notion of non-robust features, which are features that correlate well with the labels but are not aligned with human perception. The paper then shows empirically that non-robust features exist in natural images and text, and that models trained on clean data tend to rely on them. The paper also presents a toy example where non-robust features can be explicitly constructed and manipulated, and where adversarial vulnerability can be traced back to a misalignment between human and model robustness.

## Main Contributions

The paper makes the following contributions:

- It proposes a new hypothesis that adversarial examples are caused by non-robust features that are learned by models because they are highly predictive of the labels.
- It develops a theoretical framework to formalize the concept of non-robust features and to measure their impact on model performance and robustness.
- It provides empirical evidence that non-robust features exist in natural images and text, and that models trained on clean data tend to exploit them.
- It demonstrates a simple setting where non-robust features can be explicitly constructed and manipulated, and where adversarial vulnerability can be explained by a misalignment between human and model robustness.

## Method Summary

[1]: https://arxiv.org/pdf/1905.02175v4.pdf "Adversarial Examples Are Not Bugs, They Are Features - arXiv.org"
[2]: https://arxiv.org/abs/1905.02175 "Title: Adversarial Examples Are Not Bugs, They Are Features - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/1905.02175 "[1905.02175] Adversarial Examples Are Not Bugs, They Are Features"

Here is a summary of the method section of the paper:

- The paper introduces a theoretical framework to formalize the concept of non-robust features and to measure their impact on model performance and robustness. The paper defines non-robust features as features that are predictive of the labels but are not invariant to small perturbations. The paper also defines robust and non-robust accuracy as the accuracy of a model on clean and adversarially perturbed inputs, respectively.
- The paper then shows empirically that non-robust features exist in natural images and text, and that models trained on clean data tend to exploit them. The paper uses a technique called adversarial training, which involves augmenting the training data with adversarially perturbed inputs, to reduce the model's reliance on non-robust features. The paper finds that adversarial training leads to lower standard accuracy but higher robust accuracy, suggesting a trade-off between the two.
- The paper also demonstrates a simple setting where non-robust features can be explicitly constructed and manipulated, and where adversarial vulnerability can be explained by a misalignment between human and model robustness. The paper considers a synthetic dataset where each input is composed of two parts: a human-readable part and a random noise part. The paper assigns labels to the inputs based on either the human-readable part or the noise part, creating two different datasets. The paper then trains models on these datasets and evaluates their performance and robustness. The paper finds that models trained on the noise-based dataset learn to rely on non-robust features that are imperceptible to humans, and are vulnerable to adversarial perturbations that modify the noise part. On the other hand, models trained on the human-based dataset learn to rely on robust features that are aligned with human perception, and are resilient to adversarial perturbations that modify the human-readable part.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define non-robust features as features that are predictive of the labels but not invariant to small perturbations
def non_robust_features(x, y):
  return features(x) that correlate with y but change under perturbations(x)

# Define robust and non-robust accuracy as the accuracy of a model on clean and adversarially perturbed inputs, respectively
def robust_accuracy(model, x, y):
  return accuracy(model, x, y)

def non_robust_accuracy(model, x, y):
  return accuracy(model, perturbations(x), y)

# Train models on natural images and text using standard or adversarial training
def standard_training(model, data):
  for x, y in data:
    model.update(x, y)

def adversarial_training(model, data):
  for x, y in data:
    x_adv = perturbations(x)
    model.update(x_adv, y)

# Evaluate the performance and robustness of the models on clean and adversarially perturbed inputs
def evaluate(model, data):
  print(robust_accuracy(model, data.x, data.y))
  print(non_robust_accuracy(model, data.x, data.y))

# Create a synthetic dataset where each input is composed of two parts: a human-readable part and a random noise part
def create_synthetic_data(size):
  data = []
  for i in range(size):
    human_part = random_human_readable_image_or_text()
    noise_part = random_noise()
    x = concatenate(human_part, noise_part)
    data.append(x)
  return data

# Assign labels to the inputs based on either the human-readable part or the noise part
def label_by_human_part(data):
  for x in data:
    human_part = extract_human_part(x)
    y = classify_by_human_part(human_part)
    x.label = y

def label_by_noise_part(data):
  for x in data:
    noise_part = extract_noise_part(x)
    y = classify_by_noise_part(noise_part)
    x.label = y

# Train models on the synthetic datasets and evaluate their performance and robustness
data_human = create_synthetic_data(size)
data_noise = create_synthetic_data(size)

label_by_human_part(data_human)
label_by_noise_part(data_noise)

model_human = train_model(data_human)
model_noise = train_model(data_noise)

evaluate(model_human, data_human)
evaluate(model_noise, data_noise)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import torch
import torchvision
import transformers

# Define non-robust features as features that are predictive of the labels but not invariant to small perturbations
def non_robust_features(x, y, model):
  # Compute the gradient of the model's loss with respect to the input x
  x.requires_grad = True
  loss = torch.nn.CrossEntropyLoss()(model(x), y)
  grad = torch.autograd.grad(loss, x)[0]

  # Compute the correlation between the gradient and the input x
  corr = torch.mean(grad * x)

  # Return the absolute value of the correlation as a measure of non-robustness
  return torch.abs(corr)

# Define robust and non-robust accuracy as the accuracy of a model on clean and adversarially perturbed inputs, respectively
def robust_accuracy(model, x, y):
  # Predict the labels for the clean inputs x
  y_pred = torch.argmax(model(x), dim=1)

  # Compute the accuracy as the fraction of correct predictions
  acc = torch.mean((y_pred == y).float())

  # Return the accuracy
  return acc

def non_robust_accuracy(model, x, y):
  # Generate adversarial perturbations for the inputs x using the fast gradient sign method
  epsilon = 0.01 # The maximum perturbation size
  x.requires_grad = True
  loss = torch.nn.CrossEntropyLoss()(model(x), y)
  grad = torch.autograd.grad(loss, x)[0]
  perturbations = epsilon * torch.sign(grad)

  # Add the perturbations to the inputs x
  x_adv = x + perturbations

  # Predict the labels for the adversarially perturbed inputs x_adv
  y_pred = torch.argmax(model(x_adv), dim=1)

  # Compute the accuracy as the fraction of correct predictions
  acc = torch.mean((y_pred == y).float())

  # Return the accuracy
  return acc

# Train models on natural images and text using standard or adversarial training
def standard_training(model, data_loader, optimizer, epochs):
  # Loop over the number of epochs
  for epoch in range(epochs):
    # Loop over the batches of data
    for x, y in data_loader:
      # Zero the gradients of the optimizer
      optimizer.zero_grad()

      # Forward pass the inputs x through the model and compute the loss with respect to the labels y
      loss = torch.nn.CrossEntropyLoss()(model(x), y)

      # Backward pass the loss and update the model parameters using the optimizer
      loss.backward()
      optimizer.step()

def adversarial_training(model, data_loader, optimizer, epochs):
  # Loop over the number of epochs
  for epoch in range(epochs):
    # Loop over the batches of data
    for x, y in data_loader:
      # Zero the gradients of the optimizer
      optimizer.zero_grad()

      # Generate adversarial perturbations for the inputs x using the fast gradient sign method
      epsilon = 0.01 # The maximum perturbation size
      x.requires_grad = True
      loss = torch.nn.CrossEntropyLoss()(model(x), y)
      grad = torch.autograd.grad(loss, x)[0]
      perturbations = epsilon * torch.sign(grad)

      # Add the perturbations to the inputs x
      x_adv = x + perturbations

      # Forward pass the adversarially perturbed inputs x_adv through the model and compute the loss with respect to the labels y
      loss = torch.nn.CrossEntropyLoss()(model(x_adv), y)

      # Backward pass the loss and update the model parameters using the optimizer
      loss.backward()
      optimizer.step()

# Evaluate the performance and robustness of the models on clean and adversarially perturbed inputs
def evaluate(model, data_loader):
  # Initialize variables to store the total accuracy and non-robustness scores
  total_acc = 0.0
  total_non_robustness = 0.0

  # Loop over the batches of data
  for x, y in data_loader:
    # Compute and accumulate the robust accuracy for the clean inputs x and labels y
    acc = robust_accuracy(model, x, y)
    total_acc += acc

    # Compute and accumulate the non-robustness score for the inputs x and labels y using a reference model (e.g., a pre-trained ResNet)
    reference_model = torchvision.models.resnet18(pretrained=True)
    non_robustness = non_robust_features(x, y, reference_model)
    total_non_robustness += non_robustness

  # Compute the average accuracy and non-robustness score over the data
  avg_acc = total_acc / len(data_loader)
  avg_non_robustness = total_non_robustness / len(data_loader)

  # Print the results
  print(f"Robust accuracy: {avg_acc:.4f}")
  print(f"Non-robustness score: {avg_non_robustness:.4f}")

# Create a synthetic dataset where each input is composed of two parts: a human-readable part and a random noise part
def create_synthetic_data(size, input_type):
  # Initialize an empty list to store the inputs
  data = []

  # Loop over the size of the dataset
  for i in range(size):
    # Generate a random human-readable image or text depending on the input type
    if input_type == "image":
      human_part = random_human_readable_image()
    elif input_type == "text":
      human_part = random_human_readable_text()
    else:
      raise ValueError("Invalid input type")

    # Generate a random noise image or text depending on the input type
    if input_type == "image":
      noise_part = random_noise_image()
    elif input_type == "text":
      noise_part = random_noise_text()
    else:
      raise ValueError("Invalid input type")

    # Concatenate the human-readable part and the noise part to form the input
    x = concatenate(human_part, noise_part)

    # Append the input to the data list
    data.append(x)

  # Return the data list as a numpy array or a torch tensor depending on the input type
  if input_type == "image":
    return np.array(data)
  elif input_type == "text":
    return torch.tensor(data)
  else:
    raise ValueError("Invalid input type")

# Assign labels to the inputs based on either the human-readable part or the noise part
def label_by_human_part(data, input_type):
  # Initialize an empty list to store the labels
  labels = []

  # Loop over the inputs in the data
  for x in data:
    # Extract the human-readable part from the input
    human_part = extract_human_part(x, input_type)

    # Classify the human-readable part by using a pre-trained model (e.g., a ResNet for images or a BERT for text)
    if input_type == "image":
      model = torchvision.models.resnet18(pretrained=True)
      y = model(human_part)
    elif input_type == "text":
      model = transformers.BertForSequenceClassification.from_pretrained("bert-base-uncased")
      y = model(human_part)
    else:
      raise ValueError("Invalid input type")

    # Append the label to the labels list
    labels.append(y)

  # Return the labels list as a numpy array or a torch tensor depending on the input type
  if input_type == "image":
    return np.array(labels)
  elif input_type == "text":
    return torch.tensor(labels)
  else:
    raise ValueError("Invalid input type")

def label_by_noise_part(data, input_type):
  # Initialize an empty list to store the labels
  labels = []

  # Loop over the inputs in the data
  for x in data:
    # Extract the noise part from the input
    noise_part = extract_noise_part(x, input_type)

    # Classify the noise part by using a simple model (e.g., a linear classifier for images or text)
    if input_type == "image":
      model = torch.nn.Linear(noise_part.shape[0], num_classes)
      y = model(noise_part)
    elif input_type == "text":
      model = torch.nn.Linear(noise_part.shape[0], num_classes)
      y = model(noise_part)
    else:
      raise ValueError("Invalid input type")

    # Append the label to the labels list
    labels.append(y)

  # Return the labels list as a numpy array or a torch tensor depending on the input type
  if input_type == "image":
    return np.array(labels)
  elif input_type == "text":
    return torch.tensor(labels)
  else:
    raise ValueError("Invalid input type")

# Train models on the synthetic datasets and evaluate their performance and robustness
def train_and_evaluate(input_type, num_classes, size, epochs):
  # Create synthetic datasets where each input is composed of two parts: a human-readable part and a random noise part
  data_human = create_synthetic_data(size, input_type)
  data_noise = create_synthetic_data(size, input_type)

  # Assign labels to the inputs based on either the human-readable part or the noise part
  label_by_human_part(data_human, input_type)
  label_by_noise_part(data_noise, input_type