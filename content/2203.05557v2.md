---
title: 2203.05557v2 Conditional Prompt Learning for Vision-Language Models
date: 2022-03-06
---

# [Conditional Prompt Learning for Vision-Language Models](http://arxiv.org/abs/2203.05557v2)

authors: Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu


## What, Why and How

[1]: https://arxiv.org/pdf/2203.05557v2.pdf "Conditional Prompt Learning for Vision-Language Models - arXiv.org"
[2]: https://arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"
[3]: https://arxiv.org/pdf/2209.05557v2 "arXiv:2209.05557v2 [cs.LG] 23 Sep 2022"
[4]: http://export.arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"

Here is a summary of the paper:

- **What**: The paper proposes a method called **Conditional Context Optimization (CoCoOp)**, which adapts pre-trained vision-language models to downstream datasets by learning dynamic prompts that are conditional on the input image.
- **Why**: The paper addresses a problem of a previous method called **Context Optimization (CoOp)**, which learns static prompts that are not generalizable to unseen classes or domains. The paper aims to improve the transferability and domain generalization of prompt learning for vision-language models.
- **How**: The paper extends CoOp by adding a lightweight neural network that generates an input-conditional token for each image. The token is concatenated with the context words to form a dynamic prompt. The paper evaluates CoCoOp on several benchmarks and shows that it outperforms CoOp and manual prompts in terms of accuracy, generalization and transferability.

The paper can be found at [this link](https://arxiv.org/pdf/2203.05557v2.pdf).

## Main Contributions

[1]: https://arxiv.org/pdf/2203.05557v2.pdf "Conditional Prompt Learning for Vision-Language Models - arXiv.org"
[2]: https://arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"
[3]: https://arxiv.org/pdf/2209.05557v2 "arXiv:2209.05557v2 [cs.LG] 23 Sep 2022"
[4]: http://export.arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"

According to the paper, the main contributions are:

- The paper proposes **CoCoOp**, a novel method for conditional prompt learning that adapts pre-trained vision-language models to downstream datasets by learning dynamic prompts that are conditional on the input image.
- The paper identifies and analyzes a critical problem of **CoOp**, a previous method for prompt learning that learns static prompts that are not generalizable to unseen classes or domains.
- The paper conducts extensive experiments on several benchmarks and shows that **CoCoOp** outperforms **CoOp** and manual prompts in terms of accuracy, generalization and transferability. The paper also introduces a new benchmark called **DOSCO** (DOmain Shift in COntext) to evaluate the domain generalization ability of prompt learning methods.

The paper can be found at [this link](https://arxiv.org/pdf/2203.05557v2.pdf).

## Method Summary

[1]: https://arxiv.org/pdf/2203.05557v2.pdf "Conditional Prompt Learning for Vision-Language Models - arXiv.org"
[2]: https://arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"
[3]: https://arxiv.org/pdf/2209.05557v2 "arXiv:2209.05557v2 [cs.LG] 23 Sep 2022"
[4]: http://export.arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"

Here is a summary of the method section of the paper:

- The paper reviews the basics of **CLIP** and **CoOp**, two methods for adapting pre-trained vision-language models to downstream datasets using prompts.
- The paper introduces **CoCoOp**, a novel method that extends CoOp by learning dynamic prompts that are conditional on the input image. The paper defines the problem formulation, the objective function, and the optimization algorithm for CoCoOp.
- The paper describes the architecture of CoCoOp, which consists of three components: a pre-trained vision-language model, a context encoder, and a conditional token generator. The paper explains how each component works and how they are integrated.
- The paper discusses some implementation details and ablation studies of CoCoOp, such as the choice of prompt template, the initialization of context words, and the effect of different conditional token generators.

The paper can be found at [this link](https://arxiv.org/pdf/2203.05557v2.pdf).


## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2203.05557v2.pdf "Conditional Prompt Learning for Vision-Language Models - arXiv.org"
[2]: https://arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"
[3]: https://arxiv.org/pdf/2209.05557v2 "arXiv:2209.05557v2 [cs.LG] 23 Sep 2022"
[4]: http://export.arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"

Here is a possible pseudo code for the method section of the paper:

```python
# Define the pre-trained vision-language model
VLM = CLIP or ALIGN

# Define the prompt template
prompt = "a photo of [class], a type of [context]"

# Define the context encoder
CE = MLP or Transformer

# Define the conditional token generator
CTG = MLP or Transformer

# Initialize the context words randomly or from a vocabulary
context = init_context()

# Define the loss function
loss = cross_entropy

# Define the optimizer
optimizer = Adam

# Loop over the training data
for image, label in data:

  # Generate a conditional token for the image using CTG
  token = CTG(image)

  # Encode the context words using CE
  encoded_context = CE(context)

  # Concatenate the token and the encoded context to form a dynamic prompt
  prompt = concat(token, encoded_context)

  # Fill in the prompt template with the prompt and the label
  text = fill(prompt, label)

  # Encode the image and the text using VLM
  image_embed, text_embed = VLM(image, text)

  # Compute the similarity score between the image and the text embeddings
  score = dot_product(image_embed, text_embed)

  # Compute the loss using the score and the label
  loss = loss(score, label)

  # Update the parameters of CE and CTG using backpropagation and optimizer
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2203.05557v2.pdf "Conditional Prompt Learning for Vision-Language Models - arXiv.org"
[2]: https://arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"
[3]: https://arxiv.org/pdf/2209.05557v2 "arXiv:2209.05557v2 [cs.LG] 23 Sep 2022"
[4]: http://export.arxiv.org/abs/2203.05557 "[2203.05557] Conditional Prompt Learning for Vision-Language Models"

Here is a possible pseudo code for the method section of the paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import clip

# Define the pre-trained vision-language model
VLM = clip.load("ViT-B/32", jit=False)[0] # or ALIGN

# Define the prompt template
prompt = "a photo of [class], a type of [context]"

# Define the context encoder
CE = nn.Sequential(
  nn.Linear(768, 768), # or TransformerEncoderLayer
  nn.ReLU(),
  nn.Linear(768, 768)
)

# Define the conditional token generator
CTG = nn.Sequential(
  nn.Conv2d(3, 64, 3, 2, 1), # or TransformerEncoderLayer
  nn.ReLU(),
  nn.Conv2d(64, 128, 3, 2, 1),
  nn.ReLU(),
  nn.Conv2d(128, 256, 3, 2, 1),
  nn.ReLU(),
  nn.Flatten(),
  nn.Linear(256 * 8 * 8, 768)
)

# Initialize the context words randomly or from a vocabulary
context = torch.randn(1, 768) # or clip.tokenize(["pet"])

# Define the loss function
loss = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.Adam([CE.parameters(), CTG.parameters()], lr=0.001)

# Load the training data
data = torchvision.datasets.ImageFolder("data/train", transform=torchvision.transforms.ToTensor())
loader = torch.utils.data.DataLoader(data, batch_size=32, shuffle=True)

# Loop over the training data
for epoch in range(10):
  
  # Loop over the batches
  for image, label in loader:

    # Generate a conditional token for the image using CTG
    token = CTG(image)

    # Encode the context words using CE
    encoded_context = CE(context)

    # Concatenate the token and the encoded context to form a dynamic prompt
    prompt = torch.cat([token, encoded_context], dim=1)

    # Fill in the prompt template with the prompt and the label
    text = fill(prompt, label) # use string formatting

    # Encode the image and the text using VLM
    image_embed, text_embed = VLM.encode_image(image), VLM.encode_text(text)

    # Compute the similarity score between the image and the text embeddings
    score = torch.matmul(image_embed, text_embed.T)

    # Compute the loss using the score and the label
    loss = loss(score, label)

    # Update the parameters of CE and CTG using backpropagation and optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print the loss value every 100 batches
    if batch % 100 == 0:
      print(f"Epoch {epoch}, Batch {batch}, Loss {loss.item()}")
```