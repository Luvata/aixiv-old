---
title: 2210.03919v4 CLIP-PAE  Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation
date: 2022-10-04
---

# [CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation](http://arxiv.org/abs/2210.03919v4)

authors: Chenliang Zhou, Fangcheng Zhong, Cengiz Oztireli


## What, Why and How

[1]: https://arxiv.org/abs/2210.03919 "[2210.03919] CLIP-PAE: Projection-Augmentation Embedding to Extract ..."
[2]: http://export.arxiv.org/abs/2105.03919v4 "[2105.03919v4] Active XY model on a substrate: Density fluctuations and ..."
[3]: http://export.arxiv.org/abs/2104.03919v4 "[2104.03919v4] Dead time duration and active reset influence on the ..."

Here is a summary of the paper:

- **What**: The paper proposes a new method for text-guided image manipulation using CLIP, a model that learns a joint embedding space for images and text. The method is called CLIP-PAE, which stands for Projection-Augmentation Embedding. It aims to extract relevant features for a disentangled, interpretable, and controllable image manipulation.
- **Why**: The paper addresses the problem of using text embeddings as the optimization target for image manipulation, which often introduces undesired artifacts in the resulting images. The paper also seeks to improve the disentanglement, interpretability, and controllability of image manipulation, which are hard to guarantee with existing methods.
- **How**: The paper introduces a new optimization target called PAE, which is computed by projecting the text embedding onto a corpus subspace spanned by relevant prompts that capture specific image characteristics. The paper also augments the PAE with additional constraints to enforce semantic consistency and diversity. The paper demonstrates the effectiveness of CLIP-PAE on text-guided semantic face editing, and shows that it outperforms existing methods in terms of quality, accuracy, disentanglement, interpretability, and controllability.

## Main Contributions

The paper claims the following contributions:

- A novel optimization target for text-guided image manipulation using CLIP, called PAE, which extracts relevant features from the joint embedding space by projecting the text embedding onto a corpus subspace.
- A simple and general paradigm for defining corpus subspaces using relevant prompts that capture specific image characteristics, and a method for augmenting the PAE with additional constraints to enforce semantic consistency and diversity.
- A comprehensive evaluation of CLIP-PAE on text-guided semantic face editing, showing that it achieves state-of-the-art performance in terms of quality, accuracy, disentanglement, interpretability, and controllability.

## Method Summary

[1]: https://arxiv.org/abs/2210.03919 "[2210.03919] CLIP-PAE: Projection-Augmentation Embedding to Extract ..."
[2]: https://arxiv.org/abs/2210.04319 "[2210.04319] Dissecting adaptive methods in GANs - arXiv.org"
[3]: http://export.arxiv.org/abs/2105.03919v4 "[2105.03919v4] Active XY model on a substrate: Density fluctuations and ..."

Here is a summary of the method section of the paper:

- The paper introduces CLIP-PAE, a new optimization target for text-guided image manipulation using CLIP, which is a model that learns a joint embedding space for images and text.
- The paper defines corpus subspaces as linear subspaces spanned by relevant prompts that capture specific image characteristics, such as facial attributes, expressions, poses, etc. The paper uses a simple and general paradigm for defining corpus subspaces using a set of positive and negative prompts for each characteristic.
- The paper computes PAE by projecting the text embedding onto the corpus subspace that corresponds to the desired image characteristic. The paper also augments the PAE with additional constraints to enforce semantic consistency and diversity. The paper uses cosine similarity as the distance metric between embeddings.
- The paper incorporates CLIP-PAE into an existing CLIP-based image manipulation algorithm, called CLIP-Guided Diffusion (CGD), which uses a diffusion model to generate images from noise. The paper replaces the original optimization target of CGD with PAE, and modifies the loss function accordingly.
- The paper evaluates CLIP-PAE on text-guided semantic face editing, which is a task of manipulating facial images according to textual descriptions. The paper uses CelebA-HQ as the dataset, and uses various metrics and human studies to measure the quality, accuracy, disentanglement, interpretability, and controllability of the manipulated images. The paper compares CLIP-PAE with several baselines and ablations, and shows that it outperforms them in most aspects.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define corpus subspaces using positive and negative prompts for each image characteristic
corpus_subspaces = {}
for characteristic in image_characteristics:
  positive_prompts = get_positive_prompts(characteristic)
  negative_prompts = get_negative_prompts(characteristic)
  corpus_subspace = span(positive_prompts) - span(negative_prompts)
  corpus_subspaces[characteristic] = corpus_subspace

# Define a diffusion model to generate images from noise
diffusion_model = DiffusionModel()

# Define a CLIP model to embed images and text into a joint space
clip_model = CLIPModel()

# Define a function to compute PAE by projecting text embedding onto corpus subspace
def compute_pae(text, characteristic):
  text_embedding = clip_model.embed_text(text)
  corpus_subspace = corpus_subspaces[characteristic]
  pae = project(text_embedding, corpus_subspace)
  return pae

# Define a function to augment PAE with additional constraints
def augment_pae(pae, image_embedding, noise_embedding):
  # Enforce semantic consistency between PAE and image embedding
  pae = pae + alpha * cosine_similarity(pae, image_embedding)
  # Enforce semantic diversity between PAE and noise embedding
  pae = pae + beta * cosine_similarity(pae, noise_embedding)
  return pae

# Define a function to manipulate an image according to a text description
def manipulate_image(image, text):
  # Embed the image into the joint space
  image_embedding = clip_model.embed_image(image)
  # Generate a noise image from the diffusion model
  noise_image = diffusion_model.sample_noise()
  # Embed the noise image into the joint space
  noise_embedding = clip_model.embed_image(noise_image)
  # Compute PAE for each image characteristic
  paes = {}
  for characteristic in image_characteristics:
    pae = compute_pae(text, characteristic)
    pae = augment_pae(pae, image_embedding, noise_embedding)
    paes[characteristic] = pae
  # Define a loss function to minimize the distance between PAE and image embedding
  def loss_function(image):
    image_embedding = clip_model.embed_image(image)
    loss = 0
    for characteristic in image_characteristics:
      pae = paes[characteristic]
      loss += cosine_distance(pae, image_embedding)
    return loss
  # Optimize the noise image using the loss function and the diffusion model
  manipulated_image = diffusion_model.optimize(noise_image, loss_function)
  return manipulated_image
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import ddpm

# Define the image characteristics and the corresponding prompts
image_characteristics = ["age", "gender", "hair_color", "eye_color", "skin_color", "emotion", "pose"]
prompts = {
  "age": {
    "positive": ["a young person", "a teenager", "a child"],
    "negative": ["an old person", "a senior citizen", "a middle-aged person"]
  },
  "gender": {
    "positive": ["a woman", "a female", "a girl"],
    "negative": ["a man", "a male", "a boy"]
  },
  "hair_color": {
    "positive": ["blonde hair", "golden hair", "yellow hair"],
    "negative": ["black hair", "brown hair", "red hair"]
  },
  # and so on for the other characteristics
}

# Define the hyperparameters
alpha = 0.1 # weight for semantic consistency
beta = 0.01 # weight for semantic diversity
num_steps = 1000 # number of optimization steps
learning_rate = 0.01 # learning rate for optimization

# Load the pretrained CLIP model
clip_model = clip.load("ViT-B/32")

# Load the pretrained diffusion model
diffusion_model = ddpm.load("celebahq")

# Define a function to embed prompts into the joint space using CLIP
def embed_prompts(prompts):
  prompt_embeddings = []
  for prompt in prompts:
    prompt_tensor = clip.tokenize(prompt)
    prompt_embedding = clip_model.encode_text(prompt_tensor)
    prompt_embeddings.append(prompt_embedding)
  return torch.stack(prompt_embeddings)

# Define a function to compute the span of a set of embeddings using SVD
def compute_span(embeddings):
  u, s, v = torch.svd(embeddings)
  span = torch.matmul(embeddings, v)
  return span

# Define a function to project an embedding onto a subspace using dot product
def project(embedding, subspace):
  projection = torch.sum(embedding * subspace, dim=1)
  return projection

# Define a function to compute cosine similarity between two embeddings using dot product and norm
def cosine_similarity(embedding1, embedding2):
  similarity = torch.dot(embedding1, embedding2) / (torch.norm(embedding1) * torch.norm(embedding2))
  return similarity

# Define a function to compute cosine distance between two embeddings using cosine similarity
def cosine_distance(embedding1, embedding2):
  distance = 1 - cosine_similarity(embedding1, embedding2)
  return distance

# Define corpus subspaces using positive and negative prompts for each image characteristic
corpus_subspaces = {}
for characteristic in image_characteristics:
  positive_prompts = prompts[characteristic]["positive"]
  negative_prompts = prompts[characteristic]["negative"]
  positive_embeddings = embed_prompts(positive_prompts)
  negative_embeddings = embed_prompts(negative_prompts)
  positive_subspace = compute_span(positive_embeddings)
  negative_subspace = compute_span(negative_embeddings)
  corpus_subspace = positive_subspace - negative_subspace
  corpus_subspaces[characteristic] = corpus_subspace

# Define a function to compute PAE by projecting text embedding onto corpus subspace
def compute_pae(text, characteristic):
  text_tensor = clip.tokenize(text)
  text_embedding = clip_model.encode_text(text_tensor)
  corpus_subspace = corpus_subspaces[characteristic]
  pae = project(text_embedding, corpus_subspace)
  return pae

# Define a function to augment PAE with additional constraints
def augment_pae(pae, image_embedding, noise_embedding):
  # Enforce semantic consistency between PAE and image embedding
  pae = pae + alpha * cosine_similarity(pae, image_embedding)
  # Enforce semantic diversity between PAE and noise embedding
  pae = pae + beta * cosine_similarity(pae, noise_embedding)
  return pae

# Define a function to manipulate an image according to a text description
def manipulate_image(image, text):
  # Preprocess the image and convert it to a tensor
  image_tensor = torchvision.transforms.ToTensor()(image)
  
  # Embed the image into the joint space using CLIP
  image_embedding = clip_model.encode_image(image_tensor)

  # Generate a noise image from the diffusion model
  noise_image_tensor = diffusion_model.sample_noise()

   # Embed the noise image into the joint space using CLIP
   noise_embedding = clip_model.encode_image(noise_image_tensor)

  # Compute PAE for each image characteristic
  paes = {}
  for characteristic in image_characteristics:
    pae = compute_pae(text, characteristic)
    pae = augment_pae(pae, image_embedding, noise_embedding)
    paes[characteristic] = pae

  # Define a loss function to minimize the distance between PAE and image embedding
  def loss_function(image_tensor):
    image_embedding = clip_model.encode_image(image_tensor)
    loss = 0
    for characteristic in image_characteristics:
      pae = paes[characteristic]
      loss += cosine_distance(pae, image_embedding)
    return loss

  # Optimize the noise image using the loss function and the diffusion model
  manipulated_image_tensor = diffusion_model.optimize(noise_image_tensor, loss_function, num_steps, learning_rate)

  # Postprocess the manipulated image and convert it to a PIL image
  manipulated_image = torchvision.transforms.ToPILImage()(manipulated_image_tensor)

  return manipulated_image
```