---
title: 2306.00547v2 AvatarStudio  Text-driven Editing of 3D Dynamic Human Head Avatars
date: 2023-06-01
---

# [AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars](http://arxiv.org/abs/2306.00547v2)

authors: Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt


## What, Why and How

[1]: https://arxiv.org/abs/2306.00547 "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars"
[2]: https://arxiv.org/pdf/2306.00547v2.pdf "arXiv.org"
[3]: https://arxiv.org/pdf/2106.00547v2.pdf "arXiv:2106.00547v2 [hep-ph] 1 Aug 2022"
[4]: https://arxiv-export2.library.cornell.edu/pdf/2301.00547v2 "arxiv-export2.library.cornell.edu"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes AvatarStudio, a text-based method for editing the appearance of a dynamic full head avatar captured using neural radiance field (NeRF).
- **Why**: The paper aims to enable the creation of virtual characters with various applications such as extended reality and media production, and to overcome the limitations of existing data modalities for editing head movements.
- **How**: The paper introduces an optimization strategy for incorporating multiple keyframes representing different camera viewpoints and time stamps of a video performance into a single diffusion model. Then, it edits the dynamic NeRF by introducing view-and-time-aware Score Distillation Sampling (VT-SDS) following a model-based guidance approach. Finally, it propagates the edits to remaining time steps via a pretrained deformation network.



## Main Contributions

[1]: https://arxiv.org/abs/2306.00547 "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars"
[2]: https://arxiv.org/pdf/2306.00547v2.pdf "arXiv.org"
[3]: https://arxiv.org/pdf/2106.00547v2.pdf "arXiv:2106.00547v2 [hep-ph] 1 Aug 2022"
[4]: https://arxiv-export2.library.cornell.edu/pdf/2301.00547 "arxiv-export2.library.cornell.edu"

The paper lists the following contributions:

- **A text-based method for editing the appearance of a dynamic full head avatar** that can handle various types of edits such as changing hair color, adding glasses, changing facial features, etc.
- **An optimization strategy for incorporating multiple keyframes into a single diffusion model** that can capture the personalized appearance and expression of the subject across different camera viewpoints and time stamps.
- **A view-and-time-aware Score Distillation Sampling (VT-SDS) technique** that can edit the dynamic NeRF representation following a model-based guidance approach that leverages pretrained models for text-to-image synthesis and face parsing.
- **A pretrained deformation network** that can propagate the edits from the canonical space to the remaining time steps of the video performance, ensuring 3D- and time-consistency of the edits.
- **A comprehensive evaluation** of the proposed method on various datasets and scenarios, showing that it outperforms existing approaches in terms of visual quality, user preference, and edit genuineness.


## Method Summary

[1]: https://arxiv.org/abs/2306.00547 "AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars"
[2]: https://arxiv.org/pdf/2306.00547v2.pdf "arXiv.org"
[3]: https://arxiv.org/pdf/2106.00547v2.pdf "arXiv:2106.00547v2 [hep-ph] 1 Aug 2022"
[4]: https://arxiv-export2.library.cornell.edu/pdf/2301.00547v2 "arxiv-export2.library.cornell.edu"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first captures a dynamic full head performance using a NeRF-based method that reconstructs the 3D geometry and appearance of the subject from a monocular video.
- The paper then edits the appearance of the captured performance using a text-to-image diffusion model that can generate realistic and diverse images from natural language descriptions.
- The paper introduces an optimization strategy for training the diffusion model on multiple keyframes extracted from the video performance, representing different camera viewpoints and time stamps. The optimization minimizes the reconstruction loss between the generated and ground truth images, as well as the perceptual and style losses to preserve the identity and expression of the subject.
- The paper then edits the dynamic NeRF representation using a view-and-time-aware Score Distillation Sampling (VT-SDS) technique that samples from the diffusion model conditioned on the text input, the camera viewpoint, and the time stamp. The sampling follows a model-based guidance approach that leverages pretrained models for text-to-image synthesis and face parsing to provide additional supervision for the editing process.
- The paper then propagates the edits from the canonical space to the remaining time steps of the video performance using a pretrained deformation network that can warp the canonical NeRF according to the head pose and expression of each frame. The deformation network ensures 3D- and time-consistency of the edits by preserving the geometry and motion of the original performance.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a monocular video of a human head performance and a text description of the desired edit
# Output: an edited video of the same performance with the appearance modified according to the text

# Capture the dynamic full head performance using NeRF
neural_radiance_field = capture_neural_radiance_field(video)

# Extract keyframes from the video representing different camera viewpoints and time stamps
keyframes = extract_keyframes(video)

# Train a text-to-image diffusion model on the keyframes using an optimization strategy
diffusion_model = train_diffusion_model(keyframes)

# Edit the dynamic NeRF representation using VT-SDS following a model-based guidance approach
edited_neural_radiance_field = edit_neural_radiance_field(neural_radiance_field, diffusion_model, text)

# Propagate the edits to the remaining time steps using a pretrained deformation network
edited_video = propagate_edits(edited_neural_radiance_field, deformation_network)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: a monocular video of a human head performance and a text description of the desired edit
# Output: an edited video of the same performance with the appearance modified according to the text

# Capture the dynamic full head performance using NeRF
# Adapted from https://github.com/yenchenlin/nerf-pytorch/blob/master/run_nerf.py

# Define the NeRF model as a multi-layer perceptron (MLP) with skip connections
def NeRF_MLP(input_size, hidden_size, output_size, num_layers):
  # Initialize the MLP layers and weights
  layers = []
  for i in range(num_layers):
    if i == 0:
      # Input layer
      layers.append(nn.Linear(input_size, hidden_size))
    elif i == num_layers - 1:
      # Output layer
      layers.append(nn.Linear(hidden_size, output_size))
    else:
      # Hidden layer
      layers.append(nn.Linear(hidden_size, hidden_size))
      # Skip connection every 4 layers
      if i % 4 == 0:
        layers.append(nn.Linear(input_size, hidden_size))
    # Apply ReLU activation except for the last layer
    if i < num_layers - 1:
      layers.append(nn.ReLU())
  # Return the MLP model
  return nn.Sequential(*layers)

# Define the NeRF model that takes 5D input (3D location and 2D direction) and outputs 4D vector (RGB color and density)
nerf_model = NeRF_MLP(input_size=5, hidden_size=256, output_size=4, num_layers=8)

# Define the positional encoding function that maps the input to a higher dimensional space
def positional_encoding(input, num_freqs):
  # Compute the sinusoidal functions for different frequencies
  freqs = 2.0 ** torch.linspace(0.0, num_freqs - 1, num_freqs)
  encodings = input.unsqueeze(-1) * freqs.unsqueeze(0)
  encodings = torch.cat([encodings.sin(), encodings.cos()], dim=-1)
  # Return the flattened encodings
  return encodings.reshape(encodings.shape[0], -1)

# Define the ray marching function that samples points along each ray and computes the accumulated color and density
def ray_marching(rays_o, rays_d, near, far, num_samples):
  # Sample points along each ray between near and far bounds
  z_vals = torch.linspace(near, far, num_samples).expand(rays_o.shape[0], num_samples)
  points = rays_o.unsqueeze(1) + rays_d.unsqueeze(1) * z_vals.unsqueeze(2)
  
  # Apply positional encoding to the sampled points and directions
  points_enc = positional_encoding(points, num_freqs=10)
  rays_d_enc = positional_encoding(rays_d, num_freqs=4)

  # Concatenate the encoded points and directions and pass them to the NeRF model
  inputs = torch.cat([points_enc, rays_d_enc.unsqueeze(1).expand(-1, num_samples, -1)], dim=-1)
  outputs = nerf_model(inputs)

  # Extract the RGB color and density from the outputs
  rgb = torch.sigmoid(outputs[..., :3])
  density = F.relu(outputs[..., 3])

  # Compute the alpha value for each sample along the ray
  delta = z_vals[..., 1:] - z_vals[..., :-1]
  alpha = 1.0 - torch.exp(-density * delta)

  # Compute the accumulated color and density along the ray using alpha compositing
  weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.0 - alpha + EPS], dim=-1), dim=-1)
  
  color = torch.sum(weights.unsqueeze(-1) * rgb, dim=-2)
  
  depth = torch.sum(weights * z_vals, dim=-1)

  return color, depth

# Define the loss function that measures the reconstruction error between the predicted and ground truth colors
def loss_function(color_pred, color_gt):
  return ((color_pred - color_gt) ** 2).mean()

# Define the optimizer for updating the NeRF model parameters
optimizer = torch.optim.Adam(nerf_model.parameters(), lr=5e-4)

# Load the video frames as a tensor of shape [num_frames, height, width, channels]
video_frames = load_video(video)

# Extract camera parameters (intrinsics and extrinsics) for each frame using COLMAP
camera_params = extract_camera_params(video_frames)

# Loop over the training iterations
for iter in range(num_iters):

  # Sample a batch of rays from the video frames
  rays_o, rays_d, color_gt = sample_rays(video_frames, camera_params)

  # Perform ray marching to predict the color for each ray
  color_pred, _ = ray_marching(rays_o, rays_d, near=2.0, far=6.0, num_samples=128)

  # Compute the loss between the predicted and ground truth colors
  loss = loss_function(color_pred, color_gt)

  # Update the NeRF model parameters using backpropagation and gradient descent
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

# Return the neural radiance field representation of the video
neural_radiance_field = nerf_model

# Edit the appearance of the captured performance using a text-to-image diffusion model
# Adapted from https://github.com/openai/guided-diffusion/blob/master/guided_diffusion/scripts/fine_tune.py

# Define the diffusion model as a U-Net with attention and noise embeddings
def Diffusion_UNet(input_channels, output_channels, hidden_channels, num_res_blocks):
  # Initialize the U-Net layers and weights
  layers = []
  # Encoder
  for i in range(num_res_blocks):
    if i == 0:
      # Input layer
      layers.append(nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1))
    else:
      # Downsample layer
      layers.append(nn.Conv2d(hidden_channels, hidden_channels * 2, kernel_size=3, stride=2, padding=1))
      hidden_channels *= 2
    # Residual block with attention
    layers.append(ResBlock(hidden_channels))
    layers.append(Attention(hidden_channels))
  # Decoder
  for i in range(num_res_blocks):
    # Residual block with attention
    layers.append(ResBlock(hidden_channels))
    layers.append(Attention(hidden_channels))
    if i < num_res_blocks - 1:
      # Upsample layer
      layers.append(nn.ConvTranspose2d(hidden_channels, hidden_channels // 2, kernel_size=4, stride=2, padding=1))
      hidden_channels //= 2
  # Output layer
  layers.append(nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1))
  # Return the U-Net model
  return nn.Sequential(*layers)

# Define the diffusion model that takes an image and predicts the mean and variance of a Gaussian distribution for each pixel
diffusion_model = Diffusion_UNet(input_channels=3, output_channels=6, hidden_channels=64, num_res_blocks=4)

# Define the noise embedding function that maps a noise level to a feature vector
def noise_embedding(noise_level):
  # Initialize the noise embedding layer and weight
  layer = nn.Embedding(num_embeddings=128, embedding_dim=64)
  # Return the embedded feature vector for the given noise level
  return layer(noise_level)

# Define the text encoder function that maps a text description to a feature vector
def text_encoder(text):
  # Initialize the CLIP model and weight
  clip_model = clip.load("ViT-B/32", jit=False)[0].eval()
  # Encode the text description using CLIP and return the feature vector
  return clip_model.encode_text(clip.tokenize(text))

# Define the loss function that measures the KL divergence between the predicted and target Gaussian distributions for each pixel
def loss_function(mean_pred, logvar_pred, mean_target, logvar_target):
  return F.kl_div(mean_pred + logvar_pred / 2 - mean_target - logvar_target / 2 + (logvar_target.exp() + (mean_target - mean_pred) ** 2) / (2 * logvar_pred.exp()) - 0.5)

# Define the optimizer for updating the diffusion model parameters
optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=1e-4)

# Extract keyframes from the video representing different camera viewpoints and time stamps
keyframes = extract_keyframes(video)

# Loop over the training iterations
for iter in range(num_iters):

  # Sample a batch of keyframes and their corresponding camera parameters
  keyframes_batch, camera_params_batch = sample_keyframes(keyframes)

  # Sample a noise level from a discrete uniform distribution
  noise_level = torch.randint(low=0, high=128)

  # Add Gaussian noise to the keyframes according to the noise level
  noisy_keyframes = keyframes_batch + torch.randn_like(keyframes_batch) * math.sqrt(noise_level / (128 - noise_level))

  # Apply positional encoding to the noisy keyframes
  noisy_keyframes_enc = positional_encoding(noisy_keyframes)

  # Concatenate the encoded noisy keyframes with the noise embeddings and pass them to the diffusion model
  inputs = torch.cat(