---
title: 2306.00409v1 Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting
date: 2023-06-01
---

# [Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting](http://arxiv.org/abs/2306.00409v1)

authors: Shubin Huang, Qiong Wu, Yiyi Zhou, Weijie Chen, Rongsheng Zhang, Xiaoshuai Sun, Rongrong Ji


## What, Why and How

[1]: https://arxiv.org/abs/2306.00409 "[2306.00409] Adapting Pre-trained Language Models to Vision-Language ..."
[2]: https://arxiv.org/abs/2306.00804 "[2306.00804] Adaptive Contextual Biasing for Transducer Based Streaming ..."
[3]: http://export.arxiv.org/abs/2305.00409v1 "[2305.00409v1] Segre embedding and biharmonicity"

Here is a summary of the paper:

- **What**: The paper proposes a novel transfer learning approach for pre-trained language models (PLMs) to adapt to vision-language (VL) tasks, such as visual question answering and image captioning, without requiring an additional fusion network for VL reasoning.
- **Why**: The paper aims to address the challenges of excessive memory overhead, redundancy in visual information, and suboptimal placement of prompt tokens when using PLMs as a stand-alone model for VL tasks.
- **How**: The paper introduces Dynamic Visual Prompting (DVP), which consists of three main components: 
    - A cross-attention module that generates text-related and compact visual prompt tokens from the processed visual features, reducing the input length of PLMs.
    - A reinforcement-learning based search algorithm that automatically finds the optimal placement of prompt tokens for different VL tasks, enhancing the performance of PLMs.
    - An adapter approach that keeps most parameters of PLMs intact when adapting to VL tasks, enabling a quick shift between single- and multi-modal tasks.

The paper applies DVP to two representative PLMs, namely BERT and T5, and conducts extensive experiments on a set of VL reasoning benchmarks, showing the advantage of DVP on efficiency and performance.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel transfer learning approach for PLMs to adapt to VL tasks via dynamic visual prompting, which can reduce the memory overhead and improve the performance of PLMs as a stand-alone model for VL reasoning.
- It introduces a cross-attention module that can generate text-related and compact visual prompt tokens from the processed visual features, which can greatly reduce the input length of PLMs and avoid redundancy in visual information.
- It devises a reinforcement-learning based search algorithm that can automatically find the optimal placement of prompt tokens for different VL tasks, which can enhance the performance of PLMs and avoid manual tuning of prompt tokens.
- It experiments with an adapter approach that can keep most parameters of PLMs intact when adapting to VL tasks, which can enable a quick shift between single- and multi-modal tasks and preserve the generalization ability of PLMs.
- It applies DVP to two representative PLMs, namely BERT and T5, and conducts extensive experiments on a set of VL reasoning benchmarks, showing the advantage of DVP on efficiency and performance over existing methods.

## Method Summary

Here is a summary of the method section of the paper:

- The paper first reviews the background of PLMs and prompt tuning, and defines the problem of adapting PLMs to VL tasks via dynamic visual prompting.
- The paper then introduces the cross-attention module that generates text-related and compact visual prompt tokens from the processed visual features. The module consists of a visual encoder, a text encoder, and a cross-attention layer that computes the attention weights between the visual and text features. The module then selects the top-k visual features with the highest attention weights as the visual prompt tokens, and projects them onto the semantic space of PLMs using a linear transformation.
- The paper next describes the reinforcement-learning based search algorithm that finds the optimal placement of prompt tokens for different VL tasks. The algorithm treats the placement as a discrete action space, and uses a policy network to learn a policy that maximizes the expected reward. The reward is defined as the performance of PLMs on a validation set after fine-tuning with the given placement. The algorithm uses policy gradient methods to update the policy network parameters, and employs an entropy regularization term to encourage exploration.
- The paper finally discusses the adapter approach that keeps most parameters of PLMs intact when adapting to VL tasks. The approach inserts adapter layers between the original layers of PLMs, and only fine-tunes the adapter parameters and the prompt token embeddings. The approach can reduce the number of parameters to be fine-tuned, and preserve the generalization ability of PLMs.

## Pseudo Code

Here is the detailed pseudo code to implement this paper:

```python
# Import the required libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from transformers import BertModel, T5Model, BertTokenizer, T5Tokenizer

# Define the hyperparameters
num_prompt_tokens = 10 # The number of visual prompt tokens
num_adapter_layers = 2 # The number of adapter layers
adapter_size = 64 # The hidden size of adapter layers
num_policy_layers = 2 # The number of policy network layers
policy_size = 32 # The hidden size of policy network layers
num_actions = 4 # The number of possible placements for prompt tokens
learning_rate = 1e-4 # The learning rate for fine-tuning and policy learning
entropy_coef = 0.01 # The coefficient for entropy regularization
gamma = 0.99 # The discount factor for reward calculation

# Load the pre-trained language model and tokenizer
plm_name = "bert-base-uncased" # or "t5-base"
plm = BertModel.from_pretrained(plm_name) # or T5Model.from_pretrained(plm_name)
tokenizer = BertTokenizer.from_pretrained(plm_name) # or T5Tokenizer.from_pretrained(plm_name)

# Define the cross-attention module
class CrossAttention(nn.Module):
    def __init__(self, plm):
        super(CrossAttention, self).__init__()
        self.plm = plm # The pre-trained language model
        self.visual_encoder = nn.Linear(2048, plm.config.hidden_size) # The visual encoder that transforms the visual features to the same dimension as the PLM hidden size
        self.text_encoder = nn.Linear(plm.config.hidden_size, plm.config.hidden_size) # The text encoder that transforms the text features to the same dimension as the PLM hidden size
        self.cross_attention = nn.MultiheadAttention(plm.config.hidden_size, plm.config.num_attention_heads) # The cross-attention layer that computes the attention weights between the visual and text features
    
    def forward(self, visual_features, text_features):
        # visual_features: a tensor of shape (batch_size, num_visual_features, 2048), containing the processed visual features from a CNN backbone
        # text_features: a tensor of shape (batch_size, num_text_features, plm.config.hidden_size), containing the output of the PLM encoder
        
        # Project the visual features to the same dimension as the PLM hidden size
        visual_features = self.visual_encoder(visual_features) # shape: (batch_size, num_visual_features, plm.config.hidden_size)
        
        # Project the text features to the same dimension as the PLM hidden size
        text_features = self.text_encoder(text_features) # shape: (batch_size, num_text_features, plm.config.hidden_size)
        
        # Transpose the features for cross-attention computation
        visual_features = visual_features.transpose(0, 1) # shape: (num_visual_features, batch_size, plm.config.hidden_size)
        text_features = text_features.transpose(0, 1) # shape: (num_text_features, batch_size, plm.config.hidden_size)
        
        # Compute the cross-attention weights between the visual and text features
        cross_attn_output, cross_attn_weights = self.cross_attention(visual_features, text_features, text_features) # shape: (num_visual_features, batch_size, plm.config.hidden_size), (batch_size, num_visual_features, num_text_features)
        
        # Select the top-k visual features with the highest attention weights as the visual prompt tokens
        topk_indices = cross_attn_weights.topk(num_prompt_tokens, dim=1)[1] # shape: (batch_size, num_prompt_tokens)
        prompt_tokens = torch.gather(visual_features, 0, topk_indices.unsqueeze(-1).expand(-1,-1,cross_attn_output.size(-1))) # shape: (batch_size, num_prompt_tokens, plm.config.hidden_size)
        
        return prompt_tokens

# Define the policy network
class PolicyNetwork(nn.Module):
    def __init__(self):
        super(PolicyNetwork, self).__init__()
        self.layers = nn.ModuleList() # A list of linear layers for policy network
        self.layers.append(nn.Linear(num_prompt_tokens * plm.config.hidden_size + plm.config.hidden_size * num_actions + num_actions + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1, policy_size)) # The first layer that takes the concatenated features of prompt tokens, text tokens, placement indicators, task type, and input length as input
        for _ in range(num_policy_layers - 2):
            self.layers.append(nn.Linear(policy_size, policy_size)) # The intermediate layers
        self.layers.append(nn.Linear(policy_size, num_actions)) # The output layer that predicts the action probabilities
    
    def forward(self, prompt_tokens, text_tokens, placement_indicators, task_type, input_length):
        # prompt_tokens: a tensor of shape (batch_size, num_prompt_tokens, plm.config.hidden_size), containing the visual prompt tokens from the cross-attention module
        # text_tokens: a tensor of shape (batch_size, num_text_tokens, plm.config.hidden_size), containing the text tokens from the PLM encoder
        # placement_indicators: a tensor of shape (batch_size, num_actions), containing the binary indicators of whether each placement is valid or not
        # task_type: a tensor of shape (batch_size,), containing the task type of the current batch (0 for VQA, 1 for GQA, 2 for SNLIVE)
        # input_length: a tensor of shape (batch_size,), containing the input length of the current batch
        
        # Flatten the prompt tokens and text tokens
        prompt_tokens = prompt_tokens.view(prompt_tokens.size(0), -1) # shape: (batch_size, num_prompt_tokens * plm.config.hidden_size)
        text_tokens = text_tokens.view(text_tokens.size(0), -1) # shape: (batch_size, num_text_tokens * plm.config.hidden_size)
        
        # Concatenate the features
        features = torch.cat([prompt_tokens, text_tokens, placement_indicators, task_type.unsqueeze(-1), input_length.unsqueeze(-1)], dim=1) # shape: (batch_size, num_prompt_tokens * plm.config.hidden_size + plm.config.hidden_size * num_actions + num_actions + 1 + 1)
        
        # Pass through the policy network layers
        for layer in self.layers[:-1]:
            features = F.relu(layer(features)) # shape: (batch_size, policy_size)
        action_probs = F.softmax(self.layers[-1](features), dim=1) # shape: (batch_size, num_actions)
        
        return action_probs

# Define the adapter module
class Adapter(nn.Module):
    def __init__(self):
        super(Adapter, self).__init__()
        self.down_project = nn.Linear(plm.config.hidden_size, adapter_size) # The down projection layer that reduces the dimension of the PLM hidden states
        self.up_project = nn.Linear(adapter_size, plm.config.hidden_size) # The up projection layer that restores the dimension of the PLM hidden states
    
    def forward(self, hidden_states):
        # hidden_states: a tensor of shape (batch_size, seq_length, plm.config.hidden_size), containing the PLM hidden states
        
        # Pass through the adapter layers
        hidden_states = self.down_project(hidden_states) # shape: (batch_size, seq_length, adapter_size)
        hidden_states = F.gelu(hidden_states) # shape: (batch_size, seq_length, adapter_size)
        hidden_states = self.up_project(hidden_states) # shape: (batch_size, seq_length, plm.config.hidden_size)
        
        return hidden_states

# Define the DVP model
class DVP(nn.Module):
    def __init__(self):
        super(DVP, self).__init__()
        self.plm = plm # The pre-trained language model
        self.cross_attention = CrossAttention(plm) # The cross-attention module
        self.policy_network = PolicyNetwork() # The policy network
        self.adapters = nn.ModuleList() # A list of adapter modules
        for _ in range(num_adapter_layers):
            self.adapters.append(Adapter())
    
    def forward(self, visual_features, input_ids):
        # visual_features: a tensor of shape (batch_size, num_visual_features, 2048), containing the processed visual features from a CNN backbone
        # input_ids: a tensor of shape (batch_size, seq_length), containing the input ids of text tokens
        
        # Encode the text tokens using the PLM encoder
        text_features = self.plm(input_ids).last_hidden_state # shape: (batch_size, seq_length, plm.config.hidden_size)
        
        # Generate the visual prompt tokens using the cross-attention module
        prompt_tokens = self.cross_attention(visual_features, text_features) # shape: (batch_size, num_prompt_tokens, plm.config.hidden_size)
        
        # Compute the placement indicators based on the input length and task type
        input_length = torch.sum(input_ids != tokenizer.pad_token_id) # shape: ()
        task_type = torch.tensor(0 if "question" in tokenizer.decode(input_ids[0]) else 1 if

```