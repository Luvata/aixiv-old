---
title: 2306.00034v1 Diagnosis and Prognosis of Head and Neck Cancer Patients using Artificial Intelligence
date: 2023-06-01
---

# [Diagnosis and Prognosis of Head and Neck Cancer Patients using Artificial Intelligence](http://arxiv.org/abs/2306.00034v1)

authors: Ikboljon Sobirov


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00034v1 "Diagnosis and Prognosis of Head and Neck Cancer Patients using ..."
[2]: https://arxiv.org/abs/2306.00034 "[2306.00034] Diagnosis and Prognosis of Head and Neck ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00034 "[2306.00034] Diagnosis and Prognosis of Head and Neck Cancer Patients ..."

Here is a summary of the paper:

- **What**: The paper presents two approaches for head and neck (H&N) cancer tumor segmentation using artificial intelligence, and two architectures for patient prognosis prediction using medical imaging and clinical data.
- **Why**: The paper aims to help clinicians with the diagnosis and prognosis of H&N cancer patients, which is a challenging and time-consuming task that requires manual analysis of multiple sources of information.
- **How**: The paper explores and compares vision transformer (ViT)-based and convolutional neural network-based models for tumor segmentation, and proposes a novel 2D perspective to working with 3D data. The paper also proposes an ensemble of several models that won the HECKTOR 2021 challenge prognosis task, and a ViT-based framework that concurrently performs patient outcome prediction and tumor segmentation, which outperforms the ensemble model.

## Main Contributions

The contributions of this paper are:

- It provides a comprehensive study of ViT-based and CNN-based models for H&N tumor segmentation, and shows that ViT-based models can achieve comparable or better results than CNN-based models with fewer parameters and less training data.
- It introduces a novel 2D perspective to working with 3D data for tumor segmentation, which reduces the computational complexity and memory requirements of the models, and improves the segmentation performance.
- It proposes two new architectures for patient prognosis prediction using medical imaging and clinical data, one of which won the HECKTOR 2021 challenge prognosis task, and the other one which concurrently performs patient outcome prediction and tumor segmentation, and outperforms the ensemble model.

## Method Summary

The method section of the paper consists of four subsections:

- **Data**: The paper describes the data sources and preprocessing steps for the tumor segmentation and prognosis prediction tasks. The paper uses two public datasets: HECKTOR 2020 and HECKTOR 2021, which contain CT and PET scans of H&N cancer patients, along with clinical data and survival outcomes. The paper applies several preprocessing steps such as resampling, cropping, normalization, and augmentation to the data.
- **Tumor Segmentation**: The paper presents two approaches for tumor segmentation: (i) exploration and comparison of ViT-based and CNN-based models; and (ii) proposal of a novel 2D perspective to working with 3D data. The paper evaluates six models for the first approach: UNet, UNet++, ResUNet++, ViT-Base, ViT-Small, and ViT-Tiny. The paper proposes a novel method for the second approach, which converts 3D data into 2D slices along different axes, and trains a single ViT model on the concatenated slices. The paper uses dice similarity coefficient (DSC) as the main metric for tumor segmentation.
- **Prognosis Prediction**: The paper proposes two new architectures for patient prognosis prediction using medical imaging and clinical data: (i) an ensemble of several models that won the HECKTOR 2021 challenge prognosis task; and (ii) a ViT-based framework that concurrently performs patient outcome prediction and tumor segmentation. The paper uses four models for the ensemble approach: ResNet-50, ResNet-101, ResNet-152, and DenseNet-201. The paper uses a modified version of ViT-Tiny for the concurrent approach, which takes both CT and PET scans as inputs, and outputs both a tumor mask and a survival probability. The paper uses concordance index (CI) as the main metric for prognosis prediction.
- **Implementation Details**: The paper provides the details of the model architectures, hyperparameters, optimization methods, loss functions, and evaluation protocols for both tumor segmentation and prognosis prediction tasks. The paper also reports the hardware specifications and running times of the experiments.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Data preprocessing
for each dataset in [HECKTOR 2020, HECKTOR 2021]:
  resample CT and PET scans to 1mm isotropic resolution
  crop scans to a fixed size of 144x144x144 voxels
  normalize scans by subtracting mean and dividing by standard deviation
  augment scans by random flipping, rotation, scaling, and elastic deformation

# Tumor segmentation
# Approach 1: ViT-based vs CNN-based models
for each model in [UNet, UNet++, ResUNet++, ViT-Base, ViT-Small, ViT-Tiny]:
  initialize model with random weights or pre-trained weights
  define loss function as weighted sum of dice loss and cross entropy loss
  define optimizer as Adam with learning rate of 1e-4 and weight decay of 1e-5
  train model for 100 epochs with batch size of 2 on HECKTOR 2020 dataset
  evaluate model on HECKTOR 2020 test set and report DSC

# Approach 2: Novel 2D perspective to working with 3D data
initialize ViT-Tiny model with pre-trained weights
define loss function as weighted sum of dice loss and cross entropy loss
define optimizer as Adam with learning rate of 1e-4 and weight decay of 1e-5
for each epoch in range(100):
  for each scan in HECKTOR 2020 dataset:
    slice scan along three axes (axial, sagittal, coronal) with step size of 4mm
    concatenate slices along each axis into a single image
    stack three concatenated images along channel dimension
    feed stacked image to ViT-Tiny model and compute loss
    update model parameters using optimizer
evaluate model on HECKTOR 2020 test set and report DSC

# Prognosis prediction
# Approach 1: Ensemble of several models
for each model in [ResNet-50, ResNet-101, ResNet-152, DenseNet-201]:
  initialize model with pre-trained weights
  define loss function as Cox proportional hazards loss
  define optimizer as Adam with learning rate of 1e-4 and weight decay of 1e-5
  train model for 100 epochs with batch size of 16 on HECKTOR 2021 dataset
  evaluate model on HECKTOR 2021 test set and report CI

# Approach 2: ViT-based framework for concurrent outcome prediction and tumor segmentation
initialize ViT-Tiny model with pre-trained weights
modify ViT-Tiny model to have two heads: one for segmentation and one for prognosis
define loss function as weighted sum of dice loss, cross entropy loss, and Cox proportional hazards loss
define optimizer as Adam with learning rate of 1e-4 and weight decay of 1e-5
train model for 100 epochs with batch size of 2 on HECKTOR 2021 dataset
evaluate model on HECKTOR 2021 test set and report DSC and CI

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.models as models
import timm # for vision transformer models
import nibabel as nib # for loading medical images
import albumentations as A # for data augmentation
from lifelines.utils import concordance_index # for prognosis evaluation

# Define constants
CT_MEAN = -300 # mean value for CT scans normalization
CT_STD = 300 # standard deviation value for CT scans normalization
PET_MEAN = 2.5 # mean value for PET scans normalization
PET_STD = 2.5 # standard deviation value for PET scans normalization
SEGMENTATION_WEIGHT = 0.8 # weight for segmentation loss in concurrent model
PROGNOSIS_WEIGHT = 0.2 # weight for prognosis loss in concurrent model

# Define data preprocessing functions
def resample(scan, new_spacing=(1,1,1)):
  # Resample a scan to a new spacing using linear interpolation
  # scan: a nibabel image object
  # new_spacing: a tuple of three numbers indicating the new spacing in mm
  # return: a resampled nibabel image object

  # Get the original shape and spacing of the scan
  shape = scan.shape
  spacing = scan.header.get_zooms()

  # Compute the new shape based on the new spacing and the original shape and spacing
  new_shape = np.round(shape * spacing / new_spacing).astype(int)

  # Get the affine transformation matrix of the scan
  affine = scan.affine

  # Create a new affine matrix with the new spacing
  new_affine = np.copy(affine)
  new_affine[:3,:3] = affine[:3,:3] * spacing / new_spacing

  # Resample the scan using linear interpolation and the new affine matrix
  data = scan.get_fdata()
  resampled_data = F.interpolate(torch.from_numpy(data).unsqueeze(0).unsqueeze(0), size=new_shape, mode='trilinear', align_corners=True).squeeze(0).squeeze(0).numpy()
  resampled_scan = nib.Nifti1Image(resampled_data, new_affine)

  return resampled_scan

def crop(scan, mask, size=(144,144,144)):
  # Crop a scan and its corresponding mask to a fixed size centered at the tumor region
  # scan: a nibabel image object of CT or PET scan
  # mask: a nibabel image object of tumor mask
  # size: a tuple of three numbers indicating the desired size in voxels
  # return: a cropped nibabel image object of scan and mask

  # Get the data arrays of the scan and mask
  scan_data = scan.get_fdata()
  mask_data = mask.get_fdata()

  # Find the bounding box coordinates of the tumor region in the mask
  x_min, x_max = np.where(np.any(mask_data, axis=(1,2)))[0][[0,-1]]
  y_min, y_max = np.where(np.any(mask_data, axis=(0,2)))[0][[0,-1]]
  z_min, z_max = np.where(np.any(mask_data, axis=(0,1)))[0][[0,-1]]

  # Compute the center coordinates of the tumor region
  x_center = (x_min + x_max) // 2
  y_center = (y_min + y_max) // 2
  z_center = (z_min + z_max) // 2

  # Compute the half size of the desired size
  x_half = size[0] // 2
  y_half = size[1] // 2
  z_half = size[2] // 2

  # Compute the start and end coordinates of the cropping region
  x_start = max(0, x_center - x_half)
  x_end = min(scan.shape[0], x_center + x_half)
  y_start = max(0, y_center - y_half)
  y_end = min(scan.shape[1], y_center + y_half)
  z_start = max(0, z_center - z_half)
  z_end = min(scan.shape[2], z_center + z_half)

  # Crop the scan and mask data using the computed coordinates
  cropped_scan_data = scan_data[x_start:x_end, y_start:y_end, z_start:z_end]
  cropped_mask_data = mask_data[x_start:x_end, y_start:y_end, z_start:z_end]

  # Pad the cropped data with zeros if the size is smaller than the desired size
  if cropped_scan_data.shape != size:
    pad_width = [(x_half - (x_center - x_start), x_half - (x_end - x_center)),
                 (y_half - (y_center - y_start), y_half - (y_end - y_center)),
                 (z_half - (z_center - z_start), z_half - (z_end - z_center))]
    cropped_scan_data = np.pad(cropped_scan_data, pad_width, mode='constant', constant_values=0)
    cropped_mask_data = np.pad(cropped_mask_data, pad_width, mode='constant', constant_values=0)

  # Create new nibabel image objects with the cropped data and the original affine matrix
  cropped_scan = nib.Nifti1Image(cropped_scan_data, scan.affine)
  cropped_mask = nib.Nifti1Image(cropped_mask_data, mask.affine)

  return cropped_scan, cropped_mask

def normalize(scan, mean, std):
  # Normalize a scan by subtracting mean and dividing by standard deviation
  # scan: a nibabel image object of CT or PET scan
  # mean: a number indicating the mean value for normalization
  # std: a number indicating the standard deviation value for normalization
  # return: a normalized nibabel image object of scan

  # Get the data array of the scan
  scan_data = scan.get_fdata()

  # Normalize the scan data using the given mean and std values
  normalized_scan_data = (scan_data - mean) / std

  # Create a new nibabel image object with the normalized data and the original affine matrix
  normalized_scan = nib.Nifti1Image(normalized_scan_data, scan.affine)

  return normalized_scan

def augment(scan, mask):
  # Augment a scan and its corresponding mask using random transformations
  # scan: a nibabel image object of CT or PET scan
  # mask: a nibabel image object of tumor mask
  # return: an augmented nibabel image object of scan and mask

  # Define the augmentation pipeline using albumentations library
  augmentation = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.ElasticTransform(alpha=10, sigma=3, alpha_affine=3, p=0.5)
  ])

  # Get the data arrays of the scan and mask
  scan_data = scan.get_fdata()
  mask_data = mask.get_fdata()

  # Augment the scan and mask data using the same random transformations
  augmented_data = augmentation(image=scan_data, mask=mask_data)

  # Get the augmented scan and mask data from the augmented data dictionary
  augmented_scan_data = augmented_data['image']
  augmented_mask_data = augmented_data['mask']

  # Create new nibabel image objects with the augmented data and the original affine matrix
  augmented_scan = nib.Nifti1Image(augmented_scan_data, scan.affine)
  augmented_mask = nib.Nifti1Image(augmented_mask_data, mask.affine)

  return augmented_scan, augmented_mask

# Define tumor segmentation models
class UNet(nn.Module):
  # A standard UNet model with four encoding and decoding blocks
  def __init__(self, in_channels, out_channels):
    super(UNet, self).__init__()
    self.in_channels = in_channels
    self.out_channels = out_channels

    # Define the encoding blocks
    self.encoder1 = self.conv_block(in_channels, 64)
    self.encoder2 = self.conv_block(64, 128)
    self.encoder3 = self.conv_block(128, 256)
    self.encoder4 = self.conv_block(256, 512)

    # Define the max pooling layer
    self.pool = nn.MaxPool3d(2)

    # Define the bottleneck block
    self.bottleneck = self.conv_block(512, 1024)

    # Define the upsampling layer
    self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)

    # Define the decoding blocks
    self.decoder4 = self.conv_block(1024 + 512, 512)
    self.decoder3 = self.conv_block(512 + 256, 256)
    self.decoder2 = self.conv_block(256 + 128, 128)
    self.decoder1 = self.conv_block(128 + 64, 64)

    # Define the output layer
    self.out_conv = nn.Conv3d(64, out_channels, kernel_size=1)

  
  def