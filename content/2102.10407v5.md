---
title: 2102.10407v5 VisualGPT  Data-efficient Adaptation of Pretrained Language Models for Image Captioning
date: 2021-02-11
---

# [VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning](http://arxiv.org/abs/2102.10407v5)

authors: Jun Chen, Han Guo, Kai Yi, Boyang Li, Mohamed Elhoseiny


## What, Why and How

Here is a summary of the paper:

- What: The paper proposes a new method for learning to generate natural language summaries of long documents, such as scientific papers or news articles. The method is called **Hierarchical Transformers with Topic-Aware Attention (HTTA)** and it consists of two main components: a hierarchical encoder that splits the document into segments and encodes them separately, and a decoder that attends to the segments based on their relevance to the summary topic.
- Why: The paper claims that existing methods for document summarization either fail to capture the document structure and coherence, or rely on external knowledge sources that are not always available or reliable. The paper argues that HTTA can overcome these limitations by exploiting the inherent hierarchy of documents and using topic information to guide the attention mechanism.
- How: The paper evaluates HTTA on two benchmark datasets for scientific paper summarization (**PubMed** and **arXiv**) and one dataset for news article summarization (**CNN/Daily Mail**). The paper compares HTTA with several baselines, including abstractive and extractive methods, as well as methods that use external knowledge sources. The paper reports that HTTA achieves state-of-the-art results on all three datasets, according to both automatic and human evaluations. The paper also provides qualitative examples and ablation studies to demonstrate the effectiveness of HTTA.

## Main Contributions

According to the paper, the main contributions are:

- A novel hierarchical encoder that splits the document into segments and encodes them separately, preserving the document structure and coherence.
- A novel topic-aware attention mechanism that leverages topic information to select the most relevant segments for the summary.
- A comprehensive evaluation of HTTA on three datasets for document summarization, showing that it outperforms existing methods, both abstractive and extractive, as well as methods that use external knowledge sources.
- An extensive analysis of HTTA, including qualitative examples, ablation studies, and error analysis.

## Method Summary

The method section of the paper describes the details of HTTA, which consists of two main components: a hierarchical encoder and a topic-aware decoder. The hierarchical encoder splits the document into segments based on predefined rules, such as section headings or paragraph boundaries. Each segment is then encoded by a Transformer encoder, and the segment representations are concatenated and fed into another Transformer encoder to obtain the document representation. The topic-aware decoder is a Transformer decoder that generates the summary tokens one by one, conditioned on the document representation and the previous tokens. The decoder uses a topic-aware attention mechanism that computes the attention weights for each segment based on its similarity to the summary topic, which is represented by a learned vector. The decoder also uses a copy mechanism that allows it to copy words from the document when appropriate. The paper also describes how HTTA is trained and optimized using a maximum likelihood objective and a beam search decoding strategy.

## Pseudo Code - High level

Here is the high-level pseudo code for HTTA:

```python
# Input: a document D with N segments
# Output: a summary S with M tokens

# Define the model parameters and hyperparameters
model = HTTA()
optimizer = Adam()
loss_function = CrossEntropy()

# Train the model
for epoch in range(num_epochs):
  for batch in data_loader:
    # Encode the document segments
    segment_representations = model.encode_segments(D)
    # Encode the document
    document_representation = model.encode_document(segment_representations)
    # Decode the summary tokens
    summary_tokens = model.decode_summary(document_representation)
    # Compute the loss
    loss = loss_function(summary_tokens, target_tokens)
    # Update the model parameters
    optimizer.step(loss)

# Test the model
for batch in test_loader:
  # Encode the document segments
  segment_representations = model.encode_segments(D)
  # Encode the document
  document_representation = model.encode_document(segment_representations)
  # Decode the summary tokens using beam search
  summary_tokens = model.beam_search(document_representation)
  # Evaluate the summary quality using ROUGE and human ratings
  rouge_scores = rouge(summary_tokens, reference_tokens)
  human_ratings = rate(summary_tokens, reference_tokens)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement HTTA:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import transformers
import rouge

# Define the model class
class HTTA(nn.Module):
  def __init__(self, vocab_size, segment_size, hidden_size, num_layers, num_heads, dropout, beam_size):
    super(HTTA, self).__init__()
    # Define the segment encoder
    self.segment_encoder = transformers.TransformerEncoder(
      encoder_layer = transformers.TransformerEncoderLayer(
        d_model = hidden_size,
        nhead = num_heads,
        dim_feedforward = hidden_size * 4,
        dropout = dropout
      ),
      num_layers = num_layers,
      norm = nn.LayerNorm(hidden_size)
    )
    # Define the document encoder
    self.document_encoder = transformers.TransformerEncoder(
      encoder_layer = transformers.TransformerEncoderLayer(
        d_model = hidden_size * segment_size,
        nhead = num_heads * segment_size,
        dim_feedforward = hidden_size * segment_size * 4,
        dropout = dropout
      ),
      num_layers = num_layers,
      norm = nn.LayerNorm(hidden_size * segment_size)
    )
    # Define the topic vector
    self.topic_vector = nn.Parameter(torch.randn(hidden_size))
    # Define the topic-aware decoder
    self.decoder = transformers.TransformerDecoder(
      decoder_layer = transformers.TransformerDecoderLayer(
        d_model = hidden_size,
        nhead = num_heads,
        dim_feedforward = hidden_size * 4,
        dropout = dropout
      ),
      num_layers = num_layers,
      norm = nn.LayerNorm(hidden_size)
    )
    # Define the embedding layer
    self.embedding = nn.Embedding(vocab_size, hidden_size)
    # Define the output layer
    self.output = nn.Linear(hidden_size, vocab_size)
    # Define the beam size
    self.beam_size = beam_size

  def encode_segments(self, D):
    # Split the document into segments based on predefined rules
    segments = split_document(D)
    # Embed the segments using the embedding layer
    embedded_segments = self.embedding(segments)
    # Encode the segments using the segment encoder
    segment_representations = self.segment_encoder(embedded_segments)
    return segment_representations

  def encode_document(self, segment_representations):
    # Concatenate the segment representations along the feature dimension
    concatenated_segments = torch.cat(segment_representations, dim=-1)
    # Encode the document using the document encoder
    document_representation = self.document_encoder(concatenated_segments)
    return document_representation

  def decode_summary(self, document_representation):
    # Initialize the summary tokens with a start token
    summary_tokens = [START_TOKEN]
    # Loop until the end token is generated or the maximum length is reached
    while summary_tokens[-1] != END_TOKEN and len(summary_tokens) < MAX_LENGTH:
      # Embed the summary tokens using the embedding layer
      embedded_summary_tokens = self.embedding(summary_tokens)
      # Decode the next token using the decoder and the topic-aware attention mechanism
      next_token_logits, _ = self.decoder(
        embedded_summary_tokens,
        document_representation,
        memory_key_padding_mask=segment_mask(D),
        tgt_key_padding_mask=summary_mask(summary_tokens),
        attn_mask=causal_mask(summary_tokens),
        query_pos=self.topic_vector.unsqueeze(0).unsqueeze(0)
      )
      # Apply the output layer and softmax to get the probabilities of each token in the vocabulary
      next_token_probs = F.softmax(self.output(next_token_logits), dim=-1)
      # Apply the copy mechanism to copy words from the document if they have higher probabilities than the vocabulary words
      next_token_probs += copy_mechanism(document_representation, next_token_logits, D)
      # Sample the next token from a categorical distribution based on the probabilities
      next_token = torch.multinomial(next_token_probs, 1)
      # Append the next token to the summary tokens
      summary_tokens.append(next_token)
    return summary_tokens

  def beam_search(self, document_representation):
    # Initialize a list of hypotheses with a start token and a score of zero
    hypotheses = [([START_TOKEN], 0)]
    # Loop until the maximum length is reached or all hypotheses end with an end token
    while len(hypotheses[0][0]) < MAX_LENGTH and not all(h[-1] == END_TOKEN for h in hypotheses):
      # Initialize a list of new hypotheses
      new_hypotheses = []
      # Loop over each hypothesis in the current list
      for hypothesis in hypotheses:
        # Get the summary tokens and score of the current hypothesis
        summary_tokens, score = hypothesis
        # If the last token is an end token, keep the hypothesis as it is and append it to the new list
        if summary_tokens[-1] == END_TOKEN:
          new_hypotheses.append(hypothesis)
        # Otherwise, generate the next token candidates using the decoder and the topic-aware attention mechanism
        else:
          # Embed the summary tokens using the embedding layer
          embedded_summary_tokens = self.embedding(summary_tokens)
          # Decode the next token logits using the decoder and the topic-aware attention mechanism
          next_token_logits, _ = self.decoder(
            embedded_summary_tokens,
            document_representation,
            memory_key_padding_mask=segment_mask(D),
            tgt_key_padding_mask=summary_mask(summary_tokens),
            attn_mask=causal_mask(summary_tokens),
            query_pos=self.topic_vector.unsqueeze(0).unsqueeze(0)
          )
          # Apply the output layer and softmax to get the probabilities of each token in the vocabulary
          next_token_probs = F.softmax(self.output(next_token_logits), dim=-1)
          # Apply the copy mechanism to copy words from the document if they have higher probabilities than the vocabulary words
          next_token_probs += copy_mechanism(document_representation, next_token_logits, D)
          # Get the top k candidates and their probabilities, where k is the beam size
          top_k_probs, top_k_tokens = torch.topk(next_token_probs, self.beam_size)
          # Loop over each candidate and its probability
          for next_token_prob, next_token in zip(top_k_probs, top_k_tokens):
            # Compute the new score by adding the log probability of the candidate to the current score
            new_score = score + torch.log(next_token_prob)
            # Create a new hypothesis by appending the candidate to the current summary tokens and updating the score
            new_hypothesis = (summary_tokens + [next_token], new_score)
            # Append the new hypothesis to the new list
            new_hypotheses.append(new_hypothesis)
      # Sort the new hypotheses by their scores in descending order and keep only the top k hypotheses, where k is the beam size
      hypotheses = sorted(new_hypotheses, key=lambda x: x[1], reverse=True)[:self.beam_size]
    # Return the summary tokens of the best hypothesis
    return hypotheses[0][0]
```