---
title: 2303.18080v2 One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models
date: 2023-03-19
---

# [One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models](http://arxiv.org/abs/2303.18080v2)

authors: Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, Stéphane Lathuilière


## What, Why and How

[1]: https://arxiv.org/abs/2303.18080 "[2303.18080] One-shot Unsupervised Domain Adaptation with Personalized ..."
[2]: https://arxiv.org/pdf/2303.18080v2 "arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2303.18080v2 "[2303.18080v2] One-shot Unsupervised Domain Adaptation with ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method for one-shot unsupervised domain adaptation (OSUDA) for semantic segmentation, where the goal is to adapt a model trained on a labeled source domain to a target domain with only one unlabeled image.
- **Why**: OSUDA is a challenging problem because the target domain may have different appearance and scene distribution from the source domain, and existing methods rely on style transfer techniques that only transfer the texture information of the target domain, but not the semantic context.
- **How**: The paper leverages text-to-image diffusion models (e.g., Stable Diffusion) to generate a synthetic target dataset with photo-realistic images that depict the style of the target domain as well as novel scenes in diverse contexts. The text interface allows the user to guide the generation of images towards desired semantic concepts while respecting the original spatial context of the single training image. The paper evaluates the proposed method on standard benchmarks and shows that it surpasses the state-of-the-art OSUDA methods by up to +7.1%.


## Main Contributions

[1]: https://arxiv.org/abs/2303.18080 "[2303.18080] One-shot Unsupervised Domain Adaptation with Personalized ..."
[2]: https://arxiv.org/pdf/2303.18080v2 "arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2303.18080v2 "[2303.18080v2] One-shot Unsupervised Domain Adaptation with ..."

According to the paper[^1^][1], the main contributions are:

- **A novel method for OSUDA** that leverages text-to-image diffusion models to generate a synthetic target dataset with photo-realistic images that depict the style of the target domain as well as novel scenes in diverse contexts.
- **A text interface** that allows the user to guide the generation of images towards desired semantic concepts while respecting the original spatial context of the single training image.
- **Extensive experiments** on standard benchmarks that show that the proposed method surpasses the state-of-the-art OSUDA methods by up to +7.1%.


## Method Summary

[1]: https://arxiv.org/abs/2303.18080 "[2303.18080] One-shot Unsupervised Domain Adaptation with Personalized ..."
[2]: https://arxiv.org/pdf/2303.18080v2 "arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2303.18080v2 "[2303.18080v2] One-shot Unsupervised Domain Adaptation with ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a method called **Data AugmenTation with diffUsion Models (DATUM)** that consists of three main steps: text generation, image generation, and model adaptation.
- **Text generation**: The paper uses a text-to-image diffusion model (e.g., Stable Diffusion) that takes as input a text prompt and outputs an image that matches the text description. The paper generates text prompts by combining semantic concepts from the source domain labels and the target domain image. For example, given a source domain label "car" and a target domain image of a snowy street, the paper generates text prompts such as "a car on a snowy street", "a red car in front of a snowman", or "a car covered with snow".
- **Image generation**: The paper uses the text prompts to generate synthetic images that depict the style of the target domain as well as novel scenes in diverse contexts. The paper uses the original target domain image as a reference to guide the image generation process and preserve the spatial context. For example, given a text prompt "a car on a snowy street" and a target domain image of a snowy street, the paper generates an image that has a car on the same street with similar lighting and perspective.
- **Model adaptation**: The paper uses the synthetic images and their corresponding text prompts as pseudo-labels to adapt the segmentation model from the source domain to the target domain. The paper uses a standard cross-entropy loss to train the model on the synthetic dataset and evaluates it on the target domain test set.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: source domain images X_s and labels Y_s, target domain image X_t
# Output: adapted segmentation model M

# Step 1: Text generation
T = [] # list of text prompts
for each label y in Y_s:
  # generate text prompt t by combining y and X_t
  t = generate_text(y, X_t)
  # append t to T
  T.append(t)

# Step 2: Image generation
X = [] # list of synthetic images
for each text prompt t in T:
  # generate synthetic image x by using t and X_t as reference
  x = generate_image(t, X_t)
  # append x to X

# Step 3: Model adaptation
# initialize segmentation model M with source domain weights
M = initialize_model(X_s, Y_s)
# train M on synthetic dataset X and T
M = train_model(M, X, T)
# evaluate M on target domain test set
evaluate_model(M, X_t)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np
import random
import nltk
import stable_diffusion

# Input: source domain images X_s and labels Y_s, target domain image X_t
# Output: adapted segmentation model M

# Hyperparameters
batch_size = 16 # number of images per batch
num_epochs = 10 # number of epochs for training
num_classes = 19 # number of segmentation classes
lr = 0.01 # learning rate for optimizer
beta = 0.999 # momentum for optimizer

# Step 1: Text generation
T = [] # list of text prompts
for each label y in Y_s:
  # generate text prompt t by combining y and X_t
  # use nltk to tokenize and tag X_t
  tokens = nltk.word_tokenize(X_t)
  tags = nltk.pos_tag(tokens)
  # randomly select a noun phrase from tags
  noun_phrase = random.choice([token for token, tag in tags if tag.startswith('NN')])
  # concatenate y and noun_phrase with a preposition
  t = y + ' ' + random.choice(['on', 'in', 'near', 'behind', 'above']) + ' ' + noun_phrase
  # append t to T
  T.append(t)

# Step 2: Image generation
X = [] # list of synthetic images
# load Stable Diffusion model from https://github.com/openai/stable-diffusion/
model = stable_diffusion.load_model()
for each text prompt t in T:
  # generate synthetic image x by using t and X_t as reference
  # use Stable Diffusion model with text conditioning and reference image conditioning
  x = model.generate(t, X_t)
  # append x to X

# Step 3: Model adaptation
# initialize segmentation model M with source domain weights
# use torchvision to load a pretrained DeepLabV3 model with ResNet-101 backbone
M = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)
# replace the classifier head with a new one for num_classes
M.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=(1,1), stride=(1,1))
# train M on synthetic dataset X and T
# use cross entropy loss as criterion
criterion = torch.nn.CrossEntropyLoss()
# use SGD as optimizer with lr and beta
optimizer = torch.optim.SGD(M.parameters(), lr=lr, momentum=beta)
# use a dataloader to load X and T in batches with shuffle
dataloader = torch.utils.data.DataLoader(list(zip(X,T)), batch_size=batch_size, shuffle=True)
# loop over num_epochs
for epoch in range(num_epochs):
  # loop over batches in dataloader
  for x_batch, t_batch in dataloader:
    # forward pass M on x_batch and get output logits
    logits = M(x_batch)['out']
    # convert t_batch to one-hot encoded labels with shape (batch_size, num_classes, height, width)
    labels = torch.nn.functional.one_hot(t_batch, num_classes=num_classes).permute(0,3,1,2)
    # compute loss between logits and labels
    loss = criterion(logits, labels)
    # backward pass and update M parameters with optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # print epoch and loss
  print(f'Epoch {epoch}, Loss {loss.item()}')
# evaluate M on target domain test set
# use mean intersection over union (mIoU) as metric
metric = torch.metrics.mIoU(num_classes=num_classes)
# loop over target domain test images X_test and labels Y_test
for x_test, y_test in zip(X_test, Y_test):
  # forward pass M on x_test and get output logits
  logits = M(x_test)['out']
  # convert logits to predictions with shape (height, width)
  preds = torch.argmax(logits, dim=1).squeeze()
  # update metric with preds and y_test
  metric.update(preds, y_test)
# compute and print final mIoU score
score = metric.compute()
print(f'mIoU score: {score}')
```