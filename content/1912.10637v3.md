---
title: 1912.10637v3 GrabAR  Occlusion-aware Grabbing Virtual Objects in AR
date: 2019-12-11
---

# [GrabAR: Occlusion-aware Grabbing Virtual Objects in AR](http://arxiv.org/abs/1912.10637v3)

authors: Xiao Tang, Xiaowei Hu, Chi-Wing Fu, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/1912.10637v3.pdf "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR - arXiv.org"
[2]: https://arxiv.org/abs/1912.10637v3 "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR"
[3]: http://export.arxiv.org/abs/2106.10637v3 "[2106.10637v3] More than Encoder: Introducing Transformer Decoder to ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents GrabAR, a new approach that directly predicts the occlusion between real hands and virtual objects in augmented reality (AR) applications, and bypasses the depth acquisition and inference.
- **Why**: The paper aims to enhance AR applications with interactions between hand (real) and grabbable objects (virtual), which is challenging due to the lack of accurate depth and mismatch between real and virtual depth.
- **How**: The paper formulates a neural network that learns to generate the occlusion mask from paired images of hand and object as inputs. The network is trained on a synthetic dataset and a real dataset, and then embedded in a prototyping AR system that supports hand grabbing of various virtual objects. The paper demonstrates the system performance, both quantitatively and qualitatively, and showcases interaction scenarios.

## Main Contributions

[1]: https://arxiv.org/pdf/1912.10637v3.pdf "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR - arXiv.org"
[2]: https://arxiv.org/abs/1912.10637v3 "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR"
[3]: http://export.arxiv.org/abs/2106.10637v3 "[2106.10637v3] More than Encoder: Introducing Transformer Decoder to ..."

According to the paper[^1^][1], the main contributions are:

- **A new approach** that directly predicts the occlusion between real hands and virtual objects in AR applications, and bypasses the depth acquisition and inference.
- **A compact neural network** that learns to generate the occlusion mask from paired images of hand and object as inputs, and can handle various hand poses and object shapes.
- **A large dataset** that includes synthetic data and real data for training the network, and reduces the burden of manual labels and addresses the domain difference.
- **A prototyping AR system** that supports real-time grabbing of virtual objects with bare hands, and demonstrates the performance of the method on various virtual objects, user studies, and interaction scenarios.

## Method Summary

[1]: https://arxiv.org/pdf/1912.10637v3.pdf "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR - arXiv.org"
[2]: https://arxiv.org/abs/1912.10637v3 "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR"
[3]: http://export.arxiv.org/abs/2106.10637v3 "[2106.10637v3] More than Encoder: Introducing Transformer Decoder to ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper formulates a **neural network** that takes paired images of hand and object as inputs, and outputs an occlusion mask that indicates which pixels belong to the hand, the object, or the background. The network consists of an encoder-decoder architecture with skip connections and residual blocks. The network is trained with a pixel-wise cross-entropy loss and a perceptual loss based on VGG features.
- The paper compiles a **large dataset** for training the network, which includes synthetic data and real data. The synthetic data are generated by rendering 3D models of hands and objects with random poses, lighting, and backgrounds. The real data are collected by capturing videos of users interacting with virtual objects in AR applications. The paper uses a semi-automatic pipeline to label the real data with occlusion masks.
- The paper embeds the trained network in a **prototyping AR system** that supports real-time grabbing of virtual objects with bare hands. The system uses a smartphone camera to capture the hand and object images, and runs the network on a GPU server to generate the occlusion mask. The system then composes the real and virtual images with the occlusion mask, and displays the result on the smartphone screen. The system also tracks the hand pose and object position to enable direct manipulation of virtual objects.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the neural network
network = EncoderDecoder(
  encoder = ResNet18(),
  decoder = ConvTranspose2d(),
  skip_connections = True,
  residual_blocks = True
)

# Define the loss functions
pixel_loss = CrossEntropyLoss()
perceptual_loss = VGGLoss()

# Load the synthetic and real datasets
synthetic_data = load_synthetic_data()
real_data = load_real_data()

# Pre-train the network on synthetic data
for epoch in range(num_epochs):
  for hand_image, object_image, occlusion_mask in synthetic_data:
    # Forward pass
    input = concatenate(hand_image, object_image)
    output = network(input)
    # Compute loss
    loss = pixel_loss(output, occlusion_mask) + perceptual_loss(output, occlusion_mask)
    # Backward pass and update weights
    loss.backward()
    optimizer.step()

# Fine-tune the network on real data
for epoch in range(num_epochs):
  for hand_image, object_image, occlusion_mask in real_data:
    # Forward pass
    input = concatenate(hand_image, object_image)
    output = network(input)
    # Compute loss
    loss = pixel_loss(output, occlusion_mask) + perceptual_loss(output, occlusion_mask)
    # Backward pass and update weights
    loss.backward()
    optimizer.step()

# Deploy the network in a prototyping AR system
while True:
  # Capture the hand and object images from the smartphone camera
  hand_image, object_image = capture_images()
  # Send the images to the GPU server and run the network
  input = concatenate(hand_image, object_image)
  output = network(input)
  # Receive the occlusion mask from the GPU server
  occlusion_mask = output.argmax(dim=1)
  # Compose the real and virtual images with the occlusion mask
  composed_image = compose_images(hand_image, object_image, occlusion_mask)
  # Display the composed image on the smartphone screen
  display_image(composed_image)
  # Track the hand pose and object position and enable direct manipulation of virtual objects
  hand_pose, object_position = track_pose_and_position(hand_image, object_image)
  manipulate_object(object_position, hand_pose)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the encoder-decoder network
class EncoderDecoder(torch.nn.Module):
  def __init__(self):
    super(EncoderDecoder, self).__init__()
    # Use ResNet18 as the encoder
    self.encoder = torchvision.models.resnet18(pretrained=True)
    # Remove the last layer of the encoder
    self.encoder = torch.nn.Sequential(*list(self.encoder.children())[:-1])
    # Use transposed convolution layers as the decoder
    self.decoder = torch.nn.Sequential(
      torch.nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
      torch.nn.BatchNorm2d(256),
      torch.nn.ReLU(),
      torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
      torch.nn.BatchNorm2d(128),
      torch.nn.ReLU(),
      torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
      torch.nn.BatchNorm2d(64),
      torch.nn.ReLU(),
      torch.nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),
      torch.nn.BatchNorm2d(32),
      torch.nn.ReLU(),
      torch.nn.ConvTranspose2d