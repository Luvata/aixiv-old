---
title: 2209.10663v2 Convolutional Bayesian Kernel Inference for 3D Semantic Mapping
date: 2022-09-11
---

# [Convolutional Bayesian Kernel Inference for 3D Semantic Mapping](http://arxiv.org/abs/2209.10663v2)

authors: Joey Wilson, Yuewei Fu, Arthur Zhang, Jingyu Song, Andrew Capodieci, Paramsothy Jayakumar, Kira Barton, Maani Ghaffari


## What, Why and How

[1]: https://arxiv.org/pdf/2209.10663v2 "arXiv:2209.10663v2 [cs.RO] 31 May 2023"
[2]: https://arxiv.org/abs/2209.10663 "Convolutional Bayesian Kernel Inference for 3D Semantic Mapping"
[3]: http://arxiv-export2.library.cornell.edu/abs/2209.10663v2 "[2209.10663v2] Convolutional Bayesian Kernel Inference for 3D Semantic ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel neural network layer called Convolutional Bayesian Kernel Inference (ConvBKI) that performs explicit Bayesian inference within a depthwise separable convolution layer.
- **Why**: The paper aims to bridge the gap between classical robotic mapping methods that are mathematically founded and interpretable, and modern deep learning methods that are efficient and powerful. The paper also wants to enable real-time 3D semantic mapping, where semantic labels are incorporated into a geometric map of the world.
- **How**: The paper applies the ConvBKI layer to the task of 3D semantic mapping using LiDAR sensor information. The ConvBKI layer learns semantic-geometric probability distributions for each voxel in the map, and updates them with new sensor observations using differentiable kernels. The paper evaluates the ConvBKI layer against state-of-the-art semantic mapping algorithms on the KITTI dataset, and shows that it achieves comparable semantic label inference results with improved latency.

## Main Contributions

According to the paper, the main contributions are:

- Creating a real-time 3D semantic mapping neural network layer, which finds middle-ground between classical robotic mapping and modern deep learning.
- Proposing novel differentiable kernels for Bayesian semantic mapping, and demonstrating improved performance through optimization.

## Method Summary

[1]: https://arxiv.org/pdf/2209.10663v2 "arXiv:2209.10663v2 [cs.RO] 31 May 2023"
[2]: https://arxiv.org/abs/2209.10663 "Convolutional Bayesian Kernel Inference for 3D Semantic Mapping"
[3]: http://arxiv-export2.library.cornell.edu/abs/2209.10663v2 "[2209.10663v2] Convolutional Bayesian Kernel Inference for 3D Semantic ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper presents a Convolutional Bayesian Kernel Inference (ConvBKI) layer that performs Bayesian inference within a depthwise separable convolution layer. The ConvBKI layer takes as input a semantic volume, which is a 3D grid of voxels that store semantic-geometric probability distributions for each voxel. The semantic volume is constructed by assigning semantic labels to 3D points from a LiDAR sensor using an off-the-shelf semantic segmentation network, and then summing the coinciding points within each voxel.
- The ConvBKI layer updates the semantic volume with new sensor observations using a depthwise filter that applies differentiable kernels to each voxel. The kernels are designed to perform Bayesian updates on the semantic-geometric distributions, taking into account the sensor model and the prior map. The kernels are also learned through backpropagation, allowing the network to optimize them for better performance.
- The paper evaluates the ConvBKI layer on the task of 3D semantic mapping using the KITTI dataset. The paper compares the ConvBKI layer with state-of-the-art semantic mapping algorithms, such as SemanticFusion and SemanticKITTI, in terms of accuracy and latency. The paper shows that the ConvBKI layer achieves comparable accuracy with improved latency, demonstrating its efficiency and reliability.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the ConvBKI layer
class ConvBKI(nn.Module):
  def __init__(self, kernel_size, num_classes):
    # Initialize the depthwise filter with random kernels
    self.filter = nn.Parameter(torch.randn(num_classes, kernel_size, kernel_size, kernel_size))
  
  def forward(self, semantic_volume):
    # Apply the depthwise filter to the semantic volume
    output = F.conv3d(semantic_volume, self.filter, groups=num_classes)
    # Normalize the output to get a valid probability distribution
    output = F.softmax(output, dim=1)
    return output

# Define the semantic mapping network
class SemanticMapping(nn.Module):
  def __init__(self, semantic_segmentation_network, convbki_layer):
    # Initialize the semantic segmentation network and the ConvBKI layer
    self.semantic_segmentation_network = semantic_segmentation_network
    self.convbki_layer = convbki_layer
  
  def forward(self, lidar_points):
    # Assign semantic labels to the lidar points using the semantic segmentation network
    semantic_labels = self.semantic_segmentation_network(lidar_points)
    # Construct the semantic volume by summing the coinciding points within each voxel
    semantic_volume = torch.bincount(semantic_labels)
    # Update the semantic volume with the ConvBKI layer
    semantic_volume = self.convbki_layer(semantic_volume)
    return semantic_volume
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# Define some constants
NUM_CLASSES = 20 # Number of semantic classes
KERNEL_SIZE = 3 # Size of the convolution kernel
VOXEL_SIZE = 0.1 # Size of each voxel in meters
MAP_SIZE = 100 # Size of the global map in voxels

# Define the ConvBKI layer
class ConvBKI(nn.Module):
  def __init__(self, kernel_size, num_classes):
    super(ConvBKI, self).__init__()
    # Initialize the depthwise filter with random kernels
    self.filter = nn.Parameter(torch.randn(num_classes, kernel_size, kernel_size, kernel_size))
  
  def forward(self, semantic_volume):
    # Apply the depthwise filter to the semantic volume
    output = F.conv3d(semantic_volume, self.filter, groups=num_classes)
    # Normalize the output to get a valid probability distribution
    output = F.softmax(output, dim=1)
    return output

# Define the semantic mapping network
class SemanticMapping(nn.Module):
  def __init__(self, semantic_segmentation_network, convbki_layer):
    super(SemanticMapping, self).__init__()
    # Initialize the semantic segmentation network and the ConvBKI layer
    self.semantic_segmentation_network = semantic_segmentation_network
    self.convbki_layer = convbki_layer
  
  def forward(self, lidar_points):
    # Assign semantic labels to the lidar points using the semantic segmentation network
    semantic_labels = self.semantic_segmentation_network(lidar_points)
    # Construct the semantic volume by summing the coinciding points within each voxel
    semantic_volume = torch.bincount(semantic_labels)
    # Update the semantic volume with the ConvBKI layer
    semantic_volume = self.convbki_layer(semantic_volume)
    return semantic_volume

# Define a function to convert lidar points to voxel coordinates
def lidar_to_voxel(lidar_points):
  # Divide the lidar points by the voxel size and round to get voxel indices
  voxel_indices = np.round(lidar_points / VOXEL_SIZE).astype(int)
  # Shift the voxel indices to fit within the map size
  voxel_indices += MAP_SIZE // 2
  # Clip the voxel indices to avoid out-of-bound errors
  voxel_indices = np.clip(voxel_indices, 0, MAP_SIZE - 1)
  return voxel_indices

# Define a function to convert voxel coordinates to one-hot vectors
def voxel_to_onehot(voxel_indices):
  # Flatten the voxel indices along the x, y, and z axes
  x_indices = voxel_indices[:, 0].flatten()
  y_indices = voxel_indices[:, 1].flatten()
  z_indices = voxel_indices[:, 2].flatten()
  # Compute the linear indices for each voxel in the map
  linear_indices = x_indices + y_indices * MAP_SIZE + z_indices * MAP_SIZE * MAP_SIZE
  # Create a one-hot vector for each linear index
  onehot_vectors = np.eye(MAP_SIZE * MAP_SIZE * MAP_SIZE)[linear_indices]
  # Reshape the one-hot vectors to match the map shape
  onehot_vectors = onehot_vectors.reshape(-1, MAP_SIZE, MAP_SIZE, MAP_SIZE)
  return onehot_vectors

# Load a pre-trained semantic segmentation network (e.g. PointNet++)
semantic_segmentation_network = load_pretrained_model()

# Create a ConvBKI layer with the given kernel size and number of classes
convbki_layer = ConvBKI(KERNEL_SIZE, NUM_CLASSES)

# Create a semantic mapping network with the given components
semantic_mapping_network = SemanticMapping(semantic_segmentation_network, convbki_layer)

# Load a LiDAR scan from the KITTI dataset (e.g. a numpy array of shape [N, 3])
lidar_points = load_lidar_scan()

# Convert the lidar points to voxel coordinates using the defined function
voxel_indices = lidar_to_voxel(lidar_points)

# Convert the voxel coordinates to one-hot vectors using the defined function
onehot_vectors = voxel_to_onehot(voxel_indices)

# Convert the one-hot vectors to a torch tensor and permute it to match PyTorch's convention (channels first)
onehot_vectors = torch.from_numpy(onehot_vectors).permute(0, 3, 1, 2)

# Pass the one-hot vectors to the semantic mapping network and get the updated semantic volume as output
semantic_volume = semantic_mapping_network(onehot_vectors)

# Print the shape and type of the semantic volume
print(semantic_volume.shape, semantic_volume.dtype)
```