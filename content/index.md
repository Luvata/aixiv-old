---
title: Hello
---

# Hello

[2306.17154v1 Generate Anything Anywhere in Any Scene](2306.17154v1.md)

[2306.17115v1 Michelangelo  Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation](2306.17115v1.md)

[2306.17046v1 Spiking Denoising Diffusion Probabilistic Models](2306.17046v1.md)

[2306.16928v1 One-2-3-45  Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization](2306.16928v1.md)

[2306.16782v1 Low-Light Enhancement in the Frequency Domain](2306.16782v1.md)

[2306.16527v1 OBELISC  An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](2306.16527v1.md)

[2306.16052v1 SVNR  Spatially-variant Noise Removal with Denoising Diffusion](2306.16052v1.md)

[2306.15955v2 Bridging the Gap  Neural Collapse Inspired Prompt Tuning for Generalization under Class Imbalance](2306.15955v2.md)

[2306.15658v1 CLIPA-v2  Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy](2306.15658v1.md)

[2306.15128v2 MIMIC  Masked Image Modeling with Image Correspondences](2306.15128v2.md)

[2306.15111v1 Semi-Supervised Image Captioning with CLIP](2306.15111v1.md)

[2306.14435v2 DragDiffusion  Harnessing Diffusion Models for Interactive Point-based Image Editing](2306.14435v2.md)

[2306.14153v1 DomainStudio  Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data](2306.14153v1.md)

[2306.13841v1 Is Pre-training Truly Better Than Meta-Learning?](2306.13841v1.md)

[2306.13651v2 Bring Your Own Data! Self-Supervised Evaluation for Large Language Models](2306.13651v2.md)

[2306.13103v1 Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks](2306.13103v1.md)

[2306.12983v1 Towards More Realistic Membership Inference Attacks on Large Diffusion Models](2306.12983v1.md)

[2306.12790v1 DiffWA  Diffusion Models for Watermark Attack](2306.12790v1.md)

[2306.12624v1 DreamEdit  Subject-driven Image Editing](2306.12624v1.md)

[2306.12422v1 DreamTime  An Improved Optimization Strategy for Text-to-3D Content Creation](2306.12422v1.md)

[2306.10008v1 CLIP2Protect  Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search](2306.10008v1.md)

[2306.09683v1 Scaling Open-Vocabulary Object Detection](2306.09683v1.md)

[2306.09346v2 Rosetta Neurons  Mining the Common Units in a Model Zoo](2306.09346v2.md)

[2306.09345v1 Evaluating Data Attribution for Text-to-Image Models](2306.09345v1.md)

[2306.09344v2 DreamSim  Learning New Dimensions of Human Visual Similarity using Synthetic Data](2306.09344v2.md)

[2306.08687v1 Norm-guided latent space exploration for text-to-image generation](2306.08687v1.md)

[2306.08276v1 TryOnDiffusion  A Tale of Two UNets](2306.08276v1.md)

[2306.08257v1 On the Robustness of Latent Diffusion Models](2306.08257v1.md)

[2306.08247v2 Diffusion in Diffusion  Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation](2306.08247v2.md)

[2306.07954v1 Rerender A Video  Zero-Shot Text-Guided Video-to-Video Translation](2306.07954v1.md)

[2306.07915v1 Image Captioners Are Scalable Vision Learners Too](2306.07915v1.md)

[2306.07754v1 Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis](2306.07754v1.md)

[2306.07716v1 Dynamically Masked Discriminator for Generative Adversarial Networks](2306.07716v1.md)

[2306.07596v1 Paste, Inpaint and Harmonize via Denoising  Subject-Driven Image Editing with Pre-Trained Diffusion Model](2306.07596v1.md)

[2306.07591v1 I See Dead People  Gray-Box Adversarial Attack on Image-To-Text Models](2306.07591v1.md)

[2306.07429v1 Explaining CLIP through Co-Creative Drawings and Interaction](2306.07429v1.md)

[2306.07280v1 Controlling Text-to-Image Diffusion by Orthogonal Finetuning](2306.07280v1.md)

[2306.07279v2 Scalable 3D Captioning with Pretrained Models](2306.07279v2.md)

[2306.06874v2 VillanDiffusion  A Unified Backdoor Attack Framework for Diffusion Models](2306.06874v2.md)

[2306.06110v1 Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings](2306.06110v1.md)

[2306.06094v1 Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding](2306.06094v1.md)

[2306.06093v1 HyP-NeRF  Learning Improved NeRF Priors using a HyperNetwork](2306.06093v1.md)

[2306.06088v1 SENS  Sketch-based Implicit Neural Shape Modeling](2306.06088v1.md)

[2306.06044v1 GANeRF  Leveraging Discriminators to Optimize Neural Radiance Fields](2306.06044v1.md)

[2306.05682v1 Lightweight Monocular Depth Estimation via Token-Sharing Transformer](2306.05682v1.md)

[2306.05500v1 Word-Level Explanations for Analyzing Bias in Text-to-Image Models](2306.05500v1.md)

[2306.05426v2 SequenceMatch  Imitation Learning for Autoregressive Sequence Modelling with Backtracking](2306.05426v2.md)

[2306.05422v1 Tracking Everything Everywhere All at Once](2306.05422v1.md)

[2306.05414v2 Improving Tuning-Free Real Image Editing with Proximal Guidance](2306.05414v2.md)

[2306.05284v1 Simple and Controllable Music Generation](2306.05284v1.md)

[2306.05178v1 SyncDiffusion  Coherent Montage via Synchronized Joint Diffusions](2306.05178v1.md)

[2306.05135v1 Does Image Anonymization Impact Computer Vision Training?](2306.05135v1.md)

[2306.05067v1 Improving Visual Prompt Tuning for Self-supervised Vision Transformers](2306.05067v1.md)

[2306.04990v1 Multi-Architecture Multi-Expert Diffusion Models](2306.04990v1.md)

[2306.04865v2 MyStyle++  A Controllable Personalized Generative Prior](2306.04865v2.md)

[2306.04744v1 WOUAF  Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models](2306.04744v1.md)

[2306.04695v1 ConceptBed  Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models](2306.04695v1.md)

[2306.04642v1 DiffusionShield  A Watermark for Copyright Protection against Generative Diffusion Models](2306.04642v1.md)

[2306.04632v1 Designing a Better Asymmetric VQGAN for StableDiffusion](2306.04632v1.md)

[2306.04387v2 M$^3$IT  A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning](2306.04387v2.md)

[2306.04356v1 Fine-Grained Visual Prompting](2306.04356v1.md)

[2306.04121v1 Matte Anything  Interactive Natural Image Matting with Segment Anything Models](2306.04121v1.md)

[2306.03421v2 Diversifying Joint Vision-Language Tokenization Learning](2306.03421v2.md)

[2306.02583v2 Stable Diffusion is Unstable](2306.02583v2.md)

[2306.02204v1 Cycle Consistency Driven Object Discovery](2306.02204v1.md)

[2306.02018v2 VideoComposer  Compositional Video Synthesis with Motion Controllability](2306.02018v2.md)

[2306.01708v1 Resolving Interference When Merging Models](2306.01708v1.md)

[2306.01669v1 Enhancing CLIP with CLIP  Exploring Pseudolabeling for Limited-Label Prompt Tuning](2306.01669v1.md)

[2306.01272v2 DeepfakeArt Challenge  A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection](2306.01272v2.md)

[2306.00989v1 Hiera  A Hierarchical Vision Transformer without the Bells-and-Whistles](2306.00989v1.md)

[2306.00987v1 StyleGAN knows Normal, Depth, Albedo, and More](2306.00987v1.md)

[2306.00986v3 Diffusion Self-Guidance for Controllable Image Generation](2306.00986v3.md)

[2306.00986v1 Diffusion Self-Guidance for Controllable Image Generation](2306.00986v1.md)

[2306.00984v1 StableRep  Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners](2306.00984v1.md)

[2306.00983v1 StyleDrop  Text-to-Image Generation in Any Style](2306.00983v1.md)

[2306.00980v2 SnapFusion  Text-to-Image Diffusion Model on Mobile Devices within Two Seconds](2306.00980v2.md)

[2306.00980v1 SnapFusion  Text-to-Image Diffusion Model on Mobile Devices within Two Seconds](2306.00980v1.md)

[2306.00974v2 Intriguing Properties of Text-guided Diffusion Models](2306.00974v2.md)

[2306.00974v1 Intriguing Properties of Text-guided Diffusion Models](2306.00974v1.md)

[2306.00966v2 The Hidden Language of Diffusion Models](2306.00966v2.md)

[2306.00966v1 The Hidden Language of Diffusion Models](2306.00966v1.md)

[2306.00738v1 ReFACT  Updating Text-to-Image Models by Editing the Text Encoder](2306.00738v1.md)

[2306.00547v2 AvatarStudio  Text-driven Editing of 3D Dynamic Human Head Avatars](2306.00547v2.md)

[2306.00473v1 Interpretable simultaneous localization of MRI corpus callosum and classification of atypical Parkinsonian disorders using YOLOv5](2306.00473v1.md)

[2306.00455v1 MindBigData 2023 MNIST-8B The 8 billion datapoints Multimodal Dataset of Brain Signals](2306.00455v1.md)

[2306.00451v1 S$^2$ME  Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-supervised Polyp Segmentation](2306.00451v1.md)

[2306.00450v1 Exploring Open-Vocabulary Semantic Segmentation without Human Labels](2306.00450v1.md)

[2306.00446v1 Evaluation of Multi-indicator And Multi-organ Medical Image Segmentation Models](2306.00446v1.md)

[2306.00440v1 Edge-guided Representation Learning for Underwater Object Detection](2306.00440v1.md)

[2306.00427v1 Out-of-distribution forgetting  vulnerability of continual learning to intra-class distribution shift](2306.00427v1.md)

[2306.00424v1 End-to-end Knowledge Retrieval with Multi-modal Queries](2306.00424v1.md)

[2306.00421v1 Introduction to Medical Imaging Informatics](2306.00421v1.md)

[2306.00416v1 Controllable Motion Diffusion Model](2306.00416v1.md)

[2306.00409v1 Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting](2306.00409v1.md)

[2306.00407v2 Towards Interactive Image Inpainting via Sketch Refinement](2306.00407v2.md)

[2306.00407v1 Towards Interactive Image Inpainting via Sketch Refinement](2306.00407v1.md)

[2306.00402v1 Discriminative Deep Feature Visualization for Explainable Face Recognition](2306.00402v1.md)

[2306.00396v1 Lightweight Vision Transformer with Bidirectional Interaction](2306.00396v1.md)

[2306.00393v1 Teacher Agent  A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning](2306.00393v1.md)

[2306.00386v1 Symmetric Uncertainty-Aware Feature Transmission for Depth Super-Resolution](2306.00386v1.md)

[2306.00385v1 HySpecNet-11k  A Large-Scale Hyperspectral Dataset for Benchmarking Learning-Based Hyperspectral Image Compression Methods](2306.00385v1.md)

[2306.00379v1 Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes](2306.00379v1.md)

[2306.00378v1 Example-based Motion Synthesis via Generative Motion Matching](2306.00378v1.md)

[2306.00370v1 Graph Switching Dynamical Systems](2306.00370v1.md)

[2306.00360v1 How Do ConvNets Understand Image Intensity?](2306.00360v1.md)

[2306.00354v1 Addressing Negative Transfer in Diffusion Models](2306.00354v1.md)

[2306.00349v1 CALICO  Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception](2306.00349v1.md)

[2306.00310v1 Prompt Algebra for Task Composition](2306.00310v1.md)

[2306.00306v1 Low-Light Image Enhancement with Wavelet-based Diffusion Models](2306.00306v1.md)

[2306.00303v1 Sea Ice Extraction via Remote Sensed Imagery  Algorithms, Datasets, Applications and Challenges](2306.00303v1.md)

[2306.00299v1 Robust Estimation of Surface Curvature Information from Point Cloud Data](2306.00299v1.md)

[2306.00294v1 Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans](2306.00294v1.md)

[2306.00283v1 Autism Disease Detection Using Transfer Learning Techniques  Performance Comparison Between Central Processing Unit vs Graphics Processing Unit Functions for Neural Networks](2306.00283v1.md)

[2306.00272v1 Accelerated Fingerprint Enhancement  A GPU-Optimized Mixed Architecture Approach](2306.00272v1.md)

[2306.00265v1 Doubly Robust Self-Training](2306.00265v1.md)

[2306.00262v1 Maximal Domain Independent Representations Improve Transfer Learning](2306.00262v1.md)

[2306.00246v1 Fine-Grained Property Value Assessment using Probabilistic Disaggregation](2306.00246v1.md)

[2306.00245v1 From Pixels to UI Actions  Learning to Follow Instructions via Graphical User Interfaces](2306.00245v1.md)

[2306.00241v1 Balancing Reconstruction and Editing Quality of GAN Inversion for Real Image Editing with StyleGAN Prior Latent Space](2306.00241v1.md)

[2306.00238v1 Bytes Are All You Need  Transformers Operating Directly On File Bytes](2306.00238v1.md)

[2306.00231v1 A Universal Latent Fingerprint Enhancer Using Transformers](2306.00231v1.md)

[2306.00228v1 Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models](2306.00228v1.md)

[2306.00219v1 Diffusion Brush  A Latent Diffusion Model-based Editing Tool for AI-generated Images](2306.00219v1.md)

[2306.00202v1 Building Manufacturing Deep Learning Models with Minimal and Imbalanced Training Data Using Domain Adaptation and Data Augmentation](2306.00202v1.md)

[2306.00200v1 Zero-shot Pose Transfer for Unrigged Stylized 3D Characters](2306.00200v1.md)

[2306.00197v1 SSL-CPCD  Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis](2306.00197v1.md)

[2306.00188v1 Multi-environment lifelong deep reinforcement learning for medical imaging](2306.00188v1.md)

[2306.00181v1 Conditionally Strongly Log-Concave Generative Models](2306.00181v1.md)

[2306.00180v1 FlowCam  Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow](2306.00180v1.md)

[2306.00150v1 Enrichment of the NLST and NSCLC-Radiomics computed tomography collections with AI-derived annotations](2306.00150v1.md)

[2306.00129v1 Self-supervised Vision Transformers for 3D Pose Estimation of Novel Objects](2306.00129v1.md)

[2306.00118v1 Neural Textured Deformable Meshes for Robust Analysis-by-Synthesis](2306.00118v1.md)

[2306.00114v1 The Canadian Cropland Dataset  A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture](2306.00114v1.md)

[2306.00112v1 Additional Positive Enables Better Representation Learning for Medical Images](2306.00112v1.md)

[2306.00103v1 ManagerTower  Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning](2306.00103v1.md)

[2306.00075v1 CAROM Air -- Vehicle Localization and Traffic Scene Reconstruction from Aerial Videos](2306.00075v1.md)

[2306.00047v1 Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-empowered Learning](2306.00047v1.md)

[2306.00042v1 Graph-based methods coupled with specific distributional distances for adversarial attack detection](2306.00042v1.md)

[2306.00034v1 Diagnosis and Prognosis of Head and Neck Cancer Patients using Artificial Intelligence](2306.00034v1.md)

[2306.00031v1 Morphological Classification of Radio Galaxies using Semi-Supervised Group Equivariant CNNs](2306.00031v1.md)

[2306.00011v1 A Self-Supervised Approach for Cluster Assessment of High-Dimensional Data](2306.00011v1.md)

[2306.00003v1 Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning](2306.00003v1.md)

[2306.00001v1 TinyissimoYOLO  A Quantized, Low-Memory Footprint, TinyML Object Detection Network for Low Power Microcontrollers](2306.00001v1.md)

[2305.20087v2 Too Large; Data Reduction for Vision-Language Pre-Training](2305.20087v2.md)

[2305.20048v2 F?D  On understanding the role of deep feature spaces on face generation evaluation](2305.20048v2.md)

[2305.20030v2 Tree-Ring Watermarks  Fingerprints for Diffusion Images that are Invisible and Robust](2305.20030v2.md)

[2305.19924v2 Joint Adaptive Representations for Image-Language Learning](2305.19924v2.md)

[2305.19725v2 Direct Learning-Based Deep Spiking Neural Networks  A Review](2305.19725v2.md)

[2305.19700v2 GaitGS  Temporal Feature Learning in Granularity and Span Dimension for Gait Recognition](2305.19700v2.md)

[2305.19693v2 Spontaneous symmetry breaking in generative diffusion models](2305.19693v2.md)

[2305.19623v2 Point-GCC  Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast](2305.19623v2.md)

[2305.19599v2 Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards](2305.19599v2.md)

[2305.19595v2 Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models](2305.19595v2.md)

[2305.19406v3 PaintSeg  Training-free Segmentation via Painting](2305.19406v3.md)

[2305.19406v2 PaintSeg  Training-free Segmentation via Painting](2305.19406v2.md)

[2305.19254v1 What Can We Learn from Unlearnable Datasets?](2305.19254v1.md)

[2305.18654v2 Faith and Fate  Limits of Transformers on Compositionality](2305.18654v2.md)

[2305.18403v2 Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning](2305.18403v2.md)

[2305.18203v2 Concept Decomposition for Visual Exploration and Inspiration](2305.18203v2.md)

[2305.18107v2 Crafting Training Degradation Distribution for the Accuracy-Generalization Trade-off in Real-World Super-Resolution](2305.18107v2.md)

[2305.18079v3 Towards a Robust Framework for NeRF Evaluation](2305.18079v3.md)

[2305.18035v2 Physics-Informed Computer Vision  A Review and Perspectives](2305.18035v2.md)

[2305.17716v3 InDL  A New Datasets and Benchmark for In-Diagram Logic Interpreting based on Visual Illusion](2305.17716v3.md)

[2305.17493v2 The Curse of Recursion  Training on Generated Data Makes Models Forget](2305.17493v2.md)

[2305.17303v2 Distilling BlackBox to Interpretable models for Efficient Transfer Learning](2305.17303v2.md)

[2305.17262v2 Im-Promptu  In-Context Composition from Image Prompts](2305.17262v2.md)

[2305.17144v2 Ghost in the Minecraft  Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory](2305.17144v2.md)

[2305.16966v3 Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection](2305.16966v3.md)

[2305.16381v1 DPOK  Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models](2305.16381v1.md)

[2305.16311v1 Break-A-Scene  Extracting Multiple Concepts from a Single Image](2305.16311v1.md)

[2305.16283v2 CommonScenes  Generating Commonsense 3D Indoor Scenes with Scene Graphs](2305.16283v2.md)

[2305.16223v2 Prompt-Free Diffusion  Taking "Text" out of Text-to-Image Diffusion Models](2305.16223v2.md)

[2305.16213v1 ProlificDreamer  High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation](2305.16213v1.md)

[2305.15391v1 A Neural Space-Time Representation for Text-to-Image Personalization](2305.15391v1.md)

[2305.15171v2 Deceptive-NeRF  Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models](2305.15171v2.md)

[2305.14849v2 DuDGAN  Improving Class-Conditional GANs via Dual-Diffusion](2305.14849v2.md)

[2305.14720v2 BLIP-Diffusion  Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](2305.14720v2.md)

[2305.14334v1 Diffusion Hyperfeatures  Searching Through Time and Space for Semantic Correspondence](2305.14334v1.md)

[2305.14330v2 Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation](2305.14330v2.md)

[2305.13501v2 LaDI-VTON  Latent Diffusion Textual-Inversion Enhanced Virtual Try-On](2305.13501v2.md)

[2305.12476v1 Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models](2305.12476v1.md)

[2305.12140v2 Movie101  A New Movie Understanding Benchmark](2305.12140v2.md)

[2305.11520v4 Late-Constraint Diffusion Guidance for Controllable Image Synthesis](2305.11520v4.md)

[2305.11520v3 Late-Constraint Diffusion Guidance for Controllable Image Synthesis](2305.11520v3.md)

[2305.10973v1 Drag Your GAN  Interactive Point-based Manipulation on the Generative Image Manifold](2305.10973v1.md)

[2305.10855v4 TextDiffuser  Diffusion Models as Text Painters](2305.10855v4.md)

[2305.10855v2 TextDiffuser  Diffusion Models as Text Painters](2305.10855v2.md)

[2305.10616v2 Evaluation Metrics for DNNs Compression](2305.10616v2.md)

[2305.10579v1 MultiPlaneNeRF  Neural Radiance Field with Non-Trainable Representation](2305.10579v1.md)

[2305.10319v2 Automatic Photo Orientation Detection with Convolutional Neural Networks](2305.10319v2.md)

[2305.10118v2 Bridging the Gap  Enhancing the Utility of Synthetic Data via Post-Processing Techniques](2305.10118v2.md)

[2305.09948v3 HICO-DET-SG and V-COCO-SG  New Data Splits for Evaluating the Systematic Generalization Performance of Human-Object Interaction Detection Models](2305.09948v3.md)

[2305.07437v5 Continual Vision-Language Representation Learning with Off-Diagonal Information](2305.07437v5.md)

[2305.06710v2 Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator](2305.06710v2.md)

[2305.06500v2 InstructBLIP  Towards General-purpose Vision-Language Models with Instruction Tuning](2305.06500v2.md)

[2305.06500v1 InstructBLIP  Towards General-purpose Vision-Language Models with Instruction Tuning](2305.06500v1.md)

[2305.06386v1 Text-To-Concept (and Back) via Cross-Model Alignment](2305.06386v1.md)

[2305.04651v1 ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation](2305.04651v1.md)

[2305.04441v1 Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models](2305.04441v1.md)

[2305.03614v2 Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition](2305.03614v2.md)

[2305.02463v1 Shap-E  Generating Conditional 3D Implicit Functions](2305.02463v1.md)

[2305.01644v1 Key-Locked Rank One Editing for Text-to-Image Personalization](2305.01644v1.md)

[2305.00650v2 Discover and Cure  Concept-aware Mitigation of Spurious Correlation](2305.00650v2.md)

[2304.14530v1 It is all about where you start  Text-to-image generation with seed selection](2304.14530v1.md)

[2304.12308v3 Segment Anything in 3D with NeRFs](2304.12308v3.md)

[2304.09172v2 Hyperbolic Image-Text Representations](2304.09172v2.md)

[2304.08715v2 EfficientNet Algorithm for Classification of Different Types of Cancer](2304.08715v2.md)

[2304.07919v2 Chain of Thought Prompt Tuning in Vision Language Models](2304.07919v2.md)

[2304.07193v1 DINOv2  Learning Robust Visual Features without Supervision](2304.07193v1.md)

[2304.07090v1 Delta Denoising Score](2304.07090v1.md)

[2304.06939v2 Multimodal C4  An Open, Billion-scale Corpus of Images Interleaved with Text](2304.06939v2.md)

[2304.06461v2 Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning](2304.06461v2.md)

[2304.06408v1 Intriguing properties of synthetic images  from generative adversarial networks to diffusion models](2304.06408v1.md)

[2304.05818v1 Gradient-Free Textual Inversion](2304.05818v1.md)

[2304.04947v1 Conditional Adapters  Parameter-efficient Transfer Learning with Fast Inference](2304.04947v1.md)

[2304.03895v3 MCDIP-ADMM  Overcoming Overfitting in DIP-based CT reconstruction](2304.03895v3.md)

[2304.03659v1 Probing Conceptual Understanding of Large Visual-Language Models](2304.03659v1.md)

[2304.03411v1 InstantBooth  Personalized Text-to-Image Generation without Test-Time Finetuning](2304.03411v1.md)

[2304.02744v3 StyleGAN Salon  Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer](2304.02744v3.md)

[2304.02643v1 Segment Anything](2304.02643v1.md)

[2304.02278v2 Calibrating Cross-modal Features for Text-Based Person Searching](2304.02278v2.md)

[2304.02234v2 JPEG Compressed Images Can Bypass Protections Against AI Editing](2304.02234v2.md)

[2304.01686v2 HyperCUT  Video Sequence from a Single Blurry Image using Unsupervised Ordering](2304.01686v2.md)

[2304.00186v4 Subject-driven Text-to-Image Generation via Apprenticeship Learning](2304.00186v4.md)

[2304.00186v3 Subject-driven Text-to-Image Generation via Apprenticeship Learning](2304.00186v3.md)

[2303.18242v1 $\infty$-Diff  Infinite Resolution Diffusion with Subsampled Mollified States](2303.18242v1.md)

[2303.18181v2 A Closer Look at Parameter-Efficient Tuning in Diffusion Models](2303.18181v2.md)

[2303.18080v2 One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models](2303.18080v2.md)

[2303.17803v5 Rethinking Local Perception in Lightweight Vision Transformer](2303.17803v5.md)

[2303.17604v1 Token Merging for Fast Stable Diffusion](2303.17604v1.md)

[2303.17599v2 Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models](2303.17599v2.md)

[2303.17591v1 Forget-Me-Not  Learning to Forget in Text-to-Image Diffusion Models](2303.17591v1.md)

[2303.17189v1 LayoutDiffusion  Controllable Diffusion Model for Layout-to-image Generation](2303.17189v1.md)

[2303.16199v1 LLaMA-Adapter  Efficient Fine-tuning of Language Models with Zero-init Attention](2303.16199v1.md)

[2303.15649v1 StyleDiffusion  Prompt-Embedding Inversion for Text-Based Editing](2303.15649v1.md)

[2303.15433v1 Anti-DreamBooth  Protecting users from personalized text-to-image synthesis](2303.15433v1.md)

[2303.14302v2 VILA  Learning Image Aesthetics from User Comments with Vision-Language Pretraining](2303.14302v2.md)

[2303.14218v2 Curricular Contrastive Regularization for Physics-aware Single Image Dehazing](2303.14218v2.md)

[2303.14157v3 Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis](2303.14157v3.md)

[2303.13703v2 End-to-End Diffusion Latent Optimization Improves Classifier Guidance](2303.13703v2.md)

[2303.13516v2 Ablating Concepts in Text-to-Image Diffusion Models](2303.13516v2.md)

[2303.13515v1 Persistent Nature  A Generative Model of Unbounded 3D Worlds](2303.13515v1.md)

[2303.13508v2 DreamBooth3D  Subject-Driven Text-to-3D Generation](2303.13508v2.md)

[2303.13495v1 ReVersion  Diffusion-Based Relation Inversion from Images](2303.13495v1.md)

[2303.13455v1 CoBIT  A Contrastive Bi-directional Image-Text Generation Model](2303.13455v1.md)

[2303.13450v1 Set-the-Scene  Global-Local Training for Generating Controllable NeRF Scenes](2303.13450v1.md)

[2303.12789v2 Instruct-NeRF2NeRF  Editing 3D Scenes with Instructions](2303.12789v2.md)

[2303.11313v3 CLIP goes 3D  Leveraging Prompt Tuning for Language Grounded 3D Recognition](2303.11313v3.md)

[2303.11306v1 Localizing Object-level Shape Variations with Text-to-Image Diffusion Models](2303.11306v1.md)

[2303.11305v3 SVDiff  Compact Parameter Space for Diffusion Fine-Tuning](2303.11305v3.md)

[2303.10735v3 SKED  Sketch-guided Text-based 3D Editing](2303.10735v3.md)

[2303.09522v2 $P+$  Extended Textual Conditioning in Text-to-Image Generation](2303.09522v2.md)

[2303.09270v1 SpectralCLIP  Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective](2303.09270v1.md)

[2303.09252v1 GridCLIP  One-Stage Object Detection by Grid-Level CLIP Representation Learning](2303.09252v1.md)

[2303.08767v3 Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion](2303.08767v3.md)

[2303.08500v1 The Devil's Advocate  Shattering the Illusion of Unexploitable Data using Diffusion Models](2303.08500v1.md)

[2303.08320v3 VideoFusion  Decomposed Diffusion Models for High-Quality Video Generation](2303.08320v3.md)

[2303.08084v1 Editing Implicit Assumptions in Text-to-Image Diffusion Models](2303.08084v1.md)

[2303.07345v2 Erasing Concepts from Diffusion Models](2303.07345v2.md)

[2303.05511v2 Scaling up GANs for Text-to-Image Synthesis](2303.05511v2.md)

[2303.05125v1 Cones  Concept Neurons in Diffusion Models for Customized Generation](2303.05125v1.md)

[2303.04803v4 Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models](2303.04803v4.md)

[2303.04248v1 TRACT  Denoising Diffusion Models with Transitive Closure Time-Distillation](2303.04248v1.md)

[2303.04068v2 VOCALExplore  Pay-as-You-Go Video Data Exploration and Model Building [Technical Report](2303.04068v2.md)

[2303.03565v2 CLIP-Layout  Style-Consistent Indoor Scene Synthesis with Semantic Furniture Embedding](2303.03565v2.md)

[2303.01818v2 Word-As-Image for Semantic Typography](2303.01818v2.md)

[2303.01806v2 When does Privileged Information Explain Away Label Noise?](2303.01806v2.md)

[2302.14051v1 Internet Explorer  Targeted Representation Learning on the Open Web](2302.14051v1.md)

[2302.14045v2 Language Is Not All You Need  Aligning Perception with Language Models](2302.14045v2.md)

[2302.13848v1 ELITE  Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation](2302.13848v1.md)

[2302.13153v1 Directed Diffusion  Direct Control of Object Placement through Attention Guidance](2302.13153v1.md)

[2302.12764v2 Modulating Pretrained Diffusion Models for Multimodal Image Synthesis](2302.12764v2.md)

[2302.12248v2 Learning Visual Representations via Language-Guided Sampling](2302.12248v2.md)

[2302.12231v2 DiffusioNeRF  Regularizing Neural Radiance Fields with Denoising Diffusion Models](2302.12231v2.md)

[2302.12228v3 Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models](2302.12228v3.md)

[2302.12192v1 Aligning Text-to-Image Models using Human Feedback](2302.12192v1.md)

[2302.12066v1 Teaching CLIP to Count to Ten](2302.12066v1.md)

[2302.10893v2 Fair Diffusion  Instructing Text-to-Image Generation Models on Fairness](2302.10893v2.md)

[2302.10893v1 Fair Diffusion  Instructing Text-to-Image Generation Models on Fairness](2302.10893v1.md)

[2302.10663v2 RealFusion  360° Reconstruction of Any Object from a Single Image](2302.10663v2.md)

[2302.10167v2 Cross-domain Compositing with Pretrained Diffusion Models](2302.10167v2.md)

[2302.08453v2 T2I-Adapter  Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](2302.08453v2.md)

[2302.05905v2 Single Motion Diffusion](2302.05905v2.md)

[2302.05905v1 Single Motion Diffusion](2302.05905v1.md)

[2302.05637v2 Dual Relation Knowledge Distillation for Object Detection](2302.05637v2.md)

[2302.05543v1 Adding Conditional Control to Text-to-Image Diffusion Models](2302.05543v1.md)

[2302.04638v2 Better Diffusion Models Further Improve Adversarial Training](2302.04638v2.md)

[2302.04578v2 Adversarial Example Does Good  Preventing Painting Imitation from Diffusion Models via Adversarial Examples](2302.04578v2.md)

[2302.04578v1 Adversarial Example Does Good  Preventing Painting Imitation from Diffusion Models via Adversarial Examples](2302.04578v1.md)

[2302.04222v3 GLAZE  Protecting Artists from Style Mimicry by Text-to-Image Models](2302.04222v3.md)

[2302.03084v2 Pic2Word  Mapping Pictures to Words for Zero-shot Composed Image Retrieval](2302.03084v2.md)

[2302.02210v2 Oscillation-free Quantization for Low-bit Vision Transformers](2302.02210v2.md)

[2302.01721v1 TEXTure  Text-Guided Texturing of 3D Shapes](2302.01721v1.md)

[2301.13838v2 Image Shortcut Squeezing  Countering Perturbative Availability Poisons with Compression](2301.13838v2.md)

[2301.13826v2 Attend-and-Excite  Attention-Based Semantic Guidance for Text-to-Image Diffusion Models](2301.13826v2.md)

[2301.13823v3 Grounding Language Models to Images for Multimodal Inputs and Outputs](2301.13823v3.md)

[2301.13319v3 [Work in progress](2301.13319v3.md)

[2301.12597v2 BLIP-2  Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](2301.12597v2.md)

[2301.12246v4 A Closer Look at Few-shot Classification Again](2301.12246v4.md)

[2301.11497v2 D$^2$CSG  Unsupervised Learning of Compact CSG Trees with Dual Complements and Dropouts](2301.11497v2.md)

[2301.09318v3 Toward Foundation Models for Earth Monitoring  Generalizable Deep Learning Models for Natural Hazard Segmentation](2301.09318v3.md)

[2301.05499v2 CLIP the Gap  A Single Domain Generalization Approach for Object Detection](2301.05499v2.md)

[2301.05225v2 Domain Expansion of Image Generators](2301.05225v2.md)

[2301.05221v1 Guiding Text-to-Image Diffusion Model Towards Grounded Generation](2301.05221v1.md)

[2301.05187v1 WIRE  Wavelet Implicit Neural Representations](2301.05187v1.md)

[2301.02241v1 CiT  Curation in Training for Effective Vision-Language Data](2301.02241v1.md)

[2301.01569v2 Learning Decorrelated Representations Efficiently Using Fast Fourier Transform](2301.01569v2.md)

[2301.00785v4 CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection](2301.00785v4.md)

[2301.00704v1 Muse  Text-To-Image Generation via Masked Generative Transformers](2301.00704v1.md)

[2301.00135v2 Translating Text Synopses to Video Storyboards](2301.00135v2.md)

[2212.12990v3 Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models](2212.12990v3.md)

[2212.12185v2 Implementation of a Blind navigation method in outdoors/indoors areas](2212.12185v2.md)

[2212.12043v1 When are Lemons Purple? The Concept Association Bias of CLIP](2212.12043v1.md)

[2212.12017v3 OPT-IML  Scaling Language Model Instruction Meta Learning through the Lens of Generalization](2212.12017v3.md)

[2212.11565v2 Tune-A-Video  One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](2212.11565v2.md)

[2212.10229v3 StyleDomain  Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation](2212.10229v3.md)

[2212.10015v2 Benchmarking Spatial Relationships in Text-to-Image Generation](2212.10015v2.md)

[2212.10015v1 Benchmarking Spatial Relationships in Text-to-Image Generation](2212.10015v1.md)

[2212.09748v2 Scalable Diffusion Models with Transformers](2212.09748v2.md)

[2212.09741v3 One Embedder, Any Task  Instruction-Finetuned Text Embeddings](2212.09741v3.md)

[2212.09737v2 Position-guided Text Prompt for Vision-Language Pre-training](2212.09737v2.md)

[2212.09737v1 Position-guided Text Prompt for Vision-Language Pre-training](2212.09737v1.md)

[2212.09686v1 A Natural Bias for Language Generation Models](2212.09686v1.md)

[2212.09597v5 Reasoning with Language Model Prompting  A Survey](2212.09597v5.md)

[2212.09262v2 Out-of-domain GAN inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation](2212.09262v2.md)

[2212.08751v1 Point-E  A System for Generating 3D Point Clouds from Complex Prompts](2212.08751v1.md)

[2212.07564v3 AirfRANS  High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier-Stokes Solutions](2212.07564v3.md)

[2212.06384v3 PV3D  A 3D Generative Model for Portrait Video Generation](2212.06384v3.md)

[2212.05400v3 How to Backdoor Diffusion Models?](2212.05400v3.md)

[2212.04489v1 SINE  SINgle Image Editing with Text-to-Image Diffusion Models](2212.04489v1.md)

[2212.04488v1 Multi-Concept Customization of Text-to-Image Diffusion](2212.04488v1.md)

[2212.03863v2 X-Paste  Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion](2212.03863v2.md)

[2212.01735v2 Neural Fourier Filter Bank](2212.01735v2.md)

[2212.00794v2 Scaling Language-Image Pre-training via Masking](2212.00794v2.md)

[2212.00259v2 Super-CLEVR  A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning](2212.00259v2.md)

[2211.17256v2 CLIPascene  Scene Sketching with Different Types and Levels of Abstraction](2211.17256v2.md)

[2211.17180v2 Nonlinear Advantage  Trained Networks Might Not Be As Complex as You Think](2211.17180v2.md)

[2211.17115v1 Multiresolution Textual Inversion](2211.17115v1.md)

[2211.16961v4 Pattern Attention Transformer with Doughnut Kernel](2211.16961v4.md)

[2211.16374v2 DATID-3D  Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model](2211.16374v2.md)

[2211.16032v1 Dimensionality-Varying Diffusion Process](2211.16032v1.md)

[2211.15661v3 What learning algorithm is in-context learning? Investigations with linear models](2211.15661v3.md)

[2211.15518v1 ReCo  Region-Controlled Text-to-Image Generation](2211.15518v1.md)

[2211.15388v2 Shifted Diffusion for Text-to-image Generation](2211.15388v2.md)

[2211.15089v3 Continuous diffusion for categorical data](2211.15089v3.md)

[2211.14305v2 SpaText  Spatio-Textual Representation for Controllable Image Generation](2211.14305v2.md)

[2211.13752v1 Sketch-Guided Text-to-Image Diffusion Models](2211.13752v1.md)

[2211.13227v1 Paint by Example  Exemplar-based Image Editing with Diffusion Models](2211.13227v1.md)

[2211.13224v1 Peekaboo  Text to Image Diffusion Models are Zero-Shot Segmentors](2211.13224v1.md)

[2211.13221v2 Latent Video Diffusion Models for High-Fidelity Long Video Generation](2211.13221v2.md)

[2211.12914v2 Open-vocabulary Attribute Detection](2211.12914v2.md)

[2211.12739v2 Texts as Images in Prompt Tuning for Multi-Label Image Recognition](2211.12739v2.md)

[2211.12572v1 Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation](2211.12572v1.md)

[2211.12503v1 Is the Elephant Flying? Resolving Ambiguities in Text-to-Image Generative Models](2211.12503v1.md)

[2211.12446v2 EDICT  Exact Diffusion Inversion via Coupled Transformations](2211.12446v2.md)

[2211.11738v3 SPARF  Neural Radiance Fields from Sparse and Noisy Poses](2211.11738v3.md)

[2211.11559v1 Visual Programming  Compositional visual reasoning without training](2211.11559v1.md)

[2211.11492v1 ClipCrop  Conditioned Cropping Driven by Vision-Language Model](2211.11492v1.md)

[2211.11018v2 MagicVideo  Efficient Video Generation With Latent Diffusion Models](2211.11018v2.md)

[2211.10656v1 Parallel Diffusion Models of Operator and Image for Blind Inverse Problems](2211.10656v1.md)

[2211.10440v2 Magic3D  High-Resolution Text-to-3D Content Creation](2211.10440v2.md)

[2211.10288v1 Just a Matter of Scale? Reevaluating Scale Equivariance in Convolutional Neural Networks](2211.10288v1.md)

[2211.09800v2 InstructPix2Pix  Learning to Follow Image Editing Instructions](2211.09800v2.md)

[2211.09794v1 Null-text Inversion for Editing Real Images using Guided Diffusion Models](2211.09794v1.md)

[2211.08422v3 Mechanistic Mode Connectivity](2211.08422v3.md)

[2211.07825v1 Direct Inversion  Optimization-Free Text-Driven Real Image Editing with Diffusion Models](2211.07825v1.md)

[2211.07600v1 Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures](2211.07600v1.md)

[2211.01324v5 eDiff-I  Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers](2211.01324v5.md)

[2210.16579v2 INR-V  A Continuous Representation Space for Video-based Generative Tasks](2210.16579v2.md)

[2210.16478v3 GPA-Net No-Reference Point Cloud Quality Assessment with Multi-task Graph Convolutional Network](2210.16478v3.md)

[2210.14306v4 Reading Between the Lines  Modeling User Behavior and Costs in AI-Assisted Programming](2210.14306v4.md)

[2210.14124v1 Lafite2  Few-shot Text-to-Image Generation](2210.14124v1.md)

[2210.13512v3 Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup](2210.13512v3.md)

[2210.13382v4 Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task](2210.13382v4.md)

[2210.10606v1 DALLE-2 is Seeing Double  Flaws in Word-to-Concept Mapping in Text2Image Models](2210.10606v1.md)

[2210.10362v3 CPL  Counterfactual Prompt Learning for Vision and Language Models](2210.10362v3.md)

[2210.09477v3 UniTune  Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image](2210.09477v3.md)

[2210.09304v1 Non-Contrastive Learning Meets Language-Image Pre-Training](2210.09304v1.md)

[2210.09276v3 Imagic  Text-Based Real Image Editing with Diffusion Models](2210.09276v3.md)

[2210.07883v2 One Model to Edit Them All  Free-Form Text-Driven Image Manipulation with Semantic Modulations](2210.07883v2.md)

[2210.07229v1 Mass-Editing Memory in a Transformer](2210.07229v1.md)

[2210.06031v2 Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning](2210.06031v2.md)

[2210.05810v2 A unified model for continuous conditional video prediction](2210.05810v2.md)

[2210.04506v1 Bridging CLIP and StyleGAN through Latent Alignment for Image Editing](2210.04506v1.md)

[2210.03919v4 CLIP-PAE  Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation](2210.03919v4.md)

[2210.03372v2 Pre-trained Adversarial Perturbations](2210.03372v2.md)

[2210.02399v1 Phenaki  Variable Length Video Generation From Open Domain Textual Description](2210.02399v1.md)

[2210.02396v2 Temporally Consistent Transformers for Video Generation](2210.02396v2.md)

[2210.02303v1 Imagen Video  High Definition Video Generation with Diffusion Models](2210.02303v1.md)

[2210.02249v1 LDEdit  Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models](2210.02249v1.md)

[2210.00957v1 UnGANable  Defending Against GAN-based Face Manipulation](2210.00957v1.md)

[2210.00445v3 ManiCLIP  Multi-Attribute Face Manipulation from Text](2210.00445v3.md)

[2209.15270v1 ERNIE-ViL 2.0  Multi-view Contrastive Learning for Image-Text Pre-training](2209.15270v1.md)

[2209.14988v1 DreamFusion  Text-to-3D using 2D Diffusion](2209.14988v1.md)

[2209.14916v2 Human Motion Diffusion Model](2209.14916v2.md)

[2209.14792v1 Make-A-Video  Text-to-Video Generation without Text-Video Data](2209.14792v1.md)

[2209.14169v2 CALIP  Zero-Shot Enhancement of CLIP with Parameter-free Attention](2209.14169v2.md)

[2209.13430v2 UniCLIP  Unified Framework for Contrastive Language-Image Pre-training](2209.13430v2.md)

[2209.13284v2 Frame Interpolation for Dynamic Scenes with Implicit Flow Encoding](2209.13284v2.md)

[2209.12746v2 LSAP  Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space](2209.12746v2.md)

[2209.11711v3 Best Prompts for Text-to-Image Models and How to Find Them](2209.11711v3.md)

[2209.10811v2 IntereStyle  Encoding an Interest Region for Robust StyleGAN Inversion](2209.10811v2.md)

[2209.10663v2 Convolutional Bayesian Kernel Inference for 3D Semantic Mapping](2209.10663v2.md)

[2209.09019v1 LAVIS  A Library for Language-Vision Intelligence](2209.09019v1.md)

[2209.07526v2 OmniVL One Foundation Model for Image-Language and Video-Language Tasks](2209.07526v2.md)

[2209.07511v1 Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](2209.07511v1.md)

[2209.07046v2 Exploring Visual Interpretability for Contrastive Language-Image Pre-training](2209.07046v2.md)

[2209.06970v2 Generative Visual Prompt  Unifying Distributional Control of Pre-Trained Generative Models](2209.06970v2.md)

[2209.06430v4 CLIP-ViP  Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment](2209.06430v4.md)

[2209.06103v1 VL-Taboo  An Analysis of Attribute-based Zero-shot Capabilities of Vision-Language Models](2209.06103v1.md)

[2209.04066v2 TEACH  Temporal Action Composition for 3D Humans](2209.04066v2.md)

[2209.03953v1 Text-Free Learning of a Natural Language Interface for Pretrained Face Generators](2209.03953v1.md)

[2209.03320v1 What does a platypus look like? Generating customized prompts for zero-shot image classification](2209.03320v1.md)

[2209.03160v2 AI Illustrator  Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation](2209.03160v2.md)

[2209.01540v5 An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling](2209.01540v5.md)

[2209.00647v1 Visual Prompting via Image Inpainting](2209.00647v1.md)

[2209.00588v2 Transformers are Sample-Efficient World Models](2209.00588v2.md)

[2208.14649v3 Injecting Image Details into CLIP's Feature Space](2208.14649v3.md)

[2208.13628v2 Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment](2208.13628v2.md)

[2208.13518v1 LogicRank  Logic Induced Reranking for Generative Text-to-Image Systems](2208.13518v1.md)

[2208.13474v1 Prompt Tuning with Soft Context Sharing for Vision-Language Models](2208.13474v1.md)

[2208.12408v1 User-Controllable Latent Transformer for StyleGAN Image Layout Editing](2208.12408v1.md)

[2208.12242v2 DreamBooth  Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](2208.12242v2.md)

[2208.09932v1 Improving GANs for Long-Tailed Data through Group Spectral Regularization](2208.09932v1.md)

[2208.09021v3 VAuLT  Augmenting the Vision-and-Language Transformer for Sentiment Classification on Social Media](2208.09021v3.md)

[2208.08914v1 Prompt Vision Transformer for Domain Generalization](2208.08914v1.md)

[2208.08831v2 Discovering Bugs in Vision Models using Off-the-shelf Image Generation and Captioning](2208.08831v2.md)

[2208.08232v2 HELP ME THINK  A Simple Prompting Strategy for Non-experts to Create Customized Content with Models](2208.08232v2.md)

[2208.07862v1 StyleFaceV  Face Video Generation via Decomposing and Recomposing Pretrained StyleGAN3](2208.07862v1.md)

[2208.05516v4 Quality Not Quantity  On the Interaction between Dataset Design and Robustness of CLIP](2208.05516v4.md)

[2208.02843v1 TIC  Text-Guided Image Colorization](2208.02843v1.md)

[2208.02816v1 Expanding Language-Image Pretrained Models for General Video Recognition](2208.02816v1.md)

[2208.02515v2 Fine-Grained Semantically Aligned Vision-Language Pre-Training](2208.02515v2.md)

[2208.01626v1 Prompt-to-Prompt Image Editing with Cross Attention Control](2208.01626v1.md)

[2208.01618v1 An Image is Worth One Word  Personalizing Text-to-Image Generation using Textual Inversion](2208.01618v1.md)

[2208.00005v1 Testing Relational Understanding in Text-Guided Image Generation](2208.00005v1.md)

[2207.14288v1 Rewriting Geometric Rules of a GAN](2207.14288v1.md)

[2207.12661v1 Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training](2207.12661v1.md)

[2207.12396v2 Exploring CLIP for Assessing the Look and Feel of Images](2207.12396v2.md)

[2207.11678v2 Quad-Net  Quad-domain Network for CT Metal Artifact Reduction](2207.11678v2.md)

[2207.10642v1 Generative Multiplane Images  Making a 2D GAN 3D-Aware](2207.10642v1.md)

[2207.09367v1 Cycle Encoding of a StyleGAN Encoder for Improved Reconstruction and Editability](2207.09367v1.md)

[2207.08134v1 Editing Out-of-domain GAN Inversion via Differential Activations](2207.08134v1.md)

[2207.07885v3 Clover  Towards A Unified Video-Language Alignment and Fusion Model](2207.07885v3.md)

[2207.06604v1 Rethinking Super-Resolution as Text-Guided Details Generation](2207.06604v1.md)

[2207.06555v1 Supervised Attribute Information Removal and Reconstruction for Image Manipulation](2207.06555v1.md)

[2207.05300v1 SD-GAN  Semantic Decomposition for Face Image Synthesis with Discrete Attribute](2207.05300v1.md)

[2207.03411v1 VecGAN  Image-to-Image Translation with Interpretable Latent Directions](2207.03411v1.md)

[2207.02812v3 Towards Counterfactual Image Manipulation via CLIP](2207.02812v3.md)

[2207.01696v2 TM2T  Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts](2207.01696v2.md)

[2207.01077v3 Can Language Understand Depth?](2207.01077v3.md)

[2207.00691v1 American == White in Multimodal Language-and-Image AI](2207.00691v1.md)

[2207.00221v2 VL-CheckList  Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations](2207.00221v2.md)

[2206.10886v1 Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation](2206.10886v1.md)

[2206.08954v2 Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning](2206.08954v2.md)

[2206.08010v3 MoDi  Unconditional Motion Synthesis from Diverse Data](2206.08010v3.md)

[2206.04647v1 VideoINR  Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution](2206.04647v1.md)

[2206.03429v2 Generating Long Videos of Dynamic Scenes](2206.03429v2.md)

[2206.02903v1 Polymorphic-GAN  Generating Aligned Samples across Multiple Domains with Learned Morph Maps](2206.02903v1.md)

[2206.02779v2 Blended Latent Diffusion](2206.02779v2.md)

[2206.02338v2 OrdinalCLIP  Learning Rank Prompts for Language-Guided Ordinal Regression](2206.02338v2.md)

[2206.02104v1 ContraCLIP  Interpretable GAN generation driven by pairs of contrasting sentences](2206.02104v1.md)

[2206.01843v2 Visual Clues  Bridging Vision and Language Foundations for Image Paragraph Captioning](2206.01843v2.md)

[2206.01714v6 Compositional Visual Generation with Composable Diffusion Models](2206.01714v6.md)

[2206.00629v2 CLIP4IDC  CLIP for Image Difference Captioning](2206.00629v2.md)

[2206.00477v1 Anti-Forgery  Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations](2206.00477v1.md)

[2206.00364v2 Elucidating the Design Space of Diffusion-Based Generative Models](2206.00364v2.md)

[2206.00048v2 PandA  Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs](2206.00048v2.md)

[2205.15868v1 CogVideo  Large-scale Pretraining for Text-to-Video Generation via Transformers](2205.15868v1.md)

[2205.15585v2 Decomposing NeRF for Editing via Feature Field Distillation](2205.15585v2.md)

[2205.14870v2 Compressible-composable NeRF via Rank-residual Decomposition](2205.14870v2.md)

[2205.14459v2 CyCLIP  Cyclic Contrastive Language-Image Pretraining](2205.14459v2.md)

[2205.12309v1 Structured Prompt Tuning](2205.12309v1.md)

[2205.12219v3 Aerial Vision-and-Dialog Navigation](2205.12219v3.md)

[2205.10747v4 Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners](2205.10747v4.md)

[2205.10178v2 Visually-Augmented Language Modeling](2205.10178v2.md)

[2205.03146v3 CLIP-CLOP  CLIP-Guided Collage and Photomontage](2205.03146v3.md)

[2205.02837v2 BlobGAN  Spatially Disentangled Scene Representations](2205.02837v2.md)

[2205.01917v2 CoCa  Contrastive Captioners are Image-Text Foundation Models](2205.01917v2.md)

[2205.00823v1 CenterCLIP  Token Clustering for Efficient Text-Video Retrieval](2205.00823v1.md)

[2205.00363v3 Visual Spatial Reasoning](2205.00363v3.md)

[2204.14217v2 CogView2  Faster and Better Text-to-Image Generation via Hierarchical Transformers](2204.14217v2.md)

[2204.14198v2 Flamingo  a Visual Language Model for Few-Shot Learning](2204.14198v2.md)

[2204.14109v2 TEMOS  Generating diverse human motions from textual descriptions](2204.14109v2.md)

[2204.14095v2 PyramidCLIP  Hierarchical Feature Alignment for Vision-language Model Pretraining](2204.14095v2.md)

[2204.13100v1 Few-Shot Head Swapping in the Wild](2204.13100v1.md)

[2204.12696v1 Grasping the Arrow of Time from the Singularity  Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN](2204.12696v1.md)

[2204.11824v3 Semi-Parametric Neural Image Synthesis](2204.11824v3.md)

[2204.10965v5 CLIP-Dissect  Automatic Description of Neuron Representations in Deep Vision Networks](2204.10965v5.md)

[2204.10965v4 CLIP-Dissect  Automatic Description of Neuron Representations in Deep Vision Networks](2204.10965v4.md)

[2204.09222v2 K-LITE  Learning Transferable Visual Models with External Knowledge](2204.09222v2.md)

[2204.07156v2 Any-resolution Training for High-resolution Image Synthesis](2204.07156v2.md)

[2204.06160v1 Neural Texture Extraction and Distribution for Controllable Person Image Synthesis](2204.06160v1.md)

[2204.05991v2 ReCLIP  A Strong Zero-Shot Baseline for Referring Expression Comprehension](2204.05991v2.md)

[2204.04588v1 Robust Cross-Modal Representation Learning with Progressive Self-Distillation](2204.04588v1.md)

[2204.03641v1 Unsupervised Image-to-Image Translation with Generative Prior](2204.03641v1.md)

[2204.03610v1 Unified Contrastive Learning in Image-Text-Label Space](2204.03610v1.md)

[2204.03458v2 Video Diffusion Models](2204.03458v2.md)

[2204.02849v2 KNN-Diffusion  Image Generation via Large-Scale Retrieval](2204.02849v2.md)

[2204.02547v1 Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation](2204.02547v1.md)

[2204.02329v4 Can language models learn from explanations in context?](2204.02329v4.md)

[2204.01694v3 "This is my unicorn, Fluffy"  Personalizing frozen vision-language representations](2204.01694v3.md)

[2204.01159v1 Shape-Pose Disentanglement using SE(3)-equivariant Vector Neurons](2204.01159v1.md)

[2204.00923v4 Word separation in continuous sign language using isolated signs and post-processing](2204.00923v4.md)

[2204.00598v2 Socratic Models  Composing Zero-Shot Multimodal Reasoning with Language](2204.00598v2.md)

[2203.17274v2 Exploring Visual Prompts for Adapting Large-Scale Models](2203.17274v2.md)

[2203.17272v2 MyStyle  A Personalized Generative Prior](2203.17272v2.md)

[2203.17247v3 VL-InterpreT  An Interactive Visualization Tool for Interpreting Vision-Language Transformers](2203.17247v3.md)

[2203.15320v1 Dressing in the Wild by Watching Dance Videos](2203.15320v1.md)

[2203.15243v2 Fine-tuning Image Transformers using Learnable Memory](2203.15243v2.md)

[2203.14465v2 STaR  Bootstrapping Reasoning With Reasoning](2203.14465v2.md)

[2203.14463v2 Large-scale Bilingual Language-Image Contrastive Learning](2203.14463v2.md)

[2203.14074v1 V3GAN  Decomposing Background, Foreground and Motion for Video Generation](2203.14074v1.md)

[2203.13371v2 FitCLIP  Refining Large-Scale Pretrained Image-Text Models for Zero-Shot Video Understanding Tasks](2203.13371v2.md)

[2203.13333v2 CLIP-Mesh  Generating textured meshes from text using pretrained image-text models](2203.13333v2.md)

[2203.13131v1 Make-A-Scene  Scene-Based Text-to-Image Generation with Human Priors](2203.13131v1.md)

[2203.12119v2 Visual Prompt Tuning](2203.12119v2.md)

[2203.11926v3 Focal Modulation Networks](2203.11926v3.md)

[2203.09517v2 TensoRF  Tensorial Radiance Fields](2203.09517v2.md)

[2203.08063v1 MotionCLIP  Exposing Human Motion Generation to CLIP Space](2203.08063v1.md)

[2203.07190v1 CLIP Models are Few-shot Learners  Empirical Studies on VQA and Visual Entailment](2203.07190v1.md)

[2203.06386v2 Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation](2203.06386v2.md)

[2203.05796v1 Democratizing Contrastive Language-Image Pre-training  A CLIP Benchmark of Data, Model, and Supervision](2203.05796v1.md)

[2203.05557v2 Conditional Prompt Learning for Vision-Language Models](2203.05557v2.md)

[2203.05081v1 NLX-GPT  A Model for Natural Language Explanations in Vision and Vision-Language Tasks](2203.05081v1.md)

[2203.04279v1 Probabilistic Warp Consistency for Weakly-Supervised Semantic Correspondences](2203.04279v1.md)

[2203.04036v2 StyleHEAT  One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN](2203.04036v2.md)

[2203.02588v2 A Quality Index Metric and Method for Online Self-Assessment of Autonomous Vehicles Sensory Perception](2203.02588v2.md)

[2203.02573v1 Show Me What and Tell Me How  Video Synthesis via Multimodal Conditioning](2203.02573v1.md)

[2203.02053v2 Mind the Gap  Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](2203.02053v2.md)

[2203.01993v2 Polarity Sampling  Quality and Diversity Control of Pre-Trained Generative Networks via Singular Values](2203.01993v2.md)

[2203.01914v2 Playable Environments  Video Manipulation in Space and Time](2203.01914v2.md)

[2203.00759v2 HyperPrompt  Prompt-based Task-Conditioning of Transformers](2203.00759v2.md)

[2203.00386v1 CLIP-GEN  Language-Free Training of a Text-to-Image Generator with CLIP](2203.00386v1.md)

[2202.14020v1 State-of-the-Art in the Architecture, Methods and Applications of StyleGAN](2202.14020v1.md)

[2202.12362v1 StyleCLIPDraw  Coupling Content and Style in Text-to-Drawing Translation](2202.12362v1.md)

[2202.12211v1 Self-Distilled StyleGAN  Towards Generation from Internet Photos](2202.12211v1.md)

[2202.11094v5 GroupViT  Semantic Segmentation Emerges from Text Supervision](2202.11094v5.md)

[2202.10816v2 Why Fair Labels Can Yield Unfair Predictions  Graphical Conditions for Introduced Unfairness](2202.10816v2.md)

[2202.10571v1 Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks](2202.10571v1.md)

[2202.08926v1 On Guiding Visual Attention with Language Specification](2202.08926v1.md)

[2202.08614v2 Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time](2202.08614v2.md)

[2202.06687v1 Domain Adaptation via Prompt Learning](2202.06687v1.md)

[2202.05910v1 Multi-level Latent Space Structuring for Generative Control](2202.05910v1.md)

[2202.05822v2 CLIPasso  Semantically-Aware Object Sketching](2202.05822v2.md)

[2202.04200v1 MaskGIT  Masked Generative Image Transformer](2202.04200v1.md)

[2202.04040v1 Self-Conditioned Generative Adversarial Networks for Image Editing](2202.04040v1.md)

[2202.02713v1 FEAT  Face Editing with Attention](2202.02713v1.md)

[2202.01344v1 Formal Mathematics Statement Curriculum Learning](2202.01344v1.md)

[2201.13433v1 Third Time's the Charm? Image and Video Editing with StyleGAN3](2201.13433v1.md)

[2201.13168v1 SPAGHETTI  Editing Implicit Shapes Through Part Aware Generation](2201.13168v1.md)

[2201.12086v2 BLIP  Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](2201.12086v2.md)

[2201.11903v6 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](2201.11903v6.md)

[2201.10326v3 ShapeFormer  Transformer-based Shape Completion via Sparse Representation](2201.10326v3.md)

[2201.08361v2 Stitch it in Time  GAN-Based Facial Editing of Real Videos](2201.08361v2.md)

[2201.05078v2 CLIP-Event  Connecting Text and Images with Event Structures](2201.05078v2.md)

[2201.02193v2 Realistic Full-Body Anonymization with Surface-Guided GANs](2201.02193v2.md)

[2201.01873v2 NeuralMLS  Geometry-Aware Control Point Deformation](2201.01873v2.md)

[2201.00308v3 DiffuseVAE  Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents](2201.00308v3.md)

[2112.14757v2 A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-language Model](2112.14757v2.md)

[2112.14683v4 StyleGAN-V  A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2](2112.14683v4.md)

[2112.12750v1 SLIP  Self-supervision meets Language-Image Pre-training](2112.12750v1.md)

[2112.10752v2 High-Resolution Image Synthesis with Latent Diffusion Models](2112.10752v2.md)

[2112.10741v3 GLIDE  Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](2112.10741v3.md)

[2112.10098v1 Initiative Defense against Facial Manipulation](2112.10098v1.md)

[2112.09130v3 Ensembling Off-the-shelf Models for GAN Training](2112.09130v3.md)

[2112.08614v2 KAT  A Knowledge Augmented Transformer for Vision-and-Language](2112.08614v2.md)

[2112.07804v2 Tackling the Generative Learning Trilemma with Denoising Diffusion GANs](2112.07804v2.md)

[2112.07133v2 CLIP-Lite  Information Efficient Visual Representation Learning with Language Supervision](2112.07133v2.md)

[2112.06825v2 VL-Adapter  Parameter-Efficient Transfer Learning for Vision-and-Language Tasks](2112.06825v2.md)

[2112.05142v2 HairCLIP  Design Your Hair by Text and Reference Image](2112.05142v2.md)

[2112.05139v3 CLIP-NeRF  Text-and-Image Driven Manipulation of Neural Radiance Fields](2112.05139v3.md)

[2112.05131v1 Plenoxels  Radiance Fields without Neural Networks](2112.05131v1.md)

[2112.04482v3 FLAVA  A Foundational Language And Vision Alignment Model](2112.04482v3.md)

[2112.03902v2 MS-TCT  Multi-Scale Temporal ConvTransformer for Action Detection](2112.03902v2.md)

[2112.03857v2 Grounded Language-Image Pre-training](2112.03857v2.md)

[2112.03517v1 CG-NeRF  Conditional Generative Neural Radiance Fields](2112.03517v1.md)

[2112.02815v2 Make It Move  Controllable Image-to-Video Generation with Text Descriptions](2112.02815v2.md)

[2112.02300v2 Unsupervised Domain Generalization by Learning a Bridge Across Domains](2112.02300v2.md)

[2112.02236v3 SemanticStyleGAN  Learning Compositional Generative Priors for Controllable Image Synthesis and Editing](2112.02236v3.md)

[2112.01573v1 FuseDream  Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization](2112.01573v1.md)

[2112.01518v2 DenseCLIP  Language-Guided Dense Prediction with Context-Aware Prompting](2112.01518v2.md)

[2112.01455v2 Zero-Shot Text-Guided Object Generation with Dream Fields](2112.01455v2.md)

[2112.01071v2 Extract Free Dense Labels from CLIP](2112.01071v2.md)

[2112.00719v2 HyperInverter  Improving StyleGAN Inversion via Hypernetwork](2112.00719v2.md)

[2112.00374v3 CLIPstyler  Image Style Transfer with a Single Text Condition](2112.00374v3.md)

[2111.15666v2 HyperStyle  StyleGAN Inversion with HyperNetworks for Real Image Editing](2111.15666v2.md)

[2111.15640v3 Diffusion Autoencoders  Toward a Meaningful and Decodable Representation](2111.15640v3.md)

[2111.15078v1 SketchEdit  Mask-Free Local Image Manipulation with Partial Sketches](2111.15078v1.md)

[2111.14818v2 Blended Diffusion for Text-driven Editing of Natural Images](2111.14818v2.md)

[2111.14447v2 ZeroCap  Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic](2111.14447v2.md)

[2111.13010v1 Attribute-specific Control Units in StyleGAN for Fine-grained Image Manipulation](2111.13010v1.md)

[2111.12417v1 NÜWA  Visual Synthesis Pre-training for Neural visUal World creAtion](2111.12417v1.md)

[2111.12159v1 Rhythm is a Dancer  Music-Driven Motion Synthesis with Global Structure](2111.12159v1.md)

[2111.11426v4 Neural Fields in Visual Computing and Beyond](2111.11426v4.md)

[2111.10050v3 Combined Scaling for Zero-shot Transfer Learning](2111.10050v3.md)

[2111.08851v5 Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities](2111.08851v5.md)

[2111.07991v3 LiT  Zero-Shot Transfer with Locked-image text Tuning](2111.07991v3.md)

[2111.07783v1 FILIP  Fine-grained Interactive Language-Image Pre-Training](2111.07783v1.md)

[2111.05610v2 CLIP2TV  Align, Match and Distill for Video-Text Retrieval](2111.05610v2.md)

[2111.03930v2 Tip-Adapter  Training-free CLIP-Adapter for Better Vision-Language Modeling](2111.03930v2.md)

[2111.03186v1 EditGAN  High-Precision Semantic Image Editing](2111.03186v1.md)

[2111.01619v1 StyleGAN of All Trades  Image Manipulation with Only Pretrained StyleGAN](2111.01619v1.md)

[2110.09788v1 CIPS-3D  A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis](2110.09788v1.md)

[2110.09383v1 Neuro-Symbolic Forward Reasoning](2110.09383v1.md)

[2110.08398v2 Mind the Gap  Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks](2110.08398v2.md)

[2110.06848v3 Decoupled Contrastive Learning](2110.06848v3.md)

[2110.05433v1 Mesh Draping  Parametrization-Free Neural Mesh Transfer](2110.05433v1.md)

[2110.05208v2 Supervision Exists Everywhere  A Data Efficient Contrastive Language-Image Pre-training Paradigm](2110.05208v2.md)

[2110.04544v1 CLIP-Adapter  Better Vision-Language Models with Feature Adapters](2110.04544v1.md)

[2110.02037v2 Autoregressive Diffusion Models](2110.02037v2.md)

[2110.00948v2 Interactive Segmentation for COVID-19 Infection Quantification on Longitudinal CT scans](2110.00948v2.md)

[2109.14084v2 VideoCLIP  Contrastive Pre-training for Zero-shot Video-Text Understanding](2109.14084v2.md)

[2109.13912v2 PDC-Net+  Enhanced Probabilistic Dense Correspondence Network](2109.13912v2.md)

[2109.12098v1 CLIPort  What and Where Pathways for Robotic Manipulation](2109.12098v1.md)

[2109.11797v3 CPT  Colorful Prompt Tuning for Pre-trained Vision-Language Models](2109.11797v3.md)

[2109.04448v1 Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers](2109.04448v1.md)

[2108.08782v1 Causal Attention for Unbiased Visual Recognition](2108.08782v1.md)

[2108.03489v1 Impact of Aliasing on Generalization in Deep Convolutional Networks](2108.03489v1.md)

[2108.02671v2 Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware](2108.02671v2.md)

[2108.01073v2 SDEdit  Guided Image Synthesis and Editing with Stochastic Differential Equations](2108.01073v2.md)

[2108.00946v2 StyleGAN-NADA  CLIP-Guided Domain Adaptation of Image Generators](2108.00946v2.md)

[2107.12518v2 Segmentation in Style  Unsupervised Semantic Image Segmentation with Stylegan and CLIP](2107.12518v2.md)

[2107.11186v1 LARGE  Latent-Based Regression through GAN Semantics](2107.11186v1.md)

[2107.08430v2 YOLOX  Exceeding YOLO Series in 2021](2107.08430v2.md)

[2107.07437v1 StyleFusion  A Generative Model for Disentangling Spatial Segments](2107.07437v1.md)

[2107.06383v1 How Much Can CLIP Benefit Vision-and-Language Tasks?](2107.06383v1.md)

[2107.00650v2 CLIP-It! Language-Guided Video Summarization](2107.00650v2.md)

[2106.14851v2 Data Poisoning Won't Save You From Facial Recognition](2106.14851v2.md)

[2106.14843v1 CLIPDraw  Exploring Text-to-Drawing Synthesis through Language-Image Encoders](2106.14843v1.md)

[2106.13884v2 Multimodal Few-Shot Learning with Frozen Language Models](2106.13884v2.md)

[2106.11097v1 CLIP2Video  Mastering Video-Text Retrieval via Image CLIP](2106.11097v1.md)

[2106.09696v2 BABEL  Bodies, Action and Behavior with English Labels](2106.09696v2.md)

[2106.09685v2 LoRA  Low-Rank Adaptation of Large Language Models](2106.09685v2.md)

[2106.09679v1 JOKR  Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting](2106.09679v1.md)

[2106.09141v1 Probing Image-Language Transformers for Verb Understanding](2106.09141v1.md)

[2106.05744v1 Pivotal Tuning for Latent-based Editing of Real Images](2106.05744v1.md)

[2106.05665v2 Learning Runtime Decisions for Adaptive Real-Time Perception](2106.05665v2.md)

[2106.04489v1 Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](2106.04489v1.md)

[2106.04477v2 MoCo-Flow  Neural Motion Consensus Flow for Dynamic Humans in Stationary Monocular Cameras](2106.04477v2.md)

[2106.02795v3 Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding](2106.02795v3.md)

[2106.00329v3 Consistent Two-Flow Network for Tele-Registration of Point Clouds](2106.00329v3.md)

[2106.00178v3 Language-Driven Image Style Transfer](2106.00178v3.md)

[2105.14548v2 Z2P  Instant Visualization of Point Clouds](2105.14548v2.md)

[2105.11087v1 Recent Advances and Trends in Multimodal Deep Learning  A Review](2105.11087v1.md)

[2105.08222v1 Decorating Your Own Bedroom  Locally Controlling Image Generation with Generative Adversarial Networks](2105.08222v1.md)

[2105.05233v4 Diffusion Models Beat GANs on Image Synthesis](2105.05233v4.md)

[2105.02769v1 Computer-Aided Design as Language](2105.02769v1.md)

[2105.01937v4 FLEX  Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction](2105.01937v4.md)

[2105.01604v1 Orienting Point Clouds with Dipole Propagation](2105.01604v1.md)

[2105.00162v2 Generative Art Using Neural Visual Grammars and Dual Encoders](2105.00162v2.md)

[2104.14806v1 GODIVA  Generating Open-DomaIn Videos from nAtural Descriptions](2104.14806v1.md)

[2104.14754v2 Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing](2104.14754v2.md)

[2104.10054v1 T2VLAD  Global-Local Sequence Alignment for Text-Video Retrieval](2104.10054v1.md)

[2104.09621v1 Engineering Sketch Generation for Computer-Aided Design](2104.09621v1.md)

[2104.09125v2 SAPE  Spatially-Adaptive Progressive Encoding for Neural Optimization](2104.09125v2.md)

[2104.08860v2 CLIP4Clip  An Empirical Study of CLIP for End to End Video Clip Retrieval](2104.08860v2.md)

[2104.06820v1 Few-shot Image Generation via Cross-domain Correspondence](2104.06820v1.md)

[2104.06490v2 DatasetGAN  Efficient Labeled Data Factory with Minimal Human Effort](2104.06490v2.md)

[2104.05764v1 Domain Adaptive Monocular Depth Estimation With Semantic Information](2104.05764v1.md)

[2104.03960v1 Modulated Periodic Activations for Generalizable Local Functional Representations](2104.03960v1.md)

[2104.03308v3 Warp Consistency for Unsupervised Learning of Dense Correspondences](2104.03308v3.md)

[2104.02699v2 ReStyle  A Residual-Based StyleGAN Encoder via Iterative Refinement](2104.02699v2.md)

[2104.00677v1 Putting NeRF on a Diet  Semantically Consistent Few-Shot View Synthesis](2104.00677v1.md)

[2104.00670v1 Unconstrained Scene Generation with Locally Conditioned Radiance Fields](2104.00670v1.md)

[2103.17249v1 StyleCLIP  Text-Driven Manipulation of StyleGAN Imagery](2103.17249v1.md)

[2103.15812v4 LatentKeypointGAN  Controlling GANs via Latent Keypoints](2103.15812v4.md)

[2103.15679v1 Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](2103.15679v1.md)

[2103.15536v1 Cloud2Curve  Generation and Vectorization of Parametric Sketches](2103.15536v1.md)

[2103.14024v2 PlenOctrees for Real-time Rendering of Neural Radiance Fields](2103.14024v2.md)

[2103.13413v1 Vision Transformers for Dense Prediction](2103.13413v1.md)

[2103.05247v2 Pretrained Transformers as Universal Computation Engines](2103.05247v2.md)

[2103.03243v1 Anycost GANs for Interactive Image Synthesis and Editing](2103.03243v1.md)

[2103.02992v1 Clusterplot  High-dimensional Cluster Visualization](2103.02992v1.md)

[2103.00020v1 Learning Transferable Visual Models From Natural Language Supervision](2103.00020v1.md)

[2102.10407v5 VisualGPT  Data-efficient Adaptation of Pretrained Language Models for Image Captioning](2102.10407v5.md)

[2102.07074v4 TransGAN  Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up](2102.07074v4.md)

[2102.06108v1 SWAGAN  A Style-based Wavelet-driven Generative Model](2102.06108v1.md)

[2102.05918v2 Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](2102.05918v2.md)

[2102.02779v2 Unifying Vision-and-Language Tasks via Text Generation](2102.02779v2.md)

[2102.02766v1 Designing an Encoder for StyleGAN Image Manipulation](2102.02766v1.md)

[2102.02754v2 Only a Matter of Style  Age Transformation Using a Style-Based Regression Model](2102.02754v2.md)

[2102.00529v1 Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers](2102.00529v1.md)

[2101.07922v2 LowKey  Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition](2101.07922v2.md)

[2101.05278v5 GAN Inversion  A Survey](2101.05278v5.md)

[2101.01710v2 Learning Accurate Dense Correspondences and When to Trust Them](2101.01710v2.md)

[2101.00190v1 Prefix-Tuning  Optimizing Continuous Prompts for Generation](2101.00190v1.md)

[2012.12352v4 Seeing past words  Testing the cross-modal capabilities of pretrained V&L models on counting tasks](2012.12352v4.md)

[2012.08673v2 A Closer Look at the Robustness of Vision-and-Language Pre-trained Models](2012.08673v2.md)

[2012.07463v2 Parameter-Efficient Transfer Learning with Diff Pruning](2012.07463v2.md)

[2012.05901v2 Robust Consistent Video Depth Estimation](2012.05901v2.md)

[2012.03308v3 TediGAN  Text-Guided Diverse Face Image Generation and Manipulation](2012.03308v3.md)

[2012.02189v2 Learned Initializations for Optimizing Coordinate-Based Neural Representations](2012.02189v2.md)

[2012.00926v2 pi-GAN  Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis](2012.00926v2.md)

[2011.12026v2 Adversarial Generation of Continuous Images](2011.12026v2.md)

[2011.03775v2 Text-to-Image Generation Grounded by Fine-Grained User Attention](2011.03775v2.md)

[2010.10596v3 Counterfactual Explanations and Algorithmic Recourses for Machine Learning  A Review](2010.10596v3.md)

[2010.02502v4 Denoising Diffusion Implicit Models](2010.02502v4.md)

[2010.02315v1 SMILE  Semantically-guided Multi-attribute Image and Layout Editing](2010.02315v1.md)

[2010.02036v2 BalaGAN  Image Translation Between Imbalanced Domains via Cross-Modal Transfer](2010.02036v2.md)

[2009.13856v1 Neural Alignment for Face De-pixelization](2009.13856v1.md)

[2009.09144v1 Differentiable Refraction-Tracing for Mesh Reconstruction of Transparent Objects](2009.09144v1.md)

[2009.07823v4 GOCor  Bringing Globally Optimized Correspondence Volumes into Your Neural Network](2009.07823v4.md)

[2009.02276v2 Witches' Brew  Industrial Scale Data Poisoning via Gradient Matching](2009.02276v2.md)

[2009.02216v1 SketchPatch  Sketch Stylization via Seamless Patch-level Synthesis](2009.02216v1.md)

[2009.01579v1 DESC  Domain Adaptation for Depth Estimation via Semantic Consistency](2009.01579v1.md)

[2008.08999v1 Object Properties Inferring from and Transfer for Human Interaction Motions](2008.08999v1.md)

[2008.06471v3 Self-Sampling for Neural Point Cloud Consolidation](2008.06471v3.md)

[2008.04556v4 Text as Neural Operator  Image Manipulation by Text Instruction](2008.04556v4.md)

[2008.04200v1 Describe What to Change  A Text-guided Unsupervised Image-to-Image Translation Approach](2008.04200v1.md)

[2008.00951v2 Encoding in Style  a StyleGAN Encoder for Image-to-Image Translation](2008.00951v2.md)

[2007.14628v2 Solving the Blind Perspective-n-Point Problem End-To-End With Robust Differentiable Geometric Optimization](2007.14628v2.md)

[2007.12944v1 MRGAN  Multi-Rooted 3D Shape Generation with Unsupervised Part Disentanglement](2007.12944v1.md)

[2007.11301v3 DeepSVG  A Hierarchical Generative Network for Vector Graphics Animation](2007.11301v3.md)

[2007.06676v3 UnRectDepthNet  Self-Supervised Monocular Depth Estimation using a Generic Framework for Handling Common Camera Distortion Models](2007.06676v3.md)

[2007.06600v4 Closed-Form Factorization of Latent Semantics in GANs](2007.06600v4.md)

[2007.02190v2 BézierSketch  A generative model for scalable vector sketches](2007.02190v2.md)

[2007.00074v1 Deep Geometric Texture Synthesis](2007.00074v1.md)

[2006.12075v1 MotioNet  3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency](2006.12075v1.md)

[2006.12030v1 DO-Conv  Depthwise Over-parameterized Convolutional Layer](2006.12030v1.md)

[2006.11239v2 Denoising Diffusion Probabilistic Models](2006.11239v2.md)

[2006.10739v1 Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](2006.10739v1.md)

[2006.10569v2 Towards a Neural Graphics Pipeline for Controllable Image Generation](2006.10569v2.md)

[2006.09011v2 Improved Techniques for Training Score-Based Generative Models](2006.09011v2.md)

[2006.06676v2 Training Generative Adversarial Networks with Limited Data](2006.06676v2.md)

[2006.05724v1 Real-time single image depth perception in the wild with handheld devices](2006.05724v1.md)

[2005.12420v2 Network Bending  Expressive Manipulation of Deep Generative Models](2005.12420v2.md)

[2005.11084v1 Point2Mesh  A Self-Prior for Deformable Meshes](2005.11084v1.md)

[2005.07728v3 Face Identity Disentanglement via Latent Space Mapping](2005.07728v3.md)

[2005.05751v1 Unpaired Motion Style Transfer from Video to Animation](2005.05751v1.md)

[2005.05732v1 Skeleton-Aware Networks for Deep Motion Retargeting](2005.05732v1.md)

[2005.00770v2 Exploring and Predicting Transferability across NLP Tasks](2005.00770v2.md)

[2004.14367v2 Editing in Style  Uncovering the Local Semantics of GANs](2004.14367v2.md)

[2004.14071v1 Image Morphing with Perceptual Constraints and STN Alignment](2004.14071v1.md)

[2004.09965v4 Single Pair Cross-Modality Super Resolution](2004.09965v4.md)

[2004.04977v2 SESAME  Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects](2004.04977v2.md)

[2004.02546v3 GANSpace  Discovering Interpretable GAN Controls](2004.02546v3.md)

[2004.02222v3 Structural-analogy from a Single Image Pair](2004.02222v3.md)

[2003.13326v1 PointGMM  a Neural GMM Network for Point Clouds](2003.13326v1.md)

[2003.08934v2 NeRF  Representing Scenes as Neural Radiance Fields for View Synthesis](2003.08934v2.md)

[2003.08073v1 Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation](2003.08073v1.md)

[2003.07238v2 A Rotation-Invariant Framework for Deep Point Cloud Analysis](2003.07238v2.md)

[2003.01279v3 Disrupting Deepfakes  Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems](2003.01279v3.md)

[2003.00196v3 First Order Motion Model for Image Animation](2003.00196v3.md)

[2002.12655v2 A U-Net Based Discriminator for Generative Adversarial Networks](2002.12655v2.md)

[2002.10102v5 GANHopper  Multi-Hop GAN for Unsupervised Image-to-Image Translation](2002.10102v5.md)

[2002.05867v2 Transformers as Soft Reasoners over Language](2002.05867v2.md)

[2001.08740v2 Audiovisual SlowFast Networks for Video Recognition](2001.08740v2.md)

[2001.03640v2 Unsupervised multi-modal Styled Content Generation](2001.03640v2.md)

[2001.02600v3 Deep Learning for Free-Hand Sketch  A Survey](2001.02600v3.md)

[1912.12396v1 MulGAN  Facial Attribute Editing by Exemplar](1912.12396v1.md)

[1912.10637v3 GrabAR  Occlusion-aware Grabbing Virtual Objects in AR](1912.10637v3.md)

[1912.05524v3 GLU-Net  Global-Local Universal Network for Dense Flow and Correspondences](1912.05524v3.md)

[1912.04958v2 Analyzing and Improving the Image Quality of StyleGAN](1912.04958v2.md)

[1911.02001v1 Dancing to Music](1911.02001v1.md)

[1910.01442v2 CLEVRER  CoLlision Events for Video REpresentation and Reasoning](1910.01442v2.md)

[1909.03459v1 Blind Geometric Distortion Correction on Images Through Deep Learning](1909.03459v1.md)

[1907.10844v1 PU-GAN  a Point Cloud Upsampling Adversarial Network](1907.10844v1.md)

[1907.06571v2 Adversarial Video Generation on Complex Datasets](1907.06571v2.md)

[1907.05600v3 Generative Modeling by Estimating Gradients of the Data Distribution](1907.05600v3.md)

[1907.01341v3 Towards Robust Monocular Depth Estimation  Mixing Datasets for Zero-shot Cross-dataset Transfer](1907.01341v3.md)

[1907.01108v2 Language2Pose  Natural Language Grounded Pose Forecasting](1907.01108v2.md)

[1906.07901v1 Multimodal Abstractive Summarization for How2 Videos](1906.07901v1.md)

[1906.02361v1 Explain Yourself! Leveraging Language Models for Commonsense Reasoning](1906.02361v1.md)

[1906.01526v1 Cross-Domain Cascaded Deep Feature Translation](1906.01526v1.md)

[1905.02175v4 Adversarial Examples Are Not Bugs, They Are Features](1905.02175v4.md)

[1905.01684v2 Unsupervised Detection of Distinctive Regions on 3D Shapes](1905.01684v2.md)

[1905.01680v1 Learning Character-Agnostic Motion for Motion Retargeting in 2D](1905.01680v1.md)

[1904.11486v2 Making Convolutional Networks Shift-Invariant Again](1904.11486v2.md)

[1904.09709v1 STGAN  A Unified Selective Transfer Network for Arbitrary Image Attribute Editing](1904.09709v1.md)

[1904.08475v2 Image Resizing by Reconstruction from Deep Features](1904.08475v2.md)

[1904.06913v4 Implicit Pairs for Boosting Unpaired Image-to-Image Translation](1904.06913v4.md)

[1904.03349v3 Progressive Pose Attention Transfer for Person Image Generation](1904.03349v3.md)

[1904.02756v1 Blind Visual Motif Removal from a Single Image](1904.02756v1.md)

[1904.02028v1 CAM-Convs  Camera-Aware Multi-Scale Convolutions for Single-View Depth](1904.02028v1.md)

[1903.10170v3 LOGAN  Unpaired Shape Transform in Latent Overcomplete Space](1903.10170v3.md)

[1902.07987v2 Learning representations of irregular particle-detector geometry with distance-weighted graph networks](1902.07987v2.md)

[1901.04544v2 PointWise  An Unsupervised Point-wise Feature Learning Network](1901.04544v2.md)

[1901.04530v2 CrossNet  Latent Cross-Consistency for Unpaired Image Translation](1901.04530v2.md)

[1812.08861v3 Animating Arbitrary Objects via Deep Motion Transfer](1812.08861v3.md)

[1812.04948v3 A Style-Based Generator Architecture for Generative Adversarial Networks](1812.04948v3.md)

[1812.03982v3 SlowFast Networks for Video Recognition](1812.03982v3.md)

[1812.02784v2 StoryGAN  A Sequential Conditional GAN for Story Visualization](1812.02784v2.md)

[1811.11286v3 Patch-based Progressive 3D Point Set Upsampling](1811.11286v3.md)

[1811.10597v2 GAN Dissection  Visualizing and Understanding Generative Adversarial Networks](1811.10597v2.md)

[1811.10153v2 Spatially Controllable Image Synthesis with Internal Representation Collaging](1811.10153v2.md)

[1811.07441v4 CompoNet  Learning to Generate the Unseen by Part Synthesis and Composition](1811.07441v4.md)

[1810.08363v1 Generative Low-Shot Network Expansion](1810.08363v1.md)

[1809.11096v2 Large Scale GAN Training for High Fidelity Natural Image Synthesis](1809.11096v2.md)

[1809.05910v2 MeshCNN  A Network with an Edge](1809.05910v2.md)

[1808.07371v2 Everybody Dance Now](1808.07371v2.md)

[1808.06847v1 Deep Video-Based Performance Cloning](1808.06847v1.md)

[1808.06601v2 Video-to-Video Synthesis](1808.06601v2.md)

[1808.03981v4 SAGNet Structure-aware Generative Network for 3D-Shape Modeling](1808.03981v4.md)

[1807.11152v1 Pose Guided Human Video Generation](1807.11152v1.md)

[1807.09193v5 GRAINS  Generative Recursive Autoencoders for INdoor Scenes](1807.09193v5.md)

[1807.06010v1 EC-Net  an Edge-aware Point set Consolidation Network](1807.06010v1.md)

[1807.03130v1 Unsupervised Natural Image Patch Learning](1807.03130v1.md)

[1806.10206v5 Deep Feature Factorization For Concept Discovery](1806.10206v5.md)

[1805.04487v1 Non-Stationary Texture Synthesis by Adversarial Expansion](1805.04487v1.md)

[1805.04140v2 Neural Best-Buddies  Sparse Cross-Domain Correspondence](1805.04140v2.md)

[1804.08497v2 ALIGNet  Partial-Shape Agnostic Alignment via Unsupervised Learning](1804.08497v2.md)

[1803.10039v1 Learning Depth from Single Images with Deep Neural Network Embedding Focal Length](1803.10039v1.md)

[1803.09263v4 P2P-NET  Bidirectional Point Displacement Net for Shape Transform](1803.09263v4.md)

[1803.08457v5 Clustering-driven Deep Embedding with Pairwise Constraints](1803.08457v5.md)

[1802.02341v1 Outlier Detection for Robust Multi-dimensional Scaling](1802.02341v1.md)

[1801.06761v2 PU-Net  Point Cloud Upsampling Network](1801.06761v2.md)

[1711.08278v1 Neuron-level Selective Context Aggregation for Scene Segmentation](1711.08278v1.md)

[1711.00937v2 Neural Discrete Representation Learning](1711.00937v2.md)

[1710.10196v3 Progressive Growing of GANs for Improved Quality, Stability, and Variation](1710.10196v3.md)

[1710.00421v1 Video Generation From Text](1710.00421v1.md)

[1709.05424v2 NIMA  Neural Image Assessment](1709.05424v2.md)

[1708.05980v3 Attentive Semantic Video Generation using Captions](1708.05980v3.md)

[1707.05572v1 Fast Feature Fool  A data independent approach to universal adversarial perturbations](1707.05572v1.md)

[1707.04993v2 MoCoGAN  Decomposing Motion and Content for Video Generation](1707.04993v2.md)

[1706.08033v2 Decomposing Motion and Content for Natural Video Sequence Prediction](1706.08033v2.md)

[1706.03762v5 Attention Is All You Need](1706.03762v5.md)

[1705.09406v2 Multimodal Machine Learning  A Survey and Taxonomy](1705.09406v2.md)

[1703.09928v3 Bundle Optimization for Multi-aspect Embedding](1703.09928v3.md)

[1703.06114v3 Deep Sets](1703.06114v3.md)

[1701.08931v1 Co-segmentation for Space-Time Co-located Collections](1701.08931v1.md)

[1612.06890v1 CLEVR  A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](1612.06890v1.md)

[1612.04869v2 Border-Peeling Clustering](1612.04869v2.md)

[1611.07004v3 Image-to-Image Translation with Conditional Adversarial Networks](1611.07004v3.md)

[1611.06624v3 Temporal Generative Adversarial Nets with Singular Value Clipping](1611.06624v3.md)

[1609.09106v4 HyperNetworks](1609.09106v4.md)

[1609.02612v3 Generating Videos with Scene Dynamics](1609.02612v3.md)

[1608.05180v2 A Holistic Approach for Data-Driven Object Cutout](1608.05180v2.md)

[1604.02703v6 Synthesizing Training Images for Boosting Human 3D Pose Estimation](1604.02703v6.md)

[1512.04150v1 Learning Deep Features for Discriminative Localization](1512.04150v1.md)

[1511.07247v3 NetVLAD  CNN architecture for weakly supervised place recognition](1511.07247v3.md)

[1511.06434v2 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](1511.06434v2.md)

[1511.02799v4 Neural Module Networks](1511.02799v4.md)

[1510.03023v1 Printed Perforated Lampshades for Continuous Projective Images](1510.03023v1.md)

[1506.06724v1 Aligning Books and Movies  Towards Story-like Visual Explanations by Watching Movies and Reading Books](1506.06724v1.md)

[1505.01809v3 Language Models for Image Captioning  The Quirks and What Works](1505.01809v3.md)

[1503.03585v8 Deep Unsupervised Learning using Nonequilibrium Thermodynamics](1503.03585v8.md)

[1412.2306v2 Deep Visual-Semantic Alignments for Generating Image Descriptions](1412.2306v2.md)

[1411.2539v1 Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](1411.2539v1.md)

[1406.5679v1 Deep Fragment Embeddings for Bidirectional Image Sentence Mapping](1406.5679v1.md)

[1301.3666v2 Zero-Shot Learning Through Cross-Modal Transfer](1301.3666v2.md)

[1212.4490v1 Sketch-to-Design  Context-based Part Assembly](1212.4490v1.md)

