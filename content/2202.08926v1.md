---
title: 2202.08926v1 On Guiding Visual Attention with Language Specification
date: 2022-02-09
---

# [On Guiding Visual Attention with Language Specification](http://arxiv.org/abs/2202.08926v1)

authors: Suzanne Petryk, Lisa Dunlap, Keyan Nasseri, Joseph Gonzalez, Trevor Darrell, Anna Rohrbach


## What, Why and How

[1]: https://arxiv.org/pdf/2202.08926v1.pdf "On Guiding Visual Attention with Language Speciﬁcation - arXiv.org"
[2]: https://arxiv.org/abs/2202.08926 "On Guiding Visual Attention with Language Specification"
[3]: http://export.arxiv.org/abs/2112.08926v1 "[2112.08926v1] On the accuracy and performance of the lattice Boltzmann ..."

Here is a summary of the paper:

- **What**: The paper proposes a new framework called **Guiding visual Attention with Language Specification (GALS)**, which uses language specification of visual categories to supervise the spatial attention of a convolutional neural network (CNN) classifier[^1^][1].
- **Why**: The paper aims to address the problem of visual classification with biased and noisy data, where the CNN may learn unwanted correlations or distractors instead of task-relevant features. The paper argues that language specification can provide a useful prior for disambiguating what features are important for the task[^1^][1].
- **How**: The paper uses a pretrained large-scale multimodal model to ground task-relevant words or phrases from the language specification into attention maps. These attention maps are then used to guide the CNN's attention away from distracting context during training. The paper evaluates the proposed method on several classification tasks with biased and noisy data, and shows that it improves performance and fairness metrics compared to baseline models[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2202.08926v1.pdf "On Guiding Visual Attention with Language Speciﬁcation - arXiv.org"
[2]: https://arxiv.org/abs/2202.08926 "On Guiding Visual Attention with Language Specification"
[3]: http://export.arxiv.org/abs/2112.08926v1 "[2112.08926v1] On the accuracy and performance of the lattice Boltzmann ..."

The paper claims the following contributions:

- A novel framework for **guiding visual attention with language specification** that leverages a pretrained multimodal model to ground task-relevant words or phrases into spatial attention maps[^1^][1].
- A method for **supervising the spatial attention of a CNN classifier** with the attention maps generated by the multimodal model, which can constrain the classifier to focus on task-relevant features and ignore distractors[^1^][1].
- An extensive evaluation of the proposed method on several **visual classification tasks with biased and noisy data**, such as Waterbirds, CelebA, and iNaturalist, where it outperforms baseline models and improves worst-group accuracy and fairness metrics[^1^][1].

## Method Summary

[1]: https://arxiv.org/pdf/2202.08926v1.pdf "On Guiding Visual Attention with Language Speciﬁcation - arXiv.org"
[2]: https://arxiv.org/abs/2202.08926 "On Guiding Visual Attention with Language Specification"
[3]: http://export.arxiv.org/abs/2112.08926v1 "[2112.08926v1] On the accuracy and performance of the lattice Boltzmann ..."

Here is a summary of the method section of the paper:

- The paper describes the proposed framework **GALS** in three steps: **language specification**, **attention grounding**, and **attention supervision**[^1^][1].
- In the **language specification** step, the paper assumes that each visual category is associated with a language word or phrase that describes the task-relevant features. For example, for the Waterbirds dataset, the language specification is "a photo of a bird"[^1^][1].
- In the **attention grounding** step, the paper uses a pretrained multimodal model, such as CLIP [33], to generate an attention map for each image given the language specification. The attention map highlights the regions of the image that are relevant for the language query. The paper uses a simple thresholding technique to binarize the attention map and obtain a binary mask[^1^][1].
- In the **attention supervision** step, the paper uses the binary mask to supervise the spatial attention of a CNN classifier during training. The paper modifies the standard cross-entropy loss function to penalize the classifier for attending to regions outside the mask. The paper also introduces a regularization term to encourage sparsity and diversity of attention[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a set of images X, a set of labels Y, a language specification L
# Output: a trained CNN classifier f
# Hyperparameters: alpha, beta, gamma, tau

# Initialize a pretrained multimodal model M (e.g., CLIP)
# Initialize a CNN classifier f with random weights
# Initialize an optimizer opt for f

# For each epoch:
  # Shuffle X and Y
  # For each batch of images x and labels y:
    # Generate attention maps A = M(x, L) # A is a tensor of shape [batch_size, height, width]
    # Binarize A by thresholding at tau to get binary masks M # M is a tensor of shape [batch_size, height, width]
    # Compute logits z = f(x) # z is a tensor of shape [batch_size, num_classes]
    # Compute cross-entropy loss L_ce = -sum(y * log(softmax(z))) / batch_size
    # Compute spatial attention maps S = f.get_attention(x) # S is a tensor of shape [batch_size, height, width]
    # Compute attention supervision loss L_as = -sum(M * log(S) + (1 - M) * log(1 - S)) / batch_size
    # Compute attention regularization loss L_ar = alpha * entropy(S) + beta * diversity(S) + gamma * sparsity(S)
    # Compute total loss L_total = L_ce + L_as + L_ar
    # Update f by opt.step(L_total)
  # Evaluate f on validation set and save the best model
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensors and neural networks
import torchvision # for image datasets and transforms
import clip # for pretrained multimodal model
import timm # for CNN models

# Define the hyperparameters
num_epochs = 100 # number of training epochs
batch_size = 32 # batch size for training and validation
learning_rate = 1e-4 # learning rate for optimizer
alpha = 0.01 # weight for entropy regularization
beta = 0.01 # weight for diversity regularization
gamma = 0.01 # weight for sparsity regularization
tau = 0.5 # threshold for binarizing attention maps

# Load the dataset and the dataloaders
dataset = torchvision.datasets.ImageFolder("path/to/dataset") # load the image dataset from a folder
transform = torchvision.transforms.Compose([ # define the image transformations
  torchvision.transforms.Resize((224, 224)), # resize the images to 224 x 224 pixels
  torchvision.transforms.ToTensor(), # convert the images to tensors
  torchvision.transforms.Normalize((0.481, 0.457, 0.408), (0.268, 0.261, 0.276)) # normalize the images with mean and std from CLIP
])
dataset = dataset.map(transform) # apply the transformations to the dataset
train_set, val_set = torch.utils.data.random_split(dataset, [0.8 * len(dataset), 0.2 * len(dataset)]) # split the dataset into train and validation sets
train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True) # create the train dataloader
val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False) # create the validation dataloader

# Load the pretrained multimodal model and freeze its parameters
model = clip.load("ViT-B/32", device="cuda") # load the CLIP model with vision transformer backbone on GPU
for param in model.parameters():
  param.requires_grad = False # freeze the model parameters

# Define the language specification as a tensor
language = "a photo of a bird" # define the language specification as a string
language = clip.tokenize(language).to("cuda") # tokenize the language and move it to GPU

# Define the CNN classifier and the optimizer
classifier = timm.create_model("resnet18", pretrained=True, num_classes=len(dataset.classes)) # create a ResNet-18 classifier with pretrained weights
classifier.to("cuda") # move the classifier to GPU
optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate) # create an Adam optimizer for the classifier

# Define the loss functions
cross_entropy_loss = torch.nn.CrossEntropyLoss() # define the cross-entropy loss function

def attention_supervision_loss(mask, attention):
  # mask: a tensor of shape [batch_size, height, width] with binary values indicating task-relevant regions
  # attention: a tensor of shape [batch_size, height, width] with values in [0, 1] indicating spatial attention
  return -torch.mean(mask * torch.log(attention) + (1 - mask) * torch.log(1 - attention)) # return the negative log-likelihood loss

def entropy(attention):
  # attention: a tensor of shape [batch_size, height, width] with values in [0, 1] indicating spatial attention
  return -torch.mean(attention * torch.log(attention) + (1 - attention) * torch.log(1 - attention)) # return the entropy of attention

def diversity(attention):
  # attention: a tensor of shape [batch_size, height, width] with values in [0, 1] indicating spatial attention
  batch_size = attention.size(0) # get the batch size
  attention = attention.view(batch_size, -1) # flatten the attention maps to vectors of shape [batch_size, height * width]
  gram_matrix = torch.matmul(attention, attention.t()) / (height * width) # compute the Gram matrix of shape [batch_size, batch_size]
  identity_matrix = torch.eye(batch_size).to("cuda") # create an identity matrix of shape [batch_size, batch_size] on GPU
  return torch.mean((gram_matrix - identity_matrix) ** 2) # return the mean squared error between Gram matrix and identity matrix

def sparsity(attention):
  # attention: a tensor of shape [batch_size, height, width] with values in [0, 1] indicating spatial attention
  return torch.mean(torch.abs(attention)) # return the mean absolute value of attention

# Define a function to get the spatial attention map from the classifier
def get_attention(classifier, x):
  # classifier: a CNN model
  # x: a tensor of shape [batch_size, 3, height, width] with input images
  features = classifier.forward_features(x) # get the features from the classifier of shape [batch_size, channels, height, width]
  attention = torch.mean(features, dim=1) # average the features across channels to get attention of shape [batch_size, height, width]
  attention = torch.sigmoid(attention) # apply sigmoid to get values in [0, 1]
  return attention # return the attention map

# Define a function to evaluate the classifier on the validation set
def evaluate(classifier, val_loader):
  # classifier: a CNN model
  # val_loader: a dataloader for the validation set
  classifier.eval() # set the classifier to evaluation mode
  correct = 0 # initialize the number of correct predictions
  total = 0 # initialize the total number of predictions
  with torch.no_grad(): # disable gradient computation
    for x, y in val_loader: # iterate over the validation batches
      x = x.to("cuda") # move the images to GPU
      y = y.to("cuda") # move the labels to GPU
      z = classifier(x) # get the logits from the classifier
      _, y_pred = torch.max(z, dim=1) # get the predicted labels by taking the argmax of logits
      correct += torch.sum(y_pred == y) # count the number of correct predictions
      total += y.size(0) # count the total number of predictions
  accuracy = correct / total # compute the accuracy
  return accuracy # return the accuracy

# Train the classifier with GALS
best_accuracy = 0 # initialize the best accuracy on validation set
best_model = None # initialize the best model
for epoch in range(num_epochs): # iterate over the epochs
  classifier.train() # set the classifier to training mode
  for x, y in train_loader: # iterate over the train batches
    x = x.to("cuda") # move the images to GPU
    y = y.to("cuda") # move the labels to GPU

    # Generate attention maps from the multimodal model
    with torch.no_grad(): # disable gradient computation
      image_features = model.encode_image(x) # encode the images into features of shape [batch_size, 512]
      text_features = model.encode_text(language) # encode the language into features of shape [1, 512]
      logits_per_image = image_features @ text_features.t() # compute the logits per image of shape [batch_size, 1]
      probs = logits_per_image.softmax(dim=-1) # compute the probabilities by applying softmax
      attention_maps = model.visual.attn_blocks[-1].attn_probs[0] # get the last attention maps from the multimodal model of shape [12, batch_size, height/32, width/32]
      attention_maps = torch.einsum("nchw,n->nchw", attention_maps, probs.squeeze()) # weight the attention maps by the probabilities
      attention_maps = torch.mean(attention_maps, dim=0) # average the attention maps across heads of shape [batch_size, height/32, width/32]
      attention_maps = torch.nn.functional.interpolate(attention_maps.unsqueeze(1), size=(height, width)) # upsample the attention maps to match the image size of shape [batch_size, 1, height, width]
      attention_maps = attention_maps.squeeze(1) # remove the singleton dimension of shape [batch_size, height, width]

    # Binarize attention maps by thresholding at tau
    masks = (attention_maps > tau).float() # apply thresholding to get binary masks of shape [batch_size, height, width]

    optimizer.zero_grad() # reset the optimizer gradients

    # Compute logits from the classifier
    z = classifier(x) # get the logits from the classifier of shape [batch_size, num_classes]

    # Compute cross-entropy loss
    L_ce = cross_entropy_loss(z, y) # compute the cross-entropy loss

    # Compute spatial attention maps from the classifier
    S = get_attention(classifier, x) # get the spatial attention maps from the classifier of shape [batch_size, height, width]

    # Compute attention supervision loss
    L_as = attention_supervision_loss(masks, S) # compute the attention supervision loss

    # Compute attention regularization loss
    L_ar = alpha * entropy(S) + beta * diversity(S) + gamma * sparsity(S) # compute the attention regularization loss

    # Compute total loss
    L_total = L_ce + L_as + L_ar # compute the total loss

    L_total.backward() # compute the gradients