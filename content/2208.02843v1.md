---
title: 2208.02843v1 TIC  Text-Guided Image Colorization
date: 2022-08-03
---

# [TIC: Text-Guided Image Colorization](http://arxiv.org/abs/2208.02843v1)

authors: Subhankar Ghosh, Prasun Roy, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein


## What, Why and How

[1]: https://arxiv.org/abs/2208.02843v1 "[2208.02843v1] TIC: Text-Guided Image Colorization - arXiv.org"
[2]: https://arxiv.org/pdf/2208.02843.pdf "1 TIC: Text-Guided Image Colorization - arXiv.org"
[3]: http://export.arxiv.org/abs/2201.02843v1 "[2201.02843v1] Theoretical Calculation of the Quadratic Zeeman Shift ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel deep network called **TIC: Text-Guided Image Colorization** that takes two inputs (a grayscale image and a text description) and tries to predict the relevant color gamut for the image.
- **Why**: The paper aims to improve the fidelity of the image colorization process by integrating textual descriptions as an auxiliary condition, along with the grayscale image that is to be colorized. The paper claims that this is one of the first attempts to incorporate textual conditioning in the colorization pipeline.
- **How**: The paper uses a generative adversarial network (GAN) framework that consists of a generator and a discriminator. The generator takes the grayscale image and the encoded text description as inputs and produces a colorized image. The discriminator takes the colorized image and the text description as inputs and tries to distinguish between real and fake images. The paper uses different loss functions to train the network, such as perceptual loss, adversarial loss, reconstruction loss, and color consistency loss. The paper evaluates the proposed model using different metrics and datasets and compares it with state-of-the-art colorization algorithms.

## Main Contributions

The contributions of this paper are:

- It proposes a novel GAN pipeline that exploits textual descriptions as an auxiliary condition for image colorization.
- It extensively evaluates the framework using qualitative and quantitative measures and shows that it outperforms the state-of-the-art colorization algorithms.

## Method Summary

The method section of the above paper describes the proposed network architecture and the loss functions used to train the model. The network consists of two components: a generator and a discriminator. The generator takes a grayscale image and a text description as inputs and produces a colorized image as output. The text description is encoded using a pre-trained BERT model and concatenated with the grayscale image. The generator uses a U-Net structure with skip connections and residual blocks. The discriminator takes a colorized image and a text description as inputs and tries to classify the image as real or fake. The discriminator uses a PatchGAN structure with convolutional layers and spectral normalization. The paper uses four loss functions to train the model: perceptual loss, adversarial loss, reconstruction loss, and color consistency loss. The perceptual loss measures the feature similarity between the colorized image and the ground truth image using a pre-trained VGG-16 model. The adversarial loss measures the ability of the generator to fool the discriminator and the ability of the discriminator to distinguish between real and fake images. The reconstruction loss measures the pixel-wise difference between the colorized image and the ground truth image. The color consistency loss measures the difference between the predicted colors and the colors mentioned in the text description. The paper uses a weighted sum of these loss functions to optimize the model parameters.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator and discriminator networks
generator = UNet(grayscale_image, text_description)
discriminator = PatchGAN(colorized_image, text_description)

# Define the loss functions
perceptual_loss = VGG16(colorized_image, ground_truth_image)
adversarial_loss = BCE(discriminator(colorized_image, text_description), real_or_fake_label)
reconstruction_loss = L1(colorized_image, ground_truth_image)
color_consistency_loss = L1(predicted_colors, text_colors)

# Define the total loss
total_loss = perceptual_loss + lambda1 * adversarial_loss + lambda2 * reconstruction_loss + lambda3 * color_consistency_loss

# Train the model
for epoch in epochs:
  for batch in batches:
    # Get the grayscale image, text description, and ground truth image from the batch
    grayscale_image, text_description, ground_truth_image = batch

    # Generate the colorized image using the generator
    colorized_image = generator(grayscale_image, text_description)

    # Compute the losses using the loss functions
    perceptual_loss = VGG16(colorized_image, ground_truth_image)
    adversarial_loss = BCE(discriminator(colorized_image, text_description), real_or_fake_label)
    reconstruction_loss = L1(colorized_image, ground_truth_image)
    color_consistency_loss = L1(predicted_colors, text_colors)

    # Compute the total loss
    total_loss = perceptual_loss + lambda1 * adversarial_loss + lambda2 * reconstruction_loss + lambda3 * color_consistency_loss

    # Update the model parameters using backpropagation and an optimizer
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import transformers
import numpy as np
import PIL.Image as Image

# Define the hyperparameters
batch_size = 16 # The number of images and texts in a batch
num_epochs = 100 # The number of epochs to train the model
learning_rate = 0.0002 # The learning rate for the optimizer
lambda1 = 0.001 # The weight for the adversarial loss
lambda2 = 10 # The weight for the reconstruction loss
lambda3 = 1 # The weight for the color consistency loss

# Define the image and text transforms
image_transform = transforms.Compose([
  transforms.Resize((256, 256)), # Resize the image to 256x256 pixels
  transforms.ToTensor(), # Convert the image to a tensor
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize the image with mean and std of 0.5
])

text_transform = transformers.BertTokenizer.from_pretrained('bert-base-uncased') # Use a pre-trained BERT tokenizer

# Define the generator network
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()

    # Define the encoder part of the U-Net
    self.encoder1 = nn.Sequential(
      nn.Conv2d(4, 64, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 4 (3 for grayscale image and 1 for text encoding) and output channels 64
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder2 = nn.Sequential(
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 64 and output channels 128
      nn.BatchNorm2d(128), # Batch normalization layer with output channels 128
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder3 = nn.Sequential(
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 128 and output channels 256
      nn.BatchNorm2d(256), # Batch normalization layer with output channels 256
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder4 = nn.Sequential(
      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 256 and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder5 = nn.Sequential(
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 512 and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder6 = nn.Sequential(
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 512 and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder7 = nn.Sequential(
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 512 and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.LeakyReLU(0.2) # Leaky ReLU activation function with negative slope 0.2
    )

    self.encoder8 = nn.Sequential(
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # Convolutional layer with input channels 512 and output channels 512
      nn.ReLU() # ReLU activation function
    )

    # Define the decoder part of the U-Net
    self.decoder1 = nn.Sequential(
      nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 512 and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.Dropout(0.5), # Dropout layer with probability 0.5
      nn.ReLU() # ReLU activation function
    )

    self.decoder2 = nn.Sequential(
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 1024 (512 from decoder1 and 512 from encoder7) and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.Dropout(0.5), # Dropout layer with probability 0.5
      nn.ReLU() # ReLU activation function
    )

    self.decoder3 = nn.Sequential(
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 1024 (512 from decoder2 and 512 from encoder6) and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.Dropout(0.5), # Dropout layer with probability 0.5
      nn.ReLU() # ReLU activation function
    )

    self.decoder4 = nn.Sequential(
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 1024 (512 from decoder3 and 512 from encoder5) and output channels 512
      nn.BatchNorm2d(512), # Batch normalization layer with output channels 512
      nn.ReLU() # ReLU activation function
    )

    self.decoder5 = nn.Sequential(
      nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 1024 (512 from decoder4 and 512 from encoder4) and output channels 256
      nn.BatchNorm2d(256), # Batch normalization layer with output channels 256
      nn.ReLU() # ReLU activation function
    )

    self.decoder6 = nn.Sequential(
      nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 512 (256 from decoder5 and 256 from encoder3) and output channels 128
      nn.BatchNorm2d(128), # Batch normalization layer with output channels 128
      nn.ReLU() # ReLU activation function
    )

    self.decoder7 = nn.Sequential(
      nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 256 (128 from decoder6 and 128 from encoder2) and output channels 64
      nn.BatchNorm2d(64), # Batch normalization layer with output channels 64
      nn.ReLU() # ReLU activation function
    )

    self.decoder8 = nn.Sequential(
      nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1), # Transposed convolutional layer with input channels 128 (64 from decoder7 and 64 from encoder1) and output channels 3 (for RGB image)
      nn.Tanh() # Tanh activation function to produce values between -1 and +1
    )

    # Define the residual blocks to improve the performance of the network
    self.residual_block = lambda x: x + self.residual(x) # A residual block that adds the input x to the output of a residual function

    self.residual = lambda x: self.residual_conv(self.residual_relu(self.residual_bn(x))) # A residual function that applies a batch normalization, a ReLU activation and a convolutional layer to the input x

    self.residual_bn = lambda x: self.bn(self.conv(x)) # A batch normalization layer that applies a convolutional layer before the batch normalization

    self.residual_relu = lambda x: self.relu(x) # A ReLU activation function

    self.residual_conv = lambda x: self.conv(x) # A convolutional layer

    self.bn = lambda x: torch.nn.BatchNorm2d(x