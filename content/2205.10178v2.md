---
title: 2205.10178v2 Visually-Augmented Language Modeling
date: 2022-05-11
---

# [Visually-Augmented Language Modeling](http://arxiv.org/abs/2205.10178v2)

authors: Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei


## What, Why and How

[1]: https://arxiv.org/abs/2205.10178 "[2205.10178] Visually-Augmented Language Modeling - arXiv.org"
[2]: https://arxiv.org/pdf/2205.10178v2 "VISUALLY-AUGMENTED LANGUAGE MODELING - arXiv.org"
[3]: http://export.arxiv.org/abs/2110.10178 "[2110.10178] Repulsive Black Holes and Higher-Derivatives"

Here is a summary of the paper at [^1^][2] in terms of What, Why, and How:

- **What**: The paper proposes a novel pre-training framework, named **VaLM**, to **Visually-augment text tokens with retrieved relevant images for Language Modeling**.
- **Why**: The paper aims to address the limitation of current large-scale pre-trained language models that rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary.
- **How**: The paper builds on a novel **text-vision alignment method** via an image retrieval module that fetches corresponding images given a textual context. With the visually-augmented context, VaLM uses a **visual knowledge fusion layer** to enable multimodal grounded language modeling by attending on both text context and visual knowledge in images.

## Main Contributions

[1]: https://arxiv.org/abs/2205.10178 "[2205.10178] Visually-Augmented Language Modeling - arXiv.org"
[2]: https://arxiv.org/pdf/2205.10178v2 "VISUALLY-AUGMENTED LANGUAGE MODELING - arXiv.org"
[3]: http://export.arxiv.org/abs/2110.10178 "[2110.10178] Repulsive Black Holes and Higher-Derivatives"

According to the paper at [^1^][2], the main contributions are:

- **A novel pre-training framework, VaLM**, that visually-augments text tokens with retrieved relevant images for language modeling.
- **A novel text-vision alignment method**, that leverages an image retrieval module to fetch corresponding images given a textual context.
- **A novel visual knowledge fusion layer**, that enables multimodal grounded language modeling by attending on both text context and visual knowledge in images.
- **Empirical evidence** that VaLM outperforms the text-only baseline on various multimodal commonsense reasoning tasks, such as object color and size reasoning.

## Method Summary

[1]: https://arxiv.org/abs/2205.10178 "[2205.10178] Visually-Augmented Language Modeling - arXiv.org"
[2]: https://arxiv.org/pdf/2205.10178v2 "VISUALLY-AUGMENTED LANGUAGE MODELING - arXiv.org"
[3]: http://export.arxiv.org/abs/2110.10178 "[2110.10178] Repulsive Black Holes and Higher-Derivatives"

Here is a summary of the method section of the paper at [^1^][2]:

- The paper introduces a novel pre-training framework, VaLM, that consists of three main components: **a text encoder**, **an image retrieval module**, and **a visual knowledge fusion layer**.
- The **text encoder** is a Transformer-based model that encodes the input text tokens into contextualized embeddings. The text encoder can be initialized with any pre-trained language model, such as BERT or GPT-3.
- The **image retrieval module** is a neural network that takes the text embeddings as input and outputs a set of image queries for each token. The image queries are used to retrieve relevant images from an external image database, such as ImageNet or Conceptual Captions. The retrieved images are then resized and encoded into visual embeddings using a pre-trained vision encoder, such as ResNet or ViT.
- The **visual knowledge fusion layer** is a Transformer-based model that fuses the text embeddings and the visual embeddings into multimodal embeddings. The fusion layer uses a cross-attention mechanism to attend on both text context and visual knowledge in images. The fusion layer also applies a masking strategy to mask out some text tokens or images during training, which encourages the model to learn from both modalities. The output of the fusion layer is used for language modeling objectives, such as masked language modeling or next token prediction.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the text encoder, image retrieval module, and visual knowledge fusion layer
text_encoder = Transformer(pretrained_model)
image_retrieval = NeuralNetwork()
vision_encoder = CNN(pretrained_model)
fusion_layer = Transformer()

# Define the image database and the language modeling objectives
image_database = ImageNet or ConceptualCaptions
lm_objectives = MaskedLM or NextTokenPrediction

# Pre-train VaLM on a large text corpus
for each text in text_corpus:
  # Encode the text tokens into embeddings
  text_embeddings = text_encoder(text)

  # Generate image queries for each token
  image_queries = image_retrieval(text_embeddings)

  # Retrieve relevant images for each token from the image database
  images = retrieve_images(image_queries, image_database)

  # Resize and encode the images into embeddings
  image_embeddings = vision_encoder(images)

  # Fuse the text embeddings and the image embeddings into multimodal embeddings
  multimodal_embeddings = fusion_layer(text_embeddings, image_embeddings)

  # Apply masking strategy to mask out some text tokens or images
  masked_text, masked_images = mask(text, images)

  # Compute the loss and update the parameters using the language modeling objectives
  loss = lm_objectives(masked_text, masked_images, multimodal_embeddings)
  update_parameters(loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import faiss

# Define the hyperparameters
batch_size = 256
max_length = 512
num_epochs = 100
learning_rate = 1e-4
image_size = 224
num_images = 5
mask_prob = 0.15

# Define the text encoder, image retrieval module, and visual knowledge fusion layer
text_encoder = transformers.AutoModel.from_pretrained("bert-base-uncased")
image_retrieval = torch.nn.Linear(text_encoder.config.hidden_size, num_images)
vision_encoder = torchvision.models.resnet50(pretrained=True)
fusion_layer = transformers.EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")

# Define the image database and the index
image_database = torchvision.datasets.ImageNet(root="data", split="train", transform=torchvision.transforms.ToTensor())
image_index = faiss.IndexFlatL2(vision_encoder.fc.out_features)
image_index.add(torch.stack([vision_encoder(image) for image in image_database]))

# Define the tokenizer and the language modeling objectives
tokenizer = transformers.AutoTokenizer.from_pretrained("bert-base-uncased")
masked_lm = transformers.BertForMaskedLM.from_pretrained("bert-base-uncased")
next_token_prediction = transformers.BertLMHeadModel.from_pretrained("bert-base-uncased")

# Define the optimizer and the scheduler
optimizer = torch.optim.Adam(fusion_layer.parameters(), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

# Pre-train VaLM on a large text corpus
for epoch in range(num_epochs):
  for batch in text_corpus.batch(batch_size):
    # Tokenize the text and pad to max length
    input_ids = tokenizer(batch, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt").input_ids

    # Encode the text tokens into embeddings
    text_embeddings = text_encoder(input_ids).last_hidden_state

    # Generate image queries for each token
    image_queries = image_retrieval(text_embeddings)

    # Retrieve relevant images for each token from the image database using faiss
    images = []
    for query in image_queries:
      distances, indices = image_index.search(query, num_images)
      images.append(torch.stack([image_database[i] for i in indices]))

    # Resize and encode the images into embeddings
    images = torchvision.transforms.Resize(image_size)(torch.stack(images))
    image_embeddings = vision_encoder(images)

    # Fuse the text embeddings and the image embeddings into multimodal embeddings
    multimodal_embeddings = fusion_layer(text_embeddings, image_embeddings).last_hidden_state

    # Apply masking strategy to mask out some text tokens or images with mask_prob probability
    masked_text = input_ids.clone()
    masked_images = images.clone()
    for i in range(batch_size):
      for j in range(max_length):
        if torch.rand(1) < mask_prob:
          masked_text[i][j] = tokenizer.mask_token_id
          masked_images[i][j] = torch.zeros(image_size, image_size, 3)

    # Compute the loss and update the parameters using the language modeling objectives
    loss_masked_lm = masked_lm(masked_text, labels=input_ids).loss
    loss_next_token_prediction = next_token_prediction(multimodal_embeddings[:, :-1], labels=input_ids[:, 1:]).loss
    loss = loss_masked_lm + loss_next_token_prediction
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    scheduler.step()

    # Print the loss and the progress
    print(f"Epoch {epoch}, Batch {batch}, Loss {loss}")
```