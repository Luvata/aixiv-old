---
title: 2005.07728v3 Face Identity Disentanglement via Latent Space Mapping
date: 2020-05-08
---

# [Face Identity Disentanglement via Latent Space Mapping](http://arxiv.org/abs/2005.07728v3)

authors: Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2005.07728 "Face Identity Disentanglement via Latent Space Mapping"
[2]: https://arxiv.org/pdf/2005.07728 "arXiv.org e-Print archive"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method that learns how to represent data in a disentangled way, with minimal supervision, using a pre-trained image generator. The method focuses on the domain of human faces and aims to separate identity from other facial attributes.
- **Why**: The paper argues that learning disentangled representations of data is a fundamental problem in artificial intelligence, as it allows generative models to control and compose the disentangled factors in the synthesis process. The paper also claims that current methods require extensive supervision and training, or compromise quality.
- **How**: The paper's key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, the paper leverages both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. The paper demonstrates its approach on the complex and high dimensional domain of human heads, and evaluates it qualitatively and quantitatively, showing that it successfully disentangles identity from other facial attributes, surpassing existing methods.

## Main Contributions

[1]: https://arxiv.org/abs/2005.07728 "Face Identity Disentanglement via Latent Space Mapping"
[2]: https://arxiv.org/pdf/2005.07728 "arXiv.org e-Print archive"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

According to the paper[^1^][1], the main contributions are:

- **A novel method** that learns how to represent data in a disentangled way, with minimal supervision, using a pre-trained image generator.
- **A decoupling scheme** that separates the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN.
- **A latent space mapping technique** that leverages both the state-of-the-art quality and the rich and expressive latent space of the pre-trained image generator, without the burden of training it.
- **An extensive evaluation** that demonstrates the success of the method on the complex and high dimensional domain of human heads, showing that it surpasses existing methods in terms of identity disentanglement, de-identification operations, and temporal identity coherency.

## Method Summary

[1]: https://arxiv.org/abs/2005.07728 "Face Identity Disentanglement via Latent Space Mapping"
[2]: https://arxiv.org/pdf/2005.07728 "arXiv.org e-Print archive"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the method section of the paper[^1^][1]:

- The method consists of three main components: a pre-trained image generator, a latent space mapping network, and a disentanglement loss function.
- The pre-trained image generator is an unconditional generative adversarial network (GAN), such as StyleGAN, that can synthesize realistic images of human faces from latent vectors. The latent space of the generator is assumed to be rich and expressive, capturing various facial attributes and variations.
- The latent space mapping network is a neural network that learns to map an input image into a latent vector that can be fed into the pre-trained image generator to reconstruct the input image. The mapping network is trained with a reconstruction loss that measures the pixel-wise difference between the input and the output images, and a perceptual loss that measures the feature-wise difference between the input and the output images using a pre-trained face recognition network.
- The disentanglement loss function is designed to encourage the mapping network to separate identity from other facial attributes in the latent space. The loss function consists of two terms: an identity preservation term and an identity variation term. The identity preservation term penalizes the mapping network for changing the identity of the input image when modifying other facial attributes, such as expression or pose. The identity variation term penalizes the mapping network for keeping the same identity of the input image when modifying the identity attribute. The loss function is computed using a pre-trained face recognition network that can extract identity features from images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load a pre-trained image generator G and a pre-trained face recognition network F
G = load_generator()
F = load_face_recognizer()

# Define a latent space mapping network M
M = define_mapping_network()

# Define a reconstruction loss L_r that measures the pixel-wise and feature-wise difference between images
L_r = define_reconstruction_loss()

# Define a disentanglement loss L_d that measures the identity preservation and variation between images
L_d = define_disentanglement_loss()

# Initialize the parameters of M randomly
M.init_parameters()

# For each iteration
for i in range(iterations):

  # Sample a batch of input images X
  X = sample_images()

  # Compute the latent vectors Z by applying M to X
  Z = M(X)

  # Compute the output images Y by applying G to Z
  Y = G(Z)

  # Compute the reconstruction loss between X and Y
  R = L_r(X, Y)

  # Compute the disentanglement loss between X and Y using F
  D = L_d(X, Y, F)

  # Compute the total loss as a weighted sum of R and D
  L = alpha * R + beta * D

  # Update the parameters of M by minimizing L using gradient descent
  M.update_parameters(L)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import stylegan2_pytorch as sg2
import facenet_pytorch as fn

# Load a pre-trained image generator G and a pre-trained face recognition network F
G = sg2.load_pretrained_model('stylegan2-ffhq-config-f.pt')
F = fn.InceptionResnetV1(pretrained='vggface2').eval()

# Define a latent space mapping network M as a fully connected network with ReLU activations
M = torch.nn.Sequential(
  torch.nn.Linear(3 * 256 * 256, 1024),
  torch.nn.ReLU(),
  torch.nn.Linear(1024, 512),
  torch.nn.ReLU(),
  torch.nn.Linear(512, 512)
)

# Define a reconstruction loss L_r that measures the pixel-wise and feature-wise difference between images
def L_r(X, Y):
  # Compute the pixel-wise mean squared error between X and Y
  pixel_loss = torch.nn.functional.mse_loss(X, Y)

  # Compute the feature-wise mean squared error between X and Y using F
  feature_loss = torch.nn.functional.mse_loss(F(X), F(Y))

  # Return the weighted sum of pixel_loss and feature_loss
  return pixel_loss + feature_loss

# Define a disentanglement loss L_d that measures the identity preservation and variation between images
def L_d(X, Y, F):
  # Compute the identity features of X and Y using F
  I_X = F(X)
  I_Y = F(Y)

  # Compute the cosine similarity between I_X and I_Y
  sim = torch.nn.functional.cosine_similarity(I_X, I_Y)

  # Compute the identity preservation term as the mean squared error between sim and 1
  id_preserve = torch.nn.functional.mse_loss(sim, torch.ones_like(sim))

  # Compute the identity variation term as the mean squared error between sim and -1
  id_vary = torch.nn.functional.mse_loss(sim, -torch.ones_like(sim))

  # Return the weighted sum of id_preserve and id_vary
  return id_preserve + id_vary

# Initialize the parameters of M randomly
M.apply(torch.nn.init.xavier_uniform_)

# Define an optimizer for M using Adam with a learning rate of 0.001
optimizer = torch.optim.Adam(M.parameters(), lr=0.001)

# Define a data loader for input images X using ImageFolder with a batch size of 32 and random cropping and resizing to (256, 256)
data_loader = torch.utils.data.DataLoader(
  torchvision.datasets.ImageFolder('data', transform=torchvision.transforms.Compose([
    torchvision.transforms.RandomCrop(256),
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor()
  ])),
  batch_size=32,
  shuffle=True
)

# Define the number of iterations as the number of batches in the data loader
iterations = len(data_loader)

# Define the weights alpha and beta for the reconstruction loss and the disentanglement loss as 0.5 and 0.5 respectively
alpha = 0.5
beta = 0.5

# For each iteration
for i in range(iterations):

  # Get a batch of input images X from the data loader
  X, _ = next(iter(data_loader))

  # Compute the latent vectors Z by applying M to X
  Z = M(X.view(-1, 3 * 256 * 256))

  # Compute the output images Y by applying G to Z
  Y = G(Z)

  # Compute the reconstruction loss between X and Y
  R = L_r(X, Y)

  # Compute the disentanglement loss between X and Y using F
  D = L_d(X, Y, F)

  # Compute the total loss as a weighted sum of R and D
  L = alpha * R + beta * D

  # Update the parameters of M by minimizing L using gradient descent
  optimizer.zero_grad()
  L.backward()
  optimizer.step()
```