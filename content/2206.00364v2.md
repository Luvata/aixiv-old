---
title: 2206.00364v2 Elucidating the Design Space of Diffusion-Based Generative Models
date: 2022-06-01
---

# [Elucidating the Design Space of Diffusion-Based Generative Models](http://arxiv.org/abs/2206.00364v2)

authors: Tero Karras, Miika Aittala, Timo Aila, Samuli Laine


## What, Why and How

[1]: https://arxiv.org/pdf/2206.00364v2.pdf "Elucidating the Design Space of Diffusion-Based Generative Models"
[2]: https://arxiv.org/abs/2206.00364 "Elucidating the Design Space of Diffusion-Based Generative Models"
[3]: https://arxiv-export1.library.cornell.edu/abs/2206.00364 "[2206.00364] Elucidating the Design Space of Diffusion-Based Generative ..."

Here is a summary of the paper:

- **What**: The paper is about **diffusion-based generative models**, which are a type of neural network models that can synthesize realistic images from noise by reversing a diffusion process[^1^][1].
- **Why**: The paper aims to **elucidate the design space** of these models and to **identify and propose improvements** to their sampling and training processes, as well as their score network preconditioning[^1^][1].
- **How**: The paper presents a **design space** that clearly separates the concrete design choices of diffusion-based generative models, such as the noise level parameterization, the sampling schedule, the training dynamics, and the score network architecture[^1^][1]. The paper then introduces several changes to these components, such as using a **linear noise level parameterization**, a **uniform sampling schedule**, a **reverse KL divergence loss**, and a **residual score network** with **spectral normalization** and **skip connections**[^1^][1]. The paper evaluates these changes on various image datasets and shows that they lead to **new state-of-the-art results** in terms of image quality, training cost, and generation speed[^1^][1].

## Main Contributions

The paper makes the following contributions:

- It presents a **design space** that clearly separates the concrete design choices of diffusion-based generative models and provides insights into their effects and interactions.
- It proposes several changes to the sampling and training processes of these models, such as using a **linear noise level parameterization**, a **uniform sampling schedule**, and a **reverse KL divergence loss**.
- It proposes several changes to the score network preconditioning, such as using a **residual score network** with **spectral normalization** and **skip connections**.
- It demonstrates that these changes lead to **new state-of-the-art results** on various image datasets, such as CIFAR-10 and ImageNet-64, in terms of image quality (measured by FID), training cost (measured by GPU hours), and generation speed (measured by network evaluations per image).
- It shows that these changes are **modular** and can be applied to improve the efficiency and quality of pre-trained score networks from previous work.

## Method Summary

[1]: https://arxiv.org/pdf/2206.00364v2.pdf "Elucidating the Design Space of Diffusion-Based Generative Models"
[2]: https://arxiv.org/abs/2206.00364 "Elucidating the Design Space of Diffusion-Based Generative Models"
[3]: https://arxiv-export1.library.cornell.edu/abs/2206.00364 "[2206.00364] Elucidating the Design Space of Diffusion-Based Generative ..."

Here is a summary of the method section of the paper:

- The paper presents a **design space** for diffusion-based generative models that consists of four components: the **noise level parameterization**, the **sampling schedule**, the **training dynamics**, and the **score network architecture**[^1^][1].
- The paper introduces several changes to each component and explains their rationale and effects. For example, the paper proposes to use a **linear noise level parameterization** that simplifies the sampling process and reduces the number of network evaluations per image[^1^][1]. The paper also proposes to use a **uniform sampling schedule** that avoids the need for annealing and improves the quality of intermediate samples[^1^][1].
- The paper compares different choices for each component and shows their impact on image quality, training cost, and generation speed. For example, the paper shows that using a **reverse KL divergence loss** instead of a forward KL divergence loss leads to lower FID scores and faster convergence[^1^][1]. The paper also shows that using a **residual score network** with **spectral normalization** and **skip connections** improves the stability and expressiveness of the score function[^1^][1].
- The paper evaluates the proposed changes on various image datasets, such as CIFAR-10 and ImageNet-64, and shows that they lead to new state-of-the-art results in terms of FID, GPU hours, and network evaluations per image[^1^][1]. The paper also shows that these changes are modular and can be applied to improve the efficiency and quality of pre-trained score networks from previous work[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the noise level parameterization
def noise_level(t):
  return 1 - t / T # linear parameterization

# Define the sampling schedule
def sampling_schedule(t):
  return t # uniform schedule

# Define the training dynamics
def training_dynamics(x_t, x_t_1, s_t):
  return reverse_kl_divergence(x_t, x_t_1, s_t) # reverse KL loss

# Define the score network architecture
def score_network(x_t):
  return residual_network(x_t) # residual network with spectral normalization and skip connections

# Train the score network
for each batch of images x:
  for t in range(1, T + 1):
    # Corrupt the images with Gaussian noise
    x_t = x + noise_level(t) * N(0, I)
    # Compute the score of the noisy images
    s_t = score_network(x_t)
    # Corrupt the images again with Gaussian noise
    x_t_1 = x_t + sqrt(noise_level(t) - noise_level(t + 1)) * N(0, I)
    # Compute the training loss
    loss = training_dynamics(x_t, x_t_1, s_t)
    # Update the score network parameters
    update_parameters(loss)

# Sample images from the score network
for each batch of noise vectors z:
  # Initialize the images with noise
  x_T = z
  for t in range(T, 0, -1):
    # Compute the score of the noisy images
    s_t = score_network(x_t)
    # Refine the images by reversing the diffusion process
    x_t_1 = (x_t - sqrt(noise_level(t) - noise_level(t - 1)) * N(0, I)) / (1 - noise_level(t - 1)) + sqrt(noise_level(t - 1) / (1 - noise_level(t - 1))) * s_t
  # Return the final images
  return x_0
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the hyperparameters
T = 100 # number of diffusion steps
batch_size = 64 # batch size for training and sampling
image_size = 32 # image size for CIFAR-10 dataset
num_channels = 3 # number of channels for CIFAR-10 dataset
num_classes = 10 # number of classes for CIFAR-10 dataset
num_epochs = 100 # number of epochs for training
learning_rate = 0.0002 # learning rate for optimizer
beta_1 = 0.9 # beta_1 for optimizer
beta_2 = 0.999 # beta_2 for optimizer

# Define the noise level parameterization
def noise_level(t):
  return 1 - t / T # linear parameterization

# Define the sampling schedule
def sampling_schedule(t):
  return t # uniform schedule

# Define the reverse KL divergence loss
def reverse_kl_divergence(x_t, x_t_1, s_t):
  # Compute the mean and variance of the conditional distribution p(x_t | x_t+1)
  mean = (x_t - sqrt(noise_level(t) - noise_level(t + 1)) * N(0, I)) / (1 - noise_level(t + 1))
  var = noise_level(t + 1) / (1 - noise_level(t + 1))
  # Compute the log probability of x_t under p(x_t | x_t+1)
  log_prob = -0.5 * ((x_t - mean) ** 2 / var + log(2 * pi * var))
  # Compute the score of x_t under p(x_t | x_t+1)
  score = (x_t - mean) / var
  # Compute the reverse KL divergence loss
  loss = torch.mean(0.5 * s_t ** 2 - s_t * score + log_prob)
  return loss

# Define the residual block for the score network
class ResBlock(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
    super(ResBlock, self).__init__()
    # Define the convolutional layers with spectral normalization and ReLU activation
    self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))
    self.relu1 = nn.ReLU()
    self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding))
    self.relu2 = nn.ReLU()
    # Define the skip connection if the input and output channels are different
    self.skip = None
    if in_channels != out_channels:
      self.skip = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1))

  def forward(self, x):
    # Apply the first convolutional layer and activation
    out = self.conv1(x)
    out = self.relu1(out)
    # Apply the second convolutional layer and activation
    out = self.conv2(out)
    out = self.relu2(out)
    # Add the skip connection if any
    if self.skip is not None:
      x = self.skip(x)
    out = out + x
    return out

# Define the score network architecture
class ScoreNetwork(nn.Module):
  def __init__(self):
    super(ScoreNetwork, self).__init__()
    # Define the input layer with spectral normalization and ReLU activation
    self.input_layer = nn.utils.spectral_norm(nn.Conv2d(num_channels + num_classes + T + T ** 2, num_channels * T ** (3/4), kernel_size=3, stride=1, padding=1))
    self.input_relu = nn.ReLU()
    # Define the residual blocks with spectral normalization and ReLU activation
    self.res_blocks = nn.ModuleList()
    for i in range(int(log(T) / log(4))):
      in_channels = num_channels * T ** ((i + 3) / 4)
      out_channels = num_channels * T ** ((i + 4) / 4)
      res_block = ResBlock(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
      self.res_blocks.append(res_block)
    # Define the output layer with spectral normalization and tanh activation
    self.output_layer = nn.utils.spectral_norm(nn.Conv2d(num_channels * T ** (3/4), num_channels, kernel_size=3, stride=1, padding=1))
    self.output_tanh = nn.Tanh()

  def forward(self, x, y, t):
    # Concatenate the input image, the class label, the noise level, and the sampling step
    x = torch.cat([x, y, t * torch.ones_like(x[:, :1]), t ** 2 * torch.ones_like(x[:, :1])], dim=1)
    # Apply the input layer and activation
    out = self.input_layer(x)
    out = self.input_relu(out)
    # Apply the residual blocks and activation
    for res_block in self.res_blocks:
      out = res_block(out)
    # Apply the output layer and activation
    out = self.output_layer(out)
    out = self.output_tanh(out)
    return out

# Load the CIFAR-10 dataset
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Create the score network and the optimizer
score_network = ScoreNetwork().to(device)
optimizer = optim.Adam(score_network.parameters(), lr=learning_rate, betas=(beta_1, beta_2))

# Train the score network
for epoch in range(num_epochs):
  for i, (x, y) in enumerate(train_loader):
    # Move the data to the device
    x = x.to(device)
    y = y.to(device)
    # One-hot encode the class labels
    y = F.one_hot(y, num_classes=num_classes).float()
    # Sample a random diffusion step
    t = torch.randint(1, T + 1, (batch_size,), device=device).float() / T
    # Corrupt the images with Gaussian noise
    x_t = x + noise_level(t) * torch.randn_like(x)
    # Compute the score of the noisy images
    s_t = score_network(x_t, y, t)
    # Corrupt the images again with Gaussian noise
    x_t_1 = x_t + torch.sqrt(noise_level(t) - noise_level(t + 1)) * torch.randn_like(x)
    # Compute the training loss
    loss = reverse_kl_divergence(x_t, x_t_1, s_t)
    # Update the score network parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # Print the epoch and the loss
  print(f'Epoch {epoch}, Loss {loss.item()}')

# Sample images from the score network
with torch.no_grad():
  for i in range(10):
    # Sample a random class label
    y = torch.randint(0, num_classes, (batch_size,), device=device)
    # One-hot encode the class label
    y = F.one_hot(y, num_classes=num_classes).float()
    # Initialize the images with noise
    x_T = torch.randn(batch_size, num_channels, image_size, image_size).to(device)
    for t in range(T - 1, -1, -1):
      # Compute the score of the noisy images
      s_t = score_network(x_T / sqrt(1 - noise_level(t)), y, t / T)
      # Refine the images by reversing the diffusion process
      x_T_1 = (x_T - sqrt(noise_level(t) - noise_level(t - 1)) * torch.randn_like(x_T)) / (1 - noise_level(t - 1)) + sqrt(noise_level(t - 1) / (1 - noise_level(t - 1))) * s_t
      x_T = x_T_1
    # Save the final images
    torchvision.utils.save_image(x_0 * 0.5 + 0.5, f'samples_{i}.png')
```