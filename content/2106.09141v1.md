---
title: 2106.09141v1 Probing Image-Language Transformers for Verb Understanding
date: 2021-06-10
---

# [Probing Image-Language Transformers for Verb Understanding](http://arxiv.org/abs/2106.09141v1)

authors: Lisa Anne Hendricks, Aida Nematzadeh


## What, Why and How

[1]: https://arxiv.org/pdf/2106.09141v1.pdf "Probing Image–Language Transformers for Verb Understanding - arXiv.org"
[2]: https://arxiv.org/abs/2106.09141 "Probing Image-Language Transformers for Verb Understanding"
[3]: https://arxiv.org/pdf/2106.10165.pdf "The Principles of Deep Learning Theory arXiv:2106.10165v2 [cs.LG] 24 ..."

Here is a summary of the paper:

```
What: The paper introduces a benchmark for probing image-language transformers for verb understanding, called SVO-Probes, which consists of image-sentence pairs where each pair is annotated with whether the sentence corresponds to the image or not.

Why: The paper aims to shed light on the quality of the pretrained representations of image-language transformers, and whether they can distinguish different types of verbs or rely solely on nouns in a given sentence.

How: The paper collects a dataset of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset), and uses it to evaluate several image-language transformers on different aspects of language (i.e., subject, verb, and object). The paper also investigates what category of verbs are particularly challenging for the models.
```


## Main Contributions

[1]: https://arxiv.org/pdf/2106.09141v1.pdf "Probing Image–Language Transformers for Verb Understanding - arXiv.org"
[2]: https://arxiv.org/abs/2106.09141 "Probing Image-Language Transformers for Verb Understanding"
[3]: https://arxiv.org/pdf/2106.10165.pdf "The Principles of Deep Learning Theory arXiv:2106.10165v2 [cs.LG] 24 ..."

Here are some of the contributions of the paper:

- The paper introduces a new benchmark for probing image-language transformers for verb understanding, called SVO-Probes, which consists of image-sentence pairs where each pair is annotated with whether the sentence corresponds to the image or not.
- The paper collects a dataset of 421 verbs that are either visual or commonly found in the pretraining data (i.e., the Conceptual Captions dataset), and uses it to evaluate several image-language transformers on different aspects of language (i.e., subject, verb, and object).
- The paper investigates what category of verbs are particularly challenging for the models, and provides analysis and insights on their performance.

## Method Summary

[1]: https://arxiv.org/pdf/2106.09141v1.pdf "Probing Image–Language Transformers for Verb Understanding - arXiv.org"
[2]: https://arxiv.org/abs/2106.09141 "Probing Image-Language Transformers for Verb Understanding"
[3]: https://arxiv.org/pdf/2106.10165.pdf "The Principles of Deep Learning Theory arXiv:2106.10165v2 [cs.LG] 24 ..."

Here is a summary of the method section of the paper:

```
The paper uses two types of methods to evaluate image-language transformers: (1) a binary classification task, where the models are given an image-sentence pair and asked to predict whether the sentence corresponds to the image or not, and (2) a ranking task, where the models are given a sentence and a set of images, and asked to rank the images according to their relevance to the sentence. The paper uses four image-language transformers as baselines: CLIP (Radford et al., 2021), ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), and UNITER (Chen et al., 2020). The paper also uses two ablation studies to investigate the effect of pretraining data and verb categories on the models' performance.
```


## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2106.09141v1.pdf "Probing Image–Language Transformers for Verb Understanding - arXiv.org"
[2]: https://arxiv.org/abs/2106.09141 "Probing Image-Language Transformers for Verb Understanding"
[3]: https://arxiv.org/pdf/2106.10165.pdf "The Principles of Deep Learning Theory arXiv:2106.10165v2 [cs.LG] 24 ..."

Here is a high-level pseudo code for the paper:

```
# Define a set of 421 verbs that are either visual or common in pretraining data
verbs = ["eat", "cut", "run", ...]

# Define a function to generate image-sentence pairs for each verb
def generate_pairs(verb):
  # Find images that contain the verb in their captions
  images = search_images(verb)
  # Randomly sample a positive image and its caption
  pos_image, pos_caption = sample(images)
  # Randomly sample three negative images that do not match the verb, subject, or object
  neg_images = sample(images, filter=lambda x: x != pos_image and x != pos_caption)
  # Return a list of image-sentence pairs with labels
  return [(pos_image, pos_caption, True)] + [(neg_image, pos_caption, False) for neg_image in neg_images]

# Define a function to evaluate a model on a binary classification task
def evaluate_binary(model, pairs):
  # For each pair, feed the image and the sentence to the model and get its prediction
  predictions = [model.predict(image, sentence) for image, sentence, label in pairs]
  # Compare the predictions with the ground truth labels and compute accuracy
  accuracy = compute_accuracy(predictions, labels)
  # Return the accuracy score
  return accuracy

# Define a function to evaluate a model on a ranking task
def evaluate_ranking(model, pairs):
  # For each pair, feed the sentence and all four images to the model and get its scores
  scores = [model.score(sentence, images) for images, sentence, label in pairs]
  # Sort the images by their scores and check if the positive image is ranked first
  ranks = [sort(images, key=lambda x: scores[x]) for images, sentence, label in pairs]
  hits = [ranks[i][0] == pairs[i][0] for i in range(len(pairs))]
  # Compute the percentage of hits and return it
  hit_rate = compute_percentage(hits)
  return hit_rate

# Define a list of image-language transformers to evaluate
models = [CLIP, ViLBERT, LXMERT, UNITER]

# For each model, load its pretrained weights and fine-tune it on SVO-Probes dataset
for model in models:
  model.load_pretrained()
  model.fine_tune(SVO_Probes)

# For each model and each verb, generate image-sentence pairs and evaluate the model on both tasks
results = {}
for model in models:
  results[model] = {}
  for verb in verbs:
    pairs = generate_pairs(verb)
    binary_accuracy = evaluate_binary(model, pairs)
    ranking_hit_rate = evaluate_ranking(model, pairs)
    results[model][verb] = (binary_accuracy, ranking_hit_rate)

# Analyze the results and report the performance of each model on different aspects of language
analyze_results(results)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2106.09141v1.pdf "Probing Image–Language Transformers for Verb Understanding - arXiv.org"
[2]: https://arxiv.org/abs/2106.09141 "Probing Image-Language Transformers for Verb Understanding"
[3]: https://arxiv.org/pdf/2106.10165.pdf "The Principles of Deep Learning Theory arXiv:2106.10165v2 [cs.LG] 24 ..."

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import pandas as pd
import numpy as np
import requests
import json

# Define a set of 421 verbs that are either visual or common in pretraining data
verbs = ["eat", "cut", "run", ...]

# Define a function to download images from Flickr given a query
def download_images(query, num=100):
  # Use the Flickr API to get the image URLs and captions for the query
  api_key = "your_api_key"
  url = f"https://www.flickr.com/services/rest/?method=flickr.photos.search&api_key={api_key}&text={query}&extras=url_o,description&format=json&nojsoncallback=1"
  response = requests.get(url)
  data = json.loads(response.text)
  photos = data["photos"]["photo"]
  # Download the images and save them in a folder named after the query
  folder = query.replace(" ", "_")
  os.makedirs(folder, exist_ok=True)
  images = []
  captions = []
  for photo in photos[:num]:
    image_url = photo["url_o"]
    caption = photo["description"]["_content"]
    image_name = image_url.split("/")[-1]
    image_path = os.path.join(folder, image_name)
    try:
      image_data = requests.get(image_url).content
      with open(image_path, "wb") as f:
        f.write(image_data)
      images.append(image_path)
      captions.append(caption)
    except:
      continue
  # Return a list of image paths and captions
  return images, captions

# Define a function to generate image-sentence pairs for each verb
def generate_pairs(verb):
  # Download images that contain the verb in their captions
  images, captions = download_images(verb)
  # Randomly sample a positive image and its caption
  pos_index = np.random.randint(len(images))
  pos_image = images[pos_index]
  pos_caption = captions[pos_index]
  # Randomly sample three negative images that do not match the verb, subject, or object
  neg_images = []
  while len(neg_images) < 3:
    neg_index = np.random.randint(len(images))
    if neg_index != pos_index:
      neg_image = images[neg_index]
      neg_caption = captions[neg_index]
      # Use spaCy to parse the sentences and extract the subject, verb, and object
      spacy_nlp = spacy.load("en_core_web_sm")
      pos_doc = spacy_nlp(pos_caption)
      neg_doc = spacy_nlp(neg_caption)
      pos_svo = [token.text for token in pos_doc if token.dep_ in ["nsubj", "ROOT", "dobj"]]
      neg_svo = [token.text for token in neg_doc if token.dep_ in ["nsubj", "ROOT", "dobj"]]
      # Check if any of the SVO elements match between the positive and negative sentences
      match = False
      for pos_element in pos_svo:
        for neg_element in neg_svo:
          if pos_element.lower() == neg_element.lower():
            match = True
            break
        if match:
          break
      # If no match, add the negative image to the list
      if not match:
        neg_images.append(neg_image)
  # Return a list of image-sentence pairs with labels
  return [(pos_image, pos_caption, True)] + [(neg_image, pos_caption, False) for neg_image in neg_images]

# Define a function to load an image and preprocess it for a model
def load_image(image_path, model_name):
  # Load the image using PIL and convert it to RGB mode
  image = Image.open(image_path).convert("RGB")
  # Resize and normalize the image according to the model's requirements
  if model_name == "CLIP":
    transform = torchvision.transforms.Compose([
        torchvision.transforms.Resize(224, interpolation=Image.BICUBIC),
        torchvision.transforms.CenterCrop(224),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])
  else:
    transform = torchvision.transforms.Compose([
        torchvision.transforms.Resize((224,224)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),
    ])
  image_tensor = transform(image).unsqueeze(0)
  # Return the image tensor
  return image_tensor

# Define a function to tokenize a sentence and preprocess it for a model
def tokenize_sentence(sentence, model_name, tokenizer):
  # Tokenize the sentence using the model's tokenizer
  tokens = tokenizer.encode(sentence, return_tensors="pt")
  # Add special tokens according to the model's requirements
  if model_name == "CLIP":
    tokens = torch.cat([tokens, torch.tensor([[tokenizer.eos_token_id]])], dim=-1)
  elif model_name == "ViLBERT":
    tokens = torch.cat([torch.tensor([[tokenizer.cls_token_id]]), tokens, torch.tensor([[tokenizer.sep_token_id]])], dim=-1)
  elif model_name == "LXMERT":
    tokens = torch.cat([torch.tensor([[tokenizer.cls_token_id]]), tokens, torch.tensor([[tokenizer.sep_token_id]])], dim=-1)
  elif model_name == "UNITER":
    tokens = torch.cat([torch.tensor([[tokenizer.cls_token_id]]), tokens, torch.tensor([[tokenizer.sep_token_id]])], dim=-1)
  # Return the token tensor
  return tokens

# Define a function to evaluate a model on a binary classification task
def evaluate_binary(model, pairs, model_name, tokenizer):
  # Initialize a list to store the predictions
  predictions = []
  # For each pair, feed the image and the sentence to the model and get its prediction
  for image, sentence, label in pairs:
    # Load and preprocess the image
    image_tensor = load_image(image, model_name)
    # Tokenize and preprocess the sentence
    token_tensor = tokenize_sentence(sentence, model_name, tokenizer)
    # Feed the image and the sentence to the model and get its output
    output = model(image_tensor, token_tensor)
    # Get the prediction from the output according to the model's head
    if model_name == "CLIP":
      prediction = output.logits_per_image[0].item()
    elif model_name == "ViLBERT":
      prediction = output.logits[0].item()
    elif model_name == "LXMERT":
      prediction = output[0][0].item()
    elif model_name == "UNITER":
      prediction = output[0][0].item()
    # Append the prediction to the list
    predictions.append(prediction)
  # Compare the predictions with the ground truth labels and compute accuracy
  accuracy = compute_accuracy(predictions, labels)
  # Return the accuracy score
  return accuracy

# Define a function to evaluate a model on a ranking task
def evaluate_ranking(model, pairs, model_name, tokenizer):
  # Initialize a list to store the scores and the ranks
  scores = []
  ranks = []
  # For each pair, feed the sentence and all four images to the model and get its scores
  for images, sentence, label in pairs:
    # Load and preprocess all four images
    image_tensors = [load_image(image, model_name) for image in images]
    image_tensor = torch.cat(image_tensors, dim=0)
    # Tokenize and preprocess the sentence
    token_tensor = tokenize_sentence(sentence, model_name, tokenizer)
    token_tensor = token_tensor.repeat(4,1)
    # Feed the images and the sentence to the model and get its output
    output = model(image_tensor, token_tensor)
    # Get the scores from the output according to the model's head
    if model_name == "CLIP":
      score = output.logits_per_image.squeeze().tolist()
    elif model_name == "ViLBERT":
      score = output.logits.squeeze().tolist()
    elif model_name == "LXMERT":
      score = output[0].squeeze().tolist()
    elif model_name == "UNITER":
      score = output[0].squeeze().tolist()
    # Append the scores to the list
    scores.append(score)
  # Sort the images by their scores and check if the positive image is ranked first
  for i in range(len(pairs)):
    rank = sorted(zip(images,scores[i]), key=lambda x: x[1], reverse=True)
    hit = rank[0][0] == pairs[i][0]
    ranks.append(rank)
    hits.append(hit)
  # Compute the percentage of hits and return it
  hit_rate = compute_percentage(hits)
  return hit_rate

# Define a list of image-language transformers to evaluate
models = ["CLIP", "ViLBERT", "LXMERT", "UNITER"]

# For each model, load its pretrained weights and fine-tune it on SVO-Probes dataset
for model_name in models:
  # Load the pretrained model and tokenizer from HuggingFace or OpenAI
  if model_name == "CLIP":
    model = clip.load("ViT-B/32", jit=False)[0].eval()
    tokenizer = clip.tokenize
  else:
    model_class = transformers.AutoModelForImageTextRetrieval if model_name == "UNITER" else transformers.AutoModelFor