---
title: 2206.10886v1 Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation
date: 2022-06-11
---

# [Optical Flow Regularization of Implicit Neural Representations for Video Frame Interpolation](http://arxiv.org/abs/2206.10886v1)

authors: Weihao Zhuang, Tristan Hascoet, Ryoichi Takashima, Tetsuya Takiguchi


## What, Why and How

[1]: https://arxiv.org/abs/2206.10886v1 "[2206.10886v1] Optical Flow Regularization of Implicit Neural ..."
[2]: https://arxiv.org/pdf/2206.10886 "arXiv:2206.10886v1 [cs.CV] 22 Jun 2022"
[3]: http://export.arxiv.org/abs/2301.10886v1 "[2301.10886v1] Automatic Intrinsic Reward Shaping for Exploration in ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method for video frame interpolation using implicit neural representations (INR) and optical flow regularization.
- **Why**: The paper aims to overcome the limitations of classical methods that rely on hand-crafted heuristics or large training data to interpolate pixel intensities between consecutive frames of a video.
- **How**: The paper leverages the property of INR to carry meaningful representations of signal derivatives and explicitly constrains the derivatives of the INR to satisfy the optical flow constraint equation. The paper shows that this approach can achieve state-of-the-art results on limited motion ranges using only a target video and its optical flow, without learning the interpolation operator from additional training data. The paper also demonstrates that this approach can improve the ability of narrow networks to fit the observed frames, which suggests potential applications to video compression and INR optimization.

## Main Contributions

[1]: https://arxiv.org/abs/2206.10886v1 "[2206.10886v1] Optical Flow Regularization of Implicit Neural ..."
[2]: https://arxiv.org/pdf/2206.10886 "arXiv:2206.10886v1 [cs.CV] 22 Jun 2022"
[3]: http://export.arxiv.org/abs/2301.10886v1 "[2301.10886v1] Automatic Intrinsic Reward Shaping for Exploration in ..."

The paper[^1^][2] lists the following contributions:

- **A novel method for VFI using INR and optical flow regularization**: The paper proposes to use INR to model the video frames as continuous functions of time and space and to constrain the derivatives of the INR to satisfy the optical flow constraint equation. This allows for a more accurate and robust interpolation of intermediate frames than classical methods.
- **A state-of-the-art performance on limited motion ranges**: The paper shows that the proposed method can achieve better results than existing methods on videos with limited motion ranges, such as Middlebury dataset and UCF101 dataset, using only a target video and its optical flow as input, without requiring any additional training data or interpolation operator.
- **An improved ability of narrow networks to fit the observed frames**: The paper demonstrates that the proposed method can enhance the fitting ability of narrow networks by constraining the INR derivatives, which reduces the number of parameters and improves the generalization ability of the INR. This suggests potential applications to video compression and INR optimization.

## Method Summary

[1]: https://arxiv.org/abs/2206.10886v1 "[2206.10886v1] Optical Flow Regularization of Implicit Neural ..."
[2]: https://arxiv.org/pdf/2206.10886 "arXiv:2206.10886v1 [cs.CV] 22 Jun 2022"
[3]: http://export.arxiv.org/abs/2301.10886v1 "[2301.10886v1] Automatic Intrinsic Reward Shaping for Exploration in ..."

Here is a summary of the method section of the paper[^1^][2]:

- The paper introduces the concept of INR and its derivatives, and explains how they can be used to model video frames as continuous functions of time and space.
- The paper formulates the optical flow constraint equation and shows how it can be used to regularize the derivatives of the INR to ensure temporal consistency between consecutive frames.
- The paper describes the optimization process of the INR parameters using gradient descent and the loss function that combines the optical flow regularization term and the reconstruction term.
- The paper presents the details of the network architecture, the input and output formats, and the hyperparameters used for the experiments.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the INR network as a fully connected network with ReLU activations
INR = FCN()

# Define the input and output tensors for the INR network
input = [x, y, t] # pixel coordinates and time
output = [r, g, b] # pixel intensities

# Define the optical flow constraint equation as a function of INR derivatives
def OFC(INR):
  # Compute the partial derivatives of INR with respect to x, y, and t
  dINR_dx = grad(INR, x)
  dINR_dy = grad(INR, y)
  dINR_dt = grad(INR, t)

  # Compute the optical flow vectors from the observed frames
  u, v = optical_flow(frame_0, frame_1)

  # Return the optical flow constraint equation
  return dINR_dt + u * dINR_dx + v * dINR_dy

# Define the loss function as a combination of optical flow regularization and reconstruction terms
def loss(INR):
  # Compute the optical flow regularization term as the L2 norm of OFC
  OF_reg = norm(OFC(INR))

  # Compute the reconstruction term as the L2 distance between INR output and observed frames
  rec = distance(INR(input), frame_0) + distance(INR(input), frame_1)

  # Return the weighted sum of the two terms
  return lambda * OF_reg + rec

# Initialize the INR parameters randomly
params = random()

# Optimize the INR parameters using gradient descent
for epoch in epochs:
  # Compute the gradient of the loss with respect to the parameters
  grad_loss = grad(loss, params)

  # Update the parameters using a learning rate
  params = params - lr * grad_loss

# Interpolate intermediate frames using the optimized INR network
for t in [0.25, 0.5, 0.75]:
  # Generate input tensor with fixed t and varying x and y
  input = [x, y, t]

  # Generate output tensor using INR network
  output = INR(input)

  # Save output tensor as an image file
  save(output, "frame_" + str(t) + ".png")
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import cv2

# Define the INR network as a fully connected network with ReLU activations
class INR(nn.Module):
  def __init__(self, input_dim, output_dim, hidden_dim, num_layers):
    super(INR, self).__init__()
    # Define the input layer
    self.input_layer = nn.Linear(input_dim, hidden_dim)
    # Define the hidden layers
    self.hidden_layers = nn.ModuleList()
    for i in range(num_layers - 1):
      self.hidden_layers.append(nn.Linear(hidden_dim, hidden_dim))
    # Define the output layer
    self.output_layer = nn.Linear(hidden_dim, output_dim)
    # Define the activation function
    self.relu = nn.ReLU()

  def forward(self, x):
    # Pass the input through the input layer and apply ReLU
    x = self.relu(self.input_layer(x))
    # Pass the output through the hidden layers and apply ReLU
    for layer in self.hidden_layers:
      x = self.relu(layer(x))
    # Pass the output through the output layer and return it
    x = self.output_layer(x)
    return x

# Define the input and output dimensions for the INR network
input_dim = 3 # pixel coordinates and time
output_dim = 3 # pixel intensities

# Define the hidden dimension and number of layers for the INR network
hidden_dim = 256
num_layers = 4

# Create an instance of the INR network
INR = INR(input_dim, output_dim, hidden_dim, num_layers)

# Define the optical flow constraint equation as a function of INR derivatives
def OFC(INR):
  # Compute the partial derivatives of INR with respect to x, y, and t using torch.autograd.grad
  dINR_dx = torch.autograd.grad(INR(output), input[:, 0], grad_outputs=torch.ones_like(output), create_graph=True)[0]
  dINR_dy = torch.autograd.grad(INR(output), input[:, 1], grad_outputs=torch.ones_like(output), create_graph=True)[0]
  dINR_dt = torch.autograd.grad(INR(output), input[:, 2], grad_outputs=torch.ones_like(output), create_graph=True)[0]

  # Compute the optical flow vectors from the observed frames using cv2.calcOpticalFlowFarneback
  u, v = cv2.calcOpticalFlowFarneback(frame_0, frame_1, None, pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0)

  # Convert the optical flow vectors to torch tensors and reshape them to match the input shape
  u = torch.from_numpy(u).float().view(-1, 1)
  v = torch.from_numpy(v).float().view(-1, 1)

  # Return the optical flow constraint equation as a torch tensor
  return dINR_dt + u * dINR_dx + v * dINR_dy

# Define the loss function as a combination of optical flow regularization and reconstruction terms
def loss(INR):
  # Compute the optical flow regularization term as the L2 norm of OFC using torch.norm
  OF_reg = torch.norm(OFC(INR))

  # Compute the reconstruction term as the L2 distance between INR output and observed frames using torch.nn.MSELoss
  rec_loss = nn.MSELoss()
  rec_0 = rec_loss(INR(input), frame_0)
  rec_1 = rec_loss(INR(input), frame_1)
  rec = rec_0 + rec_1

  # Return the weighted sum of the two terms using a lambda parameter
  lambda = 0.01
  return lambda * OF_reg + rec

# Initialize the INR parameters randomly using torch.nn.init.normal_
torch.nn.init.normal_(INR.parameters())

# Create an optimizer using torch.optim.Adam with a learning rate parameter
lr = 0.001
optimizer = optim.Adam(INR.parameters(), lr=lr)

# Load the target video and its optical flow as numpy arrays using cv2.VideoCapture and cv2.calcOpticalFlowFarneback
video = cv2.VideoCapture("target_video.mp4")
frame_0 = video.read()[1]
frame_1 = video.read()[1]
u, v = cv2.calcOpticalFlowFarneback(frame_0, frame_1, None, pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.2, flags=0)

# Convert the frames and the optical flow to torch tensors and normalize them to [0, 1] range
frame_0 = torch.from_numpy(frame_0).float() / 255.0
frame_1 = torch.from_numpy(frame_1).float() / 255.0
u = torch.from_numpy(u).float() / np.max(np.abs(u))
v = torch.from_numpy(v).float() / np.max(np.abs(v))

# Reshape the frames and the optical flow to match the input and output shapes of the INR network
frame_0 = frame_0.view(-1, 3)
frame_1 = frame_1.view(-1, 3)
u = u.view(-1, 1)
v = v.view(-1, 1)

# Generate the input tensor with fixed x and y coordinates and varying t values using torch.meshgrid and torch.cat
x = torch.linspace(0, 1, frame_0.shape[0])
y = torch.linspace(0, 1, frame_0.shape[1])
t = torch.linspace(0, 1, 2)
x_grid, y_grid, t_grid = torch.meshgrid(x, y, t)
input = torch.cat([x_grid.reshape(-1, 1), y_grid.reshape(-1, 1), t_grid.reshape(-1, 1)], dim=1)

# Optimize the INR parameters using gradient descent for a number of epochs
epochs = 100
for epoch in range(epochs):
  # Zero the gradients of the optimizer
  optimizer.zero_grad()

  # Compute the loss using the loss function
  loss_value = loss(INR)

  # Compute the gradient of the loss using torch.autograd.backward
  loss_value.backward()

  # Update the parameters using the optimizer
  optimizer.step()

  # Print the loss value every 10 epochs
  if epoch % 10 == 0:
    print("Epoch:", epoch, "Loss:", loss_value.item())

# Interpolate intermediate frames using the optimized INR network for t values in [0.25, 0.5, 0.75]
for t in [0.25, 0.5, 0.75]:
  # Generate input tensor with fixed t and varying x and y using torch.meshgrid and torch.cat
  x = torch.linspace(0, 1, frame_0.shape[0])
  y = torch.linspace(0, 1, frame_0.shape[1])
  x_grid, y_grid = torch.meshgrid(x, y)
  input = torch.cat([x_grid.reshape(-1, 1), y_grid.reshape(-1, 1), torch.full_like(x_grid.reshape(-1, 1), t)], dim=1)

  # Generate output tensor using INR network
  output = INR(input)

  # Reshape output tensor to match the image shape and convert it to numpy array
  output = output.view(frame_0.shape[0], frame_0.shape[1], frame_0.shape[2]).numpy()

  # Save output tensor as an image file using cv2.imwrite
  cv2.imwrite("frame_" + str(t) + ".png", output * 255)
```