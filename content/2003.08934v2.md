---
title: 2003.08934v2 NeRF  Representing Scenes as Neural Radiance Fields for View Synthesis
date: 2020-03-09
---

# [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](http://arxiv.org/abs/2003.08934v2)

authors: Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng


## What, Why and How

[1]: https://arxiv.org/pdf/2003.08934v2.pdf "NeRF: Representing Scenes as Neural Radiance Fields for ... - arXiv.org"
[2]: https://arxiv.org/abs/2003.08934 "[2003.08934] NeRF: Representing Scenes as Neural Radiance Fields for ..."
[3]: http://export.arxiv.org/abs/2001.08934v2 "[2001.08934v2] Study of space charge in the ICARUS T600 detector"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method for synthesizing novel views of complex scenes by optimizing a continuous 5D scene function using a sparse set of input views. The scene function is represented by a fully-connected deep network that outputs the volume density and view-dependent emitted radiance at any spatial location and viewing direction. The method is called **NeRF** (Neural Radiance Fields).
- **Why**: The paper aims to address the long-standing problem of view synthesis in a new way by directly optimizing a continuous scene representation to minimize the error of rendering a set of captured images. The paper claims that NeRF achieves state-of-the-art results for neural rendering and view synthesis, and can render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings.
- **How**: The paper describes how to effectively optimize neural radiance fields to render novel views of scenes with complicated geometry and appearance. The paper uses techniques from volume rendering to accumulate samples of the scene function along rays to render the scene from any viewpoint. The paper also introduces several improvements to speed up the optimization and rendering process, such as hierarchical sampling, positional encoding, and fine-tuning. The paper evaluates NeRF on synthetic and real datasets, and compares it with prior work on image-based rendering and neural scene representation.

## Main Contributions

According to the paper, the main contributions are:

- Introducing a novel representation for scenes as continuous 5D neural radiance fields that can be optimized to render photorealistic novel views of complex scenes from a sparse set of input images.
- Demonstrating that NeRF can synthesize high-resolution novel views of real objects and scenes captured in natural settings, outperforming prior work on neural rendering and view synthesis.
- Proposing several techniques to improve the efficiency and quality of NeRF, such as hierarchical sampling, positional encoding, and fine-tuning.

## Method Summary

The method section of the paper can be summarized as follows:

- The paper defines a scene as a continuous 5D function that maps a spatial location and a viewing direction to a volume density and a view-dependent color. The paper uses a fully-connected deep network to represent this function, and optimizes its parameters to minimize the error of rendering a set of input images with known camera poses.
- The paper uses classic volume rendering techniques to render an image from the scene function by sampling 5D coordinates along camera rays and accumulating their colors and densities. The paper uses Monte Carlo integration to approximate the rendering integral, and uses automatic differentiation to compute the gradients for optimization.
- The paper introduces several improvements to speed up the optimization and rendering process, such as hierarchical sampling, positional encoding, and fine-tuning. Hierarchical sampling reduces the number of samples required to render an image by using a coarse network to guide the sampling of a fine network. Positional encoding enhances the representational capacity of the network by applying a sinusoidal transformation to the input coordinates. Fine-tuning adapts the network to a specific viewpoint by optimizing it on a single image.


## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define a scene function as a fully-connected network
def scene_function(x, y, z, theta, phi):
  # Apply positional encoding to the input coordinates
  x, y, z, theta, phi = positional_encoding(x, y, z, theta, phi)
  # Concatenate the spatial and directional coordinates
  input = concatenate(x, y, z, theta, phi)
  # Feed the input to the network and output density and color
  density, color = network(input)
  return density, color

# Define a volume rendering function that renders an image from the scene function
def volume_rendering(scene_function, camera_pose):
  # Initialize an empty image
  image = zeros(width, height)
  # Loop over the pixels of the image
  for i in range(width):
    for j in range(height):
      # Compute the ray origin and direction for the pixel
      ray_origin, ray_direction = get_ray(camera_pose, i, j)
      # Sample points along the ray using hierarchical sampling
      points = hierarchical_sampling(scene_function, ray_origin, ray_direction)
      # Loop over the sampled points
      for point in points:
        # Evaluate the scene function at the point
        density, color = scene_function(point.x, point.y, point.z, ray_direction.theta, ray_direction.phi)
        # Compute the transmittance along the ray
        transmittance = exp(-sum(density * point.distance))
        # Accumulate the color and density to the image
        image[i][j] += color * density * transmittance
  return image

# Define an optimization function that optimizes the scene function to minimize the rendering error
def optimization(scene_function, input_images):
  # Loop over the input images
  for input_image in input_images:
    # Get the camera pose of the input image
    camera_pose = input_image.camera_pose
    # Render an image from the scene function using volume rendering
    rendered_image = volume_rendering(scene_function, camera_pose)
    # Compute the loss as the mean squared error between the input and rendered images
    loss = mean_squared_error(input_image, rendered_image)
    # Compute the gradients of the loss with respect to the scene function parameters
    gradients = backward(loss)
    # Update the scene function parameters using gradient descent
    scene_function.update(gradients)

# Define a fine-tuning function that adapts the scene function to a specific viewpoint
def fine_tuning(scene_function, target_image):
  # Get the camera pose of the target image
  camera_pose = target_image.camera_pose
  # Render an image from the scene function using volume rendering
  rendered_image = volume_rendering(scene_function, camera_pose)
  # Compute the loss as the mean squared error between the target and rendered images
  loss = mean_squared_error(target_image, rendered_image)
  # Compute the gradients of the loss with respect to the scene function parameters
  gradients = backward(loss)
  # Update the scene function parameters using gradient descent with a small learning rate
  scene_function.update(gradients, learning_rate=0.01)

# Main procedure

# Initialize a scene function as a fully-connected network with random weights
scene_function = initialize_network()

# Load a set of input images with known camera poses
input_images = load_images()

# Optimize the scene function to minimize the rendering error on the input images
optimization(scene_function, input_images)

# Load a target image with a novel viewpoint
target_image = load_image()

# Fine-tune the scene function to adapt to the target viewpoint
fine_tuning(scene_function, target_image)

# Render a novel view from the scene function using volume rendering
novel_view = volume_rendering(scene_function, target_image.camera_pose)

# Display or save the novel view
show_or_save(novel_view)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# Define some constants
WIDTH = 800 # Image width in pixels
HEIGHT = 600 # Image height in pixels
N_SAMPLES = 64 # Number of samples per ray
N_FREQS = 10 # Number of frequencies for positional encoding
L_RATE = 0.01 # Learning rate for optimization
EPSILON = 1e-3 # Small value to avoid division by zero

# Define a scene function as a fully-connected network
def scene_function(x, y, z, theta, phi):
  # Apply positional encoding to the input coordinates
  x, y, z, theta, phi = positional_encoding(x, y, z, theta, phi)
  # Concatenate the spatial and directional coordinates
  input = tf.concat([x, y, z, theta, phi], axis=-1)
  # Feed the input to the network and output density and color
  density = tf.keras.layers.Dense(1)(input)
  color = tf.keras.layers.Dense(3)(input)
  return density, color

# Define a positional encoding function that applies a sinusoidal transformation to the input coordinates
def positional_encoding(x, y, z, theta, phi):
  # Create a list of frequencies for each coordinate
  freqs = [2**i for i in range(N_FREQS)]
  # Create empty tensors to store the encoded coordinates
  x_encoded = tf.zeros_like(x)
  y_encoded = tf.zeros_like(y)
  z_encoded = tf.zeros_like(z)
  theta_encoded = tf.zeros_like(theta)
  phi_encoded = tf.zeros_like(phi)
  # Loop over the frequencies
  for freq in freqs:
    # Compute the sine and cosine of the coordinate times the frequency
    x_sin = tf.sin(x * freq)
    x_cos = tf.cos(x * freq)
    y_sin = tf.sin(y * freq)
    y_cos = tf.cos(y * freq)
    z_sin = tf.sin(z * freq)
    z_cos = tf.cos(z * freq)
    theta_sin = tf.sin(theta * freq)
    theta_cos = tf.cos(theta * freq)
    phi_sin = tf.sin(phi * freq)
    phi_cos = tf.cos(phi * freq)
    # Concatenate the sine and cosine along the last dimension
    x_encoded = tf.concat([x_encoded, x_sin, x_cos], axis=-1)
    y_encoded = tf.concat([y_encoded, y_sin, y_cos], axis=-1)
    z_encoded = tf.concat([z_encoded, z_sin, z_cos], axis=-1)
    theta_encoded = tf.concat([theta_encoded, theta_sin, theta_cos], axis=-1)
    phi_encoded = tf.concat([phi_encoded, phi_sin, phi_cos], axis=-1)
  return x_encoded, y_encoded, z_encoded, theta_encoded, phi_encoded

# Define a volume rendering function that renders an image from the scene function
def volume_rendering(scene_function, camera_pose):
  # Initialize an empty image
  image = tf.zeros((HEIGHT, WIDTH, 3))
  # Loop over the pixels of the image
  for i in range(HEIGHT):
    for j in range(WIDTH):
      # Compute the ray origin and direction for the pixel
      ray_origin, ray_direction = get_ray(camera_pose, i, j)
      # Sample points along the ray using hierarchical sampling
      points = hierarchical_sampling(scene_function, ray_origin, ray_direction)
      # Loop over the sampled points in reverse order (from far to near)
      for point in reversed(points):
        # Evaluate the scene function at the point
        density, color = scene_function(point.x, point.y, point.z,
                                        ray_direction.theta,
                                        ray_direction.phi)
        # Compute the transmittance along the ray using the previous point's distance and density
        transmittance = tf.exp(-density * point.distance) if point.previous else 1.0
        # Accumulate the color and density to the image using alpha compositing
        image[i][j] += color * density * transmittance * (1 - image[i][j])
  return image

# Define a get_ray function that computes the ray origin and direction for a pixel given a camera pose
def get_ray(camera_pose, i , j):
  # Get the camera position and orientation from the camera pose matrix
  camera_position = camera_pose[:3,-1]
  camera_orientation = camera_pose[:3,:3]
  # Compute the pixel position in normalized device coordinates ([-1,+1] range)
  pixel_x = (j + 0.5) / WIDTH * 2 - 1
  pixel_y = (i + 0.5) / HEIGHT * 2 - 1
  # Compute the pixel position in camera coordinates
  pixel_camera_x = pixel_x * camera_focal_length
  pixel_camera_y = pixel_y * camera_focal_length
  pixel_camera_z = -1
  # Compute the pixel position in world coordinates by applying the camera orientation
  pixel_world_x = camera_orientation[0,0] * pixel_camera_x + camera_orientation[0,1] * pixel_camera_y + camera_orientation[0,2] * pixel_camera_z
  pixel_world_y = camera_orientation[1,0] * pixel_camera_x + camera_orientation[1,1] * pixel_camera_y + camera_orientation[1,2] * pixel_camera_z
  pixel_world_z = camera_orientation[2,0] * pixel_camera_x + camera_orientation[2,1] * pixel_camera_y + camera_orientation[2,2] * pixel_camera_z
  # Compute the ray origin as the camera position
  ray_origin = camera_position
  # Compute the ray direction as the normalized vector from the camera position to the pixel position
  ray_direction = tf.math.l2_normalize([pixel_world_x - camera_position[0],
                                        pixel_world_y - camera_position[1],
                                        pixel_world_z - camera_position[2]])
  # Convert the ray direction from Cartesian to spherical coordinates
  ray_direction_theta = tf.math.acos(ray_direction[2])
  ray_direction_phi = tf.math.atan2(ray_direction[1], ray_direction[0])
  return ray_origin, RayDirection(ray_direction, ray_direction_theta, ray_direction_phi)

# Define a RayDirection class that stores the ray direction in Cartesian and spherical coordinates
class RayDirection:
  def __init__(self, vector, theta, phi):
    self.vector = vector # A 3D vector in Cartesian coordinates
    self.theta = theta # The polar angle in radians
    self.phi = phi # The azimuthal angle in radians

# Define a hierarchical sampling function that samples points along a ray using a coarse and a fine network
def hierarchical_sampling(scene_function, ray_origin, ray_direction):
  # Define the near and far bounds of the ray
  near = EPSILON
  far = max_distance
  # Define the number of samples for the coarse and fine networks
  n_coarse_samples = N_SAMPLES // 2
  n_fine_samples = N_SAMPLES // 2
  # Define the coarse and fine networks as copies of the scene function with different resolutions
  coarse_network = tf.keras.models.clone_model(scene_function)
  coarse_network.set_weights(scene_function.get_weights())
  fine_network = tf.keras.models.clone_model(scene_function)
  fine_network.set_weights(scene_function.get_weights())
  # Sample points uniformly along the ray for the coarse network
  coarse_t = tf.linspace(near, far, n_coarse_samples)
  coarse_points = [ray_origin + t * ray_direction.vector for t in coarse_t]
  # Evaluate the coarse network at the sampled points
  coarse_densities, coarse_colors = zip(*[coarse_network(point.x, point.y, point.z,
                                                         ray_direction.theta,
                                                         ray_direction.phi)
                                          for point in coarse_points])
  # Compute the cumulative distribution function (CDF) of the coarse densities
  coarse_cdf = tf.math.cumsum(coarse_densities)
  # Sample points from the inverse CDF for the fine network
  fine_u = tf.random.uniform((n_fine_samples,))
  fine_t = inverse_cdf(coarse_cdf, coarse_t, fine_u)
  fine_points = [ray_origin + t * ray_direction.vector for t in fine_t]
  # Evaluate the fine network at the sampled points
  fine_densities, fine_colors = zip(*[fine_network(point.x, point.y, point.z,
                                                    ray_direction.theta,
                                                    ray_direction.phi)
                                      for point in fine_points])
  
```