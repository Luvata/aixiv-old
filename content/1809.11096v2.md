---
title: 1809.11096v2 Large Scale GAN Training for High Fidelity Natural Image Synthesis
date: 2018-09-12
---

# [Large Scale GAN Training for High Fidelity Natural Image Synthesis](http://arxiv.org/abs/1809.11096v2)

authors: Andrew Brock, Jeff Donahue, Karen Simonyan


## What, Why and How

[1]: https://arxiv.org/pdf/1809.11096v2.pdf "arXiv.org e-Print archive"
[2]: https://www.researchgate.net/publication/341039960_Editing_in_Style_Uncovering_the_Local_Semantics_of_GANs "Editing in Style: Uncovering the Local Semantics of GANs"
[3]: https://arxiv.org/abs/1809.11096v2 "Large Scale GAN Training for High Fidelity Natural Image Synthesis"

Here is a summary of the paper in terms of What, Why, and How:

```
What: The paper presents a method for training large-scale Generative Adversarial Networks (GANs) to synthesize high-fidelity natural images.

Why: The paper aims to close the gap between the quality of images generated by GANs and real-world images from the ImageNet dataset.

How: The paper introduces two architectural changes (self-attention and hierarchical latent spaces) and a regularization scheme (orthogonal regularization) that improve the scalability and stability of GAN training. The paper also applies a simple sampling technique (truncation trick) that allows fine control over the trade-off between sample fidelity and variety.
```

## Main Contributions

The contributions of this paper are:

- It demonstrates that GANs benefit dramatically from scaling, and trains models with two to four times as many parameters and eight times the batch size compared to prior art.
- It introduces two simple, general architectural changes (self-attention and hierarchical latent spaces) that improve scalability, and modifies a regularization scheme (orthogonal regularization) to improve conditioning, demonstrably boosting performance.
- It discovers instabilities specific to large scale GANs, and characterizes them empirically. It demonstrates that a combination of novel and existing techniques can reduce these instabilities, but complete training stability can only be achieved at a dramatic cost to performance.
- It applies a simple sampling technique (truncation trick) that allows explicit, fine-grained control of the trade-off between sample variety and fidelity.

## Method Summary

[1]: https://arxiv.org/abs/1809.11096 "[1809.11096] Large Scale GAN Training for High Fidelity ... - arXiv.org"
[2]: https://arxiv.org/pdf/1809.11096v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2211.11096v2 "[2211.11096v2] Let Offline RL Flow: Training Conservative Agents in the ..."

Here is a summary of the method section of the paper:

```
The method section of the paper describes the following aspects of the proposed approach:

- The model architecture, which consists of a conditional generator and a discriminator, both using self-attention and hierarchical latent spaces to capture long-range dependencies and multi-scale features.
- The training procedure, which uses orthogonal regularization to improve the conditioning of the generator and spectral normalization to stabilize the discriminator. The paper also uses a two time-scale update rule to balance the learning rates of the generator and the discriminator.
- The sampling technique, which uses a truncation trick to reduce the variance of the generator's input and control the trade-off between sample fidelity and variety. The paper also introduces a truncation threshold to avoid sampling from low-quality regions of the latent space.
- The evaluation metrics, which include Inception Score (IS), Fr√©chet Inception Distance (FID), Precision and Recall, and Kernel Inception Distance (KID).
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the generator G and the discriminator D
G = ConditionalGenerator()
D = ConditionalDiscriminator()

# Apply orthogonal regularization to G and spectral normalization to D
G = OrthogonalRegularizer(G)
D = SpectralNormalizer(D)

# Define the loss functions for G and D
G_loss = hinge_loss(D(G(z, y), y), 1) # z is the latent vector, y is the class label
D_loss = hinge_loss(D(x, y), 1) + hinge_loss(D(G(z, y), y), -1) # x is the real image

# Define the optimizers for G and D with different learning rates
G_optimizer = Adam(lr=0.00005)
D_optimizer = Adam(lr=0.0002)

# Define the truncation threshold T
T = 0.4

# Train G and D alternately for N iterations
for i in range(N):
  # Sample a batch of latent vectors z and class labels y
  z = sample_normal(0, 1)
  y = sample_uniform(0, C) # C is the number of classes
  
  # Apply truncation trick to z by clipping its values to [-T, T]
  z = clip(z, -T, T)
  
  # Update D by maximizing D_loss
  D_optimizer.zero_grad()
  D_loss.backward()
  D_optimizer.step()
  
  # Update G by minimizing G_loss
  G_optimizer.zero_grad()
  G_loss.backward()
  G_optimizer.step()
  
# Generate samples from G using z and y
samples = G(z, y)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import numpy as np

# Define the self-attention module
class SelfAttention(nn.Module):
  def __init__(self, in_channels):
    super(SelfAttention, self).__init__()
    # Define the projection matrices for query, key, and value
    self.query = nn.Conv2d(in_channels, in_channels // 8, 1)
    self.key = nn.Conv2d(in_channels, in_channels // 8, 1)
    self.value = nn.Conv2d(in_channels, in_channels, 1)
    # Define the scaling factor
    self.scale = np.sqrt(in_channels // 8)
    # Define the output projection matrix
    self.gamma = nn.Parameter(torch.zeros(1))
  
  def forward(self, x):
    # Get the batch size and the feature map size
    batch_size, channels, height, width = x.size()
    # Project the input to get the query, key, and value tensors
    query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1) # (B, N, C)
    key = self.key(x).view(batch_size, -1, height * width) # (B, C, N)
    value = self.value(x).view(batch_size, -1, height * width) # (B, C, N)
    # Compute the attention map by matrix multiplication and scaling
    attention_map = F.softmax(query @ key / self.scale, dim=-1) # (B, N, N)
    # Compute the output by matrix multiplication and reshaping
    out = value @ attention_map.permute(0, 2, 1) # (B, C, N)
    out = out.view(batch_size, channels, height, width) # (B, C, H ,W)
    # Add the output to the input with a learnable weight
    out = self.gamma * out + x
    return out

# Define the conditional batch normalization module
class ConditionalBatchNorm2d(nn.Module):
  def __init__(self, in_channels, num_classes):
    super(ConditionalBatchNorm2d,self).__init__()
    # Define the batch normalization layer
    self.bn = nn.BatchNorm2d(in_channels)
    # Define the embedding layer for class labels
    self.embed = nn.Embedding(num_classes,in_channels * 2)
  
  def forward(self,x,y):
    # Get the batch normalization parameters from the embedding layer
    bn_params = self.embed(y) # (B,C*2)
    gamma,beta = bn_params.chunk(2,dim=1) # (B,C),(B,C)
    gamma = gamma.unsqueeze(2).unsqueeze(3) # (B,C,H,W)
    beta = beta.unsqueeze(2).unsqueeze(3) # (B,C,H,W)
    
    # Apply batch normalization to x and scale and shift with gamma and beta
    out = self.bn(x) 
    out = gamma * out + beta 
    return out

# Define the residual block for the generator
class GBlock(nn.Module):
  def __init__(self,in_channels,out_channels,num_classes):
    super(GBlock,self).__init__()
    
    # Define the conditional batch normalization layers for both input and output channels
    self.cbn1 = ConditionalBatchNorm2d(in_channels,num_classes)
    self.cbn2 = ConditionalBatchNorm2d(out_channels,num_classes)

    # Define the convolutional layers for up-sampling and down-sampling
    self.conv_upsample = nn.ConvTranspose2d(in_channels,out_channels,kernel_size=4,stride=2,padding=1,bias=False)