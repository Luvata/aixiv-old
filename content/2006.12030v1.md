---
title: 2006.12030v1 DO-Conv  Depthwise Over-parameterized Convolutional Layer
date: 2020-06-13
---

# [DO-Conv: Depthwise Over-parameterized Convolutional Layer](http://arxiv.org/abs/2006.12030v1)

authors: Jinming Cao, Yangyan Li, Mingchao Sun, Ying Chen, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen, Changhe Tu


## What, Why and How

[1]: https://arxiv.org/pdf/2006.12030v1 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2006.12030 "DO-Conv: Depthwise Over-parameterized Convolutional Layer"
[3]: https://www.researchgate.net/publication/342377355_DO-Conv_Depthwise_Over-parameterized_Convolutional_Layer "(PDF) DO-Conv: Depthwise Over-parameterized Convolutional Layer"

Here is a summary of the paper:

- **What**: The paper proposes a new convolutional layer called DO-Conv, which adds an extra depthwise convolution to a conventional convolutional layer. This results in an over-parameterization of the layer, meaning that it has more learnable parameters than necessary for the same linear operation.
- **Why**: The paper claims that over-parameterization can improve the performance of CNNs on various vision tasks, such as image classification, detection, and segmentation. The paper also shows that DO-Conv can be folded into a single convolutional layer during inference, without increasing the computational complexity or memory footprint.
- **How**: The paper introduces the formulation of DO-Conv and analyzes its properties. The paper also conducts extensive experiments on several benchmark datasets and architectures, demonstrating that replacing conventional convolutional layers with DO-Conv layers can boost the accuracy and robustness of CNNs. The paper provides a reference implementation of DO-Conv in Tensorflow, PyTorch and GluonCV[^1^][1].

## Main Contributions

The paper makes the following contributions:

- It proposes a novel convolutional layer called DO-Conv, which augments a conventional convolutional layer with an additional depthwise convolution, resulting in an over-parameterization of the layer.
- It shows that over-parameterization can enhance the performance of CNNs on various vision tasks, such as image classification, detection, and segmentation, by providing more flexibility and diversity for the network to learn.
- It proves that DO-Conv can be folded into a single convolutional layer during inference, without increasing the computational complexity or memory footprint, making it a practical and efficient alternative to the conventional convolutional layer.
- It conducts extensive experiments on several benchmark datasets and architectures, demonstrating that replacing conventional convolutional layers with DO-Conv layers can boost the accuracy and robustness of CNNs, especially when the network depth or width is limited.
- It provides a reference implementation of DO-Conv in Tensorflow, PyTorch and GluonCV, making it easy for researchers and practitioners to adopt and apply DO-Conv in their own projects.

## Method Summary

[1]: https://arxiv.org/pdf/2006.12030v1 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2006.12030 "DO-Conv: Depthwise Over-parameterized Convolutional Layer"
[3]: https://www.researchgate.net/publication/342377355_DO-Conv_Depthwise_Over-parameterized_Convolutional_Layer "(PDF) DO-Conv: Depthwise Over-parameterized Convolutional Layer"

Here is a summary of the method section of the paper:

- The paper introduces the formulation of DO-Conv, which is composed of a conventional convolution followed by a depthwise convolution. The paper defines the input, output, and parameters of DO-Conv, and shows how it can be implemented efficiently using existing deep learning frameworks[^1^][1].
- The paper analyzes the properties of DO-Conv, such as its over-parameterization ratio, its rank, and its relation to other convolutional layers. The paper proves that DO-Conv can be folded into a single convolutional layer during inference, without changing the output or increasing the computation or memory cost[^1^][1].
- The paper discusses the benefits of over-parameterization for CNNs, such as providing more flexibility and diversity for the network to learn, improving the generalization ability and robustness of the network, and accelerating the training process of the network[^1^][1].
- The paper provides some insights and guidelines for applying DO-Conv to different network architectures and tasks. The paper suggests that DO-Conv can be more effective when replacing convolutional layers with small kernel sizes or low channel numbers, and when applied to tasks that require fine-grained feature extraction or semantic understanding[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define DO-Conv layer
class DOConv(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
    super(DOConv, self).__init__()
    # Conventional convolution parameters
    self.W = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
    self.b = nn.Parameter(torch.randn(out_channels))
    # Depthwise convolution parameters
    self.D = nn.Parameter(torch.randn(in_channels, kernel_size, kernel_size))
  
  def forward(self, x):
    # Apply conventional convolution
    y = F.conv2d(x, self.W, self.b, stride=stride, padding=padding)
    # Apply depthwise convolution
    z = F.conv2d(x * self.D, torch.eye(in_channels), stride=stride, padding=padding)
    # Add the outputs
    return y + z

# Define CNN model with DO-Conv layers
class DOConvNet(nn.Module):
  def __init__(self):
    super(DOConvNet, self).__init__()
    # Replace some convolutional layers with DO-Conv layers
    self.conv1 = DOConv(3, 64, 3, padding=1)
    self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
    self.conv3 = DOConv(128, 256, 3, padding=1)
    self.conv4 = nn.Conv2d(256, 512, 3, padding=1)
    # Other layers and operations
    self.pool = nn.MaxPool2d(2)
    self.fc1 = nn.Linear(512 * 4 * 4, 1024)
    self.fc2 = nn.Linear(1024, 10)
    self.relu = nn.ReLU()
    self.softmax = nn.Softmax(dim=1)

  def forward(self, x):
    # Apply DO-Conv and Conv layers alternately
    x = self.relu(self.pool(self.conv1(x)))
    x = self.relu(self.pool(self.conv2(x)))
    x = self.relu(self.pool(self.conv3(x)))
    x = self.relu(self.pool(self.conv4(x)))
    # Flatten the output
    x = x.view(-1, 512 * 4 * 4)
    # Apply fully connected layers
    x = self.relu(self.fc1(x))
    x = self.softmax(self.fc2(x))
    return x

# Train and test the model on a vision task
model = DOConvNet()
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()
for epoch in range(num_epochs):
  # Train the model on training data
  for batch_x, batch_y in train_loader:
    optimizer.zero_grad()
    output = model(batch_x)
    loss = criterion(output, batch_y)
    loss.backward()
    optimizer.step()
  # Test the model on validation data
  with torch.no_grad():
    accuracy = 0
    for batch_x, batch_y in val_loader:
      output = model(batch_x)
      predictions = torch.argmax(output, dim=1)
      accuracy += torch.sum(predictions == batch_y).item()
    accuracy /= len(val_loader.dataset)
    print(f"Epoch {epoch}, accuracy {accuracy}")
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

# Define hyperparameters
num_epochs = 10
batch_size = 128
learning_rate = 0.01

# Define DO-Conv layer
class DOConv(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
    super(DOConv, self).__init__()
    # Conventional convolution parameters
    self.W = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
    self.b = nn.Parameter(torch.randn(out_channels))
    # Depthwise convolution parameters
    self.D = nn.Parameter(torch.randn(in_channels, kernel_size, kernel_size))
  
  def forward(self, x):
    # Apply conventional convolution
    y = F.conv2d(x, self.W, self.b, stride=stride, padding=padding)
    # Apply depthwise convolution
    z = F.conv2d(x * self.D, torch.eye(in_channels), stride=stride, padding=padding)
    # Add the outputs
    return y + z

# Define CNN model with DO-Conv layers
class DOConvNet(nn.Module):
  def __init__(self):
    super(DOConvNet, self).__init__()
    # Replace some convolutional layers with DO-Conv layers
    self.conv1 = DOConv(3, 64, 3, padding=1)
    self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
    self.conv3 = DOConv(128, 256, 3, padding=1)
    self.conv4 = nn.Conv2d(256, 512, 3, padding=1)
    # Other layers and operations
    self.pool = nn.MaxPool2d(2)
    self.fc1 = nn.Linear(512 * 4 * 4, 1024)
    self.fc2 = nn.Linear(1024, 10)
    self.relu = nn.ReLU()
    self.softmax = nn.Softmax(dim=1)

  def forward(self, x):
    # Apply DO-Conv and Conv layers alternately
    x = self.relu(self.pool(self.conv1(x)))
    x = self.relu(self.pool(self.conv2(x)))
    x = self.relu(self.pool(self.conv3(x)))
    x = self.relu(self.pool(self.conv4(x)))
    # Flatten the output
    x = x.view(-1, 512 * 4 * 4)
    # Apply fully connected layers
    x = self.relu(self.fc1(x))
    x = self.softmax(self.fc2(x))
    return x

# Load and preprocess CIFAR-10 dataset
transform = transforms.Compose([
  transforms.ToTensor(),
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
train_set = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_set,
                                          batch_size=batch_size,
                                          shuffle=True)
val_set = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform