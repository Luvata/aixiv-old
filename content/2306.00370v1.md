---
title: 2306.00370v1 Graph Switching Dynamical Systems
date: 2023-06-01
---

# [Graph Switching Dynamical Systems](http://arxiv.org/abs/2306.00370v1)

authors: Yongtuo Liu, Sara Magliacane, Miltiadis Kofinas, Efstratios Gavves


## What, Why and How

[1]: https://arxiv.org/abs/2306.00370 "[2306.00370] Graph Switching Dynamical Systems - arXiv.org"
[2]: https://arxiv.org/pdf/2306.00370 "Graph Switching Dynamical Systems - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00370 "[2306.00370] Graph Switching Dynamical Systems"

Here is a summary of the paper based on the search results:

- **What**: The paper proposes a novel graph-based approach for switching dynamical systems, called GRAph Switching dynamical Systems (GRASS), which can model complex time series data with interacting objects and dynamic mode-switching behaviour[^1^][1].
- **Why**: The paper aims to address the limitations of existing methods for switching dynamical systems, which either focus on single objects or assume independent objects, and cannot capture the inter-object dependencies and interactions that affect the mode-switching behaviour[^1^][1].
- **How**: The paper uses a dynamic graph to characterize the interactions between objects and their modes, and learns both intra-object and inter-object mode-switching behaviour using a variational inference framework. The paper also introduces two new datasets for this setting, a synthesized ODE-driven particles dataset and a real-world Salsa Couple Dancing dataset, and shows that GRASS can consistently outperform previous state-of-the-art methods[^1^][1] [^2^][2].

## Main Contributions

[1]: https://arxiv.org/abs/2306.00370 "[2306.00370] Graph Switching Dynamical Systems - arXiv.org"
[2]: https://arxiv.org/pdf/2301.00370v1.pdf "arXiv:2301.00370v1 [quant-ph] 1 Jan 2023"
[3]: http://export.arxiv.org/abs/2206.00370v1 "[2206.00370v1] Simulation of dielectric axion haloscopes with deep ..."

According to the search results, the main contributions of the paper are:

- **Proposing a novel graph-based approach for switching dynamical systems**, called GRASS, which can model complex time series data with interacting objects and dynamic mode-switching behaviour[^1^][1].
- **Introducing a variational inference framework** for learning both intra-object and inter-object mode-switching behaviour using a dynamic graph[^1^][1].
- **Introducing two new datasets** for the interacting object setting for switching dynamical systems, a synthesized ODE-driven particles dataset and a real-world Salsa Couple Dancing dataset[^1^][1].
- **Showing that GRASS can consistently outperform previous state-of-the-art methods** on both synthetic and real-world datasets[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/abs/2306.00370 "[2306.00370] Graph Switching Dynamical Systems - arXiv.org"
[2]: https://arxiv.org/pdf/2306.00307.pdf "arXiv:2306.00307v1 [math.NA] 1 Jun 2023"
[3]: https://arxiv-export3.library.cornell.edu/abs/2012.00370v1 "[2012.00370v1] Double machine learning for (weighted) dynamic treatment ..."

Here is a summary of the method section based on the search results:

- The paper proposes a novel graph-based approach for switching dynamical systems, called GRASS, which uses a dynamic graph to characterize the interactions between objects and their modes[^1^][1].
- The paper models the per-object dynamics as a non-linear function of the object's state and mode, and the inter-object dynamics as a function of the object's state, mode, and graph structure[^1^][1].
- The paper assumes that the graph structure is unknown and time-varying, and learns it from the data using a Bernoulli distribution with a logistic function[^1^][1].
- The paper introduces a variational inference framework for learning both intra-object and inter-object mode-switching behaviour using a dynamic graph[^1^][1].
- The paper uses a recurrent neural network (RNN) to parameterize the variational distribution of the latent variables, and optimizes the evidence lower bound (ELBO) using stochastic gradient descent[^1^][1].

## Pseudo Code

Here is a possible pseudo code to implement the paper:

```python
# Define the hyperparameters
n_objects = number of objects in the data
n_modes = number of modes for each object
n_states = dimension of the state vector for each object
n_features = dimension of the feature vector for each object
n_hidden = dimension of the hidden state of the RNN
n_epochs = number of training epochs
batch_size = size of mini-batch
learning_rate = learning rate for SGD
alpha = weight decay parameter
beta = graph sparsity parameter

# Define the model components
# Non-linear function for per-object dynamics
def f(state, mode, feature):
  # state: n_states x 1 vector
  # mode: n_modes x 1 one-hot vector
  # feature: n_features x 1 vector
  # output: n_states x 1 vector
  return a neural network with inputs state, mode, feature and output state

# Function for inter-object dynamics
def g(state, mode, graph):
  # state: n_states x n_objects matrix
  # mode: n_modes x n_objects matrix
  # graph: n_objects x n_objects matrix
  # output: n_states x n_objects matrix
  return state * mode * graph

# RNN for variational distribution
def q(state, mode, graph):
  # state: n_states x n_objects x T tensor (T is the number of time steps)
  # mode: n_modes x n_objects x T tensor
  # graph: n_objects x n_objects x T tensor
  # output: (n_modes + n_objects^2) x n_objects x T tensor (concatenation of mode and graph)
  initialize h_0 as a n_hidden x n_objects matrix of zeros
  for t in range(T):
    h_t = a neural network with inputs h_(t-1), state[:,:,t], mode[:,:,t], graph[:,:,t] and output h_t
    q_t = a neural network with input h_t and output q_t (mode and graph probabilities)
    concatenate q_t along the third dimension
  return q

# Define the loss function (negative ELBO)
def loss(state, feature, q):
  # state: n_states x n_objects x T tensor
  # feature: n_features x n_objects x T tensor
  # q: (n_modes + n_objects^2) x n_objects x T tensor
  initialize loss as zero
  for t in range(T):
    # Sample mode and graph from q[:,:,t] using Gumbel-softmax trick
    mode = sample_mode(q[:n_modes,:,t])
    graph = sample_graph(q[n_modes:,:,t])
    # Predict next state using f and g functions
    pred_state = f(state[:,:,t], mode, feature[:,:,t]) + g(state[:,:,t], mode, graph)
    # Compute reconstruction error (squared error)
    recon_error = sum((pred_state - state[:,:,t+1])**2)
    # Compute regularization error (graph sparsity and weight decay)
    reg_error = beta * sum(graph) + alpha * sum(model parameters**2)
    # Compute entropy of q (sum of binary entropies)
    entropy = -sum(q[:,:,t] * log(q[:,:,t]) + (1-q[:,:,t]) * log(1-q[:,:,t]))
    # Update loss as weighted sum of errors and entropy
    loss = loss + recon_error + reg_error - entropy
  return loss

# Define the optimizer (SGD with momentum)
optimizer = SGD(model parameters, learning_rate, momentum)

# Load the data (state and feature tensors)
data = load_data()

# Train the model using mini-batch SGD
for epoch in range(n_epochs):
  shuffle data
  for batch in data:
    state, feature = batch
    q = q(state, mode, graph) # forward pass
    loss = loss(state, feature, q) # compute loss
    loss.backward() # compute gradients
    optimizer.step() # update parameters

```