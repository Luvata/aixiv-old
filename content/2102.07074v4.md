---
title: 2102.07074v4 TransGAN  Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up
date: 2021-02-08
---

# [TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up](http://arxiv.org/abs/2102.07074v4)

authors: Yifan Jiang, Shiyu Chang, Zhangyang Wang


## What, Why and How

[1]: https://arxiv.org/abs/2102.07074 "[2102.07074] TransGAN: Two Pure Transformers Can Make One Strong GAN ..."
[2]: https://www.scribd.com/document/560833595/Research-Notes "Research Notes | PDF | Computing | Cybernetics - Scribd"
[3]: https://arxiv.org/pdf/2102.07074 "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel generative adversarial network (GAN) architecture that uses only pure transformer-based models for both the generator and the discriminator, called TransGAN.
- **Why**: The paper aims to explore the potential of transformers as "universal" models for computer vision tasks, especially for challenging ones such as image generation. The paper also wants to understand how transformer-based generation models differ from convolutional ones in terms of training dynamics and visual quality.
- **How**: The paper designs a memory-friendly transformer-based generator that progressively increases feature resolution, and a multi-scale discriminator that captures semantic contexts and low-level textures. The paper also introduces a new module of grid self-attention for further reducing memory consumption and enabling high-resolution generation. The paper develops a unique training recipe that includes data augmentation, modified normalization, and relative position encoding to stabilize the training of TransGAN. The paper evaluates TransGAN on various image datasets and compares it with state-of-the-art GANs using convolutional backbones. The paper also visualizes the attention maps and feature maps of TransGAN to analyze its behavior.

## Main Contributions

[1]: https://arxiv.org/abs/2102.07074 "[2102.07074] TransGAN: Two Pure Transformers Can Make One Strong GAN ..."
[2]: https://www.scribd.com/document/560833560/Research-Notes "Research Notes | PDF | Cybernetics | Cognition - Scribd"
[3]: https://arxiv.org/pdf/2102.07074 "arXiv.org e-Print archive"

The paper claims the following contributions[^1^][1]:

- It is the first work to build a GAN completely free of convolutions, using only pure transformer-based architectures for both the generator and the discriminator.
- It introduces a new module of grid self-attention that enables high-resolution image generation with transformers while reducing memory consumption.
- It develops a unique training recipe that includes data augmentation, modified normalization, and relative position encoding to stabilize the training of TransGAN.
- It achieves highly competitive performance compared to state-of-the-art GANs using convolutional backbones on various image datasets, and sets new records on STL-10.
- It provides insights into the transformer-based generation models by visualizing their attention maps and feature maps.

## Method Summary

[1]: https://arxiv.org/abs/2102.07074 "[2102.07074] TransGAN: Two Pure Transformers Can Make One Strong GAN ..."
[2]: https://www.scribd.com/document/560833560/Research-Notes "Research Notes | PDF | Cybernetics | Cognition - Scribd"
[3]: https://arxiv.org/pdf/2102.07074 "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the details of the TransGAN architecture, which consists of a transformer-based generator and a transformer-based discriminator. The generator takes a latent vector as input and outputs an image through a series of upsampling stages, each with a transformer encoder module. The discriminator takes an image as input and outputs a probability of being real or fake through a series of downsampling stages, each with a transformer encoder module. Both the generator and the discriminator use multi-head self-attention and feed-forward networks as the basic building blocks of the transformer encoder module.
- The paper introduces a new module of grid self-attention, which is a variant of self-attention that operates on a regular grid of patches instead of a sequence of patches. The grid self-attention allows the model to capture spatial relationships between patches more efficiently and effectively, and reduces the memory consumption by avoiding storing the full attention matrix. The paper uses grid self-attention in the last stage of the generator and the first stage of the discriminator to enable high-resolution image generation.
- The paper develops a unique training recipe that includes data augmentation, modified normalization, and relative position encoding to stabilize the training of TransGAN. The paper uses random cropping, horizontal flipping, color jittering, and Gaussian blur as data augmentation techniques to increase the diversity and robustness of the training data. The paper modifies the layer normalization in the transformer encoder module to exclude the class token and position embeddings from normalization, which helps preserve their information. The paper also adds relative position encoding to the self-attention mechanism to provide spatial cues for the model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator and discriminator models
generator = TransformerGenerator()
discriminator = TransformerDiscriminator()

# Define the loss function and optimizer
loss = BinaryCrossEntropyLoss()
optimizer = Adam()

# Define the data loader and augmentation
data_loader = DataLoader(dataset)
augmentation = RandomCrop + Flip + ColorJitter + Blur

# Train the models
for epoch in range(num_epochs):
  for batch in data_loader:
    # Apply data augmentation
    batch = augmentation(batch)
    
    # Generate fake images from random latent vectors
    z = torch.randn(batch_size, latent_dim)
    fake_images = generator(z)
    
    # Compute the discriminator outputs for real and fake images
    real_outputs = discriminator(batch)
    fake_outputs = discriminator(fake_images)
    
    # Compute the generator and discriminator losses
    gen_loss = loss(fake_outputs, torch.ones(batch_size))
    dis_loss = loss(real_outputs, torch.ones(batch_size)) + loss(fake_outputs, torch.zeros(batch_size))
    
    # Update the generator and discriminator parameters
    optimizer.zero_grad()
    gen_loss.backward()
    optimizer.step()
    
    optimizer.zero_grad()
    dis_loss.backward()
    optimizer.step()
    
  # Evaluate the models on validation data and metrics
  evaluate(generator, discriminator, val_data, inception_score, FID)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Define the transformer encoder module
def TransformerEncoder(input_dim, hidden_dim, num_heads, dropout):
  # Define the multi-head self-attention layer
  self_attention = MultiHeadSelfAttention(input_dim, num_heads)
  
  # Define the feed-forward network layer
  feed_forward = FeedForwardNetwork(input_dim, hidden_dim)
  
  # Define the layer normalization layers
  norm1 = LayerNorm(input_dim)
  norm2 = LayerNorm(input_dim)
  
  # Define the dropout layers
  dropout1 = Dropout(dropout)
  dropout2 = Dropout(dropout)
  
  # Define the forward function
  def forward(x):
    # Apply self-attention and residual connection
    y = self_attention(x)
    y = dropout1(y)
    x = x + y
    
    # Apply layer normalization
    x = norm1(x)
    
    # Apply feed-forward network and residual connection
    y = feed_forward(x)
    y = dropout2(y)
    x = x + y
    
    # Apply layer normalization
    x = norm2(x)
    
    return x
  
  return forward

# Define the grid self-attention module
def GridSelfAttention(input_dim, num_heads):
  # Define the multi-head self-attention layer
  self_attention = MultiHeadSelfAttention(input_dim, num_heads)
  
  # Define the forward function
  def forward(x):
    # Reshape the input from (B, H*W, C) to (B*H, W, C)
    B, HW, C = x.shape
    H = int(math.sqrt(HW))
    W = H
    x = x.reshape(B, H, W, C).transpose(1, 2).reshape(B*H, W, C)
    
    # Apply self-attention and reshape the output back to (B, H*W, C)
    y = self_attention(x)
    y = y.reshape(B, W, H, C).transpose(1, 2).reshape(B, HW, C)
    
    return y
  
  return forward

# Define the generator model
def TransformerGenerator(latent_dim, image_size):
  # Define the number of stages and channels for each stage
  num_stages = int(math.log2(image_size)) - 2
  channels = [latent_dim] + [min(1024 // (2**i), 384) for i in range(num_stages)]
  
  # Define the linear embedding layer for the latent vector
  linear_embedding = Linear(latent_dim, channels[0] * 16)
  
  # Define the transformer encoder modules for each stage
  transformer_encoders = []
  for i in range(num_stages):
    transformer_encoders.append(TransformerEncoder(channels[i], channels[i] * 4, num_heads=4))
  
  # Define the grid self-attention module for the last stage
  grid_self_attention = GridSelfAttention(channels[-1], num_heads=4)
  
  # Define the upsampling layers for each stage
  upsamplings = []
  for i in range(num_stages):
    upsamplings.append(Upsample(scale_factor=2))
  
  # Define the convolutional layers for each stage
  convolutions = []
  for i in range(num_stages):
    convolutions.append(Conv2d(channels[i], channels[i+1], kernel_size=3, padding=1))
  
  # Define the output convolutional layer
  output_convolution = Conv2d(channels[-1], num_channels=3, kernel_size=1)
  
  # Define the forward function
  def forward(z):
    # Embed the latent vector into a tensor of shape (B, C0*16)
    x = linear_embedding(z)
    
    # Reshape the tensor into a patch of shape (B, C0*4, 2*2) and add a class token at the beginning of each sequence
    B = x.shape[0]
    class_token = torch.randn(B, channels[0], device=x.device) / math.sqrt(channels[0])
    x = x.reshape(B, channels[0] * 4 , -1).transpose(1 ,2).unsqueeze(1) 
    x = torch.cat([class_token.unsqueeze(1), x], dim=1) 
    
    # Add position embeddings to the patch and apply a non-linearity
    position_embeddings = torch.randn(17 , channels[0], device=x.device) / math.sqrt(channels[0])
    x = x + position_embeddings 
    x = gelu(x)
    
    # Loop over the stages
    for i in range(num_stages):
      # Apply the transformer encoder module
      x = transformer_encoders[i](x)
      
      # Remove the class token and reshape the tensor into an image of shape (B, Ci, 4*2**i, 4*2**i)
      x = x[:, 1:, :]
      x = x.transpose(1, 2).reshape(B, channels[i], 4 * 2**i, 4 * 2**i)
      
      # Apply the upsampling layer
      x = upsamplings[i](x)
      
      # Apply the convolutional layer
      x = convolutions[i](x)
      
      # Reshape the image into a patch of shape (B, Ci+1*16, (2*2**i)**2 / 16) and add a class token at the beginning of each sequence
      class_token = torch.randn(B, channels[i+1], device=x.device) / math.sqrt(channels[i+1])
      x = x.reshape(B, channels[i+1] * 16 , -1).transpose(1 ,2).unsqueeze(1) 
      x = torch.cat([class_token.unsqueeze(1), x], dim=1) 
      
      # Add position embeddings to the patch and apply a non-linearity
      position_embeddings = torch.randn((2*2**i)**2 / 16 + 1 , channels[i+1], device=x.device) / math.sqrt(channels[i+1])
      x = x + position_embeddings 
      x = gelu(x)
    
    # Apply the grid self-attention module
    x = grid_self_attention(x)
    
    # Remove the class token and reshape the tensor into an image of shape (B, Cn, image_size, image_size)
    x = x[:, 1:, :]
    x = x.transpose(1, 2).reshape(B, channels[-1], image_size, image_size)
    
    # Apply the output convolutional layer and a tanh activation
    x = output_convolution(x)
    x = tanh(x)
    
    return x
  
  return forward

# Define the discriminator model
def TransformerDiscriminator(image_size):
  # Define the number of stages and channels for each stage
  num_stages = int(math.log2(image_size)) - 2
  channels = [3] + [min(1024 // (2**i), 384) for i in range(num_stages)]
  
  # Define the grid self-attention module for the first stage
  grid_self_attention = GridSelfAttention(channels[0], num_heads=4)
  
  # Define the convolutional layers for each stage
  convolutions = []
  for i in range(num_stages):
    convolutions.append(Conv2d(channels[i], channels[i+1], kernel_size=3, padding=1))
  
  # Define the downsampling layers for each stage
  downsamplings = []
  for i in range(num_stages):
    downsamplings.append(AvgPool2d(kernel_size=3, stride=2, padding=1))
  
  # Define the transformer encoder modules for each stage
  transformer_encoders = []
  for i in range(num_stages):
    transformer_encoders.append(TransformerEncoder(channels[i+1], channels[i+1] * 4, num_heads=4))
  
  # Define the output linear layer
  output_linear = Linear(channels[-1], num_classes=1)
  
  # Define the forward function
  def forward(x):
    # Apply the grid self-attention module
    x = grid_self_attention(x)
    
    # Reshape the image into a patch of shape (B, C0*16, (image_size/4)**2 /16) and add a class token at the beginning of each sequence
    B = x.shape[0]
    class_token = torch.randn(B, channels[0], device=x.device) / math.sqrt(channels[0])
    x = x.reshape(B, channels[0] * 16 , -1).transpose(1 ,2).unsqueeze(1) 
    x = torch.cat([class_token.unsqueeze(1), x], dim=1) 
    
    # Add position embeddings to the patch and apply a non-linearity
    position_embeddings = torch.randn((image_size/4)**2 /16 + 1 , channels[0], device=x.device) / math.sqrt(channels[0])
    x = x + position_embeddings 
    x = gelu(x)
    
    # Loop over the stages
    for i in range(num_stages):
      # Apply the convolutional layer
      x = convolutions[i](x)
      
      # Apply the downsampling layer
      x = downsamplings[i](x)
      
      # Reshape the image into a