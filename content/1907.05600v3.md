---
title: 1907.05600v3 Generative Modeling by Estimating Gradients of the Data Distribution
date: 2019-07-06
---

# [Generative Modeling by Estimating Gradients of the Data Distribution](http://arxiv.org/abs/1907.05600v3)

authors: Yang Song, Stefano Ermon


## What, Why and How

[1]: https://arxiv.org/abs/1907.05600 "[1907.05600] Generative Modeling by Estimating Gradients of the Data ..."
[2]: https://arxiv.org/pdf/1907.05600v3.pdf "arXiv:1907.05600v3 [cs.LG] 10 Oct 2020"
[3]: https://arxiv-export1.library.cornell.edu/abs/1907.05600 "[1907.05600] Generative Modeling by Estimating Gradients of the Data ..."

Here is a summary of the paper:

- **What**: The paper introduces a new generative model that uses gradients of the data distribution estimated with score matching to produce samples via Langevin dynamics[^1^][1].
- **Why**: The paper aims to overcome some of the limitations of existing generative models, such as specialized architectures, surrogate losses, adversarial training, and model evaluation[^1^][1].
- **How**: The paper proposes to perturb the data with different levels of Gaussian noise and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels[^1^][1]. For sampling, the paper proposes an annealed Langevin dynamics where gradients corresponding to gradually decreasing noise levels are used as the sampling process gets closer to the data manifold[^1^][1]. The paper also provides a learning objective that can be used for principled model comparisons[^1^][1]. The paper evaluates the proposed model on MNIST, CelebA and CIFAR-10 datasets and demonstrates its effectiveness in generating high-quality samples and learning useful representations[^1^][1].

## Main Contributions

Some of the contributions of this paper are:

- It introduces a new generative model based on estimating and sampling from the score of the data distribution.
- It proposes a novel method to estimate scores for different noise levels by perturbing the data with Gaussian noise and using score matching.
- It proposes an annealed Langevin dynamics for sampling that adapts to the noise level and the data manifold.
- It provides a learning objective that can be used for principled model comparisons and does not require sampling during training or adversarial methods.
- It achieves state-of-the-art results on CIFAR-10 and comparable results to GANs on MNIST and CelebA datasets.
- It demonstrates that the proposed model learns effective representations via image inpainting experiments.


## Method Summary

The method section of the paper consists of four subsections:

- **Score Matching**: This subsection reviews the score matching technique for estimating the score of a data distribution given a parametric model. It also introduces the denoising score matching method for estimating the score of a perturbed data distribution given a noise level.
- **Noise Conditional Score Network**: This subsection presents the proposed model architecture that can estimate scores for different noise levels using a single network. It also describes how to train the network using denoising score matching and how to use it for sampling via annealed Langevin dynamics.
- **Model Comparison**: This subsection discusses how to compare different generative models using the proposed learning objective, which is based on the Kullback-Leibler divergence between the data distribution and the model distribution. It also shows how to estimate this objective using score matching and importance sampling.
- **Implementation Details**: This subsection provides some details on the network architecture, hyperparameters, optimization algorithm, and evaluation metrics used in the experiments.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the network architecture and hyperparameters
network = NoiseConditionalScoreNetwork()
noise_levels = [sigma_1, sigma_2, ..., sigma_K]
learning_rate = 0.001
batch_size = 64
num_epochs = 100
num_samples = 1000
num_steps = 100
step_size = 0.01

# Train the network using denoising score matching
for epoch in range(num_epochs):
  for batch in data_loader:
    # Sample noise levels uniformly
    noise_level = random.choice(noise_levels)
    # Add Gaussian noise to the data
    noisy_batch = batch + normal(0, noise_level)
    # Compute the score of the noisy data using the network
    score = network(noisy_batch, noise_level)
    # Compute the denoising score matching loss
    loss = 0.5 * mean((score + noisy_batch / noise_level) ** 2)
    # Update the network parameters using gradient descent
    network.backward(loss)
    network.update(learning_rate)

# Sample from the network using annealed Langevin dynamics
samples = []
for i in range(num_samples):
  # Initialize a random sample
  sample = normal(0, 1)
  # Loop over the noise levels in descending order
  for noise_level in reversed(noise_levels):
    # Run Langevin dynamics for a fixed number of steps
    for j in range(num_steps):
      # Compute the score of the sample using the network
      score = network(sample, noise_level)
      # Update the sample using gradient ascent and Gaussian noise
      sample = sample + step_size * score + normal(0, sqrt(step_size * noise_level))
  # Append the final sample to the list
  samples.append(sample)

# Evaluate the samples using inception score or other metrics
inception_score = compute_inception_score(samples)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the network architecture
class NoiseConditionalScoreNetwork(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a convolutional neural network with residual blocks and gated activation units
    # The network takes an image and a noise level as inputs and outputs a score vector
    # The network has skip connections from the input to the output
    self.conv1 = torch.nn.Conv2d(3, 64, 3, padding=1)
    self.resblocks = torch.nn.ModuleList([ResBlock(64) for _ in range(8)])
    self.conv2 = torch.nn.Conv2d(64, 3, 3, padding=1)
    self.gate = torch.nn.Sigmoid()

  def forward(self, x, sigma):
    # Normalize the input by the noise level
    x = x / sigma
    # Concatenate the noise level to the input as an extra channel
    sigma = sigma.view(-1, 1, 1, 1).repeat(1, 1, x.shape[2], x.shape[3])
    x = torch.cat([x, sigma], dim=1)
    # Apply the first convolutional layer
    x = self.conv1(x)
    # Apply the residual blocks
    for resblock in self.resblocks:
      x = resblock(x)
    # Apply the second convolutional layer
    x = self.conv2(x)
    # Apply the gated activation unit
    x = x * self.gate(x)
    # Add the skip connection from the input to the output
    x = x + x[:, :3]
    # Return the score vector
    return x

# Define the residual block module
class ResBlock(torch.nn.Module):
  def __init__(self, channels):
    super().__init__()
    # Use two convolutional layers with batch normalization and gated activation units
    self.conv1 = torch.nn.Conv2d(channels, channels, 3, padding=1)
    self.bn1 = torch.nn.BatchNorm2d(channels)
    self.conv2 = torch.nn.Conv2d(channels, channels, 3, padding=1)
    self.bn2 = torch.nn.BatchNorm2d(channels)
    self.gate = torch.nn.Sigmoid()

  def forward(self, x):
    # Save the input for the skip connection
    skip = x
    # Apply the first convolutional layer
    x = self.conv1(x)
    # Apply batch normalization and gated activation unit
    x = self.bn1(x) * self.gate(x)
    # Apply the second convolutional layer
    x = self.conv2(x)
    # Apply batch normalization and gated activation unit
    x = self.bn2(x) * self.gate(x)
    # Add the skip connection from the input to the output
    x = x + skip
    # Return the output
    return x

# Define the hyperparameters
noise_levels = [0.01, 0.02, 0.05, 0.1] # The list of noise levels to use for training and sampling
learning_rate = 0.001 # The learning rate for gradient descent
batch_size = 64 # The batch size for training
num_epochs = 100 # The number of epochs for training
num_samples = 1000 # The number of samples to generate
num_steps = 100 # The number of steps for Langevin dynamics at each noise level
step_size = 0.01 # The step size for Langevin dynamics

# Create an instance of the network and a device object
network = NoiseConditionalScoreNetwork()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
network.to(device)

# Create an optimizer and a loss function for training
optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)
loss_function = torch.nn.MSELoss()

# Load the CIFAR-10 dataset and create a data loader for training
transform = torchvision.transforms.ToTensor()
dataset = torchvision.datasets.CIFAR10(root="data", train=True, download=True, transform=transform)
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Train the network using denoising score matching
for epoch in range(num_epochs):
  for batch in data_loader:
    # Move the batch to the device and normalize it to [-1, 1]
    batch = batch[0].to(device) * 2 - 1 
    # Sample noise levels uniformly from the list
    noise_level = np.random.choice(noise_levels)
    # Add Gaussian noise to the batch
    noisy_batch = batch + torch.randn_like(batch) * noise_level
    # Compute the score of the noisy batch using the network
    score = network(noisy_batch, torch.tensor(noise_level).to(device))
    # Compute the denoising score matching loss
    loss = 0.5 * loss_function(score + noisy_batch / noise_level, batch / noise_level)
    # Update the network parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # Print the epoch and the loss
  print(f"Epoch {epoch + 1}, Loss {loss.item()}")

# Sample from the network using annealed Langevin dynamics
samples = []
for i in range(num_samples):
  # Initialize a random sample from a standard normal distribution
  sample = torch.randn(3, 32, 32).to(device)
  # Loop over the noise levels in descending order
  for noise_level in reversed(noise_levels):
    # Run Langevin dynamics for a fixed number of steps
    for j in range(num_steps):
      # Compute the score of the sample using the network
      score = network(sample, torch.tensor(noise_level).to(device))
      # Update the sample using gradient ascent and Gaussian noise
      sample = sample + step_size * score + torch.randn_like(sample) * np.sqrt(step_size * noise_level)
  # Denormalize the sample to [0, 1] and append it to the list
  sample = (sample + 1) / 2
  samples.append(sample)

# Evaluate the samples using inception score or other metrics
inception_score = compute_inception_score(samples)
```