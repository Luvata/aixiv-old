---
title: 2306.10008v1 CLIP2Protect  Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search
date: 2023-06-11
---

# [CLIP2Protect: Protecting Facial Privacy using Text-Guided Makeup via Adversarial Latent Search](http://arxiv.org/abs/2306.10008v1)

authors: Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar


## What, Why and How

[1]: https://arxiv.org/pdf/2306.10008v1 "Protected - arXiv.org"
[2]: https://arxiv.org/abs/2306.10008 "[2306.10008] CLIP2Protect: Protecting Facial Privacy using Text-Guided ..."
[3]: http://export.arxiv.org/abs/2306.10008 "[2306.10008] CLIP2Protect: Protecting Facial Privacy using Text-Guided ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model. The approach allows users to specify makeup text prompts that guide the generation of naturalistic and transferable adversarial faces that can deceive black-box face recognition systems.
- **Why**: The paper addresses the serious privacy concerns due to the success of deep learning based face recognition systems that can enable unauthorized tracking of users in the digital world. Existing methods for enhancing privacy fail to generate naturalistic images that can protect facial privacy without compromising user experience.
- **How**: The paper uses a two-step approach: (1) Invert the given face image into the latent space and finetune the generative model to achieve an accurate reconstruction of the given image from its latent code. This step produces a good initialization, aiding the generation of high-quality faces that resemble the given identity. (2) Use user-defined makeup text prompts and identity-preserving regularization to guide the search for adversarial codes in the latent space. The paper evaluates the proposed approach on various face recognition tasks and commercial systems, and shows that it outperforms existing methods in terms of black-box transferability and naturalness[^1^][1] [^2^][2].


## Main Contributions

[1]: https://arxiv.org/pdf/2306.10008v1 "Protected - arXiv.org"
[2]: https://arxiv.org/abs/2306.10008 "[2306.10008] CLIP2Protect: Protecting Facial Privacy using Text-Guided ..."
[3]: http://export.arxiv.org/abs/2306.10008 "[2306.10008] CLIP2Protect: Protecting Facial Privacy using Text-Guided ..."

The paper lists the following contributions:

- A novel two-step approach for facial privacy protection that relies on finding adversarial latent codes in the low-dimensional manifold of a pretrained generative model.
- A user-friendly interface that allows users to specify makeup text prompts that guide the generation of naturalistic and transferable adversarial faces that can deceive black-box face recognition systems.
- A comprehensive evaluation of the proposed approach on various face recognition tasks and commercial systems, showing that it outperforms existing methods in terms of black-box transferability and naturalness.
- A public code release and a project page with more details and results.[^1^][1] [^2^][2]


## Method Summary

[1]: https://arxiv.org/pdf/2306.10008v1 "Protected - arXiv.org"
[2]: https://arxiv.org/abs/2306.10008 "[2306.10008] CLIP2Protect: Protecting Facial Privacy using Text-Guided ..."
[3]: http://export.arxiv.org/abs/2306.10008 "[2306.10008] CLIP2Protect: Protecting Facial Privacy using Text-Guided ..."

Here is a summary of the method section of the paper:

- **The paper uses a two-step approach for facial privacy protection:**
    - **Step 1: Invert the given face image into the latent space and finetune the generative model to achieve an accurate reconstruction of the given image from its latent code.** The paper uses a pretrained StyleGAN2 [24] model as the generative model and employs an encoder network [25] to invert the given face image into the latent space. The paper also finetunes the StyleGAN2 model using a reconstruction loss and a perceptual loss [26] to improve the quality and fidelity of the reconstructed image.
    - **Step 2: Use user-defined makeup text prompts and identity-preserving regularization to guide the search for adversarial codes in the latent space.** The paper uses CLIP [49], a pretrained vision-language model, to measure the semantic similarity between the makeup text prompt and the generated image. The paper also uses an identity-preserving regularization term that penalizes large changes in the identity features extracted by a pretrained face recognition model [61]. The paper optimizes the latent code using gradient descent to minimize a weighted combination of these two terms, resulting in an adversarial image that matches the makeup text prompt and preserves the identity of the original image.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a face image x and a makeup text prompt t
# Output: an adversarial image x_adv that matches t and preserves the identity of x

# Step 1: Invert x into the latent space and finetune the generative model
z = encoder(x) # use an encoder network to invert x into the latent space
G = StyleGAN2() # use a pretrained StyleGAN2 model as the generative model
G = finetune(G, z, x) # finetune G using a reconstruction loss and a perceptual loss

# Step 2: Use t and identity-preserving regularization to guide the search for adversarial codes
CLIP = CLIP() # use a pretrained CLIP model to measure semantic similarity
FR = FR() # use a pretrained face recognition model to extract identity features
lambda = 0.01 # set the weight for the identity-preserving regularization term
for i in range(max_iterations):
  x_adv = G(z) # generate an adversarial image from the latent code
  L_clip = -CLIP(t, x_adv) # compute the CLIP loss as the negative similarity score
  L_id = FR(x) - FR(x_adv) # compute the identity loss as the difference in identity features
  L_total = L_clip + lambda * L_id # compute the total loss as a weighted combination of the two terms
  z = z - lr * grad(L_total, z) # update the latent code using gradient descent

return x_adv # return the final adversarial image
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import stylegan2_pytorch
import face_recognition

# Define the hyperparameters
max_iterations = 1000 # the maximum number of iterations for the adversarial search
lambda = 0.01 # the weight for the identity-preserving regularization term
lr = 0.01 # the learning rate for the gradient descent

# Load the pretrained models
G = stylegan2_pytorch.load_pretrained('ffhq') # load a pretrained StyleGAN2 model on FFHQ dataset
encoder = stylegan2_pytorch.load_pretrained('encoder') # load a pretrained encoder network that maps images to latent codes
CLIP = clip.load('ViT-B/32') # load a pretrained CLIP model with vision transformer backbone
FR = face_recognition.load_model('facenet') # load a pretrained face recognition model based on Facenet

# Define the loss functions
def reconstruction_loss(x, x_hat):
  # compute the pixel-wise mean squared error between x and x_hat
  return torch.mean((x - x_hat) ** 2)

def perceptual_loss(x, x_hat):
  # compute the feature-wise mean squared error between x and x_hat using a pretrained VGG network
  vgg = torchvision.models.vgg16(pretrained=True).features # load a pretrained VGG network and extract its features
  x_features = vgg(x) # extract the features of x using VGG
  x_hat_features = vgg(x_hat) # extract the features of x_hat using VGG
  return torch.mean((x_features - x_hat_features) ** 2)

def clip_loss(t, x):
  # compute the negative CLIP similarity score between t and x
  t_encoded = CLIP.encode_text(t) # encode t into a text embedding using CLIP
  x_encoded = CLIP.encode_image(x) # encode x into an image embedding using CLIP
  return -torch.dot(t_encoded, x_encoded) / (torch.norm(t_encoded) * torch.norm(x_encoded)) # compute the cosine similarity and negate it

def identity_loss(x, x_adv):
  # compute the difference in identity features between x and x_adv using a face recognition model
  x_features = FR(x) # extract the identity features of x using FR
  x_adv_features = FR(x_adv) # extract the identity features of x_adv using FR
  return torch.norm(x_features - x_adv_features) # compute the L2 norm of the difference

# Define the main function that takes a face image x and a makeup text prompt t as input and returns an adversarial image x_adv as output
def clip2protect(x, t):
  # Step 1: Invert x into the latent space and finetune the generative model
  z = encoder(x) # use the encoder network to invert x into the latent space
  z.requires_grad = True # enable gradient computation for z
  optimizer = torch.optim.Adam([z], lr=lr) # create an optimizer for z
  for i in range(max_iterations):
    optimizer.zero_grad() # clear the previous gradients
    x_hat = G(z) # generate a reconstructed image from z using G
    L_rec = reconstruction_loss(x, x_hat) # compute the reconstruction loss between x and x_hat
    L_per = perceptual_loss(x, x_hat) # compute the perceptual loss between x and x_hat
    L_total = L_rec + L_per # compute the total loss as a sum of the two losses
    L_total.backward() # compute the gradients of the total loss with respect to z
    optimizer.step() # update z using gradient descent

  # Step 2: Use t and identity-preserving regularization to guide the search for adversarial codes in the latent space
  for i in range(max_iterations):
    optimizer.zero_grad() # clear the previous gradients
    x_adv = G(z) # generate an adversarial image from z using G
    L_clip = clip_loss(t, x_adv) # compute the CLIP loss between t and x_adv
    L_id = identity_loss(x, x_adv) # compute the identity loss between x and x_adv
    L_total = L_clip + lambda * L_id # compute the total loss as a weighted combination of the two losses
    L_total.backward() # compute the gradients of the total loss with respect to z
    optimizer.step() # update z using gradient descent

  return x_adv # return the final adversarial image

```