---
title: 2203.05081v1 NLX-GPT  A Model for Natural Language Explanations in Vision and Vision-Language Tasks
date: 2022-03-06
---

# [NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks](http://arxiv.org/abs/2203.05081v1)

authors: Fawaz Sammani, Tanmoy Mukherjee, Nikos Deligiannis


## What, Why and How

[1]: https://arxiv.org/abs/2203.05081v1 "[2203.05081v1] NLX-GPT: A Model for Natural Language Explanations in ..."
[2]: http://export.arxiv.org/abs/2203.05081 "[2203.05081] NLX-GPT: A Model for Natural Language Explanations in ..."
[3]: https://arxiv.org/pdf/2203.05081.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: NLX-GPT is a model for natural language explanations in vision and vision-language tasks, such as visual question answering (VQA).

Why: NLX-GPT aims to generate human-friendly, high-level and fine-grained explanations of the decision-making process of a black box system, without relying on a separate task model or region proposals.

How: NLX-GPT is a language model that is pre-trained on image-caption pairs and then fine-tuned to predict an answer and an explanation for a given image and question. NLX-GPT also introduces two new evaluation measures for explanations: explain-predict and retrieval-based attack.
```

## Main Contributions

[1]: https://arxiv.org/abs/2203.05081v1 "[2203.05081v1] NLX-GPT: A Model for Natural Language Explanations in ..."
[2]: http://export.arxiv.org/abs/2203.05081 "[2203.05081] NLX-GPT: A Model for Natural Language Explanations in ..."
[3]: https://arxiv.org/pdf/2203.05081.pdf "arXiv.org e-Print archive"

Here are some of the contributions of the paper:

- NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it for vision and vision-language tasks, without relying on a separate task model or region proposals.
- Pre-training on large scale data of image-caption pairs for general understanding of images, and then formulating the answer as a text prediction task along with the explanation.
- Two new evaluation measures for explanations: explain-predict and retrieval-based attack, a self-evaluation framework that requires no labels.

## Method Summary

[1]: https://arxiv.org/abs/2203.05081v1 "[2203.05081v1] NLX-GPT: A Model for Natural Language Explanations in ..."
[2]: http://export.arxiv.org/abs/2203.05081 "[2203.05081] NLX-GPT: A Model for Natural Language Explanations in ..."
[3]: https://arxiv.org/pdf/2203.05081.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

```
The method section consists of four subsections: (1) NLX-GPT, (2) Pre-training, (3) Fine-tuning, and (4) Evaluation.

- NLX-GPT: The authors describe the architecture and input format of their model, which is based on GPT-2. They use a special token <IMG> to indicate the image input, which is encoded by a CNN and projected to the same dimension as the text embeddings. They also use special tokens <ANS> and <EXP> to indicate the answer and explanation outputs, respectively. They use a cross-entropy loss to train the model to predict the next token given the previous tokens and the image.

- Pre-training: The authors explain how they pre-train their model on large scale data of image-caption pairs from Conceptual Captions dataset. They use a masked language modeling objective to predict the masked tokens in the captions, as well as an image-text matching objective to predict whether an image and a caption are matched or not.

- Fine-tuning: The authors describe how they fine-tune their model on three vision and vision-language tasks: VQA, VQA-X, and VCR-X. They use the same input format as in pre-training, but with different datasets and answer candidates. They also use different evaluation metrics for each task.

- Evaluation: The authors introduce two new evaluation measures for explanations: explain-predict and retrieval-based attack. Explain-predict measures how well the explanation can predict the answer given the image and question. Retrieval-based attack measures how well the explanation can distinguish between the correct answer and a distractor answer given the image and question.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2203.05081v1 "[2203.05081v1] NLX-GPT: A Model for Natural Language Explanations in ..."
[2]: http://export.arxiv.org/abs/2203.05081 "[2203.05081] NLX-GPT: A Model for Natural Language Explanations in ..."
[3]: https://arxiv.org/pdf/2203.05081.pdf "arXiv.org e-Print archive"

Here is a high-level pseudo code for the paper:

```
# Define the NLX-GPT model based on GPT-2
model = NLX-GPT()

# Pre-train the model on image-caption pairs
for image, caption in Conceptual_Captions:
  # Mask some tokens in the caption
  masked_caption = mask(caption)
  # Encode the image and project it to the same dimension as text embeddings
  image_embedding = project(encode(image))
  # Concatenate the image embedding and the masked caption with special tokens
  input = [<IMG>, image_embedding, <SEP>, masked_caption]
  # Predict the next token for each masked position
  output = model(input)
  # Compute the cross-entropy loss between output and caption
  loss = cross_entropy(output, caption)
  # Update the model parameters
  model.update(loss)

# Fine-tune the model on vision and vision-language tasks
for task in [VQA, VQA-X, VCR-X]:
  for image, question, answer, explanation in task_data:
    # Encode the image and project it to the same dimension as text embeddings
    image_embedding = project(encode(image))
    # Concatenate the image embedding, question, answer and explanation with special tokens
    input = [<IMG>, image_embedding, <SEP>, question, <ANS>, answer, <EXP>, explanation]
    # Predict the next token for each position after <ANS> and <EXP>
    output = model(input)
    # Compute the cross-entropy loss between output and answer + explanation
    loss = cross_entropy(output, answer + explanation)
    # Update the model parameters
    model.update(loss)

# Evaluate the model on explain-predict and retrieval-based attack measures
for task in [VQA, VQA-X, VCR-X]:
  for image, question, answer, explanation in task_data:
    # Encode the image and project it to the same dimension as text embeddings
    image_embedding = project(encode(image))
    # Concatenate the image embedding and question with special tokens
    input = [<IMG>, image_embedding, <SEP>, question]
    # Generate an answer and an explanation using beam search
    output = model.generate(input)
    # Split the output into answer and explanation parts
    answer, explanation = split(output)
    # Evaluate the answer using task-specific metrics
    evaluate_answer(answer)
    # Evaluate the explanation using explain-predict and retrieval-based attack measures
    evaluate_explanation(explanation)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2203.05081v1 "[2203.05081v1] NLX-GPT: A Model for Natural Language Explanations in ..."
[2]: http://export.arxiv.org/abs/2203.05081 "[2203.05081] NLX-GPT: A Model for Natural Language Explanations in ..."
[3]: https://arxiv.org/pdf/2203.05081.pdf "arXiv.org e-Print archive"

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import datasets

# Define the NLX-GPT model based on GPT-2
class NLX-GPT(transformers.GPT2LMHeadModel):
  def __init__(self, config):
    super().__init__(config)
    # Add an image encoder layer
    self.image_encoder = torchvision.models.resnet50(pretrained=True)
    # Add a projection layer to match the text embedding dimension
    self.projection = torch.nn.Linear(self.image_encoder.fc.out_features, config.n_embd)
    # Define the special tokens
    self.img_token = "<IMG>"
    self.ans_token = "<ANS>"
    self.exp_token = "<EXP>"

  def forward(self, input_ids, attention_mask, image=None):
    # If image is given, encode it and project it
    if image is not None:
      image_features = self.image_encoder(image)
      image_embedding = self.projection(image_features)
      # Find the position of the image token in the input ids
      img_pos = (input_ids == self.tokenizer.encode(self.img_token)[0]).nonzero(as_tuple=True)[1]
      # Replace the image token with the image embedding
      input_ids[:, img_pos] = image_embedding
    # Call the parent class forward method
    return super().forward(input_ids, attention_mask)

# Instantiate the model and the tokenizer
model = NLX-GPT.from_pretrained("gpt2")
tokenizer = transformers.GPT2Tokenizer.from_pretrained("gpt2")
# Add the special tokens to the tokenizer
tokenizer.add_special_tokens({"additional_special_tokens": [model.img_token, model.ans_token, model.exp_token]})
# Resize the model embeddings to match the new vocabulary size
model.resize_token_embeddings(len(tokenizer))

# Pre-train the model on image-caption pairs
# Load the Conceptual Captions dataset
dataset = datasets.load_dataset("conceptual_captions")
# Define the pre-training hyperparameters
batch_size = 32
num_epochs = 10
learning_rate = 1e-4
# Define the optimizer and the scheduler
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataset) * num_epochs // batch_size)
# Define the pre-training loop
for epoch in range(num_epochs):
  for batch in dataset.shuffle().batch(batch_size):
    # Get the image and caption from the batch
    image = batch["image"]
    caption = batch["caption"]
    # Mask some tokens in the caption with a probability of 15%
    masked_caption, labels = tokenizer.mask_tokens(caption, mlm_probability=0.15)
    # Encode the image and masked caption with special tokens
    input_ids = tokenizer([model.img_token] + masked_caption, return_tensors="pt", padding=True).input_ids
    # Create attention mask to ignore padding tokens
    attention_mask = input_ids != tokenizer.pad_token_id
    # Forward pass the model and get the logits
    logits = model(input_ids, attention_mask, image).logits
    # Compute the cross-entropy loss between logits and labels, ignoring padding tokens
    loss = torch.nn.functional.cross_entropy(logits.view(-1, len(tokenizer)), labels.view(-1), ignore_index=tokenizer.pad_token_id)
    # Backward pass and update the model parameters
    loss.backward()
    optimizer.step()
    scheduler.step()
    model.zero_grad()
  # Save the model checkpoint after each epoch
  model.save_pretrained(f"nlx-gpt-pretrained-{epoch}")

# Fine-tune the model on vision and vision-language tasks
# Define a list of tasks and their corresponding datasets and metrics
tasks = ["VQA", "VQA-X", "VCR-X"]
datasets = [datasets.load_dataset("vqa_v2"), datasets.load_dataset("vqa_x"), datasets.load_dataset("vcr_x")]
metrics = [datasets.load_metric("accuracy"), datasets.load_metric("bleu"), datasets.load_metric("rouge")]
# Define the fine-tuning hyperparameters
batch_size = 16
num_epochs = 5
learning_rate = 5e-5
# Fine-tune the model for each task
for task, dataset, metric in zip(tasks, datasets, metrics):
  # Load the pre-trained model and tokenizer
  model = NLX-GPT.from_pretrained(f"nlx-gpt-pretrained-{num_epochs - 1}")
  tokenizer = transformers.GPT2Tokenizer.from_pretrained(f"nlx-gpt-pretrained-{num_epochs - 1}")
  # Define the optimizer and the scheduler
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataset) * num_epochs // batch_size)
  # Define the fine-tuning loop
  for epoch in range(num_epochs):
    for batch in dataset.shuffle().batch(batch_size):
      # Get the image, question, answer and explanation from the batch
      image = batch["image"]
      question = batch["question"]
      answer = batch["answer"]
      explanation = batch["explanation"]
      # Encode the image, question, answer and explanation with special tokens
      input_ids = tokenizer([model.img_token] + question + [model.ans_token] + answer + [model.exp_token] + explanation, return_tensors="pt", padding=True).input_ids
      # Create attention mask to ignore padding tokens
      attention_mask = input_ids != tokenizer.pad_token_id
      # Forward pass the model and get the logits
      logits = model(input_ids, attention_mask, image).logits
      # Compute the cross-entropy loss between logits and answer + explanation, ignoring padding tokens
      labels = tokenizer(answer + explanation, return_tensors="pt", padding=True).input_ids
      loss = torch.nn.functional.cross_entropy(logits.view(-1, len(tokenizer)), labels.view(-1), ignore_index=tokenizer.pad_token_id)
      # Backward pass and update the model parameters
      loss.backward()
      optimizer.step()
      scheduler.step()
      model.zero_grad()
    # Save the model checkpoint after each epoch
    model.save_pretrained(f"nlx-gpt-finetuned-{task}-{epoch}")

# Evaluate the model on explain-predict and retrieval-based attack measures
# Define a list of tasks and their corresponding datasets and metrics
tasks = ["VQA", "VQA-X", "VCR-X"]
datasets = [datasets.load_dataset("vqa_v2"), datasets.load_dataset("vqa_x"), datasets.load_dataset("vcr_x")]
metrics = [datasets.load_metric("accuracy"), datasets.load_metric("bleu"), datasets.load_metric("rouge")]
# Evaluate the model for each task
for task, dataset, metric in zip(tasks, datasets, metrics):
  # Load the fine-tuned model and tokenizer
  model = NLX-GPT.from_pretrained(f"nlx-gpt-finetuned-{task}-{num_epochs - 1}")
  tokenizer = transformers.GPT2Tokenizer.from_pretrained(f"nlx-gpt-finetuned-{task}-{num_epochs - 1}")
  # Define the evaluation loop
  for batch in dataset.batch(batch_size):
    # Get the image, question, answer and explanation from the batch
    image = batch["image"]
    question = batch["question"]
    answer = batch["answer"]
    explanation = batch["explanation"]
    # Encode the image and question with special tokens
    input_ids = tokenizer([model.img_token] + question, return_tensors="pt", padding=True).input_ids
    # Create attention mask to ignore padding tokens
    attention_mask = input_ids != tokenizer.pad_token_id
    # Generate an answer and an explanation using beam search
    output_ids = model.generate(input_ids, attention_mask, image, num_beams=5)
    output = tokenizer.decode(output_ids[0])
    # Split the output into answer and explanation parts
    answer_pred, explanation_pred = output.split(model.exp_token)
    # Evaluate the answer using task-specific metrics
    metric.add(prediction=answer_pred, reference=answer)
    # Evaluate the explanation using explain-predict and retrieval-based attack measures
    explain_predict_score = explain_predict(explanation_pred, image, question)
    retrieval_based_attack_score = retrieval_based_attack(explanation_pred, image, question, answer)
  # Compute the average scores for each metric
  answer_score = metric.compute()
  explain_predict_score = torch.mean(explain_predict_score)
  retrieval_based_attack_score = torch.mean(retrieval_based_attack_score)
  # Print the scores for each task
  print(f"Task: {task}")
  print(f"Answer score: {answer_score}")
  print(f"Explain-predict score: {explain_predict_score}")
  print(f"Retrieval-based attack score: {retrieval_based_attack_score}")
```