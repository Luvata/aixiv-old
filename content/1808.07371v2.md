---
title: 1808.07371v2 Everybody Dance Now
date: 2018-08-08
---

# [Everybody Dance Now](http://arxiv.org/abs/1808.07371v2)

authors: Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros


## What, Why and How

[1]: https://arxiv.org/abs/1808.07371 "[1808.07371] Everybody Dance Now - arXiv.org"
[2]: https://arxiv.org/pdf/1808.07371v2.pdf "arXiv.org e-Print archive"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method for "do as I do" motion transfer, which can transfer the performance of a source person dancing to a novel target person after only a few minutes of the target performing standard moves.
- **Why**: The paper aims to enable untrained amateurs to imitate the motions of professional dancers or martial artists, and to create realistic and compelling videos of motion transfer. The paper also provides a forensics tool for detecting synthetic content and an open-source dataset of videos for training and motion transfer.
- **How**: The paper approaches the problem as video-to-video translation using pose as an intermediate representation. The paper extracts poses from the source subject and applies a learned pose-to-appearance mapping to generate the target subject. The paper predicts two consecutive frames for temporally coherent video results and introduces a separate pipeline for realistic face synthesis. The paper evaluates the method on various source and target subjects and compares it with existing methods.

## Main Contributions

According to the paper, the main contributions are:

- A simple and effective method for "do as I do" motion transfer that can produce realistic and compelling videos of novel target subjects performing the motions of source subjects.
- A forensics tool for reliable synthetic content detection that can distinguish videos synthesized by the proposed method from real data.
- A first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer.

## Method Summary

[1]: https://arxiv.org/abs/1808.07371 "[1808.07371] Everybody Dance Now - arXiv.org"
[2]: https://arxiv.org/pdf/1808.07371v2.pdf "arXiv.org e-Print archive"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the method section of the paper at [^1^][2]:

- The method consists of three main components: pose estimation, pose normalization, and video generation.
- Pose estimation is done by applying a pre-trained pose detector to both the source and target videos, and extracting 18 keypoints for each person in each frame. The keypoints are then converted to heatmaps and concatenated to form a pose tensor.
- Pose normalization is done by applying a spatial transformer network to the pose tensor of the target subject, which aligns the target pose with the source pose in terms of scale, rotation, and translation. This ensures that the target subject can imitate the source motion regardless of their initial pose or body shape.
- Video generation is done by applying a pix2pixHD network to the normalized pose tensor of the target subject, which maps the pose to an appearance image. The network is trained on pairs of target pose tensors and target appearance images, and uses two discriminators to enforce realism and temporal consistency. The network also predicts two consecutive frames for each input pose tensor, which reduces flickering and improves smoothness.
- For realistic face synthesis, the method uses a separate pipeline that extracts face regions from both the source and target videos, applies a face alignment network to align the faces in terms of landmarks and expressions, and uses a CycleGAN network to transfer the style of the source face to the target face. The synthesized face is then blended with the generated appearance image using Poisson blending.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: source video S, target video T
# Output: motion transfer video V

# Pose estimation
P_S = pose_detector(S) # pose tensor of source video
P_T = pose_detector(T) # pose tensor of target video

# Pose normalization
P_T_norm = spatial_transformer(P_T, P_S) # normalized pose tensor of target video

# Video generation
V = pix2pixHD(P_T_norm) # appearance image of target video

# Face synthesis
F_S = face_extractor(S) # face region of source video
F_T = face_extractor(T) # face region of target video
F_S_align = face_aligner(F_S) # aligned face region of source video
F_T_align = face_aligner(F_T) # aligned face region of target video
F_T_style = CycleGAN(F_T_align, F_S_align) # style transferred face region of target video
V_face = poisson_blender(V, F_T_style) # blended appearance image with synthesized face

# Final output
V = V_face # motion transfer video with realistic face
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: source video S, target video T
# Output: motion transfer video V

# Import libraries
import cv2 # for video processing
import numpy as np # for numerical operations
import torch # for deep learning
import torchvision # for computer vision
import face_alignment # for face alignment
import poissonblending # for poisson blending

# Load pre-trained models
pose_detector = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True) # for pose estimation
spatial_transformer = torch.load('spatial_transformer.pth') # for pose normalization
pix2pixHD = torch.load('pix2pixHD.pth') # for video generation
face_aligner = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False) # for face alignment
CycleGAN = torch.load('CycleGAN.pth') # for face synthesis

# Define constants
N = 18 # number of keypoints
H = 256 # height of pose tensor and appearance image
W = 256 # width of pose tensor and appearance image
C = 3 # number of channels of appearance image

# Define helper functions
def heatmap(x, y):
  # Generate a 2D Gaussian heatmap centered at (x, y)
  sigma = 1.0 # standard deviation of Gaussian
  X = np.arange(0, W)
  Y = np.arange(0, H)
  X, Y = np.meshgrid(X, Y)
  Z = np.exp(-((X - x)**2 + (Y - y)**2) / (2 * sigma**2))
  return Z

def extract_pose(video):
  # Extract pose tensor from video
  pose_tensor = []
  for frame in video:
    keypoints = pose_detector(frame)['keypoints'][0] # shape: (N, 3)
    heatmaps = [heatmap(x, y) for x, y, v in keypoints] # shape: (N, H, W)
    heatmaps = np.stack(heatmaps) # shape: (N, H, W)
    pose_tensor.append(heatmaps)
  pose_tensor = np.stack(pose_tensor) # shape: (T, N, H, W)
  return pose_tensor

def generate_video(pose_tensor):
  # Generate appearance image from pose tensor
  appearance_image = []
  for i in range(len(pose_tensor) - 1):
    input_pose = np.concatenate([pose_tensor[i], pose_tensor[i+1]]) # shape: (2*N, H, W)
    input_pose = torch.from_numpy(input_pose).unsqueeze(0) # shape: (1, 2*N, H, W)
    output_image = pix2pixHD(input_pose) # shape: (1, C, H, W)
    output_image = output_image.squeeze(0).numpy() # shape: (C, H, W)
    appearance_image.append(output_image)
  appearance_image = np.stack(appearance_image) # shape: (T-1, C, H, W)
  return appearance_image

def extract_face(video):
  # Extract face region from video
  face_region = []
  for frame in video:
    landmarks = face_aligner.get_landmarks(frame)[0] # shape: (68, 2)
    x_min = int(np.min(landmarks[:,0]))
    x_max = int(np.max(landmarks[:,0]))
    y_min = int(np.min(landmarks[:,1]))
    y_max = int(np.max(landmarks[:,1]))
    region = frame[y_min:y_max,x_min:x_max,:] # shape: (R_h, R_w, C)
    face_region.append(region)
  face_region = np.stack(face_region) # shape: (T, R_h, R_w, C)
  return face_region

def align_face(face_region):
  # Align face region by landmarks and expressions
  aligned_face_region = []
  for region in face_region:
    landmarks = face_aligner.get_landmarks(region)[0] # shape: (68, 2)
    mean_landmarks = np.mean(landmarks,axis=0) # shape: (2,)
    std_landmarks = np.std(landmarks,axis=0) # shape: (2,)
    normalized_landmarks = (landmarks - mean_landmarks) / std_landmarks # shape: (68, 2)
    aligned_region = cv2.warpAffine(region,normalized_landmarks,(H,W)) # shape: (H,W,C)
    aligned_face_region.append(aligned_region)
  aligned_face_region = np.stack(aligned_face_region) # shape: (T, H, W, C)
  return aligned_face_region

def transfer_style(face_region, style_region):
  # Transfer style of source face region to target face region
  style_transfer_region = []
  for i in range(len(face_region)):
    input_face = face_region[i] # shape: (H, W, C)
    input_style = style_region[i] # shape: (H, W, C)
    input_face = torch.from_numpy(input_face).permute(2,0,1).unsqueeze(0) # shape: (1, C, H, W)
    input_style = torch.from_numpy(input_style).permute(2,0,1).unsqueeze(0) # shape: (1, C, H, W)
    output_face = CycleGAN(input_face,input_style) # shape: (1, C, H, W)
    output_face = output_face.squeeze(0).permute(1,2,0).numpy() # shape: (H, W, C)
    style_transfer_region.append(output_face)
  style_transfer_region = np.stack(style_transfer_region) # shape: (T, H, W, C)
  return style_transfer_region

def blend_face(appearance_image, face_region):
  # Blend appearance image with synthesized face region
  blended_image = []
  for i in range(len(appearance_image)):
    input_image = appearance_image[i] # shape: (C, H, W)
    input_face = face_region[i] # shape: (H, W, C)
    input_image = input_image.permute(1,2,0).numpy() # shape: (H,W,C)
    output_image = poissonblending.blend(input_image,input_face) # shape: (H,W,C)
    blended_image.append(output_image)
  blended_image = np.stack(blended_image) # shape: (T-1,H,W,C)
  return blended_image

# Main function
def motion_transfer(S,T):
  # Motion transfer from source video S to target video T
  P_S = extract_pose(S) # pose tensor of source video
  P_T = extract_pose(T) # pose tensor of target video
  P_T_norm = spatial_transformer(P_T,P_S) # normalized pose tensor of target video
  V = generate_video(P_T_norm) # appearance image of target video
  F_S = extract_face(S) # face region of source video
  F_T = extract_face(T) # face region of target video
  F_S_align = align_face(F_S) # aligned face region of source video
  F_T_align = align_face(F_T) # aligned face region of target video
  F_T_style = transfer_style(F_T_align,F_S_align) # style transferred face region of target video
  V_face = blend_face(V,F_T_style) # blended appearance image with synthesized face
  V = V_face # motion transfer video with realistic face
  return V
```