---
title: 2212.09262v2 Out-of-domain GAN inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation
date: 2022-12-10
---

# [Out-of-domain GAN inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation](http://arxiv.org/abs/2212.09262v2)

authors: Xin Yang, Xiaogang Xu, Yingcong Chen


## What, Why and How

[1]: https://arxiv.org/abs/2212.09262 "[2212.09262] Out-of-domain GAN inversion via Invertibility ... - arXiv.org"
[2]: https://arxiv.org/pdf/2212.09262.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2212.09262v2 "[2212.09262v2] Out-of-domain GAN inversion via Invertibility ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel framework for out-of-domain GAN inversion and manipulation of human face images. It introduces a new module to decompose the input images into in-domain and out-of-domain partitions with invertibility masks, and blends the out-of-domain regions from the input image with the in-domain GAN inversion results to enhance fidelity.
- **Why**: The paper aims to address the challenge of reconstructing and editing out-of-domain areas (e.g., background, accessories) in the image that are beyond the generation ability of the pre-trained GAN model. Existing methods either ignore or distort these areas, resulting in low-quality and unrealistic outputs.
- **How**: The paper leverages a spatial alignment module and an invertibility detector module to iteratively align the generated features to the input geometry and reduce the reconstruction error in the in-domain regions, while precisely predicting the out-of-domain regions with invertibility masks. Then, it uses a blending module to fuse the out-of-domain regions from the input image with the in-domain GAN inversion results. The paper evaluates its method on various tasks such as identity preservation, attribute manipulation, and image restoration, and demonstrates its superiority over existing methods in terms of fidelity and realism.


## Main Contributions

[1]: https://arxiv.org/abs/2212.09262 "[2212.09262] Out-of-domain GAN inversion via Invertibility ... - arXiv.org"
[2]: https://arxiv.org/pdf/2212.09262.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2212.09262v2 "[2212.09262v2] Out-of-domain GAN inversion via Invertibility ..."

According to the paper[^1^][1], the main contributions are:

- **A novel framework for out-of-domain GAN inversion and manipulation of human face images** that decomposes the input images into in-domain and out-of-domain partitions with invertibility masks and blends them to enhance fidelity.
- **A new invertibility detector module** that predicts the out-of-domain regions more accurately by learning with a spatial alignment module that reduces the reconstruction error in the in-domain regions.
- **A new blending module** that fuses the out-of-domain regions from the input image with the in-domain GAN inversion results using a soft mask and a gradient-based refinement.
- **Extensive experiments** that demonstrate the superiority of the proposed method over existing methods in terms of fidelity and realism on various tasks such as identity preservation, attribute manipulation, and image restoration.

## Method Summary

[1]: https://arxiv.org/abs/2212.09262 "[2212.09262] Out-of-domain GAN inversion via Invertibility ... - arXiv.org"
[2]: https://arxiv.org/pdf/2212.09262.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2212.09262v2 "[2212.09262v2] Out-of-domain GAN inversion via Invertibility ..."

Here is a summary of the method section of the paper:

- The paper proposes a novel framework for out-of-domain GAN inversion and manipulation of human face images, which consists of three main modules: an invertibility detector module, a spatial alignment module, and a blending module.
- The invertibility detector module aims to decompose the input image into in-domain and out-of-domain partitions with invertibility masks. It takes the input image and the latent code as inputs, and outputs a soft mask that indicates the out-of-domain regions. The mask is learned by minimizing the reconstruction error between the input image and the generated image in the out-of-domain regions, while maximizing it in the in-domain regions.
- The spatial alignment module aims to align the generated features to the input geometry and reduce the reconstruction error in the in-domain regions. It takes the input image and the latent code as inputs, and outputs a warped latent code that preserves the spatial structure of the input image. The warped latent code is learned by minimizing a perceptual loss between the input image and the generated image in the in-domain regions, while ignoring the out-of-domain regions.
- The blending module aims to fuse the out-of-domain regions from the input image with the in-domain GAN inversion results to enhance fidelity. It takes the input image, the generated image, and the invertibility mask as inputs, and outputs a blended image that combines the best of both worlds. The blended image is refined by minimizing a gradient loss between the input image and the blended image in the out-of-domain regions, while preserving the in-domain regions.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a human face image x
# Output: a photo-realistic GAN inversion and manipulation result y

# Pre-trained GAN model
G = StyleGAN2()

# Encoder network
E = Encoder()

# Invertibility detector network
D = Detector()

# Spatial alignment network
A = Aligner()

# Blending network
B = Blender()

# Latent code initialization
z = E(x)

# Iterative optimization
for i in range(num_iterations):

  # Generate image from latent code
  x_hat = G(z)

  # Predict invertibility mask
  m = D(x, z)

  # Align latent code to input geometry
  z = A(x, z, m)

  # Blend out-of-domain regions from input image
  y = B(x, x_hat, m)

# Return the final result
return y
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np
import cv2

# Input: a human face image x
# Output: a photo-realistic GAN inversion and manipulation result y

# Pre-trained GAN model
G = StyleGAN2(pretrained=True)

# Encoder network
E = Encoder(input_channels=3, output_channels=512, num_layers=5)

# Invertibility detector network
D = Detector(input_channels=515, output_channels=1, num_layers=4)

# Spatial alignment network
A = Aligner(input_channels=515, output_channels=512, num_layers=3)

# Blending network
B = Blender(input_channels=7, output_channels=3, num_layers=4)

# Loss functions
L_rec = torch.nn.L1Loss() # reconstruction loss
L_per = torch.nn.MSELoss() # perceptual loss
L_gra = torch.nn.L1Loss() # gradient loss

# Optimizer
optimizer = torch.optim.Adam(params=[E.parameters(), D.parameters(), A.parameters(), B.parameters()], lr=0.0001)

# Latent code initialization
z = E(x)

# Iterative optimization
for i in range(num_iterations):

  # Generate image from latent code
  x_hat = G(z)

  # Predict invertibility mask
  m = D(torch.cat([x, z], dim=1))

  # Align latent code to input geometry
  z = A(torch.cat([x, z], dim=1), m)

  # Blend out-of-domain regions from input image
  y = B(torch.cat([x, x_hat, m], dim=1))

  # Compute losses
  loss_rec = L_rec(x_hat * m, x * m) # reconstruction loss in in-domain regions
  loss_per = L_per(vgg(x_hat * (1 - m)), vgg(x * (1 - m))) # perceptual loss in out-of-domain regions
  loss_gra = L_gra(sobel(y), sobel(x)) # gradient loss in out-of-domain regions

  # Total loss
  loss = loss_rec + loss_per + loss_gra

  # Backpropagation and update parameters
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

# Return the final result
return y

# Helper functions

def vgg(x):
  # Extract features from VGG19 network pretrained on ImageNet
  model = torchvision.models.vgg19(pretrained=True).features.eval()
  return model(x)

def sobel(x):
  # Apply Sobel filter to compute image gradients
  kernel_x = np.array([[1,0,-1],[2,0,-2],[1,0,-1]])
  kernel_y = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])
  grad_x = cv2.filter2D(x, -1, kernel_x)
  grad_y = cv2.filter2D(x, -1, kernel_y)
  return torch.sqrt(grad_x**2 + grad_y**2)
```