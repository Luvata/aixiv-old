---
title: 2202.05822v2 CLIPasso  Semantically-Aware Object Sketching
date: 2022-02-06
---

# [CLIPasso: Semantically-Aware Object Sketching](http://arxiv.org/abs/2202.05822v2)

authors: Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, Ariel Shamir


## What, Why and How

[1]: https://arxiv.org/abs/2202.05822 "[2202.05822] CLIPasso: Semantically-Aware Object Sketching - arXiv.org"
[2]: https://arxiv.org/pdf/2202.05822.pdf "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2205.05822v2.pdf "arXiv:2205.05822v2 [math.NT] 17 May 2022"
[4]: https://arxiv-export1.library.cornell.edu/abs/2202.05822 "[2202.05822] CLIPasso: Semantically-Aware Object Sketching"

Here is a summary of the paper:

- What: The paper presents **CLIPasso**, an object sketching method that can achieve different levels of abstraction, guided by geometric and semantic simplifications.
- Why: The paper aims to address the challenge of creating abstract depictions of objects that require semantic understanding and prior knowledge of high-level concepts, which are difficult for machines to achieve.
- How: The paper leverages the ability of **CLIP** (Contrastive-Language-Image-Pretraining) to distill semantic concepts from sketches and images alike. The paper defines a sketch as a set of **Bezier curves** and uses a differentiable rasterizer to optimize the parameters of the curves directly with respect to a CLIP-based perceptual loss. The paper controls the abstraction degree by varying the number of strokes.

The summary is based on the abstract and introduction of the paper[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2202.05822 "[2202.05822] CLIPasso: Semantically-Aware Object Sketching - arXiv.org"
[2]: https://arxiv.org/pdf/2202.05822.pdf "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2205.05822v2.pdf "arXiv:2205.05822v2 [math.NT] 17 May 2022"
[4]: http://export.arxiv.org/abs/2210.05822v2 "[2210.05822v2] The Future of High Energy Physics Software and Computing"

According to the paper[^1^][2], the main contributions are:

- A novel sketch generation method that can produce abstract sketches of objects with varying levels of detail, guided by geometric and semantic simplifications.
- A novel use of CLIP as a perceptual loss function that can capture the semantic similarity between sketches and images, without requiring explicit sketch datasets for training.
- A novel sketch representation as a set of Bezier curves that can be optimized using a differentiable rasterizer and gradient descent, allowing for smooth and continuous strokes.
- A comprehensive evaluation of the proposed method on various datasets and tasks, demonstrating its ability to generate diverse and recognizable sketches that preserve the essential visual features of the subject drawn.

## Method Summary

[1]: https://arxiv.org/abs/2202.05822 "[2202.05822] CLIPasso: Semantically-Aware Object Sketching - arXiv.org"
[2]: https://arxiv.org/pdf/2202.05822.pdf "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2205.05822v2.pdf "arXiv:2205.05822v2 [math.NT] 17 May 2022"
[4]: http://export.arxiv.org/abs/2210.05822v2 "[2210.05822v2] The Future of High Energy Physics Software and Computing"

Here is a summary of the method section of the paper:

- The paper defines a sketch as a set of **Bezier curves** that can be parameterized by their control points and weights. Each curve is associated with a stroke width and a color. The paper uses a differentiable rasterizer to convert the curves into an image representation that can be compared with the input image.
- The paper uses **CLIP** as a perceptual loss function that measures the semantic similarity between the sketch image and the input image. CLIP is a pre-trained model that can encode both images and text into a common embedding space, where the cosine similarity reflects the semantic relatedness. The paper uses CLIP to encode the sketch image and a text description of the input image (e.g., "a horse") and computes the loss as the negative cosine similarity between them.
- The paper optimizes the sketch parameters using gradient descent with respect to the CLIP loss. The paper initializes the sketch parameters randomly and updates them iteratively until convergence or a maximum number of iterations. The paper controls the level of abstraction by varying the number of curves in the sketch, which affects the amount of detail and complexity that can be represented.
- The paper evaluates the proposed method on various datasets and tasks, such as sketching objects from different categories, sketching objects with different poses and viewpoints, sketching objects with different styles and levels of abstraction, and sketching objects given different text descriptions.

The summary is based on sections 3 and 4 of the paper[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the sketch as a set of Bezier curves
sketch = BezierCurveSet(num_curves, num_control_points)

# Define the text description of the input image
text = "a horse"

# Encode the text using CLIP
text_embedding = CLIP.encode_text(text)

# Define the CLIP loss function
def clip_loss(sketch, text_embedding):
  # Rasterize the sketch into an image
  sketch_image = rasterize(sketch)
  # Encode the sketch image using CLIP
  sketch_embedding = CLIP.encode_image(sketch_image)
  # Compute the cosine similarity between the embeddings
  similarity = cosine_similarity(sketch_embedding, text_embedding)
  # Return the negative similarity as the loss
  return -similarity

# Initialize the sketch parameters randomly
sketch.init_random()

# Optimize the sketch parameters using gradient descent
optimizer = GradientDescent(clip_loss, sketch.parameters)

# Iterate until convergence or maximum iterations
while not converged and iterations < max_iterations:
  # Update the sketch parameters using the optimizer
  sketch.parameters = optimizer.step()
  # Compute the current loss
  loss = clip_loss(sketch, text_embedding)
  # Check for convergence
  if loss < threshold:
    converged = True
  # Increment the iterations
  iterations += 1

# Return the final sketch
return sketch
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import clip

# Define the Bezier curve class
class BezierCurve:
  # Initialize the curve with control points and weights
  def __init__(self, control_points, weights):
    # Control points are a 2D array of shape (num_control_points, 2)
    self.control_points = control_points
    # Weights are a 1D array of shape (num_control_points,)
    self.weights = weights
  
  # Evaluate the curve at a given parameter value t
  def evaluate(self, t):
    # Use the de Casteljau algorithm to compute the point on the curve
    # https://en.wikipedia.org/wiki/De_Casteljau%27s_algorithm
    # Initialize the point array with the control points
    points = self.control_points.copy()
    # Initialize the weight array with the weights
    weights = self.weights.copy()
    # Loop over the degree of the curve
    for i in range(1, len(points)):
      # Loop over the remaining points
      for j in range(len(points) - i):
        # Interpolate the points using t and the weights
        points[j] = (1 - t) * points[j] * weights[j] + t * points[j + 1] * weights[j + 1]
        # Interpolate the weights using t
        weights[j] = (1 - t) * weights[j] + t * weights[j + 1]
    # Return the final point and weight
    return points[0], weights[0]

# Define the Bezier curve set class
class BezierCurveSet:
  # Initialize the set with a number of curves and control points per curve
  def __init__(self, num_curves, num_control_points):
    # Initialize an empty list of curves
    self.curves = []
    # Initialize an empty list of parameters
    self.parameters = []
    # Loop over the number of curves
    for i in range(num_curves):
      # Initialize the control points randomly in [0, 1] x [0, 1]
      control_points = np.random.rand(num_control_points, 2)
      # Initialize the weights randomly in [0, 1]
      weights = np.random.rand(num_control_points)
      # Create a Bezier curve object with the control points and weights
      curve = BezierCurve(control_points, weights)
      # Append the curve to the list of curves
      self.curves.append(curve)
      # Append the control points and weights to the list of parameters
      self.parameters.append(control_points)
      self.parameters.append(weights)
    # Convert the list of parameters to a 1D array
    self.parameters = np.concatenate(self.parameters)

  # Rasterize the set into an image of a given size
  def rasterize(self, size):
    # Create an empty image array of shape (size, size, 3)
    image = np.zeros((size, size, 3))
    # Loop over the curves in the set
    for curve in self.curves:
      # Sample a number of points on the curve using a fixed step size
      step_size = 0.01
      t_values = np.arange(0, 1 + step_size, step_size)
      points = [curve.evaluate(t)[0] for t in t_values]
      # Scale and round the points to fit the image size
      points = np.round(points * (size - 1)).astype(int)
      # Draw the points on the image using a fixed color and stroke width
      color = np.array([0, 0, 0]) # black color
      stroke_width = 3 # pixels
      for point in points:
        # Get the x and y coordinates of the point
        x, y = point[0], point[1]
        # Draw a circle around the point with the stroke width and color
        for i in range(-stroke_width, stroke_width + 1):
          for j in range(-stroke_width, stroke_width + 1):
            if i**2 + j**2 <= stroke_width**2: # inside the circle
              # Check if the coordinates are within the image bounds
              if 0 <= x + i < size and 0 <= y + j < size:
                # Set the pixel value to the color value
                image[x + i, y + j] = color 
    # Return the image array
    return image

# Define the text description of the input image
text = "a horse"

# Encode the text using CLIP
text_embedding = clip.encode_text(text)

# Define the CLIP loss function
def clip_loss(sketch, text_embedding):
  # Rasterize the sketch into an image
  sketch_image = sketch.rasterize(size=256)
  # Encode the sketch image using CLIP
  sketch_embedding = clip.encode_image(sketch_image)
  # Compute the cosine similarity between the embeddings
  similarity = torch.cosine_similarity(sketch_embedding, text_embedding)
  # Return the negative similarity as the loss
  return -similarity

# Initialize the sketch as a set of Bezier curves
sketch = BezierCurveSet(num_curves=10, num_control_points=4)

# Optimize the sketch parameters using gradient descent
optimizer = torch.optim.Adam([sketch.parameters], lr=0.01)

# Iterate until convergence or maximum iterations
converged = False
iterations = 0
max_iterations = 1000
threshold = -0.9 # a high similarity value
while not converged and iterations < max_iterations:
  # Zero the gradients
  optimizer.zero_grad()
  # Compute the current loss
  loss = clip_loss(sketch, text_embedding)
  # Backpropagate the loss
  loss.backward()
  # Update the sketch parameters using the optimizer
  optimizer.step()
  # Check for convergence
  if loss < threshold:
    converged = True
  # Increment the iterations
  iterations += 1

# Return the final sketch
return sketch
```