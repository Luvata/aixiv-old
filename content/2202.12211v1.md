---
title: 2202.12211v1 Self-Distilled StyleGAN  Towards Generation from Internet Photos
date: 2022-02-13
---

# [Self-Distilled StyleGAN: Towards Generation from Internet Photos](http://arxiv.org/abs/2202.12211v1)

authors: Ron Mokady, Michal Yarom, Omer Tov, Oran Lang, Daniel Cohen-Or, Tali Dekel, Michal Irani, Inbar Mosseri


## What, Why and How

[1]: https://arxiv.org/abs/2202.12211v1 "[2202.12211v1] Self-Distilled StyleGAN: Towards Generation from ..."
[2]: https://arxiv.org/pdf/2202.12211.pdf "arXiv:2202.12211v1 [cs.CV] 24 Feb 2022"
[3]: http://export.arxiv.org/abs/2304.12211v1 "[2304.12211v1] Perfect and almost perfect homogeneous polytopes"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a **StyleGAN-based self-distillation approach** to generate high-quality and diverse images from **raw uncurated images collected from the Internet**.
- **Why**: The paper aims to overcome the challenges of training StyleGAN on such image collections, which are: (i) they contain many **outlier images**, and (ii) they are characterized by a **multi-modal distribution**. These challenges result in degraded image synthesis quality and reduced diversity of the generated images.
- **How**: The paper introduces two main components of the self-distillation approach: (i) A **generative-based self-filtering** of the dataset to eliminate outlier images, in order to generate an adequate training set, and (ii) **Perceptual clustering** of the generated images to detect the inherent data modalities, which are then employed to improve StyleGAN's "truncation trick" in the image synthesis process. The paper demonstrates the effectiveness of the approach on new challenging and diverse domains collected from the Internet.

## Main Contributions

[1]: https://arxiv.org/abs/2202.12211v1 "[2202.12211v1] Self-Distilled StyleGAN: Towards Generation from ..."
[2]: https://arxiv.org/pdf/2202.12211.pdf "arXiv:2202.12211v1 [cs.CV] 24 Feb 2022"
[3]: http://export.arxiv.org/abs/2304.12211v1 "[2304.12211v1] Perfect and almost perfect homogeneous polytopes"

The paper[^1^][1] claims the following contributions:

- It introduces a **novel self-distillation framework** for StyleGAN that enables high-quality and diverse image generation from **raw uncurated Internet datasets**.
- It proposes a **generative-based self-filtering technique** that leverages StyleGAN's latent space to identify and remove outlier images from the training set, resulting in a cleaner and more consistent dataset.
- It devises a **perceptual clustering method** that discovers the inherent data modalities in StyleGAN's latent space, and uses them to improve the "truncation trick" for image synthesis, reducing visual artifacts while preserving better the diversity of the samples images.
- It demonstrates the effectiveness of the proposed approach on **new challenging and diverse domains** collected from the Internet, such as parrots, bikes, lions, etc., and provides new datasets and pre-trained models for these domains.

## Method Summary

[1]: https://arxiv.org/abs/2202.12211v1 "[2202.12211v1] Self-Distilled StyleGAN: Towards Generation from ..."
[2]: https://arxiv.org/pdf/2202.12211.pdf "arXiv:2202.12211v1 [cs.CV] 24 Feb 2022"
[3]: http://export.arxiv.org/abs/2304.12211v1 "[2304.12211v1] Perfect and almost perfect homogeneous polytopes"

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the **StyleGAN-based self-distillation framework** in three steps: (i) training StyleGAN on the raw Internet dataset, (ii) applying generative-based self-filtering to obtain a distilled dataset, and (iii) applying perceptual clustering to improve the truncation trick for image synthesis.
- The paper explains the **generative-based self-filtering technique** as follows: (i) sampling images from StyleGAN's latent space using a random seed, (ii) finding the nearest neighbors of the sampled images in the raw dataset using a perceptual distance metric, (iii) removing the images that are farthest from their nearest neighbors, and (iv) repeating this process until a desired dataset size is reached.
- The paper details the **perceptual clustering method** as follows: (i) sampling images from StyleGAN's latent space using a random seed, (ii) applying a pre-trained feature extractor to obtain feature vectors for the sampled images, (iii) applying k-means clustering to group the feature vectors into clusters, and (iv) assigning each cluster a different truncation value based on its visual quality and diversity.

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2202.12211v1 "[2202.12211v1] Self-Distilled StyleGAN: Towards Generation from ..."
[2]: https://arxiv.org/pdf/2202.12211.pdf "arXiv:2202.12211v1 [cs.CV] 24 Feb 2022"
[3]: http://export.arxiv.org/abs/2304.12211v1 "[2304.12211v1] Perfect and almost perfect homogeneous polytopes"

Here is the high-level pseudo code for the paper[^1^][1]:

```python
# Input: a raw Internet dataset D
# Output: a StyleGAN model G trained on a distilled dataset D' and a truncation value t for each cluster C

# Step 1: Train StyleGAN on the raw dataset D
G = train_stylegan(D)

# Step 2: Apply generative-based self-filtering to obtain a distilled dataset D'
D' = []
while len(D') < desired_size:
  # Sample an image x from G's latent space using a random seed s
  x = G(s)
  # Find the nearest neighbor y of x in D using a perceptual distance metric d
  y = find_nearest_neighbor(x, D, d)
  # If y is close enough to x, add it to D'
  if d(x, y) < threshold:
    D'.append(y)
  # Remove y from D
  D.remove(y)

# Step 3: Apply perceptual clustering to improve the truncation trick for image synthesis
# Sample a large number of images X from G's latent space using random seeds S
X = [G(s) for s in S]
# Apply a pre-trained feature extractor F to obtain feature vectors V for X
V = [F(x) for x in X]
# Apply k-means clustering to group V into k clusters C
C = kmeans(V, k)
# Assign each cluster C a different truncation value t based on its visual quality and diversity
for C in C:
  t = find_optimal_truncation(C)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2202.12211v1 "[2202.12211v1] Self-Distilled StyleGAN: Towards Generation from ..."
[2]: https://arxiv.org/pdf/2202.12211.pdf "arXiv:2202.12211v1 [cs.CV] 24 Feb 2022"
[3]: http://export.arxiv.org/abs/2304.12211v1 "[2304.12211v1] Perfect and almost perfect homogeneous polytopes"

Here is the detailed pseudo code for the paper[^1^][1]:

```python
# Input: a raw Internet dataset D
# Output: a StyleGAN model G trained on a distilled dataset D' and a truncation value t for each cluster C

# Step 1: Train StyleGAN on the raw dataset D
# Define the StyleGAN architecture and hyperparameters
G = StyleGAN(
  resolution = 256,
  mapping_layers = 8,
  style_mixing_prob = 0.9,
  truncation_psi = 0.7,
  truncation_cutoff = 8,
  randomize_noise = True,
  fmap_base = 16384,
  fmap_decay = 1.0,
  fmap_min = 1,
  fmap_max = 512
)

# Define the loss function and optimizer
loss = non_saturating_logistic_loss()
optimizer = Adam(learning_rate = 0.0015, beta1 = 0.0, beta2 = 0.99, epsilon = 1e-8)

# Define the number of training iterations and batch size
iterations = 25000
batch_size = 32

# Train StyleGAN on D using progressive growing and experience replay
for i in range(iterations):
  # Update the resolution and learning rate based on the current iteration
  resolution, learning_rate = progressive_growing(i)
  
  # Sample a batch of real images from D at the current resolution
  real_images = sample(D, batch_size, resolution)
  
  # Sample a batch of latent vectors from a normal distribution
  latent_vectors = sample_normal(batch_size, G.latent_size)
  
  # Generate a batch of fake images from G using the latent vectors
  fake_images = G(latent_vectors)
  
  # Compute the discriminator outputs for the real and fake images
  real_scores = G.D(real_images)
  fake_scores = G.D(fake_images)
  
  # Compute the generator and discriminator losses using the non-saturating logistic loss function
  gen_loss = loss.gen(fake_scores)
  dis_loss = loss.dis(real_scores, fake_scores)
  
  # Update the generator and discriminator parameters using the Adam optimizer
  optimizer.minimize(gen_loss, G.G.trainable_variables)
  optimizer.minimize(dis_loss, G.D.trainable_variables)
  
  # Optionally apply experience replay to improve stability
  if use_experience_replay():
    # Sample a batch of previous fake images from a replay buffer
    replay_images = sample(G.replay_buffer, batch_size, resolution)
    
    # Compute the discriminator outputs for the replay images
    replay_scores = G.D(replay_images)
    
    # Compute the discriminator loss using the non-saturating logistic loss function
    dis_loss_replay = loss.dis(real_scores, replay_scores)
    
    # Update the discriminator parameters using the Adam optimizer
    optimizer.minimize(dis_loss_replay, G.D.trainable_variables)
    
    # Add the current fake images to the replay buffer
    G.replay_buffer.append(fake_images)

# Step 2: Apply generative-based self-filtering to obtain a distilled dataset D'
# Define the perceptual distance metric d using a pre-trained VGG network
d = perceptual_distance(VGG)

# Define the threshold for filtering out outliers
threshold = 0.5

# Initialize an empty list for the distilled dataset D'
D' = []

# Repeat until D' reaches a desired size
while len(D') < desired_size:
  
  # Sample an image x from G's latent space using a random seed s
  s = sample_seed()
  x = G(s)
  
  # Find the nearest neighbor y of x in D using d as the distance metric
  y = find_nearest_neighbor(x, D, d)
  
  # If y is close enough to x, add it to D'
  if d(x, y) < threshold:
    D'.append(y)
  
  # Remove y from D
  D.remove(y)

# Step 3: Apply perceptual clustering to improve the truncation trick for image synthesis
# Define the feature extractor F using a pre-trained ResNet network
F = feature_extractor(ResNet)

# Define the number of clusters k for k-means clustering
k = 10

# Sample a large number of images X from G's latent space using random seeds S
S = sample_seeds(large_number)
X = [G(s) for s in S]

# Apply F to obtain feature vectors V for X
V = [F(x) for x in X]

# Apply k-means clustering to group V into k clusters C
C = kmeans(V, k)

# Assign each cluster C a different truncation value t based on its visual quality and diversity
for C in C:
  # Compute the average perceptual distance d within the cluster C
  d_avg = average_distance(C, d)
  
  # Compute the standard deviation of the perceptual distance d within the cluster C
  d_std = standard_deviation(C, d)
  
  # Find the optimal truncation value t for the cluster C that minimizes the trade-off between quality and diversity
  t = find_optimal_truncation(C, d_avg, d_std)
```