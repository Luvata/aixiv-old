---
title: 2203.15320v1 Dressing in the Wild by Watching Dance Videos
date: 2022-03-16
---

# [Dressing in the Wild by Watching Dance Videos](http://arxiv.org/abs/2203.15320v1)

authors: Xin Dong, Fuwei Zhao, Zhenyu Xie, Xijin Zhang, Daniel K. Du, Min Zheng, Xiang Long, Xiaodan Liang, Jianchao Yang


## What, Why and How

[1]: https://arxiv.org/abs/2203.15320v1 "[2203.15320v1] Dressing in the Wild by Watching Dance Videos - arXiv.org"
[2]: https://arxiv.org/pdf/2203.15320v1 ".sysu.edu.cn g@bytedance.com arXiv:2203.15320v1 [cs.CV] 29 Mar 2022"
[3]: http://export.arxiv.org/abs/2112.15320v1 "[2112.15320v1] InverseMV: Composing Piano Scores with a Convolutional ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel generative network called wFlow that can perform realistic garment transfer for in-the-wild images, such as those with loose garments, challenging poses, and cluttered backgrounds. The paper also introduces a new large-scale video dataset named Dance50k that contains diverse garments and dancing poses for self-supervised training and online cycle optimization.
- **Why**: The paper aims to address the limitations of existing garment transfer methods that overlook the in-the-wild imagery and require expensive paired datasets. The paper also seeks to boost the applicability and commercial potential of human-centric image generation.
- **How**: The paper combines the advantages of pixel flow and vertex flow to handle loose garments and hard poses respectively. The paper also leverages cross-frame consistency and cycle consistency to train the wFlow network on the Dance50k dataset without paired supervision. The paper evaluates the proposed method on several benchmarks and shows its superiority over state-of-the-art methods in terms of authenticity and naturalness.

## Main Contributions

The paper claims the following contributions:

- A novel generative network called wFlow that can effectively handle loose garments and challenging poses for in-the-wild garment transfer.
- A new large-scale video dataset named Dance50k that covers a wide variety of garments and dancing poses for self-supervised training and online cycle optimization.
- Extensive experiments that demonstrate the superiority of the proposed method over state-of-the-art methods in terms of authenticity and naturalness.

## Method Summary

[1]: https://arxiv.org/abs/2203.15320v1 "[2203.15320v1] Dressing in the Wild by Watching Dance Videos - arXiv.org"
[2]: https://arxiv.org/pdf/2203.15320v1 ".sysu.edu.cn g@bytedance.com arXiv:2203.15320v1 [cs.CV] 29 Mar 2022"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2303.15320v1 "[2303.15320v1] Noether's theorem and Lie symmetries for time-dependent ..."

Here is a summary of the method section of the paper:

- The paper proposes a novel generative network called **wFlow** that consists of three modules: a **flow estimation module**, a **flow fusion module**, and a **flow warping module**. The flow estimation module predicts two types of flows: **pixel flow** and **vertex flow**. Pixel flow is computed by a dense correspondence matching network that aligns the garment pixels between the source and the target person images. Vertex flow is computed by a sparse keypoint matching network that aligns the garment vertices between the source and the target person meshes. The flow fusion module fuses the pixel flow and the vertex flow by a weighted average based on their confidence scores. The flow warping module warps the source garment image to the target person image by applying the fused flow. The paper also introduces a **flow consistency loss** to enforce the consistency between the pixel flow and the vertex flow.
- The paper also introduces a new large-scale video dataset named **Dance50k** that contains 50,000 videos of people dancing with various garments and poses. The paper leverages the cross-frame consistency and cycle consistency to train the wFlow network on the Dance50k dataset without paired supervision. The paper also proposes an **online cycle optimization** technique that refines the generated results by minimizing the cycle reconstruction error between consecutive frames.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: source garment image I_s, source person mesh M_s, target person image I_t, target person mesh M_t
# Output: transferred garment image I_g

# Flow estimation module
pixel_flow = dense_correspondence_matching(I_s, I_t) # compute pixel flow by matching garment pixels
vertex_flow = sparse_keypoint_matching(M_s, M_t) # compute vertex flow by matching garment vertices

# Flow fusion module
pixel_confidence = compute_pixel_confidence(pixel_flow) # compute pixel flow confidence score
vertex_confidence = compute_vertex_confidence(vertex_flow) # compute vertex flow confidence score
fused_flow = weighted_average(pixel_flow, vertex_flow, pixel_confidence, vertex_confidence) # fuse pixel flow and vertex flow

# Flow warping module
I_g = warp(I_s, fused_flow) # warp source garment image to target person image

# Flow consistency loss
L_consistency = compute_flow_consistency_loss(pixel_flow, vertex_flow) # enforce consistency between pixel flow and vertex flow

# Online cycle optimization
I_g_next = warp(I_g, fused_flow_next) # warp generated image to next frame
I_g_cycle = warp(I_g_next, fused_flow_prev) # warp back to current frame
L_cycle = compute_cycle_reconstruction_loss(I_g, I_g_cycle) # minimize cycle reconstruction error
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: source garment image I_s, source person mesh M_s, target person image I_t, target person mesh M_t
# Output: transferred garment image I_g

# Define network architectures and hyperparameters
dense_correspondence_network = DenseCorrespondenceNetwork() # network for dense correspondence matching
sparse_keypoint_network = SparseKeypointNetwork() # network for sparse keypoint matching
warping_network = WarpingNetwork() # network for flow warping
optimizer = AdamOptimizer() # optimizer for network parameters
lambda_consistency = 0.1 # weight for flow consistency loss
lambda_cycle = 0.01 # weight for cycle reconstruction loss

# Flow estimation module
pixel_flow = dense_correspondence_network(I_s, I_t) # compute pixel flow by matching garment pixels with a convolutional neural network
vertex_flow = sparse_keypoint_network(M_s, M_t) # compute vertex flow by matching garment vertices with a graph neural network

# Flow fusion module
pixel_confidence = softmax(pixel_flow) # compute pixel flow confidence score by applying softmax function
vertex_confidence = softmax(vertex_flow) # compute vertex flow confidence score by applying softmax function
fused_flow = pixel_confidence * pixel_flow + vertex_confidence * vertex_flow # fuse pixel flow and vertex flow by a weighted average

# Flow warping module
I_g = warping_network(I_s, fused_flow) # warp source garment image to target person image by applying a differentiable bilinear sampling

# Flow consistency loss
L_consistency = mean_squared_error(pixel_flow, vertex_flow) # enforce consistency between pixel flow and vertex flow by computing the mean squared error

# Online cycle optimization
I_g_next = warping_network(I_g, fused_flow_next) # warp generated image to next frame by applying the fused flow of the next frame pair
I_g_cycle = warping_network(I_g_next, fused_flow_prev) # warp back to current frame by applying the inverse of the fused flow of the current frame pair
L_cycle = mean_absolute_error(I_g, I_g_cycle) # minimize cycle reconstruction error by computing the mean absolute error

# Total loss and optimization
L_total = L_consistency * lambda_consistency + L_cycle * lambda_cycle # compute the total loss as a weighted sum of the consistency loss and the cycle loss
optimizer.minimize(L_total) # update the network parameters by minimizing the total loss using Adam optimizer
```