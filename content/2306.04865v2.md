---
title: 2306.04865v2 MyStyle++  A Controllable Personalized Generative Prior
date: 2023-06-05
---

# [MyStyle++: A Controllable Personalized Generative Prior](http://arxiv.org/abs/2306.04865v2)

authors: Libing Zeng, Lele Chen, Yi Xu, Nima Kalantari


## What, Why and How

[1]: https://arxiv.org/pdf/2306.04965v1.pdf "a arXiv:2306.04965v1 [cs.CR] 8 Jun 2023"
[2]: https://arxiv.org/abs/2306.04637 "[2306.04637] Transformers as Statisticians: Provable In-Context ..."
[3]: https://browse.arxiv.org/abs/2306.04865 "[2306.04865] MyStyle++: A Controllable Personalized Generative Prior"

Here is a summary of the paper at [^1^][3] in terms of What, Why, and How:

- What: The paper proposes an approach to obtain a personalized generative prior with explicit control over a set of attributes. The approach builds upon MyStyle, a method that tunes the weights of a pre-trained StyleGAN face generator on a few images of an individual. The paper introduces MyStyle++, which extends MyStyle by adding attribute classifiers and latent space regularization to enable attribute manipulation and interpolation for the target individual.
- Why: The paper aims to address the limitations of existing methods for personalized face generation, such as lack of diversity, low fidelity, and limited control over attributes. The paper claims that MyStyle++ can generate diverse and realistic images of the target individual with fine-grained control over attributes such as age, gender, expression, pose, and hairstyle.
- How: The paper describes the technical details of MyStyle++, which consists of three main components: 1) a pre-trained StyleGAN generator that maps latent vectors to face images; 2) a set of attribute classifiers that predict the attribute values of face images; and 3) a latent space regularizer that encourages smooth transitions between different attribute values. The paper also presents the training procedure of MyStyle++, which involves tuning the weights of the generator on a few images of the target individual, while optimizing the latent vectors to match the desired attributes and minimizing the regularization loss. The paper evaluates MyStyle++ on several datasets and compares it with existing methods on various metrics such as diversity, fidelity, attribute accuracy, and interpolation quality.


## Main Contributions

According to the paper, the main contributions are:

- The paper introduces MyStyle++, a novel approach to obtain a personalized generative prior with explicit control over a set of attributes, which extends MyStyle by adding attribute classifiers and latent space regularization.
- The paper demonstrates that MyStyle++ can generate diverse and realistic images of the target individual with fine-grained control over attributes such as age, gender, expression, pose, and hairstyle, and can also perform attribute interpolation and manipulation.
- The paper provides extensive experiments and comparisons with existing methods on several datasets and metrics, showing the superiority of MyStyle++ in terms of diversity, fidelity, attribute accuracy, and interpolation quality.


## Method Summary

Here is a summary of the method section of the paper:

- The paper describes the technical details of MyStyle++, which consists of three main components: 1) a pre-trained StyleGAN generator that maps latent vectors to face images; 2) a set of attribute classifiers that predict the attribute values of face images; and 3) a latent space regularizer that encourages smooth transitions between different attribute values.
- The paper also presents the training procedure of MyStyle++, which involves tuning the weights of the generator on a few images of the target individual, while optimizing the latent vectors to match the desired attributes and minimizing the regularization loss. The paper uses gradient-based optimization methods such as Adam and L-BFGS for this purpose.
- The paper explains how MyStyle++ can perform attribute manipulation and interpolation for the target individual by modifying the latent vectors according to the attribute classifiers and the latent space regularizer. The paper also shows how MyStyle++ can generate diverse images of the target individual by sampling different latent vectors from a Gaussian distribution.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the pre-trained StyleGAN generator G
# Define the attribute classifiers C_1, C_2, ..., C_k
# Define the latent space regularizer R

# Input: a few images of the target individual X = {x_1, x_2, ..., x_n}
# Output: a personalized generative prior P

# Step 1: Tune the weights of G on X
# Initialize the weights of G as w_0
# Initialize the latent vectors of X as z_0 = {z_1, z_2, ..., z_n}
# For each iteration t:
  # Compute the reconstruction loss L_r = sum(||x_i - G(z_i; w_t)||^2 for i in 1..n)
  # Update the weights of G by w_t+1 = w_t - alpha * grad(L_r, w_t)
  # Update the latent vectors of X by z_t+1 = z_t - beta * grad(L_r, z_t)
# Return the final weights of G as w*

# Step 2: Perform attribute manipulation and interpolation for the target individual
# Input: a desired attribute vector a = (a_1, a_2, ..., a_k)
# Output: an image of the target individual with attribute vector a
# Initialize the latent vector of the image as z
# For each iteration t:
  # Compute the attribute loss L_a = sum(||C_i(G(z; w*)) - a_i||^2 for i in 1..k)
  # Compute the regularization loss L_r = R(z)
  # Compute the total loss L = L_a + lambda * L_r
  # Update the latent vector of the image by z_t+1 = z_t - gamma * grad(L, z_t)
# Return the final image as G(z*; w*)

# Step 3: Generate diverse images of the target individual
# Input: a number of images to generate n
# Output: a set of images of the target individual Y = {y_1, y_2, ..., y_n}
# For each i in 1..n:
  # Sample a latent vector from a Gaussian distribution z ~ N(0, I)
  # Generate an image by y_i = G(z; w*)
# Return Y
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing
import numpy as np # for numerical operations
import stylegan2_pytorch # for pre-trained StyleGAN generator
import torch.nn as nn # for neural network modules
import torch.optim as optim # for optimization algorithms

# Define the pre-trained StyleGAN generator G
G = stylegan2_pytorch.load_pretrained('ffhq')

# Define the attribute classifiers C_1, C_2, ..., C_k
# For simplicity, we assume they are binary classifiers with sigmoid activation
# You can use any other classifier models and attributes as you wish
C_1 = nn.Sequential(nn.Conv2d(3, 64, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(128*6*6, 1), nn.Sigmoid()) # classifier for age (0: young, 1: old)
C_2 = nn.Sequential(nn.Conv2d(3, 64, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(128*6*6, 1), nn.Sigmoid()) # classifier for gender (0: male, 1: female)
C_3 = nn.Sequential(nn.Conv2d(3, 64, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(128*6*6, 1), nn.Sigmoid()) # classifier for expression (0: neutral, 1: smiling)
C_4 = nn.Sequential(nn.Conv2d(3, 64, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(128*6*6, 1), nn.Sigmoid()) # classifier for pose (0: frontal, 1: profile)
C_5 = nn.Sequential(nn.Conv2d(3, 64, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(128*6*6, 1), nn.Sigmoid()) # classifier for hairstyle (0: short, 1: long)

# Define the latent space regularizer R
# For simplicity, we use the L2 norm of the latent vector as the regularizer
# You can use any other regularizer functions as you wish
R = lambda z: torch.norm(z)**2

# Input: a few images of the target individual X = {x_1, x_2, ..., x_n}
# Output: a personalized generative prior P

# Step 1: Tune the weights of G on X
# Initialize the weights of G as w_0
w_0 = G.state_dict()
# Initialize the latent vectors of X as z_0 = {z_1, z_2, ..., z_n}
z_0 = torch.randn(n, G.latent_dim) # n random latent vectors of size G.latent_dim
# Define the number of iterations T and the learning rates alpha and beta
T = 100 # you can adjust this value as you wish
alpha = 0.01 # you can adjust this value as you wish
beta = 0.01 # you can adjust this value as you wish
# Define the optimizers for w and z using Adam algorithm
w_optimizer = optim.Adam(G.parameters(), lr=alpha)
z_optimizer = optim.Adam(z_0.parameters(), lr=beta)
# For each iteration t:
for t in range(T):
  # Compute the reconstruction loss L_r = sum(||x_i - G(z_i; w_t)||^2 for i in 1..n)
  L_r = torch.sum(torch.square(X - G(z_t; w_t)))
  # Update the weights of G by w_t+1 = w_t - alpha * grad(L_r, w_t)
  w_optimizer.zero_grad() # clear the previous gradients
  L_r.backward() # compute the gradients of L_r with respect to w_t
  w_optimizer.step() # update w_t using the gradients
  # Update the latent vectors of X by z_t+1 = z_t - beta * grad(L_r, z_t)
  z_optimizer.zero_grad() # clear the previous gradients
  L_r.backward() # compute the gradients of L_r with respect to z_t
  z_optimizer.step() # update z_t using the gradients
# Return the final weights of G as w*
w* = G.state_dict()

# Step 2: Perform attribute manipulation and interpolation for the target individual
# Input: a desired attribute vector a = (a_1, a_2, ..., a_k)
# Output: an image of the target individual with attribute vector a
# Initialize the latent vector of the image as z
z = torch.randn(1, G.latent_dim) # a random latent vector of size G.latent_dim
# Define the number of iterations T and the learning rate gamma and the regularization coefficient lambda
T = 100 # you can adjust this value as you wish
gamma = 0.01 # you can adjust this value as you wish
lambda = 0.01 # you can adjust this value as you wish
# Define the optimizer for z using L-BFGS algorithm
z_optimizer = optim.LBFGS(z.parameters(), lr=gamma)
# For each iteration t:
for t in range(T):
  # Compute the attribute loss L_a = sum(||C_i(G(z; w*)) - a_i||^2 for i in 1..k)
  L_a = torch.sum(torch.square(C_1(G(z; w*)) - a_1) + torch.square(C_2(G(z; w*)) - a_2) + torch.square(C_3(G(z; w*)) - a_3) + torch.square(C_4(G(z; w*)) - a_4) + torch.square(C_5(G(z; w*)) - a_5))
  # Compute the regularization loss L_r = R(z)
  L_r = R(z)
  # Compute the total loss L = L_a + lambda * L_r
  L = L_a + lambda * L_r
  # Update the latent vector of the image by z_t+1 = z_t - gamma * grad(L, z_t)
  z_optimizer.zero_grad() # clear the previous gradients
  L.backward() # compute the gradients of L with respect to z_t
  z_optimizer.step() # update z_t using the gradients
# Return the final image as G(z*; w*)
y = G(z*; w*)

# Step 3: Generate diverse images of the target individual
# Input: a number of images to generate n
# Output: a set of images of the target individual Y = {y_1, y_2, ..., y_n}
# Define an empty set Y
Y = set()
# For each i in 1..n:
for i in range(n):
  # Sample a latent vector from a Gaussian distribution z ~ N(0, I)
  z = torch.randn(1, G.latent_dim) # a random latent vector of size G.latent_dim
  # Generate an image by y_i = G(z; w*)
  y_i = G(z; w*)
  # Add y_i to Y
  Y.add(y_i)
# Return Y
return Y
```