---
title: 2203.17274v2 Exploring Visual Prompts for Adapting Large-Scale Models
date: 2022-03-18
---

# [Exploring Visual Prompts for Adapting Large-Scale Models](http://arxiv.org/abs/2203.17274v2)

authors: Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, Phillip Isola


## What, Why and How

[1]: https://arxiv.org/abs/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models"
[2]: https://arxiv.org/pdf/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models - arXiv.org"
[3]: https://arxiv.org/pdf/2203.17274v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper investigates the efficacy of visual prompting to adapt large-scale models in vision. Visual prompting is a method of modifying pixel space to steer a frozen model to perform a new task.
- **Why**: The paper aims to provide a new perspective on adapting pre-trained models in vision, which is usually done by fine-tuning or linear probing. Visual prompting is inspired by the recent success of prompting methods in NLP, where a frozen language model directly adapts to a new task without updating any parameters.
- **How**: The paper follows the recent approach from prompt tuning and adversarial reprogramming, where a single image perturbation is learned such that a frozen model prompted with this perturbation performs a new task. The paper focuses on CLIP as the pre-trained model and evaluates visual prompting on various downstream tasks and datasets. The paper also analyzes the properties of the downstream dataset, prompt design, and output transformation in regard to adaptation performance.



## Main Contributions

[1]: https://arxiv.org/abs/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models"
[2]: https://arxiv.org/pdf/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models - arXiv.org"
[3]: https://arxiv.org/pdf/2203.17274v2.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- They demonstrate that visual prompting is particularly effective for CLIP and robust to distribution shift, achieving performance competitive with standard linear probes on various downstream tasks and datasets.
- They analyze the properties of the downstream dataset, prompt design, and output transformation in regard to adaptation performance, and provide insights and guidelines for visual prompting.
- They introduce a new dataset of 1000 natural images with 1000 corresponding prompts for CLIP, which can be used to evaluate visual prompting methods and generate diverse visual content.


## Method Summary

[1]: https://arxiv.org/abs/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models"
[2]: https://arxiv.org/pdf/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models - arXiv.org"
[3]: https://arxiv.org/pdf/2203.17274v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][2]:

- The paper uses CLIP as the pre-trained model, which is a vision-language model that learns to associate images and text captions in a zero-shot manner.
- The paper defines visual prompting as a method of learning a single image perturbation that modifies the input image such that a frozen model prompted with this perturbation performs a new task. The perturbation can be either additive or multiplicative, and can be applied to the whole image or a local region.
- The paper formulates visual prompting as an optimization problem, where the objective is to minimize the cross-entropy loss between the model output and the ground-truth labels. The paper uses gradient-based methods to optimize the perturbation parameters.
- The paper introduces three types of output transformations to map the model output to the downstream task labels: label words, label embeddings, and label classifiers. The paper compares the performance of these transformations on different tasks and datasets.
- The paper evaluates visual prompting on various downstream tasks and datasets, such as image classification, object detection, semantic segmentation, and image captioning. The paper also compares visual prompting with fine-tuning and linear probing methods.


## Pseudo Code - High level

[1]: https://arxiv.org/abs/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models"
[2]: https://arxiv.org/pdf/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models - arXiv.org"
[3]: https://arxiv.org/pdf/2203.17274v2.pdf "arXiv.org e-Print archive"

Here is the high-level pseudo code for the paper[^1^][2]:

- Define CLIP as the pre-trained model, which takes an image and a text caption as input and outputs a similarity score between them
- Define visual prompting as a method of learning a single image perturbation P that modifies the input image I such that CLIP(I + P, T) is high, where T is the target text caption for the new task
- Define the perturbation P as either additive or multiplicative, and either global or local
- Define the output transformation F as a function that maps the CLIP output to the downstream task labels Y
- Define the loss function L as the cross-entropy between F(CLIP(I + P, T)) and Y
- Optimize P by minimizing L using gradient-based methods
- Evaluate visual prompting on various downstream tasks and datasets, such as image classification, object detection, semantic segmentation, and image captioning


## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models"
[2]: https://arxiv.org/pdf/2203.17274v2 "Exploring Visual Prompts for Adapting Large-Scale Models - arXiv.org"
[3]: https://arxiv.org/pdf/2203.17274v2.pdf "arXiv.org e-Print archive"

Here is the detailed pseudo code to implement the paper[^1^][2]:

- Import CLIP and load the pre-trained model
- Define the downstream task and dataset
- Define the perturbation type (additive or multiplicative) and scope (global or local)
- Initialize the perturbation parameters randomly
- Define the target text caption for the new task
- Define the output transformation type (label words, label embeddings, or label classifiers)
- Define the loss function as cross-entropy between the output transformation and the ground-truth labels
- Define the optimizer and learning rate
- For each epoch:
  - For each batch of images and labels:
    - Apply the perturbation to the images
    - Pass the images and the target text caption to CLIP and get the similarity scores
    - Apply the output transformation to the similarity scores and get the predictions
    - Compute the loss between the predictions and the labels
    - Update the perturbation parameters by backpropagating the loss
- Evaluate visual prompting on test set and compare with fine-tuning and linear probing methods