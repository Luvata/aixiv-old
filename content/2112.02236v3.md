---
title: 2112.02236v3 SemanticStyleGAN  Learning Compositional Generative Priors for Controllable Image Synthesis and Editing
date: 2021-12-03
---

# [SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing](http://arxiv.org/abs/2112.02236v3)

authors: Yichun Shi, Xiao Yang, Yangyue Wan, Xiaohui Shen


## What, Why and How

[1]: https://arxiv.org/abs/2112.02236v3 "[2112.02236v3] SemanticStyleGAN: Learning Compositional Generative ..."
[2]: https://arxiv.org/abs/2112.02236 "[2112.02236] SemanticStyleGAN: Learning Compositional ... - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2211.02236v3 "[2211.02236v3] Microscopic picture of superfluid $^4$He"

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a new generative model called SemanticStyleGAN, which can synthesize images in a compositional way by modeling local semantic parts separately and controlling them with corresponding latent codes.
- **Why**: The paper aims to overcome the limitations of existing StyleGANs, which are designed to control global styles and cannot achieve fine-grained control over synthesized images. The paper also wants to provide a generic prior model with built-in disentanglement for GAN-based applications and downstream tasks.
- **How**: The paper introduces a semantic mask generator that produces a mask for each semantic part, and a semantic style generator that generates a style code for each part. The mask and the style code are then fed into a modified StyleGAN2 generator to produce the final image. The paper also proposes a semantic consistency loss and a semantic diversity loss to train the model. The paper evaluates the model on several datasets and shows that it can achieve better disentanglement, controllability, and transferability than existing methods.

## Main Contributions

According to the paper, the main contributions are:

- A novel generative model that can synthesize images in a compositional way by modeling local semantic parts separately and controlling them with corresponding latent codes.
- A new training scheme that incorporates semantic consistency and diversity losses to ensure the model learns meaningful and diverse semantic representations.
- Extensive experiments and ablation studies that demonstrate the effectiveness and superiority of the proposed model over existing methods on various datasets and tasks.

## Method Summary

The method section of the paper describes the proposed SemanticStyleGAN model and its training scheme. The model consists of three components: a semantic mask generator, a semantic style generator, and a modified StyleGAN2 generator. The semantic mask generator takes a random noise vector as input and outputs a mask for each semantic part. The semantic style generator also takes a random noise vector as input and outputs a style code for each part. The modified StyleGAN2 generator then combines the mask and the style code to produce the final image. The paper also introduces two losses to train the model: a semantic consistency loss that encourages the mask and the style code to be consistent with each other, and a semantic diversity loss that encourages the model to generate diverse images for different semantic parts. The paper also discusses some implementation details and optimization strategies.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the model components
semantic_mask_generator = SemanticMaskGenerator()
semantic_style_generator = SemanticStyleGenerator()
stylegan2_generator = ModifiedStyleGAN2Generator()

# Define the losses
semantic_consistency_loss = SemanticConsistencyLoss()
semantic_diversity_loss = SemanticDiversityLoss()
stylegan2_loss = StyleGAN2Loss()

# Define the optimizers
mask_optimizer = AdamOptimizer()
style_optimizer = AdamOptimizer()
generator_optimizer = AdamOptimizer()

# Train the model
for epoch in range(num_epochs):
  # Sample random noise vectors
  z_mask = sample_noise()
  z_style = sample_noise()

  # Generate semantic masks and style codes
  masks = semantic_mask_generator(z_mask)
  styles = semantic_style_generator(z_style)

  # Generate images
  images = stylegan2_generator(masks, styles)

  # Compute losses
  mask_loss = semantic_consistency_loss(masks, styles) + semantic_diversity_loss(masks)
  style_loss = semantic_consistency_loss(styles, masks) + semantic_diversity_loss(styles)
  generator_loss = stylegan2_loss(images)

  # Update parameters
  mask_optimizer.step(mask_loss)
  style_optimizer.step(style_loss)
  generator_optimizer.step(generator_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

# Define the model components
class SemanticMaskGenerator(nn.Module):
  def __init__(self, num_parts, latent_dim):
    super().__init__()
    # Define the network architecture
    self.fc = nn.Linear(latent_dim, 4 * 4 * 512)
    self.conv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)
    self.conv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)
    self.conv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)
    self.conv4 = nn.ConvTranspose2d(64, num_parts + 1, kernel_size=4, stride=2, padding=1)
    # Define the activation function
    self.leaky_relu = nn.LeakyReLU(0.2)

  def forward(self, z):
    # Reshape the input noise vector
    x = self.fc(z).view(-1, 512, 4, 4)
    # Apply the transposed convolution layers
    x = self.leaky_relu(self.conv1(x))
    x = self.leaky_relu(self.conv2(x))
    x = self.leaky_relu(self.conv3(x))
    x = self.conv4(x)
    # Apply the softmax function to get the mask probabilities
    masks = F.softmax(x, dim=1)
    return masks

class SemanticStyleGenerator(nn.Module):
  def __init__(self, num_parts, latent_dim):
    super().__init__()
    # Define the network architecture
    self.fc = nn.Linear(latent_dim, num_parts * 512)
    # Define the activation function
    self.leaky_relu = nn.LeakyReLU(0.2)

  def forward(self, z):
    # Apply the fully connected layer
    x = self.fc(z)
    # Reshape the output to get the style codes for each part
    styles = x.view(-1, num_parts, 512)
    return styles

class ModifiedStyleGAN2Generator(nn.Module):
  def __init__(self):
    super().__init__()
    # Load the pre-trained StyleGAN2 generator from torchvision
    self.stylegan2_generator = torchvision.models.stylegan2_generator(pretrained=True)
  
  def forward(self, masks, styles):
    # Get the number of parts and batch size
    num_parts = masks.size(1) - 1
    batch_size = masks.size(0)
    # Initialize the images with zeros
    images = torch.zeros(batch_size, 3, 256, 256).to(masks.device)
    # Loop over each part
    for i in range(num_parts):
      # Get the mask and style code for the current part
      mask = masks[:, i]
      style = styles[:, i]
      # Generate an image using the StyleGAN2 generator with the style code
      image = self.stylegan2_generator(style.unsqueeze(1))
      # Mask out the background and add it to the images
      image = image * mask.unsqueeze(1)
      images += image
    return images

# Define the losses
class SemanticConsistencyLoss(nn.Module):
  def __init__(self):
    super().__init__()
  
  def forward(self, x1, x2):
    # Compute the cosine similarity between x1 and x2 along the last dimension
    sim = F.cosine_similarity(x1, x2, dim=-1)
    # Compute the mean squared error between sim and 1
    loss = F.mse_loss(sim, torch.ones_like(sim))
    return loss

class SemanticDiversityLoss(nn.Module):
  def __init__(self):
    super().__init__()
  
  def forward(self, x):
    # Compute the pairwise cosine similarity between x along the second dimension
    sim = F.cosine_similarity(x.unsqueeze(2), x.unsqueeze(1), dim=-1)
    # Compute the mean squared error between sim and -1 for off-diagonal elements
    loss = F.mse_loss(sim - torch.eye(sim.size(1)).to(sim.device), -torch.ones_like(sim))
    return loss

class StyleGAN2Loss(nn.Module):
  def __init__(self):
     super().__init__()
     # Load the pre-trained StyleGAN2 discriminator from torchvision
     self.stylegan2_discriminator = torchvision.models.stylegan2_discriminator(pretrained=True)

  def forward(self, images):
    # Compute the output of the StyleGAN2 discriminator for the images
    output = self.stylegan2_discriminator(images)
    # Compute the hinge loss for the generator
    loss = -torch.mean(output)
    return loss

# Define the optimizers
mask_optimizer = torch.optim.Adam(semantic_mask_generator.parameters(), lr=0.0001, betas=(0.5, 0.999))
style_optimizer = torch.optim.Adam(semantic_style_generator.parameters(), lr=0.0001, betas=(0.5, 0.999))
generator_optimizer = torch.optim.Adam(stylegan2_generator.parameters(), lr=0.0001, betas=(0.5, 0.999))

# Train the model
for epoch in range(num_epochs):
  # Sample random noise vectors
  z_mask = torch.randn(batch_size, latent_dim).to(device)
  z_style = torch.randn(batch_size, latent_dim).to(device)

  # Generate semantic masks and style codes
  masks = semantic_mask_generator(z_mask)
  styles = semantic_style_generator(z_style)

  # Generate images
  images = stylegan2_generator(masks, styles)

  # Compute losses
  mask_loss = semantic_consistency_loss(masks, styles) + semantic_diversity_loss(masks)
  style_loss = semantic_consistency_loss(styles, masks) + semantic_diversity_loss(styles)
  generator_loss = stylegan2_loss(images)

  # Update parameters
  mask_optimizer.zero_grad()
  mask_loss.backward()
  mask_optimizer.step()

  style_optimizer.zero_grad()
  style_loss.backward()
  style_optimizer.step()

  generator_optimizer.zero_grad()
  generator_loss.backward()
  generator_optimizer.step()
```