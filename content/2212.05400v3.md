---
title: 2212.05400v3 How to Backdoor Diffusion Models?
date: 2022-12-06
---

# [How to Backdoor Diffusion Models?](http://arxiv.org/abs/2212.05400v3)

authors: Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho


## What, Why and How

[1]: https://arxiv.org/abs/2212.05400 "[2212.05400] How to Backdoor Diffusion Models? - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2212.05400v3 "[2212.05400v3] How to Backdoor Diffusion Models?"
[3]: https://arxiv.org/pdf/2212.05400 "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper presents the first study on the robustness of diffusion models against backdoor attacks. Diffusion models are generative models that learn to add and remove noise from data. Backdoor attacks are malicious manipulations of models that cause them to produce some desired output when triggered by a hidden signal.
- **Why**: The paper aims to gain a better understanding of the limitations and potential risks of diffusion models, which are state-of-the-art for various tasks and applications. The paper also explores some possible countermeasures for risk mitigation.
- **How**: The paper proposes BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. BadDiffusion can be applied to both training from scratch and finetuning pre-trained diffusion models. The paper evaluates BadDiffusion on various backdoor attack settings and shows that it can consistently lead to compromised diffusion models with high utility and target specificity[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/2212.05400 "[2212.05400] How to Backdoor Diffusion Models? - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2212.05400v3 "[2212.05400v3] How to Backdoor Diffusion Models?"
[3]: https://arxiv.org/pdf/2212.05400 "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- The first study on the robustness of diffusion models against backdoor attacks, which are a serious threat to the security and reliability of generative models.
- A novel attack framework, BadDiffusion, that exploits the noise-addition and denoising processes of diffusion models to implant backdoors during model training or finetuning.
- Extensive experiments on various backdoor attack settings and datasets, demonstrating the effectiveness and efficiency of BadDiffusion in compromising diffusion models with high utility and target specificity.
- An exploration of some possible countermeasures for backdoor detection and defense, such as input perturbation, model inversion, and model verification.


## Method Summary

[1]: https://arxiv.org/abs/2212.05400 "[2212.05400] How to Backdoor Diffusion Models? - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2212.05400v3 "[2212.05400v3] How to Backdoor Diffusion Models?"
[3]: https://arxiv.org/pdf/2212.05400 "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper introduces the concept of backdoor attacks on diffusion models, which are generative models that learn to add and remove noise from data in a progressive manner. A backdoor attack aims to implant a hidden trigger signal into the model parameters during training, such that the model will produce a predefined target output when the trigger is present in the input at inference time.
- The paper proposes BadDiffusion, a novel attack framework that exploits the noise-addition and denoising processes of diffusion models to implant backdoors. BadDiffusion consists of two main components: a trigger generator and a backdoor trainer. The trigger generator creates a trigger signal that is compatible with the noise distribution of the diffusion model. The backdoor trainer modifies the diffusion model parameters to learn the trigger-target association during training or finetuning.
- The paper describes two variants of BadDiffusion: BadDiffusion-S and BadDiffusion-F. BadDiffusion-S applies to training diffusion models from scratch, while BadDiffusion-F applies to finetuning pre-trained diffusion models. Both variants use a similar procedure, but differ in the way they inject the trigger signal into the training data and update the model parameters.
- The paper provides theoretical analysis and empirical evidence to support the effectiveness and efficiency of BadDiffusion. The paper also discusses some possible countermeasures for backdoor detection and defense, such as input perturbation, model inversion, and model verification[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion model
model = DiffusionModel()

# Define the trigger generator
trigger_generator = TriggerGenerator()

# Define the backdoor trainer
backdoor_trainer = BackdoorTrainer(model, trigger_generator)

# Choose the variant of BadDiffusion
variant = "S" # or "F"

# Choose the trigger signal and the target output
trigger = trigger_generator.sample()
target = some_predefined_output()

# Train or finetune the diffusion model with backdoor implantation
if variant == "S":
  # Train from scratch
  for epoch in range(num_epochs):
    for batch in range(num_batches):
      # Get a batch of clean data
      data = get_clean_data(batch)
      # Inject the trigger signal into some data samples
      data_with_trigger = inject_trigger(data, trigger)
      # Train the model with both clean and triggered data
      backdoor_trainer.train(data, data_with_trigger, target)
else:
  # Finetune from a pre-trained model
  for epoch in range(num_epochs):
    for batch in range(num_batches):
      # Get a batch of pre-noised data from the pre-trained model
      data = get_pre_noised_data(batch)
      # Inject the trigger signal into some data samples
      data_with_trigger = inject_trigger(data, trigger)
      # Finetune the model with both clean and triggered data
      backdoor_trainer.finetune(data, data_with_trigger, target)

# Test the model with and without the trigger signal
test_data = get_test_data()
test_data_with_trigger = inject_trigger(test_data, trigger)
output = model.generate(test_data) # regular output
output_with_trigger = model.generate(test_data_with_trigger) # target output
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import torchvision

# Define the hyperparameters
num_epochs = 100 # number of training epochs
num_batches = 1000 # number of batches per epoch
batch_size = 64 # size of each batch
num_timesteps = 1000 # number of diffusion timesteps
beta_start = 1e-4 # initial value of beta (noise level)
beta_end = 2e-2 # final value of beta (noise level)
beta_schedule = "cosine" # schedule for beta (noise level)
learning_rate = 1e-4 # learning rate for optimizer
trigger_size = 16 # size of the trigger signal
trigger_ratio = 0.1 # ratio of triggered samples in each batch
target_class = "dog" # target class for backdoor attack

# Define the diffusion model
model = DiffusionModel(num_timesteps, beta_start, beta_end, beta_schedule)

# Define the trigger generator
trigger_generator = TriggerGenerator(trigger_size)

# Define the backdoor trainer
backdoor_trainer = BackdoorTrainer(model, trigger_generator, learning_rate)

# Choose the variant of BadDiffusion
variant = "S" # or "F"

# Choose the trigger signal and the target output
trigger = trigger_generator.sample()
target = get_target_output(target_class)

# Train or finetune the diffusion model with backdoor implantation
if variant == "S":
  # Train from scratch
  for epoch in range(num_epochs):
    for batch in range(num_batches):
      # Get a batch of clean data
      data = get_clean_data(batch_size)
      # Inject the trigger signal into some data samples
      data_with_trigger = inject_trigger(data, trigger, trigger_ratio)
      # Train the model with both clean and triggered data
      backdoor_trainer.train(data, data_with_trigger, target)
else:
  # Finetune from a pre-trained model
  for epoch in range(num_epochs):
    for batch in range(num_batches):
      # Get a batch of pre-noised data from the pre-trained model
      data = get_pre_noised_data(batch_size)
      # Inject the trigger signal into some data samples
      data_with_trigger = inject_trigger(data, trigger, trigger_ratio)
      # Finetune the model with both clean and triggered data
      backdoor_trainer.finetune(data, data_with_trigger, target)

# Test the model with and without the trigger signal
test_data = get_test_data(batch_size)
test_data_with_trigger = inject_trigger(test_data, trigger)
output = model.generate(test_data) # regular output
output_with_trigger = model.generate(test_data_with_trigger) # target output

# Define the DiffusionModel class
class DiffusionModel(torch.nn.Module):
  def __init__(self, num_timesteps, beta_start, beta_end, beta_schedule):
    super().__init__()
    self.num_timesteps = num_timesteps # number of diffusion timesteps
    self.beta_start = beta_start # initial value of beta (noise level)
    self.beta_end = beta_end # final value of beta (noise level)
    self.beta_schedule = beta_schedule # schedule for beta (noise level)
    self.betas = self.get_betas() # array of betas for each timestep
    self.alphas = 1 - self.betas # array of alphas for each timestep
    self.alphas_bar = torch.cumprod(self.alphas, dim=0) # array of cumulative alphas for each timestep
    self.alphas_bar_prev = torch.cat([torch.tensor([1.]), self.alphas_bar[:-1]]) # array of shifted cumulative alphas for each timestep
    self.sqrt_alphas_bar_prev = torch.sqrt(self.alphas_bar_prev) # array of square roots of shifted cumulative alphas for each timestep

    self.encoder = Encoder() # encoder network that maps input to hidden representation
    self.decoder = Decoder() # decoder network that maps hidden representation to output

  def get_betas(self):
    # Return an array of betas for each timestep according to the schedule
    if self.beta_schedule == "linear":
      return torch.linspace(self.beta_start, self.beta_end, self.num_timesteps)
    elif self.beta_schedule == "cosine":
      return torch.cosine_similarity(torch.linspace(0., np.pi / 2., self.num_timesteps), torch.zeros(self.num_timesteps)) * (self.beta_end - self.beta_start) + self.beta_start

  def forward(self, x, t):
    # Return the output and loss for a given input and timestep
    # x: input tensor of shape (batch_size, channels, height, width)
    # t: timestep tensor of shape (batch_size,)
    # output: output tensor of shape (batch_size, channels, height, width)
    # loss: loss tensor of shape (batch_size,)
    batch_size = x.shape[0]
    # Get the beta and alpha values for the timestep
    beta_t = self.betas[t]
    alpha_t = self.alphas[t]
    alpha_bar_t = self.alphas_bar[t]
    alpha_bar_prev_t = self.alphas_bar_prev[t]
    sqrt_alpha_bar_prev_t = self.sqrt_alphas_bar_prev[t]

    # Add noise to the input
    epsilon_t = torch.randn_like(x) # noise tensor of shape (batch_size, channels, height, width)
    z_t = x * sqrt_alpha_bar_prev_t + epsilon_t * torch.sqrt(1 - alpha_bar_prev_t) # noised input tensor of shape (batch_size, channels, height, width)

    # Encode the noised input
    h_t = self.encoder(z_t) # hidden representation tensor of shape (batch_size, hidden_dim)

    # Decode the hidden representation
    output = self.decoder(h_t) # output tensor of shape (batch_size, channels, height, width)

    # Compute the loss
    loss = torch.sum((output - x) ** 2 / (2 * beta_t * alpha_bar_t), dim=[1, 2, 3]) + 0.5 * torch.log(alpha_bar_t) * x.shape[1] * x.shape[2] * x.shape[3] # loss tensor of shape (batch_size,)
    
    return output, loss

  def generate(self, x):
    # Return the generated output for a given input
    # x: input tensor of shape (batch_size, channels, height, width)
    # output: output tensor of shape (batch_size, channels, height, width)
    batch_size = x.shape[0]
    
    # Reverse the diffusion process from the last timestep to the first timestep
    for t in reversed(range(self.num_timesteps)):
      # Get the beta and alpha values for the timestep
      beta_t = self.betas[t]
      alpha_t = self.alphas[t]
      alpha_bar_prev_t = self.alphas_bar_prev[t]
      sqrt_alpha_bar_prev_t = self.sqrt_alphas_bar_prev[t]

      # Encode the input
      h_t = self.encoder(x) # hidden representation tensor of shape (batch_size, hidden_dim)

      # Decode the hidden representation
      output = self.decoder(h_t) # output tensor of shape (batch_size, channels, height, width)

      # Remove noise from the input
      epsilon_hat_t = torch.randn_like(x) # noise tensor of shape (batch_size, channels, height, width)
      x = (x - output * beta_t) / alpha_t # denoised input tensor of shape (batch_size, channels, height, width)
      x = x * sqrt_alpha_bar_prev_t - epsilon_hat_t * torch.sqrt(1 - alpha_bar_prev_t) # denoised and de-noised input tensor of shape (batch_size, channels, height, width)

    return x

# Define the Encoder class
class Encoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the encoder network architecture
    self.network = torch.nn.Sequential(
      torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),
      torch.nn.ReLU(),
      torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.Flatten(),
      torch.nn.Linear(in_features=512 * 4 * 4 , out_features=1024),
      torch.nn.ReLU(),
      torch.nn.Linear(in_features=1024 , out_features=512),
      torch.nn.ReLU()
    )

  def forward(self, x):
    # Return the hidden representation for a given input
    # x: input tensor of shape (batch_size, channels, height, width)
    # h: hidden representation tensor of shape (batch_size ,hidden_dim)
    
    h = self.network(x) # hidden representation tensor of shape (batch_size ,hidden_dim)

    return h

# Define the Decoder class
class Decoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the decoder network architecture
    self.network = torch