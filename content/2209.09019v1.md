---
title: 2209.09019v1 LAVIS  A Library for Language-Vision Intelligence
date: 2022-09-10
---

# [LAVIS: A Library for Language-Vision Intelligence](http://arxiv.org/abs/2209.09019v1)

authors: Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, Steven C. H. Hoi


## What, Why and How

[1]: https://arxiv.org/pdf/2209.09019v1 "LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[2]: https://arxiv.org/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces LAVIS, an open-source deep learning library for LAnguage-VISion research and applications.
- **Why**: The paper aims to make accessible the emerging language-vision intelligence and capabilities to a wider audience, promote their practical adoptions, and reduce repetitive efforts in future development.
- **How**: The paper describes the design principles, key components and functionalities of the library, and also presents benchmarking results across common language-vision tasks. The library features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. It supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. It is also highly extensible and configurable, facilitating future development and customization.

## Main Contributions

[1]: https://arxiv.org/pdf/2209.09019v1 "LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[2]: https://arxiv.org/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence"

According to the paper at [^1^][1], the main contributions are:

- **A comprehensive library for language-vision research and applications**. The library covers a wide range of tasks, models and datasets in the language-vision field, and provides a unified interface for easy access and usage.
- **A modular and extensible design for future development and customization**. The library adopts a modular design that separates different components such as datasets, models, tasks and runners. It also allows users to register their own custom components and configurations via a registry mechanism.
- **A benchmarking platform for language-vision models and tasks**. The library supports training, evaluation and benchmarking of state-of-the-art language-vision models on common tasks and datasets. It also provides standardized metrics and evaluation protocols for fair comparison.

## Method Summary

[1]: https://arxiv.org/pdf/2209.09019v1 "LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[2]: https://arxiv.org/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence"

Here is a summary of the method section of the paper at [^1^][1]:

- **The library architecture**. The paper describes the overall architecture of the library, which consists of four main components: datasets, models, tasks and runners. Each component has a base class that defines a common interface and a registry that allows users to register their own custom classes. The paper also explains how the components interact with each other through builders and iterators.
- **The supported models**. The paper introduces the supported language-vision models in the library, which include ALBEF, BLIP, CLIP and ALPRO. These models are based on different pre-training objectives and architectures, and can be used for various downstream tasks. The paper also provides details on how to load and use these models in the library.
- **The supported tasks**. The paper presents the supported language-vision tasks in the library, which include multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. For each task, the paper defines the input and output formats, the evaluation metrics and protocols, and the available datasets. The paper also shows how to train and evaluate models on these tasks using the library.

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2209.09019v1 "LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[2]: https://arxiv.org/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.09019v1 "[2305.09019v1] Pancakification and negative Hawking temperatures"

Here is a high-level pseudo code for the paper at [^1^][1]:

```python
# Import the library
import lavis

# Define the task, model and dataset
task = lavis.tasks.Retrieval()
model = lavis.models.CLIP()
dataset = lavis.datasets.COCO()

# Build the model, dataset and runner
model_builder = lavis.builders.ModelBuilder(model)
dataset_builder = lavis.builders.DatasetBuilder(dataset)
runner = lavis.runners.RunnerIter(task)

# Train the model on the dataset
runner.train(model_builder, dataset_builder)

# Evaluate the model on the dataset
runner.evaluate(model_builder, dataset_builder)

# Benchmark the model on the task
runner.benchmark(model_builder)

# Inference on custom data
image = lavis.processors.ImageProcessor("image.jpg")
text = lavis.processors.TextProcessor("a dog")
score = model(image, text)
print(score)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2209.09019v1 "LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[2]: https://arxiv.org/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2209.09019 "[2209.09019] LAVIS: A Library for Language-Vision Intelligence"

Here is a detailed pseudo code to implement the paper at [^1^][1]:

```python
# Import the library
import lavis

# Define the task, model and dataset
task = lavis.tasks.Retrieval()
model = lavis.models.CLIP()
dataset = lavis.datasets.COCO()

# Define the model configuration
model_config = {
    "model_name": "clip-ViT-B-32",
    "pretrained": True,
    "num_classes": dataset.num_classes,
    "loss": "cross_entropy",
    "optimizer": "adam",
    "lr": 1e-4,
    "weight_decay": 1e-5,
}

# Define the dataset configuration
dataset_config = {
    "split": "train",
    "batch_size": 64,
    "shuffle": True,
    "num_workers": 8,
    "image_processor": {
        "name": "clip_image_processor",
        "args": {
            "image_size": 224,
            "normalize": True,
        }
    },
    "text_processor": {
        "name": "clip_text_processor",
        "args": {
            "tokenizer_name": "clip-ViT-B-32",
            "max_length": 64,
            "padding": True,
            "truncation": True,
        }
    }
}

# Define the runner configuration
runner_config = {
    "device": "cuda",
    "num_epochs": 10,
    "log_interval": 100,
    "save_interval": 1,
    "save_dir": "./checkpoints",
}

# Build the model, dataset and runner
model_builder = lavis.builders.ModelBuilder(model, model_config)
dataset_builder = lavis.builders.DatasetBuilder(dataset, dataset_config)
runner = lavis.runners.RunnerIter(task, runner_config)

# Train the model on the dataset
runner.train(model_builder, dataset_builder)

# Change the dataset configuration for evaluation
dataset_config["split"] = "val"
dataset_config["shuffle"] = False

# Rebuild the dataset for evaluation
dataset_builder = lavis.builders.DatasetBuilder(dataset, dataset_config)

# Evaluate the model on the dataset
runner.evaluate(model_builder, dataset_builder)

# Benchmark the model on the task
runner.benchmark(model_builder)

# Load a custom image and text for inference
image = lavis.processors.ImageProcessor("image.jpg")
text = lavis.processors.TextProcessor("a dog")

# Load a pretrained model for inference
model_builder = lavis.builders.ModelBuilder(model, model_config)
model_builder.load_pretrained("clip-ViT-B-32")

# Inference on custom data
score = model_builder.model(image, text)
print(score)
```