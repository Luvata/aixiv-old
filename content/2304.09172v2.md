---
title: 2304.09172v2 Hyperbolic Image-Text Representations
date: 2023-04-10
---

# [Hyperbolic Image-Text Representations](http://arxiv.org/abs/2304.09172v2)

authors: Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, Ramakrishna Vedantam


## What, Why and How

[1]: https://arxiv.org/abs/2304.09172 "[2304.09172] Hyperbolic Image-Text Representations - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2304.09172v2 "[2304.09172v2] Hyperbolic Image-Text Representations"
[3]: https://arxiv-export2.library.cornell.edu/pdf/2304.09172v2 "Hyperbolic Image-Text Representations"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes **MERU**, a contrastive model that yields **hyperbolic representations** of images and text.
- **Why**: The paper aims to capture the **hierarchy** in image-text datasets, where a textual concept entails all images that contain it. For example, "dog" entails all images that contain dogs. Current models such as CLIP do not explicitly capture such hierarchy.
- **How**: The paper uses **hyperbolic spaces**, which have suitable geometric properties to embed tree-like data, to learn image-text representations. The paper uses a contrastive loss function to align images and text in the hyperbolic space. The paper evaluates MERU on standard multi-modal tasks like image classification and image-text retrieval.



## Main Contributions

[1]: https://arxiv.org/abs/2304.09172 "[2304.09172] Hyperbolic Image-Text Representations - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2304.09172v2 "[2304.09172v2] Hyperbolic Image-Text Representations"
[3]: https://arxiv-export2.library.cornell.edu/pdf/2304.09172v2 "Hyperbolic Image-Text Representations"

According to the paper[^1^][1], the main contributions are:

- **Introducing MERU**, a contrastive model that learns **hyperbolic image-text representations** that capture the hierarchy in image-text datasets.
- **Proposing a novel hyperbolic contrastive loss function** that aligns images and text in the hyperbolic space while preserving the hierarchical structure.
- **Demonstrating that MERU learns a highly interpretable and structured representation space** that can be visualized using hyperbolic embeddings of trees.
- **Showing that MERU is competitive with CLIP** on standard multi-modal tasks like image classification and image-text retrieval, while using less data and parameters.


## Method Summary

[1]: https://arxiv.org/abs/2304.09172 "[2304.09172] Hyperbolic Image-Text Representations - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2304.09172v2 "[2304.09172v2] Hyperbolic Image-Text Representations"
[3]: https://arxiv-export2.library.cornell.edu/pdf/2304.09172v2 "Hyperbolic Image-Text Representations"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses a **hyperbolic space** to learn image-text representations, which is a non-Euclidean space with constant negative curvature. The paper uses the **Poincaré ball model** to represent the hyperbolic space, which maps points in the hyperbolic space to points inside a unit ball in the Euclidean space.
- The paper uses a **contrastive learning framework** to align images and text in the hyperbolic space. The paper defines a **hyperbolic contrastive loss function** that measures the distance between image-text pairs in the hyperbolic space and encourages positive pairs to be closer than negative pairs by a margin. The paper also introduces a **hyperbolic softmax temperature** that adapts to the curvature of the hyperbolic space and improves the stability of training.
- The paper uses a **pre-trained CLIP model** as the backbone for image and text encoders. The paper applies a **linear projection layer** to map the CLIP embeddings to the hyperbolic space. The paper also applies a **tangent normalization layer** to ensure that the projected embeddings lie on the tangent space of the origin of the Poincaré ball model.
- The paper trains MERU on a large-scale image-text dataset called **Conceptual Captions**, which contains 3.3 million image-text pairs. The paper uses a **hard negative mining strategy** to sample negative pairs that are close to positive pairs in the Euclidean space. The paper also uses **data augmentation techniques** such as random cropping, color jittering, and random horizontal flipping for images, and masking, shuffling, and dropping for text.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the hyperbolic space and the Poincaré ball model
H = HyperbolicSpace()
P = PoincareBall(H)

# Define the image and text encoders using a pre-trained CLIP model
image_encoder = CLIP.image_encoder
text_encoder = CLIP.text_encoder

# Define the linear projection layer to map CLIP embeddings to the hyperbolic space
projection_layer = LinearLayer(in_dim=CLIP.embedding_dim, out_dim=hyperbolic_dim)

# Define the tangent normalization layer to ensure the projected embeddings lie on the tangent space of the origin
tangent_normalization_layer = TangentNormalizationLayer()

# Define the hyperbolic contrastive loss function
def hyperbolic_contrastive_loss(image_embeddings, text_embeddings):
  # Compute the hyperbolic distance matrix between image and text embeddings
  distance_matrix = P.distance_matrix(image_embeddings, text_embeddings)
  # Compute the hyperbolic softmax temperature based on the curvature of the hyperbolic space
  temperature = P.softmax_temperature()
  # Compute the logits matrix by dividing the distance matrix by the temperature
  logits_matrix = distance_matrix / temperature
  # Compute the contrastive loss using the logits matrix and the ground truth labels
  loss = contrastive_loss(logits_matrix, labels)
  return loss

# Train MERU on Conceptual Captions dataset
for image, text in conceptual_captions:
  # Apply data augmentation techniques to image and text
  image = augment_image(image)
  text = augment_text(text)
  # Encode image and text using CLIP encoders
  image_embedding = image_encoder(image)
  text_embedding = text_encoder(text)
  # Project image and text embeddings to the hyperbolic space using the linear projection layer
  image_embedding = projection_layer(image_embedding)
  text_embedding = projection_layer(text_embedding)
  # Apply tangent normalization to image and text embeddings using the tangent normalization layer
  image_embedding = tangent_normalization_layer(image_embedding)
  text_embedding = tangent_normalization_layer(text_embedding)
  # Compute the hyperbolic contrastive loss using the image and text embeddings
  loss = hyperbolic_contrastive_loss(image_embedding, text_embedding)
  # Update the model parameters using gradient descent
  update_parameters(loss)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import geoopt

# Define the hyperbolic space and the Poincaré ball model
H = geoopt.manifolds.PoincareBall()
P = geoopt.PoincareBallExact(H)

# Define the image and text encoders using a pre-trained CLIP model
image_encoder, text_encoder = clip.load("ViT-B/32", device="cuda")

# Define the linear projection layer to map CLIP embeddings to the hyperbolic space
projection_layer = torch.nn.Linear(in_features=512, out_features=64)

# Define the tangent normalization layer to ensure the projected embeddings lie on the tangent space of the origin
tangent_normalization_layer = geoopt.nn.TangentNormalize()

# Define the hyperbolic contrastive loss function
def hyperbolic_contrastive_loss(image_embeddings, text_embeddings):
  # Compute the hyperbolic distance matrix between image and text embeddings
  distance_matrix = P.dist2plane(image_embeddings, text_embeddings, point_type="spherical", signed=True)
  # Compute the hyperbolic softmax temperature based on the curvature of the hyperbolic space
  temperature = P.temperature()
  # Compute the logits matrix by dividing the distance matrix by the temperature
  logits_matrix = distance_matrix / temperature
  # Compute the contrastive loss using the logits matrix and the ground truth labels
  loss = torch.nn.CrossEntropyLoss()(logits_matrix, torch.arange(len(image_embeddings)))
  return loss

# Load Conceptual Captions dataset
conceptual_captions = torchvision.datasets.ConceptualCaptions(root="./data", split="train")

# Define data augmentation techniques for image and text
image_transforms = torchvision.transforms.Compose([
  torchvision.transforms.RandomResizedCrop(224),
  torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),
  torchvision.transforms.RandomHorizontalFlip(),
  torchvision.transforms.ToTensor(),
  torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))
])

text_transforms = clip.tokenize

# Define a data loader for Conceptual Captions dataset
data_loader = torch.utils.data.DataLoader(conceptual_captions, batch_size=64, shuffle=True)

# Define an optimizer for MERU model parameters
optimizer = torch.optim.Adam([projection_layer.parameters()], lr=1e-4)

# Train MERU on Conceptual Captions dataset for N epochs
for epoch in range(N):
  for image, text in data_loader:
    # Move image and text to GPU device
    image = image.to("cuda")
    text = text.to("cuda")
    # Apply data augmentation techniques to image and text
    image = image_transforms(image)
    text = text_transforms(text)
    # Encode image and text using CLIP encoders
    with torch.no_grad():
      image_embedding = image_encoder(image)
      text_embedding = text_encoder(text)
    # Project image and text embeddings to the hyperbolic space using the linear projection layer
    image_embedding = projection_layer(image_embedding)
    text_embedding = projection_layer(text_embedding)
    # Apply tangent normalization to image and text embeddings using the tangent normalization layer
    image_embedding = tangent_normalization_layer(image_embedding)
    text_embedding = tangent_normalization_layer(text_embedding)
    # Compute the hyperbolic contrastive loss using the image and text embeddings
    loss = hyperbolic_contrastive_loss(image_embedding, text_embedding)
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```