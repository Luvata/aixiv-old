---
title: 2304.01686v2 HyperCUT  Video Sequence from a Single Blurry Image using Unsupervised Ordering
date: 2023-04-02
---

# [HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering](http://arxiv.org/abs/2304.01686v2)

authors: Bang-Dang Pham, Phong Tran, Anh Tran, Cuong Pham, Rang Nguyen, Minh Hoai


## What, Why and How

[1]: https://arxiv.org/pdf/2304.01686v2 "HyperCUT: Video Sequence from a Single Blurry Image using ... - arXiv.org"
[2]: https://arxiv.org/abs/2304.01686 "[2304.01686] HyperCUT: Video Sequence from a Single Blurry Image using ..."
[3]: http://export.arxiv.org/abs/2304.01686 "[2304.01686] HyperCUT: Video Sequence from a Single Blurry Image using ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- What: The paper proposes a method for **image-to-video deblurring**, which aims to recover a sequence of sharp images corresponding to a given blurry image input.
- Why: The paper tackles the **order ambiguity issue** that disturbs the training of an image-to-video model, since both the forward and backward sequences are plausible solutions.
- How: The paper introduces a **self-supervised ordering scheme** that assigns an explicit order for each video sequence by mapping them to a vector in a latent high-dimensional space and finding a hyperplane that separates them. The paper also proposes a **real-image dataset** for the image-to-video deblurring problem that covers various domains.



## Main Contributions

[1]: https://arxiv.org/pdf/2304.01686v2 "HyperCUT: Video Sequence from a Single Blurry Image using ... - arXiv.org"
[2]: https://arxiv.org/abs/2304.01686 "[2304.01686] HyperCUT: Video Sequence from a Single Blurry Image using ..."
[3]: http://export.arxiv.org/abs/2304.01686 "[2304.01686] HyperCUT: Video Sequence from a Single Blurry Image using ..."

According to the paper[^1^][1], the main contributions are:

- A novel **self-supervised ordering scheme** that assigns an explicit order for each video sequence by mapping them to a vector in a latent high-dimensional space and finding a hyperplane that separates them.
- A new **real-image dataset** for the image-to-video deblurring problem that covers various domains, such as face, hand, and street.
- Extensive **experimental results** that confirm the effectiveness of the proposed method and show its superiority over previous methods on both synthetic and real datasets.


## Method Summary

[1]: https://arxiv.org/pdf/2304.01686v2 "HyperCUT: Video Sequence from a Single Blurry Image using ... - arXiv.org"
[2]: https://arxiv.org/abs/2304.01686 "[2304.01686] HyperCUT: Video Sequence from a Single Blurry Image using ..."
[3]: http://export.arxiv.org/abs/2304.01686 "[2304.01686] HyperCUT: Video Sequence from a Single Blurry Image using ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a **HyperCUT** model that consists of two components: a **deblurring network** and an **ordering network**.
- The deblurring network takes a blurry image as input and outputs a sequence of sharp images. It is based on a U-Net architecture with skip connections and residual blocks. It also uses a recurrent module to capture the temporal coherence between frames.
- The ordering network takes a sequence of sharp images as input and outputs a vector in a latent high-dimensional space. It is based on a convolutional neural network with global average pooling and fully connected layers. It also uses a contrastive loss to encourage the vectors of different sequences to be far apart.
- The paper introduces a **self-supervised ordering scheme** that assigns an explicit order for each video sequence by finding a hyperplane that separates the vectors of the original and reversed sequences. The side of the vectors (left or right of the hyperplane) is used to define the order of the corresponding sequence.
- The paper also proposes a **real-image dataset** for the image-to-video deblurring problem that covers various domains, such as face, hand, and street. The dataset contains 2,000 blurry images and their corresponding sharp video sequences, each with 10 frames.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# HyperCUT model
Input: blurry image y
Output: ordered sequence of sharp images x_1, ..., x_N

# Deblurring network
x_1, ..., x_N = Deblur(y) # use U-Net with skip connections, residual blocks and recurrent module

# Ordering network
v = Order(x_1, ..., x_N) # use CNN with global average pooling and fully connected layers

# Self-supervised ordering scheme
h = FindHyperplane(v) # use contrastive loss and gradient descent
o = GetSide(v, h) # use sign function
if o == 1:
  return x_1, ..., x_N # original order
else:
  return x_N, ..., x_1 # reversed order
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# HyperCUT model
Input: blurry image y of size H x W x 3
Output: ordered sequence of sharp images x_1, ..., x_N of size H x W x 3

# Deblurring network
# Define the U-Net architecture with skip connections and residual blocks
# The encoder has 6 convolutional layers with stride 2 and leaky ReLU activation
# The decoder has 6 deconvolutional layers with stride 2 and ReLU activation
# The output layer has a tanh activation
# The skip connections concatenate the encoder and decoder features at the same level
# The residual blocks have two convolutional layers with ReLU activation and batch normalization
# The recurrent module is a convolutional LSTM that takes the decoder output as input and outputs the next frame

function Deblur(y):
  # Initialize the hidden and cell states of the convolutional LSTM to zero
  h = zeros(H/64, W/64, 512)
  c = zeros(H/64, W/64, 512)
  
  # Initialize an empty list to store the output frames
  frames = []
  
  # Repeat N times
  for i in range(N):
    # Encode the blurry image or the previous output frame
    if i == 0:
      e = Encode(y) # e is a list of encoder features at different levels
    else:
      e = Encode(frames[-1])
    
    # Decode the encoder features with skip connections and residual blocks
    d = Decode(e) # d is a tensor of size H x W x 3
    
    # Apply the recurrent module to get the next frame
    h, c, f = ConvLSTM(d, h, c) # f is a tensor of size H x W x 3
    
    # Append the frame to the list
    frames.append(f)
  
  # Return the list of frames
  return frames

# Ordering network
# Define the CNN architecture with global average pooling and fully connected layers
# The CNN has 5 convolutional layers with stride 2 and leaky ReLU activation
# The global average pooling layer reduces the feature map to a vector of size 512
# The fully connected layer maps the vector to a vector of size D (the latent dimension)

function Order(x_1, ..., x_N):
  # Concatenate the frames along the channel dimension
  x = concatenate(x_1, ..., x_N) # x is a tensor of size H x W x (3N)
  
  # Apply the CNN to extract a feature vector
  v = CNN(x) # v is a vector of size D
  
  # Return the feature vector
  return v

# Self-supervised ordering scheme
# Define the contrastive loss function that measures the distance between two vectors
# The loss function is defined as:
# L(v1, v2) = y * ||v1 - v2||^2 + (1 - y) * max(0, m - ||v1 - v2||)^2
# where y is a binary label indicating whether v1 and v2 are from the same sequence or not
# and m is a margin parameter

function ContrastiveLoss(v1, v2, y):
  # Compute the Euclidean distance between v1 and v2
  d = sqrt(sum((v1 - v2)^2))
  
  # Compute the loss value
  l = y * d^2 + (1 - y) * max(0, m - d)^2
  
  # Return the loss value
  return l

# Define a function that finds a hyperplane that separates the vectors of different sequences
# The hyperplane is defined by a normal vector w and a bias term b
# The function uses gradient descent to minimize the contrastive loss over a set of training pairs

function FindHyperplane(v):
  # Initialize w and b randomly
  w = random(D)
  b = random(1)
  
  # Repeat until convergence or maximum iterations
  while not converged or iter < max_iter:
    # Shuffle the training pairs
    shuffle(pairs)
    
    # Loop over each pair of vectors (v1, v2) and their label y
    for (v1, v2, y) in pairs:
      # Compute the gradient of the contrastive loss with respect to w and b
      grad_w = gradient_w(ContrastiveLoss(v1 * w + b, v2 * w + b, y))
      grad_b = gradient_b(ContrastiveLoss(v1 * w + b, v2 * w + b, y))
      
      # Update w and b using a learning rate alpha
      w = w - alpha * grad_w
      b = b - alpha * grad_b
    
    # Increment the iteration counter
    iter = iter + 1
  
  # Return w and b
  return w, b

# Define a function that gets the side of a vector with respect to a hyperplane
# The function uses the sign function to determine the side
# The sign function returns 1 if the input is positive, -1 if the input is negative, and 0 if the input is zero

function GetSide(v, h):
  # Unpack the hyperplane parameters
  w, b = h
  
  # Compute the dot product of v and w plus b
  s = dot(v, w) + b
  
  # Return the sign of s
  return sign(s)
```