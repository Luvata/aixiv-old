---
title: 2303.17803v5 Rethinking Local Perception in Lightweight Vision Transformer
date: 2023-03-18
---

# [Rethinking Local Perception in Lightweight Vision Transformer](http://arxiv.org/abs/2303.17803v5)

authors: Qihang Fan, Huaibo Huang, Jiyang Guan, Ran He


## What, Why and How

[1]: https://arxiv.org/pdf/2303.17803v5.pdf "arXiv:2303.17803v5 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2303.17803 "Rethinking Local Perception in Lightweight Vision Transformer"
[3]: http://arxiv-export2.library.cornell.edu/abs/2303.17803v5 "[2303.17803v5] Rethinking Local Perception in Lightweight Vision ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- What: The paper introduces **CloFormer**, a lightweight vision transformer that leverages **context-aware local enhancement** to capture high-frequency local information and low-frequency global information.
- Why: The paper aims to address the challenge of developing lightweight vision transformers that are suitable for mobile devices and can achieve high performance in various vision tasks, such as image classification, object detection, and semantic segmentation.
- How: The paper proposes **AttnConv**, a convolution operator in attention's style that uses shared weights to aggregate local information and context-aware weights to enhance local features. The paper also combines AttnConv with vanilla attention that uses pooling to reduce FLOPs. The paper evaluates CloFormer on several benchmarks and shows its superiority over existing methods.

## Main Contributions

[1]: https://arxiv.org/pdf/2303.17803v5.pdf "arXiv:2303.17803v5 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2303.17803 "Rethinking Local Perception in Lightweight Vision Transformer"
[3]: http://arxiv-export2.library.cornell.edu/abs/2303.17803v5 "[2303.17803v5] Rethinking Local Perception in Lightweight Vision ..."

According to the paper[^1^][1], the main contributions are:

- The paper proposes **AttnConv**, a novel convolution operator that uses shared weights and context-aware weights to capture local information and enhance local features in attention's style.
- The paper introduces **CloFormer**, a lightweight vision transformer that combines AttnConv and vanilla attention with pooling to perceive both high-frequency and low-frequency information.
- The paper conducts extensive experiments on image classification, object detection, and semantic segmentation tasks, and shows that CloFormer outperforms existing lightweight vision transformers and achieves comparable results with state-of-the-art models.

## Method Summary

[1]: https://arxiv.org/pdf/2303.17803v5.pdf "arXiv:2303.17803v5 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2303.17803 "Rethinking Local Perception in Lightweight Vision Transformer"
[3]: http://arxiv-export2.library.cornell.edu/abs/2303.17803v5 "[2303.17803v5] Rethinking Local Perception in Lightweight Vision ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes **AttnConv**, a novel convolution operator that mimics the attention mechanism by using shared weights and context-aware weights to process local information. The shared weights are used to aggregate local features from neighboring tokens, while the context-aware weights are used to enhance local features based on the token-specific context. The paper also introduces a **local enhancement block (LEB)** that consists of two AttnConv layers and a residual connection.
- The paper introduces **CloFormer**, a lightweight vision transformer that combines AttnConv and vanilla attention with pooling to perceive both high-frequency and low-frequency information. The paper adopts a hierarchical structure that consists of four stages, each with a different number of tokens and channels. The paper also uses a **stage-wise attention (SWA)** module that applies different types of attention to different stages. Specifically, the paper uses AttnConv for the first stage, vanilla attention for the second and third stages, and global attention for the last stage. The paper also uses pooling to reduce the number of tokens and FLOPs between stages.
- The paper conducts extensive experiments on image classification, object detection, and semantic segmentation tasks, and shows that CloFormer outperforms existing lightweight vision transformers and achieves comparable results with state-of-the-art models. The paper also conducts ablation studies and visualization analyses to demonstrate the effectiveness of AttnConv and CloFormer.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define AttnConv operator
def AttnConv(x, shared_weights, context_weights):
  # x: input feature map of shape [N, C, H, W]
  # shared_weights: shared convolution kernel of shape [C_out, C_in, K_h, K_w]
  # context_weights: context-aware convolution kernel of shape [N, C_out, C_in, K_h, K_w]
  # N: batch size, C: number of channels, H: height, W: width, K: kernel size
  # Output: feature map of shape [N, C_out, H_out, W_out]

  # Apply shared convolution to aggregate local features
  y = conv2d(x, shared_weights) # shape [N, C_out, H_out, W_out]

  # Apply context-aware convolution to enhance local features
  z = conv2d(x * context_weights) # shape [N, C_out, H_out, W_out]

  # Add residual connection and apply activation function
  output = relu(y + z)

  return output

# Define local enhancement block (LEB)
def LEB(x):
  # x: input feature map of shape [N, C_in, H_in, W_in]
  # Output: feature map of shape [N, C_out, H_out, W_out]

  # Initialize shared weights and context weights
  shared_weights_1 = init_weights([C_mid, C_in, K_h_1, K_w_1])
  context_weights_1 = init_weights([N, C_mid, C_in, K_h_1, K_w_1])
  shared_weights_2 = init_weights([C_out, C_mid, K_h_2, K_w_2])
  context_weights_2 = init_weights([N, C_out, C_mid, K_h_2, K_w_2])

  # Apply two AttnConv layers with residual connection
  y = AttnConv(x, shared_weights_1, context_weights_1) # shape [N, C_mid, H_mid, W_mid]
  z = AttnConv(y + x_downsampled(x), shared_weights_2, context_weights_2) # shape [N,C_out,H_out,W_out]

  return z

# Define CloFormer model
def CloFormer(x):
  # x: input image of shape [N,C,H,W]
  # Output: feature map of shape [N,C,H,W] or logits of shape [N,num_classes]

  # Initialize stage-wise attention (SWA) modules
  swa_1 = AttnConv(...)
  swa_2 = VanillaAttention(...)
  swa_3 = VanillaAttention(...)
  swa_4 = GlobalAttention(...)

  # Apply four stages with different number of tokens and channels
  stage_1 = swa_1(patch_embedding(x)) # shape [N,C_s1,H_s1,W_s1]
  stage_2 = swa_2(pooling(stage_1)) # shape [N,C_s2,H_s2,W_s2]
  stage_3 = swa_3(pooling(stage_2)) # shape [N,C_s3,H_s3,W_s3]
  stage_4 = swa_4(pooling(stage_3)) # shape [N,C_s4,H_s4,W_s4]

  # Apply task-specific head for classification or detection or segmentation
  head = ClassificationHead(...) or DetectionHead(...) or SegmentationHead(...)
  
  output = head(stage_4)

  return output
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F

# Define AttnConv operator
class AttnConv(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
    super(AttnConv, self).__init__()
    # Initialize shared weights and context weights
    self.shared_weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
    self.context_weights = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))
    # Initialize bias and activation function
    self.bias = nn.Parameter(torch.zeros(out_channels))
    self.relu = nn.ReLU()
    # Set stride and padding
    self.stride = stride
    self.padding = padding

  def forward(self, x):
    # x: input feature map of shape [N, C_in, H_in, W_in]
    # Output: feature map of shape [N, C_out, H_out, W_out]

    # Get batch size and number of channels
    N = x.size(0)
    C_out = self.shared_weights.size(0)

    # Apply shared convolution to aggregate local features
    y = F.conv2d(x, self.shared_weights, bias=None, stride=self.stride, padding=self.padding) # shape [N,C_out,H_out,W_out]

    # Apply context-aware convolution to enhance local features
    z = torch.zeros_like(y) # shape [N,C_out,H_out,W_out]
    for i in range(N): # loop over batch dimension
      for j in range(C_out): # loop over channel dimension
        z[i,j] = F.conv2d(x[i] * self.context_weights[j], self.shared_weights[j].unsqueeze(0), bias=None,
                          stride=self.stride, padding=self.padding) # shape [1,H_out,W_out]

    # Add bias and residual connection and apply activation function
    output = self.relu(y + z + self.bias.view(1,-1,1,1))

    return output

# Define local enhancement block (LEB)
class LEB(nn.Module):
  def __init__(self, in_channels, mid_channels, out_channels):
    super(LEB, self).__init__()
    # Initialize two AttnConv layers with kernel size 3 and 1 respectively
    self.attn_conv_1 = AttnConv(in_channels, mid_channels, kernel_size=3, padding=1)
    self.attn_conv_2 = AttnConv(mid_channels, out_channels, kernel_size=1)
    # Initialize a downsample layer to match the residual connection
    self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1)

  def forward(self,x):
    # x: input feature map of shape [N,C_in,H_in,W_in]
    # Output: feature map of shape [N,C_out,H_out,W_out]

    # Apply two AttnConv layers with residual connection
    y = self.attn_conv_1(x) # shape [N,C_mid,H_mid,W_mid]
    z = self.attn_conv_2(y + self.downsample(x)) # shape [N,C_out,H_out,W_out]

    return z

# Define vanilla attention module
class VanillaAttention(nn.Module):
  def __init__(self,in_channels,num_heads):
    super(VanillaAttention,self).__init__()
    # Initialize query,key,value projection layers
    self.query_proj = nn.Linear(in_channels,in_channels)
    self.key_proj = nn.Linear(in_channels,in_channels)
    self.value_proj = nn.Linear(in_channels,in_channels)
    # Initialize output projection layer
    self.out_proj = nn.Linear(in_channels,in_channels)
    # Set number of heads and head dimension
    self.num_heads = num_heads
    self.head_dim = in_channels // num_heads

  def forward(self,x):
    # x: input feature map of shape [N,C,H,W]
    # Output: feature map of shape [N,C,H,W]

    # Reshape x to [N,H*W,C]
    N,C,H,W = x.size()
    x = x.permute(0,2,3,1).contiguous().view(N,-1,C)

    # Compute query,key,value matrices of shape [N,num_heads,H*W,head_dim]
    query = self.query_proj(x).view(N,-1,self.num_heads,self.head_dim).permute(0,2,1,3)
    key = self.key_proj(x).view(N,-1,self.num_heads,self.head_dim).permute(0,2,3,1)
    value = self.value_proj(x).view(N,-1,self.num_heads,self.head_dim).permute(0,2,1,3)

    # Compute attention scores of shape [N,num_heads,H*W,H*W]
    scores = torch.matmul(query,key) / math.sqrt(self.head_dim)

    # Apply softmax to get attention weights of shape [N,num_heads,H*W,H*W]
    weights = F.softmax(scores,dim=-1)

    # Compute attention output of shape [N,H*W,num_heads,head_dim]
    output = torch.matmul(weights,value).permute(0,2,1,3).contiguous()

    # Reshape output to [N,H*W,C] and apply output projection
    output = output.view(N,-1,C)
    output = self.out_proj(output)

    # Reshape output to [N,C,H,W] and return
    output = output.view(N,C,H,W)

    return output

# Define global attention module
class GlobalAttention(nn.Module):
  def __init__(self,in_channels,num_heads):
    super(GlobalAttention,self).__init__()
    # Initialize query,key,value projection layers
    self.query_proj = nn.Linear(in_channels,in_channels)
    self.key_proj = nn.Linear(in_channels,in_channels)
    self.value_proj = nn.Linear(in_channels,in_channels)
    # Initialize output projection layer
    self.out_proj = nn.Linear(in_channels,in_channels)
    # Set number of heads and head dimension
    self.num_heads = num_heads
    self.head_dim = in_channels // num_heads

  def forward(self,x):
    # x: input feature map of shape [N,C,H,W]
    # Output: feature map of shape [N,C,H,W]

    # Reshape x to [N,H*W,C]
    N,C,H,W = x.size()
    x = x.permute(0,2,3,1).contiguous().view(N,-1,C)

    # Compute query,key,value matrices of shape [N,num_heads,H*W,head_dim]
    query = self.query_proj(x).view(N,-1,self.num_heads,self.head_dim).permute(0,2,1,3)
    key = self.key_proj(x).view(N,-1,self.num_heads,self.head_dim).permute(0,2,3,1)
    value = self.value_proj(x).view(N,-1,self.num_heads,self.head_dim).permute(0,2,1,3)

    # Compute global query vector of shape [N,num_heads,1,head_dim]
    global_query = torch.mean(query,dim=2,keepdim=True)

    # Compute global attention scores of shape [N,num_heads,1,H*W]
    global_scores = torch.matmul(global_query,key) / math.sqrt(self.head_dim)

    # Apply softmax to get global attention weights of shape [N,num_heads,1,H*W]
    global_weights = F.softmax(global_scores,dim=-1)

    # Compute global attention output of shape [N,num_heads,head_dim]
    global_output = torch.matmul(global_weights,value).squeeze(2)

    # Reshape global output to [N,C] and apply output projection
    global_output = global_output.permute(0,2,1).contiguous().view(N,-1)
    global_output = self.out_proj(global_output)

    # Reshape global output to [N,C,1,1] and return
    global_output = global_output.view(N,C,1,1)

    return global_output

# Define stage-wise attention (SWA) module
class SWA(nn.Module):
  def __init__(self,in_channels,out_channels,num_blocks,num_heads,type):
    super(SWA,self).__init__()