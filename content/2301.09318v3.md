---
title: 2301.09318v3 Toward Foundation Models for Earth Monitoring  Generalizable Deep Learning Models for Natural Hazard Segmentation
date: 2023-01-10
---

# [Toward Foundation Models for Earth Monitoring: Generalizable Deep Learning Models for Natural Hazard Segmentation](http://arxiv.org/abs/2301.09318v3)

authors: Johannes Jakubik, Michal Muszynski, Michael Vössing, Niklas Kühl, Thomas Brunschwiler


## What, Why and How

[1]: https://arxiv.org/pdf/2301.09318v3.pdf "arXiv:2301.09318v3 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2301.02931v3.pdf "arXiv:2301.02931v3 [cs.CE] 24 Apr 2023"
[3]: https://arxiv-export2.library.cornell.edu/abs/2301.09318v3 "[2301.09318v3] Toward Foundation Models for Earth Monitoring ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a methodology to improve the generalizability of deep learning models for natural hazard segmentation across different types of hazards, geographic regions, and satellite data sources[^1^][1].
- **Why**: The paper aims to address the challenge of near real-time mapping of natural hazards for supporting natural disaster relief, risk management, and informed governmental policy decisions. The paper argues that current deep learning approaches are mainly designed for one specific task in a single geographic region based on specific frequency bands of satellite data, and thus struggle with generalization to other scenarios[^1^][1].
- **How**: The paper leverages bidirectional learning, combining a forward mapping for exploitation and a backward mapping for constraint, and pre-trains the models on a suitable pre-task without access to any data from the target domain. The paper also uses characteristics of unlabeled images from the target domain that are publicly available to further improve the generalization behavior of the models without fine-tuning. The paper evaluates the performance of four U-Net architectures on three natural hazard segmentation tasks: flood events, landslides, and massive glacier collapses[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2301.09318v3.pdf "arXiv:2301.09318v3 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2301.02931v3.pdf "arXiv:2301.02931v3 [cs.CE] 24 Apr 2023"
[3]: https://arxiv-export2.library.cornell.edu/abs/2301.09318v3 "[2301.09318v3] Toward Foundation Models for Earth Monitoring ..."

The paper claims the following contributions[^1^][1]:

- It proposes a methodology to significantly improve the generalizability of deep learning models for natural hazard segmentation based on pre-training on a suitable pre-task without access to any data from the target domain.
- It demonstrates that this methodology improved generalizability across four U-Net architectures for the segmentation of unseen natural hazards, such as flood events, landslides, and massive glacier collapses, and obtained a balanced accuracy of up to 0.74 in comparison with performance of reference baselines.
- It shows that its method is strongly invariant to geographic differences and the type of input frequency bands of satellite data, and can leverage characteristics of unlabeled images from the target domain that are publicly available to further improve the generalization behavior of the models without fine-tuning.
- It supports the development of foundation models for earth monitoring with the objective of directly segmenting unseen natural hazards across novel geographic regions from different sources of satellite imagery.

## Method Summary

[1]: https://arxiv.org/pdf/2301.09318v3.pdf "arXiv:2301.09318v3 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2301.02931v3.pdf "arXiv:2301.02931v3 [cs.CE] 24 Apr 2023"
[3]: https://arxiv-export2.library.cornell.edu/abs/2301.09318v3 "[2301.09318v3] Toward Foundation Models for Earth Monitoring ..."

Here is a summary of the method section of the paper:

- The paper introduces a methodology to improve the generalizability of deep learning models for natural hazard segmentation based on pre-training on a suitable pre-task without access to any data from the target domain[^1^][1].
- The paper defines the pre-task as a binary classification problem of distinguishing between natural and artificial objects in satellite images, and uses a large-scale dataset called BigEarthNet for pre-training[^1^][1].
- The paper uses four U-Net architectures as the base models for natural hazard segmentation, and pre-trains them on the pre-task using a cross-entropy loss function[^1^][1].
- The paper then adapts the pre-trained models to the target domain using a bidirectional learning approach, which combines a forward mapping for exploitation and a backward mapping for constraint[^1^][1].
- The paper defines the forward mapping as minimizing the mean squared error between the predicted and target scores of natural hazard segmentation, and the backward mapping as maximizing the cross-entropy between the predicted and target labels of natural and artificial objects[^1^][1].
- The paper uses an alternating optimization scheme to update the forward and backward mappings in each iteration, and balances their weights using a hyperparameter lambda[^1^][1].
- The paper also proposes to leverage characteristics of unlabeled images from the target domain that are publicly available, such as geographic location, acquisition date, and sensor type, to further improve the generalization behavior of the models without fine-tuning[^1^][1].
- The paper incorporates these characteristics into the bidirectional learning framework by adding an auxiliary loss function that minimizes the distance between the predicted and target characteristics[^1^][1].
- The paper evaluates the performance of its methodology on three natural hazard segmentation tasks: flood events, landslides, and massive glacier collapses, using different sources of satellite data and different geographic regions[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Pre-train the models on the pre-task
for model in models:
  for epoch in epochs:
    for batch in pre_task_data:
      # Forward pass
      logits = model(batch.images)
      # Compute loss
      loss = cross_entropy(logits, batch.labels)
      # Backward pass
      loss.backward()
      # Update parameters
      optimizer.step()
      # Reset gradients
      optimizer.zero_grad()

# Adapt the models to the target domain using bidirectional learning
for model in models:
  for iteration in iterations:
    for batch in target_data:
      # Forward pass
      scores = model(batch.images)
      logits = model(batch.images)
      # Compute forward mapping loss
      forward_loss = mse(scores, batch.scores)
      # Compute backward mapping loss
      backward_loss = cross_entropy(logits, batch.labels)
      # Compute auxiliary loss
      aux_loss = distance(model.characteristics, batch.characteristics)
      # Compute total loss
      total_loss = forward_loss - lambda * backward_loss + aux_loss
      # Backward pass
      total_loss.backward()
      # Update parameters
      optimizer.step()
      # Reset gradients
      optimizer.zero_grad()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np
import pandas as pd

# Define hyperparameters
num_models = 4 # number of U-Net architectures
num_epochs = 100 # number of epochs for pre-training
num_iterations = 1000 # number of iterations for bidirectional learning
batch_size = 32 # batch size for both pre-task and target task
learning_rate = 0.01 # learning rate for both pre-task and target task
lambda = 0.1 # weight for the backward mapping

# Load the pre-task data (BigEarthNet)
pre_task_data = torchvision.datasets.BigEarthNet(root='./data', download=True)

# Load the target data (natural hazard segmentation)
target_data = pd.read_csv('./data/target_data.csv')

# Define the models (U-Net architectures)
models = [torchvision.models.segmentation.unet(pretrained=True) for _ in range(num_models)]

# Define the optimizers (Adam)
optimizers = [torch.optim.Adam(model.parameters(), lr=learning_rate) for model in models]

# Define the loss functions (cross-entropy, mean squared error, and distance)
cross_entropy = torch.nn.CrossEntropyLoss()
mse = torch.nn.MSELoss()
distance = torch.nn.CosineSimilarity()

# Pre-train the models on the pre-task
for model in models:
  for epoch in range(num_epochs):
    for batch in torch.utils.data.DataLoader(pre_task_data, batch_size=batch_size, shuffle=True):
      # Forward pass
      logits = model(batch['image'])
      # Compute loss
      loss = cross_entropy(logits, batch['label'])
      # Backward pass
      loss.backward()
      # Update parameters
      optimizer.step()
      # Reset gradients
      optimizer.zero_grad()

# Adapt the models to the target domain using bidirectional learning
for model in models:
  for iteration in range(num_iterations):
    for batch in torch.utils.data.DataLoader(target_data, batch_size=batch_size, shuffle=True):
      # Forward pass
      scores = model(batch['image'])
      logits = model(batch['image'])
      # Compute forward mapping loss
      forward_loss = mse(scores, batch['score'])
      # Compute backward mapping loss
      backward_loss = cross_entropy(logits, batch['label'])
      # Compute auxiliary loss
      aux_loss = distance(model.characteristics, batch['characteristics'])
      # Compute total loss
      total_loss = forward_loss - lambda * backward_loss + aux_loss
      # Backward pass
      total_loss.backward()
      # Update parameters
      optimizer.step()
      # Reset gradients
      optimizer.zero_grad()
```