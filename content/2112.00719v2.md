---
title: 2112.00719v2 HyperInverter  Improving StyleGAN Inversion via Hypernetwork
date: 2021-12-01
---

# [HyperInverter: Improving StyleGAN Inversion via Hypernetwork](http://arxiv.org/abs/2112.00719v2)

authors: Tan M. Dinh, Anh Tuan Tran, Rang Nguyen, Binh-Son Hua


## What, Why and How

[1]: https://arxiv.org/abs/2112.00719v2 "HyperInverter: Improving StyleGAN Inversion via Hypernetwork"
[2]: https://arxiv.org/abs/2112.00719 "HyperInverter: Improving StyleGAN Inversion via Hypernetwork"
[3]: http://export.arxiv.org/abs/2012.00719v2 "[2012.00719v2] Gull's theorem revisited - export.arxiv.org"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method for GAN inversion, which is the task of finding the latent code that corresponds to a given real image in a GAN model.
- **Why**: GAN inversion is useful for real-world image manipulation, such as editing, enhancing, or transferring attributes of images. However, existing methods have limitations in terms of reconstruction quality, editability, or inference speed.
- **How**: The paper introduces a two-phase strategy called HyperInverter, which consists of an encoder and a series of hypernetworks. The encoder maps the input image to the StyleGAN2 -space, which has good editability but low reconstruction quality. The hypernetworks recover the missing information during inversion and improve the reconstruction quality. The method is encoder-based, which makes it fast and efficient.

I hope this summary helps you understand the paper better.ðŸ˜Š

## Main Contributions

[1]: https://arxiv.org/abs/2112.00719v2 "HyperInverter: Improving StyleGAN Inversion via Hypernetwork"
[2]: https://arxiv.org/abs/2112.00719 "HyperInverter: Improving StyleGAN Inversion via Hypernetwork"
[3]: http://export.arxiv.org/abs/2012.00719v2 "[2012.00719v2] Gull's theorem revisited - export.arxiv.org"

According to the paper at [^1^][1], the main contributions are:

- **A novel two-phase strategy for GAN inversion** that combines an encoder and a series of hypernetworks to achieve high reconstruction quality, editability, and fast inference.
- **A comprehensive analysis of the trade-off between reconstruction quality and editability** in different GAN latent spaces, and a demonstration of how hypernetworks can bridge the gap.
- **Extensive experiments on two challenging datasets** (FFHQ and LSUN Church) that show the superiority of the proposed method over existing state-of-the-art methods in terms of quantitative and qualitative evaluations.

## Method Summary

[1]: https://arxiv.org/abs/2112.00719v2 "HyperInverter: Improving StyleGAN Inversion via Hypernetwork"
[2]: https://arxiv.org/abs/2112.00719 "HyperInverter: Improving StyleGAN Inversion via Hypernetwork"
[3]: http://export.arxiv.org/abs/2012.00719v2 "[2012.00719v2] Gull's theorem revisited - export.arxiv.org"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper presents a two-phase strategy for GAN inversion, which consists of an encoder and a series of hypernetworks.
- The encoder is trained to map the input image to the StyleGAN2 -space, which is a disentangled and editable latent space. However, this space has low reconstruction quality due to the truncation trick used in StyleGAN2.
- The hypernetworks are trained to generate additional latent codes that are concatenated with the encoder output and fed into the StyleGAN2 generator. The hypernetworks are conditioned on the input image and the encoder output, and they aim to recover the missing information during inversion.
- The paper uses a reconstruction loss and a perceptual loss to train the encoder and the hypernetworks jointly. The paper also introduces a regularization term to prevent overfitting and improve editability.
- The paper evaluates the proposed method on two datasets: FFHQ and LSUN Church. The paper compares the method with existing state-of-the-art methods in terms of reconstruction quality, editability, and inference speed.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a real image x and a pre-trained StyleGAN2 generator G
# Output: a latent code z that can reconstruct x and edit its attributes

# Phase 1: train an encoder E to map x to the StyleGAN2 w-space
E = Encoder()
L_rec = ReconstructionLoss()
L_per = PerceptualLoss()
for epoch in epochs:
  for batch in batches:
    x = batch
    w = E(x) # encoder output
    x_hat = G(w) # reconstructed image
    loss = L_rec(x, x_hat) + L_per(x, x_hat) # total loss
    update E parameters using loss

# Phase 2: train a series of hypernetworks H to generate additional latent codes
H = HyperNetworks()
L_reg = RegularizationLoss()
for epoch in epochs:
  for batch in batches:
    x = batch
    w = E(x) # encoder output
    z = H(x, w) # hypernetwork output
    x_hat = G(z) # reconstructed image
    loss = L_rec(x, x_hat) + L_per(x, x_hat) + L_reg(z) # total loss
    update H parameters using loss

# Inference: given a new image x, find the latent code z that can reconstruct and edit it
x = new image
w = E(x) # encoder output
z = H(x, w) # hypernetwork output
x_hat = G(z) # reconstructed image
return z
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import libraries
import torch
import torchvision
import numpy as np
import stylegan2 # pre-trained StyleGAN2 generator

# Define constants
IMG_SIZE = 256 # image size
W_DIM = 512 # w-space dimension
Z_DIM = 512 # z-space dimension
H_DIM = 256 # hypernetwork hidden dimension
NUM_LAYERS = 18 # number of StyleGAN2 layers
NUM_HYPERNETS = 9 # number of hypernetworks
LAMBDA = 0.01 # regularization coefficient

# Define encoder network
class Encoder(torch.nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Use a ResNet-50 model as the backbone
    self.backbone = torchvision.models.resnet50(pretrained=True)
    # Replace the last fully connected layer with a linear layer
    self.fc = torch.nn.Linear(self.backbone.fc.in_features, W_DIM)
  
  def forward(self, x):
    # x: input image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
    # w: output latent code of shape (batch_size, W_DIM)
    x = self.backbone(x) # extract features from backbone
    w = self.fc(x) # map features to w-space
    return w

# Define hypernetwork module
class HyperNet(torch.nn.Module):
  def __init__(self):
    super(HyperNet, self).__init__()
    # Use two linear layers with ReLU activation as the hypernetwork
    self.linear1 = torch.nn.Linear(W_DIM + IMG_SIZE * IMG_SIZE * 3, H_DIM)
    self.relu = torch.nn.ReLU()
    self.linear2 = torch.nn.Linear(H_DIM, Z_DIM)
  
  def forward(self, x, w):
    # x: input image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
    # w: encoder output of shape (batch_size, W_DIM)
    # z: hypernetwork output of shape (batch_size, Z_DIM)
    x = x.flatten(start_dim=1) # flatten image to a vector
    xw = torch.cat([x, w], dim=1) # concatenate image and encoder output
    h = self.linear1(xw) # first linear layer
    h = self.relu(h) # ReLU activation
    z = self.linear2(h) # second linear layer
    return z

# Define hypernetworks network
class HyperNetworks(torch.nn.Module):
  def __init__(self):
    super(HyperNetworks, self).__init__()
    # Use a list of hypernetwork modules as the hypernetworks network
    self.hypernets = torch.nn.ModuleList([HyperNet() for _ in range(NUM_HYPERNETS)])
  
  def forward(self, x, w):
    # x: input image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
    # w: encoder output of shape (batch_size, W_DIM)
    # z: hypernetworks output of shape (batch_size, NUM_LAYERS * Z_DIM)
    z_list = [] # a list to store the outputs of each hypernetwork module
    for i in range(NUM_HYPERNETS):
      z_i = self.hypernets[i](x, w) # get the output of the i-th hypernetwork module
      z_list.append(z_i) # append it to the list
    z = torch.cat(z_list, dim=1) # concatenate the outputs along the second dimension
    return z

# Define reconstruction loss function
def ReconstructionLoss(x, x_hat):
  # x: input image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
  # x_hat: reconstructed image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
  # loss: reconstruction loss value (scalar)
  loss = torch.nn.functional.mse_loss(x_hat, x) # use mean squared error loss
  return loss

# Define perceptual loss function
def PerceptualLoss(x, x_hat):
  # x: input image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
  # x_hat: reconstructed image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
  # loss: perceptual loss value (scalar)
  vgg16 = torchvision.models.vgg16(pretrained=True) # use a pre-trained VGG-16 model as the feature extractor
  vgg16.eval() # set the model to evaluation mode
  x_features = vgg16.features(x) # extract features from the input image
  x_hat_features = vgg16.features(x_hat) # extract features from the reconstructed image
  loss = torch.nn.functional.mse_loss(x_hat_features, x_features) # use mean squared error loss
  return loss

# Define regularization loss function
def RegularizationLoss(z):
  # z: hypernetworks output of shape (batch_size, NUM_LAYERS * Z_DIM)
  # loss: regularization loss value (scalar)
  z_mean = torch.mean(z, dim=0) # compute the mean of z along the batch dimension
  z_std = torch.std(z, dim=0) # compute the standard deviation of z along the batch dimension
  loss = torch.sum((z_mean - 0) ** 2 + (z_std - 1) ** 2) # use the L2 norm of the deviation from the standard normal distribution
  return loss

# Define total loss function
def TotalLoss(x, x_hat, z):
  # x: input image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
  # x_hat: reconstructed image of shape (batch_size, 3, IMG_SIZE, IMG_SIZE)
  # z: hypernetworks output of shape (batch_size, NUM_LAYERS * Z_DIM)
  # loss: total loss value (scalar)
  loss_rec = ReconstructionLoss(x, x_hat) # reconstruction loss
  loss_per = PerceptualLoss(x, x_hat) # perceptual loss
  loss_reg = RegularizationLoss(z) # regularization loss
  loss = loss_rec + loss_per + LAMBDA * loss_reg # total loss with a coefficient for regularization term
  return loss

# Define optimizer
def Optimizer(E, H):
  # E: encoder network
  # H: hypernetworks network
  # optimizer: Adam optimizer for E and H parameters
  params = list(E.parameters()) + list(H.parameters()) # get the parameters of E and H
  optimizer = torch.optim.Adam(params, lr=0.0001, betas=(0.9, 0.999)) # use Adam optimizer with learning rate and betas
  return optimizer

# Define training procedure
def Train(E, H, G, epochs, batches):
  # E: encoder network
  # H: hypernetworks network
  # G: StyleGAN2 generator
  # epochs: number of epochs to train
  # batches: a data loader that provides batches of real images
  optimizer = Optimizer(E, H) # get the optimizer for E and H parameters
  for epoch in range(epochs):
    for batch in batches:
      x = batch # get a batch of real images
      w = E(x) # get the encoder output
      z = H(x, w) # get the hypernetworks output
      x_hat = G(z) # get the reconstructed image from StyleGAN2 generator
      loss = TotalLoss(x, x_hat, z) # compute the total loss
      optimizer.zero_grad() # clear the gradients of E and H parameters
      loss.backward() # compute the gradients of E and H parameters
      optimizer.step() # update the E and H parameters using the gradients

# Define inference procedure
def Inference(E, H, G, x):
  # E: encoder network
  # H: hypernetworks network
  # G: StyleGAN2 generator
  # x: input image of shape (1, 3, IMG_SIZE, IMG_SIZE)
  w = E(x) # get the encoder output
  z = H(x, w) # get the hypernetworks output
  x_hat = G(z) # get the reconstructed image from StyleGAN2 generator
  return z

# Load pre-trained StyleGAN2 generator for FFHQ dataset
G = stylegan2.load_pretrained('ffhq')

# Create encoder network and hypernetworks network
E = Encoder()
H = HyperNetworks()

# Load FFHQ dataset and create a data loader for batches of real images
dataset = torchvision.datasets.ImageFolder('ffhq', transform=torchvision.transforms.Resize(IMG_SIZE)) # load FFHQ dataset and resize images to IMG_SIZE
batches = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True) # create a data loader that provides batches of real images with batch size and shuffle

# Train encoder network and hypernetworks network for a number of epochs on FFHQ dataset
Train(E, H, G, epochs=1000, batches=batches)

# Given a new image x, find the latent code z that can reconstruct and edit it using StyleGAN2 generator 
x = new image of shape (1, 3, IMG_SIZE, IMG_SIZE)
z = Inference(E, H, G, x)
```