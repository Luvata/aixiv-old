---
title: 2306.00003v1 Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning
date: 2023-06-01
---

# [Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning](http://arxiv.org/abs/2306.00003v1)

authors: Zhe Huang, Benjamin S. Wessler, Michael C. Hughes


## What, Why and How

[1]: https://arxiv.org/abs/2306.00003 "Detecting Heart Disease from Multi-View Ultrasound Images via ..."
[2]: https://arxiv.org/pdf/2306.00003 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00003 "[2306.00003] Detecting Heart Disease from Multi-View Ultrasound Images ..."

Here is a summary of the paper:

- **What**: The paper proposes a new end-to-end multiple instance learning (MIL) approach for detecting aortic stenosis (AS) from multi-view ultrasound images of the heart[^1^][1].
- **Why**: AS is a common and deadly heart disease that is under-diagnosed and under-treated. Current methods for automatic screening rely on inflexible averages across images or perform poorly with attention-based MIL[^1^][1].
- **How**: The paper introduces two key innovations: (1) a supervised attention technique that guides the network to focus on relevant views of the aortic valve, and (2) a self-supervised pretraining strategy that applies contrastive learning on the representation of the whole study instead of individual images[^1^][1]. The paper evaluates the proposed approach on an open-access dataset and an external validation set and shows that it achieves higher accuracy and lower model size than previous methods[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2306.00003 "Detecting Heart Disease from Multi-View Ultrasound Images via ..."
[2]: https://arxiv.org/pdf/2306.00003 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00003 "[2306.00003] Detecting Heart Disease from Multi-View Ultrasound Images ..."

The paper claims the following contributions[^1^][1]:

- A new end-to-end MIL approach for AS detection that leverages supervised attention to focus on relevant views of the aortic valve and self-supervised pretraining to learn from the whole study representation.
- A novel self-supervised pretraining strategy that applies contrastive learning on the representation of the whole study instead of individual images as commonly done in prior literature.
- A comprehensive evaluation of the proposed approach on an open-access dataset and an external validation set, showing that it outperforms previous methods in terms of accuracy and model size.

## Method Summary

[1]: https://arxiv.org/abs/2306.00003 "Detecting Heart Disease from Multi-View Ultrasound Images via ..."
[2]: https://arxiv.org/pdf/2306.00003 "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00003 "[2306.00003] Detecting Heart Disease from Multi-View Ultrasound Images ..."

Here is a summary of the method section of the paper:

- The paper formulates the AS detection problem as a MIL problem, where each study is a bag of images and the task is to predict the study-level label based on the image-level features[^1^][1].
- The paper proposes a Supervised Attention MIL (SAMIL) approach that consists of three components: (1) a feature extractor that encodes each image into a feature vector, (2) an attention module that computes an attention weight for each image based on its feature vector and the study-level label, and (3) a classifier that aggregates the weighted feature vectors and predicts the study-level label[^1^][1].
- The paper introduces a supervised attention technique that uses the study-level label as an additional input to the attention module, which helps guide the attention mechanism to focus on relevant views of the aortic valve[^1^][1].
- The paper also proposes a self-supervised pretraining strategy that applies contrastive learning on the representation of the whole study instead of individual images. This strategy encourages the model to learn a discriminative representation of each study that is invariant to data augmentations and different from other studies[^1^][1].
- The paper implements the feature extractor using a ResNet-18 backbone and fine-tunes it on the target dataset. The attention module and the classifier are implemented using fully-connected layers with ReLU activations[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the feature extractor, attention module and classifier
feature_extractor = ResNet18(pretrained=True)
attention_module = FullyConnectedLayer(input_dim=512, output_dim=1)
classifier = FullyConnectedLayer(input_dim=512, output_dim=2)

# Define the loss functions
mil_loss = BinaryCrossEntropyLoss()
contrastive_loss = NTXentLoss()

# Define the optimizer
optimizer = Adam(feature_extractor.parameters() + attention_module.parameters() + classifier.parameters())

# Pretrain the model using contrastive learning
for epoch in range(pretrain_epochs):
  for batch in pretrain_loader:
    # Get the studies and labels
    studies, labels = batch
    # Augment the studies twice
    studies_1 = augment(studies)
    studies_2 = augment(studies)
    # Encode the studies using the feature extractor
    features_1 = feature_extractor(studies_1)
    features_2 = feature_extractor(studies_2)
    # Average the features across images
    features_1 = mean(features_1, dim=1)
    features_2 = mean(features_2, dim=1)
    # Compute the contrastive loss
    loss = contrastive_loss(features_1, features_2)
    # Update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Fine-tune the model using supervised attention MIL
for epoch in range(finetune_epochs):
  for batch in finetune_loader:
    # Get the studies and labels
    studies, labels = batch
    # Encode the studies using the feature extractor
    features = feature_extractor(studies)
    # Compute the attention weights using the attention module and the labels
    weights = attention_module(features, labels)
    # Apply softmax to normalize the weights
    weights = softmax(weights, dim=1)
    # Weighted average the features across images
    features = sum(features * weights, dim=1)
    # Predict the study-level labels using the classifier
    logits = classifier(features)
    # Compute the MIL loss
    loss = mil_loss(logits, labels)
    # Update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import DataLoader
from contrastive_learning import NTXentLoss # A custom implementation of the contrastive loss

# Define the hyperparameters
pretrain_epochs = 100 # The number of epochs for pretraining
finetune_epochs = 50 # The number of epochs for fine-tuning
batch_size = 32 # The batch size for both pretraining and fine-tuning
learning_rate = 0.001 # The learning rate for both pretraining and fine-tuning
temperature = 0.5 # The temperature parameter for the contrastive loss

# Define the data augmentations
augment = transforms.Compose([
  transforms.RandomHorizontalFlip(),
  transforms.RandomRotation(10),
  transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
  transforms.RandomResizedCrop(224),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Define the feature extractor, attention module and classifier
feature_extractor = torchvision.models.resnet18(pretrained=True)
feature_extractor.fc = nn.Identity() # Remove the last fully-connected layer
attention_module = nn.Linear(512 + 2, 1) # A fully-connected layer that takes the image feature and the study label as input and outputs an attention weight
classifier = nn.Linear(512, 2) # A fully-connected layer that takes the study feature as input and outputs a study label

# Define the loss functions
mil_loss = nn.BCEWithLogitsLoss() # The binary cross-entropy loss with logits for the MIL task
contrastive_loss = NTXentLoss(temperature) # The contrastive loss for the self-supervised pretraining

# Define the optimizer
optimizer = optim.Adam(feature_extractor.parameters() + attention_module.parameters() + classifier.parameters(), lr=learning_rate)

# Load the datasets and dataloaders
pretrain_dataset = AS_Dataset(pretrain=True) # A custom dataset class that loads the pretraining data
finetune_dataset = AS_Dataset(pretrain=False) # A custom dataset class that loads the fine-tuning data
pretrain_loader = DataLoader(pretrain_dataset, batch_size=batch_size, shuffle=True)
finetune_loader = DataLoader(finetune_dataset, batch_size=batch_size, shuffle=True)

# Pretrain the model using contrastive learning
for epoch in range(pretrain_epochs):
  for batch in pretrain_loader:
    # Get the studies and labels
    studies, labels = batch # studies is a tensor of shape (batch_size, num_images, 3, height, width), labels is a tensor of shape (batch_size, 2)
    # Augment the studies twice
    studies_1 = augment(studies) # Apply data augmentations to each image in each study
    studies_2 = augment(studies) # Apply data augmentations to each image in each study again
    # Encode the studies using the feature extractor
    features_1 = feature_extractor(studies_1.view(-1, 3, height, width)) # Reshape the studies to a tensor of shape (batch_size * num_images, 3, height, width) and encode each image into a feature vector of shape (512)
    features_2 = feature_extractor(studies_2.view(-1, 3, height, width)) # Reshape the studies to a tensor of shape (batch_size * num_images, 3, height, width) and encode each image into a feature vector of shape (512)
    features_1 = features_1.view(batch_size, num_images, -1) # Reshape the features to a tensor of shape (batch_size, num_images, 512)
    features_2 = features_2.view(batch_size, num_images, -1) # Reshape the features to a tensor of shape (batch_size, num_images, 512)
    # Average the features across images
    features_1 = torch.mean(features_1, dim=1) # Compute the mean of the features along the image dimension to get a tensor of shape (batch_size, 512)
    features_2 = torch.mean(features_2, dim=1) # Compute the mean of the features along the image dimension to get a tensor of shape (batch_size, 512)
    # Compute the contrastive loss
    loss = contrastive_loss(features_1, features_2) # Compute the contrastive loss between the two sets of features
    # Update the model parameters
    optimizer.zero_grad() # Zero out the gradients
    loss.backward() # Backpropagate the loss
    optimizer.step() # Update the parameters

# Fine-tune the model using supervised attention MIL
for epoch in range(finetune_epochs):
  for batch in finetune_loader:
    # Get the studies and labels
    studies, labels = batch # studies is a tensor of shape (batch_size, num_images, 3, height, width), labels is a tensor of shape (batch_size, 2)
    # Encode the studies using the feature extractor
    features = feature_extractor(studies.view(-1, 3, height, width)) # Reshape the studies to a tensor of shape (batch_size * num_images, 3, height, width) and encode each image into a feature vector of shape (512)
    features = features.view(batch_size, num_images, -1) # Reshape the features to a tensor of shape (batch_size, num_images, 512)
    # Compute the attention weights using the attention module and the labels
    weights = attention_module(torch.cat([features, labels.unsqueeze(1).repeat(1, num_images, 1)], dim=-1)) # Concatenate the features and the labels along the last dimension and pass them to the attention module to get a tensor of shape (batch_size, num_images, 1)
    # Apply softmax to normalize the weights
    weights = F.softmax(weights, dim=1) # Apply softmax along the image dimension to get a tensor of shape (batch_size, num_images, 1) with values between 0 and 1 that sum to 1 for each study
    # Weighted average the features across images
    features = torch.sum(features * weights, dim=1) # Multiply the features and the weights element-wise and sum them along the image dimension to get a tensor of shape (batch_size, 512)
    # Predict the study-level labels using the classifier
    logits = classifier(features) # Pass the features to the classifier to get a tensor of shape (batch_size, 2) with logits for each class
    # Compute the MIL loss
    loss = mil_loss(logits, labels) # Compute the binary cross-entropy loss with logits between the predicted logits and the true labels
    # Update the model parameters
    optimizer.zero_grad() # Zero out the gradients
    loss.backward() # Backpropagate the loss
    optimizer.step() # Update the parameters
```