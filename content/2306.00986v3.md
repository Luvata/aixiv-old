---
title: 2306.00986v3 Diffusion Self-Guidance for Controllable Image Generation
date: 2023-06-01
---

# [Diffusion Self-Guidance for Controllable Image Generation](http://arxiv.org/abs/2306.00986v3)

authors: Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, Aleksander Holynski


## What, Why and How

[1]: https://arxiv.org/abs/2306.00986 "Diffusion Self-Guidance for Controllable Image Generation"
[2]: https://arxiv.org/pdf/2306.00986.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/2306.00986 "export.arxiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces **self-guidance**, a method that provides greater control over generated images by guiding the internal representations of diffusion models.
- **Why**: The paper aims to address the challenge of generating images that match the user's desired specifications, especially when some aspects of an image are difficult or impossible to convey through text.
- **How**: The paper demonstrates that properties such as the shape, location, and appearance of objects can be extracted from the internal representations of diffusion models and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. The paper shows how self-guidance can be used to perform various image manipulations and edits on synthetic and real images.



## Main Contributions

[1]: https://arxiv.org/abs/2306.00986 "Diffusion Self-Guidance for Controllable Image Generation"
[2]: https://arxiv.org/pdf/2306.00986.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/2306.00986 "export.arxiv.org"

According to the paper[^1^][1], the main contributions are:

- **Introducing self-guidance**, a method that provides greater control over generated images by guiding the internal representations of diffusion models.
- **Demonstrating self-guidance on various image manipulation tasks**, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more.
- **Showing that self-guidance can be used to edit real images**, by applying it to a pretrained diffusion model without any additional training or models.
- **Providing an interactive demo** that allows users to experiment with self-guidance on various image generation and editing tasks. The demo is available at this https URL.


## Method Summary

[1]: https://arxiv.org/abs/2306.00986 "Diffusion Self-Guidance for Controllable Image Generation"
[2]: https://arxiv.org/pdf/2306.00986.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/2306.00986 "export.arxiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses **diffusion models** as the base generative model, which are trained to model the distribution of natural images by reversing a Markov chain that gradually adds Gaussian noise to an image until it becomes pure noise.
- The paper proposes **self-guidance**, a method that allows users to control the sampling process of diffusion models by providing guidance signals at different stages of the Markov chain. These signals are derived from the internal representations of the diffusion model itself, and can be used to modify various properties of the generated image, such as shape, location, and appearance of objects.
- The paper defines **three types of self-guidance signals**: **mask guidance**, **feature guidance**, and **image guidance**. Mask guidance allows users to specify a binary mask that indicates which regions of the image to modify or preserve. Feature guidance allows users to specify a feature vector that represents the desired appearance of an object or region. Image guidance allows users to provide an external image that serves as a reference for the appearance or layout of the generated image.
- The paper describes **how to extract and apply self-guidance signals** from the diffusion model. To extract mask guidance, the paper uses a segmentation head that is trained on top of the diffusion model to predict semantic masks for different objects. To extract feature guidance, the paper uses a feature extractor that is trained on top of the diffusion model to encode images into feature vectors. To apply self-guidance signals, the paper modifies the sampling process of the diffusion model by adding an extra term to the denoising score function that encourages the generated image to match the guidance signals.
- The paper shows **how to compose self-guidance signals** to perform complex image manipulations, such as moving or resizing objects, swapping or blending appearances, composing objects from multiple images, and more. The paper also shows how to use self-guidance to edit real images by applying it to a pretrained diffusion model without any additional training or models.

## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper:

```
# Define a diffusion model that reverses a Markov chain of adding Gaussian noise to an image
diffusion_model = DiffusionModel()

# Define a segmentation head that predicts semantic masks for different objects
segmentation_head = SegmentationHead(diffusion_model)

# Define a feature extractor that encodes images into feature vectors
feature_extractor = FeatureExtractor(diffusion_model)

# Define a function that modifies the denoising score function of the diffusion model
def modify_score_function(score_function, guidance_signals):
  # Add an extra term to the score function that encourages the generated image to match the guidance signals
  modified_score_function = score_function + lambda * guidance_signals
  return modified_score_function

# Define a function that generates an image using self-guidance
def generate_image(diffusion_model, guidance_signals):
  # Initialize an image with pure noise
  image = sample_noise()
  # Reverse the Markov chain by applying the modified score function at each step
  for t in reversed(range(T)):
    # Get the score function at time step t
    score_function = diffusion_model.get_score_function(t)
    # Modify the score function with the guidance signals
    modified_score_function = modify_score_function(score_function, guidance_signals)
    # Sample the next image by applying the modified score function
    image = sample_image(modified_score_function, image)
  return image

# Define a function that extracts and applies mask guidance
def mask_guidance(diffusion_model, segmentation_head, mask):
  # Extract the mask guidance signal from the segmentation head
  mask_guidance_signal = segmentation_head.get_mask_guidance(mask)
  # Generate an image using self-guidance with mask guidance signal
  image = generate_image(diffusion_model, mask_guidance_signal)
  return image

# Define a function that extracts and applies feature guidance
def feature_guidance(diffusion_model, feature_extractor, feature_vector):
  # Extract the feature guidance signal from the feature extractor
  feature_guidance_signal = feature_extractor.get_feature_guidance(feature_vector)
  # Generate an image using self-guidance with feature guidance signal
  image = generate_image(diffusion_model, feature_guidance_signal)
  return image

# Define a function that extracts and applies image guidance
def image_guidance(diffusion_model, feature_extractor, reference_image):
  # Extract the feature vector from the reference image using the feature extractor
  feature_vector = feature_extractor.encode(reference_image)
  # Extract and apply feature guidance with the feature vector
  image = feature_guidance(diffusion_model, feature_extractor, feature_vector)
  return image

# Define a function that composes self-guidance signals to perform complex image manipulations
def compose_self_guidance(diffusion_model, segmentation_head, feature_extractor, images, masks, features):
  # Initialize an empty guidance signal
  guidance_signal = None
  # Loop through each input image and its corresponding mask and feature
  for image, mask, feature in zip(images, masks, features):
    # Extract and apply mask guidance with the mask
    masked_image = mask_guidance(diffusion_model, segmentation_head, mask)
    # Extract and apply feature guidance with the masked image and the feature
    guided_image = feature_guidance(diffusion_model, feature_extractor, masked_image * feature)
    # Add the guided image to the guidance signal
    if guidance_signal is None:
      guidance_signal = guided_image
    else:
      guidance_signal += guided_image
  # Generate an image using self-guidance with the composed guidance signal
  image = generate_image(diffusion_model, guidance_signal)
  return image

```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
T = 1000 # Number of time steps in the Markov chain
sigma = 0.01 # Standard deviation of the Gaussian noise
beta = 0.0001 # Weight of the guidance signal in the score function
lambda = 0.1 # Weight of the feature guidance signal in the image guidance signal

# Define a diffusion model that reverses a Markov chain of adding Gaussian noise to an image
class DiffusionModel(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define a convolutional neural network that takes an image and a time step as inputs and outputs a score function
    self.cnn = torchvision.models.resnet50(pretrained=True)
    self.cnn.fc = torch.nn.Linear(self.cnn.fc.in_features, 3 * 256 * 256)

  def forward(self, x, t):
    # Concatenate the image and the time step along the channel dimension
    x = torch.cat([x, t * torch.ones_like(x[:, :1])], dim=1)
    # Pass the input through the cnn and reshape the output to match the image size
    score_function = self.cnn(x).view(-1, 3, 256, 256)
    return score_function

  def get_score_function(self, t):
    # Define a function that returns the score function at a given time step
    def score_function(x):
      return self.forward(x, t / T)
    return score_function

# Define a segmentation head that predicts semantic masks for different objects
class SegmentationHead(torch.nn.Module):
  def __init__(self, diffusion_model):
    super().__init__()
    # Use the diffusion model as a feature extractor
    self.feature_extractor = diffusion_model.cnn
    # Freeze the parameters of the feature extractor
    for param in self.feature_extractor.parameters():
      param.requires_grad = False
    # Define a convolutional neural network that takes the features and outputs a segmentation mask
    self.cnn = torchvision.models.segmentation.fcn_resnet50(pretrained=True)
    self.cnn.backbone = self.feature_extractor

  def forward(self, x):
    # Pass the input through the cnn and return the segmentation mask
    segmentation_mask = self.cnn(x)['out']
    return segmentation_mask

  def get_mask_guidance(self, mask):
    # Define a function that returns the mask guidance signal for a given mask
    def mask_guidance_signal(x):
      # Compute the segmentation mask for the input image
      segmentation_mask = self.forward(x)
      # Compute the cross entropy loss between the segmentation mask and the target mask
      loss = torch.nn.functional.cross_entropy(segmentation_mask, mask)
      # Compute the gradient of the loss with respect to the input image
      gradient = torch.autograd.grad(loss, x)[0]
      # Return the gradient as the mask guidance signal
      return gradient
    return mask_guidance_signal

# Define a feature extractor that encodes images into feature vectors
class FeatureExtractor(torch.nn.Module):
  def __init__(self, diffusion_model):
    super().__init__()
    # Use the diffusion model as a feature extractor
    self.feature_extractor = diffusion_model.cnn
    # Freeze the parameters of the feature extractor
    for param in self.feature_extractor.parameters():
      param.requires_grad = False

  def forward(self, x):
    # Pass the input through the feature extractor and return the feature vector
    feature_vector = self.feature_extractor(x)
    return feature_vector

  def encode(self, x):
    # Define a function that encodes an image into a feature vector
    return self.forward(x)

  def get_feature_guidance(self, feature_vector):
    # Define a function that returns the feature guidance signal for a given feature vector
    def feature_guidance_signal(x):
      # Compute the feature vector for the input image
      x_feature_vector = self.forward(x)
      # Compute the mean squared error between the feature vectors
      loss = torch.nn.functional.mse_loss(x_feature_vector, feature_vector)
      # Compute the gradient of the loss with respect to the input image
      gradient = torch.autograd.grad(loss, x)[0]
      # Return the gradient as the feature guidance signal
      return gradient
    return feature_guidance_signal

# Define a function that modifies the denoising score function of the diffusion model
def modify_score_function(score_function, guidance_signals):
  # Add an extra term to the score function that encourages the generated image to match the guidance signals
  modified_score_function = score_function + beta * sum(guidance_signals)
  return modified_score_function

# Define a function that generates an image using self-guidance
def generate_image(diffusion_model, guidance_signals):
  # Initialize an image with pure noise
  image = torch.randn(1, 3, 256, 256)
  # Reverse the Markov chain by applying the modified score function at each step
  for t in reversed(range(T)):
    # Get the score function at time step t
    score_function = diffusion_model.get_score_function(t)
    # Modify the score function with the guidance signals
    modified_score_function = modify_score_function(score_function, guidance_signals)
    # Sample the next image by applying the modified score function
    image = sample_image(modified_score_function, image)
  return image

# Define a function that samples an image by applying a score function
def sample_image(score_function, image):
  # Compute the standard deviation of the Gaussian noise at the current time step
  std = sigma * np.sqrt(T) / np.sqrt(t + 1)
  # Compute the mean of the Gaussian distribution by applying the score function to the image
  mean = image - std**2 * score_function(image)
  # Sample a new image from the Gaussian distribution
  new_image = torch.normal(mean, std)
  return new_image

# Define a function that extracts and applies mask guidance
def mask_guidance(diffusion_model, segmentation_head, mask):
  # Extract the mask guidance signal from the segmentation head
  mask_guidance_signal = segmentation_head.get_mask_guidance(mask)
  # Generate an image using self-guidance with mask guidance signal
  image = generate_image(diffusion_model, [mask_guidance_signal])
  return image

# Define a function that extracts and applies feature guidance
def feature_guidance(diffusion_model, feature_extractor, feature_vector):
  # Extract the feature guidance signal from the feature extractor
  feature_guidance_signal = feature_extractor.get_feature_guidance(feature_vector)
  # Generate an image using self-guidance with feature guidance signal
  image = generate_image(diffusion_model, [feature_guidance_signal])
  return image

# Define a function that extracts and applies image guidance
def image_guidance(diffusion_model, feature_extractor, reference_image):
  # Extract the feature vector from the reference image using the feature extractor
  feature_vector = feature_extractor.encode(reference_image)
  # Extract and apply feature guidance with the feature vector
  image = feature_guidance(diffusion_model, feature_extractor, feature_vector * lambda)
  return image

# Define a function that composes self-guidance signals to perform complex image manipulations
def compose_self_guidance(diffusion_model, segmentation_head, feature_extractor, images, masks, features):
  # Initialize an empty list of guidance signals
  guidance_signals = []
  # Loop through each input image and its corresponding mask and feature
  for image, mask, feature in zip(images, masks, features):
    # Extract and apply mask guidance with the mask
    masked_image = mask_guidance(diffusion_model, segmentation_head, mask)
    # Extract and apply feature guidance with the masked image and the feature
    guided_image = feature_guidance(diffusion_model, feature_extractor, masked_image * feature)
    # Add the guided image to the list of guidance signals
    guidance_signals.append(guided_image)
  # Generate an image using self-guidance with the composed guidance signals
  image = generate_image(diffusion_model, guidance_signals)
  return image

```