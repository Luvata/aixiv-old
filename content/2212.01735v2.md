---
title: 2212.01735v2 Neural Fourier Filter Bank
date: 2022-12-02
---

# [Neural Fourier Filter Bank](http://arxiv.org/abs/2212.01735v2)

authors: Zhijie Wu, Yuhe Jin, Kwang Moo Yi


## What, Why and How

[1]: https://arxiv.org/abs/2212.01735 "[2212.01735] Neural Fourier Filter Bank - arXiv.org"
[2]: https://arxiv.org/pdf/2212.01735.pdf "arXiv:2212.01735v2 [cs.CV] 25 Apr 2023"
[3]: http://export.arxiv.org/abs/2209.01735v2 "[2209.01735v2] Maximal domains of solutions for analytic quasilinear ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a novel method called **Neural Fourier Filter Bank (NFFB)** that can provide efficient and highly detailed reconstructions of signals such as images and 3D shapes. The method is inspired by wavelets and learns a neural field that decomposes the signal both spatially and frequency-wise.
- **Why**: The paper aims to address the limitations of existing neural field methods that either rely only on spatial decomposition or frequency encodings, which provide suboptimal performance and convergence. The paper also aims to demonstrate the advantages of NFFB over the state of the art on multiple tasks and datasets.
- **How**: The paper follows the recent grid-based paradigm for spatial decomposition, but unlike existing work, encourages specific frequencies to be stored in each grid via Fourier features encodings. The paper then applies a multi-layer perceptron with sine activations, taking these Fourier encoded features in at appropriate layers so that higher-frequency components are accumulated on top of lower-frequency components sequentially, which are summed up to form the final output. The paper evaluates NFFB on 2D image fitting, 3D shape reconstruction, and neural radiance fields, and shows that NFFB outperforms the state of the art regarding model compactness and convergence speed.

## Main Contributions

According to the paper, the main contributions are:

- **A novel method for signal reconstruction that jointly performs spatial and frequency-wise decomposition inspired by wavelets.**
- **A simple and effective way to encode the input positions with Fourier features and inject them into a sine-activated MLP at appropriate layers.**
- **Extensive experiments on various tasks and datasets that demonstrate the superiority of NFFB over existing methods in terms of quality, efficiency, and scalability.**

## Method Summary

The method section of the paper consists of four subsections:

- **Preliminaries**: The paper reviews the basics of neural fields and grid-based methods, and introduces the notation and problem formulation used in the paper.
- **Neural Fourier Filter Bank**: The paper describes the proposed NFFB method in detail, including the spatial and frequency-wise decomposition, the Fourier features encoding, and the MLP architecture. The paper also provides some theoretical analysis and insights on NFFB.
- **Implementation Details**: The paper discusses some practical aspects of implementing NFFB, such as the choice of grid size, frequency range, and loss function. The paper also explains how to apply NFFB to different tasks such as image fitting, shape reconstruction, and neural radiance fields.
- **Comparison with Related Methods**: The paper compares NFFB with several existing methods that are related to NFFB in terms of motivation, design, or performance. The paper highlights the differences and similarities between NFFB and these methods, and explains why NFFB is superior or complementary to them.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a set of positions x and corresponding signals y
# Output: a neural field f that can reconstruct y from x
# Hyperparameters: grid size G, frequency range F, MLP layers L

# Initialize a grid-based neural field f with L layers and sine activations
f = GridNeuralField(L, sine)

# For each grid cell c in f
for c in f.cells:

  # Initialize a random frequency vector fc in F
  fc = random_vector(F)

  # For each layer l in f
  for l in f.layers:

    # If l is the first or the last layer
    if l == 0 or l == L - 1:

      # Do nothing
      pass

    # Else
    else:

      # Inject fc into the input of layer l
      f.inject(fc, l)

# Train f on x and y using a loss function (e.g., L1 or L2)
f.train(x, y, loss)

# Return f as the output
return f
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Define the grid-based neural field class
class GridNeuralField(nn.Module):

  # Initialize the neural field with grid size, frequency range, and MLP layers
  def __init__(self, grid_size, freq_range, mlp_layers):

    # Call the parent class constructor
    super(GridNeuralField, self).__init__()

    # Store the grid size and frequency range as attributes
    self.grid_size = grid_size
    self.freq_range = freq_range

    # Create a list of grid cells as submodules
    self.cells = nn.ModuleList()

    # For each cell in the grid
    for i in range(grid_size):

      # Create a MLP with sine activations and append it to the list
      self.cells.append(SineMLP(mlp_layers))

      # Create a random frequency vector and store it as an attribute
      self.cells[i].freq = torch.rand(freq_range.shape) * (freq_range[1] - freq_range[0]) + freq_range[0]

  # Define the forward pass of the neural field
  def forward(self, x):

    # Initialize an empty output tensor
    y = torch.zeros(x.shape[0], 1)

    # For each cell in the grid
    for i in range(self.grid_size):

      # Get the cell output by passing x through the MLP
      cell_output = self.cells[i](x)

      # Add the cell output to the output tensor
      y += cell_output

    # Return the output tensor
    return y

# Define the sine MLP class
class SineMLP(nn.Module):

  # Initialize the MLP with layer sizes and sine activations
  def __init__(self, layer_sizes):

    # Call the parent class constructor
    super(SineMLP, self).__init__()

    # Create a list of linear layers as submodules
    self.layers = nn.ModuleList()

    # For each pair of consecutive layer sizes
    for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):

      # Create a linear layer and append it to the list
      self.layers.append(nn.Linear(in_size, out_size))

  # Define the forward pass of the MLP
  def forward(self, x):

    # For each layer in the MLP except the last one
    for layer in self.layers[:-1]:

      # Pass x through the layer and apply sine activation
      x = torch.sin(layer(x))

      # If the layer has a frequency attribute
      if hasattr(layer, 'freq'):

        # Multiply x by the frequency vector element-wise
        x *= layer.freq

    # Pass x through the last layer without activation
    x = self.layers[-1](x)

    # Return x as the output
    return x

# Define a function to encode positions with Fourier features
def encode_position(x, freq):

  # Compute the sine and cosine of x times freq element-wise
  sin_x = torch.sin(x * freq)
  cos_x = torch.cos(x * freq)

  # Concatenate sin_x and cos_x along the last dimension
  x_enc = torch.cat([sin_x, cos_x], dim=-1)

  # Return x_enc as the output
  return x_enc

# Define a function to inject frequency vectors into MLP layers
def inject_freq(mlp, freq, layers):

  # For each layer index in layers
  for i in layers:

    # Get the corresponding layer from the MLP
    layer = mlp.layers[i]

    # Set the frequency vector as an attribute of the layer
    layer.freq = freq

# Define a function to train a neural field on positions and signals using a loss function and an optimizer
def train_neural_field(nf, x, y, loss_fn, optimizer, epochs):

  # For each epoch in epochs
  for epoch in range(epochs):

    # Zero the gradients of the optimizer
    optimizer.zero_grad()

    # Forward pass: get the neural field output for x
    y_pred = nf(x)

    # Compute the loss between y_pred and y using loss_fn
    loss = loss_fn(y_pred, y)

    # Backward pass: compute the gradients of loss with respect to nf parameters
    loss.backward()

    # Update nf parameters using optimizer step function
    optimizer.step()

    # Print epoch number and loss value every 10 epochs 
    if epoch % 10 == 0:
      print(f'Epoch {epoch}, Loss {loss.item()}')

# Define the hyperparameters
grid_size = 16 # number of grid cells
freq_range = (0.1, 10.0) # range of frequency vectors
mlp_layers = [2, 256, 256, 1] # sizes of MLP layers
learning_rate = 0.001 # learning rate for optimizer
epochs = 1000 # number of training epochs

# Create a random dataset of positions and signals
x = torch.rand(1000, 2) # positions in [0, 1]^2
y = torch.sin(2 * np.pi * x[:, 0]) * torch.cos(4 * np.pi * x[:, 1]) # signals as a function of positions
y = y.unsqueeze(-1) # add an extra dimension for signals

# Encode the positions with Fourier features using a random frequency vector
freq = torch.rand(2) * (freq_range[1] - freq_range[0]) + freq_range[0] # frequency vector in freq_range
x_enc = encode_position(x, freq) # encoded positions

# Create a grid-based neural field
nf = GridNeuralField(grid_size, freq_range, mlp_layers)

# Inject the frequency vector into the first and the last layer of each cell's MLP
inject_freq(nf, freq, [0, -1])

# Create a loss function (e.g., L1 loss)
loss_fn = nn.L1Loss()

# Create an optimizer (e.g., Adam)
optimizer = optim.Adam(nf.parameters(), lr=learning_rate)

# Train the neural field on the dataset
train_neural_field(nf, x_enc, y, loss_fn, optimizer, epochs)
```