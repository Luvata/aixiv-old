---
title: 2009.01579v1 DESC  Domain Adaptation for Depth Estimation via Semantic Consistency
date: 2020-09-02
---

# [DESC: Domain Adaptation for Depth Estimation via Semantic Consistency](http://arxiv.org/abs/2009.01579v1)

authors: Adrian Lopez-Rodriguez, Krystian Mikolajczyk


## What, Why and How

[1]: https://arxiv.org/pdf/2009.01579v1.pdf "DESC: Domain Adaptation for Depth Estimation via Semantic ... - arXiv.org"
[2]: https://arxiv.org/abs/2009.01579 "[2009.01579] DESC: Domain Adaptation for Depth Estimation ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2302.01579v1 "[2302.01579v1] Semantic 3D-aware Portrait Synthesis and Manipulation ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a domain adaptation approach to train a monocular depth estimation model using a fully-annotated source dataset and a non-annotated target dataset.
- **Why**: The paper aims to overcome the problem of obtaining accurate real depth annotations, which are costly and time-consuming, and the domain gap between synthetic and real data.
- **How**: The paper bridges the domain gap by leveraging semantic predictions and low-level edge features to provide guidance for the target domain. It enforces consistency between the main model and a second model trained with semantic segmentation and edge maps, and introduces priors in the form of instance heights. It evaluates its approach on standard domain adaptation benchmarks for monocular depth estimation and shows consistent improvement upon the state-of-the-art.

## Main Contributions

[1]: https://arxiv.org/pdf/2009.01579v1.pdf "DESC: Domain Adaptation for Depth Estimation via Semantic ... - arXiv.org"
[2]: https://arxiv.org/abs/2009.01579 "[2009.01579] DESC: Domain Adaptation for Depth Estimation ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2302.01579v1 "[2302.01579v1] Semantic 3D-aware Portrait Synthesis and Manipulation ..."

According to the paper at [^1^][1], the main contributions are:

- **A novel domain adaptation framework for monocular depth estimation** that leverages semantic predictions and low-level edge features to provide guidance for the target domain.
- **A consistency loss** that enforces the main model and a second model trained with semantic segmentation and edge maps to produce similar depth predictions for the target domain.
- **A height prior loss** that introduces a geometric constraint based on the instance heights to regularize the depth predictions for the target domain.
- **Extensive experiments** on standard domain adaptation benchmarks for monocular depth estimation that show consistent improvement upon the state-of-the-art.

## Method Summary

[1]: https://arxiv.org/pdf/2009.01579v1.pdf "DESC: Domain Adaptation for Depth Estimation via Semantic ... - arXiv.org"
[2]: https://arxiv.org/abs/2009.01579 "[2009.01579] DESC: Domain Adaptation for Depth Estimation ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2302.01579v1 "[2302.01579v1] Semantic 3D-aware Portrait Synthesis and Manipulation ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper adopts a **two-stage domain adaptation framework** that consists of a main model and a second model. The main model is trained on the source dataset with depth supervision and on the target dataset with self-supervision. The second model is trained on both datasets with semantic segmentation and edge detection supervision.
- The paper introduces a **consistency loss** that enforces the main model and the second model to produce similar depth predictions for the target domain. The consistency loss is computed as the L1 distance between the depth maps generated by the two models, weighted by a confidence map that reflects the semantic and edge features of the target image.
- The paper also introduces a **height prior loss** that regularizes the depth predictions for the target domain by using a geometric constraint based on the instance heights. The height prior loss is computed as the L1 distance between the predicted height of each instance and its average height in the source dataset, normalized by the camera focal length and the depth value.
- The paper evaluates its approach on two standard domain adaptation benchmarks for monocular depth estimation: SYNTHIA-to-Cityscapes and KITTI-to-Cityscapes. It compares its results with several baselines and state-of-the-art methods, and shows consistent improvement in terms of depth accuracy and visual quality.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the main model and the second model
main_model = DepthEstimator()
second_model = SemanticEdgeDetector()

# Define the losses
depth_loss = L1Loss()
consistency_loss = L1Loss()
height_prior_loss = L1Loss()

# Define the datasets
source_dataset = SyntheticDataset()
target_dataset = RealDataset()

# Define the optimizers
main_optimizer = Adam(main_model.parameters())
second_optimizer = Adam(second_model.parameters())

# Train the models
for epoch in range(num_epochs):
  # Train on the source dataset with depth supervision
  for image, depth in source_dataset:
    # Predict depth from image
    pred_depth = main_model(image)
    # Compute depth loss
    loss = depth_loss(pred_depth, depth)
    # Update main model parameters
    main_optimizer.zero_grad()
    loss.backward()
    main_optimizer.step()

  # Train on the target dataset with self-supervision, consistency loss and height prior loss
  for image in target_dataset:
    # Predict depth from image using main model
    pred_depth_main = main_model(image)
    # Predict semantic segmentation and edge detection from image using second model
    pred_semantic, pred_edge = second_model(image)
    # Predict depth from image using second model with semantic and edge features as input
    pred_depth_second = second_model(image, pred_semantic, pred_edge)
    # Compute consistency loss
    loss_consistency = consistency_loss(pred_depth_main, pred_depth_second, confidence_map(pred_semantic, pred_edge))
    # Compute height prior loss
    loss_height_prior = height_prior_loss(pred_depth_main, instance_heights(image))
    # Compute total loss
    loss = loss_consistency + lambda * loss_height_prior
    # Update main model and second model parameters
    main_optimizer.zero_grad()
    second_optimizer.zero_grad()
    loss.backward()
    main_optimizer.step()
    second_optimizer.step()

# Evaluate the models on the target dataset
for image in target_dataset:
  # Predict depth from image using main model
  pred_depth = main_model(image)
  # Compare with ground truth depth if available
  evaluate(pred_depth, ground_truth_depth(image))
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms

# Define the hyperparameters
num_epochs = 20 # Number of training epochs
batch_size = 16 # Batch size for training and evaluation
learning_rate = 0.0001 # Learning rate for optimizers
lambda = 0.1 # Weight for the height prior loss
height_mean = 1.6 # Average height of a person in meters
focal_length = 721 # Focal length of the camera in pixels

# Define the main model as a depth estimator based on ResNet-50
class DepthEstimator(nn.Module):
  def __init__(self):
    super(DepthEstimator, self).__init__()
    # Use a pretrained ResNet-50 as the encoder
    self.encoder = models.resnet50(pretrained=True)
    # Remove the last layer of the encoder
    self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])
    # Define the decoder as a series of convolutional and upsampling layers
    self.decoder = nn.Sequential(
      nn.Conv2d(2048, 1024, kernel_size=1),
      nn.ReLU(),
      nn.Upsample(scale_factor=2),
      nn.Conv2d(1024, 512, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Upsample(scale_factor=2),
      nn.Conv2d(512, 256, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Upsample(scale_factor=2),
      nn.Conv2d(256, 128, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Upsample(scale_factor=2),
      nn.Conv2d(128, 64, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Upsample(scale_factor=2),
      nn.Conv2d(64, 1, kernel_size=3, padding=1),
      nn.Sigmoid()
    )

  def forward(self, x):
    # Encode the input image
    x = self.encoder(x)
    # Decode the encoded features into a depth map
    x = self.decoder(x)
    return x

# Define the second model as a semantic and edge detector based on U-Net
class SemanticEdgeDetector(nn.Module):
  def __init__(self):
    super(SemanticEdgeDetector, self).__init__()
    # Define the encoder as a series of convolutional and max pooling layers
    self.encoder1 = nn.Sequential(
      nn.Conv2d(3, 64, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Conv2d(64, 64, kernel_size=3, padding=1),
      nn.ReLU()
    )
    self.pool1 = nn.MaxPool2d(kernel_size=2)
    self.encoder2 = nn.Sequential(
      nn.Conv2d(64, 128, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Conv2d(128, 128, kernel_size=3, padding=1),
      nn.ReLU()
    )
    self.pool2 = nn.MaxPool2d(kernel_size=2)
    self.encoder3 = nn.Sequential(
      nn.Conv2d(128, 256, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Conv2d(256, 256, kernel_size=3, padding=1),
      nn.ReLU()
    )
    self.pool3 = nn.MaxPool2d(kernel_size=2)
    self.encoder4 = nn.Sequential(
      nn.Conv2d(256, 512, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Conv2d(512, 512, kernel_size=3, padding=1),
      nn.ReLU()
    )
    self.pool4 = nn.MaxPool2d(kernel_size=2)
    self.bottleneck = nn.Sequential(
      nn.Conv2d(512, 1024, kernel_size=3, padding=1),
      nn.ReLU(),
      nn.Conv2d(1024, 1024, kernel_size=3, padding=1),
      nn.ReLU()
    )
    
    # Define the decoder as a series of convolutional and upsampling layers with skip connections
    self.upconv4 = nn.ConvTranspose2d(1024, 512,
kernel_size=2,
stride=2)
    self.decoder4 =nn.Sequential(
nn.Conv2d(1024,
512,
kernel_size=3,
padding=1),
nn.ReLU(),
nn.Conv2d(512, 512, kernel_size=3, padding=1),
nn.ReLU()
)
self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
self.decoder3 = nn.Sequential(
nn.Conv2d(512, 256, kernel_size=3, padding=1),
nn.ReLU(),
nn.Conv2d(256, 256, kernel_size=3, padding=1),
nn.ReLU()
)
self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
self.decoder2 = nn.Sequential(
nn.Conv2d(256, 128, kernel_size=3, padding=1),
nn.ReLU(),
nn.Conv2d(128, 128, kernel_size=3, padding=1),
nn.ReLU()
)
self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
self.decoder1 = nn.Sequential(
nn.Conv2d(128, 64, kernel_size=3, padding=1),
nn.ReLU(),
nn.Conv2d(64, 64, kernel_size=3, padding=1),
nn.ReLU()
)

# Define the output layers for semantic segmentation and edge detection
self.semantic_output = nn.Conv2d(64, num_classes,
kernel_size=1)
self.edge_output = nn.Conv2d(64, 1,
kernel_size=1)

def forward(self, x):
# Encode the input image
x1 = self.encoder1(x)
x = self.pool1(x1)
x2 = self.encoder2(x)
x = self.pool2(x2)
x3 = self.encoder3(x)
x = self.pool3(x3)
x4 = self.encoder4(x)
x = self.pool4(x4)
x = self.bottleneck(x)

# Decode the encoded features into semantic and edge maps
x = self.upconv4(x)
x = torch.cat([x4,x], dim=1)
x = self.decoder4(x)
x = self.upconv3(x)
x = torch.cat([x3,x], dim=1)
x = self.decoder3(x)
x = self.upconv2(x)
x = torch.cat([x2,x], dim=1)
x = self.decoder2(x)
x = self.upconv1(x)
x = torch.cat([x1,x], dim=1)
x = self.decoder1(x)

# Output the semantic and edge maps
semantic_map =
self.semantic_output(x) edge_map =
self.edge_output(x) return semantic_map,
edge_map

# Define the losses
depth_loss =
nn.L1Loss() consistency_loss =
nn.L1Loss() height_prior_loss =
nn.L1Loss()

# Define the datasets
source_dataset =
SyntheticDataset() target_dataset =
RealDataset()

# Define the data loaders
source_loader =
torch.utils.data.DataLoader(source_dataset,
batch_size=batch_size,
shuffle=True) target_loader =
torch.utils.data.DataLoader(target_dataset,
batch_size=batch_size,
shuffle=True)

# Define the models
main_model =
DepthEstimator() second_model =
SemanticEdgeDetector()

# Move the models to device (GPU or CPU)
device =
torch.device("cuda:0" if torch.cuda.is_available() else "cpu") main_model.to(device) second_model.to(device)

# Define the optimizers
main_optimizer =
optim.Adam(main_model.parameters(),
lr=learning_rate) second_optimizer =
optim.Adam(second_model.parameters(),
lr=learning_rate)

# Train the models
for epoch in range(num_epochs):
# Initialize the running losses
running_depth_loss =
0.0 running_consistency_loss =
0.0 running_height_prior_loss =
0.0

# Iterate over the source and target data loaders
for i,
(source_data,
target_data) in enumerate(zip(source_loader,
target_loader)):
# Get the source and target images and depths
source_image,
source_depth =
source_data target_image =
target_data

# Move the data to device (GPU or CPU)
source_image =
source_image.to(device) source_depth =
source_depth.to(device) target_image =
target_image.to(device)

# Predict depth from source image using main model
pred_depth_source =
main_model(source_image)

# Compute depth loss on source domain
loss_depth_source =
depth_loss(pred_depth_source,
source_depth)

# Predict depth from target image using main model
pred_depth_target_main =
main_model(target_image)

# Predict semantic segmentation and edge detection from target image using second model
pred_semantic_target,
pred_edge_target =
second_model(target_image)

# Predict depth from target image using second model with semantic and edge features as input
pred_depth_target_second =
second_model(target_image,
pred_semantic_target,
pred_edge_target)

# Compute consistency loss on target domain
loss_consistency_target =
consistency_loss(pred_depth_target_main,
pred_depth_target_second,
confidence_map(pred_sem