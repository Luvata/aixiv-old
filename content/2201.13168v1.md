---
title: 2201.13168v1 SPAGHETTI  Editing Implicit Shapes Through Part Aware Generation
date: 2022-01-14
---

# [SPAGHETTI: Editing Implicit Shapes Through Part Aware Generation](http://arxiv.org/abs/2201.13168v1)

authors: Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2201.13168 "[2201.13168] SPAGHETTI: Editing Implicit Shapes Through Part Aware ..."
[2]: https://arxiv.org/pdf/2201.13168 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2110.13168v1 "[2110.13168v1] GALLUMI: A Galaxy Luminosity Function Pipeline for ..."

Here is a summary of the paper:

- **What**: The paper introduces a method for editing implicit shapes through part aware generation, called SPAGHETTI. It allows for manipulation of implicit shapes by means of transforming, interpolating and combining shape segments together, without requiring explicit part supervision[^1^][1].
- **Why**: The paper aims to address the challenge of adopting neural implicit fields for 3D shape modeling and editing, which are attractive representations for learning based techniques[^1^][1].
- **How**: The paper proposes a generative framework that disentangles shape part representation into extrinsic and intrinsic geometric information. This enables part-level control over the shape editing process. The paper also demonstrates the modeling capabilities of SPAGHETTI using an interactive graphical interface, where users can directly edit neural implicit shapes[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2201.13168 "[2201.13168] SPAGHETTI: Editing Implicit Shapes Through Part Aware ..."
[2]: https://arxiv.org/pdf/2211.13168v1.pdf "arXiv:2211.13168v1 [hep-ph] 23 Nov 2022"
[3]: https://arxiv.org/pdf/2201.13168 "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/2110.13168v1 "[2110.13168v1] GALLUMI: A Galaxy Luminosity Function Pipeline for ..."

According to the paper[^1^][1], the main contributions are:

- A novel generative framework for editing implicit shapes through part aware generation, called SPAGHETTI, that disentangles shape part representation into extrinsic and intrinsic geometric information.
- A method for learning part-aware implicit shape representations without explicit part supervision, using a self-attention mechanism and a contrastive loss.
- A demonstration of the modeling capabilities of SPAGHETTI using an interactive graphical interface, where users can directly edit neural implicit shapes by transforming, interpolating and combining shape segments together.

## Method Summary

[1]: https://arxiv.org/abs/2201.13168 "[2201.13168] SPAGHETTI: Editing Implicit Shapes Through Part Aware ..."
[2]: https://arxiv.org/pdf/2201.13168 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2110.13168v1 "[2110.13168v1] GALLUMI: A Galaxy Luminosity Function Pipeline for ..."

Here is a summary of the method section of the paper:

- The paper proposes a generative framework for editing implicit shapes through part aware generation, called SPAGHETTI, that consists of three components: a part-aware encoder, a part-aware decoder, and a part-aware generator[^1^][1].
- The part-aware encoder takes a set of 3D points sampled from an implicit shape and outputs a set of part embeddings that capture the extrinsic and intrinsic geometric information of each part[^1^][1].
- The part-aware decoder takes a set of part embeddings and a query point and outputs the occupancy and normal values at that point[^1^][1].
- The part-aware generator takes a set of part embeddings and applies various operations on them, such as translation, rotation, scaling, interpolation and concatenation, to generate new part embeddings that correspond to edited shape parts[^1^][1].
- The paper also introduces a self-attention mechanism and a contrastive loss to learn part-aware implicit shape representations without explicit part supervision[^1^][1]. The self-attention mechanism allows the encoder to learn the relations between different parts of the shape, while the contrastive loss encourages the encoder to produce distinct embeddings for different parts[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the part-aware encoder network
def part_aware_encoder(points):
  # Encode the points using a PointNet-like network
  point_features = pointnet(points)
  # Apply self-attention to learn the relations between different parts
  part_features = self_attention(point_features)
  # Return the part embeddings
  return part_features

# Define the part-aware decoder network
def part_aware_decoder(part_features, query_point):
  # Concatenate the part features and the query point
  input_features = concat(part_features, query_point)
  # Decode the input features using a MLP network
  output_values = mlp(input_features)
  # Return the occupancy and normal values at the query point
  return output_values

# Define the part-aware generator network
def part_aware_generator(part_features, operation):
  # Apply the operation on the part features
  if operation == "translation":
    # Add a random vector to each part feature
    new_part_features = part_features + random_vector()
  elif operation == "rotation":
    # Multiply each part feature by a random rotation matrix
    new_part_features = part_features * random_rotation_matrix()
  elif operation == "scaling":
    # Multiply each part feature by a random scalar
    new_part_features = part_features * random_scalar()
  elif operation == "interpolation":
    # Interpolate between two part features using a random weight
    new_part_features = interpolate(part_features[0], part_features[1], random_weight())
  elif operation == "concatenation":
    # Concatenate two part features along a random axis
    new_part_features = concatenate(part_features[0], part_features[1], random_axis())
  else:
    # Return the original part features
    new_part_features = part_features
  # Return the new part embeddings
  return new_part_features

# Define the training procedure
def train():
  # Initialize the encoder, decoder and generator networks
  encoder = part_aware_encoder()
  decoder = part_aware_decoder()
  generator = part_aware_generator()
  # Initialize the optimizer and the loss functions
  optimizer = Adam()
  reconstruction_loss = ChamferDistance()
  contrastive_loss = NTXentLoss()
  # Loop over the training data
  for shape in shapes:
    # Sample points from the shape surface
    points = sample_points(shape)
    # Encode the points into part features using the encoder
    part_features = encoder(points)
    # Decode the occupancy and normal values at the query points using the decoder
    output_values = decoder(part_features, query_points)
    # Compute the reconstruction loss between the output values and the ground truth values
    rec_loss = reconstruction_loss(output_values, ground_truth_values)
    # Compute the contrastive loss between the positive and negative pairs of part features
    con_loss = contrastive_loss(part_features, positive_pairs, negative_pairs)
    # Compute the total loss as a weighted sum of the reconstruction and contrastive losses
    total_loss = rec_loss + lambda * con_loss
    # Update the encoder and decoder parameters using backpropagation and gradient descent
    optimizer.step(total_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the hyperparameters
num_points = 2048 # Number of points to sample from each shape
num_query_points = 1024 # Number of query points to evaluate the decoder
num_part_features = 256 # Dimension of the part features
num_output_values = 4 # Dimension of the output values (occupancy and normal)
num_heads = 8 # Number of heads for the self-attention mechanism
num_layers = 4 # Number of layers for the MLP network
num_epochs = 100 # Number of epochs for training
batch_size = 32 # Batch size for training
learning_rate = 0.001 # Learning rate for training
lambda = 0.1 # Weight for the contrastive loss

# Define the PointNet-like network for encoding the points
class PointNet(nn.Module):
  def __init__(self):
    super(PointNet, self).__init__()
    # Define the convolutional layers
    self.conv1 = nn.Conv1d(3, 64, 1)
    self.conv2 = nn.Conv1d(64, 128, 1)
    self.conv3 = nn.Conv1d(128, num_part_features, 1)
    # Define the batch normalization layers
    self.bn1 = nn.BatchNorm1d(64)
    self.bn2 = nn.BatchNorm1d(128)
    self.bn3 = nn.BatchNorm1d(num_part_features)

  def forward(self, points):
    # Reshape the points to have a channel dimension
    points = points.transpose(1, 2) # (B, N, 3) -> (B, 3, N)
    # Apply the convolutional and batch normalization layers with ReLU activation
    x = F.relu(self.bn1(self.conv1(points))) # (B, 3, N) -> (B, 64, N)
    x = F.relu(self.bn2(self.conv2(x))) # (B, 64, N) -> (B, 128, N)
    x = F.relu(self.bn3(self.conv3(x))) # (B, 128, N) -> (B, num_part_features, N)
    # Reshape the output to have a feature dimension
    x = x.transpose(1, 2) # (B, num_part_features, N) -> (B, N, num_part_features)
    return x

# Define the self-attention mechanism for learning the relations between different parts
class SelfAttention(nn.Module):
  def __init__(self):
    super(SelfAttention, self).__init__()
    # Define the linear layers for computing the query, key and value vectors
    self.query_layer = nn.Linear(num_part_features, num_part_features // num_heads)
    self.key_layer = nn.Linear(num_part_features, num_part_features // num_heads)
    self.value_layer = nn.Linear(num_part_features, num_part_features // num_heads)
    # Define the linear layer for combining the output vectors
    self.output_layer = nn.Linear(num_part_features // num_heads * num_heads , num_part_features)

  def forward(self, part_features):
    # Compute the query, key and value vectors for each part feature
    queries = self.query_layer(part_features) # (B, N, num_part_features) -> (B, N , num_part_features // num_heads)
    keys = self.key_layer(part_features) # (B ,N ,num_part_features) -> (B ,N ,num_part_features // num_heads)
    values = self.value_layer(part_features) # (B ,N ,num_part_features) -> (B ,N ,num_part_features // num_heads)
    # Reshape the vectors to have a head dimension and transpose them for matrix multiplication
    queries = queries.view(batch_size ,num_points ,num_heads ,-1).transpose(1 ,2) # (B ,N ,num_heads ,-1) -> (B ,num_heads ,N ,-1)
    keys = keys.view(batch_size ,num_points ,num_heads ,-1).transpose(1 ,2).transpose(2 ,3) # (B ,N ,num_heads ,-1) -> (B ,num_heads ,-1 ,N)
    values = values.view(batch_size ,num_points ,num_heads ,-1).transpose(1 ,2) # (B ,N ,num_heads ,-1) -> (B ,num_heads ,N ,-1)
    # Compute the scaled dot-product attention scores for each pair of part features
    scores = torch.matmul(queries ,keys) / np.sqrt(num_part_features // num_heads) # (B ,num_heads ,N ,-1) x (B ,num_heads ,-1 ,N) -> (B ,num_heads ,N ,N)
    # Apply the softmax function to obtain the attention weights
    weights = F.softmax(scores ,dim =-1) # (B ,num_heads ,N ,N) -> (B ,num_heads ,N ,N)
    # Compute the output vectors by multiplying the attention weights and the value vectors
    outputs = torch.matmul(weights ,values) # (B ,num_heads ,N ,N) x (B ,num_heads ,N ,-1) -> (B ,num_heads ,N ,-1)
    # Reshape and transpose the output vectors to have a feature dimension
    outputs = outputs.transpose(1, 2).contiguous().view(batch_size, num_points, -1) # (B, num_heads, N, -1) -> (B, N, num_part_features)
    # Apply the output layer with ReLU activation
    outputs = F.relu(self.output_layer(outputs)) # (B, N, num_part_features) -> (B, N, num_part_features)
    return outputs

# Define the MLP network for decoding the part features and query points
class MLP(nn.Module):
  def __init__(self):
    super(MLP, self).__init__()
    # Define the linear layers
    self.fc1 = nn.Linear(num_part_features + 3, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256, 128)
    self.fc4 = nn.Linear(128, num_output_values)
    # Define the batch normalization layers
    self.bn1 = nn.BatchNorm1d(512)
    self.bn2 = nn.BatchNorm1d(256)
    self.bn3 = nn.BatchNorm1d(128)

  def forward(self, input_features):
    # Apply the linear and batch normalization layers with ReLU activation
    x = F.relu(self.bn1(self.fc1(input_features))) # (B, N, num_part_features + 3) -> (B, N, 512)
    x = F.relu(self.bn2(self.fc2(x))) # (B, N, 512) -> (B, N, 256)
    x = F.relu(self.bn3(self.fc3(x))) # (B, N, 256) -> (B, N, 128)
    x = self.fc4(x) # (B, N, 128) -> (B, N, num_output_values)
    return x

# Define the Chamfer distance loss for measuring the reconstruction error
def chamfer_distance(output_values, ground_truth_values):
  # Extract the occupancy and normal values from the output and ground truth values
  output_occupancy = output_values[:, :, 0] # (B, N) 
  output_normal = output_values[:, :, 1:] # (B, N, 3)
  ground_truth_occupancy = ground_truth_values[:, :, 0] # (B, N)
  ground_truth_normal = ground_truth_values[:, :, 1:] # (B, N, 3)
  # Compute the L2 distance between the occupancy values
  occupancy_distance = torch.sum((output_occupancy - ground_truth_occupancy) ** 2) / batch_size / num_query_points
  # Compute the cosine distance between the normal values
  normal_distance = torch.sum(1 - F.cosine_similarity(output_normal, ground_truth_normal)) / batch_size / num_query_points
  # Return the sum of the occupancy and normal distances
  return occupancy_distance + normal_distance

# Define the NT-Xent loss for measuring the contrastive error
def NTXent_loss(part_features, positive_pairs, negative_pairs):
  # Compute the cosine similarity between each pair of part features
  similarities = F.cosine_similarity(part_features.unsqueeze(1), part_features.unsqueeze(0), dim=-1) # (B, N, num_part_features) -> (B, N, N)
  # Extract the positive and negative similarities from the similarity matrix
  positive_similarities = similarities[positive_pairs[:,0], positive_pairs[:,1]] # (P,)
  negative_similarities = similarities[negative_pairs[:,0], negative_pairs[:,1]] # (Q,)
  # Apply a temperature scaling factor to the similarities
  temperature = np.log(num_points)
  positive_similarities /= temperature
  negative_similarities /= temperature
  # Compute the softmax function over the negative similarities for each positive pair
  negative_softmax = torch.sum(torch.exp(negative_similarities).view(-1,num_points-2), dim=1) #(P,)
  # Compute the NT-Xent loss for each positive pair
  pair_loss = -