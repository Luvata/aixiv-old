---
title: 2304.02744v3 StyleGAN Salon  Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer
date: 2023-04-03
---

# [StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer](http://arxiv.org/abs/2304.02744v3)

authors: Sasikarn Khwanmuang, Pakkapon Phongthawee, Patsorn Sangkloy, Supasorn Suwajanakorn


## What, Why and How

[1]: https://arxiv.org/abs/2304.02744 "[2304.02744] StyleGAN Salon: Multi-View Latent Optimization for Pose ..."
[2]: https://arxiv.org/pdf/2304.02744.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2304.02744v3 "[2304.02744v3] StyleGAN Salon: Multi-View Latent Optimization for Pose ..."

Here is a summary of the paper:

- **What**: The paper proposes a method for transferring the hairstyle of a reference image to an input photo for virtual hair try-on, using StyleGAN and multi-view latent optimization.
- **Why**: The paper aims to address the challenges of preserving the face shape and identity of the input, and handling occluded or ambiguous regions in the reference hair, such as bangs or hats.
- **How**: The paper leverages two different views of reference composites, one frontal and one profile, to guide the optimization process and share information between poses. The paper also introduces a novel loss function that balances between hair transfer and face preservation.

The paper is titled **StyleGAN Salon: Multi-View Latent Optimization for Pose-Invariant Hairstyle Transfer** and was accepted to CVPR 2023[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/2304.02744 "[2304.02744] StyleGAN Salon: Multi-View Latent Optimization for Pose ..."
[2]: https://arxiv.org/pdf/2304.02744.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2304.02744v3 "[2304.02744v3] StyleGAN Salon: Multi-View Latent Optimization for Pose ..."

According to the paper[^1^][1], the main contributions are:

- A novel **multi-view latent optimization** framework that uses two different views of reference composites to guide the hairstyle transfer and handle occluded or ambiguous regions.
- A novel **loss function** that balances between hair transfer and face preservation, and incorporates perceptual, identity, and style losses.
- A **user study** that demonstrates the effectiveness of the proposed method and compares it with prior work on significantly more challenging hair transfer scenarios than previously studied.
- A **project page** that provides more details, code, and results: https://stylegan-salon.github.io/.


## Method Summary

[1]: https://arxiv.org/abs/2304.02744 "[2304.02744] StyleGAN Salon: Multi-View Latent Optimization for Pose ..."
[2]: https://arxiv.org/pdf/2304.02744.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2304.02744v3 "[2304.02744v3] StyleGAN Salon: Multi-View Latent Optimization for Pose ..."

Here is a summary of the method section of the paper:

- The paper uses **StyleGAN2**[^1^][1] as the base generative model for synthesizing realistic face images with different hairstyles.
- The paper adopts the **GAN projection**[^1^][1] technique to find the latent code of an input image that matches the StyleGAN2 distribution and produces a realistic output image.
- The paper introduces a **multi-view latent optimization**[^1^][1] framework that uses two different views of reference composites, one frontal and one profile, to guide the hairstyle transfer and handle occluded or ambiguous regions in the reference hair.
- The paper defines a **loss function**[^1^][1] that balances between hair transfer and face preservation, and incorporates perceptual, identity, and style losses.
- The paper also describes the **implementation details**[^1^][1] of the method, such as the data preprocessing, the network architectures, the optimization algorithm, and the hyperparameters.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: an input image I and a reference hair image R
# Output: a transferred image T with the hairstyle of R and the face of I

# Preprocess the images to align the faces and crop the hair regions
I_crop, R_crop = preprocess(I, R)

# Initialize the latent codes for the input and reference images
z_I = init_latent(I_crop)
z_R = init_latent(R_crop)

# Generate two views of reference composites by swapping the latent codes
R_front = generate(z_I[:8], z_R[8:])
R_side = generate(z_I[:4], z_R[4:])

# Optimize the latent code of the input image to match the reference composites
for iter in range(max_iters):
  # Generate the output image from the latent code
  T = generate(z_I)

  # Compute the loss function
  L = compute_loss(T, I, R_front, R_side)

  # Update the latent code using gradient descent
  z_I = z_I - lr * grad(L, z_I)

# Postprocess the output image to blend with the original input image
T = postprocess(T, I)

# Return the transferred image
return T
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import numpy as np
import cv2
import torch
import torchvision
import dlib

# Load the pretrained StyleGAN2 model
stylegan = load_stylegan('stylegan2-ffhq-config-f.pt')

# Load the face alignment and landmark detection models
aligner = load_aligner('shape_predictor_68_face_landmarks.dat')
detector = load_detector('mmod_human_face_detector.dat')

# Define the constants
IMG_SIZE = 1024 # the size of the output image
CROP_SIZE = 512 # the size of the cropped hair region
NUM_LAYERS = 18 # the number of layers in StyleGAN2
LR = 0.01 # the learning rate for optimization
MAX_ITERS = 1000 # the maximum number of iterations for optimization
LAMBDA_P = 1e-5 # the weight for perceptual loss
LAMBDA_I = 1e-3 # the weight for identity loss
LAMBDA_S = 1e-4 # the weight for style loss

# Define the perceptual loss network (VGG16)
vgg16 = load_vgg16('vgg16.pth')
vgg16.eval()

# Define the identity loss network (ArcFace)
arcface = load_arcface('arcface_r100_v1.pth')
arcface.eval()

# Define the function to align and crop a face image
def align_and_crop(img):
  # Detect the face in the image
  face = detector(img)

  # Align the face using landmarks
  landmarks = aligner(img, face)
  aligned_img = align(img, landmarks)

  # Crop the hair region from the aligned image
  cropped_img = crop(aligned_img, CROP_SIZE)

  # Resize the cropped image to match StyleGAN2 resolution
  resized_img = resize(cropped_img, IMG_SIZE)

  # Return the aligned and cropped image
  return resized_img

# Define the function to initialize a latent code from an image
def init_latent(img):
  # Convert the image to a tensor
  img_tensor = to_tensor(img)

  # Normalize the image tensor
  img_tensor = normalize(img_tensor)

  # Generate a random latent code
  z = torch.randn(1, NUM_LAYERS, 512)

  # Optimize the latent code to match the image using L2 loss
  for iter in range(100):
    # Generate an image from the latent code
    gen_img = stylegan(z)

    # Compute the L2 loss between the generated and input images
    L2_loss = torch.mean((gen_img - img_tensor) ** 2)

    # Update the latent code using gradient descent
    z = z - LR * grad(L2_loss, z)

  # Return the optimized latent code
  return z

# Define the function to generate an image from a latent code using StyleGAN2
def generate(z):
  # Generate an image from the latent code using StyleGAN2
  img = stylegan(z)

  # Denormalize the image tensor
  img = denormalize(img)

  # Convert the tensor to an image
  img = to_image(img)

  # Return the generated image
  return img

# Define the function to compute the perceptual loss between two images
def perceptual_loss(img1, img2):
  # Convert the images to tensors
  img1_tensor = to_tensor(img1)
  img2_tensor = to_tensor(img2)

  # Normalize the image tensors
  img1_tensor = normalize(img1_tensor)
  img2_tensor = normalize(img2_tensor)

  # Extract features from VGG16 layers relu1_1, relu2_1, relu3_1, relu4_1, relu5_1 
  features1 = vgg16(img1_tensor, [0,5,10,17,24])
  features2 = vgg16(img2_tensor, [0,5,10,17,24])

  # Compute the L2 loss between corresponding features
  L2_losses = [torch.mean((f1 - f2) ** 2) for f1,f2 in zip(features1, features2)]

  # Sum up the L2 losses and multiply by lambda_p
  p_loss = LAMBDA_P * sum(L2_losses)

  # Return the perceptual loss 
  return p_loss

# Define the function to compute the identity loss between two images 
def identity_loss(img1, img2):
   # Convert the images to tensors 
   img1_tensor = to_tensor(img1)
   img2_tensor = to_tensor(img2)

   # Normalize the image tensors 
   img1_tensor = normalize(img1_tensor)
   img2_tensor = normalize(img2_tensor)

   # Extract features from ArcFace 
   features1 = arcface(img1_tensor)
   features2 = arcface(img2_tensor)

   # Compute the cosine similarity between the features 
   cos_sim = torch.dot(features1, features2) / (torch.norm(features1) * torch.norm(features2))

   # Compute the identity loss as 1 - cosine similarity and multiply by lambda_i 
   i_loss = LAMBDA_I * (1 - cos_sim)

   # Return the identity loss 
   return i_loss

# Define the function to compute the style loss between two images
def style_loss(img1, img2):
  # Convert the images to tensors
  img1_tensor = to_tensor(img1)
  img2_tensor = to_tensor(img2)

  # Normalize the image tensors
  img1_tensor = normalize(img1_tensor)
  img2_tensor = normalize(img2_tensor)

  # Extract features from VGG16 layers relu1_1, relu2_1, relu3_1, relu4_1, relu5_1 
  features1 = vgg16(img1_tensor, [0,5,10,17,24])
  features2 = vgg16(img2_tensor, [0,5,10,17,24])

  # Compute the gram matrices of the features
  gram_matrices1 = [torch.matmul(f1.transpose(0,1), f1) for f1 in features1]
  gram_matrices2 = [torch.matmul(f2.transpose(0,1), f2) for f2 in features2]

  # Compute the L2 loss between corresponding gram matrices
  L2_losses = [torch.mean((g1 - g2) ** 2) for g1,g2 in zip(gram_matrices1, gram_matrices2)]

  # Sum up the L2 losses and multiply by lambda_s
  s_loss = LAMBDA_S * sum(L2_losses)

  # Return the style loss
  return s_loss

# Define the function to compute the total loss function
def compute_loss(T, I, R_front, R_side):
  # Compute the perceptual loss between T and I
  p_loss_T_I = perceptual_loss(T, I)

  # Compute the perceptual loss between T and R_front
  p_loss_T_Rf = perceptual_loss(T, R_front)

  # Compute the perceptual loss between T and R_side
  p_loss_T_Rs = perceptual_loss(T, R_side)

  # Compute the identity loss between T and I
  i_loss_T_I = identity_loss(T, I)

  # Compute the style loss between T and R_front
  s_loss_T_Rf = style_loss(T, R_front)

  # Compute the style loss between T and R_side
  s_loss_T_Rs = style_loss(T, R_side)

  # Sum up all the losses to get the total loss
  total_loss = p_loss_T_I + p_loss_T_Rf + p_loss_T_Rs + i_loss_T_I + s_loss_T_Rf + s_loss_T_Rs

  # Return the total loss
  return total_loss

# Define the function to blend an output image with an original input image
def postprocess(T, I):
  # Detect the face in the output image
  face_T = detector(T)

  # Align the face using landmarks
  landmarks_T = aligner(T, face_T)
  
  # Crop the hair region from the output image
  cropped_T = crop(T, CROP_SIZE)

  # Resize the cropped image to match the original input image size
  resized_T = resize(cropped_T, I.shape[:2])

  # Create a mask for the hair region using edge detection and dilation
  mask_T = edge_detect(resized_T)
  mask_T = dilate(mask_T)

  # Blend the resized output image with the original input image using alpha blending
  alpha = mask_T / np.max(mask_T)
  blended_img = alpha * resized_T + (1 - alpha) * I

  # Return the blended image
  return blended_img

# Define the main function to perform hairstyle transfer 
def hairstyle_transfer(I, R):
  
    # Align and crop the input and reference images 
    I_crop = align_and_crop(I)
    R_crop = align_and_crop(R)

    # Initialize the latent codes for the input and reference images 
    z_I = init_latent(I_crop)
    z_R = init_latent(R_crop)

    # Generate two views of reference composites by swapping the latent codes 
    R_front = generate(z_I[:8], z_R[8:])
    R_side = generate(z_I[:4], z_R[4:])

    # Optimize the