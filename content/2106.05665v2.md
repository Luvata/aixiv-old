---
title: 2106.05665v2 Learning Runtime Decisions for Adaptive Real-Time Perception
date: 2021-06-06
---

# [Learning Runtime Decisions for Adaptive Real-Time Perception](http://arxiv.org/abs/2106.05665v2)

authors: Anurag Ghosh, Akshay Nambi, Vaibhav Balloli, Aditya Singh, Tanuja Ganu


## What, Why and How

[1]: https://arxiv.org/pdf/2106.05665v2.pdf "arXiv:2106.05665v2 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2103.05665v2.pdf "arXiv:2103.05665v2 [quant-ph] 31 Aug 2021"
[3]: http://export.arxiv.org/abs/2106.05665 "[2106.05665] Learning Runtime Decisions for Adaptive Real-Time Perception"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes **Chanakya**, a learned approximate execution framework that automatically learns runtime decisions for adaptive real-time perception tasks, such as object detection and segmentation.
- **Why**: The paper aims to address the challenges of balancing accuracy and latency in real-time perception, which are influenced by intrinsic and extrinsic context, such as image content and resource contention. The paper argues that traditional execution frameworks are sub-optimal and inflexible, as they rely on rule-based decision algorithms and operate with a fixed latency budget.
- **How**: The paper leverages the streaming perception paradigm, where the input is processed in chunks over time, to train Chanakya via novel rewards that balance accuracy and latency implicitly. Chanakya simultaneously considers intrinsic and extrinsic context, and predicts decisions such as input resolution, temporal stride, and model architecture in a flexible manner. Chanakya is designed with low overhead in mind, and outperforms state-of-the-art static and dynamic execution policies on public datasets on both server GPUs and edge devices.

## Main Contributions

[1]: https://arxiv.org/pdf/2106.05665v2.pdf "arXiv:2106.05665v2 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2103.05665v2.pdf "arXiv:2103.05665v2 [quant-ph] 31 Aug 2021"
[3]: http://export.arxiv.org/abs/2106.05665 "[2106.05665] Learning Runtime Decisions for Adaptive Real-Time Perception"

According to the paper at [^1^][1], the main contributions are:

- **Chanakya**, a learned approximate execution framework that automatically learns runtime decisions for adaptive real-time perception tasks, such as object detection and segmentation.
- A novel training scheme that balances accuracy and latency implicitly, without approximating either objectives, and leverages the streaming perception paradigm.
- A comprehensive evaluation of Chanakya on public datasets and different hardware platforms, showing that it outperforms state-of-the-art static and dynamic execution policies.

## Method Summary

[1]: https://arxiv.org/pdf/2106.05665v2.pdf "arXiv:2106.05665v2 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2103.05665v2.pdf "arXiv:2103.05665v2 [quant-ph] 31 Aug 2021"
[3]: http://export.arxiv.org/abs/2106.05665 "[2106.05665] Learning Runtime Decisions for Adaptive Real-Time Perception"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces **Chanakya**, a learned approximate execution framework that automatically learns runtime decisions for adaptive real-time perception tasks, such as object detection and segmentation.
- Chanakya leverages the **streaming perception paradigm**, where the input is processed in chunks over time, and each chunk can be processed at different resolutions, temporal strides, and model architectures.
- Chanakya consists of two components: a **decision network** and a **perception network**. The decision network takes as input the intrinsic and extrinsic context, and predicts the optimal decisions for each chunk. The perception network takes as input the chunk and the decisions, and outputs the perception results.
- Chanakya is trained via **novel rewards** that balance accuracy and latency implicitly, without approximating either objectives. The rewards are based on the difference between the perception results of each chunk and a reference result obtained by processing the whole input at the highest resolution and model architecture.
- Chanakya is designed with **low overhead** in mind, by using lightweight decision networks, efficient perception networks, and parallel processing of chunks.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the decision network and the perception network
decision_network = DecisionNetwork()
perception_network = PerceptionNetwork()

# Define the reference result obtained by processing the whole input at the highest resolution and model architecture
reference_result = perception_network(input, max_resolution, max_model)

# Split the input into chunks over time
chunks = split(input)

# Initialize the output list
output = []

# For each chunk
for chunk in chunks:
  # Get the intrinsic and extrinsic context
  intrinsic_context = get_intrinsic_context(chunk)
  extrinsic_context = get_extrinsic_context()

  # Predict the optimal decisions for the chunk using the decision network
  decisions = decision_network(intrinsic_context, extrinsic_context)

  # Process the chunk using the perception network with the predicted decisions
  result = perception_network(chunk, decisions)

  # Append the result to the output list
  output.append(result)

  # Compute the reward based on the difference between the result and the reference result
  reward = compute_reward(result, reference_result)

  # Update the decision network and the perception network using the reward
  update(decision_network, perception_network, reward)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
num_chunks = 10 # The number of chunks to split the input into
num_decisions = 3 # The number of decisions to predict for each chunk
num_epochs = 100 # The number of epochs to train the networks
learning_rate = 0.01 # The learning rate for the optimizer
gamma = 0.9 # The discount factor for the reward

# Define the decision network and the perception network
# The decision network is a convolutional neural network that takes as input the intrinsic and extrinsic context and outputs a probability distribution over the possible decisions
# The perception network is a ResNet-50 model that takes as input the chunk and the decisions and outputs a feature vector for object detection or segmentation
decision_network = torch.nn.Sequential(
  torch.nn.Conv2d(3, 32, 3, padding=1),
  torch.nn.ReLU(),
  torch.nn.MaxPool2d(2),
  torch.nn.Conv2d(32, 64, 3, padding=1),
  torch.nn.ReLU(),
  torch.nn.MaxPool2d(2),
  torch.nn.Conv2d(64, 128, 3, padding=1),
  torch.nn.ReLU(),
  torch.nn.MaxPool2d(2),
  torch.nn.Flatten(),
  torch.nn.Linear(128 * 8 * 8 + 1, num_decisions), # The extrinsic context is a scalar value representing the resource contention
  torch.nn.Softmax(dim=1)
)

perception_network = torchvision.models.resnet50(pretrained=True)

# Define the optimizer and the loss function for the networks
optimizer = torch.optim.Adam(list(decision_network.parameters()) + list(perception_network.parameters()), lr=learning_rate)
loss_function = torch.nn.MSELoss()

# Define the reference result obtained by processing the whole input at the highest resolution and model architecture
# The input is a tensor of shape (3, H, W), where H and W are the height and width of the image
# The reference result is a tensor of shape (2048), where 2048 is the dimension of the feature vector from ResNet-50
reference_result = perception_network(input, max_resolution, max_model)

# Split the input into chunks over time
# The chunks are tensors of shape (3, H / num_chunks, W)
chunks = torch.split(input, H / num_chunks, dim=1)

# Initialize the output list
output = []

# For each epoch
for epoch in range(num_epochs):
  # Initialize the total reward and loss
  total_reward = 0
  total_loss = 0

  # For each chunk
  for i in range(num_chunks):
    # Get the intrinsic and extrinsic context
    # The intrinsic context is the chunk itself
    # The extrinsic context is a random number between 0 and 1 representing the resource contention
    intrinsic_context = chunks[i]
    extrinsic_context = np.random.rand()

    # Predict the optimal decisions for the chunk using the decision network
    # The decisions are tensors of shape (num_decisions), where each element is a probability of choosing a certain resolution, temporal stride, or model architecture
    decisions = decision_network(torch.cat([intrinsic_context, extrinsic_context], dim=0))

    # Sample a decision from the probability distribution using epsilon-greedy exploration
    # Epsilon is a small number that decreases over time to balance exploration and exploitation
    epsilon = 0.1 / (epoch + 1)
    if np.random.rand() < epsilon:
      decision = np.random.randint(num_decisions)
    else:
      decision = torch.argmax(decisions)

    # Process the chunk using the perception network with the sampled decision
    # The result is a tensor of shape (2048)
    result = perception_network(chunks[i], decisions[decision])

    # Append the result to the output list
    output.append(result)

    # Compute the reward based on the difference between the result and the reference result
    # The reward is a scalar value that is higher if the result is closer to the reference result and lower if it is farther away
    reward = -torch.norm(result - reference_result)

    # Update the total reward
    total_reward += reward * (gamma ** i) # Discount future rewards by gamma

    # Compute the loss based on the difference between the predicted decisions and the sampled decision
    # The loss is a scalar value that is higher if the predicted decisions are different from the sampled decision and lower if they are similar
    loss = -torch.log(decisions[decision])

    # Update the total loss
    total_loss += loss

    # Update the decision network and the perception network using the reward and the loss
    optimizer.zero_grad()
    (reward + loss).backward()
    optimizer.step()

  # Print the epoch, the total reward, and the total loss
  print(f"Epoch: {epoch}, Total reward: {total_reward}, Total loss: {total_loss}")
```