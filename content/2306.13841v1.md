---
title: 2306.13841v1 Is Pre-training Truly Better Than Meta-Learning?
date: 2023-06-14
---

# [Is Pre-training Truly Better Than Meta-Learning?](http://arxiv.org/abs/2306.13841v1)

authors: Brando Miranda, Patrick Yu, Saumya Goyal, Yu-Xiong Wang, Sanmi Koyejo


## What, Why and How

[1]: https://arxiv.org/pdf/2306.13841v1.pdf "Is Pre-Training Truly Better than Meta-Learning? - arXiv.org"
[2]: https://arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning? - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning?"

Here is a summary of the paper:

```
What: The paper compares pre-training (PT) and meta-learning (MAML) methods for few-shot learning tasks across various datasets.

Why: The paper challenges the current belief that PT is always better than MAML and investigates the role of formal diversity of datasets in determining the performance of both methods.

How: The paper conducts an extensive empirical study using 21 few-shot learning benchmarks and a rigorous statistical tool (effect size) to measure the practical significance of the difference between PT and MAML. The paper also computes the diversity coefficient of each dataset to quantify its formal diversity.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2306.13841v1.pdf "Is Pre-Training Truly Better than Meta-Learning? - arXiv.org"
[2]: https://arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning? - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning?"

Here is a list of the contributions of the paper:

```
- The paper clarifies previous claims that pre-training is better than meta-learning for few-shot learning tasks and shows that the formal diversity of datasets is a key factor in determining the performance of both methods.
- The paper conducts an extensive empirical study using 21 few-shot learning benchmarks, including the large-scale Meta-Data set, and compares pre-training and meta-learning methods under a fair setting with the same architecture, optimizer, and convergence criterion.
- The paper uses a more rigorous statistical tool (effect size) to measure the practical significance of the difference between pre-training and meta-learning methods and shows that the average difference is small (less than 0.2) according to classical statistical thresholds.
- The paper also shows no significant difference between a meta-learning model and a pre-training model with GPT-2 on Openwebtext, a natural language processing dataset.
```


## Method Summary

[1]: https://arxiv.org/pdf/2306.13841v1.pdf "Is Pre-Training Truly Better than Meta-Learning? - arXiv.org"
[2]: https://arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning? - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning?"

Here is a summary of the method section of the paper:

```
The paper uses two main methods for few-shot learning: pre-training (PT) and meta-learning (MAML). PT involves training a model on a large-scale dataset and then fine-tuning the final layer on a few-shot task. MAML involves training a model on multiple tasks and then adapting it to a new task with a few gradient steps. The paper compares PT and MAML on 21 few-shot learning benchmarks, covering different domains such as computer vision, natural language processing, reinforcement learning, and more. The paper uses the same architecture (ResNet-18 or GPT-2) and optimizer (Adam) for both methods and trains them until convergence. The paper also uses the effect size (Cohen's d) as a statistical tool to measure the practical significance of the difference between PT and MAML on each dataset. The paper also computes the diversity coefficient of each dataset to quantify its formal diversity, which is defined as the average distance between tasks in terms of input distribution, output distribution, and loss function.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2306.13841v1.pdf "Is Pre-Training Truly Better than Meta-Learning? - arXiv.org"
[2]: https://arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning? - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning?"

Here is a high-level pseudo code for the paper:

```
# Define the model architecture (ResNet-18 or GPT-2)
model = ResNet18() or GPT2()

# Define the optimizer (Adam)
optimizer = Adam(model.parameters())

# Define the effect size function (Cohen's d)
def effect_size(x, y):
  # Compute the mean and standard deviation of x and y
  mean_x = mean(x)
  mean_y = mean(y)
  std_x = std(x)
  std_y = std(y)

  # Compute the pooled standard deviation
  pooled_std = sqrt(((len(x) - 1) * std_x ** 2 + (len(y) - 1) * std_y ** 2) / (len(x) + len(y) - 2))

  # Compute the effect size
  d = (mean_x - mean_y) / pooled_std

  # Return the effect size
  return d

# Define the diversity coefficient function
def diversity_coefficient(dataset):
  # Compute the average distance between tasks in terms of input distribution, output distribution, and loss function
  distance = average(distance(input, output, loss) for each pair of tasks in dataset)

  # Return the diversity coefficient
  return distance

# For each dataset in the 21 few-shot learning benchmarks
for dataset in benchmarks:

  # For each method in pre-training (PT) and meta-learning (MAML)
  for method in [PT, MAML]:

    # Train the model using the method on the dataset
    model.train(method, dataset)

    # Evaluate the model on a few-shot task from the dataset
    accuracy = model.evaluate(task)

    # Store the accuracy for later comparison
    accuracies[method].append(accuracy)

  # Compute the effect size between PT and MAML accuracies
  d = effect_size(accuracies[PT], accuracies[MAML])

  # Compute the diversity coefficient of the dataset
  dc = diversity_coefficient(dataset)

  # Report the effect size and the diversity coefficient
  print(f"Effect size: {d}, Diversity coefficient: {dc}")
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2306.13841v1.pdf "Is Pre-Training Truly Better than Meta-Learning? - arXiv.org"
[2]: https://arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning? - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.13841 "[2306.13841] Is Pre-training Truly Better Than Meta-Learning?"

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np
import scipy.stats

# Define the model architecture (ResNet-18 or GPT-2)
if task == "vision":
  model = torchvision.models.resnet18(pretrained=True)
elif task == "language":
  model = transformers.GPT2Model.from_pretrained("gpt2")

# Define the optimizer (Adam)
optimizer = torch.optim.Adam(model.parameters())

# Define the effect size function (Cohen's d)
def effect_size(x, y):
  # Compute the mean and standard deviation of x and y
  mean_x = np.mean(x)
  mean_y = np.mean(y)
  std_x = np.std(x)
  std_y = np.std(y)

  # Compute the pooled standard deviation
  pooled_std = np.sqrt(((len(x) - 1) * std_x ** 2 + (len(y) - 1) * std_y ** 2) / (len(x) + len(y) - 2))

  # Compute the effect size
  d = (mean_x - mean_y) / pooled_std

  # Return the effect size
  return d

# Define the diversity coefficient function
def diversity_coefficient(dataset):
  # Initialize an empty list to store the distances between tasks
  distances = []

  # For each pair of tasks in the dataset
  for i in range(len(dataset)):
    for j in range(i + 1, len(dataset)):

      # Get the input, output, and loss function of each task
      input_i, output_i, loss_i = dataset[i]
      input_j, output_j, loss_j = dataset[j]

      # Compute the distance between the input distributions using Kullback-Leibler divergence
      input_dist_i = torch.distributions.Categorical(input_i)
      input_dist_j = torch.distributions.Categorical(input_j)
      input_distance = torch.distributions.kl_divergence(input_dist_i, input_dist_j)

      # Compute the distance between the output distributions using Kullback-Leibler divergence
      output_dist_i = torch.distributions.Categorical(output_i)
      output_dist_j = torch.distributions.Categorical(output_j)
      output_distance = torch.distributions.kl_divergence(output_dist_i, output_dist_j)

      # Compute the distance between the loss functions using mean squared error
      loss_distance = torch.nn.functional.mse_loss(loss_i, loss_j)

      # Compute the total distance as the sum of the three distances
      total_distance = input_distance + output_distance + loss_distance

      # Append the total distance to the list
      distances.append(total_distance)

  # Compute the average distance between tasks
  distance = np.mean(distances)

  # Return the diversity coefficient
  return distance

# For each dataset in the 21 few-shot learning benchmarks
for dataset in benchmarks:

  # Initialize two empty lists to store the accuracies of PT and MAML methods
  accuracies_PT = []
  accuracies_MAML = []

  # For each method in pre-training (PT) and meta-learning (MAML)
  for method in [PT, MAML]:

    # Train the model using the method on the dataset
    if method == PT:
      # Train the model on a large-scale dataset with a fixed number of epochs
      model.train(large_dataset, epochs)

    elif method == MAML:
      # Train the model on multiple tasks with a fixed number of gradient steps per task and a fixed number of meta-epochs
      for meta_epoch in range(meta_epochs):
        for task in tasks:
          # Get the input and output of the task
          input, output = task

          # Compute the loss on the task
          loss = model(input, output)

          # Update the model parameters with one gradient step on the task loss
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

    # Evaluate the model on a few-shot task from the dataset
    # Get the input and output of the few-shot task
    input, output = few_shot_task

    # If using PT, fine-tune the final layer of the model on a few examples from the few-shot task with a fixed number of epochs
    if method == PT:
      model.fine_tune(input, output, epochs)

    # If using MAML, adapt the model to the few-shot task with a few gradient steps on a few examples from the few-shot task
    elif method == MAML:
      for step in range(steps):
        # Compute the loss on a few examples from the few-shot task
        loss = model(input, output)

        # Update the model parameters with one gradient step on the few-shot task loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Compute the accuracy of the model on the few-shot task
    accuracy = model.accuracy(input, output)

    # Store the accuracy for later comparison
    if method == PT:
      accuracies_PT.append(accuracy)
    elif method == MAML:
      accuracies_MAML.append(accuracy)

  # Compute the effect size between PT and MAML accuracies
  d = effect_size(accuracies_PT, accuracies_MAML)

  # Compute the diversity coefficient of the dataset
  dc = diversity_coefficient(dataset)

  # Report the effect size and the diversity coefficient
  print(f"Effect size: {d}, Diversity coefficient: {dc}")
```