---
title: 2204.10965v4 CLIP-Dissect  Automatic Description of Neuron Representations in Deep Vision Networks
date: 2022-04-11
---

# [CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks](http://arxiv.org/abs/2204.10965v4)

authors: Tuomas Oikarinen, Tsui-Wei Weng


## What, Why and How

[1]: https://arxiv.org/abs/2204.10965 "[2204.10965] CLIP-Dissect: Automatic Description of Neuron ... - arXiv.org"
[2]: https://arxiv.org/abs/2204.01697 "[2204.01697] MaxViT: Multi-Axis Vision Transformer - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.10965v4 "[2204.10965v4] CLIP-Dissect: Automatic Description of Neuron ..."

Here is a summary of the paper:

- **What**: The paper proposes CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks.
- **Why**: The paper aims to provide more interpretable and explainable representations of deep vision networks by labeling internal neurons with open-ended concepts without the need for any labeled data or human examples.
- **How**: The paper leverages recent advances in multimodal vision/language models, such as CLIP, to compare the activation patterns of internal neurons with natural language descriptions of visual concepts and assign the most relevant labels to each neuron. The paper also evaluates the accuracy and quality of the labels using ground-truth annotations and human judgments.

## Main Contributions

According to the paper, the main contributions are:

- A novel technique to automatically label internal neurons in vision networks with open-ended concepts using multimodal vision/language models.
- A comprehensive evaluation of the proposed technique on five layers of ResNet-50 and comparison with existing methods.
- A demonstration of the usefulness and generality of the proposed technique for various applications, such as neuron editing, neuron visualization, and network pruning.

## Method Summary

The method section of the paper consists of three subsections:

- **Neuron Labeling**: The paper describes how to use CLIP, a multimodal vision/language model, to assign labels to internal neurons based on their activation patterns. The paper defines a neuron activation vector as the average activation of a neuron over a set of images, and a concept description vector as the CLIP text embedding of a natural language description of a visual concept. The paper then computes the cosine similarity between the neuron activation vector and the concept description vector for each neuron and each concept, and selects the top-k concepts with the highest similarity as the labels for each neuron.
- **Label Evaluation**: The paper evaluates the accuracy and quality of the labels using two metrics: label precision and label diversity. Label precision measures how well the labels match the ground-truth annotations of the last layer neurons, which are available from ImageNet classes. Label diversity measures how many distinct concepts are covered by the labels across all neurons in a layer. The paper compares the proposed technique with existing methods, such as NetDissect and Concept Bottleneck Models, on both metrics using ResNet-50 as the vision network.
- **Label Applications**: The paper demonstrates the usefulness and generality of the labels for various applications, such as neuron editing, neuron visualization, and network pruning. Neuron editing allows modifying the output of a vision network by manipulating the activation of individual neurons based on their labels. Neuron visualization allows generating synthetic images that maximally activate a given neuron or a combination of neurons based on their labels. Network pruning allows reducing the size and complexity of a vision network by removing neurons with low activation or redundant labels. The paper shows examples of these applications using ResNet-50 and CLIP-Dissect labels.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Load a vision network (e.g., ResNet-50) and a multimodal vision/language model (e.g., CLIP)
vision_network = load_vision_network()
clip_model = load_clip_model()

# Define a set of images and a set of natural language descriptions of visual concepts
images = load_images()
concepts = load_concepts()

# For each layer of the vision network
for layer in vision_network.layers:

  # For each neuron in the layer
  for neuron in layer.neurons:

    # Compute the neuron activation vector as the average activation over the images
    neuron_activation_vector = average(neuron.activate(images))

    # For each concept in the concepts
    for concept in concepts:

      # Compute the concept description vector as the CLIP text embedding of the concept
      concept_description_vector = clip_model.embed_text(concept)

      # Compute the cosine similarity between the neuron activation vector and the concept description vector
      similarity = cosine_similarity(neuron_activation_vector, concept_description_vector)

      # Store the similarity score for the neuron-concept pair
      store_similarity(neuron, concept, similarity)

  # For each neuron in the layer
  for neuron in layer.neurons:

    # Select the top-k concepts with the highest similarity scores as the labels for the neuron
    labels = select_top_k_concepts(neuron, k)

    # Store the labels for the neuron
    store_labels(neuron, labels)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip

# Load a vision network (e.g., ResNet-50) and a multimodal vision/language model (e.g., CLIP)
vision_network = torchvision.models.resnet50(pretrained=True)
clip_model, clip_preprocess = clip.load("ViT-B/32", device="cuda")

# Define a set of images and a set of natural language descriptions of visual concepts
images = load_images() # a list of PIL images
concepts = load_concepts() # a list of strings

# Preprocess the images for the vision network and the CLIP model
vision_images = torch.stack([torchvision.transforms.ToTensor()(image) for image in images]).to("cuda")
clip_images = torch.stack([clip_preprocess(image) for image in images]).to("cuda")

# Define a function to register a forward hook for a given layer
def register_hook(layer):
  def hook(module, input, output):
    # Store the output of the layer as a global variable
    global layer_output
    layer_output = output
  # Register the hook and return the handle
  handle = layer.register_forward_hook(hook)
  return handle

# Define a function to compute the neuron activation vector for a given neuron index
def compute_neuron_activation_vector(neuron_index):
  # Forward pass the images through the vision network
  vision_network(vision_images)
  # Extract the activation of the neuron from the layer output
  neuron_activation = layer_output[:, neuron_index, :, :]
  # Average the activation over the spatial and batch dimensions
  neuron_activation_vector = neuron_activation.mean(dim=(0, 2, 3))
  # Return the neuron activation vector
  return neuron_activation_vector

# Define a function to compute the concept description vector for a given concept
def compute_concept_description_vector(concept):
  # Encode the concept as a CLIP text embedding
  concept_description_vector = clip_model.encode_text(clip.tokenize([concept]).to("cuda"))
  # Return the concept description vector
  return concept_description_vector

# Define a function to compute the cosine similarity between two vectors
def cosine_similarity(vector1, vector2):
  # Normalize the vectors to unit length
  vector1 = vector1 / vector1.norm()
  vector2 = vector2 / vector2.norm()
  # Compute the dot product between the vectors
  similarity = torch.dot(vector1, vector2)
  # Return the similarity score
  return similarity

# Define a dictionary to store the similarity scores for each neuron-concept pair
similarity_scores = {}

# Define a dictionary to store the labels for each neuron
neuron_labels = {}

# Define the number of concepts to select as labels for each neuron
k = 10

# For each layer of interest in the vision network (e.g., conv1, layer1, layer2, layer3, layer4)
for layer_name in ["conv1", "layer1", "layer2", "layer3", "layer4"]:

  # Get the layer object from the vision network by name
  layer = getattr(vision_network, layer_name)

  # Register a forward hook for the layer and get the handle
  handle = register_hook(layer)

  # Get the number of neurons (channels) in the layer
  num_neurons = layer_output.shape[1]

  # For each neuron in the layer
  for neuron_index in range(num_neurons):

    # Compute the neuron activation vector for the neuron index
    neuron_activation_vector = compute_neuron_activation_vector(neuron_index)

    # For each concept in the concepts
    for concept in concepts:

      # Compute the concept description vector for the concept
      concept_description_vector = compute_concept_description_vector(concept)

      # Compute the cosine similarity between the neuron activation vector and the concept description vector
      similarity = cosine_similarity(neuron_activation_vector, concept_description_vector)

      # Store the similarity score for the neuron-concept pair in the dictionary
      similarity_scores[(layer_name, neuron_index, concept)] = similarity

    # Select the top-k concepts with the highest similarity scores as the labels for the neuron
    labels = sorted(concepts, key=lambda c: similarity_scores[(layer_name, neuron_index, c)], reverse=True)[:k]

    # Store the labels for the neuron in the dictionary
    neuron_labels[(layer_name, neuron_index)] = labels

  # Remove the forward hook for the layer using the handle
  handle.remove()
```