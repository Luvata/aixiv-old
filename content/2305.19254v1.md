---
title: 2305.19254v1 What Can We Learn from Unlearnable Datasets?
date: 2023-05-20
---

# [What Can We Learn from Unlearnable Datasets?](http://arxiv.org/abs/2305.19254v1)

authors: Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom Goldstein


## What, Why and How

[1]: https://arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org"
[2]: https://arxiv.org/pdf/2305.19250v1.pdf "arXiv:2305.19250v1 [math.AT] 30 May 2023"
[3]: http://export.arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets?"

Sure, I can try to summarize the paper for you. Here is my attempt:

**What**: The paper is about **unlearnable datasets**, which are methods to protect data privacy by preventing deep neural networks from generalizing on them. The paper challenges some of the assumptions and claims about unlearnable datasets, and proposes an attack to learn from them.

**Why**: The paper aims to show that unlearnable datasets are **not effective** in preserving data privacy, and that neural networks can still learn useful features from them that can be reweighed for high test performance. The paper also wants to understand the conditions under which unlearnable datasets induce learning shortcuts, and how to exploit them.

**How**: The paper uses a combination of **theoretical analysis** and **empirical experiments** to make its findings. The paper first reviews some of the existing methods to create unlearnable datasets, such as adding linearly separable perturbations or adversarial examples. The paper then provides a counterexample to show that linear separability is not a necessary condition for unlearnability, and proposes an orthogonal projection attack that can learn from unlearnable datasets published in ICML 2021 and ICLR 2023. The paper also demonstrates that networks trained on unlearnable datasets can be fine-tuned to achieve high accuracy on clean data, suggesting that they do not forget the original features.

I hope this summary is helpful. You can find the full paper at [^1^][1].

[^1^][1]: [2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org


## Main Contributions

[1]: https://arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org"
[2]: https://arxiv.org/pdf/2305.19250v1.pdf "arXiv:2305.19250v1 [math.AT] 30 May 2023"
[3]: http://export.arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets?"

According to the paper, the main contributions are:

- They show that **neural networks trained on unlearnable datasets can learn useful features** that can be reweighed for high test performance, suggesting that image privacy is not preserved.
- They provide a **counterexample** to demonstrate that **linear separability of perturbations is not a necessary condition** for unlearnability, and explain why it is not a reliable indicator of learning shortcuts.
- They propose an **orthogonal projection attack** which allows learning from unlearnable datasets published in ICML 2021 and ICLR 2023, and show that it is **significantly less complex** than recently proposed techniques.

You can find these contributions in the abstract and the conclusion sections of the paper at [^1^][1].

[^1^][1]: [2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org


## Method Summary

[1]: https://arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org"
[2]: https://arxiv.org/pdf/2305.19250v1.pdf "arXiv:2305.19250v1 [math.AT] 30 May 2023"
[3]: http://export.arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets?"

Here is my summary of the method section of the paper:

The paper uses two types of methods: **theoretical analysis** and **empirical experiments**.

The theoretical analysis consists of:

- Providing a **counterexample** to show that linear separability of perturbations is not a necessary condition for unlearnability, and explaining why it is not a reliable indicator of learning shortcuts. The counterexample is based on adding adversarial examples to the dataset, which are not linearly separable from the clean data, but still make the dataset unlearnable.
- Proposing an **orthogonal projection attack** which allows learning from unlearnable datasets published in ICML 2021 and ICLR 2023. The attack is based on projecting the perturbed data onto the orthogonal complement of the perturbation subspace, and then training a network on the projected data. The attack is significantly less complex than recently proposed techniques, such as gradient descent inversion or gradient matching.

The empirical experiments consist of:

- Demonstrating that **neural networks trained on unlearnable datasets can learn useful features** that can be reweighed for high test performance, suggesting that image privacy is not preserved. The experiments use two methods to reweigh the features: fine-tuning and linear probing. The experiments show that both methods can achieve high accuracy on clean data, even when the networks are trained on unlearnable datasets with different types of perturbations.
- Evaluating the **orthogonal projection attack** on various unlearnable datasets, and comparing it with other attacks and baselines. The experiments show that the orthogonal projection attack can achieve high accuracy on clean data, and outperforms other attacks and baselines in most cases.

You can find the details of the methods in sections 3 and 4 of the paper at [^1^][1].

[^1^][1]: [2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org


## Pseudo Code - High level

[1]: https://arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org"
[2]: https://arxiv.org/pdf/2305.19250v1.pdf "arXiv:2305.19250v1 [math.AT] 30 May 2023"
[3]: http://export.arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets?"

Here is my attempt to give the high-level pseudo code for the paper:

The paper has two main algorithms: **fine-tuning** and **orthogonal projection attack**.

The fine-tuning algorithm is:

```python
# Input: a network f trained on an unlearnable dataset D, a clean dataset D', a learning rate lr, and a number of epochs T
# Output: a fine-tuned network f' that performs well on D'

# Initialize f' as f
f' = f

# Loop for T epochs
for t in range(T):

  # Shuffle D'
  D' = shuffle(D')

  # Loop over batches of D'
  for x, y in batch(D'):

    # Compute the loss of f' on the batch
    loss = cross_entropy(f'(x), y)

    # Update f' using gradient descent
    f' = f' - lr * gradient(loss, f')

# Return f'
return f'
```

The orthogonal projection attack algorithm is:

```python
# Input: an unlearnable dataset D with perturbations v, a clean dataset D', a learning rate lr, and a number of epochs T
# Output: a network f that performs well on D'

# Initialize a random network f
f = random_network()

# Compute the orthogonal projection matrix P
P = I - v * v.T / norm(v)**2

# Loop for T epochs
for t in range(T):

  # Shuffle D
  D = shuffle(D)

  # Loop over batches of D
  for x, y in batch(D):

    # Project the batch onto the orthogonal complement of v
    x' = P * x

    # Compute the loss of f on the projected batch
    loss = cross_entropy(f(x'), y)

    # Update f using gradient descent
    f = f - lr * gradient(loss, f')

# Return f
return f
```

You can find the details of the algorithms in section 4 of the paper at [^1^][1].

[^1^][1]: [2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org"
[2]: https://arxiv.org/pdf/2305.19250v1.pdf "arXiv:2305.19250v1 [math.AT] 30 May 2023"
[3]: http://export.arxiv.org/abs/2305.19254 "[2305.19254] What Can We Learn from Unlearnable Datasets?"

Here is my attempt to give the detailed pseudo code for the paper:

The paper has two main algorithms: **fine-tuning** and **orthogonal projection attack**.

The fine-tuning algorithm is:

```python
# Input: a network f trained on an unlearnable dataset D, a clean dataset D', a learning rate lr, a number of epochs T, and a batch size B
# Output: a fine-tuned network f' that performs well on D'

# Import PyTorch library
import torch

# Initialize f' as f
f' = f

# Define the cross-entropy loss function
loss_fn = torch.nn.CrossEntropyLoss()

# Define the stochastic gradient descent optimizer
optimizer = torch.optim.SGD(f'.parameters(), lr=lr)

# Loop for T epochs
for t in range(T):

  # Shuffle D'
  D' = shuffle(D')

  # Initialize the running loss
  running_loss = 0.0

  # Loop over batches of D'
  for i in range(0, len(D'), B):

    # Get the batch of inputs and labels
    x, y = D'[i:i+B]

    # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward pass
    outputs = f'(x)

    # Compute the loss
    loss = loss_fn(outputs, y)

    # Backward pass and optimize
    loss.backward()
    optimizer.step()

    # Print statistics
    running_loss += loss.item()
    if i % (10 * B) == 0:
      print('[%d, %5d] loss: %.3f' % (t + 1, i + B, running_loss / 10))
      running_loss = 0.0

# Return f'
return f'
```

The orthogonal projection attack algorithm is:

```python
# Input: an unlearnable dataset D with perturbations v, a clean dataset D', a learning rate lr, a number of epochs T, and a batch size B
# Output: a network f that performs well on D'

# Import PyTorch library
import torch

# Initialize a random network f
f = torch.nn.Sequential(
  torch.nn.Conv2d(3, 32, kernel_size=3),
  torch.nn.ReLU(),
  torch.nn.MaxPool2d(2),
  torch.nn.Conv2d(32, 64, kernel_size=3),
  torch.nn.ReLU(),
  torch.nn.MaxPool2d(2),
  torch.nn.Flatten(),
  torch.nn.Linear(64 * 6 * 6, 128),
  torch.nn.ReLU(),
  torch.nn.Linear(128, 10)
)

# Define the cross-entropy loss function
loss_fn = torch.nn.CrossEntropyLoss()

# Define the stochastic gradient descent optimizer
optimizer = torch.optim.SGD(f.parameters(), lr=lr)

# Compute the orthogonal projection matrix P
P = torch.eye(3 * 32 * 32) - v * v.T / torch.norm(v)**2

# Loop for T epochs
for t in range(T):

  # Shuffle D
  D = shuffle(D)

  # Initialize the running loss
  running_loss = 0.0

  # Loop over batches of D
  for i in range(0, len(D), B):

    # Get the batch of inputs and labels
    x, y = D[i:i+B]

    # Reshape x to a matrix of size (B, C * H * W)
    x = x.view(B, -1)

    # Project x onto the orthogonal complement of v
    x' = P @ x.T

    # Reshape x' back to a tensor of size (B, C, H, W)
    x' = x'.T.view(B, 3, 32, 32)

    # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward pass
    outputs = f(x')

    # Compute the loss
    loss = loss_fn(outputs, y)

    # Backward pass and optimize
    loss.backward()
    optimizer.step()

    # Print statistics
    running_loss += loss.item()
    if i % (10 * B) == 0:
      print('[%d, %5d] loss: %.3f' % (t + 1, i + B, running_loss / 10))
      running_loss = 0.0

# Return f
return f
```

You can find the details of the algorithms in section 4 of the paper at [^1^][1].

[^1^][1]: [2305.19254] What Can We Learn from Unlearnable Datasets? - arXiv.org