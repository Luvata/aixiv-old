---
title: 2306.13103v1 Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks
date: 2023-06-14
---

# [Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks](http://arxiv.org/abs/2306.13103v1)

authors: Hongcheng Gao, Hao Zhang, Yinpeng Dong, Zhijie Deng


## What, Why and How

[1]: https://arxiv.org/abs/2306.13103 "[2306.13103] Evaluating the Robustness of Text-to-image Diffusion ..."
[2]: https://arxiv.org/pdf/2306.13103 "Abstract - arXiv.org"
[3]: https://arxiv.org/pdf/2305.13103v1.pdf "arXiv:2305.13103v1 [hep-th] 22 May 2023"

Here is a summary of the paper:

```
What: The paper evaluates the robustness of text-to-image diffusion models against real-world attacks that introduce realistic errors to the input texts.

Why: The paper aims to answer whether existing text-to-image diffusion models are robust against variations over input texts, which is important for their safety and fidelity in real-world applications.

How: The paper develops novel distribution-based attack objectives to mislead text-to-image diffusion models in a black-box manner, and performs extensive experiments on popular models to demonstrate their effectiveness and reveal their non-trivial robustness issues.
```

## Main Contributions

[1]: https://arxiv.org/abs/2306.13103 "[2306.13103] Evaluating the Robustness of Text-to-image Diffusion ..."
[2]: https://arxiv.org/pdf/2306.13103 "Abstract - arXiv.org"
[3]: https://arxiv.org/pdf/2305.13103v1.pdf "arXiv:2305.13103v1 [hep-th] 22 May 2023"

The paper claims to make the following contributions:

- It provides the first robustness evaluation of text-to-image diffusion models against real-world attacks that introduce realistic errors to the input texts.
- It develops novel distribution-based attack objectives to mislead text-to-image diffusion models in a black-box manner, without any knowledge of the model.
- It performs extensive experiments on popular text-to-image diffusion models and reveals their non-trivial robustness issues.
- It provides an in-depth analysis of its method and shows that it is not designed to attack the text encoder in text-to-image diffusion models solely.

## Method Summary

[1]: https://arxiv.org/abs/2306.13103 "[2306.13103] Evaluating the Robustness of Text-to-image Diffusion ..."
[2]: https://arxiv.org/pdf/2306.13103 "Abstract - arXiv.org"
[3]: https://arxiv.org/pdf/2305.13103v1.pdf "arXiv:2305.13103v1 [hep-th] 22 May 2023"

Here is a summary of the method section:

```
The method consists of three main steps: attack space construction, attack objective formulation, and attack algorithm design.

- Attack space construction: The authors define an attack space that consists of realistic errors that humans can make when writing texts, such as typo, glyph, phonetic, and semantic errors. They ensure that the altered texts are semantically consistent with the original texts and do not introduce apocryphal information.
- Attack objective formulation: The authors propose two distribution-based attack objectives to measure the discrepancy between the distributions of images generated from the original and altered texts. The first objective is based on the Wasserstein distance and the second one is based on the Kullback-Leibler divergence. They also introduce a regularization term to avoid trivial solutions that make the altered texts unreadable or meaningless.
- Attack algorithm design: The authors design a black-box attack algorithm that does not require any knowledge of the text-to-image diffusion model. The algorithm iteratively updates the altered texts by estimating the gradients of the attack objectives using finite differences and applying projected gradient descent. The algorithm terminates when a successful attack is found or a maximum number of iterations is reached.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.13103 "[2306.13103] Evaluating the Robustness of Text-to-image Diffusion ..."
[2]: https://arxiv.org/pdf/2306.13103 "Abstract - arXiv.org"
[3]: https://arxiv.org/pdf/2305.13103v1.pdf "arXiv:2305.13103v1 [hep-th] 22 May 2023"

Here is a high-level pseudo code for the paper:

```
# Input: a text-to-image diffusion model F, an original text x, a set of realistic errors E
# Output: an altered text x' that successfully attacks F

# Step 1: Construct the attack space
A = {} # initialize an empty set of altered texts
for each error e in E:
  x_e = apply e to x # generate an altered text by applying e to x
  if x_e is semantically consistent with x: # check semantic consistency
    A = A union {x_e} # add x_e to the attack space

# Step 2: Formulate the attack objective
L = 0 # initialize the attack objective
for each x' in A:
  D_x = F(x) # generate a distribution of images from x
  D_x' = F(x') # generate a distribution of images from x'
  L = L + W(D_x, D_x') + K(D_x, D_x') + R(x') # compute the attack objective as a combination of Wasserstein distance, KL divergence, and regularization

# Step 3: Design the attack algorithm
x' = x # initialize the altered text as the original text
while L > epsilon and t < T: # repeat until the attack objective is below a threshold or the maximum number of iterations is reached
  g = estimate_gradient(L, x') # estimate the gradient of the attack objective with respect to x' using finite differences
  x' = project(x' - alpha * g) # update x' by applying projected gradient descent
  t = t + 1 # increment the iteration counter
return x' # return the altered text
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2306.13103 "[2306.13103] Evaluating the Robustness of Text-to-image Diffusion ..."
[2]: https://arxiv.org/pdf/2306.13103 "Abstract - arXiv.org"
[3]: https://arxiv.org/pdf/2305.13103v1.pdf "arXiv:2305.13103v1 [hep-th] 22 May 2023"

Here is a detailed pseudo code for the paper:

```
# Input: a text-to-image diffusion model F, an original text x, a set of realistic errors E, a threshold epsilon, a step size alpha, and a maximum number of iterations T
# Output: an altered text x' that successfully attacks F

# Step 1: Construct the attack space
A = {} # initialize an empty set of altered texts
for each error e in E:
  x_e = apply e to x # generate an altered text by applying e to x
  if x_e is semantically consistent with x: # check semantic consistency using a pre-trained language model
    A = A union {x_e} # add x_e to the attack space

# Step 2: Formulate the attack objective
L = 0 # initialize the attack objective
for each x' in A:
  D_x = F(x) # generate a distribution of images from x by sampling from the reverse process of F
  D_x' = F(x') # generate a distribution of images from x' by sampling from the reverse process of F
  L = L + W(D_x, D_x') + K(D_x, D_x') + R(x') # compute the attack objective as a combination of Wasserstein distance, KL divergence, and regularization
  # W(D_x, D_x') is computed by approximating the optimal transport map using Sinkhorn iterations
  # K(D_x, D_x') is computed by estimating the entropy and cross-entropy terms using Monte Carlo sampling
  # R(x') is computed by measuring the edit distance between x and x' normalized by the length of x

# Step 3: Design the attack algorithm
x' = x # initialize the altered text as the original text
t = 0 # initialize the iteration counter
while L > epsilon and t < T: # repeat until the attack objective is below a threshold or the maximum number of iterations is reached
  g = 0 # initialize the gradient estimate
  for each character c in x':
    for each possible replacement r in E:
      x'_r = replace c with r in x' # generate a perturbed text by replacing c with r in x'
      L_r = compute L for x'_r using Step 2 # compute the attack objective for x'_r
      g_c = (L_r - L) / (r - c) # estimate the gradient for c using finite differences
      g = g + g_c # accumulate the gradient estimate
  x' = project(x' - alpha * g) # update x' by applying projected gradient descent and clipping each character to its valid range
  t = t + 1 # increment the iteration counter
return x' # return the altered text
```