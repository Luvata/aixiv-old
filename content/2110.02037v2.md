---
title: 2110.02037v2 Autoregressive Diffusion Models
date: 2021-10-03
---

# [Autoregressive Diffusion Models](http://arxiv.org/abs/2110.02037v2)

authors: Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans


## What, Why and How

[1]: https://arxiv.org/pdf/2110.02037v2.pdf "Rianne van den Berg, Tim Salimans - arXiv.org"
[2]: https://arxiv.org/abs/2110.02037 "[2110.02037] Autoregressive Diffusion Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.02037v2 "[2112.02037v2] Lithium-ion battery degradation: how to model it"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces Autoregressive Diffusion Models (ARDMs), a new class of generative models that can handle discrete data and support parallel generation.
- **Why**: The paper aims to overcome the limitations of existing models such as Autoregressive Models (ARMs) and discrete diffusion models, which either require a fixed generation order, causal masking, or a large number of network calls to generate data.
- **How**: The paper shows that ARDMs generalize order-agnostic ARMs and absorbing discrete diffusion models under mild assumptions. The paper also proposes an efficient training objective and a flexible sampling scheme for ARDMs. The paper demonstrates the effectiveness of ARDMs on image generation and lossless compression tasks.

## Main Contributions

[1]: https://arxiv.org/pdf/2110.02037v2.pdf "Rianne van den Berg, Tim Salimans - arXiv.org"
[2]: https://arxiv.org/abs/2110.02037 "[2110.02037] Autoregressive Diffusion Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.02037v2 "[2112.02037v2] Lithium-ion battery degradation: how to model it"

The paper[^1^][1] claims the following contributions:

- **A new model class**: The paper introduces ARDMs, which can model discrete data without requiring a fixed generation order or causal masking, and can generate data in parallel with a flexible budget.
- **A theoretical analysis**: The paper shows that ARDMs generalize order-agnostic ARMs and absorbing discrete diffusion models under mild assumptions, and derive the optimal generation order for ARDMs.
- **An efficient training objective**: The paper proposes a training objective for ARDMs that is similar to modern probabilistic diffusion models and scales well to high-dimensional data.
- **Empirical results**: The paper demonstrates that ARDMs outperform discrete diffusion models on image generation tasks with fewer steps, and achieve state-of-the-art results on lossless compression tasks with fewer network calls.

## Method Summary

[1]: https://arxiv.org/pdf/2110.02037v2.pdf "Rianne van den Berg, Tim Salimans - arXiv.org"
[2]: https://arxiv.org/abs/2110.02037 "[2110.02037] Autoregressive Diffusion Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.02037v2 "[2112.02037v2] Lithium-ion battery degradation: how to model it"

Here is a summary of the method section of the paper[^1^][1]:

- **The ARDM framework**: The paper defines ARDMs as a class of models that can generate discrete data by applying a series of autoregressive transformations to a random initial state. The paper also shows how ARDMs can be seen as a generalization of order-agnostic ARMs and absorbing discrete diffusion models under mild assumptions.
- **The generation order**: The paper derives the optimal generation order for ARDMs based on minimizing the expected number of network calls. The paper also proposes a heuristic algorithm to approximate the optimal order efficiently.
- **The training objective**: The paper proposes a training objective for ARDMs that is similar to modern probabilistic diffusion models and scales well to high-dimensional data. The paper also shows how to compute the gradients of the objective using the chain rule and the log-derivative trick.
- **The sampling scheme**: The paper proposes a flexible sampling scheme for ARDMs that can generate data in parallel with a given budget. The paper also shows how to adapt the scheme to different scenarios such as sequential or batched generation.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the ARDM model class
class ARDM(nn.Module):
  def __init__(self, num_steps, num_categories):
    # Initialize the model parameters
    self.num_steps = num_steps
    self.num_categories = num_categories
    self.embeddings = nn.Embedding(num_categories, num_categories)
    self.transformer = nn.Transformer(num_categories, num_categories)

  def forward(self, x):
    # Apply the autoregressive transformations to x
    for t in range(self.num_steps):
      # Embed x into a latent space
      z = self.embeddings(x)
      # Apply the transformer to z
      z = self.transformer(z)
      # Sample x from a categorical distribution based on z
      x = torch.multinomial(torch.softmax(z, dim=-1), 1)
    return x

# Define the optimal generation order function
def optimal_order(model, budget):
  # Initialize the order and the cost
  order = []
  cost = 0
  # Loop until the budget is exhausted or all variables are generated
  while cost < budget and len(order) < model.num_categories:
    # Find the variable that minimizes the expected cost
    min_cost = float('inf')
    min_var = None
    for i in range(model.num_categories):
      if i not in order:
        # Compute the expected cost of generating i given the current order
        exp_cost = expected_cost(model, order + [i])
        if exp_cost < min_cost:
          min_cost = exp_cost
          min_var = i
    # Add the variable to the order and update the cost
    order.append(min_var)
    cost += min_cost
  return order

# Define the training objective function
def train(model, data_loader, optimizer):
  # Loop over the data batches
  for x in data_loader:
    # Initialize the loss
    loss = 0
    # Loop over the model steps
    for t in range(model.num_steps):
      # Embed x into a latent space
      z = model.embeddings(x)
      # Apply the transformer to z
      z = model.transformer(z)
      # Compute the negative log-likelihood of x given z
      nll = -torch.log(torch.gather(torch.softmax(z, dim=-1), -1, x.unsqueeze(-1)).squeeze(-1))
      # Compute the gradient of nll with respect to z using the log-derivative trick
      grad_nll = -torch.autograd.grad(nll.sum(), z, retain_graph=True)[0] / torch.softmax(z, dim=-1)
      # Compute the loss as the inner product of grad_nll and z
      loss += (grad_nll * z).sum()
    # Update the model parameters using the optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Define the sampling scheme function
def sample(model, budget):
  # Initialize the output and the order
  output = torch.randint(model.num_categories, (model.num_categories,))
  order = optimal_order(model, budget)
  # Loop over the order
  for i in order:
    # Embed output into a latent space
    z = model.embeddings(output)
    # Apply the transformer to z
    z = model.transformer(z)
    # Sample output[i] from a categorical distribution based on z[i]
    output[i] = torch.multinomial(torch.softmax(z[i], dim=-1), 1)
  return output

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary modules
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data

# Define the hyperparameters
num_steps = 10 # The number of autoregressive steps
num_categories = 256 # The number of categories for discrete data
num_layers = 6 # The number of layers for the transformer
num_heads = 8 # The number of heads for the transformer
dim_model = 512 # The dimension of the model
dim_feedforward = 2048 # The dimension of the feedforward network in the transformer
dropout = 0.1 # The dropout rate for the transformer
batch_size = 64 # The batch size for training and sampling
num_epochs = 100 # The number of epochs for training
learning_rate = 0.0001 # The learning rate for the optimizer
budget = 1000 # The budget for sampling

# Define the ARDM model class
class ARDM(nn.Module):
  def __init__(self, num_steps, num_categories, num_layers, num_heads, dim_model, dim_feedforward, dropout):
    # Initialize the model parameters
    super().__init__()
    self.num_steps = num_steps
    self.num_categories = num_categories
    self.embeddings = nn.Embedding(num_categories, dim_model)
    self.transformer = nn.Transformer(dim_model, num_heads, num_layers, dim_feedforward, dropout)

  def forward(self, x):
    # Apply the autoregressive transformations to x
    for t in range(self.num_steps):
      # Embed x into a latent space
      z = self.embeddings(x)
      # Apply the transformer to z
      z = self.transformer(z)
      # Sample x from a categorical distribution based on z
      x = torch.multinomial(torch.softmax(z, dim=-1), 1)
    return x

# Define the expected cost function
def expected_cost(model, order):
  # Initialize the expected cost and the probability vector
  exp_cost = 0
  p = torch.ones(model.num_categories) / model.num_categories
  # Loop over the order
  for i in order:
    # Update the expected cost as the entropy of p[i]
    exp_cost += -p[i] * torch.log(p[i])
    # Update p as the marginal distribution of p given i using Bayes' rule
    p = p * model.transformer(model.embeddings(torch.arange(model.num_categories)))[:, i]
    p /= p.sum()
  return exp_cost

# Define the optimal generation order function
def optimal_order(model, budget):
  # Initialize the order and the cost
  order = []
  cost = 0
  # Loop until the budget is exhausted or all variables are generated
  while cost < budget and len(order) < model.num_categories:
    # Find the variable that minimizes the expected cost
    min_cost = float('inf')
    min_var = None
    for i in range(model.num_categories):
      if i not in order:
        # Compute the expected cost of generating i given the current order
        exp_cost = expected_cost(model, order + [i])
        if exp_cost < min_cost:
          min_cost = exp_cost
          min_var = i
    # Add the variable to the order and update the cost
    order.append(min_var)
    cost += min_cost
  return order

# Define the training objective function
def train(model, data_loader, optimizer):
  # Loop over the data batches
  for x in data_loader:
    # Initialize the loss
    loss = 0
    # Loop over the model steps
    for t in range(model.num_steps):
      # Embed x into a latent space
      z = model.embeddings(x)
      # Apply the transformer to z
      z = model.transformer(z)
      # Compute the negative log-likelihood of x given z using cross-entropy loss 
      nll = nn.CrossEntropyLoss()(z.view(-1, model.num_categories), x.view(-1))
      # Compute the gradient of nll with respect to z using the log-derivative trick 
      grad_nll = -torch.autograd.grad(nll.sum(), z, retain_graph=True)[0] / torch.softmax(z, dim=-1)
      # Compute the loss as the inner product of grad_nll and z 
      loss += (grad_nll * z).sum()
    # Update the model parameters using the optimizer 
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Define the sampling scheme function 
def sample(model, budget):
  # Initialize the output and the order 
  output = torch.randint(model.num_categories, (model.num_categories,))
  order = optimal_order(model, budget)
  # Loop over the order 
  for i in order:
    # Embed output into a latent space 
    z = model.embeddings(output)
    # Apply the transformer to z 
    z = model.transformer(z)
    # Sample output[i] from a categorical distribution based on z[i] 
    output[i] = torch.multinomial(torch.softmax(z[i], dim=-1), 1)
  return output

# Load the data 
data_loader = data.DataLoader(data.Dataset(...), batch_size=batch_size, shuffle=True)

# Create the model 
model = ARDM(num_steps, num_categories, num_layers, num_heads, dim_model, dim_feedforward, dropout)

# Create the optimizer 
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Train the model 
for epoch in range(num_epochs):
  train(model, data_loader, optimizer)
  print(f"Epoch {epoch}: Loss {loss.item()}")

# Sample from the model 
output = sample(model, budget)
print(f"Output: {output}")
```