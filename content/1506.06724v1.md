---
title: 1506.06724v1 Aligning Books and Movies  Towards Story-like Visual Explanations by Watching Movies and Reading Books
date: 2015-06-07
---

# [Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books](http://arxiv.org/abs/1506.06724v1)

authors: Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler


## What, Why and How

[1]: https://arxiv.org/abs/1506.06724 "[1506.06724] Aligning Books and Movies: Towards Story-like Visual ..."
[2]: https://arxiv.org/pdf/1506.06724.pdf "Abstract arXiv:1506.06724v1 [cs.CV] 22 Jun 2015"
[3]: http://export.arxiv.org/abs/1512.06724v1 "[1512.06724v1] Prescribed curvature tensor in locally conformally flat ..."

Here is a summary of the paper:

- **What**: The paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets[^1^][1].
- **Why**: The paper argues that books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story[^1^][1]. The paper also claims that grounding descriptions in books to vision would allow us to get textual explanations or stories behind visual information rather than simplistic captions[^1^][1].
- **How**: The paper proposes a context-aware CNN to combine information from multiple sources[^1^][1]. To align movies and books, the paper exploits a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book[^1^][1]. The paper demonstrates good quantitative performance for movie/book alignment and shows several qualitative examples that showcase the diversity of tasks their model can be used for[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/1506.06724 "Aligning Books and Movies: Towards Story-like Visual Explanations by ..."
[2]: https://ieeexplore.ieee.org/document/7410368 "Aligning Books and Movies: Towards Story-Like Visual Explanations by ..."
[3]: https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.712.2025 "Aligning Books and Movies: Towards Story-like Visual Explanations by ..."
[4]: https://ui.adsabs.harvard.edu/abs/2015arXiv150606724Z/abstract "Aligning Books and Movies: Towards Story-like Visual Explanations by ..."

Some of the contributions of this paper are:

- It proposes a novel task of aligning books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets[^1^][1] [^2^][2].
- It introduces a neural sentence embedding that is trained in an unsupervised way from a large corpus of books[^1^][1] [^2^][2] [^3^][3] [^4^][4].
- It develops a video-text neural embedding for computing similarities between movie clips and sentences in the book[^1^][1] [^2^][2] [^3^][3] [^4^][4].
- It designs a context-aware CNN to combine information from multiple sources[^1^][1] [^2^][2].
- It demonstrates good quantitative performance for movie/book alignment and shows several qualitative examples that showcase the diversity of tasks their model can be used for[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/abs/1506.06724 "Aligning Books and Movies: Towards Story-like Visual Explanations by ..."
[2]: https://ieeexplore.ieee.org/document/7410368 "Aligning Books and Movies: Towards Story-Like Visual Explanations by ..."
[3]: https://www.researchgate.net/publication/279068490_Aligning_Books_and_Movies_Towards_Story-Like_Visual_Explanations_by_Watching_Movies_and_Reading_Books "Aligning Books and Movies: Towards Story-Like Visual Explanations by ..."
[4]: https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.712.2025 "Aligning Books and Movies: Towards Story-like Visual Explanations by ..."

Here is a summary of the method section of the paper:

- The paper proposes a neural sentence embedding that is trained in an unsupervised way from a large corpus of books[^1^][1] [^2^][2] [^3^][3] [^4^][4]. The embedding is based on the skip-thought vector model [^1^][1] that encodes sentences into fixed-length vectors that capture their semantic and syntactic properties.
- The paper also develops a video-text neural embedding for computing similarities between movie clips and sentences in the book[^1^][1] [^2^][2] [^3^][3] [^4^][4]. The embedding is based on the Fisher vector representation [^1^][1] that aggregates local features extracted from video frames and audio signals into a global descriptor.
- The paper then designs a context-aware CNN to combine information from multiple sources[^1^][1] [^2^][2]. The CNN takes as input the video-text neural embedding, the subtitle embedding, and the temporal context embedding, and outputs a score for each candidate sentence in the book for a given movie clip[^1^][1].
- The paper finally describes how to use the context-aware CNN to perform movie/book alignment[^1^][1]. The alignment is formulated as an optimization problem that maximizes the sum of scores for all movie clips while satisfying some constraints on the order and length of the sentences[^1^][1]. The paper uses dynamic programming to solve the problem efficiently[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the neural sentence embedding model
def skip_thought(sentence):
  # Encode the sentence into a fixed-length vector using an RNN
  # Return the vector

# Define the video-text neural embedding model
def fisher_vector(clip):
  # Extract local features from video frames and audio signals using SIFT and MFCC
  # Aggregate the local features into a global descriptor using Gaussian mixture models
  # Return the descriptor

# Define the context-aware CNN model
def context_cnn(clip, sentence, subtitle, context):
  # Compute the video-text neural embedding for the clip and the sentence
  v = fisher_vector(clip)
  s = skip_thought(sentence)
  # Compute the subtitle embedding for the subtitle
  t = skip_thought(subtitle)
  # Compute the temporal context embedding for the context
  c = skip_thought(context)
  # Concatenate the embeddings and feed them to a CNN with two convolutional layers and a fully connected layer
  x = concatenate(v, s, t, c)
  y = cnn(x)
  # Return the score for the clip-sentence pair
  return y

# Define the movie/book alignment algorithm
def align(movie, book):
  # Initialize an empty list of alignments
  alignments = []
  # For each clip in the movie:
  for clip in movie:
    # Initialize an empty list of candidate sentences from the book
    candidates = []
    # For each sentence in the book:
    for sentence in book:
      # Compute the score for the clip-sentence pair using the context-aware CNN model
      score = context_cnn(clip, sentence, clip.subtitle, clip.context)
      # Add the sentence and the score to the candidates list
      candidates.append((sentence, score))
    # Sort the candidates by their scores in descending order
    candidates.sort(key=lambda x: x[1], reverse=True)
    # Select the best candidate as the alignment for the clip
    alignment = candidates[0][0]
    # Add the alignment to the alignments list
    alignments.append(alignment)
  # Return the alignments list
  return alignments
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import cv2
import librosa

# Define the hyperparameters
vocab_size = 20000 # The size of the vocabulary
embed_size = 4800 # The size of the sentence embedding
hidden_size = 2400 # The size of the RNN hidden state
num_mixtures = 64 # The number of Gaussian mixtures for Fisher vector
num_filters = 512 # The number of filters for CNN
kernel_size = 3 # The kernel size for CNN
window_size = 25 # The window size for MFCC in milliseconds
hop_size = 10 # The hop size for MFCC in milliseconds

# Define the neural sentence embedding model
class SkipThought(nn.Module):
  def __init__(self, vocab_size, embed_size, hidden_size):
    super(SkipThought, self).__init__()
    # Initialize the word embedding layer
    self.embed = nn.Embedding(vocab_size, embed_size)
    # Initialize the RNN encoder and decoder
    self.encoder = nn.GRU(embed_size, hidden_size, bidirectional=True)
    self.decoder = nn.GRU(embed_size, hidden_size * 2)
    # Initialize the linear layer for output
    self.linear = nn.Linear(hidden_size * 2, vocab_size)
  
  def forward(self, sentence):
    # Embed the sentence into a sequence of vectors
    embedded = self.embed(sentence)
    # Encode the sentence into a fixed-length vector using an RNN
    output, hidden = self.encoder(embedded)
    encoded = torch.cat((hidden[0], hidden[1]), dim=1)
    # Decode the sentence using another RNN and predict the next and previous words
    output, hidden = self.decoder(embedded, encoded.unsqueeze(0).repeat(1, 1, 1))
    output = self.linear(output)
    # Return the encoded vector and the output logits
    return encoded, output

# Define the video-text neural embedding model
class FisherVector(nn.Module):
  def __init__(self, num_mixtures):
    super(FisherVector, self).__init__()
    # Initialize the parameters of the Gaussian mixture model
    self.means = nn.Parameter(torch.randn(num_mixtures, 128))
    self.covars = nn.Parameter(torch.ones(num_mixtures, 128))
    self.weights = nn.Parameter(torch.ones(num_mixtures))
  
  def forward(self, clip):
    # Extract local features from video frames and audio signals using SIFT and MFCC
    features = []
    for frame in clip.frames:
      keypoints, descriptors = cv2.SIFT().detectAndCompute(frame, None)
      features.append(descriptors)
    mfccs = librosa.feature.mfcc(clip.audio, sr=clip.sr, n_mfcc=13,
                                 win_length=int(window_size * clip.sr / 1000),
                                 hop_length=int(hop_size * clip.sr / 1000))
    features.append(mfccs.T)
    features = torch.from_numpy(np.vstack(features))
    # Compute the posterior probabilities of the features given the Gaussian mixture model
    diff = features.unsqueeze(1) - self.means.unsqueeze(0)
    covars_inv = torch.rsqrt(self.covars + 1e-6)
    log_prob = -0.5 * torch.sum((diff * covars_inv) ** 2, dim=-1) - \
               torch.log(self.covars).sum(-1) / 2 - torch.log(self.weights) - \
               np.log(2 * np.pi) * 64
    prob = torch.softmax(log_prob, dim=-1)
    # Compute the first and second order statistics of the features
    u = torch.sum(prob.unsqueeze(-1) * (diff * covars_inv), dim=0) / torch.sqrt(self.weights)
    v = torch.sum(prob.unsqueeze(-1) * ((diff ** 2) / self.covars - 1), dim=0) / (2 * torch.sqrt(self.weights))
    # Concatenate and normalize the statistics to form a global descriptor
    descriptor = torch.cat((u.flatten(), v.flatten()))
    descriptor /= torch.norm(descriptor)
    # Return the descriptor
    return descriptor

# Define the context-aware CNN model
class ContextCNN(nn.Module):
  def __init__(self, embed_size, num_filters, kernel_size):
    super(ContextCNN, self).__init__()
    # Initialize the convolutional layers with ReLU activation and max pooling
    self.conv1 = nn.Conv1d(in_channels=4, out_channels=num_filters, kernel_size=kernel_size)
    self.relu1 = nn.ReLU()
    self.pool1 = nn.MaxPool1d(kernel_size=2)
    self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size)
    self.relu2 = nn.ReLU()
    self.pool2 = nn.MaxPool1d(kernel_size=2)
    # Initialize the fully connected layer with sigmoid activation
    self.linear = nn.Linear(in_features=num_filters * (embed_size - 2 * (kernel_size - 1) - 2) // 4, out_features=1)
    self.sigmoid = nn.Sigmoid()
  
  def forward(self, clip, sentence, subtitle, context):
    # Compute the video-text neural embedding for the clip and the sentence
    v = fisher_vector(clip)
    s = skip_thought(sentence)
    # Compute the subtitle embedding for the subtitle
    t = skip_thought(subtitle)
    # Compute the temporal context embedding for the context
    c = skip_thought(context)
    # Concatenate the embeddings and feed them to a CNN with two convolutional layers and a fully connected layer
    x = torch.cat((v, s, t, c), dim=0).unsqueeze(0)
    x = self.pool1(self.relu1(self.conv1(x)))
    x = self.pool2(self.relu2(self.conv2(x)))
    x = x.flatten()
    y = self.sigmoid(self.linear(x))
    # Return the score for the clip-sentence pair
    return y

# Define the movie/book alignment algorithm
def align(movie, book):
  # Initialize an empty list of alignments
  alignments = []
  # For each clip in the movie:
  for clip in movie:
    # Initialize an empty list of candidate sentences from the book
    candidates = []
    # For each sentence in the book:
    for sentence in book:
      # Compute the score for the clip-sentence pair using the context-aware CNN model
      score = context_cnn(clip, sentence, clip.subtitle, clip.context)
      # Add the sentence and the score to the candidates list
      candidates.append((sentence, score))
    # Sort the candidates by their scores in descending order
    candidates.sort(key=lambda x: x[1], reverse=True)
    # Select the best candidate as the alignment for the clip
    alignment = candidates[0][0]
    # Add the alignment to the alignments list
    alignments.append(alignment)
  # Return the alignments list
  return alignments
```