---
title: 2206.04647v1 VideoINR  Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution
date: 2022-06-05
---

# [VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution](http://arxiv.org/abs/2206.04647v1)

authors: Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, Xiaolong Wang


## What, Why and How

[1]: https://arxiv.org/pdf/2206.04647v1 "Abstract arXiv:2206.04647v1 [eess.IV] 9 Jun 2022"
[2]: https://arxiv.org/abs/2206.04647 "[2206.04647] VideoINR: Learning Video Implicit Neural Representation ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2206.04647 "[2206.04647] VideoINR: Learning Video Implicit Neural Representation ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes Video Implicit Neural Representation (VideoINR), a method to learn a continuous representation of video data that can be decoded to videos of arbitrary spatial resolution and frame rate.
- **Why**: The paper aims to address the problem of Space-Time Video Super-Resolution (STVSR), which is to increase the spatial resolution and frame rate of a low-resolution and low frame rate video. Existing STVSR methods only support a fixed up-sampling scale, which limits their flexibility and applications.
- **How**: The paper uses a neural network to map any 3D space-time coordinate to an RGB value, which forms an implicit representation of the video. The network is trained with a reconstruction loss and a perceptual loss on a set of discrete frames sampled from the video. The trained network can then generate high-resolution and high frame rate videos by querying any desired coordinates. The paper also introduces a temporal consistency loss to enforce smoothness across frames.

## Main Contributions

[1]: https://arxiv.org/pdf/2206.04647v1 "Abstract arXiv:2206.04647v1 [eess.IV] 9 Jun 2022"
[2]: https://arxiv.org/abs/2206.04647 "[2206.04647] VideoINR: Learning Video Implicit Neural Representation ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2206.04647 "[2206.04647] VideoINR: Learning Video Implicit Neural Representation ..."

The paper claims the following contributions[^1^][1]:

- **A novel Video Implicit Neural Representation (VideoINR) that can encode and decode videos of arbitrary spatial resolution and frame rate.**
- **A temporal consistency loss that enforces smoothness across frames and improves the quality of the generated videos.**
- **Extensive experiments on various datasets and up-sampling scales, showing that VideoINR outperforms existing STVSR methods on continuous and out-of-training-distribution scales, and achieves competitive results on common scales.**

## Method Summary

[1]: https://arxiv.org/pdf/2206.04647v1 "Abstract arXiv:2206.04647v1 [eess.IV] 9 Jun 2022"
[2]: https://arxiv.org/abs/2206.04647 "[2206.04647] VideoINR: Learning Video Implicit Neural Representation ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2206.04647 "[2206.04647] VideoINR: Learning Video Implicit Neural Representation ..."

Here is a summary of the method section of the paper[^1^][1]:

- **Video Implicit Neural Representation (VideoINR)**: The paper uses a fully connected neural network to map any 3D space-time coordinate (x,y,t) to an RGB value. The network takes the coordinate as input and passes it through several layers of sine activation functions and linear transformations. The output of the network is a 3D vector representing the color of the pixel at that coordinate. The network is trained with a reconstruction loss that measures the L1 distance between the generated and ground truth pixels, and a perceptual loss that measures the feature similarity in a pre-trained VGG network.
- **Temporal Consistency Loss**: The paper introduces a temporal consistency loss that enforces smoothness across frames. The loss is computed by taking the gradient of the RGB values along the temporal dimension and penalizing large changes. The paper also uses a temporal consistency weight that increases with the distance from the input frames, to encourage more smoothness for interpolated frames.
- **Training and Inference**: The paper trains the network on a set of discrete frames sampled from the video, with a fixed spatial resolution and frame rate. During inference, the network can generate videos of arbitrary spatial resolution and frame rate by querying any desired coordinates. The paper also uses a nearest neighbor interpolation scheme to fill in missing pixels when generating high-resolution videos.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the network architecture
def VideoINR(x,y,t):
  # x, y, t are the space-time coordinates
  # z is the latent vector
  z = [x,y,t]
  # Apply several layers of sine activation and linear transformation
  for i in range(num_layers):
    z = sin(W[i] @ z + b[i])
  # Output the RGB value
  return W_out @ z + b_out

# Define the reconstruction loss
def reconstruction_loss(x,y,t,R):
  # x, y, t are the space-time coordinates
  # R is the ground truth RGB value
  # L1 is the L1 distance function
  return L1(VideoINR(x,y,t), R)

# Define the perceptual loss
def perceptual_loss(x,y,t,R):
  # x, y, t are the space-time coordinates
  # R is the ground truth RGB value
  # VGG is a pre-trained VGG network
  # F is a feature extraction function
  return L1(F(VGG(VideoINR(x,y,t))), F(VGG(R)))

# Define the temporal consistency loss
def temporal_consistency_loss(x,y,t):
  # x, y, t are the space-time coordinates
  # grad_t is the gradient function along the temporal dimension
  return L1(grad_t(VideoINR(x,y,t)),0)

# Define the total loss
def total_loss(x,y,t,R,w):
  # x, y, t are the space-time coordinates
  # R is the ground truth RGB value
  # w is the temporal consistency weight
  return reconstruction_loss(x,y,t,R) + perceptual_loss(x,y,t,R) + w * temporal_consistency_loss(x,y,t)

# Train the network on a set of discrete frames
for epoch in range(num_epochs):
  for batch in range(num_batches):
    # Sample a set of coordinates and corresponding RGB values from the video
    x,y,t,R = sample_batch(video)
    # Compute the temporal consistency weight based on the distance from input frames
    w = compute_weight(t)
    # Update the network parameters by minimizing the total loss
    update_params(total_loss(x,y,t,R,w))

# Generate videos of arbitrary spatial resolution and frame rate
def generate_video(video, scale_s, scale_t):
  # video is the input low-resolution and low frame rate video
  # scale_s and scale_t are the spatial and temporal up-sampling scales
  # output is the output high-resolution and high frame rate video
  output = []
  for t in range(0, video.length * scale_t):
    frame = []
    for x in range(0, video.width * scale_s):
      for y in range(0, video.height * scale_s):
        # Query the network for each coordinate
        pixel = VideoINR(x/scale_s, y/scale_s, t/scale_t)
        frame.append(pixel)
    # Fill in missing pixels by nearest neighbor interpolation
    frame = interpolate(frame)
    output.append(frame)
  return output

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models

# Define the network architecture
class VideoINR(nn.Module):
  def __init__(self, num_layers, hidden_dim, out_dim):
    super(VideoINR, self).__init__()
    # num_layers is the number of hidden layers
    # hidden_dim is the dimension of the hidden units
    # out_dim is the dimension of the output units (3 for RGB)
    self.num_layers = num_layers
    self.hidden_dim = hidden_dim
    self.out_dim = out_dim
    # Initialize the network parameters randomly
    self.W = nn.ParameterList([nn.Parameter(torch.randn(hidden_dim, hidden_dim)) for i in range(num_layers)])
    self.b = nn.ParameterList([nn.Parameter(torch.randn(hidden_dim)) for i in range(num_layers)])
    self.W_out = nn.Parameter(torch.randn(out_dim, hidden_dim))
    self.b_out = nn.Parameter(torch.randn(out_dim))

  def forward(self, x):
    # x is a tensor of shape (batch_size, 3) containing the space-time coordinates
    # z is a tensor of shape (batch_size, hidden_dim) containing the latent vector
    z = x
    # Apply several layers of sine activation and linear transformation
    for i in range(self.num_layers):
      z = torch.sin(torch.matmul(z, self.W[i]) + self.b[i])
    # Output a tensor of shape (batch_size, out_dim) containing the RGB values
    return torch.matmul(z, self.W_out) + self.b_out

# Define the reconstruction loss
def reconstruction_loss(x,y,t,R,model):
  # x, y, t are tensors of shape (batch_size,) containing the space-time coordinates
  # R is a tensor of shape (batch_size, 3) containing the ground truth RGB values
  # model is an instance of VideoINR class
  # L1 is the L1 distance function
  # Concatenate x, y, t into a tensor of shape (batch_size, 3)
  coords = torch.stack([x,y,t], dim=1)
  # Generate a tensor of shape (batch_size, 3) containing the predicted RGB values
  pred = model(coords)
  # Return a scalar tensor containing the reconstruction loss
  return L1(pred, R)

# Define the perceptual loss
def perceptual_loss(x,y,t,R,model):
  # x, y, t are tensors of shape (batch_size,) containing the space-time coordinates
  # R is a tensor of shape (batch_size, 3) containing the ground truth RGB values
  # model is an instance of VideoINR class
  # VGG is a pre-trained VGG network
  # F is a feature extraction function that returns a tensor of shape (batch_size, 512)
  # L1 is the L1 distance function
  # Concatenate x, y, t into a tensor of shape (batch_size, 3)
  coords = torch.stack([x,y,t], dim=1)
  # Generate a tensor of shape (batch_size, 3) containing the predicted RGB values
  pred = model(coords)
  # Reshape and normalize the tensors to match the input format of VGG network
  pred = pred.view(-1,3,1,1) * 255 - torch.tensor([103.9390 ,116.7790 ,123.6800]).view(1,-1).unsqueeze(-1).unsqueeze(-1)
  R = R.view(-1,3,1,1) * 255 - torch.tensor([103.9390 ,116.7790 ,123.6800]).view(1,-1).unsqueeze(-1).unsqueeze(-1)
  # Extract features from VGG network for both predicted and ground truth pixels
  pred_feat = F(VGG(pred))
  R_feat = F(VGG(R))
  # Return a scalar tensor containing the perceptual loss
  return L1(pred_feat,R_feat)

# Define the temporal consistency loss
def temporal_consistency_loss(x,y,t,model):
  # x, y, t are tensors of shape (batch_size,) containing the space-time coordinates
  # model is an instance of VideoINR class
  # grad_t is a function that computes the gradient along the temporal dimension and returns a tensor of shape (batch_size,)
  # L1 is the L1 distance function
  # Concatenate x, y, t into a tensor of shape (batch_size, 3)
  coords = torch.stack([x,y,t], dim=1)
  # Generate a tensor of shape (batch_size, 3) containing the predicted RGB values
  pred = model(coords)
  # Compute the gradient along the temporal dimension for each RGB channel
  grad_t_r = grad_t(pred[:,0])
  grad_t_g = grad_t(pred[:,1])
  grad_t_b = grad_t(pred[:,2])
  # Return a scalar tensor containing the temporal consistency loss
  return L1(grad_t_r,0) + L1(grad_t_g,0) + L1(grad_t_b,0)

# Define the total loss
def total_loss(x,y,t,R,model,w):
  # x, y, t are tensors of shape (batch_size,) containing the space-time coordinates
  # R is a tensor of shape (batch_size, 3) containing the ground truth RGB values
  # model is an instance of VideoINR class
  # w is a scalar tensor containing the temporal consistency weight
  return reconstruction_loss(x,y,t,R,model) + perceptual_loss(x,y,t,R,model) + w * temporal_consistency_loss(x,y,t,model)

# Train the network on a set of discrete frames
# Initialize the network with random parameters
model = VideoINR(num_layers=8, hidden_dim=256, out_dim=3)
# Initialize the optimizer with Adam algorithm
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Initialize the VGG network with pre-trained weights
VGG = models.vgg16(pretrained=True).features[:16]
# Freeze the VGG parameters
for param in VGG.parameters():
  param.requires_grad = False
# Define the feature extraction function that returns the output of the last convolutional layer
F = lambda x: VGG(x).view(x.size(0),-1)
# Define the L1 distance function
L1 = nn.L1Loss()
# Define the gradient function along the temporal dimension
grad_t = lambda x: x[1:] - x[:-1]
# Define the number of epochs and batches
num_epochs = 100
num_batches = 1000
# Define the batch size and the video size
batch_size = 64
video_width = 64
video_height = 64
video_length = 32
# Loop over epochs
for epoch in range(num_epochs):
  # Loop over batches
  for batch in range(num_batches):
    # Sample a set of coordinates and corresponding RGB values from the video randomly
    x = torch.randint(0, video_width, (batch_size,))
    y = torch.randint(0, video_height, (batch_size,))
    t = torch.randint(0, video_length, (batch_size,))
    R = video[t,x,y,:]
    # Compute the temporal consistency weight based on the distance from input frames
    w = torch.exp(torch.abs(t - video_length/2))
    # Zero the gradients of the optimizer
    optimizer.zero_grad()
    # Compute the total loss by forward propagation
    loss = total_loss(x,y,t,R,model,w)
    # Update the network parameters by back propagation and optimization step
    loss.backward()
    optimizer.step()
  # Print the epoch and loss information
  print(f"Epoch {epoch}, Loss {loss.item()}")

# Generate videos of arbitrary spatial resolution and frame rate
def generate_video(video, scale_s, scale_t):
  # video is a tensor of shape (video_length, video_width, video_height, 3) containing the input low-resolution and low frame rate video
  # scale_s and scale_t are integers containing the spatial and temporal up-sampling scales
  # output is a tensor of shape (video_length * scale_t, video_width * scale_s, video_height * scale_s, 3) containing the output high-resolution and high frame rate video
  output = []
  for t in range(0, video_length * scale_t):
    frame = []
    for x in range(0, video_width * scale_s):
      for y in range(0, video_height * scale_s):
        # Query the network for each coordinate with float values normalized to [0,1]
        pixel = model(torch.tensor([x/video_width/scale_s, y/video_height/scale_s, t/video_length/scale_t]))
        frame.append(pixel)
    # Reshape the frame into a tensor of shape (video_width * scale_s, video_height * scale_s, 3)
    frame = torch.stack(frame).view(video_width * scale_s, video_height * scale_s, -1)
    # Fill in missing pixels by nearest neighbor interpolation 
    frame = interpolate(frame)
    output.append(frame)
  # Reshape the output into a tensor of shape (video_length * scale_t, video_width * scale_s, video_height * scale_s, 3)
  output = torch.stack(output).view(video_length

```