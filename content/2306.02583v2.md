---
title: 2306.02583v2 Stable Diffusion is Unstable
date: 2023-06-03
---

# [Stable Diffusion is Unstable](http://arxiv.org/abs/2306.02583v2)

authors: Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu


## What, Why and How

[1]: https://arxiv.org/abs/2306.02583 "[2306.02583] Stable Diffusion is Untable - arXiv.org"
[2]: https://arxiv.org/pdf/2306.01583v1.pdf "arXiv:2306.01583v1 [cond-mat.stat-mech] 2 Jun 2023"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to generate small perturbations to text prompts that can fool text-to-image models into producing images that do not match the original prompts.
- **Why**: The paper aims to uncover the lack of robustness in text-to-image models and to provide insights into their generation process and failure modes.
- **How**: The paper introduces Auto-attack on Text-to-image Models (ATM), a gradient-based approach that learns a Gumbel Softmax distribution over words to replace or extend the original prompts. ATM can sample multiple attack samples simultaneously and evaluate their success rate based on the presence or absence of the desired subjects in the generated images. The paper also analyzes four attack patterns based on different factors that affect the generation process.


## Main Contributions

[1]: https://arxiv.org/abs/2306.02583 "[2306.02583] Stable Diffusion is Untable - arXiv.org"
[2]: https://arxiv.org/pdf/2306.01583v1.pdf "arXiv:2306.01583v1 [cond-mat.stat-mech] 2 Jun 2023"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

The paper claims to make the following contributions:

- It proposes **Auto-attack on Text-to-image Models (ATM)**, a gradient-based method to generate perturbations to text prompts that can fool text-to-image models.
- It achieves a high success rate of **91.1%** in short-text attacks and **81.2%** in long-text attacks on state-of-the-art text-to-image models.
- It reveals four attack patterns based on different factors that affect the generation process: **the variability in generation speed, the similarity of coarse-grained characteristics, the polysemy of words, and the positioning of words**.
- It provides insights into the lack of robustness and the failure modes of text-to-image models and suggests possible directions for improvement.

## Method Summary

[1]: https://arxiv.org/abs/2306.02583 "[2306.02583] Stable Diffusion is Untable - arXiv.org"
[2]: https://arxiv.org/pdf/2306.01583v1.pdf "arXiv:2306.01583v1 [cond-mat.stat-mech] 2 Jun 2023"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces **Auto-attack on Text-to-image Models (ATM)**, a gradient-based method to generate perturbations to text prompts that can fool text-to-image models.
- ATM consists of two steps: **perturbation generation** and **perturbation evaluation**.
- In the perturbation generation step, ATM learns a Gumbel Softmax distribution over words to replace or extend the original prompts. The Gumbel Softmax distribution allows for a continuous relaxation of the discrete word selection process and enables gradient-based optimization. ATM uses a pre-trained language model to encode the text prompts and a pre-trained image classifier to guide the perturbation generation. The objective function of ATM is to maximize the cross-entropy loss between the predicted class of the generated image and the target class, while minimizing the semantic similarity between the original and perturbed prompts.
- In the perturbation evaluation step, ATM samples multiple attack samples from the learned Gumbel Softmax distribution and evaluates their success rate based on the presence or absence of the desired subjects in the generated images. ATM uses a pre-trained object detector to detect and count the objects in the images and compares them with the expected number of objects based on the original prompts. ATM defines four types of attacks: **complete disappearance**, **partial disappearance**, **blending**, and **replacement**. ATM reports the success rate of each type of attack as well as the overall success rate.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the text-to-image model, the language model, the image classifier, and the object detector
text_to_image_model = load_pretrained_model()
language_model = load_pretrained_model()
image_classifier = load_pretrained_model()
object_detector = load_pretrained_model()

# Define the hyperparameters
num_samples = 100 # number of attack samples to generate
num_classes = 1000 # number of image classes
num_objects = 80 # number of object categories
tau = 0.1 # temperature parameter for Gumbel Softmax
alpha = 0.5 # trade-off parameter for semantic similarity
beta = 0.5 # trade-off parameter for word length

# Define the original text prompt and the target class
original_prompt = "a blue bird sitting on a branch"
target_class = "cat"

# Encode the original prompt and the target class using the language model and the image classifier
original_prompt_embedding = language_model.encode(original_prompt)
target_class_embedding = image_classifier.encode(target_class)

# Initialize the Gumbel Softmax distribution over words
gumbel_softmax = GumbelSoftmax(num_words, tau)

# Define the perturbation generation step
def perturb(prompt_embedding, target_class_embedding):
  # Sample a word from the Gumbel Softmax distribution
  word = gumbel_softmax.sample()
  # Replace or extend the prompt with the sampled word
  perturbed_prompt = replace_or_extend(prompt_embedding, word)
  # Generate an image from the perturbed prompt using the text-to-image model
  perturbed_image = text_to_image_model.generate(perturbed_prompt)
  # Predict the class of the perturbed image using the image classifier
  predicted_class = image_classifier.predict(perturbed_image)
  # Compute the cross-entropy loss between the predicted class and the target class
  cross_entropy_loss = cross_entropy(predicted_class, target_class_embedding)
  # Compute the semantic similarity between the original and perturbed prompts using the language model
  semantic_similarity = language_model.similarity(original_prompt_embedding, perturbed_prompt_embedding)
  # Compute the word length of the perturbed prompt
  word_length = len(perturbed_prompt)
  # Compute the total loss as a weighted sum of cross-entropy loss, semantic similarity, and word length
  total_loss = cross_entropy_loss - alpha * semantic_similarity - beta * word_length
  # Return the perturbed prompt, the perturbed image, and the total loss
  return perturbed_prompt, perturbed_image, total_loss

# Define the perturbation evaluation step
def evaluate(perturbed_prompt, perturbed_image):
  # Detect and count the objects in the perturbed image using the object detector
  detected_objects = object_detector.detect(perturbed_image)
  object_count = len(detected_objects)
  # Extract the expected number of objects from the original prompt
  expected_objects = extract_objects(original_prompt)
  expected_count = len(expected_objects)
  # Compare the object count with the expected count and classify the attack type
  if object_count == 0:
    attack_type = "complete disappearance"
  elif object_count < expected_count:
    attack_type = "partial disappearance"
  elif object_count == expected_count:
    attack_type = "blending"
  else:
    attack_type = "replacement"
  # Return the attack type
  return attack_type

# Initialize an empty list to store the attack samples and their types
attack_samples = []
attack_types = []

# Repeat for num_samples times
for i in range(num_samples):
  # Generate a perturbation using the perturb function
  perturbed_prompt, perturbed_image, total_loss = perturb(original_prompt_embedding, target_class_embedding)
  # Evaluate the perturbation using the evaluate function
  attack_type = evaluate(perturbed_prompt, perturbed_image)
  # Append the perturbation and its type to the list
  attack_samples.append((perturbed_prompt, perturbed_image))
  attack_types.append(attack_type)

# Compute and report the success rate of each type of attack and the overall success rate
success_rate_by_type = compute_success_rate_by_type(attack_types)
overall_success_rate = compute_overall_success_rate(attack_types)
report_success_rate(success_rate_by_type, overall_success_rate)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch # for tensor operations
import transformers # for language model
import clip # for image classifier
import dalle_pytorch # for text-to-image model
import detectron2 # for object detector
import nltk # for natural language processing

# Define the text-to-image model, the language model, the image classifier, and the object detector
text_to_image_model = dalle_pytorch.DALLE.load_from_checkpoint("dalle.pt") # load a pre-trained DALL-E model
language_model = transformers.AutoModel.from_pretrained("bert-base-uncased") # load a pre-trained BERT model
image_classifier = clip.load("ViT-B/32", jit=False)[0].eval() # load a pre-trained CLIP model
object_detector = detectron2.model_zoo.get("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml") # load a pre-trained Faster R-CNN model

# Define the hyperparameters
num_samples = 100 # number of attack samples to generate
num_classes = 1000 # number of image classes
num_objects = 80 # number of object categories
tau = 0.1 # temperature parameter for Gumbel Softmax
alpha = 0.5 # trade-off parameter for semantic similarity
beta = 0.5 # trade-off parameter for word length
lr = 0.01 # learning rate for gradient descent
max_iter = 100 # maximum number of iterations for gradient descent

# Define the original text prompt and the target class
original_prompt = "a blue bird sitting on a branch"
target_class = "cat"

# Encode the original prompt and the target class using the language model and the image classifier
original_prompt_embedding = language_model.encode(original_prompt) # a tensor of shape (1, hidden_size)
target_class_embedding = image_classifier.encode_text(target_class) # a tensor of shape (1, embed_dim)

# Initialize the Gumbel Softmax distribution over words
gumbel_softmax = torch.nn.GumbelSoftmax(tau) # a module that applies the Gumbel Softmax function
num_words = len(language_model.vocab) # the size of the vocabulary
word_weights = torch.randn(1, num_words, requires_grad=True) # a tensor of shape (1, num_words) that stores the weights for each word

# Define the perturbation generation step
def perturb(prompt_embedding, target_class_embedding):
  # Initialize an optimizer for gradient descent
  optimizer = torch.optim.Adam([word_weights], lr=lr)
  # Repeat for max_iter times
  for i in range(max_iter):
    # Sample a word from the Gumbel Softmax distribution
    word_probs = gumbel_softmax(word_weights) # a tensor of shape (1, num_words) that stores the probabilities for each word
    word_index = torch.argmax(word_probs) # an integer that stores the index of the sampled word
    word = language_model.vocab[word_index] # a string that stores the sampled word
    # Replace or extend the prompt with the sampled word
    perturbed_prompt = replace_or_extend(prompt_embedding, word) # a tensor of shape (1, hidden_size) that stores the perturbed prompt embedding
    perturbed_prompt_text = language_model.decode(perturbed_prompt) # a string that stores the perturbed prompt text
    # Generate an image from the perturbed prompt using the text-to-image model
    perturbed_image = text_to_image_model.generate_images(perturbed_prompt_text) # a tensor of shape (1, 3, height, width) that stores the perturbed image
    # Predict the class of the perturbed image using the image classifier
    perturbed_image_embedding = image_classifier.encode_image(perturbed_image) # a tensor of shape (1, embed_dim) that stores the perturbed image embedding
    predicted_class_logits = image_classifier(perturbed_image_embedding, target_class_embedding) # a tensor of shape (1,) that stores the logits for the target class
    predicted_class_probs = torch.softmax(predicted_class_logits, dim=0) # a tensor of shape (1,) that stores the probabilities for the target class
    predicted_class = torch.argmax(predicted_class_probs) # an integer that stores the predicted class index
    # Compute the cross-entropy loss between the predicted class and the target class
    cross_entropy_loss = torch.nn.CrossEntropyLoss()(predicted_class_logits, target_class_embedding)
    # Compute the semantic similarity between the original and perturbed prompts using the language model
    semantic_similarity = torch.cosine_similarity(original_prompt_embedding, perturbed_prompt_embedding, dim=0) # a scalar that stores the cosine similarity
    # Compute the word length of the perturbed prompt
    word_length = len(perturbed_prompt_text) # an integer that stores the word length
    # Compute the total loss as a weighted sum of cross-entropy loss, semantic similarity, and word length
    total_loss = cross_entropy_loss - alpha * semantic_similarity - beta * word_length # a scalar that stores the total loss
    # Update the word weights using gradient descent
    optimizer.zero_grad() # clear the previous gradients
    total_loss.backward() # compute the gradients
    optimizer.step() # update the weights
  # Return the perturbed prompt, the perturbed image, and the total loss
  return perturbed_prompt_text, perturbed_image, total_loss

# Define the perturbation evaluation step
def evaluate(perturbed_prompt, perturbed_image):
  # Detect and count the objects in the perturbed image using the object detector
  detected_objects = object_detector.detect(perturbed_image) # a list of dictionaries that store the detected objects and their attributes
  object_count = len(detected_objects) # an integer that stores the number of detected objects
  # Extract the expected number of objects from the original prompt
  expected_objects = nltk.tokenize.word_tokenize(original_prompt) # a list of strings that store the words in the original prompt
  expected_objects = [word for word in expected_objects if word in object_detector.classes] # a list of strings that store the words that match the object categories
  expected_count = len(expected_objects) # an integer that stores the number of expected objects
  # Compare the object count with the expected count and classify the attack type
  if object_count == 0:
    attack_type = "complete disappearance"
  elif object_count < expected_count:
    attack_type = "partial disappearance"
  elif object_count == expected_count:
    attack_type = "blending"
  else:
    attack_type = "replacement"
  # Return the attack type
  return attack_type

# Initialize an empty list to store the attack samples and their types
attack_samples = []
attack_types = []

# Repeat for num_samples times
for i in range(num_samples):
  # Generate a perturbation using the perturb function
  perturbed_prompt, perturbed_image, total_loss = perturb(original_prompt_embedding, target_class_embedding)
  # Evaluate the perturbation using the evaluate function
  attack_type = evaluate(perturbed_prompt, perturbed_image)
  # Append the perturbation and its type to the list
  attack_samples.append((perturbed_prompt, perturbed_image))
  attack_types.append(attack_type)

# Compute and report the success rate of each type of attack and the overall success rate
success_rate_by_type = compute_success_rate_by_type(attack_types)
overall_success_rate = compute_overall_success_rate(attack_types)
report_success_rate(success_rate_by_type, overall_success_rate)
```