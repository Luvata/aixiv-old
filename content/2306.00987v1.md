---
title: 2306.00987v1 StyleGAN knows Normal, Depth, Albedo, and More
date: 2023-06-01
---

# [StyleGAN knows Normal, Depth, Albedo, and More](http://arxiv.org/abs/2306.00987v1)

authors: Anand Bhattad, Daniel McKee, Derek Hoiem, D. A. Forsyth


## What, Why and How

[1]: https://arxiv.org/abs/2306.00987 "[2306.00987] StyleGAN knows Normal, Depth, Albedo, and More - arXiv.org"
[2]: https://arxiv.org/abs/2306.00763 "[2306.00763] Learning Disentangled Prompts for Compositional Image ..."
[3]: http://export.arxiv.org/abs/2306.00987 "[2306.00987] StyleGAN knows Normal, Depth, Albedo, and More"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- What: The paper demonstrates that StyleGAN, a pretrained image generative model, can produce intrinsic images, such as depth, normal, albedo or shading, by adding a fixed offset to the latent vector.
- Why: The paper aims to show that StyleGAN has learned and represented intrinsic images, which are image-like maps of scene properties, and that this property is not an accident of the training regime or a generic image regression ability.
- How: The paper proposes a simple procedure to find the fixed offset for each type of intrinsic image by optimizing a loss function that measures the similarity between the generated image and the intrinsic image. The paper also compares the quality and robustness of the intrinsic images obtained from StyleGAN with those obtained by state-of-the-art image regression techniques.

## Main Contributions

The paper claims the following contributions:

- It shows that StyleGAN can easily be induced to produce intrinsic images by adding a fixed offset to the latent vector, and that this offset is independent of the latent vector and the image content.
- It shows that there are image transformations that StyleGAN will not produce in this fashion, such as segmentation or edge detection, suggesting that StyleGAN is not a generic image regression engine.
- It shows that the intrinsic images obtained from StyleGAN compare well both qualitatively and quantitatively with those obtained by using state-of-the-art image regression techniques, and that StyleGAN's intrinsic images are robust to relighting effects, unlike state-of-the-art methods.

## Method Summary

The method section of the paper consists of three parts:

- The first part describes the generative model and the latent space used in the paper, which is StyleGAN and its intermediate latent space W. It also introduces the notation and definitions used in the paper, such as the latent vector w, the offset vector d_c, and the intrinsic image function I_c.
- The second part explains how to find the offset vector d_c for a given type of intrinsic image c by optimizing a loss function that measures the similarity between the generated image G(w+d_c) and the intrinsic image I_c(G(w)). It also discusses some implementation details and challenges, such as choosing the loss function, initializing the offset vector, and dealing with multiple channels or scales.
- The third part presents some experiments and results to validate the proposed method and to compare it with state-of-the-art image regression techniques. It also shows some examples of using the offset vector to manipulate the generated images or to transfer the style of one domain to another.

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Load a pretrained StyleGAN model and its latent space W
stylegan = load_model("stylegan")
W = stylegan.W

# Define a function to generate an image from a latent vector
def generate_image(w):
  return stylegan.synthesize(w)

# Define a function to compute the intrinsic image of a given type c from an image
def intrinsic_image(image, c):
  # Use a state-of-the-art image regression technique to compute the intrinsic image
  # For example, for depth, use MiDaS (https://github.com/intel-isl/MiDaS)
  # For normal, use DORN (https://github.com/hufu6371/DORN)
  # For albedo or shading, use IntrinsicNet (https://github.com/junyanz/IntrinsicNet)
  # For segmentation, use DeepLabV3+ (https://github.com/tensorflow/models/tree/master/research/deeplab)
  # For edge detection, use HED (https://github.com/s9xie/hed)
  return regression_model(image, c)

# Define a function to measure the similarity between two images
def similarity(image1, image2):
  # Use a suitable loss function depending on the type of intrinsic image
  # For example, for depth or normal, use L1 loss
  # For albedo or shading, use perceptual loss (https://arxiv.org/abs/1603.08155)
  # For segmentation or edge detection, use cross entropy loss
  return loss_function(image1, image2)

# Define a function to find the offset vector for a given type of intrinsic image c
def find_offset(c):
  # Initialize the offset vector d_c randomly or with zeros
  d_c = random_vector(W.shape) or zero_vector(W.shape)
  # Set the learning rate and the number of iterations
  lr = 0.01
  num_iter = 1000
  # Loop over the iterations
  for i in range(num_iter):
    # Sample a random latent vector w from W
    w = random_vector(W.shape)
    # Generate an image G(w) from w
    G_w = generate_image(w)
    # Compute the intrinsic image I_c(G(w)) from G(w)
    I_c_G_w = intrinsic_image(G_w, c)
    # Generate an image G(w+d_c) from w+d_c
    G_w_d_c = generate_image(w+d_c)
    # Compute the similarity loss between G(w+d_c) and I_c(G(w))
    loss = similarity(G_w_d_c, I_c_G_w)
    # Update d_c by gradient descent
    d_c = d_c - lr * gradient(loss, d_c)
  # Return the offset vector d_c
  return d_c

# Define a function to synthesize an image of any class in the style of target domain c
def synthesize_image(class, c):
  # Find the offset vector d_c for the target domain c
  d_c = find_offset(c)
  # Sample a random latent vector w from W that corresponds to the class label
  w = random_vector(W.shape, class)
  # Generate an image G(w+d_c) from w+d_c
  G_w_d_c = generate_image(w+d_c)
  # Return the synthesized image G(w+d_c)
  return G_w_d_c

```