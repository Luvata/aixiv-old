---
title: 1911.02001v1 Dancing to Music
date: 2019-11-03
---

# [Dancing to Music](http://arxiv.org/abs/1911.02001v1)

authors: Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, Jan Kautz


## What, Why and How

[1]: https://arxiv.org/pdf/1911.02001 "Dancing to Music - arXiv.org"
[2]: https://ideas.repec.org/p/arx/papers/1911.09511.html "A Practical Introduction to Regression Discontinuity Designs"
[3]: https://arxiv.org/pdf/1911.02001v1 "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a synthesis-by-analysis learning framework to generate dance from music. The framework consists of two phases: analysis and synthesis. In the analysis phase, the model decomposes a dance into a series of basic dance units, which are the building blocks of the dance. In the synthesis phase, the model composes a dance by organizing multiple basic dance units according to the input music.
- **Why**: The paper aims to address the challenging but interesting generative task of music-to-dance creation, which has potential applications in arts and sports. Existing methods are limited by their retrieval-based approach, which lacks creativity. The paper formulates the task from a generative perspective, which requires modeling the correlation between music and dance, the multimodality of dance, and the spatio-temporal complexity of body movements.
- **How**: The paper leverages prior knowledge and a large amount of paired music and dance data to design and train the framework. The framework consists of two modules: a Dance Unit Analysis Network (DUAN) and a Music-conditioned Dance Synthesis Network (MDSN). The DUAN learns to segment and encode a dance into a sequence of basic dance units using an attention-based recurrent neural network. The MDSN learns to generate a sequence of basic dance units conditioned on the input music using a conditional variational autoencoder with an autoregressive decoder. The generated dance units are then decoded into realistic and diverse body movements using an adversarial network.

## Main Contributions

According to the paper, the main contributions are:

- The paper proposes a novel synthesis-by-analysis learning framework for music-to-dance generation, which decomposes the task into two phases: analysis and synthesis.
- The paper introduces the concept of basic dance units, which are the atomic elements of a dance, and designs a network to learn to segment and encode a dance into a sequence of basic dance units.
- The paper develops a network to learn to generate a sequence of basic dance units conditioned on the input music, which captures the multimodality and diversity of dance movements.
- The paper demonstrates that the proposed framework can synthesize realistic, diverse, style-consistent, and beat-matching dances from music. The paper also provides extensive qualitative and quantitative evaluations and comparisons with existing methods.

## Method Summary

The method section of the paper describes the proposed synthesis-by-analysis learning framework for music-to-dance generation. The framework consists of two modules: a Dance Unit Analysis Network (DUAN) and a Music-conditioned Dance Synthesis Network (MDSN). The DUAN takes a dance video as input and outputs a sequence of basic dance units, which are the atomic elements of a dance. The MDSN takes a music clip as input and outputs a sequence of basic dance units that are aligned with the music. The generated dance units are then decoded into realistic body movements using an adversarial network.

The DUAN is composed of three components: a pose estimator, a dance unit segmentor, and a dance unit encoder. The pose estimator extracts the 2D human poses from each frame of the input dance video using OpenPose [4]. The dance unit segmentor learns to divide the pose sequence into segments that correspond to basic dance units using an attention-based recurrent neural network. The dance unit encoder learns to map each segment of poses into a latent vector that represents the basic dance unit using another recurrent neural network.

The MDSN is composed of two components: a music encoder and a dance unit generator. The music encoder learns to extract the musical features from the input music clip using a convolutional neural network. The dance unit generator learns to generate a sequence of basic dance units conditioned on the musical features using a conditional variational autoencoder with an autoregressive decoder. The decoder uses an LSTM [12] to model the temporal dependency among the basic dance units.

The generated basic dance units are then decoded into realistic body movements using an adversarial network, which consists of a generator and a discriminator. The generator learns to map each basic dance unit into a sequence of 3D human poses using another LSTM. The discriminator learns to distinguish between the real and generated 3D poses using a convolutional neural network. The 3D poses are then projected onto 2D images using a camera model and rendered into a dance video using SMPL [18].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a music clip M
# Output: a dance video D

# Define the network modules
DUAN = Dance Unit Analysis Network()
MDSN = Music-conditioned Dance Synthesis Network()
GAN = Adversarial Network()

# Train the network modules using paired music and dance data
for each (M, D) in data:
  # Extract 2D poses from D using OpenPose
  P = OpenPose(D)
  # Segment and encode P into basic dance units using DUAN
  U = DUAN(P)
  # Extract musical features from M using MDSN
  F = MDSN.encode(M)
  # Generate basic dance units from F using MDSN
  U_hat = MDSN.generate(F)
  # Decode U_hat into 3D poses using GAN
  P_hat = GAN.generate(U_hat)
  # Update the network parameters using loss functions
  update(DUAN, MDSN, GAN, P, U, F, U_hat, P_hat)

# Generate a dance video from a new music clip
# Extract musical features from M using MDSN
F = MDSN.encode(M)
# Generate basic dance units from F using MDSN
U_hat = MDSN.generate(F)
# Decode U_hat into 3D poses using GAN
P_hat = GAN.generate(U_hat)
# Project and render P_hat into a dance video using SMPL and camera model
D = SMPL(P_hat) * camera_model
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a music clip M
# Output: a dance video D

# Define the network modules
DUAN = Dance Unit Analysis Network()
MDSN = Music-conditioned Dance Synthesis Network()
GAN = Adversarial Network()

# Define the hyperparameters
batch_size = 16 # the number of music and dance pairs in each batch
seq_len = 64 # the length of the pose sequence in each segment
latent_dim = 128 # the dimension of the latent vector for basic dance units
music_dim = 128 # the dimension of the musical features
pose_dim = 50 # the dimension of the 2D or 3D human poses
hidden_dim = 256 # the dimension of the hidden state for recurrent networks
num_units = 10 # the number of basic dance units
num_layers = 2 # the number of layers for recurrent networks
kernel_size = 3 # the kernel size for convolutional networks
stride = 2 # the stride for convolutional networks
learning_rate = 0.001 # the learning rate for optimization
beta1 = 0.9 # the beta1 parameter for Adam optimizer
beta2 = 0.999 # the beta2 parameter for Adam optimizer

# Define the loss functions
L1_loss = mean absolute error between real and generated poses
KL_loss = Kullback-Leibler divergence between prior and posterior distributions of basic dance units
adv_loss = binary cross entropy between real and fake labels for discriminator and generator
rec_loss = L1_loss + KL_loss # reconstruction loss for MDSN
gen_loss = L1_loss + adv_loss # generation loss for GAN

# Define the optimizers
DUAN_optimizer = Adam(DUAN.parameters(), learning_rate, beta1, beta2)
MDSN_optimizer = Adam(MDSN.parameters(), learning_rate, beta1, beta2)
GAN_optimizer = Adam(GAN.parameters(), learning_rate, beta1, beta2)

# Train the network modules using paired music and dance data
for epoch in range(num_epochs):
  for batch in data.batch(batch_size):
    # Extract 2D poses from D using OpenPose
    P = OpenPose(D)
    # Segment and encode P into basic dance units using DUAN
    U, U_mu, U_logvar = DUAN(P)
    # Extract musical features from M using MDSN
    F = MDSN.encode(M)
    # Generate basic dance units from F using MDSN
    U_hat, U_hat_mu, U_hat_logvar = MDSN.generate(F)
    # Decode U_hat into 3D poses using GAN
    P_hat = GAN.generate(U_hat)
    # Compute the discriminator outputs for real and fake poses
    D_real = GAN.discriminate(P)
    D_fake = GAN.discriminate(P_hat)
    # Compute the loss values for each network module
    DUAN_loss = L1_loss(U, U_hat) + KL_loss(U_mu, U_logvar)
    MDSN_loss = rec_loss(P, P_hat) + KL_loss(U_hat_mu, U_hat_logvar)
    GAN_gen_loss = gen_loss(P, P_hat) + adv_loss(D_fake, 1)
    GAN_dis_loss = adv_loss(D_real, 1) + adv_loss(D_fake, 0)
    # Update the network parameters using loss values and optimizers
    DUAN_optimizer.zero_grad()
    DUAN_loss.backward()
    DUAN_optimizer.step()
    MDSN_optimizer.zero_grad()
    MDSN_loss.backward()
    MDSN_optimizer.step()
    GAN_optimizer.zero_grad()
    GAN_gen_loss.backward()
    GAN_dis_loss.backward()
    GAN_optimizer.step()

# Generate a dance video from a new music clip
# Extract musical features from M using MDSN
F = MDSN.encode(M)
# Generate basic dance units from F using MDSN
U_hat, U_hat_mu, U_hat_logvar = MDSN.generate(F)
# Decode U_hat into 3D poses using GAN
P_hat = GAN.generate(U_hat)
# Project and render P_hat into a dance video using SMPL and camera model
D = SMPL(P_hat) * camera_model

```