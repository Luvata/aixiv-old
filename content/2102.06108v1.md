---
title: 2102.06108v1 SWAGAN  A Style-based Wavelet-driven Generative Model
date: 2021-02-07
---

# [SWAGAN: A Style-based Wavelet-driven Generative Model](http://arxiv.org/abs/2102.06108v1)

authors: Rinon Gal, Dana Cohen, Amit Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2102.06108v1 "SWAGAN: A Style-based Wavelet-driven Generative Model"
[2]: https://arxiv.org/pdf/2102.06108v1.pdf "SWAGAN: A Style-based WAvelet-driven Generative Model - arXiv.org"
[3]: http://export.arxiv.org/abs/2101.06108v1 "[2101.06108v1] Double Slit Experiment in the Non-Commutative Plane and ..."

Here is a summary of the paper:

- **What**: The paper presents a novel generative model called SWAGAN that uses wavelets to generate high-quality images with realistic high-frequency content.
- **Why**: The paper aims to address the issue of spectral bias and degradation in quality for high-frequency content in existing generative models, such as StyleGAN2.
- **How**: The paper incorporates wavelets throughout the generator and discriminator architectures of StyleGAN2, and implements progressive generation in the frequency domain. The paper also shows that SWAGAN can perform various editing tasks on the generated images, and that its latent space retains the desirable properties of StyleGAN2.

## Main Contributions

[1]: https://arxiv.org/abs/2102.06108v1 "SWAGAN: A Style-based Wavelet-driven Generative Model"
[2]: https://arxiv.org/pdf/2102.06108v1.pdf "SWAGAN: A Style-based WAvelet-driven Generative Model - arXiv.org"
[3]: http://export.arxiv.org/abs/2101.06108v1 "[2101.06108v1] Double Slit Experiment in the Non-Commutative Plane and ..."

The paper claims the following contributions:

- A novel generative model called SWAGAN that uses wavelets to generate high-quality images with realistic high-frequency content.
- A frequency-aware latent representation that allows for progressive generation in the frequency domain and preserves the editing capabilities of StyleGAN2.
- A demonstration of the advantages of SWAGAN over StyleGAN2 on various datasets and tasks, such as face synthesis, texture generation, and image manipulation.

## Method Summary

[1]: https://arxiv.org/abs/2102.06108v1 "SWAGAN: A Style-based Wavelet-driven Generative Model"
[2]: https://arxiv.org/pdf/2102.06108v1.pdf "SWAGAN: A Style-based WAvelet-driven Generative Model - arXiv.org"
[3]: http://export.arxiv.org/abs/2101.06108v1 "[2101.06108v1] Double Slit Experiment in the Non-Commutative Plane and ..."

Here is a summary of the method section of the paper:

- The paper adopts the StyleGAN2 framework, which consists of a generator and a discriminator that operate on image and feature spaces, respectively.
- The paper modifies the generator and the discriminator to work on the wavelet domain, by applying wavelet transforms to the input images and features, and using inverse wavelet transforms to reconstruct the output images and features.
- The paper uses a frequency-aware latent representation that consists of four latent vectors, each corresponding to a different frequency band of the wavelet transform. The paper also uses adaptive instance normalization (AdaIN) to modulate the latent vectors with style vectors derived from a mapping network.
- The paper implements progressive generation in the frequency domain, by gradually increasing the resolution and complexity of the generator and the discriminator. The paper also uses skip connections and residual blocks to improve the information flow across different scales.
- The paper uses a combination of adversarial loss, perceptual loss, and R1 regularization to train the generator and the discriminator. The paper also uses a wavelet-based perceptual loss that measures the similarity between wavelet coefficients of real and generated images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator network
def generator(z):
  # z is a list of four latent vectors, each corresponding to a frequency band
  # Initialize the output image as an empty tensor
  image = torch.zeros(1, 3, 4, 4)
  # Loop over the frequency bands from low to high
  for i in range(4):
    # Apply wavelet transform to the output image
    image = wavelet_transform(image)
    # Get the style vector from the mapping network
    style = mapping_network(z[i])
    # Modulate the latent vector with the style vector using AdaIN
    latent = adain(z[i], style)
    # Generate the image features for the current frequency band
    features = generate_features(latent, image.shape)
    # Add the features to the output image
    image = image + features
  # Apply inverse wavelet transform to the output image
  image = inverse_wavelet_transform(image)
  # Return the output image
  return image

# Define the discriminator network
def discriminator(x):
  # x is an input image
  # Apply wavelet transform to the input image
  x = wavelet_transform(x)
  # Initialize the output feature as an empty tensor
  feature = torch.zeros(1, 512, 4, 4)
  # Loop over the frequency bands from high to low
  for i in range(3, -1, -1):
    # Extract the image features for the current frequency band
    x_feature = extract_features(x, feature.shape)
    # Add the image features to the output feature
    feature = feature + x_feature
    # Apply inverse wavelet transform to the output feature
    feature = inverse_wavelet_transform(feature)
  # Classify the output feature as real or fake
  score = classify(feature)
  # Return the score
  return score

# Define the training loop
def train(generator, discriminator):
  # Loop over the training data
  for real_image in data_loader:
    # Sample four latent vectors for each frequency band
    z = [torch.randn(1, 512) for _ in range(4)]
    # Generate a fake image using the generator
    fake_image = generator(z)
    # Compute the adversarial loss for the generator and the discriminator
    g_loss = adversarial_loss(discriminator(fake_image), True)
    d_loss = adversarial_loss(discriminator(real_image), True) + adversarial_loss(discriminator(fake_image), False)
    # Compute the perceptual loss for the generator using a wavelet-based metric
    g_loss += perceptual_loss(wavelet_transform(real_image), wavelet_transform(fake_image))
    # Compute the R1 regularization for the discriminator
    d_loss += r1_regularization(discriminator(real_image))
    # Update the generator and the discriminator parameters using gradient descent
    update_parameters(generator, g_loss)
    update_parameters(discriminator, d_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import pywt

# Define the wavelet transform function
def wavelet_transform(x):
  # x is a tensor of shape (batch_size, channels, height, width)
  # Use the Haar wavelet as the basis function
  wavelet = pywt.Wavelet('haar')
  # Apply the 2D discrete wavelet transform to each channel of x
  # The output is a list of four tensors, each of shape (batch_size, channels, height/2, width/2)
  # The first tensor is the low-frequency approximation coefficients (LL)
  # The other three tensors are the high-frequency detail coefficients (LH, HL, HH)
  coeffs = pywt.dwt2(x, wavelet, mode='zero')
  # Concatenate the four tensors along the channel dimension
  # The output is a tensor of shape (batch_size, channels*4, height/2, width/2)
  output = torch.cat(coeffs, dim=1)
  # Return the output tensor
  return output

# Define the inverse wavelet transform function
def inverse_wavelet_transform(x):
  # x is a tensor of shape (batch_size, channels*4, height, width)
  # Use the Haar wavelet as the basis function
  wavelet = pywt.Wavelet('haar')
  # Split the input tensor into four tensors along the channel dimension
  # Each tensor is of shape (batch_size, channels, height, width)
  # The first tensor is the low-frequency approximation coefficients (LL)
  # The other three tensors are the high-frequency detail coefficients (LH, HL, HH)
  coeffs = torch.split(x, x.shape[1]//4, dim=1)
  # Apply the 2D inverse discrete wavelet transform to each channel of x
  # The output is a tensor of shape (batch_size, channels, height*2, width*2)
  output = pywt.idwt2(coeffs, wavelet, mode='zero')
  # Return the output tensor
  return output

# Define the mapping network
class MappingNetwork(nn.Module):
  def __init__(self):
    super().__init__()
    # Define eight fully connected layers with LeakyReLU activation
    self.fc_layers = nn.Sequential(
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
      nn.Linear(512, 512),
      nn.LeakyReLU(0.2),
    )
  
  def forward(self, z):
    # z is a latent vector of shape (batch_size, 512)
    # Apply the fully connected layers to z
    w = self.fc_layers(z)
    # w is a style vector of shape (batch_size, 512)
    # Return w
    return w

# Define the AdaIN function
def adain(x, y):
  # x is a latent vector of shape (batch_size, channels)
  # y is a style vector of shape (batch_size, channels)
  # Normalize x to have zero mean and unit variance along the channel dimension
  x_mean = x.mean(dim=1, keepdim=True)
  x_std = x.std(dim=1, keepdim=True) + 1e-8
  x_norm = (x - x_mean) / x_std
  # Compute the mean and standard deviation of y along the channel dimension
  y_mean = y.mean(dim=1, keepdim=True)
  y_std = y.std(dim=1, keepdim=True) + 1e-8
  # Modulate x_norm with y_mean and y_std
  output = y_std * x_norm + y_mean
  # output is a modulated latent vector of shape (batch_size, channels)
  # Return output
  return output

# Define the generator network
class Generator(nn.Module):
  
```