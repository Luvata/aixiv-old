---
title: 2306.00440v1 Edge-guided Representation Learning for Underwater Object Detection
date: 2023-06-01
---

# [Edge-guided Representation Learning for Underwater Object Detection](http://arxiv.org/abs/2306.00440v1)

authors: Linhui Dai, Hong Liu, Pinhao Song, Hao Tang, Runwei Ding, Shengquan Li


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00440v1.pdf "Linhui Dai, Hong Liu, Pinhao Song, Hao Tang, Runwei Ding ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.00440 "Edge-guided Representation Learning for Underwater Object Detection"
[3]: http://export.arxiv.org/abs/2306.00440 "[2306.00440] Edge-guided Representation Learning for Underwater Object ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes an Edge-guided Representation Learning Network (ERL-Net) for underwater object detection (UOD).
- **Why**: UOD is a vital computer vision task for marine applications, but it faces challenges such as low-contrast, small objects, and mimicry of aquatic organisms. The paper aims to address these challenges by focusing on the edges of underwater objects, which are highly unique and discriminative.
- **How**: The paper introduces three main components in ERL-Net: an edge-guided attention module, a feature aggregation module, and a wide and asymmetric receptive field block. The edge-guided attention module models the explicit boundary information of underwater objects, which generates more discriminative features. The feature aggregation module aggregates the multi-scale discriminative features by regrouping them into three levels, effectively aggregating global and local information for locating and recognizing underwater objects. The wide and asymmetric receptive field block enables features to have a wider receptive field, allowing the model to focus on more small object information. The paper evaluates ERL-Net on three challenging underwater datasets and shows that it achieves superior performance on the UOD task.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel edge-guided representation learning network (ERL-Net) for underwater object detection (UOD), which leverages the edge cues of underwater objects to enhance the discriminative ability of the model.
- It introduces an edge-guided attention module, a feature aggregation module, and a wide and asymmetric receptive field block, which are designed to address the specific challenges of UOD, such as low-contrast, small objects, and mimicry of aquatic organisms.
- It conducts comprehensive experiments on three challenging underwater datasets and demonstrates that ERL-Net outperforms the state-of-the-art methods on the UOD task.

## Method Summary

The method section of the paper describes the proposed edge-guided representation learning network (ERL-Net) for underwater object detection (UOD). The paper adopts a one-stage detector as the backbone of ERL-Net and introduces three main components: an edge-guided attention module (EGAM), a feature aggregation module (FAM), and a wide and asymmetric receptive field block (WARB). The EGAM is designed to model the explicit boundary information of underwater objects, which can generate more discriminative features for UOD. The EGAM consists of two sub-modules: an edge extraction sub-module (EESM) and an edge-guided attention sub-module (EASM). The EESM uses a Sobel operator to extract the edge maps of the input images, and the EASM uses a channel-wise attention mechanism to fuse the edge maps with the backbone features. The FAM is designed to aggregate the multi-scale discriminative features from different levels of the backbone, which can effectively capture global and local information for locating and recognizing underwater objects. The FAM consists of three sub-modules: a feature regrouping sub-module (FRSM), a feature fusion sub-module (FFSM), and a feature refinement sub-module (FRFM). The FRSM regroups the backbone features into three levels according to their semantic information and spatial resolution. The FFSM fuses the features from different levels using element-wise addition and convolution operations. The FRFM refines the fused features using depth-wise separable convolutions and residual connections. The WARB is designed to enable the features to have a wider receptive field, which can help the model to focus on more small object information. The WARB consists of two sub-modules: a wide receptive field sub-module (WRFSM) and an asymmetric receptive field sub-module (ARFSM). The WRFSM uses dilated convolutions to increase the receptive field of the features without reducing their resolution. The ARFSM uses asymmetric convolutions to capture more horizontal and vertical information of the features. The paper shows the overall architecture of ERL-Net in Figure 2 and provides more details of each component in Figure 3.

## Pseudo Code

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

# Define the edge extraction sub-module (EESM)
class EESM(nn.Module):
    def __init__(self):
        super(EESM, self).__init__()
        # Define the Sobel operator for horizontal and vertical edges
        self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)
        self.sobel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=torch.float32).view(1, 1, 3, 3)

    def forward(self, x):
        # x is the input image of shape (B, C, H, W)
        # Convert x to grayscale by averaging the channels
        x_gray = torch.mean(x, dim=1, keepdim=True) # (B, 1, H, W)
        # Apply the Sobel operator to get the edge maps
        edge_x = F.conv2d(x_gray, self.sobel_x.to(x.device), padding=1) # (B, 1, H, W)
        edge_y = F.conv2d(x_gray, self.sobel_y.to(x.device), padding=1) # (B, 1, H, W)
        # Compute the magnitude of the edges
        edge_mag = torch.sqrt(edge_x ** 2 + edge_y ** 2) # (B, 1, H, W)
        # Normalize the edge maps to [0, 1]
        edge_norm = (edge_mag - edge_mag.min()) / (edge_mag.max() - edge_mag.min()) # (B, 1, H, W)
        return edge_norm

# Define the edge-guided attention sub-module (EASM)
class EASM(nn.Module):
    def __init__(self):
        super(EASM, self).__init__()
        # Define a convolution layer to reduce the channel dimension of the backbone features
        self.conv = nn.Conv2d(256, 64, kernel_size=1)

    def forward(self, x_ftr, x_edge):
        # x_ftr is the backbone feature of shape (B, C_ftr=256 , H_ftr , W_ftr )
        # x_edge is the edge map of shape (B , C_edge=1 , H_edge , W_edge )
        # Resize the edge map to match the size of the backbone feature
        x_edge = F.interpolate(x_edge , size=(x_ftr.shape[2], x_ftr.shape[3]), mode='bilinear', align_corners=False) # (B , C_edge=1 , H_ftr , W_ftr )
        # Reduce the channel dimension of the backbone feature
        x_ftr = self.conv(x_ftr) # (B , C_ftr=64 , H_ftr , W_ftr )
        # Compute the channel-wise attention weights by multiplying the backbone feature and the edge map
        att_weight = torch.mul(x_ftr , x_edge) # (B , C_ftr=64 , H_ftr , W_ftr )
        # Normalize the attention weights by softmax along the channel dimension
        att_weight = F.softmax(att_weight , dim=1) # (B , C_ftr=64 , H_ftr , W_ftr )
        # Apply the attention weights to the backbone feature
        att_out = torch.mul(x_ftr , att_weight) # (B , C_ftr=64 , H_ftr , W_ftr )
        return att_out

# Define the edge-guided attention module (EGAM)
class EGAM(nn.Module):
    def __init__(self):
        super(EGAM , self).__init__()
        # Define an instance of EESM
        self.eesm = EESM()
        # Define an instance of EASM
        self.easm = EASM()

    def forward(self , x_img , x_ftr):
        # x_img is the input image of shape (B , C_img=3 , H_img , W_img )
        # x_ftr is the backbone feature of shape (B , C_ftr=256 , H_ftr , W_ftr )
        # Extract the edge map from the input image using EESM
        x_edge = self.eesm(x_img) # (B , C_edge=1 , H_edge , W_edge )
        # Fuse the edge map with the backbone feature using EASM
        x_att = self.easm(x_ftr , x_edge) # (B , C_ftr=64 , H_ftr , W_ftr )
        return x_att

# Define the feature regrouping sub-module (FRSM)
class FRSM(nn.Module):
    def __init__(self):
        super(FRSM, self).__init__()
        # Define a list of convolution layers to reduce the channel dimension of the backbone features
        self.conv_list = nn.ModuleList([nn.Conv2d(64, 32, kernel_size=1) for _ in range(5)])

    def forward(self, x_list):
        # x_list is a list of backbone features from different levels, each of shape (B, C=64, H, W)
        # Reduce the channel dimension of each backbone feature
        x_list = [self.conv_list[i](x) for i, x in enumerate(x_list)] # a list of (B, C=32, H, W)
        # Regroup the backbone features into three levels according to their semantic information and spatial resolution
        x_level1 = torch.cat([x_list[0], F.interpolate(x_list[1], size=(x_list[0].shape[2], x_list[0].shape[3]), mode='bilinear', align_corners=False)], dim=1) # (B, C=64, H/8, W/8)
        x_level2 = torch.cat([x_list[2], F.interpolate(x_list[3], size=(x_list[2].shape[2], x_list[2].shape[3]), mode='bilinear', align_corners=False)], dim=1) # (B, C=64, H/16, W/16)
        x_level3 = torch.cat([x_list[4], F.interpolate(x_list[3], size=(x_list[4].shape[2], x_list[4].shape[3]), mode='bilinear', align_corners=False)], dim=1) # (B, C=64, H/32, W/32)
        return [x_level1, x_level2, x_level3]

# Define the feature fusion sub-module (FFSM)
class FFSM(nn.Module):
    def __init__(self):
        super(FFSM, self).__init__()
        # Define a list of convolution layers to fuse the features from different levels
        self.conv_list = nn.ModuleList([nn.Conv2d(64, 64, kernel_size=3, padding=1) for _ in range(3)])

    def forward(self, x_list):
        # x_list is a list of regrouped features from different levels, each of shape (B, C=64, H, W)
        # Fuse the features from different levels using element-wise addition and convolution operations
        x_fuse1 = self.conv_list[0](x_list[0] + F.interpolate(x_list[1], size=(x_list[0].shape[2], x_list[0].shape[3]), mode='bilinear', align_corners=False)) # (B, C=64, H/8, W/8)
        x_fuse2 = self.conv_list[1](x_list[1] + F.interpolate(x_fuse1, size=(x_list[1].shape[2], x_list[1].shape[3]), mode='bilinear', align_corners=False)) # (B, C=64, H/16, W/16)
        x_fuse3 = self.conv_list[2](x_list[2] + F.interpolate(x_fuse2, size=(x_list[2].shape[2], x_list[2].shape[3]), mode='bilinear', align_corners=False)) # (B, C=64, H/32, W/32)
        return [x_fuse1 , x_fuse2 , x_fuse3]

# Define the feature refinement sub-module (FRFM)
class FRFM(nn.Module):
    def __init__(self):
        super(FRFM , self).__init__()
        # Define a list of depth-wise separable convolution layers to refine the fused features
        self.dwconv_list = nn.ModuleList([nn.Sequential(nn.Conv2d(64 , 64 , kernel_size=3 , padding=1 , groups=64), nn.Conv2d(64 , 64 , kernel_size=1)) for _ in range(3)])

    def forward(self , x_list):
        # x_list is a list of fused features from different levels , each of shape (B , C=64 , H , W )
        # Refine the fused features using depth-wise separable convolutions and residual connections
        x_refine1 = self.dwconv_list