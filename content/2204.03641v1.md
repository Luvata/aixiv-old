---
title: 2204.03641v1 Unsupervised Image-to-Image Translation with Generative Prior
date: 2022-04-04
---

# [Unsupervised Image-to-Image Translation with Generative Prior](http://arxiv.org/abs/2204.03641v1)

authors: Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy


## What, Why and How

[1]: https://arxiv.org/abs/2204.03641 "Unsupervised Image-to-Image Translation with Generative Prior"
[2]: https://arxiv.org/abs/2208.03641v1 "[2208.03641v1] No More Strided Convolutions or Pooling: A New CNN ..."
[3]: http://arxiv-export3.library.cornell.edu/abs/2204.03641 "[2204.03641] Unsupervised Image-to-Image Translation with Generative Prior"

Here is a summary of the paper:

- **What**: The paper proposes a novel framework for unsupervised image-to-image translation, which can transform images from one domain to another without paired data. For example, it can convert a photo of a cat into a sketch of a cat, or a photo of a flower into a painting of a flower.
- **Why**: The paper aims to improve the quality and applicability of image translation models, which are useful for various applications such as data augmentation, style transfer, and domain adaptation. Existing models struggle to handle complex domains with large visual differences, such as natural scenes and cartoons.
- **How**: The paper leverages the generative prior from pre-trained class-conditional GANs (e.g., BigGAN) to learn rich content correspondences across various domains. It introduces a coarse-to-fine scheme, where it first distills the generative prior to capture a robust coarse-level content representation that can link objects at an abstract semantic level, and then adapts the fine-level content features for more accurate multi-level content correspondences. The paper evaluates the proposed framework on several challenging and distant domains, and shows that it outperforms state-of-the-art methods in terms of quality and diversity of the translations.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel framework for unsupervised image-to-image translation that leverages the generative prior from pre-trained class-conditional GANs to learn rich content correspondences across various domains.
- It introduces a coarse-to-fine scheme that distills the generative prior to capture a robust coarse-level content representation and adapts the fine-level content features for more accurate multi-level content correspondences.
- It demonstrates the superiority of the proposed framework over state-of-the-art methods in terms of quality and diversity of the translations, even for challenging and distant domains.

## Method Summary

[1]: https://arxiv.org/abs/2204.03641 "Unsupervised Image-to-Image Translation with Generative Prior"
[2]: https://arxiv.org/abs/2304.03641 "[2304.03641] A Block Coordinate Descent Method for Nonsmooth Composite ..."
[3]: http://arxiv-export3.library.cornell.edu/abs/2204.03641 "[2204.03641] Unsupervised Image-to-Image Translation with Generative Prior"

Here is a summary of the method section of the paper:

- The paper presents a novel framework called **Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT)**, which consists of three components: a **generative prior distillation module**, a **content adaptation module**, and a **translation module**.
- The **generative prior distillation module** aims to distill the generative prior from pre-trained class-conditional GANs (e.g., BigGAN) to capture a robust coarse-level content representation that can link objects at an abstract semantic level. It does so by training a **distillation network** that can reconstruct images from both domains using the latent codes from the pre-trained GANs. It also trains a **discriminator network** that can classify the images and the latent codes according to their domains and classes.
- The **content adaptation module** aims to adapt the fine-level content features for more accurate multi-level content correspondences. It does so by training a **content encoder network** that can extract fine-grained features from images and align them with the coarse-level features from the generative prior. It also trains a **cycle consistency network** that can enforce cycle consistency between the original and reconstructed images in both domains.
- The **translation module** aims to translate images from one domain to another using the learned content correspondences. It does so by training a **translation network** that can generate images in the target domain given the content features from the source domain. It also trains a **discriminator network** that can distinguish between real and fake images in the target domain.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: images from two domains X and Y
# Output: translated images from X to Y and vice versa

# Pre-train a class-conditional GAN on a large-scale dataset
GAN = pre_train_GAN()

# Initialize the networks for GP-UNIT
DistillNet = init_distill_net()
DistillDisc = init_distill_disc()
ContentEnc = init_content_enc()
CycleNet = init_cycle_net()
TransNet = init_trans_net()
TransDisc = init_trans_disc()

# Train GP-UNIT with the following objectives
for epoch in range(num_epochs):
  # Sample images and latent codes from both domains
  x, y = sample_images(X, Y)
  z_x, z_y = sample_latent_codes(GAN)

  # Distill the generative prior to obtain coarse-level content features
  c_x, c_y = DistillNet(z_x, z_y)
  loss_distill = reconstruction_loss(x, y, DistillNet(c_x, c_y))
  loss_distill_disc = adversarial_loss(DistillDisc(x, y, c_x, c_y))

  # Adapt the fine-level content features to align with the coarse-level features
  f_x, f_y = ContentEnc(x, y)
  loss_content = alignment_loss(f_x, f_y, c_x, c_y)
  loss_cycle = cycle_consistency_loss(x, y, CycleNet(f_x, f_y))

  # Translate images from one domain to another using the content features
  x2y = TransNet(f_x, c_y)
  y2x = TransNet(f_y, c_x)
  loss_trans = reconstruction_loss(x, y, TransNet(ContentEnc(x2y, y2x), c_x, c_y))
  loss_trans_disc = adversarial_loss(TransDisc(x2y, y2x))

  # Update the networks with gradient descent
  update_networks(loss_distill, loss_distill_disc, loss_content, loss_cycle, loss_trans, loss_trans_disc)

# Return the translated images
return x2y, y2x
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
batch_size = 16 # the number of images in each batch
num_epochs = 100 # the number of training epochs
lr = 0.0002 # the learning rate for Adam optimizer
beta1 = 0.5 # the beta1 parameter for Adam optimizer
beta2 = 0.999 # the beta2 parameter for Adam optimizer
lambda_distill = 10 # the weight for distillation loss
lambda_content = 1 # the weight for content alignment loss
lambda_cycle = 10 # the weight for cycle consistency loss
lambda_trans = 10 # the weight for translation loss

# Load the images from two domains X and Y
X = load_images('X')
Y = load_images('Y')

# Pre-train a class-conditional GAN on a large-scale dataset
GAN = pre_train_GAN('ImageNet')

# Initialize the networks for GP-UNIT
DistillNet = init_distill_net() # a network that maps latent codes to coarse-level content features and vice versa
DistillDisc = init_distill_disc() # a network that discriminates images and latent codes according to their domains and classes
ContentEnc = init_content_enc() # a network that extracts fine-level content features from images
CycleNet = init_cycle_net() # a network that reconstructs images from fine-level content features
TransNet = init_trans_net() # a network that translates images from one domain to another using content features
TransDisc = init_trans_disc() # a network that discriminates images according to their domains

# Define the loss functions
reconstruction_loss = torch.nn.L1Loss() # the L1 loss for image reconstruction
adversarial_loss = torch.nn.BCEWithLogitsLoss() # the binary cross entropy loss with logits for adversarial learning
alignment_loss = torch.nn.MSELoss() # the mean squared error loss for feature alignment

# Define the optimizers
optimizer_distill_net = torch.optim.Adam(DistillNet.parameters(), lr=lr, betas=(beta1, beta2))
optimizer_distill_disc = torch.optim.Adam(DistillDisc.parameters(), lr=lr, betas=(beta1, beta2))
optimizer_content_enc = torch.optim.Adam(ContentEnc.parameters(), lr=lr, betas=(beta1, beta2))
optimizer_cycle_net = torch.optim.Adam(CycleNet.parameters(), lr=lr, betas=(beta1, beta2))
optimizer_trans_net = torch.optim.Adam(TransNet.parameters(), lr=lr, betas=(beta1, beta2))
optimizer_trans_disc = torch.optim.Adam(TransDisc.parameters(), lr=lr, betas=(beta1, beta2))

# Train GP-UNIT with the following objectives
for epoch in range(num_epochs):
  # Shuffle and batch the images from both domains
  X_batches = shuffle_and_batch(X, batch_size)
  Y_batches = shuffle_and_batch(Y, batch_size)

  # Iterate over the batches
  for x, y in zip(X_batches, Y_batches):
    # Move the images to GPU if available
    x = x.to('cuda' if torch.cuda.is_available() else 'cpu')
    y = y.to('cuda' if torch.cuda.is_available() else 'cpu')

    # Sample latent codes from both domains using GAN's conditional sampler
    z_x = GAN.sample_latent_codes(x.size(0), x.size(1)) # sample latent codes conditioned on x's classes
    z_y = GAN.sample_latent_codes(y.size(0), y.size(1)) # sample latent codes conditioned on y's classes

    # Distill the generative prior to obtain coarse-level content features
    c_x, c_y = DistillNet(z_x, z_y) # map latent codes to coarse-level content features and vice versa

    # Compute the distillation loss as the reconstruction loss of images and latent codes
    loss_distill_x = reconstruction_loss(x, DistillNet(c_x)) # reconstruct x from c_x and compare with x
    loss_distill_y = reconstruction_loss(y, DistillNet(c_y)) # reconstruct y from c_y and compare with y
    loss_distill_zx = reconstruction_loss(z_x, DistillNet(x)) # reconstruct z_x from x and compare with z_x
    loss_distill_zy = reconstruction_loss(z_y, DistillNet(y)) # reconstruct z_y from y and compare with z_y

    loss_distill = lambda_distill * (loss_distill_x + loss_distill_y + loss_distill_zx + loss_distill_zy)

    # Compute the distillation discriminator loss as the adversarial loss of images and latent codes
    pred_x, pred_zx = DistillDisc(x, z_x) # predict the domain and class labels of x and z_x
    pred_y, pred_zy = DistillDisc(y, z_y) # predict the domain and class labels of y and z_y

    # Create the target labels for real and fake samples
    real_label = torch.ones(batch_size).to('cuda' if torch.cuda.is_available() else 'cpu')
    fake_label = torch.zeros(batch_size).to('cuda' if torch.cuda.is_available() else 'cpu')

    # Create the target labels for class labels
    class_label_x = torch.argmax(x.size(1)).to('cuda' if torch.cuda.is_available() else 'cpu') # get the class label of x
    class_label_y = torch.argmax(y.size(1)).to('cuda' if torch.cuda.is_available() else 'cpu') # get the class label of y

    # Compute the adversarial loss for real and fake samples
    loss_distill_disc_x_real = adversarial_loss(pred_x, real_label) # compare pred_x with real_label
    loss_distill_disc_y_real = adversarial_loss(pred_y, real_label) # compare pred_y with real_label
    loss_distill_disc_zx_fake = adversarial_loss(pred_zx, fake_label) # compare pred_zx with fake_label
    loss_distill_disc_zy_fake = adversarial_loss(pred_zy, fake_label) # compare pred_zy with fake_label

    # Compute the classification loss for class labels
    loss_distill_disc_x_class = adversarial_loss(pred_x, class_label_x) # compare pred_x with class_label_x
    loss_distill_disc_y_class = adversarial_loss(pred_y, class_label_y) # compare pred_y with class_label_y
    loss_distill_disc_zx_class = adversarial_loss(pred_zx, class_label_x) # compare pred_zx with class_label_x
    loss_distill_disc_zy_class = adversarial_loss(pred_zy, class_label_y) # compare pred_zy with class_label_y

    loss_distill_disc = (loss_distill_disc_x_real + loss_distill_disc_y_real + loss_distill_disc_zx_fake + loss_distill_disc_zy_fake + 
                         loss_distill_disc_x_class + loss_distill_disc_y_class + loss_distill_disc_zx_class + loss_distill_disc_zy_class)

    # Update the distillation network and the distillation discriminator network with gradient descent
    optimizer_distill_net.zero_grad()
    optimizer_distill_disc.zero_grad()
    (loss_distill + loss_distill_disc).backward()
    optimizer_distill_net.step()
    optimizer_distill_disc.step()

    # Adapt the fine-level content features to align with the coarse-level features
    f_x, f_y = ContentEnc(x, y) # extract fine-level content features from images

    # Compute the content alignment loss as the mean squared error between fine-level and coarse-level features
    loss_content_x = alignment_loss(f_x, c_x) # compare f_x with c_x
    loss_content_y = alignment_loss(f_y, c_y) # compare f_y with c_y

    loss_content = lambda_content * (loss_content_x + loss_content_y)

    # Compute the cycle consistency loss as the reconstruction loss of images from fine-level features
    x_rec = CycleNet(f_x) # reconstruct x from f_x
    y_rec = CycleNet(f_y) # reconstruct y from f_y

    loss_cycle_x = reconstruction_loss(x, x_rec) # compare x with x_rec
    loss_cycle_y = reconstruction_loss(y, y_rec) # compare y with y_rec

    loss_cycle = lambda_cycle * (loss_cycle_x + loss_cycle_y)

    # Update the content encoder network and the cycle consistency network with gradient descent
    optimizer_content_enc.zero_grad()
    optimizer_cycle_net.zero_grad()
    (loss_content + loss_cycle).backward()
    optimizer_content_enc.step()
    optimizer_cycle_net.step()

    # Translate images from one domain to another using the content features
    x2y = TransNet(f_x, c_y) # translate x to y using f_x and c_y
    y2x = TransNet(f_y, c_x) # translate y to x using f_y and c_x

    # Compute the translation loss as the reconstruction loss of images from translated images
    x2y2x = TransNet(ContentEnc(x2y), c_x) # translate x2y back to x using its content features and c_x
    y2x2y = TransNet(ContentEnc(y2x), c_y) # translate y2x back to y using its content features and c_y

    loss_trans_x = reconstruction