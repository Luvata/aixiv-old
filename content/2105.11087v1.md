---
title: 2105.11087v1 Recent Advances and Trends in Multimodal Deep Learning  A Review
date: 2021-05-12
---

# [Recent Advances and Trends in Multimodal Deep Learning: A Review](http://arxiv.org/abs/2105.11087v1)

authors: Jabeen Summaira, Xi Li, Amin Muhammad Shoib, Songyuan Li, Jabbar Abdul


## What, Why and How

[1]: https://arxiv.org/pdf/2105.11087v1 "JABEEN SUMMAIRA and XI LI AMIN MUHAMMAD SHOIB, School of ... - arXiv.org"
[2]: https://arxiv.org/abs/2105.11087 "Recent Advances and Trends in Multimodal Deep Learning: A Review"
[3]: http://export.arxiv.org/abs/2205.11087v1 "[2205.11087v1] MetaSlicing: A Novel Resource Allocation Framework for ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper is a review of recent advances and trends in multimodal deep learning (MMDL), which is a branch of machine learning that aims to create models that can process and link information using various modalities, such as image, video, text, audio, body gestures, facial expressions, and physiological signals.
- **Why**: The paper argues that MMDL is important because it can better understand and analyze the world around us, which is inherently multimodal. The paper also claims that MMDL can overcome the limitations of unimodal learning, which cannot cover all the aspects of human learning. The paper provides a comprehensive survey of various MMDL applications across different domains, such as multimedia retrieval, sentiment analysis, emotion recognition, human-computer interaction, healthcare, and education.
- **How**: The paper proposes a fine-grained taxonomy of MMDL applications based on the types of modalities involved, the tasks performed, and the architectures used. The paper also discusses the datasets and evaluation metrics used for each application. The paper then highlights the main challenges and future research directions for each domain. The paper also reviews some general issues and open problems for MMDL as a whole.

## Main Contributions

[1]: https://arxiv.org/pdf/2105.11087v1 "JABEEN SUMMAIRA and XI LI AMIN MUHAMMAD SHOIB, School of ... - arXiv.org"
[2]: https://arxiv.org/abs/2105.11087 "Recent Advances and Trends in Multimodal Deep Learning: A Review"
[3]: http://export.arxiv.org/abs/2205.11087v1 "[2205.11087v1] MetaSlicing: A Novel Resource Allocation Framework for ..."

The paper[^1^][1] claims to make the following contributions:

- It provides a **detailed analysis** of past and current baseline approaches and an **in-depth study** of recent advancements in MMDL applications across different domains.
- It proposes a **fine-grained taxonomy** of various MMDL applications based on the types of modalities involved, the tasks performed, and the architectures used.
- It discusses the **architectures and datasets** used in each application, along with their **evaluation metrics**.
- It highlights the **main issues** and **future research directions** for each domain separately, as well as some general problems and challenges for MMDL as a whole.

## Method Summary

[1]: https://arxiv.org/pdf/2105.11087v1 "JABEEN SUMMAIRA and XI LI AMIN MUHAMMAD SHOIB, School of ... - arXiv.org"
[2]: https://arxiv.org/abs/2105.11087 "Recent Advances and Trends in Multimodal Deep Learning: A Review"
[3]: http://export.arxiv.org/abs/2205.11087v1 "[2205.11087v1] MetaSlicing: A Novel Resource Allocation Framework for ..."

The method section of the paper[^1^][1] consists of the following subsections:

- **Multimodal Deep Learning**: This subsection introduces the basic concepts and definitions of MMDL, such as modalities, fusion methods, alignment methods, and representation learning methods. It also provides some examples of MMDL architectures, such as autoencoders, recurrent neural networks, convolutional neural networks, attention mechanisms, transformers, and graph neural networks.
- **Multimodal Deep Learning Applications**: This subsection presents a fine-grained taxonomy of various MMDL applications based on the types of modalities involved, the tasks performed, and the architectures used. It also reviews some representative works for each application and discusses their main challenges and future directions. The applications are categorized into the following domains: multimedia retrieval, sentiment analysis, emotion recognition, human-computer interaction, healthcare, and education.
- **Datasets and Evaluation Metrics**: This subsection summarizes the datasets and evaluation metrics used for each MMDL application. It also provides some statistics and characteristics of the datasets, such as the number of samples, the modalities involved, the annotations provided, and the data sources. The evaluation metrics include accuracy, precision, recall, F1-score, mean average precision, mean reciprocal rank, normalized discounted cumulative gain, root mean square error, mean absolute error, and Pearson correlation coefficient.

## Pseudo Code - High level

The paper does not propose a specific algorithm or model, but rather reviews various MMDL applications and architectures. Therefore, it is not possible to give a high-level pseudo code for the whole paper. However, I can try to give a general pseudo code for a typical MMDL pipeline, which may vary depending on the application and the architecture.

A general pseudo code for a typical MMDL pipeline is:

```
# Input: a set of multimodal data samples X = {x_1, x_2, ..., x_n}, where x_i = (m_1, m_2, ..., m_k) is a k-modal sample
# Output: a set of predictions or representations Y = {y_1, y_2, ..., y_n}, where y_i is the output for x_i

# Define the MMDL architecture, which consists of three main components: modality-specific encoders, fusion layer, and task-specific decoder
# The modality-specific encoders are neural networks that map each modality to a latent space
# The fusion layer is a neural network that combines the latent representations of different modalities
# The task-specific decoder is a neural network that performs the desired task on the fused representation

# Initialize the parameters of the MMDL architecture randomly or with pre-trained weights
# Define the loss function and the optimizer for training the MMDL architecture

# For each epoch in a fixed number of epochs or until convergence:
  # Shuffle the data samples X
  # For each batch of data samples X_b in X:
    # Split each sample x_i in X_b into its modalities m_1, m_2, ..., m_k
    # For each modality m_j in k modalities:
      # Encode m_j using the modality-specific encoder E_j to get the latent representation z_j
    # Concatenate or fuse the latent representations z_1, z_2, ..., z_k using the fusion layer F to get the fused representation f
    # Decode f using the task-specific decoder D to get the output y
    # Compute the loss L between y and the ground truth label or representation
    # Update the parameters of the MMDL architecture using the optimizer and the gradient of L
  # Evaluate the performance of the MMDL architecture on a validation set and save the best model
# Return the best model
```

## Pseudo Code - Detail

As I mentioned before, the paper does not propose a specific algorithm or model, but rather reviews various MMDL applications and architectures. Therefore, it is not possible to give a detailed pseudo code to implement the whole paper. However, I can try to give a more detailed pseudo code for a specific MMDL application and architecture, which may vary depending on the data and the task.

For example, suppose we want to implement a MMDL application for video captioning, which is the task of generating natural language descriptions for videos. A possible MMDL architecture for this task is the Hierarchical Recurrent Encoder-Decoder (HRED) model , which consists of three main components: a video encoder, an attention-based multimodal fusion layer, and a caption decoder.

A more detailed pseudo code for this MMDL application and architecture is:

```
# Input: a set of video-caption pairs V = {(v_1, c_1), (v_2, c_2), ..., (v_n, c_n)}, where v_i is a video and c_i is its caption
# Output: a set of generated captions C = {c'_1, c'_2, ..., c'_n}, where c'_i is the generated caption for v_i

# Define the HRED architecture, which consists of three main components: video encoder, attention-based multimodal fusion layer, and caption decoder
# The video encoder is a convolutional neural network (CNN) that extracts frame-level features from each video
# The attention-based multimodal fusion layer is a recurrent neural network (RNN) with an attention mechanism that fuses the frame-level features with the word-level features from the previous caption words
# The caption decoder is another RNN that generates the caption words conditioned on the fused features

# Initialize the parameters of the HRED architecture randomly or with pre-trained weights
# Define the loss function and the optimizer for training the HRED architecture

# For each epoch in a fixed number of epochs or until convergence:
  # Shuffle the video-caption pairs V
  # For each batch of video-caption pairs V_b in V:
    # Split each pair (v_i, c_i) in V_b into its video v_i and its caption c_i
    # For each video v_i in V_b:
      # Extract frame-level features f_i using the video encoder CNN
    # For each caption c_i in V_b:
      # Split c_i into words w_1, w_2, ..., w_T
      # Encode each word w_t using a word embedding matrix E to get word-level features e_t
    # Initialize the hidden state h_0 of the fusion layer RNN and the decoder RNN to zero vectors
    # Initialize an empty list L to store the generated words
    # For each time step t in 1 to T:
      # Compute the attention weights a_t over the frame-level features f_i using h_t-1 and e_t as queries
      # Compute the context vector c_t as the weighted sum of f_i using a_t as weights
      # Fuse c_t and e_t using the fusion layer RNN to get the fused feature g_t and update h_t
      # Generate the next word w'_t using the decoder RNN conditioned on g_t and update h'_t
      # Append w'_t to L
    # Concatenate the words in L to get the generated caption c'_i
    # Compute the loss L between c'_i and c_i using cross-entropy or other metrics
    # Update the parameters of the HRED architecture using the optimizer and the gradient of L
  # Evaluate the performance of the HRED architecture on a validation set and save the best model
# Return the best model
```
: Li et al., "Hierarchical Recurrent Encoder with Latent Topic Clustering for Video Captioning", https://arxiv.org/abs/1804.01700