---
title: 1906.07901v1 Multimodal Abstractive Summarization for How2 Videos
date: 2019-06-08
---

# [Multimodal Abstractive Summarization for How2 Videos](http://arxiv.org/abs/1906.07901v1)

authors: Shruti Palaskar, Jindrich Libovick√Ω, Spandana Gella, Florian Metze


## What, Why and How

[1]: https://arxiv.org/pdf/1906.07901v1.pdf "arXiv:1906.07901v1 [cs.CL] 19 Jun 2019"
[2]: https://arxiv.org/abs/1906.07901 "Multimodal Abstractive Summarization for How2 Videos"
[3]: http://export.arxiv.org/abs/2106.07901v1 "[2106.07901v1] FIGARO Simulation: FIlaments & GAlactic RadiO Simulation"

Here is a summary of the paper[^1^][1]:

- **What**: The paper studies **abstractive summarization** for **open-domain videos**, especially instructional videos from the **How2 corpus**. Abstractive summarization is the task of producing a shorter version of the content while preserving its information and using different words or phrases from the original source.
- **Why**: The paper aims to generate a short text summary of the video that describes the most salient content of the video, which can benefit users and video sharing platforms by providing better contextual information and user experience, and retrieving or suggesting relevant videos to users.
- **How**: The paper shows how a **multi-source sequence-to-sequence model with hierarchical attention** can integrate information from different modalities (video and audio transcripts) into a coherent output. The paper also proposes a new evaluation metric (**Content F1**) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries.

## Main Contributions

According to the paper, the contributions are:

- The first work on **abstractive summarization** for **open-domain videos** using a **multi-modal** approach.
- A new evaluation metric (**Content F1**) that captures the semantic similarity between the summary and the source modalities, and correlates better with human judgments than existing metrics like ROUGE and BLEU.
- An analysis of the effect of different modalities and their combinations on the summarization performance and the content selection.

## Method Summary

[1]: https://arxiv.org/pdf/1906.07901v1.pdf "arXiv:1906.07901v1 [cs.CL] 19 Jun 2019"
[2]: https://arxiv.org/abs/1906.07901v1 "Multimodal Abstractive Summarization for How2 Videos"
[3]: https://www.researchgate.net/publication/333892220_Multimodal_Abstractive_Summarization_for_How2_Videos "(PDF) Multimodal Abstractive Summarization for How2 Videos - ResearchGate"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses a **multi-source sequence-to-sequence model** with an encoder-decoder architecture and an attention mechanism to generate summaries from different modalities (video and audio transcripts).
- The encoder consists of **two sub-encoders**, one for each modality, that encode the input sequences into hidden states. The sub-encoders are **bidirectional GRUs** with **layer normalization** and **dropout**.
- The decoder is a **unidirectional GRU** that generates the summary tokens conditioned on the encoder hidden states and the previous decoder state. The decoder uses a **hierarchical attention mechanism** that first computes the attention weights for each modality separately and then combines them using another attention layer.
- The paper uses the **How2 corpus** of instructional videos as the data source. The corpus consists of 79,114 videos with English subtitles and Portuguese translations. The paper uses the subtitles as the audio transcripts and extracts the video transcripts using a pre-trained object detection model.
- The paper splits the data into train, validation and test sets, and preprocesses the data by tokenizing, lowercasing, and truncating the sequences. The paper also filters out videos that have no objects detected or have summaries longer than 20 words.
- The paper trains the model using **cross-entropy loss** and **Adam optimizer** with a learning rate of 0.0003 and a batch size of 64. The paper uses **beam search** with a beam size of 5 to generate summaries during inference.
- The paper evaluates the model using **ROUGE**, **BLEU**, and **Content F1** metrics. Content F1 is a new metric proposed by the paper that measures the overlap between the summary and the source modalities in terms of content words (nouns, verbs, adjectives, and adverbs).

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder sub-networks for video and audio modalities
video_encoder = BiGRU(input_size=video_feature_size, hidden_size=hidden_size)
audio_encoder = BiGRU(input_size=word_embedding_size, hidden_size=hidden_size)

# Define the decoder network with hierarchical attention
decoder = GRU(input_size=word_embedding_size, hidden_size=hidden_size)
attention = HierarchicalAttention(hidden_size=hidden_size)

# Define the loss function and the optimizer
loss = CrossEntropyLoss()
optimizer = Adam(learning_rate=0.0003)

# Load the data from the How2 corpus
data = load_data("how2")

# Preprocess the data by tokenizing, lowercasing, and truncating the sequences
data = preprocess_data(data)

# Train the model on the training set
for epoch in range(num_epochs):
  for batch in data.train:
    # Extract the video features, audio transcripts, and summaries from the batch
    video_features, audio_transcripts, summaries = batch

    # Encode the video features and audio transcripts using the sub-encoders
    video_states = video_encoder(video_features)
    audio_states = audio_encoder(audio_transcripts)

    # Initialize the decoder state with the last encoder state
    decoder_state = audio_states[-1]

    # Initialize the decoder input with a start-of-sequence token
    decoder_input = "<sos>"

    # Initialize the loss for this batch
    batch_loss = 0

    # Decode the summary tokens until the end-of-sequence token or the maximum length is reached
    for t in range(max_length):
      # Compute the attention weights for each modality and combine them using another attention layer
      video_attention_weights = attention(video_states, decoder_state)
      audio_attention_weights = attention(audio_states, decoder_state)
      combined_attention_weights = attention([video_attention_weights, audio_attention_weights], decoder_state)

      # Compute the context vector as a weighted sum of the encoder states and the combined attention weights
      context_vector = sum(combined_attention_weights * [video_states, audio_states])

      # Concatenate the context vector and the decoder input and pass it to the decoder
      decoder_input = concatenate(context_vector, decoder_input)
      decoder_output, decoder_state = decoder(decoder_input)

      # Predict the next summary token using a linear layer and a softmax function
      summary_token = softmax(linear(decoder_output))

      # Compute the cross-entropy loss between the predicted token and the target token
      target_token = summaries[t]
      token_loss = loss(summary_token, target_token)

      # Accumulate the loss for this batch
      batch_loss += token_loss

      # Update the decoder input with the target token
      decoder_input = target_token

    # Backpropagate the loss and update the model parameters
    batch_loss.backward()
    optimizer.step()

# Evaluate the model on the test set using ROUGE, BLEU, and Content F1 metrics
metrics = ["ROUGE", "BLEU", "Content F1"]
scores = {}
for metric in metrics:
  scores[metric] = 0

for batch in data.test:
  # Extract the video features, audio transcripts, and summaries from the batch
  video_features, audio_transcripts, summaries = batch

  # Encode the video features and audio transcripts using the sub-encoders
  video_states = video_encoder(video_features)
  audio_states = audio_encoder(audio_transcripts)

  # Initialize the decoder state with the last encoder state
  decoder_state = audio_states[-1]

  # Initialize the decoder input with a start-of-sequence token
  decoder_input = "<sos>"

  # Initialize an empty list to store the generated summary tokens
  generated_summary = []

  # Decode the summary tokens until the end-of-sequence token or the maximum length is reached
  for t in range(max_length):
    # Compute the attention weights for each modality and combine them using another attention layer
    video_attention_weights = attention(video_states, decoder_state)
    audio_attention_weights = attention(audio_states, decoder_state)
    combined_attention_weights = attention([video_attention_weights, audio_attention_weights], decoder_state)

    # Compute the context vector as a weighted sum of the encoder states and the combined attention weights
    context_vector = sum(combined_attention_weights * [video_states, audio_states])

    # Concatenate the context vector and the decoder input and pass it to the decoder
    decoder_input = concatenate(context_vector, decoder_input)
    decoder_output, decoder_state = decoder(decoder_input)

    # Predict the next summary token using a linear layer and a softmax function
    summary_token = softmax(linear(decoder_output))

    # Append the predicted token to the generated summary list
    generated_summary.append(summary_token)

    # Update the decoder input with the predicted token
    decoder_input = summary_token

    # Stop decoding if the end-of-sequence token is reached
    if summary_token == "<eos>":
      break

  # Compute the scores for each metric between the generated summary and the reference summary
  for metric in metrics:
    score = compute_score(metric, generated_summary, summaries)
    scores[metric] += score

# Report the average scores for each metric
for metric in metrics:
  average_score = scores[metric] / len(data.test)
  print(metric, average_score)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchtext
import torchvision
import numpy as np

# Define the hyperparameters
hidden_size = 256 # The size of the hidden states of the GRUs
video_feature_size = 2048 # The size of the video features extracted by the object detection model
word_embedding_size = 300 # The size of the word embeddings
max_length = 20 # The maximum length of the summaries
num_epochs = 10 # The number of epochs to train the model
learning_rate = 0.0003 # The learning rate for the optimizer
batch_size = 64 # The batch size for the data loader
beam_size = 5 # The beam size for the beam search

# Define the encoder sub-networks for video and audio modalities
class Encoder(nn.Module):
  def __init__(self, input_size, hidden_size):
    super(Encoder, self).__init__()
    # Initialize a bidirectional GRU with layer normalization and dropout
    self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, bidirectional=True)
    self.layer_norm = nn.LayerNorm(hidden_size * 2)
    self.dropout = nn.Dropout(0.1)

  def forward(self, x):
    # x: a tensor of shape (sequence_length, batch_size, input_size)
    # output: a tensor of shape (sequence_length, batch_size, hidden_size * 2)
    # hidden: a tensor of shape (2, batch_size, hidden_size)
    output, hidden = self.gru(x)

    # Apply layer normalization and dropout to the output
    output = self.layer_norm(output)
    output = self.dropout(output)

    return output, hidden

# Instantiate the video encoder and the audio encoder
video_encoder = Encoder(input_size=video_feature_size, hidden_size=hidden_size)
audio_encoder = Encoder(input_size=word_embedding_size, hidden_size=hidden_size)

# Define the decoder network with hierarchical attention
class Decoder(nn.Module):
  def __init__(self, input_size, hidden_size, vocab_size):
    super(Decoder, self).__init__()
    # Initialize a unidirectional GRU with layer normalization and dropout
    self.gru = nn.GRU(input_size=input_size + hidden_size * 4, hidden_size=hidden_size)
    self.layer_norm = nn.LayerNorm(hidden_size)
    self.dropout = nn.Dropout(0.1)

    # Initialize a linear layer to project the decoder output to the vocabulary size
    self.linear = nn.Linear(hidden_size, vocab_size)

    # Initialize a hierarchical attention module
    self.attention = HierarchicalAttention(hidden_size=hidden_size)

  def forward(self, x, encoder_states, decoder_state):
    # x: a tensor of shape (1, batch_size, input_size)
    # encoder_states: a list of two tensors of shape (sequence_length, batch_size, hidden_size * 2), one for each modality
    # decoder_state: a tensor of shape (1, batch_size, hidden_size)
    # output: a tensor of shape (1, batch_size, vocab_size)
    # decoder_state: a tensor of shape (1, batch_size, hidden_size)
    
    # Compute the attention weights for each modality and combine them using another attention layer
    video_attention_weights = self.attention(encoder_states[0], decoder_state) # a tensor of shape (batch_size, sequence_length)
    audio_attention_weights = self.attention(encoder_states[1], decoder_state) # a tensor of shape (batch_size, sequence_length)
    combined_attention_weights = self.attention(torch.stack([video_attention_weights, audio_attention_weights], dim=-1), decoder_state) # a tensor of shape (batch_size, 2)

    # Compute the context vector as a weighted sum of the encoder states and the combined attention weights
    context_vector = torch.sum(combined_attention_weights.unsqueeze(1) * torch.stack(encoder_states), dim=2) # a tensor of shape (batch_size, 1, hidden_size * 2)

    # Concatenate the context vector and the decoder input and pass it to the decoder
    x = torch.cat([context_vector.permute(1, 0 ,2), x], dim=-1) # a tensor of shape (1 ,batch size ,input size + hidden size * 4)
    output ,decoder_state = self.gru(x ,decoder_state) # output: a tensor of shape (1 ,batch size ,hidden size), decoder_state: a tensor of shape (1 ,batch size ,hidden size)

    # Apply layer normalization and dropout to the output
    output = self.layer_norm(output)
    output = self.dropout(output)

    # Predict the next summary token using a linear layer and a softmax function
    output = self.linear(output) # a tensor of shape (1, batch_size, vocab_size)
    output = torch.softmax(output, dim=-1) # a tensor of shape (1, batch_size, vocab_size)

    return output, decoder_state

# Define the hierarchical attention module
class HierarchicalAttention(nn.Module):
  def __init__(self, hidden_size):
    super(HierarchicalAttention, self).__init__()
    # Initialize a linear layer to project the encoder states and the decoder state to the same dimension
    self.linear = nn.Linear(hidden_size * 2, hidden_size)

    # Initialize a vector to compute the attention scores
    self.vector = nn.Parameter(torch.rand(hidden_size))

  def forward(self, encoder_states, decoder_state):
    # encoder_states: a tensor of shape (sequence_length, batch_size, hidden_size * 2)
    # decoder_state: a tensor of shape (1, batch_size, hidden_size)
    # attention_weights: a tensor of shape (batch_size, sequence_length)

    # Project the encoder states and the decoder state to the same dimension
    encoder_states = self.linear(encoder_states) # a tensor of shape (sequence_length, batch_size, hidden_size)
    decoder_state = decoder_state.permute(1, 0 ,2) # a tensor of shape (batch_size, 1, hidden_size)

    # Compute the attention scores using a dot product between the encoder states and the vector
    attention_scores = torch.tanh(encoder_states) @ self.vector # a tensor of shape (sequence_length, batch_size)
    attention_scores = attention_scores.permute(1 ,0) # a tensor of shape (batch_size ,sequence length)

    # Compute the attention weights using a softmax function
    attention_weights = torch.softmax(attention_scores ,dim=-1) # a tensor of shape (batch size ,sequence length)

    return attention_weights

# Instantiate the decoder network
decoder = Decoder(input_size=word_embedding_size ,hidden_size=hidden_size ,vocab_size=len(data.vocab))

# Define the loss function and the optimizer
loss = nn.CrossEntropyLoss()
optimizer = optim.Adam(params=list(video_encoder.parameters()) + list(audio_encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)

# Load the data from the How2 corpus
data = torchtext.data.TabularDataset(path="how2.csv", format="csv", fields=[("video_features", torchtext.data.Field(sequential=True)), ("audio_transcripts", torchtext.data.Field(sequential=True)), ("summaries", torchtext.data.Field(sequential=True))])

# Preprocess the data by tokenizing, lowercasing, and truncating the sequences
data = data.map(lambda x: {"video_features": x.video_features[:max_length], "audio_transcripts": x.audio_transcripts[:max_length], "summaries": x.summaries[:max_length]})
data = data.map(lambda x: {"video_features": x.video_features.lower(), "audio_transcripts": x.audio_transcripts.lower(), "summaries": x.summaries.lower()})

# Build the vocabulary for the audio transcripts and summaries using GloVe embeddings
data.fields["audio_transcripts"].build_vocab(data, vectors="glove.6B.300d")
data.fields["summaries"].build_vocab(data, vectors="glove.6B.300d")

# Split the data into train, validation and test sets
train_data, valid_data, test_data = data.split(split_ratio=[0.8, 0.1, 0.1])

# Create data loaders for each set
train_loader = torchtext.data.BucketIterator(train_data, batch_size=batch_size, shuffle=True)
valid_loader = torchtext.data.BucketIterator(valid_data, batch_size=batch_size)
test_loader = torchtext.data.BucketIterator(test_data, batch_size=batch_size)

# Train the model on the training set
for epoch in range(num_epochs):
  for batch in train_loader:
    # Extract the video features, audio transcripts, and summaries from the batch
    video_features = batch.video_features # a tensor of shape (sequence_length ,batch size ,video feature size)
    audio_transcripts = batch.audio_transcripts # a tensor of shape (sequence length ,batch size)
    summaries = batch.summaries # a tensor of shape (sequence length ,batch size)

    # Encode the video features and audio transcripts using the sub-encoders
    video_states ,_ = video_encoder(video_features) # a tensor of shape (sequence length ,batch size ,hidden size * 2)
    audio_states ,_ = audio_encoder(audio_transcripts) # a tensor of shape (sequence length ,batch size ,hidden size * 2)

    # Initialize the decoder state with the last