---
title: 2203.14463v2 Large-scale Bilingual Language-Image Contrastive Learning
date: 2022-03-15
---

# [Large-scale Bilingual Language-Image Contrastive Learning](http://arxiv.org/abs/2203.14463v2)

authors: Byungsoo Ko, Geonmo Gu


## What, Why and How

[1]: https://arxiv.org/abs/2203.14463v2 "Large-scale Bilingual Language-Image Contrastive Learning"
[2]: https://arxiv.org/pdf/2203.14463 "A arXiv:2203.14463v2 [cs.CV] 15 Apr 2022"
[3]: https://arxiv-export2.library.cornell.edu/abs/2208.14463v2 "[2208.14463v2] Study of the $b \\to d \\ell\\ell$ transitions in the ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper is a technical report to share the experience and findings of building a **Korean and English bilingual multimodal model** named **KELIP** that can learn visual and textual representations from 1.1 billion image-text pairs collected from the Internet.
- **Why**: The paper aims to address the limitations of existing multimodal models that are mostly trained in English and may not capture the unique expressions, cultural information, and proper nouns in languages other than English. The paper also aims to explore some multimodal-related research questions, such as the effects of augmentation, cross-lingual relation, cultural differences, and multimodal feature analogy.
- **How**: The paper introduces some simple yet effective training schemes, such as **MAE pre-training** and **multi-crop augmentation**, to improve the performance and generalization of the bilingual multimodal model. The paper also conducts extensive experiments on various downstream tasks, such as image-text retrieval, zero-shot classification, and multimodal feature analogy, to demonstrate the competitive performance of KELIP in both languages and its ability to capture cross-lingual and cross-modal relations.

## Main Contributions

[1]: https://arxiv.org/abs/2203.14463v2 "Large-scale Bilingual Language-Image Contrastive Learning"
[2]: https://arxiv.org/pdf/2203.14463 "A arXiv:2203.14463v2 [cs.CV] 15 Apr 2022"
[3]: https://arxiv-export2.library.cornell.edu/abs/2208.14463v2 "[2208.14463v2] Study of the $b \\to d \\ell\\ell$ transitions in the ..."

According to the paper[^1^][1], the main contributions are:

- **Collecting a large-scale bilingual multimodal dataset** of 1.1 billion image-text pairs (708 million Korean and 476 million English) from various sources, such as web pages, social media, and news articles.
- **Proposing a bilingual multimodal model** named KELIP that can learn visual and textual representations from both Korean and English image-text pairs using a contrastive learning objective.
- **Introducing simple yet effective training schemes**, such as MAE pre-training and multi-crop augmentation, to improve the performance and generalization of KELIP.
- **Conducting extensive experiments** on various downstream tasks, such as image-text retrieval, zero-shot classification, and multimodal feature analogy, to demonstrate the competitive performance of KELIP in both languages and its ability to capture cross-lingual and cross-modal relations.
- **Discussing multimodal-related research questions**, such as the effects of augmentation, cross-lingual relation, cultural differences, and multimodal feature analogy, and providing some insights and findings for future research.
- **Providing an open-source pre-trained KELIP** for the research community.

## Method Summary

[1]: https://arxiv.org/abs/2203.14463v2 "Large-scale Bilingual Language-Image Contrastive Learning"
[2]: https://arxiv.org/pdf/2203.14463 "A arXiv:2203.14463v2 [cs.CV] 15 Apr 2022"
[3]: https://arxiv-export2.library.cornell.edu/abs/2208.14463v2 "[2208.14463v2] Study of the $b \\to d \\ell\\ell$ transitions in the ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper adopts a **contrastive learning objective** to train the bilingual multimodal model, which aims to maximize the similarity between image-text pairs that share the same semantic meaning and minimize the similarity between pairs that do not.
- The paper uses two separate models for each modality: a **ResNet-50** model for images and a **BERT-base** model for texts. The models are pre-trained on large-scale unimodal datasets, such as ImageNet and Wikipedia, before being fine-tuned on the multimodal dataset.
- The paper collects a large-scale bilingual multimodal dataset of 1.1 billion image-text pairs from various sources, such as web pages, social media, and news articles. The paper applies some filtering and cleaning steps to ensure the quality and diversity of the dataset.
- The paper introduces two simple yet effective training schemes to improve the performance and generalization of the bilingual multimodal model: **MAE pre-training** and **multi-crop augmentation**. MAE pre-training is a masked autoencoder objective that encourages the model to reconstruct the masked regions of images or texts from the unmasked regions. Multi-crop augmentation is a data augmentation technique that randomly crops multiple regions of different sizes from an image and feeds them to the model along with the original image.
- The paper conducts extensive experiments on various downstream tasks, such as image-text retrieval, zero-shot classification, and multimodal feature analogy, to evaluate the performance of KELIP in both languages and its ability to capture cross-lingual and cross-modal relations. The paper also compares KELIP with other state-of-the-art multimodal models and analyzes the effects of different training schemes and data sources.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the image model and the text model
image_model = ResNet50(pretrained=True)
text_model = BERTBase(pretrained=True)

# Define the contrastive loss function
contrastive_loss = NTXentLoss(temperature=0.07)

# Define the MAE loss function
mae_loss = L1Loss()

# Load the bilingual multimodal dataset
dataset = load_dataset("kelip")

# For each epoch
for epoch in range(num_epochs):

  # Shuffle the dataset
  dataset.shuffle()

  # For each batch of image-text pairs
  for batch in dataset:

    # Extract the images and texts from the batch
    images = batch["images"]
    texts = batch["texts"]

    # Apply multi-crop augmentation to the images
    crops = multi_crop(images)

    # Mask some regions of the images and texts
    masked_images, image_masks = mask(images)
    masked_texts, text_masks = mask(texts)

    # Forward pass the images and texts through the models
    image_features = image_model(images + crops + masked_images)
    text_features = text_model(texts + masked_texts)

    # Compute the contrastive loss between image and text features
    loss_contrastive = contrastive_loss(image_features, text_features)

    # Compute the MAE loss between masked and unmasked regions
    loss_mae_image = mae_loss(image_model(masked_images), image_masks)
    loss_mae_text = mae_loss(text_model(masked_texts), text_masks)

    # Compute the total loss as a weighted sum of the losses
    loss_total = alpha * loss_contrastive + beta * (loss_mae_image + loss_mae_text)

    # Backward pass and update the model parameters
    loss_total.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the image model and the text model
image_model = torchvision.models.resnet50(pretrained=True)
text_model = transformers.BertModel.from_pretrained("bert-base-multilingual-cased")

# Freeze the model parameters except for the last layer
for param in image_model.parameters():
  param.requires_grad = False
for param in text_model.parameters():
  param.requires_grad = False
image_model.fc.requires_grad = True
text_model.pooler.requires_grad = True

# Define the contrastive loss function
contrastive_loss = torch.nn.NTXentLoss(temperature=0.07)

# Define the MAE loss function
mae_loss = torch.nn.L1Loss()

# Define the optimizer and the learning rate scheduler
optimizer = torch.optim.Adam([image_model.fc.parameters(), text_model.pooler.parameters()], lr=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Define the hyperparameters
num_epochs = 100
batch_size = 256
alpha = 1.0 # weight for contrastive loss
beta = 0.1 # weight for MAE loss

# Load the bilingual multimodal dataset
dataset = load_dataset("kelip")

# Split the dataset into train and validation sets
train_set, val_set = dataset.split([0.9, 0.1])

# Create data loaders for train and validation sets
train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)

# Define a function to apply multi-crop augmentation to a batch of images
def multi_crop(images):

  # Initialize an empty list to store the crops
  crops = []

  # For each image in the batch
  for image in images:

    # Randomly crop four regions of size 224x224 from the image
    crop_1 = torchvision.transforms.RandomCrop(224)(image)
    crop_2 = torchvision.transforms.RandomCrop(224)(image)
    crop_3 = torchvision.transforms.RandomCrop(224)(image)
    crop_4 = torchvision.transforms.RandomCrop(224)(image)

    # Randomly crop two regions of size 96x96 from the image
    crop_5 = torchvision.transforms.RandomCrop(96)(image)
    crop_6 = torchvision.transforms.RandomCrop(96)(image)

    # Resize the smaller crops to 224x224
    crop_5 = torchvision.transforms.Resize(224)(crop_5)
    crop_6 = torchvision.transforms.Resize(224)(crop_6)

    # Apply random horizontal flip and color jitter to all crops
    transform = torchvision.transforms.Compose([
      torchvision.transforms.RandomHorizontalFlip(),
      torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4)
    ])
    crop_1 = transform(crop_1)
    crop_2 = transform(crop_2)
    crop_3 = transform(crop_3)
    crop_4 = transform(crop_4)
    crop_5 = transform(crop_5)
    crop_6 = transform(crop_6)

    # Append the crops to the list
    crops.append(crop_1)
    crops.append(crop_2)
    crops.append(crop_3)
    crops.append(crop_4)
    crops.append(crop_5)
    crops.append(crop_6)

  # Convert the list of crops to a tensor and return it
  crops = torch.stack(crops)
  return crops

# Define a function to mask some regions of a batch of images or texts
def mask(inputs):

  # Initialize an empty list to store the masked inputs and masks
  masked_inputs = []
  masks = []

  # For each input in the batch
  for input in inputs:

    # If the input is an image
    if isinstance(input, torch.Tensor):

      # Randomly select a region size from [16,32,64]
      region_size = np.random.choice([16,32,64])

      # Randomly select a region position from [0,223-region_size]
      x_pos = np.random.randint(0, 223 - region_size + 1)
      y_pos = np.random.randint(0, 223 - region_size + 1)

      # Create a copy of the input image and mask the selected region with zeros
      masked_input = input.clone()
      masked_input[:, x_pos:x_pos+region_size, y_pos:y_pos+region_size] = 0

      # Create a mask tensor that has the same shape as the input image and has ones in the selected region and zeros elsewhere
      mask = torch.zeros_like(input)
      mask[:, x_pos:x_pos+region_size, y_pos:y_pos+region_size] = 1

    # If the input is a text
    else:

      # Convert the input text to a list of tokens using BERT tokenizer
      tokens = transformers.BertTokenizer.from_pretrained("bert-base-multilingual-cased").tokenize(input)

      # Randomly select a number of tokens to mask from [1,10]
      num_tokens = np.random.randint(1, 10 + 1)

      # Randomly select the token indices to mask
      token_indices = np.random.choice(len(tokens), size=num_tokens, replace=False)

      # Create a copy of the input text and mask the selected tokens with [MASK] token
      masked_input = input
      for index in token_indices:
        masked_input = masked_input.replace(tokens[index], "[MASK]")

      # Create a mask tensor that has the same shape as the input text and has ones in the selected tokens and zeros elsewhere
      mask = torch.zeros(len(tokens))
      mask[token_indices] = 1

    # Append the masked input and mask to the list
    masked_inputs.append(masked_input)
    masks.append(mask)

  # Convert the list of masked inputs and masks to tensors and return them
  masked_inputs = torch.stack(masked_inputs) if isinstance(inputs[0], torch.Tensor) else masked_inputs
  masks = torch.stack(masks)
  return masked_inputs, masks

# For each epoch
for epoch in range(num_epochs):

  # Set the models to training mode
  image_model.train()
  text_model.train()

  # Initialize the running loss for train set
  train_loss = 0.0

  # For each batch of image-text pairs in train set
  for batch in train_loader:

    # Extract the images and texts from the batch
    images = batch["images"]
    texts = batch["texts"]

    # Apply multi-crop augmentation to the images
    crops = multi_crop(images)

    # Mask some regions of the images and texts
    masked_images, image_masks = mask(images)
    masked_texts, text_masks = mask(texts)

    # Forward pass the images and texts through the models
    image_features = image_model(torch.cat([images, crops, masked_images], dim=0))
    text_features = text_model(texts + masked_texts).pooler_output

    # Compute the contrastive loss between image and text features
    loss_contrastive = contrastive_loss(image_features, text_features)

    # Compute the MAE loss between masked and unmasked regions
    loss_mae_image = mae_loss(image_model(masked_images), image_masks * images)
    loss_mae_text = mae_loss(text_model(masked_texts).pooler_output, text_masks.unsqueeze(-1) * text_features[:len(texts)])

    # Compute the total loss as a weighted sum of the losses
    loss_total = alpha * loss_contrastive + beta * (loss_mae_image + loss_mae_text)

    # Backward pass and update the model parameters
    optimizer.zero_grad()
    loss_total.backward()
    optimizer.step()

    # Update the running loss for train set
    train_loss += loss_total.item()

  # Set the models to evaluation mode
  image_model.eval()
  text_model.eval()

  # Initialize the running loss for validation set
  val_loss = 0.0

  # For each batch of image-text pairs in validation set
  for batch in val_loader:

    # Extract the images and texts from the batch
    images = batch["images"]
    texts = batch["texts"]

    # Forward pass the images and texts through the models
    image_features = image_model(images)
    text_features = text_model(texts).pooler_output

    # Compute the contrastive loss between image and text features
    loss_contrastive = contrastive_loss(image_features, text_features)

    # Update the running loss for validation set
    val_loss += loss_contrastive.item()

  # Compute the average losses for train and validation sets
  train_loss /= len(train_loader)
  val_loss /= len(val_loader)

  # Print the epoch number and the losses
  print(f"Epoch {epoch+1}: Train Loss = {train_loss}, Val Loss = {val_loss}")

  # Update the learning rate scheduler
  scheduler.step()
```