---
title: 1808.06847v1 Deep Video-Based Performance Cloning
date: 2018-08-07
---

# [Deep Video-Based Performance Cloning](http://arxiv.org/abs/1808.06847v1)

authors: Kfir Aberman, Mingyi Shi, Jing Liao, Dani Lischinski, Baoquan Chen, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1808.06847 "[1808.06847] Deep Video-Based Performance Cloning - arXiv.org"
[2]: https://arxiv.org/pdf/1808.06847v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1802.06847v1 "[1802.06847v1] Distribution Matching in Variational Inference"

Here is a summary of the paper[^1^][1]:

- **What**: The paper presents a new video-based performance cloning technique that can generate videos where a target actor reenacts other performances, using only ordinary video segments as input, without motion capture or depth information.
- **Why**: The paper aims to overcome the limitations of existing methods that require paired data, 3D models, or complex optimization procedures, and to enable realistic and diverse video generation for various applications such as entertainment, education, and social media.
- **How**: The paper proposes a deep generative network with two branches, both of which train the same space-time conditional generator, using shared weights. One branch uses paired data, self-generated from the reference video, to learn the appearance of the target actor in various poses. The other branch uses unpaired data to improve the generation of temporally coherent video renditions of unseen pose sequences. The paper also introduces a novel loss function that balances between appearance and motion fidelity, and a post-processing technique that enhances the realism of the generated videos.

## Main Contributions

According to the paper, the main contributions are:

- A novel video-based performance cloning technique that can generate realistic and diverse videos of a target actor reenacting other performances, using only ordinary video segments as input, without motion capture or depth information.
- A deep generative network with two branches that share the same space-time conditional generator, and a novel training scheme that leverages both paired and unpaired data to learn the appearance and dynamics of the target actor.
- A novel loss function that balances between appearance and motion fidelity, and a post-processing technique that enhances the realism of the generated videos.
- A comprehensive evaluation of the proposed method on various challenging scenarios, and a comparison with existing methods that demonstrate its superiority in terms of visual quality and diversity.

## Method Summary

Here is a summary of the method section of the paper:

- The method consists of three main steps: data preparation, network training, and video generation.
- In the data preparation step, the reference video of the target actor is segmented into frames and cropped to focus on the actor. The driving videos of other performances are also segmented and cropped, and then aligned with the reference video using a pose estimation algorithm. The aligned frames are used to generate paired data, where each pair consists of a reference frame and a driving frame with the same pose. The unpaired data consists of all the driving frames that are not aligned with any reference frame.
- In the network training step, a deep generative network with two branches is trained using both paired and unpaired data. The network consists of a space-time conditional generator that takes as input a reference frame and a driving frame, and outputs a generated frame where the target actor reenacts the driving pose. The generator is based on a U-Net architecture with skip connections and temporal convolutions. The two branches share the same generator, but use different loss functions. The paired branch uses a combination of pixel-wise L1 loss, perceptual loss, and adversarial loss to measure the appearance fidelity of the generated frames. The unpaired branch uses a cycle-consistency loss to measure the motion fidelity of the generated frames. The total loss is a weighted sum of the losses from both branches.
- In the video generation step, a new video segment of the target actor reenacting a given performance is generated by feeding the reference frame and the driving frames to the trained generator. The generated frames are then post-processed using a temporal smoothing technique and a color correction technique to enhance their realism and coherence.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Data preparation
reference_video = load_video(reference_path)
driving_videos = load_videos(driving_paths)
reference_frames = segment_and_crop(reference_video)
driving_frames = segment_and_crop(driving_videos)
aligned_frames = align_poses(reference_frames, driving_frames)
paired_data = generate_pairs(reference_frames, aligned_frames)
unpaired_data = generate_unpairs(reference_frames, driving_frames)

# Network training
generator = SpaceTimeConditionalGenerator()
discriminator = PatchGAN()
paired_loss = PixelLoss() + PerceptualLoss() + AdversarialLoss()
unpaired_loss = CycleConsistencyLoss()
total_loss = lambda_p * paired_loss + lambda_u * unpaired_loss
optimizer = Adam()
for epoch in range(num_epochs):
  for batch in paired_data:
    reference_frame, driving_frame = batch
    generated_frame = generator(reference_frame, driving_frame)
    paired_loss_value = paired_loss(generated_frame, reference_frame, driving_frame, discriminator)
    optimizer.zero_grad()
    paired_loss_value.backward()
    optimizer.step()
  for batch in unpaired_data:
    reference_frame, driving_frame = batch
    generated_frame = generator(reference_frame, driving_frame)
    reconstructed_frame = generator(generated_frame, reference_frame)
    unpaired_loss_value = unpaired_loss(reconstructed_frame, reference_frame)
    optimizer.zero_grad()
    unpaired_loss_value.backward()
    optimizer.step()

# Video generation
new_driving_video = load_video(new_driving_path)
new_driving_frames = segment_and_crop(new_driving_video)
generated_frames = []
for new_driving_frame in new_driving_frames:
  generated_frame = generator(reference_frame, new_driving_frame)
  generated_frames.append(generated_frame)
generated_video = concatenate(generated_frames)
generated_video = smooth_temporally(generated_video)
generated_video = correct_color(generated_video)
save_video(generated_video, output_path)

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import cv2 # for video processing
import numpy as np # for numerical operations
import torch # for deep learning
import torchvision # for computer vision
import torch.nn as nn # for neural network modules
import torch.nn.functional as F # for neural network functions
import torch.optim as optim # for optimization algorithms
from torchvision.models import vgg19 # for perceptual loss
from torchvision.transforms import ToTensor, ToPILImage # for image conversion

# Define constants
IMAGE_SIZE = 256 # the size of the input and output images
NUM_CHANNELS = 3 # the number of channels in the images (RGB)
NUM_FRAMES = 32 # the number of frames in each video segment
NUM_EPOCHS = 100 # the number of epochs for training
BATCH_SIZE = 4 # the batch size for training
LAMBDA_P = 10 # the weight for the paired loss
LAMBDA_U = 1 # the weight for the unpaired loss
LEARNING_RATE = 0.0002 # the learning rate for the optimizer
BETA_1 = 0.5 # the beta_1 parameter for the optimizer
BETA_2 = 0.999 # the beta_2 parameter for the optimizer

# Define helper functions
def load_video(path):
  # Load a video from a given path and return a list of frames as numpy arrays
  video = cv2.VideoCapture(path)
  frames = []
  while video.isOpened():
    ret, frame = video.read()
    if ret:
      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # convert to RGB format
      frames.append(frame)
    else:
      break
  video.release()
  return frames

def load_videos(paths):
  # Load multiple videos from a list of paths and return a list of lists of frames as numpy arrays
  videos = []
  for path in paths:
    video = load_video(path)
    videos.append(video)
  return videos

def segment_and_crop(video):
  # Segment a video into equal-length segments and crop each segment to a fixed size and return a list of segments as numpy arrays
  segments = []
  num_segments = len(video) // NUM_FRAMES # compute the number of segments
  for i in range(num_segments):
    segment = video[i * NUM_FRAMES : (i + 1) * NUM_FRAMES] # extract a segment of frames
    segment = np.stack(segment, axis=0) # stack the frames along the first axis
    segment = center_crop(segment) # crop the segment to a fixed size
    segments.append(segment)
  return segments

def center_crop(segment):
  # Crop a segment of frames to a fixed size by taking the center region and return a cropped segment as a numpy array
  h, w = segment.shape[1:3] # get the height and width of the frames
  x1 = (w - IMAGE_SIZE) // 2 # compute the left coordinate of the crop region
  y1 = (h - IMAGE_SIZE) // 2 # compute the top coordinate of the crop region
  x2 = x1 + IMAGE_SIZE # compute the right coordinate of the crop region
  y2 = y1 + IMAGE_SIZE # compute the bottom coordinate of the crop region
  cropped_segment = segment[:, y1:y2, x1:x2, :] # crop the segment using slicing
  return cropped_segment

def align_poses(reference_frames, driving_frames):
  # Align the poses of two lists of frames using a pose estimation algorithm and return a list of aligned frames as numpy arrays
  aligned_frames = []
  pose_estimator = PoseEstimator() # initialize a pose estimator object (not implemented here)
  reference_poses = pose_estimator.estimate(reference_frames) # estimate the poses of the reference frames
  driving_poses = pose_estimator.estimate(driving_frames) # estimate the poses of the driving frames
  pose_matcher = PoseMatcher() # initialize a pose matcher object (not implemented here)
  matches = pose_matcher.match(reference_poses, driving_poses) # match the poses between reference and driving frames using some similarity metric (e.g., L2 distance)
  for match in matches:
    reference_index, driving_index = match # get the indices of matched frames
    reference_frame = reference_frames[reference_index] # get the reference frame by index
    driving_frame = driving_frames[driving_index] # get the driving frame by index
    aligned_frame = driving_frame.copy() # make a copy of the driving frame to avoid modifying it directly
    aligned_frame[reference_frame == 0] = 0 # mask out the background of the driving frame using the reference frame
    aligned_frames.append(aligned_frame) # append the aligned frame to the list
  return aligned_frames

def generate_pairs(reference_frames, aligned_frames):
  # Generate paired data from two lists of frames and return a list of pairs as numpy arrays
  pairs = []
  for i in range(len(reference_frames)):
    reference_frame = reference_frames[i] # get the reference frame by index
    aligned_frame = aligned_frames[i] # get the aligned frame by index
    pair = np.stack([reference_frame, aligned_frame], axis=0) # stack the frames along the first axis
    pairs.append(pair) # append the pair to the list
  return pairs

def generate_unpairs(reference_frames, driving_frames):
  # Generate unpaired data from two lists of frames and return a list of unpaired frames as numpy arrays
  unpaired_data = []
  for driving_frame in driving_frames:
    if driving_frame not in aligned_frames: # check if the driving frame is not aligned with any reference frame
      unpaired_data.append(driving_frame) # append the driving frame to the list
  return unpaired_data

def to_tensor(segment):
  # Convert a segment of frames from a numpy array to a torch tensor and return a tensor
  segment = segment.astype(np.float32) / 255.0 # normalize the pixel values to [0, 1] range
  segment = segment.transpose(0, 3, 1, 2) # transpose the axes to match the PyTorch format (N, C, H, W)
  segment = torch.from_numpy(segment) # convert from numpy array to torch tensor
  return segment

def to_image(tensor):
  # Convert a tensor of frames from a torch tensor to a PIL image and return an image
  tensor = tensor.detach().cpu() # detach from the computation graph and move to CPU
  tensor = tensor * 255.0 # denormalize the pixel values to [0, 255] range
  tensor = tensor.clamp(0, 255) # clamp the pixel values to [0, 255] range
  tensor = tensor.byte() # convert from float32 to uint8
  tensor = tensor.transpose(0, 2) # transpose the axes to match the PIL format (C, H, W)
  image = ToPILImage()(tensor) # convert from torch tensor to PIL image using torchvision transform
  return image

# Define network modules
class SpaceTimeConditionalGenerator(nn.Module):
  # A space-time conditional generator that takes as input a reference frame and a driving frame and outputs a generated frame where the target actor reenacts the driving pose

  def __init__(self):
    super(SpaceTimeConditionalGenerator, self).__init__()
    self.encoder = Encoder() # an encoder module that encodes both frames into latent features (not implemented here)
    self.decoder = Decoder() # a decoder module that decodes the latent features into a generated frame (not implemented here)

  def forward(self, reference_frame, driving_frame):
    # The forward pass of the generator
    x = torch.cat([reference_frame, driving_frame], dim=1) # concatenate both frames along the channel dimension
    x = self.encoder(x) # encode both frames into latent features using the encoder module
    x = self.decoder(x) # decode the latent features into a generated frame using the decoder module
    return x

class PatchGAN(nn.Module):
  # A PatchGAN discriminator that takes as input an image and outputs a patch-wise prediction of real or fake

  def __init__(self):
    super(PatchGAN, self).__init__()
    self.conv1 = nn.Conv2d(NUM_CHANNELS, 64, kernel_size=4, stride=2, padding=1) # a convolutional layer with kernel size 4, stride 2, padding 1 and output channels 64
    self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1) # a convolutional layer with kernel size 4, stride 2, padding 1 and output channels 128
    self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1) # a convolutional layer with kernel size 4, stride 2, padding 1 and output channels 256
    self.conv4 = nn.Conv2d(256, NUM_CHANNELS * NUM_FRAMES * NUM_FRAMES // (16 * IMAGE_SIZE), kernel_size=4) 
    # a convolutional layer with kernel size IMAGE_SIZE // (NUM_FRAMES * NUM_FRAMES // (16 * IMAGE_SIZE)), stride IMAGE_SIZE // (NUM_FRAMES * NUM_FRAMES // (16 * IMAGE_SIZE)), no padding and output