---
title: 2306.08257v1 On the Robustness of Latent Diffusion Models
date: 2023-06-09
---

# [On the Robustness of Latent Diffusion Models](http://arxiv.org/abs/2306.08257v1)

authors: Jianping Zhang, Zhuoer Xu, Shiwen Cui, Changhua Meng, Weibin Wu, Michael R. Lyu


## What, Why and How

[1]: https://arxiv.org/abs/2306.08257 "[2306.08257] On the Robustness of Latent Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: https://arxiv.org/pdf/2306.08057v1.pdf "arXiv:2306.08057v1 [cs.NE] 25 May 2023"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper is about the robustness of latent diffusion models, which are a type of generative models that can synthesize and edit images by reversing a stochastic diffusion process.
- **Why**: The paper aims to analyze the robustness of latent diffusion models more thoroughly, as previous works only focus on the adversarial attacks against the encoder or the output image under white-box settings, regardless of the denoising process. The paper also proposes two automatic dataset construction pipelines for two kinds of image editing models and releases the whole dataset to facilitate the research of the robustness of latent diffusion models.
- **How**: The paper first studies the influence of the components inside latent diffusion models on their white-box robustness. Then, it evaluates the black-box robustness of latent diffusion models via transfer attacks, where it considers both prompt-transfer and model-transfer settings and possible defense mechanisms. Finally, it conducts extensive experiments on various datasets and tasks to demonstrate the effectiveness and limitations of latent diffusion models under different attack scenarios.


## Main Contributions

[1]: https://arxiv.org/abs/2306.08257 "[2306.08257] On the Robustness of Latent Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: https://arxiv.org/pdf/2306.08057v1.pdf "arXiv:2306.08057v1 [cs.NE] 25 May 2023"

According to the paper at [^1^][1], the main contributions are:

- **A comprehensive analysis of the robustness of latent diffusion models** under both white-box and black-box attack scenarios, considering the effects of different components such as the encoder, the denoiser, and the noise schedule.
- **A novel dataset construction method** for two kinds of image editing models, namely inpainting and super-resolution, which can generate realistic and diverse images with corresponding masks or low-resolution versions.
- **A public dataset release** consisting of 10K images for each editing task, along with the code and pretrained models for latent diffusion models and their variants.


## Method Summary

[1]: https://arxiv.org/abs/2306.08257 "[2306.08257] On the Robustness of Latent Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: https://arxiv.org/pdf/2306.08057v1.pdf "arXiv:2306.08057v1 [cs.NE] 25 May 2023"

Here is a summary of the method section of the paper at [^1^][1]:

- **White-box robustness analysis**: The paper investigates how different components of latent diffusion models affect their white-box robustness, such as the encoder, the denoiser, and the noise schedule. The paper also proposes a novel attack method that exploits the denoising process to generate adversarial examples that can fool latent diffusion models.
- **Black-box robustness analysis**: The paper evaluates the black-box robustness of latent diffusion models via transfer attacks, where the adversarial examples are generated by another model or another prompt. The paper considers two settings: prompt-transfer and model-transfer. The paper also explores some possible defense mechanisms, such as prompt randomization and model ensemble.
- **Dataset construction**: The paper proposes two automatic dataset construction pipelines for two kinds of image editing models, namely inpainting and super-resolution. The pipelines can generate realistic and diverse images with corresponding masks or low-resolution versions by applying various transformations and filters on existing datasets.
- **Experiments**: The paper conducts extensive experiments on various datasets and tasks to demonstrate the effectiveness and limitations of latent diffusion models under different attack scenarios. The paper also compares latent diffusion models with other generative models, such as VAEs and GANs, in terms of image quality and robustness.


## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.08257 "[2306.08257] On the Robustness of Latent Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: https://arxiv.org/pdf/2306.08057v1.pdf "arXiv:2306.08057v1 [cs.NE] 25 May 2023"

Here is a high-level pseudo code for the paper at [^1^][1]:

- **White-box robustness analysis**:
  - Input: a latent diffusion model LDM, an image x, a target label y
  - Output: an adversarial example x' that can fool LDM
  - Steps:
    - Encode x into a latent code z using the encoder of LDM
    - Generate a perturbation δ using a gradient-based method that maximizes the loss of LDM on (z + δ, y)
    - Add δ to z and decode it into x' using the denoiser of LDM
    - Return x'
- **Black-box robustness analysis**:
  - Input: a latent diffusion model LDM, an image x, a target label y, a prompt p or another model M
  - Output: an adversarial example x' that can fool LDM
  - Steps:
    - If prompt-transfer setting:
      - Generate an adversarial example x' using p and x
    - If model-transfer setting:
      - Generate an adversarial example x' using M and x
    - Return x'
- **Dataset construction**:
  - Input: a source dataset D, a task T (inpainting or super-resolution)
  - Output: a target dataset D' with corresponding masks or low-resolution versions
  - Steps:
    - For each image x in D:
      - If inpainting task:
        - Generate a random mask m using various transformations and filters
        - Apply m to x to get a corrupted image x'
        - Add (x', m, x) to D'
      - If super-resolution task:
        - Downsample x to get a low-resolution image x'
        - Add (x', x) to D'
    - Return D'
- **Experiments**:
  - Input: various datasets and tasks, latent diffusion models and their variants, other generative models, different attack and defense methods
  - Output: quantitative and qualitative results on image quality and robustness
  - Steps:
    - For each dataset and task:
      - Train latent diffusion models and their variants using the dataset
      - Train other generative models using the dataset
      - For each attack and defense method:
        - Generate adversarial examples using the attack method on the models
        - Evaluate the robustness of the models using the defense method and some metrics
        - Visualize some examples of adversarial images and their outputs
      - Compare latent diffusion models with other generative models in terms of image quality and robustness using some metrics


## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2306.08257 "[2306.08257] On the Robustness of Latent Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2306.08267 "arXiv:2306.08267v1 [math.RT] 14 Jun 2023"
[3]: https://arxiv.org/pdf/2306.08057v1.pdf "arXiv:2306.08057v1 [cs.NE] 25 May 2023"

Here is a detailed pseudo code to implement the paper at [^1^][1]:

- **White-box robustness analysis**:
  - Input: a latent diffusion model LDM, an image x, a target label y, a learning rate lr, a number of iterations n
  - Output: an adversarial example x' that can fool LDM
  - Steps:
    - Encode x into a latent code z using the encoder of LDM
    - Initialize a perturbation δ as a zero vector with the same shape as z
    - For i from 1 to n:
      - Calculate the loss L of LDM on (z + δ, y) using cross-entropy
      - Calculate the gradient g of L with respect to δ
      - Update δ by adding lr * g
      - Clip δ to a small range (e.g., [-0.01, 0.01]) to avoid overflow
    - Add δ to z and decode it into x' using the denoiser of LDM
    - Return x'
- **Black-box robustness analysis**:
  - Input: a latent diffusion model LDM, an image x, a target label y, a prompt p or another model M
  - Output: an adversarial example x' that can fool LDM
  - Steps:
    - If prompt-transfer setting:
      - Generate an adversarial example x' using p and x by following the steps in [^2^][2]
    - If model-transfer setting:
      - Generate an adversarial example x' using M and x by following the steps in [^3^][3]
    - Return x'
- **Dataset construction**:
  - Input: a source dataset D, a task T (inpainting or super-resolution), a number of images k
  - Output: a target dataset D' with corresponding masks or low-resolution versions
  - Steps:
    - Initialize D' as an empty list
    - For i from 1 to k:
      - Randomly sample an image x from D
      - If inpainting task:
        - Generate a random mask m using various transformations and filters such as rotation, scaling, cropping, blurring, etc.
        - Apply m to x to get a corrupted image x'
        - Add (x', m, x) to D'
      - If super-resolution task:
        - Downsample x by a factor of r (e.g., 4) to get a low-resolution image x'
        - Add (x', x) to D'
    - Return D'
- **Experiments**:
  - Input: various datasets and tasks, latent diffusion models and their variants, other generative models, different attack and defense methods
  - Output: quantitative and qualitative results on image quality and robustness
  - Steps:
    - For each dataset and task:
      - Train latent diffusion models and their variants using the dataset by following the steps in 
      - Train other generative models such as VAEs and GANs using the dataset by following their respective steps
      - For each attack and defense method:
        - Generate adversarial examples using the attack method on the models by following their respective steps
        - Evaluate the robustness of the models using the defense method and some metrics such as accuracy, FID score, SSIM score, etc.
        - Visualize some examples of adversarial images and their outputs using matplotlib or other tools
      - Compare latent diffusion models with other generative models in terms of image quality and robustness using some metrics such as accuracy, FID score, SSIM score, etc. and plot them using matplotlib or other tools