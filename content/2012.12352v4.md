---
title: 2012.12352v4 Seeing past words  Testing the cross-modal capabilities of pretrained V&L models on counting tasks
date: 2020-12-13
---

# [Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks](http://arxiv.org/abs/2012.12352v4)

authors: Letitia Parcalabescu, Albert Gatt, Anette Frank, Iacer Calixto


## What, Why and How

[1]: https://arxiv.org/abs/2012.12352v4 "[2012.12352v4] Seeing past words: Testing the cross-modal capabilities ..."
[2]: https://arxiv.org/pdf/2012.12352v4.pdf "arXiv:2012.12352v4 [cs.CV] 17 Jun 2021"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the paper:

- **What**: The paper investigates the reasoning ability of pretrained vision and language (V&L) models on two tasks that require multimodal integration: (1) discriminating a correct image-sentence pair from an incorrect one, and (2) counting entities in an image[^1^][1].
- **Why**: The paper aims to test the cross-modal capabilities of pretrained V&L models and to understand their reasoning and grounding mechanisms. The paper also wants to identify the limitations and challenges of these models on specific phenomena[^1^][1].
- **How**: The paper evaluates three pretrained V&L models: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. The paper uses foiling as a technique to create incorrect image-sentence pairs by modifying either the image or the sentence. The paper also creates a counting probe dataset by extracting images and sentences from COCO that contain numerical expressions. The paper analyzes the performance of the models on these tasks and discusses the possible sources of errors[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2012.12352v4 "[2012.12352v4] Seeing past words: Testing the cross-modal capabilities ..."
[2]: https://arxiv.org/pdf/2012.12352v4.pdf "arXiv:2012.12352v4 [cs.CV] 17 Jun 2021"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

The paper lists the following contributions[^1^][1]:

- **A novel dataset** for testing the cross-modal capabilities of pretrained V&L models on counting tasks, which requires both visual and linguistic reasoning.
- **A comprehensive evaluation** of three pretrained V&L models on two tasks: image-sentence alignment and counting entities in an image, in both zero-shot and finetuned settings.
- **A detailed analysis** of the performance and errors of the models on these tasks, revealing their limitations and challenges on specific phenomena.

## Method Summary

[1]: https://arxiv.org/abs/2012.12352v4 "[2012.12352v4] Seeing past words: Testing the cross-modal capabilities ..."
[2]: https://arxiv.org/pdf/2012.12352v4.pdf "arXiv:2012.12352v4 [cs.CV] 17 Jun 2021"
[3]: https://cancer.dartmouth.edu/sites/default/files/2019-05/methods-section.pdf "How to Write the Methods Section of a Research Paper"

Here is a summary of the method section of the paper:

- The paper uses **foiling** as a technique to create incorrect image-sentence pairs by modifying either the image or the sentence, such that the replacement results in a mismatch between the two modalities[^1^][2].
- The paper evaluates three pretrained V&L models: **ViLBERT**, **ViLBERT 12-in-1** and **LXMERT**, which are based on the transformer architecture and use different pretraining objectives and datasets[^1^][2].
- The paper uses two tasks to test the cross-modal capabilities of the models: (1) **image-sentence alignment**, where the models have to predict whether an image-sentence pair is correctly aligned or not, and (2) **counting entities in an image**, where the models have to produce a numerical answer based on an image and a sentence that contains a numerical expression[^1^][2].
- The paper uses two datasets for these tasks: (1) **Flickr30k Entities**, which contains images with annotated bounding boxes and captions, and (2) **COCO**, which contains images with captions that are filtered to include numerical expressions. The paper creates foiled pairs for both datasets by modifying either the image or the sentence[^1^][2].
- The paper evaluates the models in both **zero-shot** and **finetuned** settings, where zero-shot means using the pretrained models without any further training, and finetuned means training the models on the foiled pairs for task (1) and on the counting probe dataset for task (2)[^1^][2].
- The paper analyzes the performance of the models on these tasks using different metrics, such as accuracy, precision, recall, F1-score, and mean absolute error. The paper also discusses the possible sources of errors and limitations of the models[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the pretrained V&L models: ViLBERT, ViLBERT 12-in-1 and LXMERT
models = [ViLBERT, ViLBERT_12_in_1, LXMERT]

# Define the tasks: image-sentence alignment and counting entities in an image
tasks = [image_sentence_alignment, counting_entities]

# Define the datasets: Flickr30k Entities and COCO
datasets = [Flickr30k, COCO]

# Define the settings: zero-shot and finetuned
settings = [zero_shot, finetuned]

# For each model, task, dataset and setting combination
for model in models:
  for task in tasks:
    for dataset in datasets:
      for setting in settings:

        # Load the pretrained model
        model.load_pretrained()

        # If finetuned setting, train the model on the foiled pairs for task (1) or the counting probe dataset for task (2)
        if setting == finetuned:
          if task == image_sentence_alignment:
            model.train(foiled_pairs(dataset))
          elif task == counting_entities:
            model.train(counting_probe(dataset))

        # Evaluate the model on the task using the dataset
        model.evaluate(task, dataset)

        # Report the performance metrics
        model.report_metrics()

        # Analyze the errors and limitations of the model
        model.analyze_errors()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the pretrained V&L models: ViLBERT, ViLBERT 12-in-1 and LXMERT
# Use the HuggingFace transformers library to load the models and their corresponding tokenizers
models = {
  "ViLBERT": transformers.AutoModel.from_pretrained("bert-base-uncased"),
  "ViLBERT_12_in_1": transformers.AutoModel.from_pretrained("bert-base-uncased"),
  "LXMERT": transformers.AutoModel.from_pretrained("unc-nlp/lxmert-base-uncased")
}

tokenizers = {
  "ViLBERT": transformers.AutoTokenizer.from_pretrained("bert-base-uncased"),
  "ViLBERT_12_in_1": transformers.AutoTokenizer.from_pretrained("bert-base-uncased"),
  "LXMERT": transformers.AutoTokenizer.from_pretrained("unc-nlp/lxmert-base-uncased")
}

# Define the tasks: image-sentence alignment and counting entities in an image
# Use the torch.nn module to define the task-specific layers for each model
tasks = {
  "image_sentence_alignment": {
    "ViLBERT": torch.nn.Linear(768, 2),
    "ViLBERT_12_in_1": torch.nn.Linear(768, 2),
    "LXMERT": torch.nn.Linear(768, 2)
  },
  "counting_entities": {
    "ViLBERT": torch.nn.Linear(768, 1),
    "ViLBERT_12_in_1": torch.nn.Linear(768, 1),
    "LXMERT": torch.nn.Linear(768, 1)
  }
}

# Define the datasets: Flickr30k Entities and COCO
# Use the torchvision.datasets module to load the images and captions from the datasets
# Use the pandas library to load the foiled pairs and the counting probe data from csv files
datasets = {
  "Flickr30k": {
    "images": torchvision.datasets.Flickr30k(root="data/flickr30k", split="train"),
    "captions": pd.read_csv("data/flickr30k/captions.csv"),
    "foiled_pairs": pd.read_csv("data/flickr30k/foiled_pairs.csv"),
    "counting_probe": pd.read_csv("data/flickr30k/counting_probe.csv")
  },
  "COCO": {
    "images": torchvision.datasets.CocoDetection(root="data/coco", annFile="data/coco/annotations.json"),
    "captions": pd.read_json("data/coco/captions.json"),
    "foiled_pairs": pd.read_csv("data/coco/foiled_pairs.csv"),
    "counting_probe": pd.read_csv("data/coco/counting_probe.csv")
  }
}

# Define the settings: zero-shot and finetuned
settings = ["zero_shot", "finetuned"]

# Define the hyperparameters: batch size, learning rate, number of epochs, device
batch_size = 32
learning_rate = 0.0001
num_epochs = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the loss functions: cross entropy for task (1) and mean absolute error for task (2)
loss_functions = {
  "image_sentence_alignment": torch.nn.CrossEntropyLoss(),
  "counting_entities": torch.nn.L1Loss()
}

# Define the performance metrics: accuracy, precision, recall, F1-score, and mean absolute error
performance_metrics = {
  "accuracy": lambda y_true, y_pred: (y_true == y_pred).float().mean(),
  "precision": lambda y_true, y_pred: (y_true * y_pred).float().sum() / y_pred.float().sum(),
  "recall": lambda y_true, y_pred: (y_true * y_pred).float().sum() / y_true.float().sum(),
  "f1_score": lambda y_true, y_pred: 2 * (y_true * y_pred).float().sum() / (y_true + y_pred).float().sum(),
  "mean_absolute_error": lambda y_true, y_pred: (y_true - y_pred).abs().float().mean()
}

# Define a function to preprocess the images and captions for each model
def preprocess(dataset, model):
  
  # Initialize empty lists to store the processed images and captions
  images = []
  captions = []

  # For each image-caption pair in the dataset
  for image, caption in dataset:

    # Resize and normalize the image using torchvision.transforms
    image = torchvision.transforms.Resize((224, 224))(image)
    image = torchvision.transforms.ToTensor()(image)
    image = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)

    # Tokenize and encode the caption using the corresponding tokenizer
    caption = tokenizers[model].encode(caption, add_special_tokens=True, max_length=128, padding="max_length", truncation=True)

    # Append the processed image and caption to the lists
    images.append(image)
    captions.append(caption)

  # Convert the lists to tensors and return them
  images = torch.stack(images)
  captions = torch.tensor(captions)
  return images, captions

# Define a function to create data loaders for each task and dataset
def create_data_loader(task, dataset):

  # Initialize empty dictionaries to store the data loaders for each setting
  data_loaders = {
    "zero_shot": {},
    "finetuned": {}
  }

  # For each setting
  for setting in settings:

    # If zero-shot setting, use the original images and captions from the dataset
    if setting == "zero_shot":
      images, captions = preprocess(zip(dataset["images"], dataset["captions"]), model)

    # If finetuned setting, use the foiled pairs for task (1) or the counting probe dataset for task (2)
    elif setting == "finetuned":
      if task == "image_sentence_alignment":
        images, captions = preprocess(zip(dataset["foiled_pairs"]["image"], dataset["foiled_pairs"]["caption"]), model)
      elif task == "counting_entities":
        images, captions = preprocess(zip(dataset["counting_probe"]["image"], dataset["counting_probe"]["caption"]), model)

    # Create a tensor dataset from the images and captions
    tensor_dataset = torch.utils.data.TensorDataset(images, captions)

    # Create a data loader from the tensor dataset with the batch size
    data_loader = torch.utils.data.DataLoader(tensor_dataset, batch_size=batch_size)

    # Store the data loader in the dictionary
    data_loaders[setting] = data_loader

  # Return the dictionary of data loaders
  return data_loaders

# Define a function to train a model on a task and a dataset
def train(model, task, dataset):

  # Create a data loader for the finetuned setting
  data_loader = create_data_loader(task, dataset)["finetuned"]

  # Move the model and the task-specific layer to the device
  model.to(device)
  tasks[task][model].to(device)

  # Set the model to training mode
  model.train()

  # Create an optimizer using torch.optim.Adam with the learning rate
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

  # Initialize a variable to store the total loss
  total_loss = 0

  # For each batch of images and captions in the data loader
  for images, captions in data_loader:

    # Move the images and captions to the device
    images = images.to(device)
    captions = captions.to(device)

    # Forward pass: get the output of the model and the task-specific layer for the images and captions
    output = model(images, captions)
    output = tasks[task][model](output)

    # Compute the loss using the corresponding loss function
    if task == "image_sentence_alignment":
      labels = dataset["foiled_pairs"]["label"]
      loss = loss_functions[task](output, labels)
    elif task == "counting_entities":
      labels = dataset["counting_probe"]["answer"]
      loss = loss_functions[task](output, labels)

    # Backward pass: compute the gradients and update the model parameters using the optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Add the loss to the total loss
    total_loss += loss.item()

  # Return the average loss over the number of batches
  return total_loss / len(data_loader)

# Define a function to evaluate a model on a task and a dataset
def evaluate(model, task, dataset):

  # Create a data loader for the zero-shot setting
  data_loader = create_data_loader(task, dataset)["zero_shot"]

  # Move the model and the task-specific layer to the device
  model.to(device)
  tasks[task][model].to(device)

  # Set the model to evaluation mode
  model.eval()

  # Initialize empty lists to store the true labels and predicted labels or answers
  y_true = []
  y_pred = []

  # For each batch of images and captions in the data loader
  for images, captions in data_loader:

    # Move the images and captions to the device
    images = images