---
title: 2303.04068v2 VOCALExplore  Pay-as-You-Go Video Data Exploration and Model Building [Technical Report
date: 2023-03-05
---

# [VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building [Technical Report]](http://arxiv.org/abs/2303.04068v2)

authors: Maureen Daum, Enhao Zhang, Dong He, Stephen Mussmann, Brandon Haynes, Ranjay Krishna, Magdalena Balazinska


## What, Why and How

[1]: https://arxiv.org/pdf/2303.04968v2.pdf "a d, arXiv:2303.04968v2 [eess.IV] 31 May 2023"
[2]: https://arxiv.org/abs/2303.12712 "[2303.12712] Sparks of Artificial General Intelligence: Early ..."
[3]: https://lib-arxiv-014.serverfarm.cornell.edu/abs/2303.04068v2 "[2303.04068v2] VOCALExplore: Pay-as-You-Go Video Data Exploration and ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method for reconstructing cardiac cine MRI images under free-breathing conditions, which reduces motion artifacts and improves image quality. The method is called **MDAMF**, which stands for **Motion-guided Deformable Alignment and Multi-resolution Fusion**.
- **Why**: Cardiac cine MRI is a useful technique for assessing the anatomy and function of the heart, but it requires high imaging speed and motion compensation. Existing methods have not effectively utilized the temporal information to align adjacent frames or integrated the aligned features to correct errors or artifacts. MDAMF aims to address these limitations and achieve better reconstruction performance than previous methods.
- **How**: MDAMF consists of two main components: motion-guided deformable alignment and multi-resolution fusion. The former aligns adjacent frames using second-order bidirectional propagation, which captures complex motion patterns and preserves temporal consistency. The latter fuses the aligned features at multiple resolutions using residual learning and attention mechanisms, which corrects alignment errors or artifacts and enhances image details. MDAMF is trained and evaluated on a dataset of free-breathing cardiac cine MRI images from 100 patients, and it outperforms other advanced methods in terms of PSNR, SSIM, and visual effects.

## Main Contributions

The contributions of this paper are:

- It proposes a novel method for reconstructing cardiac cine MRI images under free-breathing conditions, which reduces motion artifacts and improves image quality.
- It introduces a motion-guided deformable alignment method with second-order bidirectional propagation, which captures complex motion patterns and preserves temporal consistency.
- It develops a multi-resolution fusion method with residual learning and attention mechanisms, which corrects alignment errors or artifacts and enhances image details.
- It demonstrates the superiority of the proposed method over other advanced methods in terms of PSNR, SSIM, and visual effects on a dataset of free-breathing cardiac cine MRI images from 100 patients.

## Method Summary

The method section of the above paper describes the proposed MDAMF method in detail. It consists of four subsections:

- **Problem formulation**: This subsection defines the problem of reconstructing cardiac cine MRI images from undersampled k-space data, and introduces the notation and assumptions used in the paper.
- **Motion-guided deformable alignment**: This subsection presents the motion-guided deformable alignment method, which aligns adjacent frames using second-order bidirectional propagation. It explains the motivation, formulation, and implementation of this method, and shows how it captures complex motion patterns and preserves temporal consistency.
- **Multi-resolution fusion**: This subsection presents the multi-resolution fusion method, which fuses the aligned features at multiple resolutions using residual learning and attention mechanisms. It explains the motivation, formulation, and implementation of this method, and shows how it corrects alignment errors or artifacts and enhances image details.
- **Network architecture and training**: This subsection describes the network architecture and training procedure of the proposed MDAMF method. It specifies the network structure, loss function, optimization algorithm, hyperparameters, and data augmentation techniques used in the paper.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: undersampled k-space data Y
# Output: reconstructed image X

# Define network parameters and hyperparameters
N = number of frames
C = number of channels
H = number of rows
W = number of columns
R = undersampling rate
L = number of resolution levels
alpha = weight for residual learning
beta = weight for attention mechanism
gamma = weight for temporal consistency
eta = learning rate
epochs = number of training epochs
batch_size = size of mini-batch

# Define network structure
MDAMF = a network composed of a feature extractor, a motion-guided deformable alignment module, and a multi-resolution fusion module

# Define loss function
loss = mean squared error between X and ground truth image + gamma * temporal consistency loss

# Define optimizer
optimizer = Adam with learning rate eta

# Define data augmentation techniques
augmentations = random cropping, flipping, rotating, scaling, and shifting

# Train the network
for epoch in range(epochs):
  shuffle the training data
  for batch in range(number of batches):
    # Get a mini-batch of undersampled k-space data and ground truth images
    Y_batch, X_batch = get_batch(batch_size)
    # Apply data augmentations to the mini-batch
    Y_batch, X_batch = apply_augmentations(Y_batch, X_batch)
    # Forward pass: reconstruct the image from the k-space data using MDAMF
    X_hat = MDAMF(Y_batch)
    # Compute the loss
    loss_value = loss(X_hat, X_batch)
    # Backward pass: update the network parameters using gradient descent
    optimizer.step(loss_value)

# Test the network
for test_data in test_set:
  # Get the undersampled k-space data and ground truth image
  Y_test, X_test = test_data
  # Reconstruct the image from the k-space data using MDAMF
  X_hat_test = MDAMF(Y_test)
  # Evaluate the reconstruction performance using PSNR, SSIM, and visual effects
  evaluate(X_hat_test, X_test)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: undersampled k-space data Y
# Output: reconstructed image X

# Import libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms

# Define network parameters and hyperparameters
N = number of frames
C = number of channels
H = number of rows
W = number of columns
R = undersampling rate
L = number of resolution levels
alpha = weight for residual learning
beta = weight for attention mechanism
gamma = weight for temporal consistency
eta = learning rate
epochs = number of training epochs
batch_size = size of mini-batch

# Define network structure

# Feature extractor: a convolutional layer that extracts features from k-space data
class FeatureExtractor(nn.Module):
  def __init__(self):
    super(FeatureExtractor, self).__init__()
    # Initialize the convolutional layer with C input channels and C output channels, 3x3 kernel size, and 1x1 stride
    self.conv = nn.Conv2d(C, C, 3, 1, padding=1)

  def forward(self, Y):
    # Apply the convolutional layer to the k-space data and return the feature map F
    F = self.conv(Y)
    return F

# Motion-guided deformable alignment module: a module that aligns adjacent frames using second-order bidirectional propagation
class MotionGuidedDeformableAlignment(nn.Module):
  def __init__(self):
    super(MotionGuidedDeformableAlignment, self).__init__()
    # Initialize the offset generator with C input channels and 2 output channels, 3x3 kernel size, and 1x1 stride
    self.offset_generator = nn.Conv2d(C, 2, 3, 1, padding=1)
    # Initialize the deformable convolutional layer with C input channels and C output channels, 3x3 kernel size, and 1x1 stride
    self.deform_conv = nn.Conv2d(C, C, 3, 1, padding=1)

  def forward(self, F):
    # Initialize the aligned feature map A with the same shape as F
    A = torch.zeros_like(F)
    # Loop over the frames from t=0 to t=N-1
    for t in range(N):
      # Get the current frame F_t and copy it to A_t
      F_t = F[:, :, t, :, :]
      A_t = F_t.clone()
      # Loop over the resolution levels from l=0 to l=L-1
      for l in range(L):
        # Downsample F_t and A_t by a factor of 2^l using average pooling
        F_t_l = F.avg_pool2d(F_t, 2**l)
        A_t_l = F.avg_pool2d(A_t, 2**l)
        # Generate the offset map O_t_l for the current frame using the offset generator
        O_t_l = self.offset_generator(F_t_l)
        # Loop over the adjacent frames from t-1 to t+1 with a step size of s=+/-1
        for s in [-1, 1]:
          # Get the adjacent frame F_s_l and copy it to A_s_l
          F_s_l = F[:, :, t+s, :, :]
          A_s_l = F_s_l.clone()
          # Generate the offset map O_s_l for the adjacent frame using the offset generator
          O_s_l = self.offset_generator(F_s_l)
          # Compute the second-order offset map O_ss_l by adding O_s_l and O_t_l scaled by s
          O_ss_l = O_s_l + s * O_t_l
          # Apply the deformable convolutional layer to A_s_l using O_ss_l as the offset and add the result to A_t_l
          A_t_l += self.deform_conv(A_s_l, O_ss_l)
        # Upsample A_t_l by a factor of 2^l using nearest neighbor interpolation and assign it to A_t
        A_t = F.interpolate(A_t_l, scale_factor=2**l, mode='nearest')
      # Assign A_t to A[:, :, t, :, :]
      A[:, :, t, :, :] = A_t
    # Return the aligned feature map A 
    return A

# Multi-resolution fusion module: a module that fuses the aligned features at multiple resolutions using residual learning and attention mechanisms 
class MultiResolutionFusion(nn.Module):
  def __init__(self):
    super(MultiResolutionFusion, self).__init__()
    # Initialize the fusion generator with C input channels and C output channels, 3x3 kernel size, and 1x1 stride
    self.fusion_generator = nn.Conv2d(C, C, 3, 1, padding=1)
    # Initialize the attention generator with C input channels and 1 output channel, 3x3 kernel size, and 1x1 stride
    self.attention_generator = nn.Conv2d(C, 1, 3, 1, padding=1)
    # Initialize the reconstruction generator with C input channels and 1 output channel, 3x3 kernel size, and 1x1 stride
    self.reconstruction_generator = nn.Conv2d(C, 1, 3, 1, padding=1)

  def forward(self, A):
    # Initialize the fused feature map G with the same shape as A
    G = torch.zeros_like(A)
    # Loop over the frames from t=0 to t=N-1
    for t in range(N):
      # Get the current frame A_t and copy it to G_t
      A_t = A[:, :, t, :, :]
      G_t = A_t.clone()
      # Loop over the resolution levels from l=0 to l=L-1
      for l in range(L):
        # Downsample A_t and G_t by a factor of 2^l using average pooling
        A_t_l = F.avg_pool2d(A_t, 2**l)
        G_t_l = F.avg_pool2d(G_t, 2**l)
        # Generate the fusion map F_t_l for the current frame using the fusion generator
        F_t_l = self.fusion_generator(A_t_l)
        # Generate the attention map M_t_l for the current frame using the attention generator and apply a sigmoid function
        M_t_l = torch.sigmoid(self.attention_generator(A_t_l))
        # Compute the residual map R_t_l by multiplying F_t_l and M_t_l element-wise and scaling by alpha
        R_t_l = alpha * F_t_l * M_t_l
        # Add R_t_l to G_t_l and assign it to G_t_l
        G_t_l += R_t_l
        # Upsample G_t_l by a factor of 2^l using nearest neighbor interpolation and assign it to G_t
        G_t = F.interpolate(G_t_l, scale_factor=2**l, mode='nearest')
      # Assign G_t to G[:, :, t, :, :]
      G[:, :, t, :, :] = G_t
    # Generate the reconstructed image X by applying the reconstruction generator to G and scaling by beta
    X = beta * self.reconstruction_generator(G)
    # Return the reconstructed image X 
    return X

# MDAMF: a network composed of a feature extractor, a motion-guided deformable alignment module, and a multi-resolution fusion module
class MDAMF(nn.Module):
  def __init__(self):
    super(MDAMF, self).__init__()
    # Initialize the feature extractor
    self.feature_extractor = FeatureExtractor()
    # Initialize the motion-guided deformable alignment module
    self.motion_guided_deformable_alignment = MotionGuidedDeformableAlignment()
    # Initialize the multi-resolution fusion module
    self.multi_resolution_fusion = MultiResolutionFusion()

  def forward(self, Y):
    # Extract features from k-space data using the feature extractor
    F = self.feature_extractor(Y)
    # Align adjacent frames using the motion-guided deformable alignment module
    A = self.motion_guided_deformable_alignment(F)
    # Fuse aligned features at multiple resolutions using the multi-resolution fusion module
    X = self.multi_resolution_fusion(A)
    # Return the reconstructed image X 
    return X

# Define loss function

# Temporal consistency loss: a function that measures the smoothness of temporal changes in the reconstructed image 
def temporal_consistency_loss(X):
  # Initialize the loss value with zero
  loss_value = 0.0
  # Loop over the frames from t=0 to t=N-2
  for t in range(N-1):
    # Get the current frame X_t and the next frame X_(t+1)
    X_t = X[:, :, t]
    X_tp1 = X[:, :, t+1]
    # Compute the absolute difference between X_t and X_(t+1) and square it element-wise
    diff = torch.pow(torch.abs(X_tp1 - X_tp1), 2)
    # Sum up all the elements in diff and add it to loss_value
    loss_value += torch.sum(diff)
  # Return the loss value 
  return loss_value

# Mean squared error: a function that measures the average squared difference between the reconstructed image and the ground truth image 
def mean_squared_error(X