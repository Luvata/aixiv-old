---
title: 2306.00402v1 Discriminative Deep Feature Visualization for Explainable Face Recognition
date: 2023-06-01
---

# [Discriminative Deep Feature Visualization for Explainable Face Recognition](http://arxiv.org/abs/2306.00402v1)

authors: Zewei Xu, Yuhang Lu, Touradj Ebrahimi


## What, Why and How

[1]: https://arxiv.org/abs/2306.00402 "[2306.00402] Discriminative Deep Feature Visualization for Explainable ..."
[2]: https://arxiv.org/pdf/2306.00402v1.pdf "Discriminative Deep Feature Visualization for Explainable ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.00402v1 "[2305.00402v1] Control Variate Sliced Wasserstein Estimators"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- What: The paper proposes a novel method for **explainable face recognition** (XFR) that visualizes the discriminative deep features and the salient regions between input faces.
- Why: The paper aims to address the problem of **lack of explainability** for the predictions of deep convolutional neural networks (DCNNs) in face recognition (FR) tasks, which can have serious consequences in safety and security critical domains.
- How: The paper introduces a **face reconstruction-based explanation module** that reveals the correspondence between the deep feature and the facial regions, and a **visual saliency explanation algorithm** that produces visual saliency maps that represent similar and dissimilar regions between input faces. The paper also presents a detailed analysis of the generated visual explanation to show the effectiveness of the proposed method.

## Main Contributions

[1]: https://arxiv.org/abs/2306.00402 "[2306.00402] Discriminative Deep Feature Visualization for Explainable ..."
[2]: https://arxiv.org/pdf/2306.00402v1.pdf "Discriminative Deep Feature Visualization for Explainable ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.00402v1 "[2305.00402v1] Control Variate Sliced Wasserstein Estimators"

The paper claims the following contributions[^1^][1]:

- A **face reconstruction-based explanation module** that can reconstruct the input facial image from the deep feature and highlight the facial regions that are related to the feature.
- A **visual saliency explanation algorithm** that can generate visual saliency maps to show the similar and dissimilar regions between input faces based on the deep feature distance.
- A **detailed analysis** of the proposed method on various face recognition tasks, such as verification, identification, and attribute prediction, to demonstrate its effectiveness and robustness.

## Method Summary

[1]: https://arxiv.org/abs/2306.00402 "[2306.00402] Discriminative Deep Feature Visualization for Explainable ..."
[2]: https://arxiv.org/pdf/2306.00402v1.pdf "Discriminative Deep Feature Visualization for Explainable ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.00402v1 "[2305.00402v1] Control Variate Sliced Wasserstein Estimators"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper adopts a **pre-trained deep face recognition model** as the backbone network to extract deep features from input facial images.
- The paper proposes a **face reconstruction-based explanation module** that consists of a decoder network and a feature selector. The decoder network can reconstruct the input facial image from the deep feature, and the feature selector can select a subset of the deep feature that corresponds to a specific facial region. The paper also introduces a **feature-region correspondence loss** to train the explanation module and ensure that the selected feature can faithfully represent the facial region.
- The paper introduces a **visual saliency explanation algorithm** that takes two input faces and their deep features as inputs and generates visual saliency maps that highlight the similar and dissimilar regions between them. The algorithm first computes the **deep feature distance** between the two faces, which reflects their similarity in the feature space. Then, it applies the **face reconstruction-based explanation module** to each face and obtains the reconstructed faces and the selected features for each facial region. Next, it computes the **region-wise feature distance** between the two faces for each region, which reflects their similarity in each region. Finally, it normalizes the region-wise feature distance by the deep feature distance and produces visual saliency maps that show which regions are more or less similar between the two faces.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: two facial images x1 and x2
# Output: visual saliency maps s1 and s2

# Load the pre-trained deep face recognition model f
f = load_model()

# Extract the deep features z1 and z2 from x1 and x2
z1 = f(x1)
z2 = f(x2)

# Compute the deep feature distance d between z1 and z2
d = distance(z1, z2)

# Load the face reconstruction-based explanation module g
g = load_module()

# Initialize the visual saliency maps s1 and s2 as zero matrices
s1 = zeros(x1.shape)
s2 = zeros(x2.shape)

# For each facial region r in {left eye, right eye, nose, mouth, etc.}
for r in regions:

  # Select the subset of deep features z1_r and z2_r that correspond to r
  z1_r = g.select(z1, r)
  z2_r = g.select(z2, r)

  # Compute the region-wise feature distance d_r between z1_r and z2_r
  d_r = distance(z1_r, z2_r)

  # Normalize the region-wise feature distance by the deep feature distance
  s_r = d_r / d

  # Assign the normalized region-wise feature distance to the corresponding region in s1 and s2
  s1[r] = s_r
  s2[r] = s_r

# Return the visual saliency maps s1 and s2
return s1, s2
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the hyperparameters
num_regions = 8 # the number of facial regions
feature_dim = 512 # the dimension of the deep feature
region_dim = 64 # the dimension of the region-wise feature
decoder_dim = 256 # the dimension of the decoder network
lr = 0.001 # the learning rate
epochs = 100 # the number of training epochs
batch_size = 32 # the batch size

# Load the pre-trained deep face recognition model f
f = torchvision.models.resnet18(pretrained=True)
f.fc = torch.nn.Linear(f.fc.in_features, feature_dim) # replace the last layer with a linear layer
f.eval() # set the model to evaluation mode

# Define the face reconstruction-based explanation module g
class ExplanationModule(torch.nn.Module):

  def __init__(self):
    super(ExplanationModule, self).__init__()

    # Define the decoder network that can reconstruct the input image from the deep feature
    self.decoder = torch.nn.Sequential(
      torch.nn.Linear(feature_dim, decoder_dim),
      torch.nn.ReLU(),
      torch.nn.Linear(decoder_dim, 64 * 64 * 3), # assume the input image size is 64 x 64 x 3
      torch.nn.Sigmoid()
    )

    # Define the feature selector that can select a subset of deep features for each region
    self.selector = torch.nn.Parameter(torch.randn(num_regions, feature_dim)) # initialize as a random matrix

  def forward(self, z):
    # Input: a batch of deep features z of shape (batch_size, feature_dim)
    # Output: a batch of reconstructed images x_hat of shape (batch_size, 64, 64, 3) and a list of selected features z_r for each region r

    # Reconstruct the input image from the deep feature using the decoder network
    x_hat = self.decoder(z) # shape: (batch_size, 64 * 64 * 3)
    x_hat = x_hat.view(-1, 3, 64, 64) # reshape to (batch_size, 3, 64, 64)

    # Initialize an empty list to store the selected features for each region
    z_r_list = []

    # For each region r
    for r in range(num_regions):

      # Select a subset of deep features for region r using the feature selector
      z_r = torch.matmul(z, self.selector[r]) # shape: (batch_size, region_dim)

      # Append z_r to the list
      z_r_list.append(z_r)

    # Return the reconstructed image and the list of selected features
    return x_hat, z_r_list

# Instantiate the explanation module g
g = ExplanationModule()

# Define the feature-region correspondence loss function L_fr
def L_fr(x, x_hat, z_r_list):
  # Input: a batch of input images x of shape (batch_size, 3, 64, 64), a batch of reconstructed images x_hat of shape (batch_size, 3, 64, 64), and a list of selected features z_r_list for each region r
  # Output: a scalar loss value

  # Compute the reconstruction loss between x and x_hat using mean squared error (MSE)
  L_rec = torch.nn.functional.mse_loss(x_hat, x)

  # Initialize an empty list to store the region-wise reconstruction losses
  L_reg_list = []

  # For each region r
  for r in range(num_regions):

    # Get the selected feature z_r for region r from the list
    z_r = z_r_list[r] # shape: (batch_size, region_dim)

    # Reconstruct the region r from z_r using the decoder network
    x_r_hat = g.decoder(z_r) # shape: (batch_size, 64 * 64 * 3)
    x_r_hat = x_r_hat.view(-1, 3, 64, 64) # reshape to (batch_size, 3, 64, 64)

    # Compute the region-wise reconstruction loss between x and x_r_hat using MSE and append it to the list
    L_reg_list.append(torch.nn.functional.mse_loss(x[:, :, r * (64 // num_regions):(r + 1) * (64 // num_regions), :], x_r_hat[:, :, r * (64 // num_regions):(r + 1) * (64 // num_regions), :]))

  
```