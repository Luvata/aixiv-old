---
title: 2106.09685v2 LoRA  Low-Rank Adaptation of Large Language Models
date: 2021-06-10
---

# [LoRA: Low-Rank Adaptation of Large Language Models](http://arxiv.org/abs/2106.09685v2)

authors: Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen


## What, Why and How

[1]: https://arxiv.org/pdf/2106.09685v2.pdf "arXiv:2106.09685v2 [cs.CL] 16 Oct 2021"
[2]: https://arxiv.org/abs/2106.09685 "[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2202.09685v2 "[2202.09685v2] Scalable Fine-Grained Parallel Cycle Enumeration Algorithms"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes **LoRA**, a method for **Low-Rank Adaptation** of large language models, such as GPT-3, RoBERTa, and DeBERTa, to downstream tasks or domains.
- **Why**: The paper aims to address the challenge of **operational efficiency** when deploying large-scale pre-trained language models that require full fine-tuning for adaptation, which is costly in terms of storage, memory, and latency.
- **How**: The paper introduces a **reparametrization technique** that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. The paper also provides an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA.

## Main Contributions

[1]: https://arxiv.org/pdf/2106.09685v2.pdf "arXiv:2106.09685v2 [cs.CL] 16 Oct 2021"
[2]: https://arxiv.org/abs/2106.09685 "[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2202.09685v2 "[2202.09685v2] Scalable Fine-Grained Parallel Cycle Enumeration Algorithms"

According to the paper at [^1^][1], the main contributions are:

- **LoRA**, a novel method for low-rank adaptation of large language models that reduces the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times compared to full fine-tuning.
- **Empirical evidence** that LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and no additional inference latency.
- **An investigation** into rank-deficiency in language model adaptation, which reveals that most of the adaptation can be captured by low-rank matrices and provides insights into the design of LoRA.
- **A package** that facilitates the integration of LoRA with PyTorch models and provides implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2.

## Method Summary

[1]: https://arxiv.org/pdf/2106.09685v2.pdf "arXiv:2106.09685v2 [cs.CL] 16 Oct 2021"
[2]: https://arxiv.org/abs/2106.09685 "[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2202.09685v2 "[2202.09685v2] Scalable Fine-Grained Parallel Cycle Enumeration Algorithms"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces a **reparametrization technique** that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, as shown in Figure 1 of the paper.
- The paper defines the **rank decomposition matrices** as A = UΣ and B = V T , where U and V are orthogonal matrices and Σ is a diagonal matrix. The paper also defines the **rank** of the decomposition as r = rank(Σ).
- The paper shows how to apply the reparametrization technique to different types of layers in the Transformer architecture, such as self-attention, cross-attention, feed-forward network, and layer normalization.
- The paper describes how to **train** the rank decomposition matrices using gradient descent and how to **initialize** them using random orthogonal matrices or singular value decomposition (SVD).
- The paper discusses how to choose the **optimal rank** for each layer based on empirical results and theoretical analysis. The paper also provides a heuristic formula for estimating the optimal rank based on the model size and the task size.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load a pre-trained language model
model = load_pretrained_model()

# Freeze the pre-trained model weights
model.freeze_weights()

# For each layer in the model
for layer in model.layers:
  # Inject rank decomposition matrices A and B into the layer
  layer.inject_rank_decomposition_matrices()
  # Initialize A and B using random orthogonal matrices or SVD
  layer.initialize_rank_decomposition_matrices()
  # Choose the optimal rank for the layer based on empirical results or heuristic formula
  layer.choose_optimal_rank()

# Train the rank decomposition matrices using gradient descent on a downstream task
model.train_rank_decomposition_matrices(task)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import transformers

# Define a function to generate a random orthogonal matrix
def random_orthogonal_matrix(size):
  # Generate a random matrix
  matrix = torch.randn(size)
  # Apply QR decomposition
  Q, R = torch.qr(matrix)
  # Return the Q matrix
  return Q

# Define a function to perform singular value decomposition (SVD)
def svd(matrix):
  # Apply SVD
  U, S, V = torch.svd(matrix)
  # Return the U, S, and V matrices
  return U, S, V

# Define a function to inject rank decomposition matrices into a layer
def inject_rank_decomposition_matrices(layer):
  # Get the original weight matrix of the layer
  W = layer.weight
  # Get the shape of the weight matrix
  H, D = W.shape
  # Choose a rank for the decomposition
  r = choose_optimal_rank(H, D)
  # Create two new parameters A and B for the layer
  layer.A = torch.nn.Parameter(torch.empty(H, r))
  layer.B = torch.nn.Parameter(torch.empty(r, D))
  # Replace the original weight matrix with A * B
  layer.weight = torch.matmul(layer.A, layer.B)

# Define a function to initialize rank decomposition matrices using random orthogonal matrices or SVD
def initialize_rank_decomposition_matrices(layer, method="random"):
  # Get the original weight matrix of the layer
  W = layer.weight
  # Get the shape of the weight matrix
  H, D = W.shape
  # Get the rank of the decomposition
  r = layer.A.shape[1]
  # If the method is random
  if method == "random":
    # Initialize A and B using random orthogonal matrices
    layer.A.data = random_orthogonal_matrix((H, r))
    layer.B.data = random_orthogonal_matrix((r, D))
  # If the method is SVD
  elif method == "svd":
    # Initialize A and B using SVD
    U, S, V = svd(W)
    layer.A.data = U[:, :r] * torch.sqrt(S[:r])
    layer.B.data = torch.sqrt(S[:r]) * V[:, :r].T

# Define a function to choose the optimal rank for a layer based on empirical results or heuristic formula
def choose_optimal_rank(H, D):
  # If there are empirical results for the layer type and size
  if empirical_results_exist(H, D):
    # Return the optimal rank from the empirical results
    return get_optimal_rank_from_empirical_results(H, D)
  # Else
  else:
    # Return the optimal rank from the heuristic formula
    return get_optimal_rank_from_heuristic_formula(H, D)

# Define a function to get the optimal rank from the heuristic formula
def get_optimal_rank_from_heuristic_formula(H, D):
  # Define some constants for the formula
  alpha = ...
  beta = ...
  gamma = ...
  delta = ...
  epsilon = ...
  zeta = ...
  
  # Compute the model size in billions of parameters (B)
  B = model.num_parameters() / (10 ** 9)
  
  # Compute the task size in millions of examples (M)
  M = task.num_examples() / (10 ** 6)
  
  # Compute the optimal rank using the formula (see Equation (5) in Appendix C.1 of the paper)
  
```