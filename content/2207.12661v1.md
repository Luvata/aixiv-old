---
title: 2207.12661v1 Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training
date: 2022-07-13
---

# [Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training](http://arxiv.org/abs/2207.12661v1)

authors: Haoxuan You, Luowei Zhou, Bin Xiao, Noel Codella, Yu Cheng, Ruochen Xu, Shih-Fu Chang, Lu Yuan


## What, Why and How

[1]: https://arxiv.org/abs/2207.12661v1 "[2207.12661v1] Learning Visual Representation from Modality-Shared ..."
[2]: https://arxiv.org/pdf/2207.12661v1 "Learning Visual Representation from Modality-Shared Contrastive ..."
[3]: https://arxiv.org/pdf/2204.12661v1.pdf "arXiv:2204.12661v1 [quant-ph] 27 Apr 2022"
[4]: http://export.arxiv.org/abs/2207.12661 "[2207.12661] Learning Visual Representation from Modality ... - arXiv"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a Modality-Shared Contrastive Language-Image Pre-training (MS-CLIP) framework, where parameters in vision encoder and text encoder can be shared across modalities during contrastive pre-training.
- **Why**: The paper aims to improve the performance and efficiency of contrastive language-image pre-training, which has shown great utility to learn transferable features for a range of downstream tasks by mapping multiple modalities into a shared embedding space.
- **How**: The paper investigates how many parameters of a transformer model can be shared across modalities, and examines different architectural design choices that position the proportion of parameters shared along a spectrum. The paper also introduces light-weight modality-specific parallel modules to enhance the modality-shared encoder. The paper evaluates the proposed MS-CLIP approach on zero-shot ImageNet classification and linear probing on 24 downstream vision tasks, and compares it with vanilla CLIP and other baselines. The paper also analyzes the semantic alignment and attention patterns of the learned embeddings.

## Main Contributions

[1]: https://arxiv.org/abs/2207.12661v1 "[2207.12661v1] Learning Visual Representation from Modality-Shared ..."
[2]: https://arxiv.org/pdf/2207.12661v1 "Learning Visual Representation from Modality-Shared Contrastive ..."
[3]: https://arxiv.org/pdf/2204.12661v1.pdf "arXiv:2204.12661v1 [quant-ph] 27 Apr 2022"
[4]: http://export.arxiv.org/abs/2207.12661 "[2207.12661] Learning Visual Representation from Modality ... - arXiv"

According to the paper at [^1^][1], the main contributions are:

- The paper proposes a novel Modality-Shared Contrastive Language-Image Pre-training (MS-CLIP) framework, where parameters in vision encoder and text encoder can be shared across modalities during contrastive pre-training.
- The paper investigates how many parameters of a transformer model can be shared across modalities, and examines different architectural design choices that position the proportion of parameters shared along a spectrum.
- The paper introduces light-weight modality-specific parallel modules to enhance the modality-shared encoder, and shows that they improve the performance and efficiency of MS-CLIP.
- The paper evaluates the proposed MS-CLIP approach on zero-shot ImageNet classification and linear probing on 24 downstream vision tasks, and compares it with vanilla CLIP and other baselines. The paper shows that MS-CLIP outperforms vanilla CLIP by up to 13% relative in zero-shot ImageNet classification (pre-trained on YFCC-100M), while simultaneously supporting a reduction of parameters. In addition, MS-CLIP outperforms vanilla CLIP by 1.6 points in linear probing on a collection of 24 downstream vision tasks.
- The paper analyzes the semantic alignment and attention patterns of the learned embeddings, and discovers that sharing parameters leads to semantic concepts from different modalities being encoded more closely in the embedding space, facilitating the transferring of common semantic structure (e.g., attention patterns) from language to vision.

## Method Summary

[1]: https://arxiv.org/abs/2207.12661v1 "[2207.12661v1] Learning Visual Representation from Modality-Shared ..."
[2]: https://arxiv.org/pdf/2207.12661v1 "Learning Visual Representation from Modality-Shared Contrastive ..."
[3]: https://arxiv.org/pdf/2204.12661v1.pdf "arXiv:2204.12661v1 [quant-ph] 27 Apr 2022"
[4]: http://export.arxiv.org/abs/2207.12661 "[2207.12661] Learning Visual Representation from Modality ... - arXiv"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper adopts the contrastive learning framework of CLIP [47], where image-caption pairs are sampled from a large-scale dataset (YFCC-100M [49]) and encoded into a shared embedding space by a vision encoder and a text encoder. The paper uses a temperature-scaled cosine similarity as the contrastive loss function, and optimizes the model with AdamW [28] optimizer and linear learning rate decay.
- The paper explores different ways of sharing parameters between the vision encoder and the text encoder, which are both based on transformers [55]. The paper considers four variants of MS-CLIP: (i) MS-CLIP-Separate, where no parameters are shared; (ii) MS-CLIP-Shared, where all parameters except the input embeddings are shared; (iii) MS-CLIP-Mixed, where only the feed-forward network (FFN) parameters are shared; and (iv) MS-CLIP-Hybrid, where only the attention parameters are shared.
- The paper also introduces modality-specific parallel modules to enhance the modality-shared encoder. These modules are light-weight sub-networks that run in parallel with the shared encoder and process each modality separately. The paper considers two types of parallel modules: (i) Parallel FFN, which adds an extra FFN layer after each shared layer; and (ii) Parallel Attention, which adds an extra attention layer after each shared layer. The paper combines these parallel modules with different MS-CLIP variants to form new models.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the vision encoder and the text encoder as transformers
vision_encoder = Transformer(num_layers, num_heads, dim, dropout)
text_encoder = Transformer(num_layers, num_heads, dim, dropout)

# Define the modality-specific parallel modules as FFN or attention layers
parallel_FFN = FFN(dim, dropout)
parallel_attention = Attention(num_heads, dim, dropout)

# Define the contrastive loss function as temperature-scaled cosine similarity
def contrastive_loss(image_embeddings, text_embeddings, temperature):
  logits = image_embeddings @ text_embeddings.T / temperature
  labels = torch.eye(logits.size(0)).to(logits.device)
  loss = cross_entropy(logits, labels)
  return loss

# Define the MS-CLIP model with different parameter sharing schemes
def MS_CLIP(image, text, mode):
  # mode can be one of "Separate", "Shared", "Mixed", or "Hybrid"
  if mode == "Separate":
    # No parameters are shared
    image_embeddings = vision_encoder(image)
    text_embeddings = text_encoder(text)
  elif mode == "Shared":
    # All parameters except the input embeddings are shared
    image_embeddings = vision_encoder(image_embed(image))
    text_embeddings = vision_encoder(text_embed(text))
  elif mode == "Mixed":
    # Only the FFN parameters are shared
    image_embeddings = vision_encoder(image, share_FFN=True)
    text_embeddings = text_encoder(text, share_FFN=True)
  elif mode == "Hybrid":
    # Only the attention parameters are shared
    image_embeddings = vision_encoder(image, share_attention=True)
    text_embeddings = text_encoder(text, share_attention=True)

  # Optionally add parallel modules to enhance the modality-shared encoder
  image_embeddings += parallel_FFN(image_embeddings) # or parallel_attention(image_embeddings)
  text_embeddings += parallel_FFN(text_embeddings) # or parallel_attention(text_embeddings)

  # Normalize the embeddings to unit length
  image_embeddings = normalize(image_embeddings)
  text_embeddings = normalize(text_embeddings)

  # Return the embeddings and the contrastive loss
  loss = contrastive_loss(image_embeddings, text_embeddings, temperature)
  return image_embeddings, text_embeddings, loss

# Sample image-caption pairs from a large-scale dataset
dataset = YFCC-100M()
image, text = dataset.sample()

# Train the MS-CLIP model with AdamW optimizer and linear learning rate decay
optimizer = AdamW(model.parameters(), lr=init_lr, weight_decay=weight_decay)
scheduler = LinearDecayScheduler(optimizer, total_steps, final_lr)
for step in range(total_steps):
  optimizer.zero_grad()
  image_embeddings, text_embeddings, loss = MS_CLIP(image, text, mode)
  loss.backward()
  optimizer.step()
  scheduler.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
import random

# Define some hyperparameters
num_layers = 12 # number of transformer layers
num_heads = 12 # number of attention heads
dim = 768 # hidden dimension
dropout = 0.1 # dropout rate
temperature = 0.07 # temperature for contrastive loss
init_lr = 5e-4 # initial learning rate
final_lr = 1e-5 # final learning rate
weight_decay = 0.2 # weight decay for optimizer
batch_size = 256 # batch size for training
total_steps = 400000 # total number of training steps

# Define the transformer layer as a class
class TransformerLayer(nn.Module):
  def __init__(self, dim, num_heads, dropout):
    super().__init__()
    self.attention = nn.MultiheadAttention(dim, num_heads, dropout=dropout)
    self.norm1 = nn.LayerNorm(dim)
    self.dropout1 = nn.Dropout(dropout)
    self.ffn = nn.Sequential(
      nn.Linear(dim, dim * 4),
      nn.GELU(),
      nn.Linear(dim * 4, dim),
    )
    self.norm2 = nn.LayerNorm(dim)
    self.dropout2 = nn.Dropout(dropout)

  def forward(self, x, share_FFN=False, share_attention=False):
    # x: (seq_len, batch_size, dim)
    if share_FFN:
      # Use the shared FFN parameters from the first modality
      ffn = self.ffn[0].weight.data.clone()
      self.ffn[0].weight.data.copy_(ffn.t())
      self.ffn[2].weight.data.copy_(ffn)
    if share_attention:
      # Use the shared attention parameters from the first modality
      qkv = self.attention.in_proj_weight.data.clone()
      self.attention.in_proj_weight.data.copy_(qkv.t())
      out = self.attention.out_proj.weight.data.clone()
      self.attention.out_proj.weight.data.copy_(out.t())
    # Apply self-attention and residual connection
    att_output, _ = self.attention(x, x, x)
    x = x + self.dropout1(att_output)
    x = self.norm1(x)
    # Apply feed-forward network and residual connection
    ffn_output = self.ffn(x)
    x = x + self.dropout2(ffn_output)
    x = self.norm2(x)
    return x

# Define the transformer encoder as a class
class TransformerEncoder(nn.Module):
  def __init__(self, num_layers, num_heads, dim, dropout):
    super().__init__()
    self.layers = nn.ModuleList([TransformerLayer(dim, num_heads, dropout) for _ in range(num_layers)])

  def forward(self, x, share_FFN=False, share_attention=False):
    # x: (seq_len, batch_size, dim)
    for layer in self.layers:
      x = layer(x, share_FFN=share_FFN, share_attention=share_attention)
    return x

# Define the vision encoder as a class
class VisionEncoder(nn.Module):
  def __init__(self, num_layers, num_heads, dim, dropout):
    super().__init__()
    self.patch_embed = nn.Conv2d(3, dim, kernel_size=16, stride=16) # patch embedding layer
    self.pos_embed = nn.Parameter(torch.randn(1, 196 + 1, dim)) # positional embedding for patches and [CLS] token
    self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) # learnable [CLS] token for image representation
    self.transformer = TransformerEncoder(num_layers, num_heads, dim, dropout) # transformer encoder

  def forward(self, image):
    # image: (batch_size, 3, 256, 256)
    batch_size = image.size(0)
    patches = self.patch_embed(image) # (batch_size, dim, 16, 16)
    patches = patches.flatten(2).transpose(1, 2) # (batch_size, 196, dim)
    cls_token = self.cls_token.expand(batch_size, -1 , -1) # (batch_size ,1 ,dim)
    x = torch.cat([cls_token ,patches], dim=1) # (batch_size ,197 ,dim)
    x += self.pos_embed # add positional embedding
    x = x.transpose(0, 1) # (197, batch_size, dim)
    x = self.transformer(x) # apply transformer encoder
    x = x.transpose(0, 1) # (batch_size, 197, dim)
    image_embeddings = x[:, 0] # (batch_size, dim)
    return image_embeddings

# Define the text encoder as a class
class TextEncoder(nn.Module):
  def __init__(self, num_layers, num_heads, dim, dropout, vocab_size):
    super().__init__()
    self.token_embed = nn.Embedding(vocab_size, dim) # token embedding layer
    self.pos_embed = nn.Parameter(torch.randn(1, 76 + 1, dim)) # positional embedding for tokens and [CLS] token
    self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) # learnable [CLS] token for text representation
    self.transformer = TransformerEncoder(num_layers, num_heads, dim, dropout) # transformer encoder

  def forward(self, text):
    # text: (batch_size, 76)
    batch_size = text.size(0)
    tokens = self.token_embed(text) # (batch_size, 76, dim)
    cls_token = self.cls_token.expand(batch_size, -1 , -1) # (batch_size ,1 ,dim)
    x = torch.cat([cls_token ,tokens], dim=1) # (batch_size ,77 ,dim)
    x += self.pos_embed # add positional embedding
    x = x.transpose(0, 1) # (77, batch_size, dim)
    x = self.transformer(x) # apply transformer encoder
    x = x.transpose(0, 1) # (batch_size, 77, dim)
    text_embeddings = x[:, 0] # (batch_size, dim)
    return text_embeddings

# Define the parallel FFN module as a class
class ParallelFFN(nn.Module):
  def __init__(self, dim, dropout):
    super().__init__()
    self.ffn = nn.Sequential(
      nn.Linear(dim, dim * 4),
      nn.GELU(),
      nn.Linear(dim * 4, dim),
      nn.Dropout(dropout),
    )

  def forward(self, x):
    # x: (batch_size, dim)
    return self.ffn(x)

# Define the parallel attention module as a class
class ParallelAttention(nn.Module):
  def __init__(self, num_heads, dim, dropout):
    super().__init__()
    self.attention = nn.MultiheadAttention(dim, num_heads, dropout=dropout)

  def forward(self, x):
    # x: (batch_size ,dim)
    x = x.unsqueeze(0) # (1 ,batch_size ,dim)
    att_output ,_ = self.attention(x ,x ,x) # (1 ,batch_size ,dim)
    att_output = att_output.squeeze(0) # (batch_size ,dim)
    return att_output

# Define the contrastive loss function as temperature-scaled cosine similarity
def contrastive_loss(image_embeddings ,text_embeddings ,temperature):
  logits = image_embeddings @ text_embeddings.T / temperature # (batch_size ,batch_size)
  labels = torch.eye(logits.size(0)).to(logits.device) # (batch_size ,batch_size)
  loss = F.cross_entropy(logits ,labels) # scalar
  return loss

# Define the MS-CLIP model with different parameter sharing schemes
def MS_CLIP(image ,text ,mode):
  # mode can be one of "Separate" ,"Shared" ,"Mixed" ,"Hybrid"
  if mode == "Separate":
    # No parameters are shared
    image_embeddings = vision_encoder(image)
    text_embeddings = text_encoder(text)
  elif mode == "Shared":
    # All parameters except the input embeddings are shared
    image_embeddings = vision_encoder(image_embed(image))
    text_embeddings = vision_encoder(text_embed(text))
  elif mode == "Mixed":
    # Only the FFN parameters are shared
    image_embeddings = vision_encoder(image ,share_FFN=True)
    text_embeddings = text_encoder(text ,share_FFN=True)
  elif mode == "Hybrid":
    # Only the attention parameters are shared
    image_embeddings = vision_encoder(image ,share_attention=True)
    text_embeddings = text_encoder(text ,share_attention=True)

  # Optionally add parallel modules to enhance the modality-shared encoder
  image_embeddings += parallel_FFN(image_embeddings) # or parallel_attention(image_embeddings)
  text_embeddings += parallel_FFN(text_embeddings) # or parallel_attention(text_embeddings)

  # Normalize the embeddings to unit length
  image_embeddings = F.normalize(image_embeddings ,p=2 ,dim=-1)
  text_embeddings = F.normalize(text_embeddings ,p=2 ,dim=-1)

  # Return the embeddings and the contrastive loss
  loss = contrastive_loss(image_embeddings