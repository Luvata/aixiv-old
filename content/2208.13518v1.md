---
title: 2208.13518v1 LogicRank  Logic Induced Reranking for Generative Text-to-Image Systems
date: 2022-08-14
---

# [LogicRank: Logic Induced Reranking for Generative Text-to-Image Systems](http://arxiv.org/abs/2208.13518v1)

authors: BjÃ¶rn Deiseroth, Patrick Schramowski, Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting


## What, Why and How

[1]: https://arxiv.org/abs/2208.13518v1 "[2208.13518v1] LogicRank: Logic Induced Reranking for Generative Text ..."
[2]: https://arxiv.org/abs/2208.13518 "[2208.13518] LogicRank: Logic Induced Reranking for Generative Text-to ..."
[3]: http://export.arxiv.org/abs/2305.13518v1 "[2305.13518v1] Extended uncertainty principle and Van der Waals black holes"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes **LogicRank**, a neuro-symbolic reasoning framework that can improve the accuracy and consistency of generative text-to-image systems.
- **Why**: The paper argues that state-of-the-art text-to-image models like DALL-E still struggle with generating precise images from logical statements, and that CLIP is not able to rerank those images effectively. The paper demonstrates this problem using the draw bench benchmark, which consists of statements that require logical reasoning and visual understanding.
- **How**: The paper introduces LogicRank, which consists of two components: a logic parser that converts natural language statements into logical forms, and a logic evaluator that computes the truth value of logical forms given an image. LogicRank can be used to rerank the images generated by text-to-image models based on their logical consistency with the input statement. LogicRank can also be used to fine-tune text-to-image models towards more logical precision. The paper evaluates LogicRank on the draw bench benchmark and shows that it outperforms CLIP and other baselines in terms of accuracy and diversity.

## Main Contributions

According to the paper at , the main contributions are:

- The paper presents **LogicRank**, a novel neuro-symbolic reasoning framework that can improve the accuracy and consistency of generative text-to-image systems.
- The paper introduces the **draw bench benchmark**, a new dataset of natural language statements that require logical reasoning and visual understanding for text-to-image generation.
- The paper shows that **LogicRank** outperforms CLIP and other baselines on the draw bench benchmark in terms of accuracy and diversity, and that it can also be used to fine-tune text-to-image models towards more logical precision.

## Method Summary

The method section of the paper at  describes the following steps:

- The paper first defines the **text-to-image generation task** as a mapping from a natural language statement to a set of images that satisfy the statement.
- The paper then introduces the **logic parser**, which is a neural network that converts natural language statements into logical forms using a predefined grammar and vocabulary. The logic parser is trained on a synthetic dataset of statement-logical form pairs generated by a rule-based system.
- The paper also introduces the **logic evaluator**, which is a neural network that computes the truth value of a logical form given an image. The logic evaluator is trained on a synthetic dataset of logical form-image-truth value triples generated by a rule-based system.
- The paper then describes how to use **LogicRank** to rerank the images generated by text-to-image models based on their logical consistency with the input statement. LogicRank computes a score for each image by applying the logic evaluator to the logical form parsed from the statement. LogicRank then sorts the images by their scores in descending order and returns the top-k images as the final output.
- The paper also describes how to use **LogicRank** to fine-tune text-to-image models towards more logical precision. LogicRank uses the logic evaluator as a reward function to update the parameters of text-to-image models using policy gradient methods. LogicRank aims to maximize the expected reward of generating images that are logically consistent with the input statement.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the text-to-image generation task
def generate_images(statement):
  # Parse the statement into a logical form
  logical_form = logic_parser(statement)
  # Generate a set of images from the statement using a text-to-image model
  images = text_to_image_model(statement)
  # Rerank the images based on their logical consistency with the statement using LogicRank
  scores = [logic_evaluator(logical_form, image) for image in images]
  sorted_images = sort(images, scores, reverse=True)
  # Return the top-k images as the final output
  return sorted_images[:k]

# Define the fine-tuning task
def fine_tune_model(statement):
  # Parse the statement into a logical form
  logical_form = logic_parser(statement)
  # Generate an image from the statement using a text-to-image model
  image = text_to_image_model(statement)
  # Compute the reward based on the logical consistency of the image with the statement using LogicRank
  reward = logic_evaluator(logical_form, image)
  # Update the parameters of the text-to-image model using policy gradient methods
  text_to_image_model.update_parameters(reward)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch
import transformers
import PIL

# Define the grammar and vocabulary for the logic parser
grammar = """
  S -> A | Q | C
  A -> P O | N P O
  Q -> W P O | W N P O
  C -> S CONJ S
  P -> 'is' | 'has' | 'contains' | 'wears'
  O -> 'a' N | 'an' N | 'the' N | N
  N -> 'cat' | 'dog' | 'bird' | 'hat' | 'shirt' | 'red' | 'blue' | 'green'
  W -> 'what' | 'which'
  CONJ -> 'and' | 'or' | 'not'
"""
vocabulary = ['is', 'has', 'contains', 'wears', 'a', 'an', 'the', 
              'cat', 'dog', 'bird', 'hat', 'shirt', 'red', 'blue', 
              'green', 'what', 'which', 'and', 'or', 'not']

# Define the logic parser model as a sequence-to-sequence transformer
logic_parser_model = transformers.EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 
                                                                                      'bert-base-uncased')
logic_parser_model.config.decoder_start_token_id = vocabulary.index('S')
logic_parser_model.config.eos_token_id = vocabulary.index('.')
logic_parser_model.config.pad_token_id = vocabulary.index('[PAD]')
logic_parser_model.config.vocab_size = len(vocabulary)
logic_parser_model.resize_token_embeddings(len(vocabulary))

# Define the logic evaluator model as a vision-language transformer
logic_evaluator_model = transformers.ViTForImageUnderstandingWithText.from_pretrained('google/vit-base-patch16-224-in21k')
logic_evaluator_model.config.vocab_size = len(vocabulary)
logic_evaluator_model.resize_token_embeddings(len(vocabulary))

# Define the text-to-image model as a VQGAN-CLIP model
text_to_image_model = VQGAN_CLIP()

# Define the loss function and optimizer for the logic parser model
logic_parser_loss = torch.nn.CrossEntropyLoss(ignore_index=vocabulary.index('[PAD]'))
logic_parser_optimizer = torch.optim.Adam(logic_parser_model.parameters(), lr=0.0001)

# Define the loss function and optimizer for the logic evaluator model
logic_evaluator_loss = torch.nn.BCEWithLogitsLoss()
logic_evaluator_optimizer = torch.optim.Adam(logic_evaluator_model.parameters(), lr=0.0001)

# Define the optimizer for the text-to-image model
text_to_image_optimizer = torch.optim.Adam(text_to_image_model.parameters(), lr=0.0001)

# Define the function to generate synthetic data for training the logic parser model
def generate_logic_parser_data(batch_size):
  # Initialize the input and output tensors
  input_ids = torch.zeros((batch_size, max_length), dtype=torch.long)
  output_ids = torch.zeros((batch_size, max_length), dtype=torch.long)
  # For each batch example
  for i in range(batch_size):
    # Generate a random statement using the grammar rules
    statement = generate_random_statement(grammar)
    # Generate a corresponding logical form using a rule-based system
    logical_form = generate_logical_form(statement)
    # Tokenize the statement and logical form using the vocabulary
    statement_tokens = tokenize(statement, vocabulary)
    logical_form_tokens = tokenize(logical_form, vocabulary)
    # Add the special tokens and padding tokens
    statement_tokens = ['[CLS]'] + statement_tokens + ['[SEP]'] + ['[PAD]'] * (max_length - len(statement_tokens) - 2)
    logical_form_tokens = ['S'] + logical_form_tokens + ['.'] + ['[PAD]'] * (max_length - len(logical_form_tokens) - 2)
    # Convert the tokens to ids using the vocabulary
    statement_ids = [vocabulary.index(token) for token in statement_tokens]
    logical_form_ids = [vocabulary.index(token) for token in logical_form_tokens]
    # Store the ids in the input and output tensors
    input_ids[i] = torch.tensor(statement_ids)
    output_ids[i] = torch.tensor(logical_form_ids)
  # Return the input and output tensors
  return input_ids, output_ids

# Define the function to generate synthetic data for training the logic evaluator model
def generate_logic_evaluator_data(batch_size):
  # Initialize the input and output tensors
  input_ids = torch.zeros((batch_size, max_length), dtype=torch.long)
  image_tensors = torch.zeros((batch_size, 3, 224, 224), dtype=torch.float)
  output_labels = torch.zeros((batch_size, 1), dtype=torch.float)
  # For each batch example
  for i in range(batch_size):
    # Generate a random logical form using the grammar rules
    logical_form = generate_random_logical_form(grammar)
    # Generate a random image using PIL
    image = generate_random_image()
    # Convert the image to a tensor
    image_tensor = PIL.ImageToTensor()(image)
    # Generate a truth value for the logical form and image using a rule-based system
    truth_value = evaluate_logical_form(logical_form, image)
    # Tokenize the logical form using the vocabulary
    logical_form_tokens = tokenize(logical_form, vocabulary)
    # Add the special tokens and padding tokens
    logical_form_tokens = ['[CLS]'] + logical_form_tokens + ['[SEP]'] + ['[PAD]'] * (max_length - len(logical_form_tokens) - 2)
    # Convert the tokens to ids using the vocabulary
    logical_form_ids = [vocabulary.index(token) for token in logical_form_tokens]
    # Store the ids, image tensor and truth value in the input and output tensors
    input_ids[i] = torch.tensor(logical_form_ids)
    image_tensors[i] = image_tensor
    output_labels[i] = torch.tensor(truth_value)
  # Return the input and output tensors
  return input_ids, image_tensors, output_labels

# Define the function to train the logic parser model for one epoch
def train_logic_parser_model(epoch):
  # Set the model to training mode
  logic_parser_model.train()
  # Initialize the epoch loss
  epoch_loss = 0.0
  # For each batch of data
  for batch in range(num_batches):
    # Generate synthetic data for training
    input_ids, output_ids = generate_logic_parser_data(batch_size)
    # Forward pass the model and get the logits
    logits = logic_parser_model(input_ids=input_ids, decoder_input_ids=output_ids).logits
    # Compute the loss
    loss = logic_parser_loss(logits.view(-1, len(vocabulary)), output_ids.view(-1))
    # Backward pass and update the parameters
    loss.backward()
    logic_parser_optimizer.step()
    logic_parser_optimizer.zero_grad()
    # Accumulate the epoch loss
    epoch_loss += loss.item()
  # Print the epoch loss
  print(f'Epoch {epoch}, Logic Parser Loss: {epoch_loss / num_batches}')

# Define the function to train the logic evaluator model for one epoch
def train_logic_evaluator_model(epoch):
  # Set the model to training mode
  logic_evaluator_model.train()
  # Initialize the epoch loss
  epoch_loss = 0.0
  # For each batch of data
  for batch in range(num_batches):
    # Generate synthetic data for training
    input_ids, image_tensors, output_labels = generate_logic_evaluator_data(batch_size)
    # Forward pass the model and get the logits
    logits = logic_evaluator_model(input_ids=input_ids, pixel_values=image_tensors).logits
    # Compute the loss
    loss = logic_evaluator_loss(logits, output_labels)
    # Backward pass and update the parameters
    loss.backward()
    logic_evaluator_optimizer.step()
    logic_evaluator_optimizer.zero_grad()
    # Accumulate the epoch loss
    epoch_loss += loss.item()
  # Print the epoch loss
  print(f'Epoch {epoch}, Logic Evaluator Loss: {epoch_loss / num_batches}')

# Define the function to fine-tune the text-to-image model for one epoch using LogicRank as a reward function
def fine_tune_text_to_image_model(epoch):
  # Set the model to training mode
  text_to_image_model.train()
  # Initialize the epoch reward
  epoch_reward = 0.0
  # For each batch of data
  for batch in range(num_batches):
    # Generate a random statement using the grammar rules
    statement = generate_random_statement(grammar)
    # Parse the statement into a logical form using the logic parser model
    logical_form = logic_parser_model.generate(input_ids=tokenize(statement, vocabulary), 
                                               max_length=max_length,
                                               num_beams=num_beams,
                                               do_sample=do_sample,
                                               top_k=top_k,
                                               top_p=top_p)[0]