---
title: 2112.02815v2 Make It Move  Controllable Image-to-Video Generation with Text Descriptions
date: 2021-12-03
---

# [Make It Move: Controllable Image-to-Video Generation with Text Descriptions](http://arxiv.org/abs/2112.02815v2)

authors: Yaosi Hu, Chong Luo, Zhenzhong Chen


## What, Why and How

[1]: https://arxiv.org/abs/2112.02815 "[2112.02815] Make It Move: Controllable Image-to-Video Generation with ..."
[2]: https://arxiv.org/abs/2112.08782 "[2112.08782] Improved YOLOv5 network for real-time multi ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.02815v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: The paper proposes a novel video generation task, named Text-Image-to-Video generation (TI2V), which aims at generating videos from a static image and a text description. The paper also introduces a Motion Anchor-based video GEnerator (MAGE) to address the challenges of TI2V task.

Why: The paper claims that generating controllable videos conforming to user intentions is an appealing yet challenging topic in computer vision. The paper argues that existing methods for video generation either lack controllability or diversity, or require complex inputs or annotations.

How: The paper designs a motion anchor (MA) structure to store appearance-motion aligned representation from text and image inputs. The paper also allows the injection of explicit condition and implicit randomness to model the uncertainty and increase the diversity of the generated videos. The paper uses three-dimensional axial transformers to interact MA with the given image and generate next frames recursively. The paper evaluates the proposed method on two new video-text paired datasets based on MNIST and CATER, and compares it with several baselines and ablations.
```

## Main Contributions

[1]: https://arxiv.org/abs/2112.02815 "[2112.02815] Make It Move: Controllable Image-to-Video Generation with ..."
[2]: https://arxiv.org/abs/2112.08782 "[2112.08782] Improved YOLOv5 network for real-time multi ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.02815v2.pdf "arXiv.org e-Print archive"

The paper claims to make the following contributions:

- It proposes a novel video generation task, named Text-Image-to-Video generation (TI2V), which enables maneuverable control in line with user intentions.
- It introduces a Motion Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA) structure to store appearance-motion aligned representation from text and image inputs.
- It allows the injection of explicit condition and implicit randomness to model the uncertainty and increase the diversity of the generated videos.
- It uses three-dimensional axial transformers to interact MA with the given image and generate next frames recursively.
- It builds two new video-text paired datasets based on MNIST and CATER for evaluation.

## Method Summary

[1]: https://arxiv.org/abs/2112.02815 "[2112.02815] Make It Move: Controllable Image-to-Video Generation with ..."
[2]: https://arxiv.org/abs/2112.08782 "[2112.08782] Improved YOLOv5 network for real-time multi ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.02815v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

```
The method section consists of four subsections: problem formulation, motion anchor, motion anchor-based video generator, and training details.

- Problem formulation: The paper defines the TI2V task as generating a video sequence V = {I_1, ..., I_T} from an image I_0 and a text description D. The paper assumes that D contains both appearance and motion information, and that V should be consistent with both I_0 and D.

- Motion anchor: The paper introduces a motion anchor (MA) structure to store appearance-motion aligned representation from text and image inputs. The paper uses a text encoder to encode D into a sequence of text embeddings E = {e_1, ..., e_N}, and then applies a self-attention mechanism to obtain a global text embedding e_g. The paper also uses an image encoder to encode I_0 into a feature map F_0. The paper then computes the MA as M = F_0 * e_g, where * denotes element-wise multiplication.

- Motion anchor-based video generator: The paper proposes a MAGE model to generate videos from MA and I_0. The paper uses a three-dimensional axial transformer to interact MA with I_0 and generate the first frame I_1. The paper then updates MA by concatenating I_1 with F_0 along the channel dimension, and repeats the process to generate the subsequent frames. The paper also allows the injection of explicit condition and implicit randomness to model the uncertainty and increase the diversity of the generated videos.

- Training details: The paper describes the implementation details of the proposed method, such as the network architecture, the loss function, the optimization algorithm, and the hyperparameters. The paper also introduces two new video-text paired datasets based on MNIST and CATER for evaluation.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2112.02815 "[2112.02815] Make It Move: Controllable Image-to-Video Generation with ..."
[2]: https://arxiv.org/abs/2112.08782 "[2112.08782] Improved YOLOv5 network for real-time multi ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.02815v2.pdf "arXiv.org e-Print archive"

Here is a high-level pseudo code for the paper:

```
# Input: an image I_0 and a text description D
# Output: a video sequence V = {I_1, ..., I_T}

# Encode D into a sequence of text embeddings E = {e_1, ..., e_N}
E = text_encoder(D)

# Compute a global text embedding e_g using self-attention
e_g = self_attention(E)

# Encode I_0 into a feature map F_0
F_0 = image_encoder(I_0)

# Compute the motion anchor M as element-wise multiplication of F_0 and e_g
M = F_0 * e_g

# Initialize V as an empty list
V = []

# Loop for T steps
for t in range(T):

  # Inject explicit condition and implicit randomness into M
  M = inject_condition_and_randomness(M)

  # Interact M with I_0 using a 3D axial transformer and generate the next frame I_t+1
  I_t+1 = axial_transformer(M, I_0)

  # Append I_t+1 to V
  V.append(I_t+1)

  # Update M by concatenating I_t+1 with F_0 along the channel dimension
  M = concatenate(I_t+1, F_0)

# Return V as the generated video sequence
return V
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2112.02815 "[2112.02815] Make It Move: Controllable Image-to-Video Generation with ..."
[2]: https://arxiv.org/abs/2112.08782 "[2112.08782] Improved YOLOv5 network for real-time multi ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.02815v2.pdf "arXiv.org e-Print archive"

Here is a detailed pseudo code to implement the paper:

```
# Input: an image I_0 and a text description D
# Output: a video sequence V = {I_1, ..., I_T}

# Define the network architecture and hyperparameters
text_encoder = a 2-layer bidirectional LSTM with 256 hidden units
image_encoder = a ResNet-50 backbone with 2048 output channels
axial_transformer = a 3D axial transformer with 12 layers, 8 heads, and 2048 hidden units
T = the number of frames to generate
C = the number of explicit conditions to sample from D
Z = the dimension of implicit randomness to sample from N(0,1)
lambda_1, lambda_2, lambda_3 = the weights for reconstruction loss, diversity loss, and consistency loss

# Encode D into a sequence of text embeddings E = {e_1, ..., e_N}
E = text_encoder(D)

# Compute a global text embedding e_g using self-attention
e_g = self_attention(E)

# Encode I_0 into a feature map F_0
F_0 = image_encoder(I_0)

# Compute the motion anchor M as element-wise multiplication of F_0 and e_g
M = F_0 * e_g

# Initialize V as an empty list
V = []

# Loop for T steps
for t in range(T):

  # Sample C explicit conditions from E using Gumbel-Softmax
  C_e = gumbel_softmax(E, C)

  # Sample Z implicit randomness from N(0,1)
  Z_e = normal(0,1,Z)

  # Concatenate C_e and Z_e along the channel dimension
  CZ_e = concatenate(C_e, Z_e)

  # Inject CZ_e into M by adding it to each spatial location of M
  M = M + CZ_e

  # Interact M with I_0 using a 3D axial transformer and generate the next frame I_t+1
  I_t+1 = axial_transformer(M, I_0)

  # Append I_t+1 to V
  V.append(I_t+1)

  # Update M by concatenating I_t+1 with F_0 along the channel dimension
  M = concatenate(I_t+1, F_0)

# Define the loss function as a combination of reconstruction loss, diversity loss, and consistency loss
L_rec = mean squared error between V and ground truth video sequence
L_div = negative entropy of C_e across different frames
L_con = cosine similarity between e_g and global video embedding computed from V using self-attention
L = lambda_1 * L_rec + lambda_2 * L_div + lambda_3 * L_con

# Optimize the network parameters using Adam optimizer
optimizer = Adam(learning_rate=0.0002)
optimizer.minimize(L)

# Return V as the generated video sequence
return V
```