---
title: 2303.17604v1 Token Merging for Fast Stable Diffusion
date: 2023-03-18
---

# [Token Merging for Fast Stable Diffusion](http://arxiv.org/abs/2303.17604v1)

authors: Daniel Bolya, Judy Hoffman


## What, Why and How

[1]: https://arxiv.org/pdf/2303.17604v1.pdf "Token Merging for Fast Stable Diffusion - arXiv.org"
[2]: https://arxiv.org/pdf/2303.17603.pdf "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2303.17604v1 "arXiv.org"
[4]: http://export.arxiv.org/abs/2303.17604 "[2303.17604] Token Merging for Fast Stable Diffusion"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to speed up diffusion models for image generation by merging redundant tokens in the images.
- **Why**: Diffusion models are powerful but slow because they use transformers that scale with the square of the number of tokens. Existing methods to speed up transformers do not reduce the amount of work necessary. Token merging can exploit the natural redundancy in images and reduce the number of tokens without re-training the model.
- **How**: The paper adapts Token Merging (ToMe) [1], a method that merges tokens based on their similarity and attention scores, to diffusion models. The paper makes some diffusion-specific improvements to ToMe, such as using a different similarity metric, applying token merging only at certain timesteps, and using a different merging strategy. The paper evaluates the proposed method on Stable Diffusion [15], a state-of-the-art diffusion model, and shows that it can reduce the number of tokens by up to 60% while still producing high quality images without any extra training. The paper also shows that the speed-up stacks with existing methods such as xFormers [8], minimally impacting quality while being up to 5.4 times faster for large images.

## Main Contributions

The paper claims the following contributions:

- It is the first to apply token merging to diffusion models for image generation, which can significantly speed up generation without re-training the model or sacrificing quality.
- It proposes several improvements to ToMe that are specific to diffusion models, such as using a different similarity metric, applying token merging only at certain timesteps, and using a different merging strategy.
- It demonstrates the effectiveness of the proposed method on Stable Diffusion, a state-of-the-art diffusion model, and shows that it can reduce the number of tokens by up to 60% while still producing high quality images without any extra training.
- It shows that the speed-up stacks with existing methods such as xFormers, minimally impacting quality while being up to 5.4 times faster for large images.

## Method Summary

Here is a summary of the method section of the paper:

- The paper reviews the basics of diffusion models and token merging, and then describes how to adapt token merging to diffusion models.
- The paper uses a different similarity metric than ToMe, which is based on the cosine similarity between the token embeddings and the mean embedding of the image. This metric is more suitable for diffusion models because it captures the global context of the image better than the local attention scores used by ToMe.
- The paper applies token merging only at certain timesteps, which are determined by a threshold on the noise level. This is because token merging is more effective when the image is less noisy and more structured. The paper uses a threshold of 0.2 for Stable Diffusion, which corresponds to about 40% of the timesteps.
- The paper uses a different merging strategy than ToMe, which is based on clustering the tokens into groups and then merging each group into one token. This strategy is more efficient than ToMe's greedy algorithm, which merges tokens one by one. The paper uses k-means clustering with k equal to half of the number of tokens.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image x of size N x N
# Output: a generated image y of size N x N
# Hyperparameters: T = number of diffusion timesteps, beta = noise level schedule, tau = noise level threshold for token merging, k = number of clusters for token merging

# Initialize y to x
y = x

# Loop over diffusion timesteps from T to 1
for t in range(T, 0, -1):

  # Compute the noise level beta_t
  beta_t = beta[t]

  # If beta_t is less than tau, apply token merging
  if beta_t < tau:

    # Reshape y into a sequence of tokens of size M x M, where M = N / sqrt(k)
    y = reshape(y, (k, M, M))

    # Compute the token embeddings using a transformer encoder
    z = transformer_encoder(y)

    # Compute the mean embedding of the image
    z_mean = mean(z)

    # Compute the cosine similarity between each token embedding and the mean embedding
    sim = cosine_similarity(z, z_mean)

    # Cluster the tokens into k groups based on their similarity scores using k-means
    clusters = k_means(sim, k)

    # Merge each cluster of tokens into one token by averaging their pixel values
    y = merge(clusters)

    # Reshape y back into an image of size N x N
    y = reshape(y, (N, N))

  # Apply the diffusion model to denoise y with noise level beta_t
  y = diffusion_model(y, beta_t)

# Return the generated image y
return y
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.cluster import KMeans

# Define the transformer encoder class
class TransformerEncoder(nn.Module):
  def __init__(self, d_model, nhead, dim_feedforward, num_layers):
    super(TransformerEncoder, self).__init__()
    # Initialize the transformer encoder layer
    self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward)
    # Initialize the transformer encoder with num_layers encoder layers
    self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers)

  def forward(self, x):
    # Compute the transformer encoder output
    output = self.transformer_encoder(x)
    return output

# Define the diffusion model class
class DiffusionModel(nn.Module):
  def __init__(self, d_model, nhead, dim_feedforward, num_layers):
    super(DiffusionModel, self).__init__()
    # Initialize the transformer decoder layer
    self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward)
    # Initialize the transformer decoder with num_layers decoder layers
    self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers)
    # Initialize the output layer to project the decoder output to the image size
    self.output_layer = nn.Linear(d_model, N * N)

  def forward(self, y, beta_t):
    # Compute the noise vector epsilon_t from a normal distribution with mean 0 and variance beta_t
    epsilon_t = torch.normal(0, np.sqrt(beta_t), size=(N * N,))
    # Add the noise vector to the image y to get the noisy image x_t
    x_t = y + epsilon_t
    # Reshape x_t into a sequence of tokens of size N * N x 1
    x_t = x_t.view(N * N, 1)
    # Compute the transformer decoder output
    output = self.transformer_decoder(x_t)
    # Compute the output layer to get the predicted image y_hat_t
    y_hat_t = self.output_layer(output)
    # Reshape y_hat_t into an image of size N x N
    y_hat_t = y_hat_t.view(N, N)
    # Compute the loss function L_t as the mean squared error between y and y_hat_t
    L_t = F.mse_loss(y, y_hat_t)
    # Compute the gradient of L_t with respect to y and update y using gradient descent
    L_t.backward()
    y = y - lr * y.grad
    return y

# Define the token merging function
def token_merging(y):
  # Reshape y into a sequence of tokens of size k x M x M, where M = N / sqrt(k)
  M = int(N / np.sqrt(k))
  y = y.view(k, M * M)
  # Compute the token embeddings using a transformer encoder
  z = transformer_encoder(y)
  # Compute the mean embedding of the image
  z_mean = torch.mean(z, dim=0)
  # Compute the cosine similarity between each token embedding and the mean embedding
  sim = F.cosine_similarity(z, z_mean.unsqueeze(0), dim=1)
  # Cluster the tokens into k groups based on their similarity scores using k-means
  kmeans = KMeans(n_clusters=k)
  clusters = kmeans.fit_predict(sim.numpy())
  # Merge each cluster of tokens into one token by averaging their pixel values
  y_merged = torch.zeros(k, M * M)
  for i in range(k):
    # Get the indices of the tokens belonging to cluster i
    indices = torch.where(clusters == i)[0]
    # Average the pixel values of these tokens and assign them to y_merged[i]
    y_merged[i] = torch.mean(y[indices], dim=0)
  # Reshape y_merged back into an image of size N x N
  y_merged = y_merged.view(N, N)
  return y_merged

# Define the main function to generate an image from an input image x
def generate_image(x):
  # Initialize y to x
  y = x

  # Loop over diffusion timesteps from T to 1
  for t in range(T, 0, -1):

    # Compute the noise level beta_t
    beta_t = beta[t]

    # If beta_t is less than tau, apply token merging
    if beta_t < tau:
      y = token_merging(y)

    # Apply the diffusion model to denoise y with noise level beta_t
    y = diffusion_model(y, beta_t)

  # Return the generated image y
  return y
```