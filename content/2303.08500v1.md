---
title: 2303.08500v1 The Devil's Advocate  Shattering the Illusion of Unexploitable Data using Diffusion Models
date: 2023-03-09
---

# [The Devil's Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models](http://arxiv.org/abs/2303.08500v1)

authors: Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie


## What, Why and How

[1]: https://arxiv.org/abs/2303.08500 "[2303.08500] The Devil's Advocate: Shattering the Illusion of ..."
[2]: https://arxiv.org/pdf/2303.08305v1.pdf "arXiv:2303.08305v1 [cond-mat.mes-hall] 15 Mar 2023"
[3]: http://arxiv-export3.library.cornell.edu/abs/2303.08500 "[2303.08500] The Devil's Advocate: Shattering the Illusion of ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a novel algorithm called **AVATAR** that can **denoise** data that has been protected by **availability attacks**. Availability attacks are methods that add imperceptible noise to personal data to prevent unauthorized use by machine learning models. The paper claims that availability attacks do not make data unexploitable, and that AVATAR can recover the original patterns from the noisy data using **diffusion models**.
- **Why**: The paper aims to **challenge** the existing approaches to data protection using availability attacks, and to show that they are **not effective** against powerful denoising methods. The paper also wants to **motivate** more research into making personal data unexploitable, and to provide a **benchmark** for future methods to compare against.
- **How**: The paper uses diffusion models, which are generative models that can learn the distribution of data by reversing a Markov chain of Gaussian noise additions. The paper designs a denoising process that adapts the noise level and the number of diffusion steps according to the magnitude of the data-protecting perturbations. The paper theoretically proves that AVATAR can achieve optimal denoising performance under certain conditions. The paper also empirically evaluates AVATAR on various datasets and scenarios, and shows that it outperforms adversarial training and other baselines.

This summary is based on the abstract[^1^][1] and the introduction[^2^][2] of the paper.

## Main Contributions

[1]: https://arxiv.org/abs/2303.08500 "[2303.08500] The Devil's Advocate: Shattering the Illusion of ..."
[2]: https://arxiv.org/pdf/2303.08509v1.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2303.08500 "[2303.08500] The Devil's Advocate: Shattering the Illusion of ..."

According to the paper[^1^][1], the main contributions are:

- **The first countermeasure against availability attacks using diffusion models**: The paper introduces AVATAR, a novel algorithm that can denoise data that has been perturbed by availability attacks using diffusion models. The paper shows that AVATAR can recover the original patterns from the noisy data and achieve high accuracy on various classification tasks.
- **A rigorous theoretical analysis of AVATAR**: The paper provides a theoretical proof that AVATAR can achieve optimal denoising performance under certain conditions. The paper also derives a formula that relates the amount of required denoising to the magnitude of the data-protecting perturbations.
- **A comprehensive empirical evaluation of AVATAR**: The paper evaluates AVATAR on various datasets and scenarios, and compares it with adversarial training and other baselines. The paper shows that AVATAR outperforms the state-of-the-art methods against a suite of recent availability attacks, and is robust to different noise levels, diffusion steps, and model architectures.

## Method Summary

[1]: https://arxiv.org/abs/2303.08500 "[2303.08500] The Devil's Advocate: Shattering the Illusion of ..."
[2]: https://arxiv.org/pdf/2303.08509v1.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2303.08500 "[2303.08500] The Devil's Advocate: Shattering the Illusion of ..."

Here is a summary of the method section of the paper:

- **Diffusion models**: The paper reviews the basics of diffusion models, which are generative models that can learn the distribution of data by reversing a Markov chain of Gaussian noise additions. The paper explains how diffusion models can be trained using denoising score matching and how they can be sampled using Langevin dynamics.
- **AVATAR**: The paper introduces AVATAR, a novel algorithm that can denoise data that has been perturbed by availability attacks using diffusion models. The paper describes the main steps of AVATAR, which are: 1) estimating the noise level of the perturbed data, 2) selecting the optimal number of diffusion steps, 3) applying a denoising process using diffusion models, and 4) classifying the denoised data using a pre-trained classifier.
- **Theoretical analysis**: The paper provides a theoretical analysis of AVATAR, proving that AVATAR can achieve optimal denoising performance under certain conditions. The paper also derives a formula that relates the amount of required denoising to the magnitude of the data-protecting perturbations. The paper shows that AVATAR can recover the original patterns from the noisy data as long as the noise level is below a certain threshold.
- **Implementation details**: The paper gives some implementation details of AVATAR, such as the choice of hyperparameters, the architecture of diffusion models, and the optimization method.

This summary is based on section 3[^1^][1] and section 4[^1^][1] of the paper.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: perturbed data x_tilde, pre-trained classifier f, pre-trained diffusion model g
# Output: predicted label y_hat

# Step 1: Estimate the noise level sigma_tilde of x_tilde
sigma_tilde = estimate_noise_level(x_tilde)

# Step 2: Select the optimal number of diffusion steps T
T = select_diffusion_steps(sigma_tilde)

# Step 3: Apply the denoising process using diffusion model g
x_0 = x_tilde # initialize the original data
for t in range(1, T+1):
  # compute the reverse noise level sigma_t
  sigma_t = compute_reverse_noise_level(t, T)
  # compute the reverse noise variance alpha_t
  alpha_t = compute_reverse_noise_variance(sigma_t, sigma_tilde)
  # compute the score s_t using diffusion model g
  s_t = g(x_t)
  # update x_t using Langevin dynamics
  x_t = (x_0 - alpha_t * s_t) / (1 - alpha_t) + sqrt(2 * alpha_t) * epsilon
  # epsilon is a standard Gaussian noise

# Step 4: Classify the denoised data x_T using classifier f
y_hat = f(x_T)

# Return the predicted label y_hat
return y_hat
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: perturbed data x_tilde, pre-trained classifier f, pre-trained diffusion model g
# Output: predicted label y_hat

# Step 1: Estimate the noise level sigma_tilde of x_tilde
# Use the method proposed by Liu et al. [2018]
sigma_tilde = estimate_noise_level(x_tilde)

# Step 2: Select the optimal number of diffusion steps T
# Use the method proposed by Song and Ermon [2020]
T = select_diffusion_steps(sigma_tilde)

# Step 3: Apply the denoising process using diffusion model g
x_0 = x_tilde # initialize the original data
for t in range(1, T+1):
  # compute the reverse noise level sigma_t
  # use the formula given in section 4.1 of the paper
  sigma_t = sqrt(1 - (t / T) ** (2 * beta))
  # beta is a hyperparameter that controls the noise schedule
  # compute the reverse noise variance alpha_t
  # use the formula given in section 4.1 of the paper
  alpha_t = (sigma_t ** 2 - sigma_tilde ** 2) / (1 - sigma_tilde ** 2)
  # compute the score s_t using diffusion model g
  # use the denoising score matching objective given in section 3.1 of the paper
  s_t = g(x_t)
  # update x_t using Langevin dynamics
  # use the sampling formula given in section 3.2 of the paper
  x_t = (x_0 - alpha_t * s_t) / (1 - alpha_t) + sqrt(2 * alpha_t) * epsilon
  # epsilon is a standard Gaussian noise

# Step 4: Classify the denoised data x_T using classifier f
y_hat = f(x_T)

# Return the predicted label y_hat
return y_hat
```