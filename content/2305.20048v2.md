---
title: 2305.20048v2 F?D  On understanding the role of deep feature spaces on face generation evaluation
date: 2023-05-21
---

# [F?D: On understanding the role of deep feature spaces on face generation evaluation](http://arxiv.org/abs/2305.20048v2)

authors: Krish Kabra, Guha Balakrishnan


## What, Why and How

[1]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."
[2]: https://arxiv.org/abs/2305.15324 "[2305.15324] Model evaluation for extreme risks - arXiv.org"
[3]: https://avdata.ford.com/ "Ford AV Dataset - Home"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a new framework for language model inference, called Tree of Thoughts (ToT), which enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.
- **Why**: The paper aims to overcome the limitations of existing language models that are confined to token-level, left-to-right decision-making processes during inference, which can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.
- **How**: The paper proposes a method that allows language models to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. The paper also presents three novel tasks that require non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords, and shows that ToT significantly enhances language models' problem-solving abilities on these tasks.

## Main Contributions

The paper claims to make the following contributions:

- A new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.
- A method for generating and evaluating thoughts using language models, which allows for deliberate decision making, exploration, and search over multiple reasoning paths.
- Three novel tasks that require non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords, along with a code repository with all prompts and data.
- Empirical results that demonstrate the effectiveness of ToT in enhancing language models' problem-solving abilities on these tasks, compared to baseline methods.

## Method Summary

The method section of the paper describes the Tree of Thoughts (ToT) framework in detail. It consists of three main components: a **thought generator**, a **thought evaluator**, and a **thought selector**. The thought generator takes as input a problem statement and a partial solution, and generates a set of candidate thoughts that extend the partial solution by one step. The thought evaluator assigns a score to each candidate thought based on its relevance, coherence, and quality. The thought selector chooses the best thought based on the score and the current search strategy, which can be greedy, beam search, or Monte Carlo tree search. The chosen thought is then appended to the partial solution, and the process is repeated until a complete solution is found or a termination criterion is met. The paper also explains how to adapt ToT to different tasks and domains by using different prompts and formats for generating and evaluating thoughts.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the problem statement and the initial partial solution
problem = "Solve the game of 24 using 3, 4, 5, and 6"
partial_solution = ""

# Define the thought generator, evaluator, and selector
thought_generator = GPT-4(prompt="Given the problem {problem} and the partial solution {partial_solution}, generate a set of candidate thoughts that extend the partial solution by one step. Each thought should be a valid arithmetic expression using the given numbers and one of the four basic operations (+, -, *, /). Separate each thought by a newline.")
thought_evaluator = GPT-4(prompt="Given the problem {problem} and a candidate thought {thought}, evaluate the thought based on its relevance, coherence, and quality. Return a score between 0 and 1, where higher scores indicate better thoughts.")
thought_selector = MonteCarloTreeSearch()

# Loop until a complete solution is found or a termination criterion is met
while not is_complete(partial_solution) and not is_terminated():
  # Generate a set of candidate thoughts
  candidate_thoughts = thought_generator(problem, partial_solution)
  
  # Evaluate each candidate thought
  scores = []
  for thought in candidate_thoughts:
    score = thought_evaluator(problem, thought)
    scores.append(score)
  
  # Select the best thought based on the search strategy
  best_thought = thought_selector(candidate_thoughts, scores)
  
  # Append the best thought to the partial solution
  partial_solution += best_thought
  
# Return the final solution or an error message
if is_complete(partial_solution):
  return "The solution is: " + partial_solution
else:
  return "No solution found."
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import transformers
from mcts import MCTS

# Load the GPT-4 model and tokenizer
model = transformers.AutoModelForCausalLM.from_pretrained("gpt-4")
tokenizer = transformers.AutoTokenizer.from_pretrained("gpt-4")

# Define the problem statement and the initial partial solution
problem = "Solve the game of 24 using 3, 4, 5, and 6"
partial_solution = ""

# Define the thought generator function
def thought_generator(problem, partial_solution):
  # Construct the input prompt
  prompt = f"Given the problem {problem} and the partial solution {partial_solution}, generate a set of candidate thoughts that extend the partial solution by one step. Each thought should be a valid arithmetic expression using the given numbers and one of the four basic operations (+, -, *, /). Separate each thought by a newline."
  
  # Encode the prompt and generate a response
  input_ids = tokenizer.encode(prompt, return_tensors="pt")
  output_ids = model.generate(input_ids, max_length=256, num_return_sequences=10)
  
  # Decode the response and split it into candidate thoughts
  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
  candidate_thoughts = output_text.split("\n")
  
  # Return the candidate thoughts
  return candidate_thoughts

# Define the thought evaluator function
def thought_evaluator(problem, thought):
  # Construct the input prompt
  prompt = f"Given the problem {problem} and a candidate thought {thought}, evaluate the thought based on its relevance, coherence, and quality. Return a score between 0 and 1, where higher scores indicate better thoughts."
  
  # Encode the prompt and generate a response
  input_ids = tokenizer.encode(prompt, return_tensors="pt")
  output_ids = model.generate(input_ids, max_length=256, num_return_sequences=1)
  
  # Decode the response and convert it to a score
  output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
  score = float(output_text)
  
  # Return the score
  return score

# Define the thought selector function using Monte Carlo tree search
def thought_selector(candidate_thoughts, scores):
  # Initialize the MCTS object with a root node
  mcts = MCTS()
  root = mcts.create_node(partial_solution)
  
  # Expand the root node with the candidate thoughts and scores
  for i in range(len(candidate_thoughts)):
    child = mcts.create_node(candidate_thoughts[i], parent=root)
    child.reward = scores[i]
    root.children.append(child)
  
  # Run MCTS for a fixed number of iterations or time limit
  mcts.search(root, iterations=1000, time_limit=10)
  
  # Select the best child node based on the UCT value
  best_child = mcts.best_child(root)
  
  # Return the best thought
  return best_child.state

# Define a function to check if a partial solution is complete
def is_complete(partial_solution):
  # A partial solution is complete if it uses all four numbers and evaluates to 24
  numbers = ["3", "4", "5", "6"]
  for number in numbers:
    if number not in partial_solution:
      return False
  
  try:
    value = eval(partial_solution)
    if value == 24:
      return True
    else:
      return False
  except:
    return False

# Define a function to check if a termination criterion is met
def is_terminated():
  # A termination criterion is met if the partial solution is too long or invalid
  if len(partial_solution) > 20:
    return True
  
  try:
    eval(partial_solution)
    return False
  except:
    return True

# Loop until a complete solution is found or a termination criterion is met
while not is_complete(partial_solution) and not is_terminated():
  # Generate a set of candidate thoughts
  candidate_thoughts = thought_generator(problem, partial_solution)
  
  # Evaluate each candidate thought
  scores = []
  for thought in candidate_thoughts:
    score = thought_evaluator(problem, thought)
    scores.append(score)
  
  # Select the best thought based on the search strategy
  best_thought = thought_selector(candidate_thoughts, scores)
  
  # Append the best thought to the partial solution
  partial_solution += best_thought
  
# Return the final solution or an error message
if is_complete(partial_solution):
  print("The solution is: " + partial_solution)
else:
  print("No solution found.")
```