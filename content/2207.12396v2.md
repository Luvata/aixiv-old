---
title: 2207.12396v2 Exploring CLIP for Assessing the Look and Feel of Images
date: 2022-07-13
---

# [Exploring CLIP for Assessing the Look and Feel of Images](http://arxiv.org/abs/2207.12396v2)

authors: Jianyi Wang, Kelvin C. K. Chan, Chen Change Loy


## What, Why and How

[1]: https://arxiv.org/abs/2207.12396 "Exploring CLIP for Assessing the Look and Feel of Images"
[2]: http://export.arxiv.org/abs/2207.02696 "[2207.02696] YOLOv7: Trainable bag-of-freebies sets new state-of-the ..."
[3]: https://arxiv.org/pdf/2207.12396v2.pdf "arXiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper explores the use of Contrastive Language-Image Pre-training (CLIP) models for assessing both the quality perception (look) and abstract perception (feel) of images in a zero-shot manner.
- **Why**: The paper aims to go beyond the conventional paradigms that rely on supervised models trained with labeled data collected via laborious user study, and to leverage the rich visual language prior encapsulated in CLIP models for more flexible and diverse perceptual assessments.
- **How**: The paper discusses effective prompt designs and shows an effective prompt pairing strategy to harness the prior. The paper also provides extensive experiments on controlled datasets and Image Quality Assessment (IQA) benchmarks to demonstrate that CLIP captures meaningful priors that generalize well to different perceptual assessments.

## Main Contributions

According to the paper, the contributions are:

- The paper is the first to explore CLIP models for assessing both the look and feel of images in a zero-shot manner.
- The paper proposes a novel prompt pairing strategy that can effectively utilize the visual language prior in CLIP models for perceptual assessment tasks.
- The paper conducts comprehensive experiments on various datasets and benchmarks to show that CLIP models can achieve competitive or even superior performance compared to existing methods for image quality assessment and abstract perception assessment.

## Method Summary

[1]: https://arxiv.org/abs/2207.12396 "Exploring CLIP for Assessing the Look and Feel of Images"
[2]: https://arxiv-export-lb.library.cornell.edu/abs/2207.12396?context=cs.CV "[2207.12396] Exploring CLIP for Assessing the Look and Feel of Images"
[3]: https://arxiv.org/pdf/2207.12396v2.pdf "arXiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces the CLIP model and its architecture, which consists of a vision encoder and a text encoder that are jointly trained on a large-scale dataset of image-text pairs using contrastive learning.
- The paper proposes a zero-shot perceptual assessment framework based on CLIP, which takes an image and a text prompt as input and outputs a score that reflects the degree of alignment between the image and the prompt.
- The paper discusses how to design effective prompts for different perceptual assessment tasks, such as image quality assessment, abstract perception assessment, and style transfer evaluation. The paper also introduces a novel prompt pairing strategy that can improve the performance by comparing two prompts for the same image.
- The paper presents the experimental setup and results on various datasets and benchmarks, such as TID2013, KADID-10k, CLIVE, Aesthetic Visual Analysis (AVA), and Style Transfer Evaluation (STE). The paper compares the proposed method with existing methods and analyzes the advantages and limitations of CLIP for perceptual assessment.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load the CLIP model and its vision and text encoders
clip_model = load_clip_model()
vision_encoder = clip_model.vision_encoder
text_encoder = clip_model.text_encoder

# Define a zero-shot perceptual assessment function
def zero_shot_perceptual_assessment(image, prompt):
  # Encode the image and the prompt into feature vectors
  image_feature = vision_encoder(image)
  prompt_feature = text_encoder(prompt)
  # Compute the cosine similarity between the image and the prompt features
  score = cosine_similarity(image_feature, prompt_feature)
  # Return the score as the perceptual assessment result
  return score

# Define a prompt pairing strategy function
def prompt_pairing_strategy(image, prompt1, prompt2):
  # Compute the scores for each prompt using the zero-shot perceptual assessment function
  score1 = zero_shot_perceptual_assessment(image, prompt1)
  score2 = zero_shot_perceptual_assessment(image, prompt2)
  # Compare the scores and return the difference as the final score
  final_score = score1 - score2
  return final_score

# Define different prompts for different perceptual assessment tasks
# For example, for image quality assessment, we can use prompts like "high quality image" and "low quality image"
# For abstract perception assessment, we can use prompts like "happy image" and "sad image"
# For style transfer evaluation, we can use prompts like "image in Monet style" and "image in Picasso style"

# Apply the zero-shot perceptual assessment function or the prompt pairing strategy function to different images and prompts
# Evaluate the results and compare with existing methods
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Load the CLIP model and its vision and text encoders
clip_model, preprocess = clip.load("ViT-B/32", device="cuda")
vision_encoder = clip_model.visual
text_encoder = clip_model.text

# Define a zero-shot perceptual assessment function
def zero_shot_perceptual_assessment(image, prompt):
  # Preprocess the image and convert it to a tensor
  image_tensor = preprocess(image).unsqueeze(0).to("cuda")
  # Encode the image and the prompt into feature vectors
  with torch.no_grad():
    image_feature = vision_encoder(image_tensor)
    prompt_feature = text_encoder(prompt).float()
  # Normalize the feature vectors
  image_feature = image_feature / image_feature.norm(dim=-1, keepdim=True)
  prompt_feature = prompt_feature / prompt_feature.norm(dim=-1, keepdim=True)
  # Compute the cosine similarity between the image and the prompt features
  score = (100.0 * image_feature @ prompt_feature.T).softmax(dim=-1)[0, 0]
  # Return the score as the perceptual assessment result
  return score.item()

# Define a prompt pairing strategy function
def prompt_pairing_strategy(image, prompt1, prompt2):
  # Compute the scores for each prompt using the zero-shot perceptual assessment function
  score1 = zero_shot_perceptual_assessment(image, prompt1)
  score2 = zero_shot_perceptual_assessment(image, prompt2)
  # Compare the scores and return the difference as the final score
  final_score = score1 - score2
  return final_score

# Define different prompts for different perceptual assessment tasks
# For example, for image quality assessment, we can use prompts like "high quality image" and "low quality image"
# For abstract perception assessment, we can use prompts like "happy image" and "sad image"
# For style transfer evaluation, we can use prompts like "image in Monet style" and "image in Picasso style"

# Load different images and prompts for different perceptual assessment tasks
# For example, we can load images from TID2013 dataset for image quality assessment
# We can load images from AVA dataset for abstract perception assessment
# We can load images from STE dataset for style transfer evaluation

# Apply the zero-shot perceptual assessment function or the prompt pairing strategy function to different images and prompts
# Evaluate the results and compare with existing methods
```