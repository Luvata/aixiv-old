---
title: 2112.10752v2 High-Resolution Image Synthesis with Latent Diffusion Models
date: 2021-12-11
---

# [High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2112.10752v2)

authors: Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj√∂rn Ommer


## What, Why and How

[1]: https://arxiv.org/abs/2112.10752 "High-Resolution Image Synthesis with Latent Diffusion Models"
[2]: https://www.scribd.com/document/598177320/2112-10752 "2112.10752 | PDF | Data Compression | Applied Mathematics"
[3]: https://arxiv.org/pdf/2112.10752v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper proposes a new method for high-resolution image synthesis with latent diffusion models (LDMs), which are diffusion models (DMs) applied in the latent space of pretrained autoencoders.
- **Why**: The paper aims to overcome the limitations of pixel-based DMs, which are computationally expensive and require sequential evaluations, and previous latent-based DMs, which lose too much detail due to aggressive downsampling.
- **How**: The paper introduces cross-attention layers into the LDM architecture, which allow for flexible and convolutional generation with various conditioning inputs such as text or bounding boxes. The paper also shows how to choose an optimal trade-off between complexity reduction and detail preservation by using suitable autoencoding models. The paper evaluates the LDMs on various image synthesis tasks, such as inpainting, super-resolution, text-to-image synthesis, and class-conditional image synthesis, and demonstrates state-of-the-art or competitive performance.

## Main Contributions

[1]: https://arxiv.org/abs/2112.10752 "High-Resolution Image Synthesis with Latent Diffusion Models"
[2]: https://www.scribd.com/document/598177320/2112-10752 "2112.10752 | PDF | Data Compression | Applied Mathematics"
[3]: https://arxiv.org/pdf/2112.10752v2.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- **A new method for high-resolution image synthesis with latent diffusion models (LDMs)**, which are diffusion models (DMs) applied in the latent space of powerful pretrained autoencoders.
- **A novel cross-attention mechanism for LDMs**, which enables flexible and convolutional generation with various conditioning inputs such as text or bounding boxes.
- **A systematic analysis of the trade-off between complexity reduction and detail preservation in LDMs**, which shows how to choose suitable autoencoding models for different image synthesis tasks.
- **Extensive experiments on various image synthesis tasks**, such as inpainting, super-resolution, text-to-image synthesis, and class-conditional image synthesis, which demonstrate state-of-the-art or competitive performance of LDMs.

## Method Summary

[1]: https://arxiv.org/abs/2112.10752 "High-Resolution Image Synthesis with Latent Diffusion Models"
[2]: https://www.scribd.com/document/598177320/2112-10752 "2112.10752 | PDF | Data Compression | Applied Mathematics"
[3]: https://arxiv.org/pdf/2112.10752v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- **Latent diffusion models (LDMs)**: The paper defines LDMs as diffusion models (DMs) that operate in the latent space of pretrained autoencoders. The paper shows how to train LDMs by minimizing a variational lower bound on the log-likelihood of the data, which consists of two terms: a reconstruction loss and a KL-divergence term. The paper also shows how to sample from LDMs by reversing the diffusion process with a sequence of denoising steps.
- **Cross-attention for LDMs**: The paper introduces a novel cross-attention mechanism for LDMs, which allows them to condition the generation process on arbitrary inputs such as text or bounding boxes. The paper modifies the denoising function of LDMs to include a cross-attention layer that attends to the conditioning input and updates the latent state accordingly. The paper also shows how to use positional embeddings and masking techniques to handle different types of conditioning inputs.
- **Autoencoding models for LDMs**: The paper analyzes the trade-off between complexity reduction and detail preservation in LDMs, which depends on the choice of autoencoding models. The paper proposes to use different autoencoding models for different image synthesis tasks, such as VQGAN [17] for text-to-image synthesis, StyleGAN2 [28] for class-conditional image synthesis, and ESRGAN [72] for super-resolution. The paper also shows how to fine-tune the autoencoding models on specific datasets to improve the performance of LDMs.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the autoencoding model
AE = AutoEncoder() # e.g. VQGAN, StyleGAN2, ESRGAN
# Pretrain or fine-tune the autoencoding model on a specific dataset
AE.train(dataset)

# Define the latent diffusion model
LDM = LatentDiffusionModel()
# Define the denoising function with cross-attention
def denoise(latent, t, cond):
  # latent: the current latent state
  # t: the current diffusion step
  # cond: the conditioning input (e.g. text, bounding box)
  # Apply cross-attention between latent and cond
  latent = cross_attention(latent, cond)
  # Apply a convolutional network to update latent
  latent = conv_net(latent, t)
  return latent

# Train the latent diffusion model by minimizing the variational lower bound
LDM.train(dataset, AE, denoise)

# Sample from the latent diffusion model with a conditioning input
def sample(cond):
  # Initialize the latent state with Gaussian noise
  latent = torch.randn(batch_size, latent_dim)
  # Reverse the diffusion process from T to 1
  for t in reversed(range(1, T+1)):
    # Apply the denoising function with cross-attention
    latent = denoise(latent, t, cond)
    # Add Gaussian noise to latent
    latent = latent + torch.randn_like(latent) * sqrt(beta_t)
  # Decode the final latent state to an image
  image = AE.decode(latent)
  return image
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# Define the hyperparameters
batch_size = 16 # the batch size for training and sampling
latent_dim = 256 # the dimension of the latent space
num_steps = 1000 # the number of diffusion steps
beta_1 = 0.0001 # the initial noise level
beta_T = 0.02 # the final noise level
beta_schedule = "cosine" # the schedule for beta_t
lr = 0.0002 # the learning rate for Adam optimizer
num_epochs = 100 # the number of epochs for training

# Define the dataset and the data loader
dataset = ... # e.g. ImageNet, COCO, DIV2K, etc.
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the autoencoding model
AE = AutoEncoder() # e.g. VQGAN, StyleGAN2, ESRGAN
# Pretrain or fine-tune the autoencoding model on a specific dataset
AE.train(dataset)

# Define the latent diffusion model
class LatentDiffusionModel(nn.Module):
  def __init__(self):
    super(LatentDiffusionModel, self).__init__()
    # Define the convolutional network for denoising
    self.conv_net = ConvNet(latent_dim)
    # Define the cross-attention layer for conditioning
    self.cross_attn = CrossAttention(latent_dim)

  def forward(self, latent, t, cond):
    # latent: the current latent state (B x L)
    # t: the current diffusion step (scalar)
    # cond: the conditioning input (B x C x H x W)
    # Apply cross-attention between latent and cond
    latent = self.cross_attn(latent, cond) # (B x L)
    # Apply a convolutional network to update latent
    latent = self.conv_net(latent, t) # (B x L)
    return latent

# Define the denoising function with cross-attention
def denoise(latent, t, cond):
  return LDM(latent, t, cond)

# Define the beta_t schedule
def get_beta_t(t):
  if beta_schedule == "cosine":
    return beta_1 + (beta_T - beta_1) * (1 - math.cos(math.pi * t / num_steps)) / 2
  elif beta_schedule == "linear":
    return beta_1 + (beta_T - beta_1) * t / num_steps
  else:
    raise ValueError("Invalid beta schedule")

# Define the loss function (variational lower bound)
def loss_function(x, x_tilde, z_1, z_T):
  # x: the original image (B x C x H x W)
  # x_tilde: the reconstructed image (B x C x H x W)
  # z_1: the initial latent state (B x L)
  # z_T: the final latent state (B x L)
  # Compute the reconstruction loss (L2 norm)
  recon_loss = F.mse_loss(x_tilde, x)
  # Compute the KL-divergence term
  kl_loss = 0.5 * torch.mean(z_1 ** 2) - torch.log(torch.mean(torch.exp(-0.5 * z_T ** 2)))
  # Return the total loss
  return recon_loss + kl_loss

# Instantiate the latent diffusion model and the optimizer
LDM = LatentDiffusionModel()
optimizer = optim.Adam(LDM.parameters(), lr=lr)

# Train the latent diffusion model by minimizing the variational lower bound
for epoch in range(num_epochs):
  for i, (x, cond) in enumerate(data_loader):
    # x: the original image (B x C x H x W)
    # cond: the conditioning input (B x C' x H' x W')
    # Encode x to a latent state z_0
    z_0 = AE.encode(x) # (B x L)
    # Sample a random diffusion step t from {1,...,T}
    t = torch.randint(1, num_steps + 1) 
    # Compute beta_t and alpha_t
    beta_t = get_beta_t(t)
    alpha_t = 1 - beta_t / (1 - beta_1)
    # Corrupt z_0 with Gaussian noise to get z_t
    z_t = torch.sqrt(alpha_t) * z_0 + torch.randn_like(z_0) * torch.sqrt(1 - alpha_t) # (B x L)
    # Denoise z_t with cross-attention to get z_1
    z_1 = denoise(z_t, t, cond) # (B x L)
    # Decode z_1 to a reconstructed image x_tilde
    x_tilde = AE.decode(z_1) # (B x C x H x W)
    # Compute the loss
    loss = loss_function(x, x_tilde, z_1, z_t)
    # Backpropagate and update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # Print the loss
    print(f"Epoch {epoch}, Batch {i}, Loss {loss.item()}")

# Sample from the latent diffusion model with a conditioning input
def sample(cond):
  # Initialize the latent state with Gaussian noise
  latent = torch.randn(batch_size, latent_dim) # (B x L)
  # Reverse the diffusion process from T to 1
  for t in reversed(range(1, num_steps + 1)):
    # Apply the denoising function with cross-attention
    latent = denoise(latent, t, cond) # (B x L)
    # Add Gaussian noise to latent
    latent = latent + torch.randn_like(latent) * sqrt(get_beta_t(t)) # (B x L)
  # Decode the final latent state to an image
  image = AE.decode(latent) # (B x C x H x W)
  return image
```