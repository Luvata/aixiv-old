---
title: 2306.00396v1 Lightweight Vision Transformer with Bidirectional Interaction
date: 2023-06-01
---

# [Lightweight Vision Transformer with Bidirectional Interaction](http://arxiv.org/abs/2306.00396v1)

authors: Qihang Fan, Huaibo Huang, Xiaoqiang Zhou, Ran He


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00396v1.pdf "Lightweight Vision Transformer with Bidirectional Interaction - arXiv.org"
[2]: https://arxiv.org/abs/2306.00396 "Lightweight Vision Transformer with Bidirectional Interaction"
[3]: http://export.arxiv.org/abs/2306.00396 "[2306.00396] Lightweight Vision Transformer with Bidirectional Interaction"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a Fully Adaptive Self-Attention (FASA) mechanism for vision transformer to model the local and global information as well as the bidirectional interaction between them in context-aware ways. Based on FASA, the paper develops a family of lightweight vision backbones, Fully Adaptive Transformer (FAT) family.
- **Why**: The paper aims to improve the computational efficiency and performance of vision transformers by exploiting the bidirectional interaction between the local and global contexts of images, which is important in the human visual system.
- **How**: The paper employs self-modulated convolutions to adaptively extract local representation while utilizing self-attention in down-sampled space to extract global representation. Subsequently, it conducts a bidirectional adaptation process between local and global representation to model their interaction. In addition, the paper introduces a fine-grained downsampling strategy to enhance the down-sampled self-attention mechanism for finer-grained global perception capability. The paper evaluates FAT on multiple vision tasks and demonstrates its impressive performance.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel Fully Adaptive Self-Attention (FASA) mechanism for vision transformer that can model the local and global information as well as the bidirectional interaction between them in context-aware ways.
- It develops a family of lightweight vision backbones, Fully Adaptive Transformer (FAT) family, based on FASA, which achieves state-of-the-art performance on multiple vision tasks with fewer parameters and computational costs than existing ConvNets and Transformers.
- It introduces a fine-grained downsampling strategy to enhance the down-sampled self-attention mechanism for finer-grained global perception capability.

## Method Summary

The method section of the paper consists of three subsections:

- **Fully Adaptive Self-Attention**: This subsection introduces the FASA mechanism, which consists of three components: self-modulated convolution, down-sampled self-attention, and bidirectional adaptation. The self-modulated convolution adaptively extracts local representation by using a modulation vector that is learned from the input feature map. The down-sampled self-attention extracts global representation by applying self-attention in a lower-resolution space. The bidirectional adaptation conducts a two-way information exchange between the local and global representation to model their interaction and enhance their context-awareness.
- **Fine-grained Downsampling Strategy**: This subsection presents the fine-grained downsampling strategy, which aims to improve the down-sampled self-attention mechanism by preserving more spatial information and reducing aliasing effects. The strategy uses a combination of average pooling and strided convolution to downsample the feature map while maintaining its original size. The strategy also applies a residual connection to fuse the original and downsampled feature maps.
- **Fully Adaptive Transformer**: This subsection describes the architecture of FAT, which is based on FASA and the fine-grained downsampling strategy. FAT consists of a stem layer, a sequence of FASA blocks, and a classification head. FAT can be scaled by varying the number of FASA blocks, the number of channels, and the downsampling rate. The paper also introduces two variants of FAT: FAT-Small and FAT-Tiny, which have different configurations and trade-offs between performance and efficiency.

## Pseudo Code

Here is the detailed pseudo code to implement this paper:

```python
# Define the FASA block
class FASA(nn.Module):
  def __init__(self, in_channels, out_channels, num_heads, kernel_size, stride):
    super(FASA, self).__init__()
    # Define the self-modulated convolution layer
    self.conv = SelfModulatedConv(in_channels, out_channels, kernel_size)
    # Define the down-sampled self-attention layer
    self.attn = DownsampledSelfAttention(out_channels, num_heads, stride)
    # Define the bidirectional adaptation layer
    self.adapt = BidirectionalAdaptation(out_channels)
    # Define the layer normalization layer
    self.ln = nn.LayerNorm(out_channels)

  def forward(self, x):
    # Get the local representation from the self-modulated convolution layer
    local = self.conv(x)
    # Get the global representation from the down-sampled self-attention layer
    global = self.attn(local)
    # Get the adapted representation from the bidirectional adaptation layer
    adapted = self.adapt(local, global)
    # Apply the layer normalization and residual connection
    output = x + self.ln(adapted)
    return output

# Define the self-modulated convolution layer
class SelfModulatedConv(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size):
    super(SelfModulatedConv, self).__init__()
    # Define the convolution layer
    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=kernel_size//2)
    # Define the modulation vector layer
    self.mod = nn.Linear(in_channels, out_channels)

  def forward(self, x):
    # Apply the convolution layer
    conv_out = self.conv(x)
    # Get the modulation vector from the input feature map
    mod_vec = torch.mean(x, dim=[2, 3]) # global average pooling
    mod_vec = self.mod(mod_vec) # linear projection
    mod_vec = torch.sigmoid(mod_vec) # sigmoid activation
    mod_vec = mod_vec.unsqueeze(2).unsqueeze(3) # reshape to match conv_out
    # Apply the modulation vector to the convolution output
    mod_out = conv_out * mod_vec
    return mod_out

# Define the down-sampled self-attention layer
class DownsampledSelfAttention(nn.Module):
  def __init__(self, channels, num_heads, stride):
    super(DownsampledSelfAttention, self).__init__()
    # Define the downsampling layer
    self.downsample = FineGrainedDownsample(channels, stride)
    # Define the multi-head attention layer
    self.mha = nn.MultiheadAttention(channels, num_heads)
  
  def forward(self, x):
    # Downsample the input feature map
    down_x = self.downsample(x)
    # Reshape the feature map to a sequence of tokens
    b, c, h, w = down_x.shape
    tokens = down_x.permute(0, 2, 3, 1).reshape(b, h*w, c)
    # Apply the multi-head attention layer
    attn_out = self.mha(tokens, tokens, tokens)[0]
    # Reshape the output to a feature map
    attn_out = attn_out.reshape(b, h, w ,c).permute(0 ,3 ,1 ,2)
    return attn_out

# Define the fine-grained downsampling layer
class FineGrainedDownsample(nn.Module):
  def __init__(self, channels, stride):
    super(FineGrainedDownsample(self).__init__()
     # Define the average pooling layer
     self.avg_pool = nn.AvgPool2d(stride)
     # Define the strided convolution layer
     self.conv = nn.Conv2d(channels ,channels ,stride ,stride)

  def forward(self ,x):
     # Apply the average pooling and strided convolution layers
     avg_out = self.avg_pool(x)
     conv_out = self.conv(x)
     # Fuse the outputs with a residual connection
     fused_out = avg_out + conv_out 
     return fused_out

# Define the bidirectional adaptation layer 
class BidirectionalAdaptation(nn.Module):
  def __init__(self ,channels):
     super(BidirectionalAdaptation ,self).__init__()
     # Define the local-to-global adaptation layer 
     self.l2g_adapt = nn.Linear(channels ,channels) 
     # Define the global-to-local adaptation layer 
     self.g2l_adapt = nn.Linear(channels ,channels)

  def forward(self ,local ,global): 
     # Reshape the local and global feature maps to sequences of tokens 
     b ,c ,h ,w = local.shape 
     local_tokens = local.permute(0 ,2 ,3 ,1).reshape(b ,h*w ,c) 
     global_tokens = global.permute(0 ,2 ,3 ,1).reshape(b ,h*w ,c) 
     # Apply the local-to-global adaptation layer 
     l2g_out = self.l2g_adapt(local_tokens) 
     l2g_out = torch.sigmoid(l2g_out) # sigmoid activation 
     l2g_out = l2g_out * global_tokens # element-wise multiplication 
     # Apply the global-to-local adaptation layer 
     g2l_out = self.g2l_adapt(global_tokens) 
     g2l_out = torch.sigmoid(g2l_out) # sigmoid activation 
     g2l_out = g2l_out * local_tokens # element-wise multiplication 
     # Reshape the outputs to feature maps 
     l2g_out = l2g_out.reshape(b ,h ,w ,c).permute(0 ,3 ,1 ,2) 
     g2l_out = g2l_out.reshape(b ,h ,w ,c).permute(0 ,3 ,1 ,2) 
     # Fuse the outputs with a residual connection 
     fused_out = local + global + l2g_out + g2l_out
     return fused_out

# Define the FAT model
class FAT(nn.Module):
  def __init__(self, num_classes, num_blocks, channels, num_heads, kernel_size, stride):
    super(FAT, self).__init__()
    # Define the stem layer
    self.stem = nn.Conv2d(3, channels, 3, padding=1)
    # Define the sequence of FASA blocks
    self.blocks = nn.Sequential(*[FASA(channels, channels, num_heads, kernel_size, stride) for _ in range(num_blocks)])
    # Define the classification head
    self.head = nn.Sequential(
      nn.AdaptiveAvgPool2d(1), # global average pooling
      nn.Flatten(), # flatten to a vector
      nn.Linear(channels, num_classes) # linear projection to logits
    )

  def forward(self, x):
    # Apply the stem layer
    x = self.stem(x)
    # Apply the sequence of FASA blocks
    x = self.blocks(x)
    # Apply the classification head
    x = self.head(x)
    return x

# Define the FAT-Small and FAT-Tiny variants
def FAT_Small(num_classes):
  return FAT(num_classes, num_blocks=8, channels=64, num_heads=4, kernel_size=3, stride=4)

def FAT_Tiny(num_classes):
  return FAT(num_classes, num_blocks=4, channels=32, num_heads=4, kernel_size=3, stride=4)
```