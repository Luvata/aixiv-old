---
title: 2003.01279v3 Disrupting Deepfakes  Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems
date: 2020-03-02
---

# [Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems](http://arxiv.org/abs/2003.01279v3)

authors: Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff


## What, Why and How

[1]: https://arxiv.org/pdf/2003.01279v3.pdf "Disrupting Deepfakes: Adversarial Attacks Against ... - arXiv.org"
[2]: https://arxiv.org/abs/2003.01279 "[2003.01279] Disrupting Deepfakes: Adversarial Attacks Against ..."
[3]: http://export.arxiv.org/abs/2103.01279v3 "[2103.01279v3] The ideal-valued index of fibrations with total space a ..."

Here is a summary of the paper:

- **What**: The paper proposes a new method of disrupting deepfakes, which are manipulated images and videos of people's faces using deep learning. The method involves generating adversarial attacks against conditional image translation networks and facial manipulation systems, which can alter the output image in undesirable ways.
- **Why**: The paper aims to prevent malicious users from generating modified images of people without their consent, which can violate their privacy and cause harm. The paper also aims to improve the robustness of image translation networks against adversarial attacks.
- **How**: The paper introduces three main contributions: (1) class transferable adversarial attacks that generalize to different classes, which means that the attacker does not need to have knowledge about the conditioning class, (2) adversarial training for generative adversarial networks (GANs) as a first step towards robust image translation networks, and (3) a spread-spectrum adversarial attack, which evades blur defenses. The paper evaluates the proposed methods on several face manipulation systems and datasets, and shows that they can effectively disrupt the output images.

## Main Contributions

The contributions of this paper are:

- A new problem of disrupting deepfakes using adversarial attacks against conditional image translation networks and facial manipulation systems.
- Class transferable adversarial attacks that generalize to different classes, which means that the attacker does not need to have knowledge about the conditioning class.
- Adversarial training for generative adversarial networks (GANs) as a first step towards robust image translation networks.
- A spread-spectrum adversarial attack, which evades blur defenses.

## Method Summary

[1]: https://arxiv.org/pdf/2003.01279v3.pdf "Disrupting Deepfakes: Adversarial Attacks Against ... - arXiv.org"
[2]: https://arxiv.org/abs/2003.01279 "[2003.01279] Disrupting Deepfakes: Adversarial Attacks Against ..."
[3]: http://export.arxiv.org/abs/1603.01279v3 "[1603.01279v3] Ideal Weyl semimetals in the chalcopyrites CuTlSe2 ..."

Here is a summary of the method section of the paper:

- The paper describes three types of adversarial attacks against conditional image translation networks and facial manipulation systems: (1) white-box attacks, where the attacker has full access to the model parameters and architecture, (2) black-box attacks, where the attacker only has access to the input and output images, and (3) gray-box attacks, where the attacker has partial access to the model parameters or architecture.
- The paper introduces a class transferable adversarial attack that can disrupt the output image regardless of the conditioning class. The attack is based on the iterative fast gradient sign method (I-FGSM), which adds a small perturbation to the input image in the direction of the gradient of the loss function with respect to the input. The loss function is defined as the negative mean squared error between the output image and a random image from a different class. The paper shows that this attack can generalize to different classes and datasets, and can fool various face manipulation systems.
- The paper also proposes an adversarial training scheme for generative adversarial networks (GANs), which are commonly used for image translation tasks. The idea is to train the generator and the discriminator with both clean and adversarial images, so that they can learn to resist adversarial attacks. The paper shows that this scheme can improve the robustness of image translation networks against white-box and black-box attacks.
- Finally, the paper presents a spread-spectrum adversarial attack, which can evade blur defenses that are often used in gray-box scenarios. The attack is based on adding a high-frequency noise to the input image, which can disrupt the output image but is invisible to human eyes. The paper shows that this attack can bypass various blur filters and degrade the quality of the output image.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a conditional image translation network F and a conditioning class c
# Define a loss function L that measures the difference between the output image and a random image from a different class
# Define a blur filter B that can be applied to the input image

# White-box attack
def white_box_attack(x, F, c, L):
  # x is the input image
  # F is the conditional image translation network
  # c is the conditioning class
  # L is the loss function
  # Initialize an adversarial image x_adv as x
  # Repeat for a fixed number of iterations:
    # Compute the gradient of L with respect to x_adv
    # Update x_adv by adding a small perturbation in the direction of the gradient
    # Clip x_adv to the valid range of pixel values
  # Return x_adv

# Black-box attack
def black_box_attack(x, F, c):
  # x is the input image
  # F is the conditional image translation network
  # c is the conditioning class
  # Initialize an adversarial image x_adv as x
  # Repeat for a fixed number of iterations:
    # Generate a random noise n with the same shape as x
    # Update x_adv by adding n to x
    # Clip x_adv to the valid range of pixel values
    # If F(x_adv, c) is similar to F(x, c), then break the loop
  # Return x_adv

# Gray-box attack
def gray_box_attack(x, F, c, B):
  # x is the input image
  # F is the conditional image translation network
  # c is the conditioning class
  # B is the blur filter
  # Initialize an adversarial image x_adv as x
  # Generate a high-frequency noise n with the same shape as x
  # Update x_adv by adding n to x
  # Clip x_adv to the valid range of pixel values
  # Apply B to x_adv and store it as x_blur
  # Return x_blur

# Adversarial training for GANs
def adversarial_training(G, D):
  # G is the generator network
  # D is the discriminator network
  # Define a generator loss function LG and a discriminator loss function LD
  # Repeat for a fixed number of epochs:
    # Sample a batch of real images x and their corresponding classes c from the data distribution p_data(x, c)
    # Generate a batch of fake images y and their corresponding classes d from the generator G and a noise distribution p_noise(z, d)
    # Generate a batch of adversarial images x_adv using white_box_attack or black_box_attack on x and F = G
    # Compute LG using D(x, c), D(y, d) and D(x_adv, c)
    # Update G by minimizing LG using gradient descent or other optimization methods
    # Compute LD using D(x, c), D(y, d) and D(x_adv, c)
    # Update D by minimizing LD using gradient descent or other optimization methods

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from PIL import Image

# Define the hyperparameters
epsilon = 0.01 # The step size for the adversarial perturbation
alpha = 0.1 # The weight for the random noise in the black-box attack
sigma = 0.1 # The standard deviation for the high-frequency noise in the gray-box attack
num_iter = 10 # The number of iterations for the white-box and black-box attacks
num_epochs = 100 # The number of epochs for the adversarial training
batch_size = 32 # The batch size for the adversarial training
lr_g = 0.0002 # The learning rate for the generator network
lr_d = 0.0002 # The learning rate for the discriminator network
beta1 = 0.5 # The beta1 parameter for the Adam optimizer
beta2 = 0.999 # The beta2 parameter for the Adam optimizer

# Define a conditional image translation network F and a conditioning class c
# For example, we can use StarGAN [6] as F and CelebA [18] as c
F = StarGAN() # Load the StarGAN model from https://github.com/yunjey/stargan
c = CelebA() # Load the CelebA dataset from https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

# Define a loss function L that measures the difference between the output image and a random image from a different class
# For example, we can use mean squared error (MSE) as L
L = nn.MSELoss()

# Define a blur filter B that can be applied to the input image
# For example, we can use Gaussian blur as B
B = transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))

# White-box attack
def white_box_attack(x, F, c, L):
  # x is the input image (a PyTorch tensor of shape [3, H, W])
  # F is the conditional image translation network (a PyTorch module)
  # c is the conditioning class (a PyTorch tensor of shape [C])
  # L is the loss function (a PyTorch module)
  # Initialize an adversarial image x_adv as x
  x_adv = x.clone()
  # Repeat for a fixed number of iterations:
  for i in range(num_iter):
    # Compute the gradient of L with respect to x_adv
    x_adv.requires_grad_(True) # Set x_adv to require gradient computation
    y = F(x_adv, c) # Apply F to x_adv with c as the conditioning class
    r = torch.randint(0, C, size=(1,)) # Sample a random class index from [0, C-1]
    z = torch.randn_like(y) # Sample a random image from a normal distribution with zero mean and unit variance
    z[r] = y[r] # Replace the random class index channel of z with y's corresponding channel
    loss = -L(y, z) # Compute the negative MSE between y and z
    loss.backward() # Backpropagate the loss to get the gradient of x_adv
    grad = x_adv.grad.data # Get the gradient data of x_adv
    # Update x_adv by adding a small perturbation in the direction of the gradient sign
    x_adv = x_adv + epsilon * grad.sign()
    # Clip x_adv to the valid range of pixel values [0, 1]
    x_adv = torch.clamp(x_adv, 0, 1)
    x_adv.requires_grad_(False) # Set x_adv to not require gradient computation
  # Return x_adv (a PyTorch tensor of shape [3, H, W])
  return x_adv

# Black-box attack
def black_box_attack(x, F, c):
  # x is the input image (a PyTorch tensor of shape [3, H, W])
  # F is the conditional image translation network (a PyTorch module)
  # c is the conditioning class (a PyTorch tensor of shape [C])
  # Initialize an adversarial image x_adv as x
  x_adv = x.clone()
  # Repeat for a fixed number of iterations:
  for i in range(num_iter):
    # Generate a random noise n with the same shape as x from a uniform distribution [-alpha/2, alpha/2]
    n = torch.rand_like(x) * alpha - alpha / 2
    # Update x_adv by adding n to x
    x_adv = x_adv + n
    # Clip x_adv to the valid range of pixel values [0, 1]
    x_adv = torch.clamp(x_adv, 0, 1)
    # If F(x_adv, c) is similar to F(x, c), then break the loop
    y = F(x, c) # Apply F to x with c as the conditioning class
    y_adv = F(x_adv, c) # Apply F to x_adv with c as the conditioning class
    sim = torch.cosine_similarity(y, y_adv, dim=0) # Compute the cosine similarity between y and y_adv
    if sim > 0.99: # If the similarity is greater than a threshold (e.g. 0.99), then stop the attack
      break
  # Return x_adv (a PyTorch tensor of shape [3, H, W])
  return x_adv

# Gray-box attack
def gray_box_attack(x, F, c, B):
  # x is the input image (a PyTorch tensor of shape [3, H, W])
  # F is the conditional image translation network (a PyTorch module)
  # c is the conditioning class (a PyTorch tensor of shape [C])
  # B is the blur filter (a PyTorch module)
  # Initialize an adversarial image x_adv as x
  x_adv = x.clone()
  # Generate a high-frequency noise n with the same shape as x from a normal distribution with zero mean and sigma standard deviation
  n = torch.randn_like(x) * sigma
  # Update x_adv by adding n to x
  x_adv = x_adv + n
  # Clip x_adv to the valid range of pixel values [0, 1]
  x_adv = torch.clamp(x_adv, 0, 1)
  # Apply B to x_adv and store it as x_blur
  x_blur = B(x_adv)
  # Return x_blur (a PyTorch tensor of shape [3, H, W])
  return x_blur

# Adversarial training for GANs
def adversarial_training(G, D):
  # G is the generator network (a PyTorch module)
  # D is the discriminator network (a PyTorch module)
  # Define a generator loss function LG and a discriminator loss function LD
  # For example, we can use binary cross entropy (BCE) as LG and LD
  LG = nn.BCELoss()
  LD = nn.BCELoss()
  # Define an optimizer for G and D using Adam algorithm
  optim_G = optim.Adam(G.parameters(), lr=lr_g, betas=(beta1, beta2))
  optim_D = optim.Adam(D.parameters(), lr=lr_d, betas=(beta1, beta2))
  # Repeat for a fixed number of epochs:
  for epoch in range(num_epochs):
    # Sample a batch of real images x and their corresponding classes c from the data distribution p_data(x, c)
    # For example, we can use CelebA dataset as p_data(x, c)
    data_loader = torch.utils.data.DataLoader(c, batch_size=batch_size, shuffle=True) # Create a data loader for CelebA dataset
    for batch_idx, (x, c) in enumerate(data_loader): # Iterate over the batches of images and classes
      # Generate a batch of fake images y and their corresponding classes d from the generator G and a noise distribution p_noise(z, d)
      z = torch.randn(batch_size, G.nz) # Sample a batch of noise vectors z from a normal distribution with zero mean and unit variance
      d = torch.randint(0, C, size=(batch_size,)) # Sample a batch of random class indices d from [0, C-1]
      y = G(z, d) # Apply G to z and d to generate a batch of fake images y
      # Generate a batch of adversarial images x_adv using white_box_attack or black_box_attack on x and F = G
      x_adv = white_box_attack(x, G, c) # Use white-box attack on x and G as an example
      # Compute LG using D(x, c), D(y, d) and D(x_adv, c)
      real_labels = torch.ones(batch_size) # Create a batch of real labels (1)
      fake_labels = torch.zeros(batch_size) # Create a batch of fake labels (0)
      real_output = D(x, c) # Apply D to real images and classes
      fake_output = D(y.detach(), d) # Apply D to fake images and classes (detach y from the computation graph)
      adv_output = D(x_adv.detach(), c