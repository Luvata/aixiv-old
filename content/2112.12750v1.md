---
title: 2112.12750v1 SLIP  Self-supervision meets Language-Image Pre-training
date: 2021-12-13
---

# [SLIP: Self-supervision meets Language-Image Pre-training](http://arxiv.org/abs/2112.12750v1)

authors: Norman Mu, Alexander Kirillov, David Wagner, Saining Xie


## What, Why and How

[1]: https://arxiv.org/pdf/2112.12750v1.pdf "Abstract arXiv:2112.12750v1 [cs.CV] 23 Dec 2021"
[2]: https://arxiv.org/abs/2112.12750v1 "SLIP: Self-supervision meets Language-Image Pre-training"
[3]: http://export.arxiv.org/abs/2112.12750 "[2112.12750] SLIP: Self-supervision meets Language-Image Pre-training"

Here is a summary of the paper:

```
What: The paper introduces SLIP, a multi-task learning framework for combining self-supervised learning and CLIP pre-training for visual representation learning.

Why: The paper aims to leverage the benefits of both self-supervision and language supervision for improving visual recognition tasks.

How: The paper pre-trains Vision Transformers with a combination of contrastive loss and CLIP loss on a large-scale image-text dataset. The paper evaluates the performance of SLIP on zero-shot transfer, linear classification, and end-to-end finetuning on ImageNet and other datasets, and compares it with CLIP and self-supervised learning baselines.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2112.12750v1.pdf "Abstract arXiv:2112.12750v1 [cs.CV] 23 Dec 2021"
[2]: https://arxiv.org/abs/2112.12750v1 "SLIP: Self-supervision meets Language-Image Pre-training"
[3]: http://export.arxiv.org/abs/2112.12750 "[2112.12750] SLIP: Self-supervision meets Language-Image Pre-training"

Some of the contributions of this paper are:

- It proposes SLIP, a novel framework that combines self-supervised learning and CLIP pre-training for visual representation learning.
- It shows that SLIP outperforms both self-supervision and language supervision on zero-shot transfer, linear classification, and end-to-end finetuning on ImageNet and other datasets.
- It conducts extensive ablation studies and analysis to validate the effectiveness of SLIP and its components.

## Method Summary

[1]: https://arxiv.org/pdf/2112.12750v1.pdf "Abstract arXiv:2112.12750v1 [cs.CV] 23 Dec 2021"
[2]: https://arxiv.org/abs/2112.12750v1 "SLIP: Self-supervision meets Language-Image Pre-training"
[3]: http://export.arxiv.org/abs/2112.12750 "[2112.12750] SLIP: Self-supervision meets Language-Image Pre-training"

Here is a summary of the method section:

```
The method section consists of four subsections: SLIP pre-training, zero-shot transfer, linear classification, and end-to-end finetuning.

- SLIP pre-training: The authors describe how they combine self-supervised learning and CLIP pre-training in a multi-task framework. They use Vision Transformers as the backbone model and train it on a large-scale image-text dataset (YFCC15M) with two losses: a contrastive loss that maximizes the agreement between different views of the same image, and a CLIP loss that maximizes the agreement between images and captions. They also introduce a temperature scaling scheme to balance the two losses and a hard negative mining strategy to improve the contrastive learning efficiency.
- Zero-shot transfer: The authors evaluate the quality of the learned representations by applying them to unseen classes without any finetuning. They use the CLIP classifier to predict the class labels from natural language descriptions and report the top-1 accuracy on various datasets, such as ImageNet, CIFAR-10, CIFAR-100, etc.
- Linear classification: The authors evaluate the transferability of the learned representations by training a linear classifier on top of the frozen features. They report the top-1 accuracy on ImageNet and other datasets, such as Places365, Food101, etc.
- End-to-end finetuning: The authors evaluate the adaptability of the learned representations by finetuning the entire model on downstream tasks. They report the results on image classification, object detection, and instance segmentation tasks using various datasets, such as ImageNet, COCO, etc.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2112.12750v1.pdf "Abstract arXiv:2112.12750v1 [cs.CV] 23 Dec 2021"
[2]: https://arxiv.org/abs/2112.12750v1 "SLIP: Self-supervision meets Language-Image Pre-training"
[3]: http://export.arxiv.org/abs/2112.12750 "[2112.12750] SLIP: Self-supervision meets Language-Image Pre-training"

Here is a high-level pseudo code for the paper:

```
# Define the model architecture
model = VisionTransformer()

# Define the loss functions
contrastive_loss = InfoNCE()
clip_loss = Softmax()

# Define the temperature scaling factor
temperature = 1.0

# Define the hard negative mining ratio
hard_negative_ratio = 0.5

# Load the image-text dataset
dataset = YFCC15M()

# Pre-train the model on the dataset
for each batch of (images, captions) in dataset:
  # Generate two views of each image by applying random augmentations
  images_1, images_2 = augment(images)

  # Encode the images and captions using the model
  image_features_1 = model(images_1)
  image_features_2 = model(images_2)
  caption_features = model(captions)

  # Compute the contrastive loss between the two views of each image
  contrastive_loss_1 = contrastive_loss(image_features_1, image_features_2)
  contrastive_loss_2 = contrastive_loss(image_features_2, image_features_1)

  # Compute the CLIP loss between the images and captions
  clip_loss_1 = clip_loss(image_features_1, caption_features)
  clip_loss_2 = clip_loss(image_features_2, caption_features)

  # Combine the two losses with temperature scaling
  total_loss = (contrastive_loss_1 + contrastive_loss_2 + clip_loss_1 + clip_loss_2) / temperature

  # Perform hard negative mining by selecting a subset of hard negatives for each positive pair
  hard_negatives = select_hard_negatives(total_loss, hard_negative_ratio)

  # Update the model parameters using gradient descent on the hard negatives
  model.update(hard_negatives)

# Evaluate the model on zero-shot transfer, linear classification, and end-to-end finetuning tasks
evaluate(model)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2112.12750v1.pdf "Abstract arXiv:2112.12750v1 [cs.CV] 23 Dec 2021"
[2]: https://arxiv.org/abs/2112.12750v1 "SLIP: Self-supervision meets Language-Image Pre-training"
[3]: http://export.arxiv.org/abs/2112.12750 "[2112.12750] SLIP: Self-supervision meets Language-Image Pre-training"

Here is a detailed pseudo code for the paper:

```
# Import the necessary libraries
import torch
import torchvision
import clip
import timm

# Define the model architecture
model = timm.create_model("vit_base_patch16_224", pretrained=False)
model.head = clip.VisionTransformerHead(768, 512)

# Define the loss functions
contrastive_loss = torch.nn.CrossEntropyLoss()
clip_loss = torch.nn.CrossEntropyLoss()

# Define the temperature scaling factor
temperature = 1.0

# Define the hard negative mining ratio
hard_negative_ratio = 0.5

# Define the optimizer and the learning rate scheduler
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# Load the image-text dataset
dataset = torchvision.datasets.YFCC15M(root="./data", download=True, transform=torchvision.transforms.ToTensor())
dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)

# Pre-train the model on the dataset
for epoch in range(100):
  for i, (images, captions) in enumerate(dataloader):
    # Generate two views of each image by applying random augmentations
    images_1 = torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(0.4, 0.4, 0.4, 0.1), torchvision.transforms.RandomGrayscale(p=0.2), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.RandomRotation(10), torchvision.transforms.RandomResizedCrop(224)], p=0.8)(images)
    images_2 = torchvision.transforms.RandomApply([torchvision.transforms.ColorJitter(0.4, 0.4, 0.4, 0.1), torchvision.transforms.RandomGrayscale(p=0.2), torchvision.transforms.RandomHorizontalFlip(), torchvision.transforms.RandomRotation(10), torchvision.transforms.RandomResizedCrop(224)], p=0.8)(images)

    # Encode the images and captions using the model
    image_features_1 = model.encode_image(images_1)
    image_features_2 = model.encode_image(images_2)
    caption_features = model.encode_text(captions)

    # Compute the contrastive loss between the two views of each image
    contrastive_loss_1 = contrastive_loss(image_features_1 @ image_features_2.T / temperature, torch.arange(len(images)))
    contrastive_loss_2 = contrastive_loss(image_features_2 @ image_features_1.T / temperature, torch.arange(len(images)))

    # Compute the CLIP loss between the images and captions
    clip_loss_1 = clip_loss(image_features_1 @ caption_features.T / temperature, torch.arange(len(images)))
    clip_loss_2 = clip_loss(image_features_2 @ caption_features.T / temperature, torch.arange(len(images)))

    # Combine the two losses with temperature scaling
    total_loss = (contrastive_loss_1 + contrastive_loss_2 + clip_loss_1 + clip_loss_2) / temperature

    # Perform hard negative mining by selecting a subset of hard negatives for each positive pair
    hard_negatives = select_hard_negatives(total_loss, hard_negative_ratio)

    # Update the model parameters using gradient descent on the hard negatives
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    scheduler.step()

    # Print the loss and the learning rate
    print(f"Epoch {epoch}, Batch {i}, Loss: {total_loss.item()}, LR: {optimizer.param_groups[0]['lr']}")

# Save the model
torch.save(model.state_dict(), "slip.pth")

# Define a function to select hard negatives for each positive pair
def select_hard_negatives(loss, ratio):
  # Sort the loss in descending order and get the indices
  sorted_loss, indices = torch.sort(loss, descending=True)

  # Compute the number of hard negatives to keep based on the ratio
  num_hard_negatives = int(len(loss) * ratio)

  # Return the subset of hard negatives using the indices
  return loss[indices[:num_hard_negatives]]

# Evaluate the model on zero-shot transfer, linear classification, and end-to-end finetuning tasks
evaluate(model)
```