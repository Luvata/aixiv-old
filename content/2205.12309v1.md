---
title: 2205.12309v1 Structured Prompt Tuning
date: 2022-05-13
---

# [Structured Prompt Tuning](http://arxiv.org/abs/2205.12309v1)

authors: Chi-Liang Liu, Hung-yi Lee, Wen-tau Yih


## What, Why and How

[1]: https://arxiv.org/abs/2205.12309v1 "[2205.12309v1] Structured Prompt Tuning - arXiv.org"
[2]: https://arxiv.org/pdf/2205.12309v1.pdf "Structured Prompt Tuning - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.12309v1 "[2305.12309v1] Uniform Pricing vs Pay as Bid in 100%-Renewables ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a method called **structured prompt tuning**, which improves the performance of prompt tuning by generating soft prompt embeddings through a hypernetwork that takes a task embedding as input.
- **Why**: The paper argues that standard prompt tuning is limited in its form of interacting with the language models and does not exploit the structure among soft prompt embeddings. Structured prompt tuning allows more flexibility in model design and can be applied to both single-task and multi-task training settings.
- **How**: The paper shows that structured prompt tuning subsumes standard prompt tuning as a special case and provides different hypernetwork architectures, such as linear layer, low rank linear layer or a multilayer perceptron, to generate soft prompt embeddings. The paper evaluates the proposed method on the GLUE benchmark and demonstrates its effectiveness and robustness compared to standard prompt tuning.

## Main Contributions

[1]: https://arxiv.org/abs/2205.12309v1 "[2205.12309v1] Structured Prompt Tuning - arXiv.org"
[2]: https://arxiv.org/pdf/2205.12309v1.pdf "Structured Prompt Tuning - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.12309v1 "[2305.12309v1] Uniform Pricing vs Pay as Bid in 100%-Renewables ..."

According to the paper at [^1^][1], the main contributions are:

- **Proposing a novel method** called structured prompt tuning, which generates soft prompt embeddings through a hypernetwork that takes a task embedding as input.
- **Generalizing the standard prompt tuning** method as a special case of structured prompt tuning with a particular hypernetwork architecture.
- **Providing different hypernetwork architectures** to generate soft prompt embeddings, such as linear layer, low rank linear layer or a multilayer perceptron.
- **Evaluating the proposed method** on the GLUE benchmark and showing its effectiveness and robustness compared to standard prompt tuning.

## Method Summary

[1]: https://arxiv.org/abs/2205.12309v1 "[2205.12309v1] Structured Prompt Tuning - arXiv.org"
[2]: https://arxiv.org/pdf/2205.12309v1.pdf "Structured Prompt Tuning - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.12309v1 "[2305.12309v1] Uniform Pricing vs Pay as Bid in 100%-Renewables ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces the **notation and background** of prompt tuning, where a sequence of tunable embeddings is prepended to the input and fed into a frozen language model. The paper defines the task embedding, the soft prompt embedding and the hypernetwork that generates the soft prompt embedding.
- The paper shows that **standard prompt tuning** can be seen as a special case of structured prompt tuning, where the hypernetwork is a linear layer that maps the task embedding to the soft prompt embedding. The paper also discusses the limitations of standard prompt tuning, such as the lack of structure among soft prompt embeddings and the sensitivity to learning rate.
- The paper presents different **hypernetwork architectures** for structured prompt tuning, such as low rank linear layer, multilayer perceptron and convolutional neural network. The paper explains how these architectures can impose implicit structures among soft prompt embeddings and reduce the number of parameters.
- The paper describes the **training procedure** for structured prompt tuning, which involves initializing the task embedding randomly or with a natural language description, optimizing the task embedding and the hypernetwork parameters with gradient descent, and using early stopping based on validation performance.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the frozen language model LM
LM = load_pretrained_model()

# Define the hypernetwork H
H = HyperNetwork()

# Define the task embedding e_t
e_t = initialize_task_embedding()

# Define the loss function L
L = CrossEntropyLoss()

# Define the optimizer O
O = Adam([H.parameters(), e_t])

# Define the validation metric M
M = Accuracy()

# Define the early stopping criterion C
C = Patience(5)

# Loop over the training epochs
for epoch in range(max_epochs):

  # Loop over the training batches
  for batch in train_loader:

    # Get the input x and the label y
    x, y = batch

    # Generate the soft prompt embedding p_t using H and e_t
    p_t = H(e_t)

    # Prepend p_t to x and get the output logits z from LM
    z = LM(concat(p_t, x))

    # Compute the loss L using z and y
    loss = L(z, y)

    # Update H and e_t using O and loss
    O.zero_grad()
    loss.backward()
    O.step()

  # Loop over the validation batches
  for batch in val_loader:

    # Get the input x and the label y
    x, y = batch

    # Generate the soft prompt embedding p_t using H and e_t
    p_t = H(e_t)

    # Prepend p_t to x and get the output logits z from LM
    z = LM(concat(p_t, x))

    # Compute the validation metric M using z and y
    metric = M(z, y)

  # Check the early stopping criterion C using metric
  if C.check(metric):

    # Break the training loop
    break

# Return H and e_t as the final model
return H, e_t

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import transformers
import datasets

# Define the frozen language model LM
LM = transformers.AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
LM.eval()
LM.requires_grad_(False)

# Define the hypernetwork H
class HyperNetwork(nn.Module):

  def __init__(self, task_embedding_size, prompt_embedding_size, hidden_size, num_layers):
    super().__init__()

    # Define the task embedding size d_t
    self.d_t = task_embedding_size

    # Define the prompt embedding size d_p
    self.d_p = prompt_embedding_size

    # Define the hidden size d_h
    self.d_h = hidden_size

    # Define the number of layers n
    self.n = num_layers

    # Define the linear layers W_1, ..., W_n
    self.W = nn.ModuleList([nn.Linear(self.d_t, self.d_h * self.d_p) for _ in range(self.n)])

    # Define the activation function sigma
    self.sigma = nn.ReLU()

  def forward(self, e_t):
    # Input: task embedding e_t of shape (d_t,)
    # Output: soft prompt embedding p_t of shape (n * d_p,)

    # Initialize p_t as e_t
    p_t = e_t

    # Loop over the linear layers W_1, ..., W_n
    for i in range(self.n):

      # Apply W_i to p_t and reshape the result to (d_h, d_p)
      h_i = self.W[i](p_t).view(self.d_h, self.d_p)

      # Apply sigma to h_i and flatten the result to (d_h * d_p,)
      p_i = self.sigma(h_i).flatten()

      # Concatenate p_i to p_t
      p_t = torch.cat([p_t, p_i])

    # Return p_t
    return p_t

# Create an instance of HyperNetwork with task embedding size 32, prompt embedding size 768, hidden size 64 and number of layers 3
H = HyperNetwork(32, 768, 64, 3)

# Define the task embedding e_t and initialize it randomly
e_t = torch.randn(32)

# Define the loss function L as cross entropy loss
L = nn.CrossEntropyLoss()

# Define the optimizer O as Adam with learning rate 0.001 and weight decay 0.01
O = torch.optim.Adam([H.parameters(), e_t], lr=0.001, weight_decay=0.01)

# Define the validation metric M as accuracy
M = datasets.load_metric("accuracy")

# Define the early stopping criterion C as patience of 5 epochs
C = datasets.load_metric("patience", patience=5)

# Load the GLUE dataset for SST-2 task
dataset = datasets.load_dataset("glue", "sst2")

# Split the dataset into train and validation sets
train_set = dataset["train"]
val_set = dataset["validation"]

# Create data loaders for train and validation sets with batch size 32 and shuffle for train set
train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=32)

# Loop over the training epochs
for epoch in range(max_epochs):

  # Set H to train mode
  H.train()

  # Loop over the training batches
  for batch in train_loader:

    # Get the input x and the label y from batch and move them to device (cpu or gpu)
    x = batch["sentence"].to(device)
    y = batch["label"].to(device)

    # Generate the soft prompt embedding p_t using H and e_t and move it to device
    p_t = H(e_t).to(device)

    # Prepend p_t to x and get the output logits z from LM
    z = LM(input_ids=torch.cat([p_t.unsqueeze(0), x], dim=1))[0]

    # Compute the loss L using z and y
    loss = L(z, y)

    # Update H and e_t using O and loss
    O.zero_grad()
    loss.backward()
    O.step()

  # Set H to eval mode
  H.eval()

  # Initialize the validation metric M to zero
  M.reset()

  # Loop over the validation batches
  for batch in val_loader:

    # Get the input x and the label y from batch and move them to device (cpu or gpu)
    x = batch["sentence"].to(device)
    y = batch["label"].to(device)

    # Generate the soft prompt embedding p_t using H and e_t and move it to device
    p_t = H(e_t).to(device)

    # Prepend p_t to x and get the output logits z from LM
    z = LM(input_ids=torch.cat([p_t.unsqueeze(0), x], dim=1))[0]

    # Compute the validation metric M using z and y
    M.add_batch(predictions=torch.argmax(z, dim=1), references=y)

  # Get the validation metric score
  score = M.compute()

  # Check the early stopping criterion C using score
  if C.check(score):

    # Break the training loop
    break

# Return H and e_t as the final model
return H, e_t

```