---
title: 2305.00650v2 Discover and Cure  Concept-aware Mitigation of Spurious Correlation
date: 2023-05-01
---

# [Discover and Cure: Concept-aware Mitigation of Spurious Correlation](http://arxiv.org/abs/2305.00650v2)

authors: Shirley Wu, Mert Yuksekgonul, Linjun Zhang, James Zou


## What, Why and How

[1]: https://arxiv.org/pdf/2305.00650v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2305.00650 "[2305.00650] Discover and Cure: Concept-aware Mitigation of Spurious ..."
[3]: http://export.arxiv.org/pdf/2305.00650 "PDF for 2305.00650 - export.arxiv.org"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes an interpretable framework, **Discover and Cure (DISC)**, to mitigate spurious correlations in deep neural networks[^1^][1] [^2^][2].
- **Why**: Spurious correlations are common in real-world data analysis and can hinder generalization beyond training environments. For example, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds[^1^][1] [^2^][2]. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process[^1^][1] [^2^][2].
- **How**: With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation[^1^][1] [^2^][2]. DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task and a skin-lesion classification task by 7.5% and 9.6%, respectively[^1^][1] [^2^][2]. Additionally, the paper offers theoretical analysis and guarantees to understand the benefits of models trained by DISC[^1^][1] [^2^][2].


## Main Contributions

[1]: https://arxiv.org/abs/2305.00650 "[2305.00650] Discover and Cure: Concept-aware Mitigation of Spurious ..."
[2]: https://arxiv.org/pdf/2305.00650v2.pdf "Abstract - arXiv.org"
[3]: http://export.arxiv.org/pdf/2305.00650 "PDF for 2305.00650 - export.arxiv.org"

According to the paper, the main contributions are:

- **A novel framework, DISC**, that mitigates spurious correlations in deep neural networks by discovering and curing spurious attributes with human-interpretable concepts[^1^][1] [^2^][2].
- **A class-level metric, Spuriousness**, that measures the degree of spurious correlation between concepts and classes across different environments[^1^][1] [^2^][2].
- **A data intervention method, Conceptual Intervention**, that modifies the training data by adding or removing concepts to reduce spurious correlation[^1^][1] [^2^][2].
- **A theoretical analysis and guarantees** for the benefits of models trained by DISC in terms of generalization error and robustness[^1^][1] [^2^][2].
- **Empirical evaluations** on two real-world datasets, showing that DISC outperforms the state-of-the-art methods on generalization and interpretability[^1^][1] [^2^][2].


## Method Summary

[1]: https://arxiv.org/abs/2305.00650 "[2305.00650] Discover and Cure: Concept-aware Mitigation of Spurious ..."
[2]: https://arxiv.org/pdf/2305.00650v2.pdf "Abstract - arXiv.org"
[3]: http://export.arxiv.org/pdf/2305.00650 "PDF for 2305.00650 - export.arxiv.org"

Here is a summary of the method section of the paper:

- The paper introduces the problem formulation and the notation for spurious correlation mitigation in Section 2[^1^][1] [^2^][2].
- The paper presents the proposed framework, DISC, in Section 3[^1^][1] [^2^][2]. It consists of two main components: spuriousness discovery and conceptual intervention.
- The paper describes the spuriousness discovery module in Section 3.1[^1^][1] [^2^][2]. It uses a concept extractor to obtain human-interpretable concepts from images, and then computes a class-level metric, spuriousness, to measure the degree of spurious correlation between concepts and classes across different environments.
- The paper explains the conceptual intervention module in Section 3.2[^1^][1] [^2^][2]. It modifies the training data by adding or removing concepts based on their spuriousness scores, and then trains a model on the intervened data to reduce spurious correlation.
- The paper provides theoretical analysis and guarantees for the generalization error and robustness of models trained by DISC in Section 4[^1^][1] [^2^][2]. It shows that DISC can achieve lower generalization error than ERM and invariant learning under certain conditions, and that DISC can improve the robustness of models against concept shifts.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a set of training environments E, a concept extractor C, a spuriousness threshold t
# Output: a model f that is robust to spurious correlations

# Initialize an ERM model f_0 by training on all environments
f_0 = train_ERM(E)

# Initialize an empty set of intervened environments E'
E' = {}

# Repeat until no more spurious concepts are found
while True:

  # Compute the spuriousness scores for all concepts and classes across environments
  S = compute_spuriousness(E, C, f_0)

  # Find the most spurious concept-class pair (c*, y*) that exceeds the threshold t
  (c*, y*) = find_most_spurious(S, t)

  # If no such pair exists, break the loop
  if (c*, y*) == None:
    break

  # For each environment e in E
  for e in E:

    # Create a copy of e
    e' = copy(e)

    # If c* is positively correlated with y* in e, remove c* from the images of class y* in e'
    if S[c*, y*, e] > 0:
      e'[y*] = remove_concept(e[y*], c*, C)

    # If c* is negatively correlated with y* in e, add c* to the images of class y* in e'
    if S[c*, y*, e] < 0:
      e'[y*] = add_concept(e[y*], c*, C)

    # Add the intervened environment e' to E'
    E'.add(e')

  # Train a new model f_1 on the intervened environments E'
  f_1 = train_ERM(E')

  # Update the model f_0 with f_1
  f_0 = f_1

# Return the final model f_0
return f_0
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torchvision
import cv2

# Define the hyperparameters
num_epochs = 10 # number of epochs for training
batch_size = 32 # batch size for training
lr = 0.01 # learning rate for training
t = 0.5 # spuriousness threshold for intervention

# Define the concept extractor C as a pre-trained ResNet-18 model
C = torchvision.models.resnet18(pretrained=True)

# Define the model f as a ResNet-50 model with random weights
f = torchvision.models.resnet50(pretrained=False)

# Define the loss function as cross-entropy loss
loss_fn = torch.nn.CrossEntropyLoss()

# Define the optimizer as stochastic gradient descent with momentum
optimizer = torch.optim.SGD(f.parameters(), lr=lr, momentum=0.9)

# Load the training environments E as a list of PyTorch datasets
E = load_environments()

# Initialize an ERM model f_0 by training on all environments
f_0 = train_ERM(E, f, loss_fn, optimizer, num_epochs, batch_size)

# Initialize an empty set of intervened environments E'
E' = []

# Repeat until no more spurious concepts are found
while True:

  # Compute the spuriousness scores for all concepts and classes across environments
  S = compute_spuriousness(E, C, f_0)

  # Find the most spurious concept-class pair (c*, y*) that exceeds the threshold t
  (c*, y*) = find_most_spurious(S, t)

  # If no such pair exists, break the loop
  if (c*, y*) == None:
    break

  # For each environment e in E
  for e in E:

    # Create a copy of e
    e' = copy(e)

    # If c* is positively correlated with y* in e, remove c* from the images of class y* in e'
    if S[c*, y*, e] > 0:
      e'[y*] = remove_concept(e[y*], c*, C)

    # If c* is negatively correlated with y* in e, add c* to the images of class y* in e'
    if S[c*, y*, e] < 0:
      e'[y*] = add_concept(e[y*], c*, C)

    # Add the intervened environment e' to E'
    E'.append(e')

  # Train a new model f_1 on the intervened environments E'
  f_1 = train_ERM(E', f, loss_fn, optimizer, num_epochs, batch_size)

  # Update the model f_0 with f_1
  f_0 = f_1

# Return the final model f_0
return f_0

# Define a function to load the training environments as PyTorch datasets
def load_environments():
  # Load the image paths and labels from a CSV file
  data = pd.read_csv('data.csv')
  
  # Split the data into different environments based on a column value
  envs = data.groupby('env')

  # Create an empty list to store the datasets
  datasets = []

  # For each environment
  for env in envs:

    # Get the image paths and labels as numpy arrays
    X = env['path'].to_numpy()
    y = env['label'].to_numpy()

    # Create a custom PyTorch dataset class that loads and transforms the images
    class CustomDataset(torch.utils.data.Dataset):
      def __init__(self, X, y):
        self.X = X
        self.y = y

      def __len__(self):
        return len(self.X)

      def __getitem__(self, index):
        # Load the image from the path
        img = cv2.imread(self.X[index])

        # Convert the image to RGB format
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Resize the image to 224 x 224 pixels
        img = cv2.resize(img, (224, 224))

        # Convert the image to a PyTorch tensor and normalize it
        img = torchvision.transforms.ToTensor()(img)
        img = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)

        # Get the label as an integer
        label = int(self.y[index])

        # Return the image and label pair as a tuple
        return (img, label)

    # Create a dataset object from the custom class
    dataset = CustomDataset(X, y)

    # Add the dataset to the list
    datasets.append(dataset)

  # Return the list of datasets
  return datasets

# Define a function to train an ERM model on a list of environments
def train_ERM(E, f, loss_fn, optimizer, num_epochs, batch_size):
  # Create an empty list to store the losses
  losses = []

  # For each epoch
  for epoch in range(num_epochs):

    # Initialize the epoch loss as zero
    epoch_loss = 0

    # Shuffle the order of the environments
    np.random.shuffle(E)

    # For each environment in E
    for e in E:

      # Create a data loader for the environment with the given batch size
      loader = torch.utils.data.DataLoader(e, batch_size=batch_size, shuffle=True)

      # For each batch of images and labels from the loader
      for X, y in loader:

        # Move the images and labels to the device (CPU or GPU)
        X = X.to(device)
        y = y.to(device)

        # Forward pass: compute the logits from the model
        logits = f(X)

        # Compute the loss
        loss = loss_fn(logits, y)

        # Backward pass: compute the gradients
        loss.backward()

        # Update the parameters
        optimizer.step()

        # Zero the gradients
        optimizer.zero_grad()

        # Add the loss to the epoch loss
        epoch_loss += loss.item()

    # Compute the average loss for the epoch
    avg_loss = epoch_loss / len(E)

    # Add the average loss to the list of losses
    losses.append(avg_loss)

    # Print the epoch and the average loss
    print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')

  # Return the trained model
  return f

# Define a function to compute the spuriousness scores for all concepts and classes across environments
def compute_spuriousness(E, C, f_0):
  # Create an empty dictionary to store the spuriousness scores
  S = {}

  # For each environment in E
  for e in E:

    # Create a data loader for the environment with a batch size of 1
    loader = torch.utils.data.DataLoader(e, batch_size=1, shuffle=False)

    # For each image and label from the loader
    for X, y in loader:

      # Move the image and label to the device (CPU or GPU)
      X = X.to(device)
      y = y.to(device)

      # Extract the concepts from the image using C
      concepts = C(X)

      # Predict the class from the image using f_0
      pred = f_0(X).argmax(dim=1)

      # For each concept in concepts
      for c in concepts:

        # For each class in classes
        for k in classes:

          # If c and k are not in S, initialize their score as zero
          if (c, k) not in S:
            S[(c, k)] = 0

          # If c and k are not in S[e], initialize their score as zero
          if (c, k) not in S[e]:
            S[e][(c, k)] = 0

          # Update their score by adding 1 if pred == k and subtracting 1 if y == k 
          if pred == k:
            S[(c, k)] += 1
            S[e][(c, k)] += 1

          if y == k:
            S[(c, k)] -= 1
            S[e][(c, k)] -= 1

  # Normalize the scores by dividing by the number of environments and images per environment 
  for (c, k) in S:
    S[(c, k)] /= len(E) * len(e)

    for e in E:
      S[e][(c, k)] /= len(e)

  # Return the spuriousness scores dictionary 
  return S

# Define a function to find the most spurious concept-class pair that exceeds the threshold t 
def find_most_spurious(S, t):
  # Initialize the most spurious pair as None 
  most_spurious = None 

  # Initialize the maximum absolute score as zero 
  max_score = 0 

  # For each concept-class pair in S 
  for (c, k) in S:

    # Compute the absolute score 
    abs_score = abs(S[(c, k)])

    # If the absolute score is greater than t and greater than max_score 
    if abs_score > t and abs_score > max_score:

      # Update max_score with abs_score 
      max_score = abs_score 

      # Update most_spurious with (c, k)