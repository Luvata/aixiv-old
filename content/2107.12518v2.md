---
title: 2107.12518v2 Segmentation in Style  Unsupervised Semantic Image Segmentation with Stylegan and CLIP
date: 2021-07-13
---

# [Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP](http://arxiv.org/abs/2107.12518v2)

authors: Daniil Pakhomov, Sanchit Hira, Narayani Wagle, Kemar E. Green, Nassir Navab


## What, Why and How

[1]: https://arxiv.org/abs/2107.12518 "[2107.12518] Segmentation in Style: Unsupervised Semantic Image ..."
[2]: https://arxiv.org/pdf/2107.12518.pdf "nwagle@jhu.edu arXiv:2107.12518v2 [cs.CV] 18 Nov 2021"
[3]: http://export.arxiv.org/abs/2106.12518v2 "[2106.12518v2] Minimal spin deflection of Kerr-Newman and ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces a method that allows to automatically segment images into semantically meaningful regions without human supervision. The method uses a pretrained StyleGAN2 generative model and CLIP to discover and label semantic classes. The method then creates a synthetic dataset with generated images and corresponding segmentation masks, and trains a segmentation model on it. The method can use natural language prompts to specify some desired semantic classes.
- **Why**: The paper aims to address the problem of slow and inconsistent image annotation, which limits the applicability of deep learning methods for semantic segmentation. The paper also aims to demonstrate that semantic classes can be discovered without human intervention, even in cases where the visual boundaries are unclear or subjective.
- **How**: The paper performs clustering in the feature space of the StyleGAN2 generative model to discover semantic classes. The paper then uses CLIP to assign labels to the clusters based on natural language prompts. The paper then generates synthetic images and segmentation masks using StyleGAN2 and the cluster labels. The paper then trains a segmentation model on the synthetic dataset and evaluates it on real images. The paper tests the method on publicly available datasets and shows state-of-the-art results.

## Main Contributions

According to the paper, the main contributions are:

- A novel method for unsupervised semantic image segmentation that leverages pretrained StyleGAN2 and CLIP models.
- A way to use natural language prompts to specify some desired semantic classes and generate synthetic data with corresponding labels.
- A demonstration of the consistency and quality of the discovered semantic classes across different images and datasets.
- A comparison of the method with semi-supervised and fully supervised methods on various benchmarks and tasks.

## Method Summary

The method section of the paper describes the following steps:

- **Clustering in StyleGAN2 feature space**: The paper uses a pretrained StyleGAN2 model to generate synthetic images and extract features from them. The paper then applies k-means clustering to the features to obtain semantic classes. The paper also uses CLIP to assign labels to the clusters based on natural language prompts.
- **Synthetic dataset generation**: The paper uses the cluster labels to generate synthetic images and segmentation masks. The paper uses a random noise vector and a random cluster label as inputs to the StyleGAN2 model, and obtains an image and a mask as outputs. The paper repeats this process to create a synthetic dataset of image-mask pairs.
- **Segmentation model training and evaluation**: The paper uses a U-Net [16] architecture as the segmentation model and trains it on the synthetic dataset. The paper then evaluates the model on real images from various datasets and tasks, such as hair segmentation, background segmentation, animal face segmentation, and cartoon face segmentation. The paper compares the results with semi-supervised and fully supervised methods and shows that the proposed method achieves competitive performance.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load a pretrained StyleGAN2 model
stylegan = load_stylegan()

# Load a pretrained CLIP model
clip = load_clip()

# Define natural language prompts for some desired semantic classes
prompts = ["hair", "skin", "eyes", "nose", "mouth", "background"]

# Generate synthetic images and features using StyleGAN2
images, features = stylegan.generate_images_and_features(num_images)

# Cluster the features using k-means
clusters = kmeans(features, num_clusters)

# Assign labels to the clusters using CLIP
labels = clip.assign_labels(clusters, prompts)

# Generate synthetic images and masks using StyleGAN2 and cluster labels
synthetic_dataset = stylegan.generate_images_and_masks(num_images, labels)

# Train a U-Net segmentation model on the synthetic dataset
unet = train_unet(synthetic_dataset)

# Evaluate the segmentation model on real images
evaluate_unet(unet, real_images)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import sklearn.cluster
import torchvision.models
import torchvision.transforms

# Load a pretrained StyleGAN2 model
stylegan = torch.hub.load('NVIDIA/StyleGAN2-ADA-PyTorch', 'generator', pretrained=True)

# Load a pretrained CLIP model
clip = torch.hub.load('openai/CLIP-ViT-B/32', 'model', jit=False)

# Define natural language prompts for some desired semantic classes
prompts = ["hair", "skin", "eyes", "nose", "mouth", "background"]

# Encode the prompts using CLIP
prompt_tokens = clip.encode_text(prompts).detach()

# Define the number of images and clusters to generate
num_images = 10000
num_clusters = 6

# Define an empty list to store synthetic images and features
images = []
features = []

# Loop over the number of images
for i in range(num_images):

  # Generate a random noise vector and a random cluster label
  noise = torch.randn(1, 512)
  label = torch.randint(0, num_clusters, (1,))

  # Generate a synthetic image using StyleGAN2
  image = stylegan(noise, label)[0]

  # Append the image to the list of images
  images.append(image)

  # Extract features from the image using CLIP
  feature = clip.encode_image(image.unsqueeze(0)).detach()

  # Append the feature to the list of features
  features.append(feature)

# Convert the lists of images and features to tensors
images = torch.stack(images)
features = torch.stack(features)

# Cluster the features using k-means
kmeans = sklearn.cluster.KMeans(n_clusters=num_clusters)
kmeans.fit(features.numpy())
clusters = kmeans.labels_

# Assign labels to the clusters using CLIP
labels = []
for cluster in clusters:

  # Compute the cosine similarity between the cluster centroid and the prompt tokens
  centroid = kmeans.cluster_centers_[cluster]
  similarity = torch.nn.functional.cosine_similarity(torch.tensor(centroid), prompt_tokens, dim=-1)

  # Find the index of the most similar prompt token
  index = similarity.argmax()

  # Append the corresponding prompt to the list of labels
  labels.append(prompts[index])

# Define an empty list to store synthetic images and masks
synthetic_dataset = []

# Loop over the number of images
for i in range(num_images):

  # Get the image and label from the lists
  image = images[i]
  label = labels[i]

  # Generate a mask using StyleGAN2 and the label index
  label_index = prompts.index(label)
  mask = stylegan(noise, torch.tensor(label_index))[0]

  # Convert the mask to binary by thresholding at 0.5
  mask = (mask > 0.5).float()

  # Append the image-mask pair to the list of synthetic data
  synthetic_dataset.append((image, mask))

# Define a U-Net segmentation model architecture (adapted from https://github.com/milesial/Pytorch-UNet)
class UNet(torch.nn.Module):
    def __init__(self):
        super(UNet, self).__init__()
                
        self.conv1_1 = torch.nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.conv1_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)

        self.conv2_1 = torch.nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv2_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.pool2 = torch.nn.MaxPool2d(kernel_size=2)

        self.conv3_1 = torch.nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.conv3_2 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.pool3 = torch.nn.MaxPool2d(kernel_size=2)

        self.conv4_1 = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.conv4_2 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)
        self.pool4 = torch.nn.MaxPool2d(kernel_size=2)

        self.conv5_1 = torch.nn.Conv2d(512, 1024, kernel_size=3, padding=1)
        self.conv5_2 = torch.nn.Conv2d(1024, 1024, kernel_size=3, padding=1)

        self.up6 = torch.nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.conv6_1 = torch.nn.Conv2d(1024, 512, kernel_size=3, padding=1)
        self.conv6_2 = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)

        self.up7 = torch.nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.conv7_1 = torch.nn.Conv2d(512, 256, kernel_size=3, padding=1)
        self.conv7_2 = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)

        self.up8 = torch.nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.conv8_1 = torch.nn.Conv2d(256, 128, kernel_size=3, padding=1)
        self.conv8_2 = torch.nn.Conv2d(128, 128, kernel_size=3, padding=1)

        self.up9 = torch.nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.conv9_1 = torch.nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv9_2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)

        self.conv10 = torch.nn.Conv2d(64, 1, kernel_size=1)

    def forward(self, x):
        c1 = torch.relu(self.conv1_1(x))
        c1 = torch.relu(self.conv1_2(c1))
        p1 = self.pool1(c1)

        c2 = torch.relu(self.conv2_1(p1))
        c2 = torch.relu(self.conv2_2(c2))
        p2 = self.pool2(c2)

        c3 = torch.relu(self.conv3_1(p2))
        c3 = torch.relu(self.conv3_2(c3))
        p3 = self.pool3(c3)

        c4 = torch.relu(self.conv4_1(p3))
        c4 = torch.relu(self.conv4_2(c4))
        p4 = self.pool4(c4)

        c5 = torch.relu(self.conv5_1(p4))
        c5 = torch.relu(self.conv5_2(c5))

        up6 = self.up6(c5)
        up6 = torch.cat([up6, c4], dim=1)
        c6 = torch.relu(self.conv6_1(up6))
        c6 = torch.relu(self.conv6_2(c6))

        up7 = self.up7(c6)
        up7 = torch.cat([up7, c3], dim=1)
        c7 = torch.relu(self.conv7_1(up7))
        c7 = torch.relu(self.conv7_2(c7))

        up8 = self.up8(c7)
        up8 = torch.cat([up8, c2], dim=1)
        c8 = torch.relu(self.conv8_1(up8))
        c8 = torch.relu(self.conv8_2(c8))

        up9 = self.up9(c8)
        up9 = torch.cat([up9, c1], dim=1)
        c9 = torch.relu(self.conv9_1(up9))
        c9 = torch.relu(self.conv9_2(c9))

        c10 = self.conv10(c9)
        
        return c10

# Instantiate a U-Net segmentation model
unet = UNet()

# Define a loss function and an optimizer
loss_function = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(unet.parameters())

# Define the number of epochs and batch size for training
num_epochs = 100
batch_size = 32

# Loop over the number of epochs
for epoch in range(num_epochs):

  # Shuffle the synthetic dataset
  np.random.shuffle(synthetic_dataset)

  # Loop over the synthetic dataset in batches
  for i in range(0, len(synthetic_dataset), batch_size):

    # Get the batch of images and masks
    images_batch = synthetic_dataset[i:i+batch_size][0]
    masks_batch = synthetic_dataset[i:i+batch_size][0]

    # Zero the gradients
    optimizer.zero_grad()

    # Forward pass the images through the segmentation model
    outputs_batch = unet(images_batch)

    # Compute the loss