---
title: 2303.08767v3 Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion
date: 2023-03-09
---

# [Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion](http://arxiv.org/abs/2303.08767v3)

authors: Inhwa Han, Serin Yang, Taesung Kwon, Jong Chul Ye


## What, Why and How

[1]: https://arxiv.org/abs/2303.08767 "[2303.08767] Highly Personalized Text Embedding for Image Manipulation ..."
[2]: https://arxiv.org/pdf/2303.08767.pdf "f g@kaist.ac.kr arXiv:2303.08767v3 [cs.CV] 19 Apr 2023"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2303.08767 "[2303.08767] Highly Personalized Text Embedding for Image Manipulation ..."

Here is a summary of the paper:

- **What**: The paper proposes a method for image manipulation by stable diffusion using highly personalized text embedding (HiPer).
- **Why**: The paper aims to overcome the limitations of previous approaches that require multiple reference images, model fine-tuning, or identifiers to preserve and manipulate image content and identity with text prompts.
- **How**: The paper decomposes the CLIP embedding space into a personalized part and a content part, and uses them as inputs to a pre-trained U-net diffusion model to generate images that match the target text while maintaining the source identity.

## Main Contributions

[1]: https://arxiv.org/abs/2303.08767 "[2303.08767] Highly Personalized Text Embedding for Image Manipulation ..."
[2]: https://arxiv.org/pdf/2303.08767.pdf "f g@kaist.ac.kr arXiv:2303.08767v3 [cs.CV] 19 Apr 2023"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2303.08767 "[2303.08767] Highly Personalized Text Embedding for Image Manipulation ..."

According to the paper, the main contributions are:

- A novel method for image manipulation by stable diffusion using highly personalized text embedding (HiPer) that decomposes the CLIP embedding space into a personalized part and a content part.
- A demonstration of the effectiveness of HiPer text embedding in preserving and manipulating image content and identity with diverse target texts and a single source image, without requiring model fine-tuning or identifiers.
- A novel understanding of the text embedding space that reveals the trade-off between personalization and content manipulation, and the potential applications of HiPer text embedding across various tasks.

## Method Summary

[1]: https://arxiv.org/abs/2303.08767 "[2303.08767] Highly Personalized Text Embedding for Image Manipulation ..."
[2]: https://arxiv.org/pdf/2303.08767.pdf "f g@kaist.ac.kr arXiv:2303.08767v3 [cs.CV] 19 Apr 2023"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2303.08767 "[2303.08767] Highly Personalized Text Embedding for Image Manipulation ..."

Here is a summary of the method section of the paper:

- The paper uses a pre-trained U-net diffusion model [^1^][2] that takes a source image and a text embedding as inputs and generates an output image that matches the text prompt.
- The paper uses CLIP  to convert text prompts into text embeddings, and decomposes the text embedding space into two parts: a personalized part and a content part.
- The personalized part is obtained by subtracting the mean of the text embeddings of multiple images with the same identity from the text embedding of the source image. This part captures the identity information of the source image that is invariant to different texts.
- The content part is obtained by subtracting the personalized part from the text embedding of the target text. This part captures the semantic information of the target text that is relevant to image manipulation.
- The paper concatenates the personalized part and the content part to form a highly personalized (HiPer) text embedding, and uses it as an input to the diffusion model along with the source image.
- The paper applies a noise schedule and a denoising score matching loss [^1^][2] to train the diffusion model in a self-supervised manner.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a pre-trained U-net diffusion model
model = DiffusionModel()

# Define a CLIP model
clip = CLIPModel()

# Define a noise schedule
noise_schedule = NoiseSchedule()

# Define a denoising score matching loss
loss = DSM_Loss()

# Given a source image and a target text
source_image = Image()
target_text = Text()

# Convert the target text into a text embedding using CLIP
target_embedding = clip.encode_text(target_text)

# Compute the mean of the text embeddings of multiple images with the same identity as the source image
identity_mean = clip.encode_text(identity_texts).mean()

# Compute the personalized part by subtracting the identity mean from the source embedding
personalized_part = clip.encode_text(source_text) - identity_mean

# Compute the content part by subtracting the personalized part from the target embedding
content_part = target_embedding - personalized_part

# Concatenate the personalized part and the content part to form a HiPer text embedding
hiper_embedding = torch.cat([personalized_part, content_part])

# Generate an output image by feeding the source image and the HiPer text embedding to the diffusion model
output_image = model.generate(source_image, hiper_embedding, noise_schedule)

# Train the diffusion model using a denoising score matching loss
model.train(loss)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define a U-net diffusion model based on https://arxiv.org/abs/2006.11239
class DiffusionModel(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the encoder and decoder blocks
    self.encoder_blocks = torch.nn.ModuleList([
      EncoderBlock(in_channels=3, out_channels=64),
      EncoderBlock(in_channels=64, out_channels=128),
      EncoderBlock(in_channels=128, out_channels=256),
      EncoderBlock(in_channels=256, out_channels=512),
      EncoderBlock(in_channels=512, out_channels=1024),
    ])
    self.decoder_blocks = torch.nn.ModuleList([
      DecoderBlock(in_channels=1024, out_channels=512),
      DecoderBlock(in_channels=512, out_channels=256),
      DecoderBlock(in_channels=256, out_channels=128),
      DecoderBlock(in_channels=128, out_channels=64),
      DecoderBlock(in_channels=64, out_channels=3),
    ])
    # Define the text embedding projection layer
    self.text_projection = torch.nn.Linear(1024, 1024)
  
  def forward(self, x, t, text_embedding):
    # Encode the input image x
    encodings = []
    for encoder_block in self.encoder_blocks:
      x = encoder_block(x)
      encodings.append(x)
    
    # Project the text embedding to match the encoder output dimension
    text_embedding = self.text_projection(text_embedding)

    # Concatenate the text embedding to the last encoding
    x = torch.cat([x, text_embedding], dim=-1)

    # Decode the output image y
    y = x
    for i, decoder_block in enumerate(self.decoder_blocks):
      y = decoder_block(y, encodings[-i-1])
    
    # Return the output image y and the predicted noise level s
    y = torch.tanh(y)
    s = self.predict_noise_level(x)
    return y, s
  
  def generate(self, x_0, text_embedding, noise_schedule):
    # Generate an output image from a source image x_0 and a text embedding using the noise schedule
    x_t = x_0
    for t in noise_schedule:
      # Add Gaussian noise to the current image x_t
      x_t = x_t + torch.randn_like(x_t) * np.sqrt(t)

      # Predict the output image y_t and the noise level s_t from x_t and text_embedding
      y_t, s_t = self.forward(x_t, t, text_embedding)

      # Compute the reverse diffusion process to update x_t
      x_t = (x_t - s_t * y_t) / (1 - s_t)
    
    # Return the final output image y_T
    return y_t
  
  def predict_noise_level(self, x):
    # Predict the noise level s from the encoder output x using a linear layer
    s = torch.nn.Linear(1024, 1)(x)
    s = torch.sigmoid(s)
    return s

# Define an encoder block based on https://arxiv.org/abs/1505.04597
class EncoderBlock(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    # Define the convolutional layers with batch normalization and ReLU activation
    self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)
    self.bn1 = torch.nn.BatchNorm2d(out_channels)
    self.relu1 = torch.nn.ReLU()
    self.conv2 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)
    self.bn2 = torch.nn.BatchNorm2d(out_channels)
    self.relu2 = torch.nn.ReLU()
  
  def forward(self, x):
    # Apply the convolutional layers to the input x
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu1(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu2(x)
    
    # Downsample the output by max pooling
    x = torch.nn.MaxPool2d(kernel_size=2)(x)

    # Return the output x
    return x

# Define a decoder block based on https://arxiv.org/abs/1505.04597
class DecoderBlock(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    # Define the convolutional layers with batch normalization and ReLU activation
    self.conv1 = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)
    self.bn1 = torch.nn.BatchNorm2d(out_channels)
    self.relu1 = torch.nn.ReLU()
    self.conv2 = torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, padding=1)
    self.bn2 = torch.nn.BatchNorm2d(out_channels)
    self.relu2 = torch.nn.ReLU()
  
  def forward(self, x, encoding):
    # Upsample the input x by nearest neighbor interpolation
    x = torch.nn.Upsample(scale_factor=2, mode='nearest')(x)

    # Concatenate the input x with the corresponding encoding from the encoder
    x = torch.cat([x, encoding], dim=-1)

    # Apply the convolutional layers to the input x
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu1(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu2(x)

    # Return the output x
    return x

# Define a noise schedule based on https://arxiv.org/abs/2006.11239
class NoiseSchedule():
  def __init__(self, T, beta_0, beta_T):
    # Define the number of timesteps T and the initial and final noise levels beta_0 and beta_T
    self.T = T
    self.beta_0 = beta_0
    self.beta_T = beta_T
  
  def __iter__(self):
    # Iterate over the noise levels from t=0 to t=T
    for t in range(self.T):
      # Compute the noise level beta_t using a cosine annealing schedule
      beta_t = self.beta_0 + (self.beta_T - self.beta_0) * (1 - np.cos(np.pi * t / self.T)) / 2
      
      # Yield the noise level beta_t
      yield beta_t

# Define a denoising score matching loss based on https://arxiv.org/abs/2006.11239
class DSM_Loss(torch.nn.Module):
  def __init__(self):
    super().__init__()
  
  def forward(self, y, s, x):
    # Compute the loss between the output image y, the noise level s, and the input image x
    # The loss is defined as E[(x - y)^2 / (2 * s) + log(s)]
    loss = torch.mean((x - y) ** 2 / (2 * s) + torch.log(s))
    
    # Return the loss
    return loss

# Load a pre-trained CLIP model
clip_model = clip.load("ViT-B/32", device="cuda")

# Load a pre-trained U-net diffusion model or initialize a new one
model = DiffusionModel().to("cuda")
model.load_state_dict(torch.load("model.pth"))

# Define a noise schedule with T=1000, beta_0=1e-4, and beta_T=0.02
noise_schedule = NoiseSchedule(T=1000, beta_0=1e-4, beta_T=0.02)

# Define a denoising score matching loss
loss_fn = DSM_Loss()

# Define an optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Define a data loader for training images and texts
data_loader = DataLoader(dataset)

# Train the model for N epochs
for epoch in range(N):
  # Loop over the batches of images and texts
  for batch in data_loader:
    # Get the source images and texts from the batch
    source_images, source_texts = batch

    # Convert the source images and texts to tensors and move them to cuda device
    source_images = torchvision.transforms.ToTensor()(source_images).to("cuda")
    source_texts = clip.tokenize(source_texts).to("cuda")

    # Encode the source texts using CLIP to get source embeddings
    source_embeddings = clip_model.encode_text(source_texts)

    # Get the target texts from the batch or generate them randomly
    target_texts = get_target_texts(batch)

    # Encode the target texts using CLIP to get target embeddings
    target_embeddings = clip_model.encode_text(target_texts)

    # Compute the mean of the text embeddings of multiple images with the same identity as the source images
    identity_means = compute_identity_means(source_images, source_embeddings)

    # Compute the personalized parts by subtracting the identity means from the source embeddings
    personalized_parts = source_embeddings - identity_means

    # Compute the content parts by subtracting the personalized parts from the target embeddings
    content_parts = target_embeddings - personalized_parts

    # Concatenate the