---
title: 2208.13474v1 Prompt Tuning with Soft Context Sharing for Vision-Language Models
date: 2022-08-14
---

# [Prompt Tuning with Soft Context Sharing for Vision-Language Models](http://arxiv.org/abs/2208.13474v1)

authors: Kun Ding, Ying Wang, Pengzhang Liu, Qiang Yu, Haojian Zhang, Shiming Xiang, Chunhong Pan


## What, Why and How

[1]: https://arxiv.org/abs/2208.13474v1 "[2208.13474v1] Prompt Tuning with Soft Context Sharing for Vision ..."
[2]: https://arxiv.org/abs/2208.13474 "[2208.13474] Prompt Tuning with Soft Context Sharing for Vision ..."
[3]: https://arxiv.org/pdf/2207.13474v1.pdf "arXiv:2207.13474v1 [physics.optics] 27 Jul 2022"
[4]: http://export.arxiv.org/pdf/2208.13474v1 "export.arxiv.org"

Here is a summary of the paper:

- **What**: The paper proposes a novel method called **SoftCPT** (Soft Context Sharing for Prompt Tuning) to fine-tune pre-trained vision-language models on multiple target few-shot tasks, simultaneously[^1^][1] [^2^][2].
- **Why**: The paper aims to leverage the task relationships and the shared knowledge among different few-shot tasks, especially in a specialized area, to improve the performance of vision-language prompt tuning[^1^][1] [^2^][2].
- **How**: The paper designs a task-shared meta network to generate prompt vector for each task using pre-defined task name together with a learnable meta prompt as input. The prompt vectors of all tasks are shared in a soft manner. The parameters of the shared meta network and the meta prompt vector are tuned on the joint training set of all target tasks[^1^][1] [^2^][2].

## Main Contributions

[1]: https://arxiv.org/abs/2208.13474v1 "[2208.13474v1] Prompt Tuning with Soft Context Sharing for Vision ..."
[2]: https://arxiv-export-lb.library.cornell.edu/abs/2208.13474 "[2208.13474] Prompt Tuning with Soft Context Sharing for Vision ..."
[3]: http://export.arxiv.org/pdf/2208.13474v1 "export.arxiv.org"

The paper claims the following contributions[^1^][1] [^2^][2]:

- It proposes a novel method **SoftCPT** to fine-tune pre-trained vision-language models on multiple target few-shot tasks, simultaneously, by sharing the prompt vectors in a soft manner.
- It designs a task-shared meta network to generate prompt vector for each task using pre-defined task name together with a learnable meta prompt as input.
- It conducts extensive experiments on three multi-task few-shot datasets and shows that **SoftCPT** outperforms the representative single-task prompt tuning method **CoOp** by a large margin.

## Method Summary

[1]: https://arxiv.org/abs/2208.13474v1 "[2208.13474v1] Prompt Tuning with Soft Context Sharing for Vision ..."
[2]: https://arxiv-export-lb.library.cornell.edu/abs/2208.13474 "[2208.13474] Prompt Tuning with Soft Context Sharing for Vision ..."
[3]: https://arxiv.org/pdf/2208.13474.pdf "arXiv.org"

Here is a summary of the method section of the paper[^1^][1] [^2^][2]:

- The paper introduces the problem formulation of few-shot image recognition and the vision-language prompt tuning framework.
- The paper presents the proposed method **SoftCPT**, which consists of three components: a task-shared meta network, a learnable meta prompt, and a joint training strategy.
- The paper describes the task-shared meta network, which takes a pre-defined task name and the meta prompt as input and outputs a prompt vector for each task. The network is composed of a word embedding layer, a transformer encoder, and a linear projection layer. The network shares parameters across different tasks and learns to generate task-specific prompt vectors.
- The paper explains the learnable meta prompt, which is a vector that encodes the common knowledge among different tasks. The meta prompt is concatenated with the task name and fed into the task-shared meta network. The meta prompt is initialized randomly and tuned during training.
- The paper discusses the joint training strategy, which aims to optimize the parameters of the task-shared meta network and the meta prompt on the joint training set of all target tasks. The paper adopts a cross-entropy loss function and an Adam optimizer. The paper also applies data augmentation and label smoothing techniques to improve generalization.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the task names and the meta prompt
task_names = ["Task 1", "Task 2", ..., "Task N"]
meta_prompt = random_vector()

# Initialize the task-shared meta network
meta_network = Transformer(word_embedding, encoder, projection)

# Load the pre-trained vision-language model
vl_model = CLIP()

# Define the loss function and the optimizer
loss_function = CrossEntropyLoss()
optimizer = Adam(meta_network.parameters() + meta_prompt)

# Loop over the epochs
for epoch in range(num_epochs):

  # Loop over the batches
  for batch in range(num_batches):

    # Get the images and labels for each task
    images = [get_images(task) for task in task_names]
    labels = [get_labels(task) for task in task_names]

    # Apply data augmentation and label smoothing
    images = augment(images)
    labels = smooth(labels)

    # Generate the prompt vectors for each task
    prompt_vectors = []
    for task in task_names:
      input = concatenate(task, meta_prompt)
      output = meta_network(input)
      prompt_vector = output[-1]
      prompt_vectors.append(prompt_vector)

    # Compute the logits and the loss for each task
    logits = []
    loss = 0
    for i in range(len(task_names)):
      logit = vl_model(images[i], prompt_vectors[i])
      logits.append(logit)
      loss += loss_function(logit, labels[i])

    # Update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the libraries
import torch
import torchvision
import clip

# Define the hyperparameters
num_tasks = 10 # The number of target few-shot tasks
num_classes = 5 # The number of classes per task
num_shots = 5 # The number of shots per class
num_epochs = 100 # The number of training epochs
batch_size = 32 # The batch size
learning_rate = 1e-4 # The learning rate
weight_decay = 1e-5 # The weight decay
label_smoothing = 0.1 # The label smoothing factor
augmentation_prob = 0.5 # The probability of applying data augmentation

# Define the task names and the meta prompt
task_names = ["Task " + str(i) for i in range(1, num_tasks + 1)]
meta_prompt = torch.randn(1, 512) # A random vector of size 512
meta_prompt.requires_grad = True # Make it trainable

# Initialize the task-shared meta network
# Use the same word embedding and encoder as CLIP
word_embedding = clip.simple_tokenizer()
encoder = clip.transformer()
# Use a linear projection layer to map the output to the prompt vector size (512)
projection = torch.nn.Linear(512, 512)
# Combine the layers into a sequential model
meta_network = torch.nn.Sequential(word_embedding, encoder, projection)

# Load the pre-trained vision-language model
vl_model = clip.load("ViT-B/32", jit=False)[0] # Use the Vision Transformer model with 32x32 patches

# Define the loss function and the optimizer
loss_function = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(meta_network.parameters() + [meta_prompt], lr=learning_rate, weight_decay=weight_decay)

# Define the data augmentation transforms
transforms = torchvision.transforms.Compose([
  torchvision.transforms.RandomResizedCrop(224), # Randomly crop and resize the image to 224x224
  torchvision.transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally
  torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), # Randomly change the color of the image
  torchvision.transforms.ToTensor(), # Convert the image to a tensor
])

# Define a function to get a batch of images and labels for a given task
def get_batch(task):
  # Load the dataset for the task
  dataset = load_dataset(task)
  # Sample num_classes * num_shots images and labels from the dataset
  images, labels = sample(dataset, num_classes * num_shots)
  # Reshape the images and labels into (num_classes, num_shots, ...)
  images = images.view(num_classes, num_shots, *images.shape[1:])
  labels = labels.view(num_classes, num_shots)
  # Split the images and labels into support set and query set
  support_images = images[:, :num_shots // 2] # Use half of the shots as support set
  support_labels = labels[:, :num_shots // 2]
  query_images = images[:, num_shots // 2:] # Use the other half as query set
  query_labels = labels[:, num_shots // 2:]
  # Flatten the support set and query set into (num_classes * num_shots // 2, ...)
  support_images = support_images.view(-1, *support_images.shape[2:])
  support_labels = support_labels.view(-1)
  query_images = query_images.view(-1, *query_images.shape[2:])
  query_labels = query_labels.view(-1)
  # Return the support set and query set as tensors
  return torch.tensor(support_images), torch.tensor(support_labels), torch.tensor(query_images), torch.tensor(query_labels)

# Define a function to apply data augmentation and label smoothing to a batch of images and labels
def augment_and_smooth(images, labels):
  # Apply data augmentation to each image with a probability of augmentation_prob
  augmented_images = []
  for image in images:
    if torch.rand(1) < augmentation_prob:
      augmented_image = transforms(image) # Apply the transforms to the image
    else:
      augmented_image = image # Keep the original image
    augmented_images.append(augmented_image)
  # Stack the augmented images into a tensor
  augmented_images = torch.stack(augmented_images)
  
  # Apply label smoothing to each label by adding label_smoothing / (num_classes - 1) to each non-target class probability and subtracting label_smoothing from the target class probability
  smoothed_labels = []
  for label in labels:
    smoothed_label = torch.full((num_classes,), label_smoothing / (num_classes - 1)) # Initialize the smoothed label with uniform probabilities
    smoothed_label[label] = 1 - label_smoothing # Set the target class probability to 1 - label_smoothing
    smoothed_labels.append(smoothed_label)
  # Stack the smoothed labels into a tensor
  smoothed_labels = torch.stack(smoothed_labels)

  # Return the augmented images and smoothed labels
  return augmented_images, smoothed_labels

# Loop over the epochs
for epoch in range(num_epochs):

  # Loop over the batches
  for batch in range(num_batches):

    # Get the support set and query set for each task
    support_sets = []
    query_sets = []
    for task in task_names:
      support_images, support_labels, query_images, query_labels = get_batch(task)
      support_sets.append((support_images, support_labels))
      query_sets.append((query_images, query_labels))

    # Apply data augmentation and label smoothing to the support set and query set for each task
    augmented_support_sets = []
    augmented_query_sets = []
    for i in range(len(task_names)):
      support_images, support_labels = support_sets[i]
      query_images, query_labels = query_sets[i]
      augmented_support_images, smoothed_support_labels = augment_and_smooth(support_images, support_labels)
      augmented_query_images, smoothed_query_labels = augment_and_smooth(query_images, query_labels)
      augmented_support_sets.append((augmented_support_images, smoothed_support_labels))
      augmented_query_sets.append((augmented_query_images, smoothed_query_labels))

    # Generate the prompt vectors for each task
    prompt_vectors = []
    for task in task_names:
      input = word_embedding(task) # Embed the task name as a sequence of tokens
      input = torch.cat([input, meta_prompt], dim=0) # Concatenate the meta prompt to the input
      input = input.unsqueeze(0) # Add a batch dimension to the input
      output = meta_network(input) # Pass the input through the meta network
      prompt_vector = output[0, -1] # Get the last token output as the prompt vector
      prompt_vectors.append(prompt_vector)

    # Compute the logits and the loss for each task
    logits = []
    loss = 0
    for i in range(len(task_names)):
      support_images, support_labels = augmented_support_sets[i]
      query_images, query_labels = augmented_query_sets[i]
      prompt_vector = prompt_vectors[i]
      logit = vl_model(query_images, prompt_vector) # Pass the query images and the prompt vector through the vision-language model to get the logit
      logits.append(logit)
      loss += loss_function(logit, support_labels) # Compute the cross-entropy loss between the logit and the support labels

    # Update the parameters
    optimizer.zero_grad() # Zero out the gradients
    loss.backward() # Backpropagate the loss
    optimizer.step() # Update the parameters using Adam optimizer
```