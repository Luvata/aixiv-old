---
title: 2306.00114v1 The Canadian Cropland Dataset  A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture
date: 2023-06-01
---

# [The Canadian Cropland Dataset: A New Land Cover Dataset for Multitemporal Deep Learning Classification in Agriculture](http://arxiv.org/abs/2306.00114v1)

authors: Amanda A. Boatswain Jacques, Abdoulaye Banir√© Diallo, Etienne Lord


## What, Why and How

[1]: https://arxiv.org/abs/2306.00014v1 "PreQuant: A Task-agnostic Quantization Approach for Pre-trained ..."
[2]: https://arxiv.org/pdf/2306.00114v1.pdf "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00114 "[2306.00114] The Canadian Cropland Dataset: A New Land Cover Dataset ..."

Here is a summary of the paper at [^1^][2]:

- **What**: The paper introduces a new land cover dataset for multitemporal deep learning classification in agriculture, called the Canadian Cropland Dataset. The dataset contains high-resolution images of 10 crop classes from 2017 to 2020, collected by Sentinel-2 satellites. The paper also provides models and source code for crop classification using single or sequential images.
- **Why**: The paper aims to address the lack of reliable, fine-grained and high-quality annotated samples for studying environmental changes and ensuring global food security through crop yield forecasting. The paper expects the dataset to propel the creation of robust agro-environmental models that can provide accurate and continuous monitoring of land cover.
- **How**: The paper describes the data collection, annotation, verification and curation process of the dataset. The paper also presents the experimental setup, results and analysis of different deep learning models for crop classification, such as ResNet, DenseNet, EfficientNet, LRCN and 3D-CNN. The paper compares the performance of single-image and sequential-image models, as well as different vegetation indices. The paper discusses the challenges and limitations of the dataset and the models, and suggests future directions for improvement.

## Main Contributions

According to the paper, the main contributions are:

- The paper introduces a new temporal patch-based dataset of Canadian croplands, enriched with labels retrieved from the Canadian Annual Crop Inventory. The dataset contains 78,536 manually verified and curated high-resolution images from 10 crop classes collected over four crop production years (2017-2020) and five months (June-October). Each instance contains 12 spectral bands, an RGB image, and additional vegetation index bands. Individually, each category contains at least 4,800 images.
- The paper provides models and source code that allow a user to predict the crop class using a single image (ResNet, DenseNet, EfficientNet) or a sequence of images (LRCN, 3D-CNN) from the same location. The paper also provides a benchmark for crop classification using different vegetation indices and temporal features.
- The paper presents an empirical investigation into the workflow of multitemporal deep learning classification in agriculture, which sheds light on the challenges and opportunities of this domain. The paper discusses the trade-offs between accuracy and complexity of different models, the impact of different vegetation indices on classification performance, and the limitations of the dataset and the models. The paper also suggests future directions for improving the dataset and the models.

## Method Summary

Here is a summary of the method section of the paper:

- The paper describes the data collection, annotation, verification and curation process of the dataset. The paper uses Sentinel-2 images from the Copernicus Open Access Hub, which have a spatial resolution of 10 m/pixel and a temporal resolution of 5 days. The paper selects images from four crop production years (2017-2020) and five months (June-October), covering the main growing season of most crops in Canada. The paper retrieves the crop labels from the Canadian Annual Crop Inventory (ACI), which is a national-scale crop map produced by Agriculture and Agri-Food Canada. The paper manually verifies and curates the labels to ensure their accuracy and consistency. The paper also generates additional vegetation index bands, such as NDVI, NDRE and EVI, to enhance the spectral information of the images. The paper splits the dataset into training (60%), validation (20%) and test (20%) sets, ensuring that there is no overlap between the locations or years of the images.
- The paper presents the experimental setup, results and analysis of different deep learning models for crop classification, such as ResNet, DenseNet, EfficientNet, LRCN and 3D-CNN. The paper uses PyTorch as the deep learning framework and trains the models on a GPU cluster. The paper uses cross-entropy loss as the objective function and Adam as the optimizer. The paper evaluates the models using accuracy, precision, recall and F1-score metrics. The paper compares the performance of single-image and sequential-image models, as well as different vegetation indices. The paper also performs ablation studies to investigate the impact of different temporal features on classification performance.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Data collection
for year in [2017, 2018, 2019, 2020]:
  for month in [June, July, August, September, October]:
    download Sentinel-2 images from Copernicus Open Access Hub
    filter images by cloud cover and quality
    crop images to 64 x 64 pixels (640 x 640 m)
    save images as GeoTIFF files with 12 spectral bands

# Data annotation
for each image:
  get the corresponding crop label from the Canadian Annual Crop Inventory
  verify and correct the label manually if needed
  save the label as a CSV file

# Data curation
for each image:
  calculate additional vegetation index bands (NDVI, NDRE, EVI)
  append the vegetation index bands to the image
  normalize the image values to [0, 1]
split the dataset into train (60%), validation (20%) and test (20%) sets
ensure no overlap between the locations or years of the images in different sets

# Model training and evaluation
define the model architecture (ResNet, DenseNet, EfficientNet, LRCN or 3D-CNN)
define the loss function (cross-entropy)
define the optimizer (Adam)
define the evaluation metrics (accuracy, precision, recall, F1-score)
for each epoch:
  for each batch of images and labels in the train set:
    feed the images to the model
    compute the output and the loss
    update the model parameters using backpropagation and gradient descent
  for each batch of images and labels in the validation set:
    feed the images to the model
    compute the output and the metrics
    save the best model based on the validation metrics
load the best model
for each batch of images and labels in the test set:
  feed the images to the model
  compute the output and the metrics
  report the test metrics and confusion matrix
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import libraries
import numpy as np
import pandas as pd
import geopandas as gpd
import rasterio as rio
import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Define constants
CROP_CLASSES = ["Barley", "Canola", "Corn", "Flax", "Lentils", "Oats", "Peas", "Soybeans", "Spring Wheat", "Winter Wheat"]
SPECTRAL_BANDS = ["B01", "B02", "B03", "B04", "B05", "B06", "B07", "B08", "B8A", "B09", "B11", "B12"]
VEGETATION_INDICES = ["NDVI", "NDRE", "EVI"]
IMAGE_SIZE = 64 # pixels
SPATIAL_RESOLUTION = 10 # meters/pixel
TEMPORAL_RESOLUTION = 5 # days
CLOUD_COVER_THRESHOLD = 0.1 # fraction of image covered by clouds
DATA_DIR = "/path/to/data/directory"
MODEL_DIR = "/path/to/model/directory"
MODEL_NAME = "ResNet" # or DenseNet, EfficientNet, LRCN, 3D-CNN
VEGETATION_INDEX = "NDVI" # or NDRE, EVI
USE_SEQUENTIAL_IMAGES = False # or True
NUM_EPOCHS = 100
BATCH_SIZE = 32
LEARNING_RATE = 0.001

# Data collection
def download_sentinel_images(year, month):
  # Use the sentinelsat library to query and download Sentinel-2 images from Copernicus Open Access Hub
  # Filter images by cloud cover and quality flags
  # Return a list of image paths

def crop_images(image_paths):
  # Use the rasterio library to crop each image to 64 x 64 pixels (640 x 640 m)
  # Save the cropped images as GeoTIFF files with 12 spectral bands
  # Return a list of cropped image paths

def collect_data():
  # Loop over the years and months of interest
  for year in [2017, 2018, 2019, 2020]:
    for month in [June, July, August, September, October]:
      # Download Sentinel-2 images for the given year and month
      image_paths = download_sentinel_images(year, month)
      # Crop the images to the desired size and resolution
      cropped_image_paths = crop_images(image_paths)
      # Save the cropped image paths to a CSV file with the year and month as the filename

# Data annotation
def get_crop_labels(image_paths):
  # Use the geopandas library to load the Canadian Annual Crop Inventory shapefile as a geodataframe
  aci = gpd.read_file("ACI_2020.shp")
  # Loop over the image paths
  for image_path in image_paths:
    # Use the rasterio library to open the image as a dataset and get its bounding box coordinates
    with rio.open(image_path) as ds:
      bbox = ds.bounds
    # Use the geopandas library to filter the ACI geodataframe by the bounding box and get the most frequent crop class within it
    aci_subset = aci[aci.intersects(bbox)]
    crop_class = aci_subset["CLASS_NAME"].mode()[0]
    # Save the crop class to a CSV file with the image path as the filename

def verify_crop_labels(label_paths):
  # Loop over the label paths
  for label_path in label_paths:
    # Load the label from the CSV file
    label = pd.read_csv(label_path)["label"][0]
    # If the label is not in the crop classes list, correct it manually by inspecting the corresponding image and ACI shapefile
    if label not in CROP_CLASSES:
      image_path = label_path.replace(".csv", ".tif")
      print(f"Invalid label: {label} for image: {image_path}")
      print(f"Please enter a valid label from: {CROP_CLASSES}")
      new_label = input()
      assert new_label in CROP_CLASSES, f"Invalid label: {new_label}"
      print(f"Corrected label: {new_label} for image: {image_path}")
      pd.DataFrame({"label": [new_label]}).to_csv(label_path)

def annotate_data():
  # Loop over the years and months of interest
  for year in [2017, 2018, 2019, 2020]:
    for month in [June, July, August, September, October]:
      # Load the cropped image paths from the CSV file with the year and month as the filename
      image_paths = pd.read_csv(f"{DATA_DIR}/{year}_{month}.csv")["image_path"].tolist()
      # Get the crop labels for the image paths
      get_crop_labels(image_paths)
      # Verify and correct the crop labels manually if needed
      label_paths = [image_path.replace(".tif", ".csv") for image_path in image_paths]
      verify_crop_labels(label_paths)

# Data curation
def calculate_vegetation_indices(image_paths):
  # Loop over the image paths
  for image_path in image_paths:
    # Use the rasterio library to open the image as a dataset and read the spectral bands as numpy arrays
    with rio.open(image_path) as ds:
      B02 = ds.read(2) # blue band
      B03 = ds.read(3) # green band
      B04 = ds.read(4) # red band
      B05 = ds.read(5) # red edge band
      B08 = ds.read(8) # near infrared band
    # Calculate the vegetation index bands as numpy arrays using the formulas from https://www.sentinel-hub.com/eoproducts/
    NDVI = (B08 - B04) / (B08 + B04) # normalized difference vegetation index
    NDRE = (B08 - B05) / (B08 + B05) # normalized difference red edge index
    EVI = 2.5 * (B08 - B04) / (B08 + 6 * B04 - 7.5 * B02 + 1) # enhanced vegetation index
    # Append the vegetation index bands to the image dataset and save it as a new GeoTIFF file with the same metadata
    with rio.open(image_path.replace(".tif", "_vi.tif"), "w", **ds.meta, count=15) as ds_vi:
      for i in range(1, 13):
        ds_vi.write(ds.read(i), i)
      ds_vi.write(NDVI, 13)
      ds_vi.write(NDRE, 14)
      ds_vi.write(EVI, 15)

def normalize_images(image_paths):
  # Loop over the image paths
  for image_path in image_paths:
    # Use the rasterio library to open the image as a dataset and read the bands as numpy arrays
    with rio.open(image_path) as ds:
      bands = [ds.read(i) for i in range(1, 16)]
    # Normalize the band values to [0, 1] by dividing by the maximum value (10000)
    bands = [band / 10000 for band in bands]
    # Save the normalized bands to a new GeoTIFF file with the same metadata
    with rio.open(image_path.replace(".tif", "_norm.tif"), "w", **ds.meta) as ds_norm:
      for i in range(1, 16):
        ds_norm.write(bands[i-1], i)

def split_data():
  # Create empty lists to store the train, validation and test sets
  train_set = []
  val_set = []
  test_set = []
  # Loop over the years and months of interest
  for year in [2017, 2018, 2019, 2020]:
    for month in [June, July, August, September, October]:
      # Load the normalized image paths and labels from the CSV files with the year and month as the filename
      image_paths = pd.read_csv(f"{DATA_DIR}/{year}_{month}_vi_norm.csv")["image_path"].tolist()
      labels = pd.read_csv(f"{DATA_DIR}/{year}_{month}_label.csv")["label"].tolist()
      # Shuffle the image paths and labels together using a random seed based on the year and month
      seed = int(f"{year}{month}")
      np.random.seed(seed)
      np.random.shuffle(image_paths)
      np.random.seed(seed)
      np.random.shuffle(labels)
      # Split the image paths and labels into train (60%), validation (20%) and test (20%) sets using a stratified sampling method based on the labels
      train_size = int(len(image_paths) * 0.6)
      val_size = int(len(image_paths) * 0.2)
      test_size = len(image_paths) - train_size - val_size
      train_image_paths, val_image_paths, test_image_paths = np.split(image_paths, [train_size, train_size + val_size])
      train_labels, val_labels, test_labels = np.split(labels, [train_size, train_size + val_size])
      # Append the train, validation and test sets to their respective lists
      train_set.extend(list(zip