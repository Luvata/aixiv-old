---
title: 2011.12026v2 Adversarial Generation of Continuous Images
date: 2020-11-13
---

# [Adversarial Generation of Continuous Images](http://arxiv.org/abs/2011.12026v2)

authors: Ivan Skorokhodov, Savva Ignatyev, Mohamed Elhoseiny


## What, Why and How

[1]: https://arxiv.org/abs/2011.12026 "[2011.12026] Adversarial Generation of Continuous Images - arXiv.org"
[2]: https://arxiv.org/pdf/2011.12026.pdf "Adversarial Generation of Continuous Images - arXiv.org"
[3]: https://scholar.archive.org/work/l4e4m2xi6zalvkvo6wmwyivr6m "Adversarial Generation of Continuous Images"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes two novel architectural techniques for building INR-based image decoders: factorized multiplicative modulation and multi-scale INRs, and uses them to build a state-of-the-art continuous image GAN.
- **Why**: The paper aims to improve the performance and scalability of continuous image generators, which represent images as implicit neural representations (INRs) instead of pixel arrays, and have several advantages such as out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior.
- **How**: The paper introduces factorized multiplicative modulation, which allows the INR-based decoder to produce parameters of an MLP that maps pixel coordinates to RGB values, and multi-scale INRs, which enable the decoder to capture different levels of detail at different scales. The paper also proposes a novel loss function that combines perceptual loss and adversarial loss to train the INR-GAN. The paper evaluates the proposed method on several image datasets and shows that it outperforms previous INR-based methods and reduces the gap with pixel-based methods.

## Main Contributions

[1]: https://arxiv.org/abs/2011.12026 "[2011.12026] Adversarial Generation of Continuous Images - arXiv.org"
[2]: https://arxiv.org/pdf/2011.12026v2 "Adversarial Generation of Continuous Images - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2011.12026?context=cs "[2011.12026] Adversarial Generation of Continuous Images"

According to the paper at [^1^][1], the main contributions are:

- The paper introduces **factorized multiplicative modulation**, which allows the INR-based decoder to produce parameters of an MLP that maps pixel coordinates to RGB values, and **multi-scale INRs**, which enable the decoder to capture different levels of detail at different scales.
- The paper proposes a novel **loss function** that combines perceptual loss and adversarial loss to train the INR-GAN.
- The paper evaluates the proposed method on several image datasets and shows that it **outperforms previous INR-based methods** and **reduces the gap with pixel-based methods**.

## Method Summary

[1]: https://arxiv.org/abs/2011.12026 "[2011.12026] Adversarial Generation of Continuous Images - arXiv.org"
[2]: https://arxiv.org/pdf/2011.12026v2 "Adversarial Generation of Continuous Images - arXiv.org"
[3]: http://export.arxiv.org/pdf/1911.12026 "Novel matter coupling in Einstein gravity arXiv:1911.12026v2 [gr-qc] 29 ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the **INR-based decoder** that produces parameters of an MLP that maps pixel coordinates to RGB values, and explains how it uses **factorized multiplicative modulation** to modulate the MLP's hidden layers with a latent code.
- The paper introduces the **multi-scale INRs** that consist of several INRs operating at different scales, and shows how they can be combined to produce high-resolution images with fine details.
- The paper presents the **INR-GAN** architecture that consists of an INR-based generator and a pixel-based discriminator, and details how it uses a novel **loss function** that combines perceptual loss and adversarial loss to train the generator.
- The paper discusses some **properties and benefits** of the INR-based decoders, such as out-of-the-box superresolution, meaningful image-space interpolation, accelerated inference of low-resolution images, an ability to extrapolate outside of image boundaries, and strong geometric prior.

## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper at :

```python
# Define the INR-based decoder that produces parameters of an MLP
def INR_decoder(z):
  # z is the latent code
  # Initialize the MLP with random weights and biases
  MLP = initialize_MLP()
  # Modulate the hidden layers of the MLP with factorized multiplicative modulation
  for layer in MLP.hidden_layers:
    layer = layer * modulation(z)
  # Return the MLP parameters
  return MLP.parameters

# Define the multi-scale INRs that consist of several INRs operating at different scales
def multi_scale_INRs(z):
  # z is the latent code
  # Initialize a list of INRs with different resolutions
  INRs = initialize_INRs()
  # For each INR, produce its parameters using the INR_decoder
  for i in range(len(INRs)):
    INRs[i].parameters = INR_decoder(z)
  # Return the list of INRs
  return INRs

# Define the INR-GAN architecture that consists of an INR-based generator and a pixel-based discriminator
def INR_GAN():
  # Initialize the generator and the discriminator
  generator = initialize_generator()
  discriminator = initialize_discriminator()
  # Initialize the loss function that combines perceptual loss and adversarial loss
  loss_function = initialize_loss_function()
  # Train the generator and the discriminator in an adversarial manner
  for epoch in range(num_epochs):
    for batch in data_loader:
      # Get a batch of real images and their corresponding coordinates
      real_images, coordinates = batch
      # Sample a batch of latent codes from a normal distribution
      latent_codes = sample_normal_distribution()
      # Generate a batch of fake images by evaluating the multi-scale INRs at the coordinates
      fake_images = evaluate_multi_scale_INRs(latent_codes, coordinates)
      # Compute the discriminator outputs for real and fake images
      real_outputs = discriminator(real_images)
      fake_outputs = discriminator(fake_images)
      # Compute the generator and discriminator losses using the loss function
      generator_loss = loss_function.generator_loss(real_outputs, fake_outputs, real_images, fake_images)
      discriminator_loss = loss_function.discriminator_loss(real_outputs, fake_outputs)
      # Update the generator and discriminator parameters using gradient descent
      update_parameters(generator_loss, generator.parameters)
      update_parameters(discriminator_loss, discriminator.parameters)
    # Print the epoch number and the losses
    print(epoch, generator_loss, discriminator_loss)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch # for tensors and neural networks
import torchvision # for image datasets and transforms
import numpy as np # for numerical computations
import matplotlib.pyplot as plt # for plotting images

# Define the hyperparameters
batch_size = 64 # the number of images in a batch
latent_dim = 256 # the dimension of the latent code
num_scales = 4 # the number of scales in the multi-scale INRs
num_epochs = 100 # the number of training epochs
learning_rate = 0.0002 # the learning rate for gradient descent
beta1 = 0.5 # the beta1 parameter for Adam optimizer
beta2 = 0.999 # the beta2 parameter for Adam optimizer
lambda_adv = 1.0 # the weight for the adversarial loss
lambda_per = 10.0 # the weight for the perceptual loss

# Define the INR-based decoder that produces parameters of an MLP
def INR_decoder(z):
  # z is the latent code of shape (batch_size, latent_dim)
  # Initialize the MLP with random weights and biases
  MLP = torch.nn.Sequential(
    torch.nn.Linear(2, 256), # input layer that takes pixel coordinates as input
    torch.nn.ReLU(), # activation function
    torch.nn.Linear(256, 256), # hidden layer 1
    torch.nn.ReLU(), # activation function
    torch.nn.Linear(256, 256), # hidden layer 2
    torch.nn.ReLU(), # activation function
    torch.nn.Linear(256, 3) # output layer that predicts RGB values
  )
  # Modulate the hidden layers of the MLP with factorized multiplicative modulation
  for i in range(1, len(MLP), 2): # loop over the hidden layers
    layer = MLP[i] # get the layer object
    w, b = layer.weight, layer.bias # get the weight and bias tensors of shape (out_features, in_features) and (out_features,)
    w_mod = torch.nn.Linear(latent_dim, w.shape[0] * w.shape[1]) # define a linear layer to modulate w with z of shape (batch_size, out_features * in_features)
    b_mod = torch.nn.Linear(latent_dim, b.shape[0]) # define a linear layer to modulate b with z of shape (batch_size, out_features)
    w_modulated = w_mod(z).view(w.shape) * w # modulate w with z and reshape it to w.shape of shape (out_features, in_features)
    b_modulated = b_mod(z).view(b.shape) * b # modulate b with z and reshape it to b.shape of shape (out_features,)
    layer.weight = torch.nn.Parameter(w_modulated) # update the weight parameter of the layer with w_modulated
    layer.bias = torch.nn.Parameter(b_modulated) # update the bias parameter of the layer with b_modulated
  # Return the MLP parameters as a list of tensors
  return [p for p in MLP.parameters()]

# Define the multi-scale INRs that consist of several INRs operating at different scales
def multi_scale_INRs(z):
  # z is the latent code of shape (batch_size, latent_dim)
  # Initialize a list of INRs with different resolutions
  INRs = [torch.nn.Identity()] * num_scales # each INR is an identity function that takes pixel coordinates as input and returns them as output
  resolutions = [4, 8, 16, 32] # a list of resolutions for each scale
  scales = [1.0, 0.5, 0.25, 0.125] # a list of scaling factors for each scale
  # For each INR, produce its parameters using the INR_decoder
  for i in range(len(INRs)):
    INRs[i].parameters = INR_decoder(z) # produce parameters of an MLP that maps pixel coordinates to RGB values using z
    INRs[i].resolution = resolutions[i] # assign a resolution to each INR
    INRs[i].scale = scales[i] # assign a scaling factor to each INR
  # Return the list of INRs
  return INRs

# Define a function that evaluates an INR at a given coordinate grid and returns an RGB image tensor 
def evaluate_INR(INR, grid):
  # INR is an identity function with parameters attribute that contains a list of tensors representing an MLP 
  # grid is a tensor of shape (batch_size, 3, height, width) that contains the pixel coordinates and a constant channel for each image
  batch_size, _, height, width = grid.shape # get the batch size and the image dimensions
  grid = grid.view(batch_size, 3, -1) # reshape the grid to (batch_size, 3, height * width)
  x, y, c = grid[:, 0, :], grid[:, 1, :], grid[:, 2, :] # get the x, y and c coordinates of shape (batch_size, height * width)
  x = x * INR.scale # scale the x coordinates by the INR scale factor
  y = y * INR.scale # scale the y coordinates by the INR scale factor
  c = c * 0 # set the c coordinates to zero
  input = torch.stack([x, y, c], dim=1) # stack the coordinates to form the input tensor of shape (batch_size, 3, height * width)
  input = input.permute(0, 2, 1) # permute the input tensor to shape (batch_size, height * width, 3)
  output = torch.zeros_like(input) # initialize the output tensor of shape (batch_size, height * width, 3)
  for i in range(len(INR.parameters) // 2): # loop over the MLP layers
    w, b = INR.parameters[2*i], INR.parameters[2*i+1] # get the weight and bias tensors for each layer
    output = torch.matmul(input, w.t()) + b # apply the linear transformation to the input tensor
    if i < len(INR.parameters) // 2 - 1: # if not the last layer
      output = torch.nn.functional.relu(output) # apply the ReLU activation function to the output tensor
    input = output # update the input tensor with the output tensor
  output = output.permute(0, 2, 1) # permute the output tensor to shape (batch_size, 3, height * width)
  output = output.view(batch_size, 3, height, width) # reshape the output tensor to shape (batch_size, 3, height, width)
  output = torch.nn.functional.sigmoid(output) # apply the sigmoid function to the output tensor to get RGB values in [0, 1] range
  return output # return the output tensor as an RGB image

# Define a function that evaluates a list of multi-scale INRs at a given coordinate grid and returns an RGB image tensor 
def evaluate_multi_scale_INRs(INRs, grid):
  # INRs is a list of identity functions with parameters, resolution and scale attributes 
  # grid is a tensor of shape (batch_size, 3, height, width) that contains the pixel coordinates and a constant channel for each image
  batch_size, _, height, width = grid.shape # get the batch size and the image dimensions
  output = torch.zeros(batch_size, 3, height, width) # initialize the output tensor of shape (batch_size, 3, height, width)
  for i in range(len(INRs)): # loop over the INRs
    INR = INRs[i] # get the current INR
    resolution = INR.resolution # get the resolution of the current INR
    scale = INR.scale # get the scale factor of the current INR
    grid_scaled = grid.clone() # clone the grid tensor
    grid_scaled[:, :2] = grid_scaled[:, :2] * scale # scale the x and y coordinates by the scale factor
    grid_scaled = torch.nn.functional.interpolate(grid_scaled, size=(resolution,resolution), mode='bilinear', align_corners=False) # interpolate the grid tensor to match the resolution of the current INR
    image = evaluate_INR(INR, grid_scaled) # evaluate the current INR at the scaled and interpolated grid and get an RGB image tensor of shape (batch_size, 3,resolution,resolution)
    image = torch.nn.functional.interpolate(image,size=(height,width), mode='bilinear', align_corners=False) # interpolate the image tensor to match the original image dimensions
    output = output + image # add the image tensor to the output tensor
  output = output / len(INRs) # average the output tensor by dividing it by the number of INRs
  return output # return

# Define a function that computes perceptual loss between two images using a pretrained VGG network 
def perceptual_loss(image1,image2):
# image1 and image2 are tensors of shape (batch_size ,3,height,width) that contain RGB images 
# Load a pretrained VGG network 
vgg=torchvision.models.vgg16(pretrained=True).eval() 
# Define a list of layers to use for perceptual loss computation 
layers=[vgg.features[i] for i in [