---
title: 2306.07429v1 Explaining CLIP through Co-Creative Drawings and Interaction
date: 2023-06-08
---

# [Explaining CLIP through Co-Creative Drawings and Interaction](http://arxiv.org/abs/2306.07429v1)

authors: Varvara Guljajeva, Mar Canet SolÃ , Isaac Joseph Clarke


## What, Why and How

[1]: https://arxiv.org/abs/2306.07429 "Explaining CLIP through Co-Creative Drawings and Interaction"
[2]: https://arxiv.org/abs/2304.07429 "[2304.07429] Identity Encoder for Personalized Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07429 "[2306.07429] Explaining CLIP through Co-Creative Drawings and Interaction"

Here is a summary of the paper:

- **What**: The paper analyses a visual archive of drawings produced by an interactive robotic art installation where audience members narrated their dreams into a system powered by **CLIPdraw** deep learning (DL) model that interpreted and transformed their dreams into images[^1^][1].
- **Why**: The paper aims to better understand the neural model and its outputs by examining and clustering the prompt-image pairs based on concept representation accuracy[^1^][1].
- **How**: The paper proposes four groupings for describing and explaining CLIP-generated results: **clear concept**, **text-to-text as image**, **indeterminacy and confusion**, and **lost in translation**. The paper also showcases some examples of the drawings and discusses the processes and results of translations between languages, sign-systems and various modules of the installation[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/2306.07429 "Explaining CLIP through Co-Creative Drawings and Interaction"
[2]: https://arxiv.org/abs/2304.07429 "[2304.07429] Identity Encoder for Personalized Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07429 "[2306.07429] Explaining CLIP through Co-Creative Drawings and Interaction"

According to the paper, the main contributions are:

- **Proposing a novel way of explaining CLIP** through co-creative drawings and interaction with a robotic art installation[^1^][1].
- **Creating a visual archive of dreams** interpreted and transformed by CLIPdraw, which can be used for further analysis and research[^1^][1].
- **Introducing four clusters for describing and explaining CLIP-generated results**: clear concept, text-to-text as image, indeterminacy and confusion, and lost in translation[^1^][1].
- **Providing insights into the processes and results of translations** between languages, sign-systems and various modules of the installation[^1^][1].


## Method Summary

[1]: https://arxiv.org/abs/2306.07429 "Explaining CLIP through Co-Creative Drawings and Interaction"
[2]: https://arxiv.org/pdf/2106.07429v1.pdf "Minghao Yue , Jinyi Yang ,1, 1 1, y 2, y Iskren Y. Georgiev ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07429 "[2306.07429] Explaining CLIP through Co-Creative Drawings and Interaction"

Here is a summary of the method section of the paper:

- The paper describes the **design and implementation** of an interactive robotic art installation called **Dreaming Machine**, which consists of four modules: a voice recorder, a speech-to-text converter, a CLIPdraw model, and a drawing robot[^1^][1].
- The paper explains how the **CLIPdraw model** works, which is a deep learning model that can generate images from text prompts by using a pre-trained CLIP model and a diffusion-based image generator[^1^][1].
- The paper details how the **visual archive** of drawings was created, which contains 1,000 prompt-image pairs collected from 500 participants who narrated their dreams into the system during two exhibitions[^1^][1].
- The paper presents the **analysis and clustering** of the prompt-image pairs based on concept representation accuracy, which is measured by comparing the similarity between the prompt and the CLIP features of the image[^1^][1].
- The paper proposes **four clusters** for describing and explaining CLIP-generated results: clear concept, text-to-text as image, indeterminacy and confusion, and lost in translation[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the CLIPdraw model
clipdraw = CLIP + Diffusion

# Define the Dreaming Machine installation
dreaming_machine = Voice Recorder + Speech-to-Text + CLIPdraw + Drawing Robot

# Collect the prompt-image pairs from the participants
visual_archive = []
for each participant:
  prompt = record and convert their dream narration
  image = clipdraw(prompt)
  visual_archive.append((prompt, image))
  draw(image) with the drawing robot

# Analyze and cluster the prompt-image pairs based on concept representation accuracy
clusters = []
for each pair in visual_archive:
  prompt_features = CLIP(prompt)
  image_features = CLIP(image)
  similarity = cosine_similarity(prompt_features, image_features)
  if similarity > threshold_1:
    assign pair to cluster "clear concept"
  elif similarity > threshold_2:
    assign pair to cluster "text-to-text as image"
  elif similarity > threshold_3:
    assign pair to cluster "indeterminacy and confusion"
  else:
    assign pair to cluster "lost in translation"
  clusters.append(cluster)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import clip
import numpy as np
import matplotlib.pyplot as plt
import speech_recognition as sr
import serial

# Define the CLIPdraw model
clip_model, preprocess = clip.load("ViT-B/32", device="cuda")
diffusion_model = torch.hub.load('openai/DALL-E', 'discrete_diffusion')

def clipdraw(prompt):
  # Encode the prompt into CLIP features
  text = clip.tokenize(prompt).to("cuda")
  with torch.no_grad():
    text_features = clip_model.encode_text(text)

  # Define the loss function for diffusion
  def loss_fn(x):
    with torch.no_grad():
      image_features = clip_model.encode_image(x)
      return -torch.cosine_similarity(text_features, image_features, dim=-1).mean()

  # Generate the image from the prompt using diffusion
  image = diffusion_model.sample(text, loss_fn, clip_denoised=True, batch_size=1)
  return image

# Define the Dreaming Machine installation
# Initialize the voice recorder
recorder = sr.Recognizer()

# Initialize the speech-to-text converter
converter = sr.Recognizer()

# Initialize the drawing robot
robot = serial.Serial('/dev/ttyACM0', 9600)

def dreaming_machine():
  # Record the dream narration from the participant
  with sr.Microphone() as source:
    print("Please narrate your dream:")
    audio = recorder.listen(source)

  # Convert the audio into text
  try:
    prompt = converter.recognize_google(audio)
    print("You said: " + prompt)
  except sr.UnknownValueError:
    print("Google Speech Recognition could not understand audio")
    return
  except sr.RequestError as e:
    print("Could not request results from Google Speech Recognition service; {0}".format(e))
    return

  # Generate the image from the prompt using CLIPdraw
  image = clipdraw(prompt)

  # Display the image on the screen
  plt.imshow(image)
  plt.show()

  # Draw the image with the drawing robot
  draw(image)

# Collect the prompt-image pairs from the participants
visual_archive = []
num_participants = 500

for i in range(num_participants):
  print("Participant " + str(i+1) + ":")
  dreaming_machine()
  visual_archive.append((prompt, image))

# Analyze and cluster the prompt-image pairs based on concept representation accuracy
clusters = []
threshold_1 = 0.9 # for clear concept cluster
threshold_2 = 0.7 # for text-to-text as image cluster
threshold_3 = 0.5 # for indeterminacy and confusion cluster

for pair in visual_archive:
  prompt, image = pair

  # Compute the CLIP features of the prompt and the image
  text = clip.tokenize(prompt).to("cuda")
  with torch.no_grad():
    prompt_features = clip_model.encode_text(text)
    image_features = clip_model.encode_image(image)

  # Compute the cosine similarity between the prompt and the image features
  similarity = torch.cosine_similarity(prompt_features, image_features, dim=-1).item()

  # Assign the pair to a cluster based on the similarity score
  if similarity > threshold_1:
    cluster = "clear concept"
  elif similarity > threshold_2:
    cluster = "text-to-text as image"
  elif similarity > threshold_3:
    cluster = "indeterminacy and confusion"
  else:
    cluster = "lost in translation"

  clusters.append(cluster)
```