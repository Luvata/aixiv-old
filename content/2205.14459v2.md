---
title: 2205.14459v2 CyCLIP  Cyclic Contrastive Language-Image Pretraining
date: 2022-05-15
---

# [CyCLIP: Cyclic Contrastive Language-Image Pretraining](http://arxiv.org/abs/2205.14459v2)

authors: Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan A. Rossi, Vishwa Vinay, Aditya Grover


## What, Why and How

[1]: https://arxiv.org/abs/2205.14459 "[2205.14459] CyCLIP: Cyclic Contrastive Language-Image Pretraining"
[2]: https://arxiv.org/pdf/2205.14459 "arXiv:2205.14459v2 [cs.CV] 26 Oct 2022"
[3]: http://export.arxiv.org/abs/2208.14459v2 "[2208.14459v2] The velocity distribution of outflows driven by choked ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes CyCLIP, a framework for contrastive representation learning over paired image-text data that explicitly optimizes for the learned representations to be geometrically consistent in the image and text space.
- **Why**: The paper shows that the standard contrastive objective used by models such as CLIP leads to inconsistent representations that can cause erroneous downstream predictions. The paper argues that consistency is a desirable property for vision-language pretraining and can improve zero-shot classification and robustness.
- **How**: The paper introduces two cycle consistency constraints that symmetrize the similarity between the mismatched image-text pairs (cross-modal consistency) and the similarity between the image-image pair and the text-text pair (in-modal consistency). The paper evaluates CyCLIP on standard benchmarks and shows significant gains over CLIP.

## Main Contributions

According to the paper at , the main contributions are:

- The paper formalizes the notion of consistency for vision-language contrastive learning and shows that it is not satisfied by the standard contrastive objective.
- The paper proposes CyCLIP, a simple and effective framework for contrastive representation learning with two additional cycle consistency constraints for mitigating the above issue.
- The paper demonstrates that CyCLIP achieves significant empirical improvements over CLIP on zero-shot classification and robustness benchmarks.

## Method Summary

[1]: https://arxiv.org/pdf/2205.14459 "arXiv:2205.14459v2 [cs.CV] 26 Oct 2022"
[2]: https://arxiv.org/abs/2205.14459 "[2205.14459] CyCLIP: Cyclic Contrastive Language-Image Pretraining"
[3]: http://export.arxiv.org/abs/2208.14459v2 "[2208.14459v2] The velocity distribution of outflows driven by choked ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first reviews the standard contrastive learning objective used by CLIP and defines the notion of consistency for vision-language pretraining. The paper shows that consistency requires that the similarity between matched image-text pairs should be equal to the similarity between mismatched image-text pairs and also equal to the similarity between matched image-image pairs or text-text pairs.
- The paper then introduces CyCLIP, a framework that adds two cycle consistency constraints to the standard contrastive objective. The first constraint symmetrizes the similarity between the two mismatched image-text pairs (cross-modal consistency), and the second constraint symmetrizes the similarity between the image-image pair and the text-text pair (in-modal consistency). The paper shows that these constraints can be implemented by adding two additional terms to the standard contrastive loss function.
- The paper also discusses some implementation details of CyCLIP, such as the choice of temperature parameter, the use of hard negative mining, and the use of data augmentation. The paper provides pseudocode for CyCLIP in Algorithm 1.

## Pseudo Code - High level

Here is the high-level pseudo code for CyCLIP:

```python
# Input: a batch of N image-text pairs (x_i, y_i) for i = 1,...,N
# Output: a contrastive loss L with cycle consistency constraints

# Define a vision encoder f and a language encoder g
f = VisionEncoder()
g = LanguageEncoder()

# Compute the image and text embeddings for each pair
z_i = f(x_i) # image embedding
w_i = g(y_i) # text embedding

# Normalize the embeddings to have unit norm
z_i = z_i / ||z_i||
w_i = w_i / ||w_i||

# Compute the cosine similarity matrix S between all embeddings
S = [z_1,...,z_N, w_1,...,w_N] @ [z_1,...,z_N, w_1,...,w_N]^T

# Define a temperature parameter tau and a hard negative mining parameter K
tau = TemperatureParameter()
K = HardNegativeMiningParameter()

# Compute the standard contrastive loss L_c using S and tau
L_c = StandardContrastiveLoss(S, tau)

# Compute the cross-modal consistency loss L_x using S, tau, and K
L_x = CrossModalConsistencyLoss(S, tau, K)

# Compute the in-modal consistency loss L_m using S and tau
L_m = InModalConsistencyLoss(S, tau)

# Compute the final loss L as a weighted sum of L_c, L_x, and L_m
L = L_c + alpha * L_x + beta * L_m # alpha and beta are hyperparameters

# Return L
return L
```

## Pseudo Code - Detail

Here is the detailed pseudo code for CyCLIP:

```python
# Input: a batch of N image-text pairs (x_i, y_i) for i = 1,...,N
# Output: a contrastive loss L with cycle consistency constraints

# Define a vision encoder f and a language encoder g
f = VisionEncoder()
g = LanguageEncoder()

# Compute the image and text embeddings for each pair
z_i = f(x_i) # image embedding
w_i = g(y_i) # text embedding

# Normalize the embeddings to have unit norm
z_i = z_i / ||z_i||
w_i = w_i / ||w_i||

# Compute the cosine similarity matrix S between all embeddings
S = [z_1,...,z_N, w_1,...,w_N] @ [z_1,...,z_N, w_1,...,w_N]^T

# Define a temperature parameter tau and a hard negative mining parameter K
tau = TemperatureParameter()
K = HardNegativeMiningParameter()

# Compute the standard contrastive loss L_c using S and tau
L_c = 0 # initialize the loss
for i in range(1, N+1): # loop over all pairs
  # Compute the logits for the positive and negative pairs
  logit_pos = S[i,i+N] / tau # positive logit is the similarity between matched image-text pair
  logit_neg = torch.cat([S[i,:i], S[i,i+1:i+N], S[i,i+N+1:]]) / tau # negative logits are the similarities between mismatched image-text pairs
  # Compute the softmax cross entropy loss for the current pair
  loss_i = SoftmaxCrossEntropy(logit_pos, logit_neg)
  # Add the loss to the total loss
  L_c += loss_i

# Compute the cross-modal consistency loss L_x using S, tau, and K
L_x = 0 # initialize the loss
for i in range(1, N+1): # loop over all pairs
  # Find the top-K hard negatives for image i and text i+N
  hard_neg_img = TopK(S[i,:i+N], K) # indices of top-K most similar images to image i (excluding itself)
  hard_neg_txt = TopK(S[i+N,N+1:], K) # indices of top-K most similar texts to text i+N (excluding itself)
  # Compute the logits for the cross-modal consistency pairs
  logit_img_txt = S[i,i+N] / tau # logit for matched image-text pair
  logit_txt_img = S[i+N,i] / tau # logit for matched text-image pair
  logit_img_img = S[i,hard_neg_img] / tau # logits for mismatched image-image pairs
  logit_txt_txt = S[i+N,N+hard_neg_txt] / tau # logits for mismatched text-text pairs
  # Compute the softmax cross entropy loss for the current pair
  loss_i_x = SoftmaxCrossEntropy(logit_img_txt, logit_img_img) + SoftmaxCrossEntropy(logit_txt_img, logit_txt_txt)
  # Add the loss to the total loss
  L_x += loss_i_x

# Compute the in-modal consistency loss L_m using S and tau
L_m = 0 # initialize the loss
for i in range(1, N+1): # loop over all pairs
  # Compute the logits for the in-modal consistency pairs
  logit_img_txt = S[i,i+N] / tau # logit for matched image-text pair
  logit_txt_img = S[i+N,i] / tau # logit for matched text-image pair
  logit_img_img = S[i,i] / tau # logit for matched image-image pair
  logit_txt_txt = S[i+N,i+N] / tau # logit for matched text-text pair
  # Compute the softmax cross entropy loss for the current pair
  loss_i_m = SoftmaxCrossEntropy(logit_img_txt, logit_img_img) + SoftmaxCrossEntropy(logit_txt_img, logit_txt_txt)
  # Add the loss to the total loss
  L_m += loss_i_m

# Compute the final loss L as a weighted sum of L_c, L_x, and L_m
L = L_c + alpha * L_x + beta * L_m # alpha and beta are hyperparameters

# Return L
return L

```