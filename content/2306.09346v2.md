---
title: 2306.09346v2 Rosetta Neurons  Mining the Common Units in a Model Zoo
date: 2023-06-10
---

# [Rosetta Neurons: Mining the Common Units in a Model Zoo](http://arxiv.org/abs/2306.09346v2)

authors: Amil Dravid, Yossi Gandelsman, Alexei A. Efros, Assaf Shocher


## What, Why and How

[1]: https://arxiv.org/abs/2306.09346v2 "Rosetta Neurons: Mining the Common Units in a Model Zoo"
[2]: https://arxiv.org/pdf/2306.09896 "Demystifying GPT Self-Repair for Code Generation - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2306.09346 "[2306.09346] Rosetta Neurons: Mining the Common Units in a Model Zoo"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper presents an algorithm for finding common features, called Rosetta Neurons, across different vision models trained for different tasks and with different types of supervision.
- **Why**: The paper aims to show that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. The paper also demonstrates various applications of the Rosetta Neurons, such as model-to-model translation and inversion-based manipulations.
- **How**: The paper uses a two-step approach to mine the Rosetta Neurons. First, it finds candidate neurons that have high mutual information with each other across different models. Second, it filters out the candidates that are not consistent across different inputs or are not interpretable by generative models. The paper evaluates the Rosetta Neurons on several popular vision models, such as ResNet50, DINO, MAE, CLIP, BigGAN, StyleGAN-2, and StyleGAN-XL[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/2306.09346v2 "Rosetta Neurons: Mining the Common Units in a Model Zoo"
[2]: https://arxiv.org/pdf/2306.09896 "Demystifying GPT Self-Repair for Code Generation - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2306.09346 "[2306.09346] Rosetta Neurons: Mining the Common Units in a Model Zoo"

According to the paper, the main contributions are:

- **The discovery of Rosetta Neurons**, common features across different vision models that capture visual concepts and structures without semantic labels.
- **An algorithm for mining Rosetta Neurons** that leverages mutual information and generative models to find and filter candidate neurons.
- **A comprehensive analysis of Rosetta Neurons** across several popular vision models with different architectures, tasks, and types of supervision.
- **Various applications of Rosetta Neurons** for model-to-model translation and inversion-based manipulations, such as cross-class alignments, shifting, zooming, and more[^1^][1].


## Method Summary

[1]: https://arxiv.org/abs/2306.09346v2 "Rosetta Neurons: Mining the Common Units in a Model Zoo"
[2]: https://arxiv.org/pdf/2306.09896 "Demystifying GPT Self-Repair for Code Generation - arXiv.org"
[3]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2306.09346 "[2306.09346] Rosetta Neurons: Mining the Common Units in a Model Zoo"

Here is a summary of the method section of the paper:

- The paper proposes a two-step algorithm for mining Rosetta Neurons across different vision models. The algorithm consists of a **candidate selection** step and a **candidate filtering** step.
- In the candidate selection step, the algorithm finds pairs of neurons from different models that have high mutual information with each other. This is done by computing the mutual information between every pair of neurons from different models using a large set of images as inputs. The algorithm then selects the top-k pairs of neurons for each model as candidates for Rosetta Neurons.
- In the candidate filtering step, the algorithm filters out the candidates that are not consistent across different inputs or are not interpretable by generative models. This is done by applying two criteria: **input consistency** and **generative interpretability**. Input consistency measures how well a candidate neuron responds to different inputs that share the same visual concept. Generative interpretability measures how well a candidate neuron can be visualized by generative models. The algorithm discards the candidates that do not meet a threshold for either criterion.
- The paper applies the algorithm to mine Rosetta Neurons across eight popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, and StyleGAN-XL[^1^][1]. The paper uses ImageNet as the input dataset and uses BigGAN and StyleGAN-2 as the generative models for visualization and interpretation[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a set of vision models M, a set of images I, a set of generative models G
# Output: a dictionary of Rosetta Neurons R

# Step 1: Candidate selection
R = {} # initialize an empty dictionary
for each pair of models (m1, m2) in M:
  MI = compute_mutual_information(m1, m2, I) # compute the mutual information matrix between m1 and m2 using I
  C = select_top_k_pairs(MI) # select the top-k pairs of neurons with highest mutual information
  R[(m1, m2)] = C # store the candidates in the dictionary

# Step 2: Candidate filtering
for each pair of models (m1, m2) in R:
  C = R[(m1, m2)] # get the candidates for (m1, m2)
  F = [] # initialize an empty list for filtered candidates
  for each pair of neurons (n1, n2) in C:
    IC = compute_input_consistency(n1, n2, I) # compute the input consistency score for (n1, n2) using I
    GI = compute_generative_interpretability(n1, n2, G) # compute the generative interpretability score for (n1, n2) using G
    if IC >= threshold_1 and GI >= threshold_2: # check if the criteria are met
      F.append((n1, n2)) # add the pair to the filtered list
  R[(m1, m2)] = F # update the dictionary with the filtered candidates

return R # return the dictionary of Rosetta Neurons
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: a set of vision models M, a set of images I, a set of generative models G
# Output: a dictionary of Rosetta Neurons R

# Step 1: Candidate selection
R = {} # initialize an empty dictionary
for each pair of models (m1, m2) in M:
  MI = compute_mutual_information(m1, m2, I) # compute the mutual information matrix between m1 and m2 using I
  C = select_top_k_pairs(MI) # select the top-k pairs of neurons with highest mutual information
  R[(m1, m2)] = C # store the candidates in the dictionary

# Step 2: Candidate filtering
for each pair of models (m1, m2) in R:
  C = R[(m1, m2)] # get the candidates for (m1, m2)
  F = [] # initialize an empty list for filtered candidates
  for each pair of neurons (n1, n2) in C:
    IC = compute_input_consistency(n1, n2, I) # compute the input consistency score for (n1, n2) using I
    GI = compute_generative_interpretability(n1, n2, G) # compute the generative interpretability score for (n1, n2) using G
    if IC >= threshold_1 and GI >= threshold_2: # check if the criteria are met
      F.append((n1, n2)) # add the pair to the filtered list
  R[(m1, m2)] = F # update the dictionary with the filtered candidates

return R # return the dictionary of Rosetta Neurons

# Function to compute mutual information between two models using a set of images
def compute_mutual_information(m1, m2, I):
  N1 = get_number_of_neurons(m1) # get the number of neurons in m1
  N2 = get_number_of_neurons(m2) # get the number of neurons in m2
  MI = initialize_matrix(N1, N2) # initialize a matrix of size N1 x N2 with zeros
  for each image i in I:
    A1 = get_activation_vector(m1, i) # get the activation vector of m1 for i
    A2 = get_activation_vector(m2, i) # get the activation vector of m2 for i
    B1 = binarize_vector(A1) # binarize A1 by applying a threshold
    B2 = binarize_vector(A2) # binarize A2 by applying a threshold
    for each neuron n1 in m1:
      for each neuron n2 in m2:
        MI[n1][n2] += compute_mutual_information_between_two_binary_variables(B1[n1], B2[n2]) # update the MI matrix by computing the mutual information between B1[n1] and B2[n2]
  MI = normalize_matrix(MI) # normalize the MI matrix by dividing each element by the number of images
  return MI # return the MI matrix

# Function to select top-k pairs of neurons with highest mutual information from a matrix
def select_top_k_pairs(MI):
  k = get_hyperparameter() # get the hyperparameter k from somewhere
  C = [] # initialize an empty list for candidates
  while len(C) < k: # loop until k pairs are selected
    (n1, n2) = find_max_element(MI) # find the indices of the maximum element in MI
    C.append((n1, n2)) # add the pair to the candidate list
    MI[n1][n2] = -inf # set the maximum element to negative infinity to avoid selecting it again
  return C # return the candidate list

# Function to compute input consistency score for a pair of neurons using a set of images
def compute_input_consistency(n1, n2, I):
  S = [] # initialize an empty list for similarity scores
  for each image i in I:
    A1 = get_activation_value(n1, i) # get the activation value of n1 for i
    A2 = get_activation_value(n2, i) # get the activation value of n2 for i
    S.append(compute_similarity_between_two_real_values(A1, A2)) # append the similarity score between A1 and A2 to S
  IC = average(S) # compute the input consistency score as the average of S
  return IC # return the input consistency score

# Function to compute generative interpretability score for a pair of neurons using a set of generative models
def compute_generative_interpretability(n1, n2, G):
  V = [] # initialize an empty list for visualization scores
  for each generative model g in G:
    I1 = generate_image_by_manipulating_neuron(g, n1) # generate an image by manipulating n1 in g
    I2 = generate_image_by_manipulating_neuron(g, n2) # generate an image by manipulating n2 in g
    V.append(compute_similarity_between_two_images(I1, I2)) # append the similarity score between I1 and I2 to V
  GI = average(V) # compute the generative interpretability score as the average of V
  return GI # return the generative interpretability score
```