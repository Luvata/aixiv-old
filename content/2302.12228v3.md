---
title: 2302.12228v3 Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models
date: 2023-02-13
---

# [Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models](http://arxiv.org/abs/2302.12228v3)

authors: Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2302.12228 "[2302.12228] Encoder-based Domain Tuning for Fast Personalization of ..."
[2]: https://arxiv-export1.library.cornell.edu/abs/2302.12228v3 "[2302.12228v3] Encoder-based Domain Tuning for Fast Personalization of ..."
[3]: https://arxiv.org/pdf/2302.12228.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper proposes an encoder-based domain-tuning approach for fast personalization of text-to-image models[^1^][1].
- **Why**: The paper aims to overcome the limitations of current personalization approaches, such as lengthy training times, high storage requirements or loss of identity[^1^][1].
- **How**: The paper employs two components: an encoder that maps a single image of a target concept into a word-embedding, and a set of regularized weight-offsets for the text-to-image model that learn how to ingest additional concepts[^1^][1]. The paper shows that this approach can personalize a model using only a single image and as few as 5 training steps, while preserving quality[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2302.12228 "[2302.12228] Encoder-based Domain Tuning for Fast Personalization of ..."
[2]: https://arxiv-export1.library.cornell.edu/abs/2302.12228v3 "[2302.12228v3] Encoder-based Domain Tuning for Fast Personalization of ..."
[3]: https://arxiv.org/pdf/2302.12228.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- **A novel encoder-based domain-tuning approach** for fast personalization of text-to-image models, which leverages an encoder that maps a single image of a target concept into a word-embedding, and a set of regularized weight-offsets for the text-to-image model that learn how to ingest additional concepts.
- **A large-scale evaluation** of the proposed approach on three domains: faces, animals and cars, showing that it outperforms existing methods in terms of quality, diversity and identity preservation, while requiring significantly less training time and storage space.
- **A user study** that demonstrates the effectiveness and usability of the proposed approach for creating personalized images from natural language prompts.

## Method Summary

[1]: https://arxiv.org/abs/2302.12228 "[2302.12228] Encoder-based Domain Tuning for Fast Personalization of ..."
[2]: https://arxiv-export1.library.cornell.edu/abs/2302.12228v3 "[2302.12228v3] Encoder-based Domain Tuning for Fast Personalization of ..."
[3]: https://arxiv.org/pdf/2302.12228.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper adopts a **pre-trained diffusion model** as the text-to-image model, which takes as input a text prompt and a noise image, and outputs a realistic image that matches the prompt[^1^][1].
- The paper introduces an **encoder** that takes as input a single image of a target concept from a given domain, such as a face, and outputs a word-embedding that represents the concept[^1^][1]. The encoder is trained on a large set of concepts from the same domain, using a contrastive loss that encourages similar embeddings for images of the same concept and dissimilar embeddings for images of different concepts[^1^][1].
- The paper also introduces a set of **regularized weight-offsets** for the text-to-image model, which are learned by minimizing the reconstruction loss between the output image and the input image of the target concept[^1^][1]. The weight-offsets are regularized by a sparsity constraint and a similarity constraint that encourage them to be sparse and similar to the original weights of the text-to-image model[^1^][1].
- The paper combines the encoder and the weight-offsets to perform **personalization** of the text-to-image model for unseen concepts from the same domain[^1^][1]. Given a single image of a new concept and a natural language prompt, the encoder produces a word-embedding for the concept, which is concatenated with the prompt and fed into the text-to-image model. The weight-offsets are added to the original weights of the text-to-image model to adjust its parameters for the new concept[^1^][1]. The paper shows that this process can be done in as few as 5 training steps, while preserving quality and diversity[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Pre-train a diffusion model on a large dataset of images and text prompts
diffusion_model = pre_train_diffusion_model(images, prompts)

# Train an encoder on a large set of concepts from a given domain
encoder = train_encoder(concepts, images)

# For each concept in the domain, compute a word-embedding using the encoder
embeddings = encoder(images)

# For each concept in the domain, train a set of weight-offsets for the diffusion model
weight_offsets = train_weight_offsets(diffusion_model, images, embeddings)

# For a new concept from the same domain, given a single image and a text prompt
new_image = input()
new_prompt = input()

# Compute a word-embedding for the new concept using the encoder
new_embedding = encoder(new_image)

# Concatenate the new embedding with the new prompt
new_input = concatenate(new_embedding, new_prompt)

# Add the weight-offsets to the original weights of the diffusion model
new_weights = diffusion_model.weights + weight_offsets

# Use the diffusion model with the new weights to generate an image that matches the new input
new_output = diffusion_model(new_input, new_weights)

# Display the generated image
show(new_output)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
batch_size = 64 # The number of images and prompts in each batch
num_epochs = 100 # The number of epochs to train the encoder and the weight-offsets
num_steps = 5 # The number of steps to personalize the diffusion model for a new concept
learning_rate = 0.001 # The learning rate for the optimizer
temperature = 0.07 # The temperature for the contrastive loss
lambda_1 = 0.01 # The regularization coefficient for the sparsity constraint
lambda_2 = 0.01 # The regularization coefficient for the similarity constraint

# Load the pre-trained diffusion model from https://github.com/openai/guided-diffusion
diffusion_model = torch.hub.load('openai/guided-diffusion', 'cifar10')

# Freeze the parameters of the diffusion model
for param in diffusion_model.parameters():
    param.requires_grad = False

# Define the encoder as a convolutional neural network that outputs a 768-dimensional vector
encoder = torchvision.models.resnet18(pretrained=True)
encoder.fc = torch.nn.Linear(encoder.fc.in_features, 768)

# Define the optimizer and the scheduler for the encoder
optimizer_encoder = torch.optim.Adam(encoder.parameters(), lr=learning_rate)
scheduler_encoder = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_encoder, num_epochs)

# Define the weight-offsets as a dictionary that maps each layer name to a tensor of the same shape as the original weights
weight_offsets = {}
for name, param in diffusion_model.named_parameters():
    weight_offsets[name] = torch.zeros_like(param)

# Define the optimizer and the scheduler for the weight-offsets
optimizer_weight_offsets = torch.optim.Adam(weight_offsets.values(), lr=learning_rate)
scheduler_weight_offsets = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_weight_offsets, num_epochs)

# Load the dataset of images and text prompts from https://github.com/openai/clip
dataset = torchvision.datasets.ImageFolder(root='data', transform=torchvision.transforms.ToTensor())
dataset.samples = [(path, prompt) for path, prompt in dataset.samples]
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Load the pre-trained CLIP model from https://github.com/openai/CLIP
clip_model, clip_preprocess = clip.load('ViT-B/32', device='cpu')

# Freeze the parameters of the CLIP model
for param in clip_model.parameters():
    param.requires_grad = False

# Define a function to compute the contrastive loss between two sets of embeddings
def contrastive_loss(embeddings_1, embeddings_2):
    # Normalize the embeddings to have unit norm
    embeddings_1 = embeddings_1 / embeddings_1.norm(dim=-1, keepdim=True)
    embeddings_2 = embeddings_2 / embeddings_2.norm(dim=-1, keepdim=True)

    # Compute the cosine similarity matrix
    similarity_matrix = torch.matmul(embeddings_1, embeddings_2.t())

    # Compute the positive similarity by taking the diagonal elements
    positive_similarity = torch.diag(similarity_matrix)

    # Compute the negative similarity by masking out the diagonal elements
    negative_similarity = similarity_matrix - torch.eye(batch_size) * 1e5

    # Compute the contrastive loss using a softmax with temperature
    loss = -torch.log(torch.exp(positive_similarity / temperature) / torch.sum(torch.exp(negative_similarity / temperature), dim=-1))

    # Return the mean loss over the batch
    return loss.mean()

# Define a function to compute the reconstruction loss between two sets of images
def reconstruction_loss(images_1, images_2):
    # Compute the mean squared error between the images
    loss = torch.nn.functional.mse_loss(images_1, images_2)

    # Return the mean loss over the batch
    return loss.mean()

# Define a function to compute the sparsity constraint for the weight-offsets
def sparsity_constraint(weight_offsets):
    # Compute the L1 norm of each weight-offset tensor
    norms = [torch.norm(offset, p=1) for offset in weight_offsets.values()]

    # Compute the mean norm over all layers
    mean_norm = torch.mean(torch.stack(norms))

    # Return the mean norm multiplied by a regularization coefficient
    return lambda_1 * mean_norm

# Define a function to compute the similarity constraint for the weight-offsets and the original weights
def similarity_constraint(weight_offsets, diffusion_model):
    # Compute the cosine similarity between each weight-offset tensor and the corresponding original weight tensor
    similarities = [torch.nn.functional.cosine_similarity(offset, diffusion_model.state_dict()[name]) for name, offset in weight_offsets.items()]

    # Compute the mean similarity over all layers
    mean_similarity = torch.mean(torch.stack(similarities))

    # Return the negative mean similarity multiplied by a regularization coefficient
    return -lambda_2 * mean_similarity

# Train the encoder and the weight-offsets on the dataset
for epoch in range(num_epochs):
    # Loop over the batches of images and prompts
    for images, prompts in dataloader:
        # Preprocess the images and the prompts using the CLIP model
        images = clip_preprocess(images)
        prompts = clip.tokenize(prompts)

        # Compute the embeddings for the images using the encoder
        image_embeddings = encoder(images)

        # Compute the embeddings for the prompts using the CLIP model
        prompt_embeddings = clip_model.encode_text(prompts)

        # Compute the contrastive loss between the image embeddings and the prompt embeddings
        loss_contrastive = contrastive_loss(image_embeddings, prompt_embeddings)

        # Generate new images from the prompts using the diffusion model
        new_images = diffusion_model(prompt_embeddings)

        # Compute the reconstruction loss between the new images and the original images
        loss_reconstruction = reconstruction_loss(new_images, images)

        # Compute the sparsity constraint for the weight-offsets
        loss_sparsity = sparsity_constraint(weight_offsets)

        # Compute the similarity constraint for the weight-offsets and the original weights
        loss_similarity = similarity_constraint(weight_offsets, diffusion_model)

        # Compute the total loss as a weighted sum of the individual losses
        loss_total = loss_contrastive + loss_reconstruction + loss_sparsity + loss_similarity

        # Backpropagate the gradients and update the parameters of the encoder and the weight-offsets
        optimizer_encoder.zero_grad()
        optimizer_weight_offsets.zero_grad()
        loss_total.backward()
        optimizer_encoder.step()
        optimizer_weight_offsets.step()

    # Update the learning rate schedulers for the encoder and the weight-offsets
    scheduler_encoder.step()
    scheduler_weight_offsets.step()

# Save the encoder and the weight-offsets to disk
torch.save(encoder, 'encoder.pth')
torch.save(weight_offsets, 'weight_offsets.pth')

# Load a new image of a concept from the same domain and a text prompt from the user
new_image = input('Enter a path to an image of a new concept: ')
new_prompt = input('Enter a text prompt for generating an image: ')

# Preprocess the new image using the CLIP model
new_image = clip_preprocess(new_image)

# Compute an embedding for the new image using the encoder
new_embedding = encoder(new_image)

# Concatenate the new embedding with an embedding for the new prompt using the CLIP model
new_input = torch.cat([new_embedding, clip_model.encode_text(new_prompt)], dim=-1)

# Personalize the diffusion model for the new concept using a few training steps
for step in range(num_steps):
    # Generate a new image from the new input using the diffusion model
    new_output = diffusion_model(new_input)

    # Compute a reconstruction loss between the new output and the original image
    loss_reconstruction = reconstruction_loss(new_output, new_image)

    # Backpropagate the gradients and update only the weight-offsets (not the encoder or the diffusion model)
    optimizer_weight_offsets.zero_grad()
    loss_reconstruction.backward()
    optimizer_weight_offsets.step()

# Display the generated image
show(new_output)
```