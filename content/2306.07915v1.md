---
title: 2306.07915v1 Image Captioners Are Scalable Vision Learners Too
date: 2023-06-08
---

# [Image Captioners Are Scalable Vision Learners Too](http://arxiv.org/abs/2306.07915v1)

authors: Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, Lucas Beyer


## What, Why and How

[1]: https://arxiv.org/abs/2306.07915 "[2306.07915] Image Captioners Are Scalable Vision Learners Too - arXiv.org"
[2]: https://arxiv.org/pdf/2306.07915.pdf "Abstract - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2306.07915 "[2306.07915] Image Captioners Are Scalable Vision Learners Too"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper compares two pretraining strategies for vision backbones: contrastive pretraining on image-text pairs (CLIP) and generative pretraining on image captioning (Cap).
- **Why**: The paper aims to show that image captioning is a more powerful pretraining strategy than was previously believed, and that it can produce competitive or even superior vision encoders than contrastive pretraining.
- **How**: The paper performs a fair comparison of CLIP and Cap, carefully matching training data, compute, and model capacity. The paper uses a standard encoder-decoder transformer for both methods, and evaluates the vision encoders on various downstream tasks such as classification, retrieval, and vision & language tasks. The paper also analyzes the effect of the model architecture and scale, as well as the pretraining data on the representation quality.



## Main Contributions

[1]: https://arxiv.org/abs/2306.07915 "[2306.07915] Image Captioners Are Scalable Vision Learners Too - arXiv.org"
[2]: https://arxiv.org/pdf/2306.07915.pdf "Abstract - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2306.07915 "[2306.07915] Image Captioners Are Scalable Vision Learners Too"

According to the paper[^1^][1], the main contributions are:

- They show that image captioning alone can produce vision encoders that are competitive with contrastively pretrained ones on classification tasks, and surpass them on vision & language tasks.
- They demonstrate that image captioning exhibits the same or better scaling behavior as contrastive pretraining along the axes of model architecture, model scale, and pretraining data.
- They provide insights into the differences between CLIP and Cap models, such as their sensitivity to data quality, their ability to generate captions, and their performance on different types of downstream tasks.


## Method Summary

[1]: https://arxiv.org/abs/2306.07915 "[2306.07915] Image Captioners Are Scalable Vision Learners Too - arXiv.org"
[2]: https://arxiv.org/pdf/2306.07915.pdf "Abstract - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2306.07915 "[2306.07915] Image Captioners Are Scalable Vision Learners Too"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses two types of models: CLIP and Cap. Both models consist of a Vision Transformer (ViT) encoder and a Transformer decoder, but differ in their pretraining objectives. CLIP uses a contrastive loss that maximizes the similarity between image and text embeddings, while Cap uses a generative loss that minimizes the cross-entropy between predicted and ground-truth captions.
- The paper uses three datasets for pretraining: Conceptual Captions 3M (CC3M) [54], Conceptual Captions 12M (CC12M) [25], and JFT-300M [48]. CC3M and CC12M are image-text datasets collected from the web, while JFT-300M is an image-only dataset with noisy labels. The paper also uses a subset of CC12M with higher quality captions, denoted as CC12M-HQ.
- The paper trains CLIP and Cap models with different architectures (base and large), different scales (small, medium, and large), and different data sources (CC3M, CC12M, CC12M-HQ, and JFT-300M). The paper matches the training data, compute, and model capacity across different models as much as possible.
- The paper evaluates the vision encoders on various downstream tasks, such as image classification (ImageNet [11], VTAB [14], Food101 [4]), image retrieval (Flickr30k [55], MSCOCO [32]), and vision & language tasks (VQA [3], NLVR2 [49], GQA [24]). The paper also evaluates the full encoder-decoder models on image captioning tasks (Flickr30k [55], MSCOCO [32]).


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the model architecture
encoder = VisionTransformer()
decoder = Transformer()

# Define the pretraining objectives
if model_type == "CLIP":
  loss = contrastive_loss(image_embedding, text_embedding)
elif model_type == "Cap":
  loss = generative_loss(predicted_caption, ground_truth_caption)

# Pretrain the model on image-text pairs
for image, text in data_loader:
  image_embedding = encoder(image)
  text_embedding, predicted_caption = decoder(text, image_embedding)
  loss = compute_loss(model_type, image_embedding, text_embedding, predicted_caption, text)
  optimizer.step(loss)

# Evaluate the model on downstream tasks
for task in tasks:
  if task == "classification" or "retrieval":
    # Use only the encoder
    image_embedding = encoder(image)
    output = task_specific_head(image_embedding)
    metric = compute_metric(output, label)
  elif task == "vision & language":
    # Use both the encoder and the decoder
    image_embedding = encoder(image)
    text_embedding, output = decoder(question, image_embedding)
    metric = compute_metric(output, answer)
  elif task == "captioning":
    # Use both the encoder and the decoder
    image_embedding = encoder(image)
    predicted_caption = decoder.generate(image_embedding)
    metric = compute_metric(predicted_caption, ground_truth_caption)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import datasets

# Define the model architecture
class VisionTransformer(torch.nn.Module):
  def __init__(self, image_size, patch_size, num_channels, num_classes, hidden_dim, num_heads, num_layers, dropout):
    super().__init__()
    # Divide the image into patches and project them to hidden_dim
    self.patch_embedding = torchvision.models.vision_transformer.PatchEmbedding(image_size, patch_size, num_channels, hidden_dim)
    # Add a learnable class token and a position embedding
    self.class_token = torch.nn.Parameter(torch.zeros(1, 1, hidden_dim))
    self.position_embedding = torch.nn.Parameter(torch.zeros(1, (image_size // patch_size) ** 2 + 1, hidden_dim))
    # Apply a transformer encoder
    self.transformer = transformers.TransformerEncoder(hidden_dim, num_heads, num_layers, dropout)
    # Apply a linear layer for classification
    self.linear = torch.nn.Linear(hidden_dim, num_classes)

  def forward(self, x):
    # x: (batch_size, num_channels, image_size, image_size)
    # Get the patch embeddings
    x = self.patch_embedding(x) # (batch_size, (image_size // patch_size) ** 2, hidden_dim)
    # Add the class token at the beginning
    x = torch.cat([self.class_token.expand(x.size(0), -1, -1), x], dim=1) # (batch_size, (image_size // patch_size) ** 2 + 1, hidden_dim)
    # Add the position embedding
    x = x + self.position_embedding # (batch_size, (image_size // patch_size) ** 2 + 1, hidden_dim)
    # Apply the transformer encoder
    x = self.transformer(x) # (batch_size, (image_size // patch_size) ** 2 + 1, hidden_dim)
    # Take the output of the class token
    x = x[:, 0] # (batch_size, hidden_dim)
    # Apply the linear layer for classification
    x = self.linear(x) # (batch_size, num_classes)
    return x

class Transformer(torch.nn.Module):
  def __init__(self, vocab_size, max_length, hidden_dim, num_heads, num_layers, dropout):
    super().__init__()
    # Embed the input tokens and add a position embedding
    self.token_embedding = torch.nn.Embedding(vocab_size, hidden_dim)
    self.position_embedding = torch.nn.Parameter(torch.zeros(1, max_length, hidden_dim))
    # Apply a transformer decoder with cross-attention
    self.transformer = transformers.TransformerDecoder(hidden_dim, num_heads, num_layers, dropout)
    # Apply a linear layer for generation
    self.linear = torch.nn.Linear(hidden_dim, vocab_size)

  def forward(self, x, y=None):
    # x: (batch_size, hidden_dim) or (batch_size, (image_size // patch_size) ** 2 + 1 , hidden_dim)
    # y: (batch_size, max_length) or None
    if y is not None:
      # Training mode
      # Get the token embeddings and add the position embedding
      y = self.token_embedding(y) # (batch_size, max_length ,hidden_dim)
      y = y + self.position_embedding # (batch_size ,max_length ,hidden_dim)
      # Apply the transformer decoder with cross-attention to x
      y = self.transformer(y ,x) # (batch_size ,max_length ,hidden_dim)
      # Apply the linear layer for generation
      y = self.linear(y) # (batch_size ,max_length ,vocab_size)
      return y
    else:
      # Inference mode
      # Initialize the output sequence with a start token
      y = torch.ones(x.size(0), 1).long().to(x.device) # (batch_size ,1)
      outputs = []
      for i in range(max_length):
        # Get the token embeddings and add the position embedding
        y_emb = self.token_embedding(y) # (batch_size ,i+1 ,hidden_dim)
        y_emb = y_emb + self.position_embedding[:, :i+1] # (batch_size ,i+1 ,hidden_dim)
        # Apply the transformer decoder with cross-attention to x
        y_out = self.transformer(y_emb ,x)[:, -1] # (batch_size ,hidden_dim)
        # Apply the linear layer for generation
        y_out = self.linear(y_out) # (batch_size ,vocab_size)
        # Get the most probable token and append it to the output sequence
        y = torch.argmax(y_out, dim=1, keepdim=True) # (batch_size ,1)
        outputs.append(y)
      # Concatenate the output tokens
      outputs = torch.cat(outputs, dim=1) # (batch_size ,max_length)
      return outputs

# Define the pretraining objectives
def contrastive_loss(image_embedding, text_embedding, temperature):
  # image_embedding: (batch_size, hidden_dim)
  # text_embedding: (batch_size, hidden_dim)
  # temperature: a scalar
  # Compute the cosine similarity between image and text embeddings
  similarity = torch.nn.functional.cosine_similarity(image_embedding.unsqueeze(1), text_embedding.unsqueeze(0), dim=2) # (batch_size, batch_size)
  # Normalize the similarity by the temperature
  similarity = similarity / temperature # (batch_size, batch_size)
  # Compute the softmax along the rows and columns
  row_softmax = torch.nn.functional.softmax(similarity, dim=1) # (batch_size, batch_size)
  col_softmax = torch.nn.functional.softmax(similarity, dim=0) # (batch_size, batch_size)
  # Compute the cross-entropy loss for each row and column
  row_loss = -torch.mean(torch.log(torch.diag(row_softmax))) # a scalar
  col_loss = -torch.mean(torch.log(torch.diag(col_softmax))) # a scalar
  # Return the average of the row and column losses
  return (row_loss + col_loss) / 2

def generative_loss(predicted_caption, ground_truth_caption):
  # predicted_caption: (batch_size, max_length, vocab_size)
  # ground_truth_caption: (batch_size, max_length)
  # Compute the cross-entropy loss between predicted and ground-truth captions
  return torch.nn.functional.cross_entropy(predicted_caption.view(-1, vocab_size), ground_truth_caption.view(-1))

# Pretrain the model on image-text pairs
# Load the image-text data
data_loader = datasets.load_dataset("conceptual_captions")
# Initialize the model
model_type = "CLIP" or "Cap"
encoder = VisionTransformer(...)
decoder = Transformer(...)
# Initialize the optimizer
optimizer = torch.optim.Adam(...)
# Initialize the temperature for contrastive loss
temperature = 0.07
# Loop over the data
for image, text in data_loader:
  # Convert the image and text to tensors
  image = torchvision.transforms.ToTensor()(image) # (num_channels, image_size, image_size)
  text = torch.tensor(tokenizer.encode(text)) # (max_length)
  # Get the image and text embeddings from the encoder and decoder
  image_embedding = encoder(image) # (hidden_dim)
  text_embedding, predicted_caption = decoder(text, image_embedding) # (hidden_dim), (max_length, vocab_size)
  # Compute the loss according to the model type
  if model_type == "CLIP":
    loss = contrastive_loss(image_embedding, text_embedding, temperature)
  elif model_type == "Cap":
    loss = generative_loss(predicted_caption, text)
  # Update the model parameters
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

# Evaluate the model on downstream tasks
# Load the downstream tasks
tasks = ["classification", "retrieval", "vision & language", "captioning"]
for task in tasks:
  task_data_loader = datasets.load_dataset(task)
  if task == "classification" or "retrieval":
    # Use only the encoder
    for image, label in task_data_loader:
      # Convert the image and label to tensors
      image = torchvision.transforms.ToTensor()(image) # (num_channels ,image_size ,image_size)
      label = torch.tensor(label) # a scalar or a vector
      # Get the image embedding from the encoder
      image_embedding = encoder(image) # (hidden_dim)
      # Apply a task-specific head to get the output
      output = task_specific_head(image_embedding) # a scalar or a vector
      # Compute the metric according to the task type
      if task == "classification":
        metric = accuracy(output ,label)
      elif task == "retrieval":
        metric = recall(output ,label)
    print(f"Metric for {task}: {metric}")