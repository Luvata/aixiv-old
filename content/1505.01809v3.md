---
title: 1505.01809v3 Language Models for Image Captioning  The Quirks and What Works
date: 2015-05-02
---

# [Language Models for Image Captioning: The Quirks and What Works](http://arxiv.org/abs/1505.01809v3)

authors: Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell


## What, Why and How

[1]: https://arxiv.org/pdf/1505.01809v3.pdf "arXiv:1505.01809v3 [cs.CL] 14 Oct 2015"
[2]: https://arxiv.org/abs/1505.01809 "Language Models for Image Captioning: The Quirks and What Works"
[3]: http://export.arxiv.org/abs/1805.01809v3 "Noncommutative Noether's Problem vs Classical Noether's Problem"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper compares two approaches for image captioning: one that uses a maximum entropy language model (ME LM) to generate sentences from a bag of words predicted by a convolutional neural network (CNN), and another that uses a recurrent neural network language model (RNN LM) to generate captions directly from the CNN features.
- **Why**: The paper aims to evaluate the strengths and weaknesses of the different language modeling approaches for image captioning, and to improve the performance and diversity of the generated captions.
- **How**: The paper uses the same state-of-the-art CNN as input to both ME LM and RNN LM models, and compares them on the COCO dataset using automatic metrics (BLEU) and human judgments. The paper also examines issues such as linguistic irregularities, caption repetition, and data set overlap. The paper proposes a hybrid model that combines aspects of both ME LM and RNN LM methods, and achieves a new record performance on the COCO dataset.

## Main Contributions

The paper claims to make the following contributions:

- It is the first to compare the ME LM and RNN LM approaches for image captioning using the same CNN as input.
- It identifies and analyzes several issues with the existing models and the COCO dataset, such as linguistic irregularities, caption repetition, and data set overlap.
- It proposes a hybrid model that combines the advantages of both ME LM and RNN LM methods, and achieves a new state-of-the-art performance on the COCO dataset.
- It conducts human evaluation experiments to assess the quality and diversity of the generated captions.

## Method Summary

[1]: https://arxiv.org/pdf/1505.01809v3.pdf "arXiv:1505.01809v3 [cs.CL] 14 Oct 2015"
[2]: https://arxiv.org/abs/1505.01809 "Language Models for Image Captioning: The Quirks and What Works"
[3]: http://export.arxiv.org/abs/1805.01809v3 "Noncommutative Noether's Problem vs Classical Noether's Problem"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes three types of language models for image captioning: ME LM, RNN LM, and a hybrid model that combines both.
- The ME LM uses a two-step process: first, a CNN predicts a bag of words from the image; second, a maximum entropy model generates a sentence that covers a minimum number of the predicted words.
- The RNN LM uses a one-step process: it takes the penultimate activation layer of the CNN as input and generates a caption sequence using a recurrent neural network with long short-term memory (LSTM) units.
- The hybrid model uses the same CNN as input, but instead of generating a bag of words, it generates a set of semantic concepts that are more abstract and diverse. Then, it uses an RNN LM to generate a caption that covers all the concepts.
- The paper uses the same state-of-the-art CNN (VGGNet) for all the models, and trains them on the COCO dataset. The paper evaluates the models using automatic metrics (BLEU) and human judgments on a subset of images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the CNN model (VGGNet)
CNN = VGGNet()

# Define the ME LM model
ME_LM = MaximumEntropyLanguageModel()

# Define the RNN LM model
RNN_LM = RecurrentNeuralNetworkLanguageModel()

# Define the hybrid model
HYBRID = HybridLanguageModel()

# Load the COCO dataset
COCO = load_dataset("COCO")

# Train the models on the COCO dataset
for image, caption in COCO:
  # Extract the CNN features from the image
  features = CNN(image)

  # Train the ME LM model
  # Predict a bag of words from the features
  words = ME_LM.predict_words(features)
  # Generate a sentence that covers the words
  sentence = ME_LM.generate_sentence(words)
  # Update the ME LM parameters based on the caption
  ME_LM.update_parameters(sentence, caption)

  # Train the RNN LM model
  # Generate a caption sequence from the features
  sequence = RNN_LM.generate_sequence(features)
  # Update the RNN LM parameters based on the caption
  RNN_LM.update_parameters(sequence, caption)

  # Train the hybrid model
  # Predict a set of concepts from the features
  concepts = HYBRID.predict_concepts(features)
  # Generate a caption that covers the concepts
  caption = HYBRID.generate_caption(concepts)
  # Update the hybrid model parameters based on the caption
  HYBRID.update_parameters(caption, caption)

# Evaluate the models on a subset of images
for image in COCO_test:
  # Extract the CNN features from the image
  features = CNN(image)

  # Generate captions using each model
  me_caption = ME_LM.generate_caption(features)
  rnn_caption = RNN_LM.generate_caption(features)
  hybrid_caption = HYBRID.generate_caption(features)

  # Compute the BLEU scores for each caption
  me_bleu = BLEU(me_caption, reference_captions)
  rnn_bleu = BLEU(rnn_caption, reference_captions)
  hybrid_bleu = BLEU(hybrid_caption, reference_captions)

  # Collect human judgments for each caption
  me_judgment = get_human_judgment(me_caption)
  rnn_judgment = get_human_judgment(rnn_caption)
  hybrid_judgment = get_human_judgment(hybrid_caption)

# Compare the models based on BLEU scores and human judgments
compare_models(ME_LM, RNN_LM, HYBRID, bleu_scores, human_judgments)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for deep learning
import torchvision # for computer vision
import nltk # for natural language processing
import numpy as np # for numerical computation
import pandas as pd # for data manipulation
import matplotlib.pyplot as plt # for visualization

# Define the CNN model (VGGNet)
# Use the pretrained model from torchvision
CNN = torchvision.models.vgg16(pretrained=True)
# Remove the last layer (classifier) and keep only the feature extractor
CNN = CNN.features
# Freeze the parameters of the CNN
for param in CNN.parameters():
  param.requires_grad = False

# Define the ME LM model
class MaximumEntropyLanguageModel(torch.nn.Module):
  def __init__(self, vocab_size, embedding_size, hidden_size):
    super(MaximumEntropyLanguageModel, self).__init__()
    # Define the word embedding layer
    self.embedding = torch.nn.Embedding(vocab_size, embedding_size)
    # Define the hidden layer
    self.hidden = torch.nn.Linear(embedding_size + hidden_size, hidden_size)
    # Define the output layer
    self.output = torch.nn.Linear(hidden_size, vocab_size)
    # Define the activation function (tanh)
    self.activation = torch.nn.Tanh()
    # Define the softmax function
    self.softmax = torch.nn.Softmax(dim=1)
  
  def forward(self, words, features):
    # Embed the words into vectors
    word_vectors = self.embedding(words)
    # Concatenate the word vectors and the features
    inputs = torch.cat([word_vectors, features], dim=1)
    # Compute the hidden state
    hidden_state = self.activation(self.hidden(inputs))
    # Compute the output logits
    output_logits = self.output(hidden_state)
    # Compute the output probabilities
    output_probs = self.softmax(output_logits)
    return output_probs
  
  def predict_words(self, features):
    # Initialize an empty list of words
    words = []
    # Set a threshold for word selection
    threshold = 0.5
    # For each feature vector in the batch
    for feature in features:
      # Repeat the feature vector to match the vocab size
      feature = feature.repeat(vocab_size, 1)
      # Generate a random word index
      word_index = np.random.randint(0, vocab_size)
      # Get the corresponding word vector
      word_vector = self.embedding(torch.tensor(word_index))
      # Concatenate the word vector and the feature vector
      input = torch.cat([word_vector, feature], dim=1)
      # Compute the output probability for the word index
      output_prob = self.forward(input)[word_index]
      # If the output probability is greater than the threshold
      if output_prob > threshold:
        # Add the word index to the list of words
        words.append(word_index)
    return words
  
  def generate_sentence(self, words):
    # Initialize an empty sentence
    sentence = ""
    # Initialize a start token (<s>)
    start_token = "<s>"
    # Initialize an end token (</s>)
    end_token = "</s>"
    # Add the start token to the sentence
    sentence += start_token + " "
    # While the end token is not in the sentence and the sentence length is less than a maximum length (20)
    while end_token not in sentence and len(sentence.split()) < 20:
      # Initialize an empty list of candidates
      candidates = []
      # For each word index in the list of words
      for word_index in words:
        # Get the corresponding word string
        word_string = index_to_word[word_index]
        # If the word string is not a punctuation mark
        if word_string not in [".", ",", "!", "?", ";", ":"]:
          # Add the word string to the candidates list
          candidates.append(word_string)
      # If there are no candidates left
      if len(candidates) == 0:
        # Break the loop
        break
      # Randomly select a candidate from the list
      candidate = np.random.choice(candidates)
      # Add the candidate to the sentence with a space
      sentence += candidate + " "
      # Remove the candidate from the list of words
      words.remove(word_to_index[candidate])
    # Add a period to the sentence if it does not end with a punctuation mark
    if sentence[-2] not in [".", ",", "!", "?", ";", ":"]:
      sentence += "."
    # Add the end token to the sentence with a space
    sentence += " " + end_token 
    return sentence
  
  def update_parameters(self, sentence, caption):
    # Convert the sentence and the caption to tensors of word indices
    sentence = torch.tensor([word_to_index[word] for word in sentence.split()])
    caption = torch.tensor([word_to_index[word] for word in caption.split()])
    # Compute the loss function (cross entropy)
    loss = torch.nn.CrossEntropyLoss()
    # Compute the loss value
    loss_value = loss(self.forward(sentence, features), caption)
    # Compute the gradients
    loss_value.backward()
    # Define the optimizer (Adam)
    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
    # Update the parameters
    optimizer.step()

# Define the RNN LM model
class RecurrentNeuralNetworkLanguageModel(torch.nn.Module):
  def __init__(self, vocab_size, embedding_size, hidden_size):
    super(RecurrentNeuralNetworkLanguageModel, self).__init__()
    # Define the word embedding layer
    self.embedding = torch.nn.Embedding(vocab_size, embedding_size)
    # Define the LSTM layer
    self.lstm = torch.nn.LSTM(embedding_size + hidden_size, hidden_size)
    # Define the output layer
    self.output = torch.nn.Linear(hidden_size, vocab_size)
    # Define the softmax function
    self.softmax = torch.nn.Softmax(dim=1)
  
  def forward(self, features):
    # Initialize the hidden state and the cell state with zeros
    hidden_state = torch.zeros(1, 1, hidden_size)
    cell_state = torch.zeros(1, 1, hidden_size)
    # Initialize an empty list of output probabilities
    output_probs = []
    # Initialize a start token (<s>)
    start_token = "<s>"
    # Get the word index of the start token
    word_index = word_to_index[start_token]
    # While the word index is not an end token (</s>) and the output length is less than a maximum length (20)
    while word_index != word_to_index["</s>"] and len(output_probs) < 20:
      # Get the corresponding word vector
      word_vector = self.embedding(torch.tensor(word_index))
      # Concatenate the word vector and the feature vector
      input = torch.cat([word_vector, feature], dim=1).unsqueeze(0).unsqueeze(0)
      # Compute the LSTM output and update the hidden state and the cell state
      lstm_output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))
      # Compute the output logits
      output_logits = self.output(lstm_output.squeeze(0))
      # Compute the output probabilities
      output_prob = self.softmax(output_logits)
      # Append the output probability to the list
      output_probs.append(output_prob)
      # Sample a word index from the output probability distribution
      word_index = torch.multinomial(output_prob, 1).item()
    
    return output_probs
  
  def generate_sequence(self, features):
    # Initialize an empty sequence
    sequence = []
    # Get the output probabilities from the forward pass
    output_probs = self.forward(features)
    # For each output probability in the list
    for output_prob in output_probs:
      # Sample a word index from the output probability distribution
      word_index = torch.multinomial(output_prob, 1).item()
      # Append the word index to the sequence
      sequence.append(word_index)
    
    return sequence
  
  def update_parameters(self, sequence, caption):
    # Convert the sequence and the caption to tensors of word indices
    sequence = torch.tensor(sequence)
    caption = torch.tensor([word_to_index[word] for word in caption.split()])
    # Compute the loss function (cross entropy)
    loss = torch.nn.CrossEntropyLoss()
    # Compute the loss value
    loss_value = loss(self.forward(features), caption)
    # Compute the gradients
    loss_value.backward()
    # Define the optimizer (Adam)
    optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
    # Update the parameters
    optimizer.step()

# Define the hybrid model
class HybridLanguageModel(torch.nn.Module):
  def __init__(self, vocab_size, embedding_size, hidden_size):
     super(HybridLanguageModel, self).__init__()
     # Define the concept prediction layer
     self.concept_prediction = torch.nn.Linear(hidden_size, vocab_size)
     # Define the RNN LM model
     self.rnn_lm = RecurrentNeuralNetworkLanguageModel(vocab_size, embedding_size, hidden_size)

  def forward(self, features):
     # Predict a set of concepts from the features using a linear layer
     concept_logits = self.concept_prediction(features)
     concept_probs = self.softmax(concept_logits)
     concepts = []
     for concept_prob in concept_probs:
       concept_index = torch.multinomial(concept_prob, 1).item()
       concepts.append(concept_index)

     return concepts