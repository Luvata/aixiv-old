---
title: 2112.08614v2 KAT  A Knowledge Augmented Transformer for Vision-and-Language
date: 2021-12-09
---

# [KAT: A Knowledge Augmented Transformer for Vision-and-Language](http://arxiv.org/abs/2112.08614v2)

authors: Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, Jianfeng Gao


## What, Why and How

[1]: https://arxiv.org/pdf/2112.08614v2.pdf "arXiv:2112.08614v2 [cs.CL] 5 May 2022"
[2]: https://arxiv.org/abs/2112.08614 "KAT: A Knowledge Augmented Transformer for Vision-and-Language"
[3]: http://export.arxiv.org/abs/2108.08614v2 "[2108.08614v2] UNIQORN: Unified Question Answering over RDF Knowledge ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel model called **Knowledge Augmented Transformer (KAT)** for vision-and-language tasks that require external knowledge, such as OK-VQA.
- **Why**: The paper aims to address the challenges of integrating implicit and explicit knowledge in multimodal transformers, such as the quality and relevance of the retrieved knowledge, and the joint reasoning over both knowledge sources during answer generation.
- **How**: The paper introduces a novel end-to-end encoder-decoder architecture that integrates implicit and explicit knowledge in both encoding and decoding stages. The encoder consists of a multimodal transformer that fuses image and question features, and a knowledge transformer that encodes retrieved knowledge facts. The decoder is a generative transformer that attends to both the multimodal and knowledge encoders, and generates answers that leverage both implicit and explicit knowledge. The paper also proposes a novel knowledge retrieval method that uses image regions as queries to retrieve relevant facts from a large-scale knowledge base. The paper evaluates the proposed model on the OK-VQA dataset and shows that it achieves a strong state-of-the-art result (+6% absolute) and improves the interpretability of model predictions.

## Main Contributions

The paper claims the following contributions:

- A novel model called **Knowledge Augmented Transformer (KAT)** that integrates implicit and explicit knowledge in an end-to-end encoder-decoder architecture for vision-and-language tasks that require external knowledge.
- A novel knowledge retrieval method that uses image regions as queries to retrieve relevant facts from a large-scale knowledge base.
- A strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA, and improved interpretability of model predictions.

## Method Summary

The method section of the paper describes the proposed model **KAT** in detail. It consists of three main components: a multimodal encoder, a knowledge encoder, and a generative decoder. The multimodal encoder fuses image and question features using a transformer. The knowledge encoder encodes retrieved knowledge facts using another transformer. The generative decoder attends to both the multimodal and knowledge encoders, and generates answers that leverage both implicit and explicit knowledge. The paper also describes the novel knowledge retrieval method that uses image regions as queries to retrieve relevant facts from a large-scale knowledge base. The paper provides the details of the model architecture, the training objective, and the inference procedure. The paper also discusses some design choices and ablation studies of the model components.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image I and a question Q
# Output: an answer A

# Extract image features using Faster R-CNN
image_features = extract_image_features(I)

# Encode question using BERT
question_features = encode_question(Q)

# Fuse image and question features using a multimodal transformer
multimodal_features = fuse_features(image_features, question_features)

# Retrieve knowledge facts using image regions as queries
knowledge_facts = retrieve_knowledge(image_features)

# Encode knowledge facts using another transformer
knowledge_features = encode_knowledge(knowledge_facts)

# Generate answer using a generative transformer that attends to both multimodal and knowledge features
answer = generate_answer(multimodal_features, knowledge_features)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: an image I and a question Q
# Output: an answer A

# Import the required libraries
import torch
import transformers
import fasterrcnn
import wikidata

# Define the model hyperparameters
num_image_regions = 36 # number of image regions to extract
image_feature_dim = 2048 # dimension of image features
question_feature_dim = 768 # dimension of question features
knowledge_feature_dim = 768 # dimension of knowledge features
answer_feature_dim = 768 # dimension of answer features
num_multimodal_layers = 12 # number of layers in multimodal transformer
num_knowledge_layers = 6 # number of layers in knowledge transformer
num_decoder_layers = 6 # number of layers in generative transformer
num_heads = 12 # number of attention heads in transformers
dropout_rate = 0.1 # dropout rate in transformers
vocab_size = 30522 # size of vocabulary for answer generation

# Initialize the model components
image_extractor = fasterrcnn.resnet50_fpn(pretrained=True) # image feature extractor
question_encoder = transformers.BertModel.from_pretrained('bert-base-uncased') # question encoder
multimodal_encoder = transformers.TransformerEncoder(num_multimodal_layers, image_feature_dim + question_feature_dim, num_heads, dropout_rate) # multimodal encoder
knowledge_encoder = transformers.TransformerEncoder(num_knowledge_layers, knowledge_feature_dim, num_heads, dropout_rate) # knowledge encoder
decoder = transformers.TransformerDecoder(num_decoder_layers, answer_feature_dim, num_heads, dropout_rate) # generative decoder
output_layer = torch.nn.Linear(answer_feature_dim, vocab_size) # output layer

# Extract image features using Faster R-CNN
image_features = image_extractor(I)[:num_image_regions] # shape: (num_image_regions, image_feature_dim)

# Encode question using BERT
question_features = question_encoder(Q)[0][:,0,:] # shape: (1, question_feature_dim)

# Fuse image and question features using a multimodal transformer
multimodal_input = torch.cat([image_features, question_features], dim=0) # shape: (num_image_regions + 1, image_feature_dim + question_feature_dim)
multimodal_mask = torch.ones(num_image_regions + 1) # shape: (num_image_regions + 1)
multimodal_features = multimodal_encoder(multimodal_input, multimodal_mask) # shape: (num_image_regions + 1, image_feature_dim + question_feature_dim)

# Retrieve knowledge facts using image regions as queries
knowledge_facts = [] # list of knowledge facts
for i in range(num_image_regions):
    region_feature = image_features[i] # shape: (image_feature_dim)
    region_query = wikidata.query_by_vector(region_feature) # query Wikidata by vector similarity
    region_fact = wikidata.get_fact_by_query(region_query) # get the most relevant fact by query
    knowledge_facts.append(region_fact) # append the fact to the list

# Encode knowledge facts using another transformer
knowledge_input = torch.stack([knowledge_encoder(fact)[0][:,0,:] for fact in knowledge_facts]) # shape: (num_image_regions, knowledge_feature_dim)
knowledge_mask = torch.ones(num_image_regions) # shape: (num_image_regions)
knowledge_features = knowledge_encoder(knowledge_input, knowledge_mask) # shape: (num_image_regions, knowledge_feature_dim)

# Generate answer using a generative transformer that attends to both multimodal and knowledge features
answer_tokens = [] # list of answer tokens
answer_token = torch.tensor([101]) # start token for answer generation
while answer_token != torch.tensor([102]): # end token for answer generation
    decoder_input = torch.cat([answer_token, answer_tokens]) # shape: (len(answer_tokens) + 1)
    decoder_mask = torch.ones(len(answer_tokens) + 1) # shape: (len(answer_tokens) + 1)
    decoder_output = decoder(decoder_input, decoder_mask, multimodal_features, multimodal_mask, knowledge_features, knowledge_mask) # shape: (len(answer_tokens) + 1, answer_feature_dim)
    answer_logits = output_layer(decoder_output[-1]) # shape: (vocab_size)
    answer_token = torch.argmax(answer_logits) # shape: ()
    answer_tokens.append(answer_token) # append the token to the list

# Convert the answer tokens to text
answer_text = transformers.BertTokenizer.from_pretrained('bert-base-uncased').convert_tokens_to_string(answer_tokens)

# Return the answer text as the output
return answer_text

```