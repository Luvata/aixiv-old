---
title: 2201.08361v2 Stitch it in Time  GAN-Based Facial Editing of Real Videos
date: 2022-01-09
---

# [Stitch it in Time: GAN-Based Facial Editing of Real Videos](http://arxiv.org/abs/2201.08361v2)

authors: Rotem Tzaban, Ron Mokady, Rinon Gal, Amit H. Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2201.08361 "Stitch it in Time: GAN-Based Facial Editing of Real Videos"
[2]: https://arxiv.org/pdf/2201.08361.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2104.08361v2 "[2104.08361v2] A Multiple Regression-Enhanced Convolution Estimator for ..."

Here is a summary of the paper:

- **What**: The paper proposes a framework for semantic editing of faces in videos using Generative Adversarial Networks (GANs).
- **Why**: The paper aims to overcome the challenges of temporal coherency and high-quality video editing that current methods face.
- **How**: The paper leverages the natural alignment of StyleGAN and the tendency of neural networks to learn low frequency functions, and demonstrates that they provide a strongly consistent prior. The paper also introduces a novel stitching technique that preserves the original video structure and reduces artifacts.

The paper is titled **Stitch it in Time: GAN-Based Facial Editing of Real Videos**[^1^][1] and was submitted to arXiv on 20 Jan 2022. It was authored by Rotem Tzaban, Ron Mokady, Rinon Gal, Amit H. Bermano, and Daniel Cohen-Or.

## Main Contributions

[1]: https://arxiv.org/abs/2201.08361 "Stitch it in Time: GAN-Based Facial Editing of Real Videos"
[2]: https://arxiv.org/pdf/2201.08361.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2104.08361v2 "[2104.08361v2] A Multiple Regression-Enhanced Convolution Estimator for ..."

According to the paper[^1^][1], the main contributions are:

- **A novel stitching technique** that preserves the original video structure and reduces artifacts by blending the edited frames with the original ones using a learned mask.
- **A temporal consistency loss** that penalizes deviations from the original video in terms of motion and appearance, and encourages smooth transitions between frames.
- **A comprehensive evaluation** of the proposed method on various semantic editing tasks, such as age progression, expression transfer, gender swap, and identity swap, on challenging, high quality, talking head videos. The paper also compares the results with existing methods and shows significant improvements in terms of visual quality and temporal coherency.

## Method Summary

[1]: https://arxiv.org/abs/2201.08361 "Stitch it in Time: GAN-Based Facial Editing of Real Videos"
[2]: https://arxiv.org/pdf/2201.08361.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2104.08361v2 "[2104.08361v2] A Multiple Regression-Enhanced Convolution Estimator for ..."

Here is a summary of the method section of the paper:

- The paper adopts the **StyleGAN2** architecture as the backbone for facial editing, and uses a **pre-trained model** on the FFHQ dataset as the starting point.
- The paper introduces a **stitching module** that takes as input the original video frame and the edited frame, and outputs a blended frame that preserves the original structure and reduces artifacts. The stitching module consists of a **mask generator** that learns to produce a soft mask that indicates which regions to blend, and a **blender** that applies the mask to the input frames using alpha blending.
- The paper also proposes a **temporal consistency loss** that enforces smooth transitions between consecutive frames and penalizes deviations from the original video in terms of motion and appearance. The temporal consistency loss consists of two terms: a **motion loss** that measures the optical flow difference between the original and edited videos, and an **appearance loss** that measures the pixel-wise difference between the original and edited videos in a low frequency band.
- The paper applies the proposed method to various semantic editing tasks, such as age progression, expression transfer, gender swap, and identity swap. The paper also provides an **interactive interface** that allows users to control the editing parameters using sliders or sketches.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a source video V_s and a target video V_t
# Output: an edited video V_e

# Load the pre-trained StyleGAN2 model
model = load_stylegan2()

# Extract the latent codes of the source and target videos
z_s = model.encode(V_s)
z_t = model.encode(V_t)

# Compute the editing direction d
d = z_t - z_s

# Apply the editing direction to the source video with a user-defined strength alpha
z_e = z_s + alpha * d

# Generate the edited frames using StyleGAN2
F_e = model.decode(z_e)

# Initialize the stitching module
stitcher = StitchingModule()

# Initialize the temporal consistency loss
loss = TemporalConsistencyLoss()

# Loop over the frames of the source and edited videos
for i in range(len(V_s)):
  # Get the original and edited frames
  F_s_i = V_s[i]
  F_e_i = F_e[i]

  # Stitch the original and edited frames using the stitching module
  F_b_i = stitcher(F_s_i, F_e_i)

  # Add the stitched frame to the edited video
  V_e[i] = F_b_i

  # Compute the temporal consistency loss between the original and edited videos
  L_i = loss(V_s[:i+1], V_e[:i+1])

  # Update the stitching module parameters using gradient descent
  stitcher.update(L_i)

# Return the edited video
return V_e
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import cv2
import stylegan2

# Define some constants
NUM_FRAMES = 100 # the number of frames in the videos
IMAGE_SIZE = 256 # the size of the images
LATENT_SIZE = 512 # the size of the latent codes
MASK_SIZE = 64 # the size of the mask
LEARNING_RATE = 0.01 # the learning rate for gradient descent

# Input: a source video V_s and a target video V_t
# Output: an edited video V_e

# Load the pre-trained StyleGAN2 model
model = stylegan2.load_model()

# Extract the latent codes of the source and target videos using a projection method
z_s = model.project(V_s)
z_t = model.project(V_t)

# Compute the editing direction d as the average difference between the latent codes
d = torch.mean(z_t - z_s, dim=0)

# Apply the editing direction to the source video with a user-defined strength alpha
z_e = z_s + alpha * d

# Generate the edited frames using StyleGAN2
F_e = model.generate(z_e)

# Initialize the stitching module as a convolutional neural network with one output channel
stitcher = torch.nn.Conv2d(6, 1, 3, padding=1)

# Initialize the temporal consistency loss as a combination of motion and appearance losses
loss = MotionLoss() + AppearanceLoss()

# Initialize an optimizer for the stitching module parameters
optimizer = torch.optim.Adam(stitcher.parameters(), lr=LEARNING_RATE)

# Loop over the frames of the source and edited videos
for i in range(NUM_FRAMES):
  # Get the original and edited frames
  F_s_i = V_s[i]
  F_e_i = F_e[i]

  # Concatenate the original and edited frames along the channel dimension
  F_c_i = torch.cat([F_s_i, F_e_i], dim=0)

  # Pass the concatenated frames through the stitching module to get a mask
  M_i = stitcher(F_c_i)

  # Resize and normalize the mask to match the image size and range [0,1]
  M_i = cv2.resize(M_i, (IMAGE_SIZE, IMAGE_SIZE))
  M_i = torch.sigmoid(M_i)

  # Blend the original and edited frames using alpha blending with the mask
  F_b_i = M_i * F_e_i + (1 - M_i) * F_s_i

  # Add the blended frame to the edited video
  V_e[i] = F_b_i

  # Compute the temporal consistency loss between the original and edited videos
  L_i = loss(V_s[:i+1], V_e[:i+1])

  # Zero out the gradients of the stitching module parameters
  optimizer.zero_grad()

  # Backpropagate the loss through the stitching module parameters
  L_i.backward()

  # Update the stitching module parameters using gradient descent
  optimizer.step()

# Return the edited video
return V_e
```