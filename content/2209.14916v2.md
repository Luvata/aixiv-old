---
title: 2209.14916v2 Human Motion Diffusion Model
date: 2022-09-15
---

# [Human Motion Diffusion Model](http://arxiv.org/abs/2209.14916v2)

authors: Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, Amit H. Bermano


## What, Why and How

[1]: https://arxiv.org/abs/2209.14916v2 "[2209.14916v2] Human Motion Diffusion Model - arXiv.org"
[2]: https://arxiv-export1.library.cornell.edu/abs/2209.14916 "[2209.14916] Human Motion Diffusion Model"
[3]: https://papersread.ai/e/human-motion-diffusion-model/ "Human Motion Diffusion Model | Papers Read on AI"
[4]: https://arxiv.org/pdf/2209.14916v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper introduces **Motion Diffusion Model (MDM)**, a diffusion-based generative model for human motion synthesis[^1^][1] [^2^][2].
- **Why**: The paper aims to address the challenges of generating natural and expressive human motion, which is a difficult task due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it[^1^][1] [^2^][2]. The paper claims that current generative solutions are either low-quality or limited in expressiveness[^1^][1] [^2^][2].
- **How**: The paper adapts diffusion models, which have shown remarkable generative capabilities in other domains, to the human motion domain[^1^][1] [^2^][2]. The paper uses a transformer-based architecture, combining insights from motion generation literature[^1^][1] [^2^][2]. A notable design choice is the prediction of the sample, rather than the noise, in each diffusion step[^1^][1] [^2^][2]. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss[^1^][1] [^2^][2]. The paper demonstrates that MDM is a generic approach, enabling different modes of conditioning, and different generation tasks[^1^][1] [^2^][2]. The paper shows that MDM achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion[^1^][1] [^2^][2].

## Main Contributions

According to the paper, the main contributions are:

- **MDM**, a classifier-free diffusion-based generative model for human motion synthesis that is trained with lightweight resources .
- A novel design choice of predicting the sample, rather than the noise, in each diffusion step, which enables the use of geometric losses and improves the quality of the generated motion .
- A generic framework that supports different modes of conditioning and generation tasks, such as text-to-motion and action-to-motion .
- State-of-the-art results on two challenging benchmarks for human motion synthesis: HumanAct12 and AMASS .

## Method Summary

[1]: https://arxiv.org/abs/2209.14916 "[2209.14916] Human Motion Diffusion Model - arXiv.org"
[2]: https://arxiv-export1.library.cornell.edu/abs/2209.14916 "[2209.14916] Human Motion Diffusion Model"
[3]: https://arxiv.org/pdf/2209.14916v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper presents the **Motion Diffusion Model (MDM)**, which is a diffusion-based generative model for human motion synthesis[^1^][1] [^2^][2].
- The paper defines the **diffusion process** as a Markov chain that transforms a data sample into Gaussian noise through a series of intermediate states[^1^][1] [^2^][2].
- The paper defines the **reverse diffusion process** as a Markov chain that reconstructs a data sample from Gaussian noise through a series of intermediate states[^1^][1] [^2^][2].
- The paper uses a **transformer-based architecture** to model the reverse diffusion process, which consists of an encoder, a decoder, and an attention mechanism[^1^][1] [^2^][2].
- The paper introduces a novel design choice of predicting the **sample**, rather than the noise, in each reverse diffusion step[^1^][1] [^2^][2]. This allows the model to use geometric losses on the locations and velocities of the motion, such as the foot contact loss[^1^][1] [^2^][2].
- The paper describes how to train the model using a combination of **reconstruction loss**, **geometric loss**, and **KL divergence loss**[^1^][1] [^2^][2].
- The paper explains how to generate motion samples from the model using different modes of **conditioning**, such as text or action labels[^1^][1] [^2^][2].
- The paper provides details on the **implementation** of the model, such as the network architecture, the hyperparameters, and the training procedure[^1^][1] [^2^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion process
def diffusion(x):
  # x is a data sample of human motion
  # T is the number of diffusion steps
  # beta_t is the noise level at step t
  # epsilon_t is the Gaussian noise at step t
  for t in range(1, T+1):
    # Apply the forward diffusion formula
    x_t = sqrt(1 - beta_t) * x + sqrt(beta_t) * epsilon_t
  return x_T # Return the final noisy state

# Define the reverse diffusion process
def reverse_diffusion(x_T):
  # x_T is a noisy state of human motion
  # T is the number of diffusion steps
  # beta_t is the noise level at step t
  # z_t is the predicted sample at step t
  # model is a transformer-based network that predicts z_t from x_t and c
  # c is an optional conditioning input, such as text or action label
  for t in range(T, 0, -1):
    # Apply the reverse diffusion formula
    z_t = (x_t - sqrt(beta_t) * model(x_t, c)) / sqrt(1 - beta_t)
    # Update x_t with z_t
    x_t = z_t
  return z_0 # Return the reconstructed data sample

# Define the training procedure
def train(model, data, c):
  # model is a transformer-based network that predicts z_t from x_t and c
  # data is a dataset of human motion samples
  # c is an optional conditioning input, such as text or action label
  for each batch of (x, c) in data:
    # Apply the diffusion process to x
    x_T = diffusion(x)
    # Apply the reverse diffusion process to x_T
    z_0 = reverse_diffusion(x_T)
    # Compute the reconstruction loss between z_0 and x
    L_rec = mean_squared_error(z_0, x)
    # Compute the geometric loss between z_0 and x
    L_geo = foot_contact_loss(z_0, x) + velocity_loss(z_0, x)
    # Compute the KL divergence loss between model and Gaussian noise distribution
    L_kl = kl_divergence(model(x_T, c), epsilon_T)
    # Compute the total loss as a weighted sum of L_rec, L_geo, and L_kl
    L_total = alpha * L_rec + beta * L_geo + gamma * L_kl
    # Update the model parameters using gradient descent on L_total

# Define the generation procedure
def generate(model, c):
  # model is a transformer-based network that predicts z_t from x_t and c
  # c is an optional conditioning input, such as text or action label
  # Sample a Gaussian noise vector epsilon_T
  epsilon_T = normal(0, 1)
  # Apply the reverse diffusion process to epsilon_T
  z_0 = reverse_diffusion(epsilon_T)
  return z_0 # Return the generated data sample

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2Model

# Define the hyperparameters
T = 1000 # Number of diffusion steps
beta_0 = 1e-4 # Initial noise level
beta_T = 2e-2 # Final noise level
alpha = 1.0 # Weight for reconstruction loss
beta = 0.1 # Weight for geometric loss
gamma = 0.01 # Weight for KL divergence loss
batch_size = 32 # Batch size for training and generation
lr = 1e-4 # Learning rate for optimizer
epochs = 100 # Number of epochs for training

# Define the beta schedule as a geometric progression from beta_0 to beta_T
beta_schedule = np.geomspace(beta_0, beta_T, T)

# Define the diffusion process
def diffusion(x):
  # x is a data sample of human motion of shape (batch_size, seq_len, num_joints * 3)
  # T is the number of diffusion steps
  # beta_t is the noise level at step t of shape (T,)
  # epsilon_t is the Gaussian noise at step t of shape (batch_size, seq_len, num_joints * 3)
  for t in range(1, T+1):
    # Sample epsilon_t from standard normal distribution
    epsilon_t = torch.randn_like(x)
    # Apply the forward diffusion formula
    x_t = torch.sqrt(1 - beta_t[t]) * x + torch.sqrt(beta_t[t]) * epsilon_t
  return x_T # Return the final noisy state of shape (batch_size, seq_len, num_joints * 3)

# Define the reverse diffusion process
def reverse_diffusion(x_T, c):
  # x_T is a noisy state of human motion of shape (batch_size, seq_len, num_joints * 3)
  # T is the number of diffusion steps
  # beta_t is the noise level at step t of shape (T,)
  # z_t is the predicted sample at step t of shape (batch_size, seq_len, num_joints * 3)
  # model is a transformer-based network that predicts z_t from x_t and c
  # c is an optional conditioning input, such as text or action label of shape (batch_size,) or (batch_size, seq_len)
  for t in range(T, 0, -1):
    # Apply the reverse diffusion formula
    z_t = (x_t - torch.sqrt(beta_t[t]) * model(x_t, c)) / torch.sqrt(1 - beta_t[t])
    # Update x_t with z_t
    x_t = z_t
  return z_0 # Return the reconstructed data sample of shape (batch_size, seq_len, num_joints * 3)

# Define the transformer-based network for predicting z_t from x_t and c
class MotionDiffusionModel(nn.Module):
  
  def __init__(self):
    super(MotionDiffusionModel, self).__init__()
    # Define the encoder for encoding c into a latent vector h_c of shape (batch_size, hidden_size)
    self.encoder = GPT2Model.from_pretrained('gpt2')
    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    self.hidden_size = self.encoder.config.hidden_size
    
    # Define the decoder for decoding h_c and x_t into z_t of shape (batch_size, seq_len, num_joints * 3)
    self.decoder = nn.TransformerDecoderLayer(d_model=self.hidden_size,
                                              nhead=8,
                                              dim_feedforward=2048,
                                              dropout=0.1,
                                              activation='gelu')
    
    # Define the output layer for projecting z_t into num_joints * 3 dimensions
    self.output_layer = nn.Linear(self.hidden_size, num_joints * 3)
  
  def forward(self, x_t, c):
    # x_t is a noisy state of human motion of shape (batch_size, seq_len, num_joints * 3)
    # c is an optional conditioning input, such as text or action label of shape (batch_size,) or (batch_size, seq_len)
    
    # If c is a text input of shape (batch_size,)
    if c.ndim == 1:
      # Tokenize c using the GPT2 tokenizer
      c_tokens = self.tokenizer(c, padding=True, truncation=True, return_tensors='pt')
      # Encode c using the GPT2 encoder
      h_c = self.encoder(**c_tokens).last_hidden_state # Shape: (batch_size, max_len, hidden_size)
      # Reduce the dimension of h_c by taking the mean over the max_len dimension
      h_c = torch.mean(h_c, dim=1) # Shape: (batch_size, hidden_size)
      # Expand the dimension of h_c to match the seq_len dimension of x_t
      h_c = h_c.unsqueeze(1).repeat(1, x_t.shape[1], 1) # Shape: (batch_size, seq_len, hidden_size)
    
    # If c is an action label input of shape (batch_size, seq_len)
    elif c.ndim == 2:
      # Embed c using a learned embedding matrix
      embedding = nn.Embedding(num_actions, self.hidden_size) # num_actions is the number of action labels
      h_c = embedding(c) # Shape: (batch_size, seq_len, hidden_size)
    
    # Concatenate x_t and h_c along the hidden_size dimension
    x_h = torch.cat([x_t, h_c], dim=-1) # Shape: (batch_size, seq_len, num_joints * 3 + hidden_size)
    
    # Apply a linear layer to reduce the dimension of x_h to hidden_size
    x_h = nn.Linear(num_joints * 3 + hidden_size, hidden_size)(x_h) # Shape: (batch_size, seq_len, hidden_size)
    
    # Apply the decoder to x_h
    z_t = self.decoder(x_h) # Shape: (batch_size, seq_len, hidden_size)
    
    # Apply the output layer to z_t
    z_t = self.output_layer(z_t) # Shape: (batch_size, seq_len, num_joints * 3)
    
    return z_t

# Define the reconstruction loss between z_0 and x
def reconstruction_loss(z_0, x):
  # z_0 is the reconstructed data sample of shape (batch_size, seq_len, num_joints * 3)
  # x is the original data sample of shape (batch_size, seq_len, num_joints * 3)
  # Compute the mean squared error between z_0 and x
  L_rec = F.mse_loss(z_0, x)
  return L_rec

# Define the geometric loss between z_0 and x
def geometric_loss(z_0, x):
  # z_0 is the reconstructed data sample of shape (batch_size, seq_len, num_joints * 3)
  # x is the original data sample of shape (batch_size, seq_len, num_joints * 3)
  
  # Define the foot contact loss as a binary cross entropy loss between the foot contact labels of z_0 and x
  def foot_contact_loss(z_0, x):
    # Extract the foot contact labels from z_0 and x
    z_fc = z_0[:, :, -2:] # Shape: (batch_size, seq_len, 2)
    x_fc = x[:, :, -2:] # Shape: (batch_size, seq_len, 2)
    # Compute the binary cross entropy loss between z_fc and x_fc
    L_fc = F.binary_cross_entropy_with_logits(z_fc, x_fc)
    return L_fc
  
  # Define the velocity loss as a mean squared error loss between the velocities of z_0 and x
  def velocity_loss(z_0, x):
    # Compute the velocities of z_0 and x by taking the difference between consecutive frames
    z_v = z_0[:, 1:, :-2] - z_0[:, :-1:, :-2] # Shape: (batch_size, seq_len - 1, num_joints * 3 - 2)
    x_v = x[:, 1:, :-2] - x[:, :-1:, :-2] # Shape: (batch_size, seq_len - 1, num_joints * 3 - 2)
    # Compute the mean squared error loss between z_v and x_v
    L_v = F.mse_loss(z_v, x_v)
    return L_v
  
  # Compute the total geometric loss as a weighted sum of foot contact loss and velocity loss
  L_geo = foot_contact_loss(z_0, x) + velocity_loss(z_0, x)
  
  return L_geo

# Define the KL divergence loss between model and Gaussian noise distribution
def kl_divergence_loss(model, epsilon_T):
  # model is a transformer-based network that predicts z_t from x_t and c
  # epsilon_T is the Gaussian noise at step T of shape (batch_size, seq_len, num_joints * 3)
  
  # Compute the log probability of epsilon_T under model