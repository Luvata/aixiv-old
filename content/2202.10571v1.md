---
title: 2202.10571v1 Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks
date: 2022-02-11
---

# [Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks](http://arxiv.org/abs/2202.10571v1)

authors: Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, Jinwoo Shin


## What, Why and How

[1]: https://arxiv.org/pdf/2202.10571v1 "arXiv:2202.10571v1 [cs.CV] 21 Feb 2022"
[2]: https://arxiv.org/abs/2202.10571 "[2202.10571] Generating Videos with Dynamics-aware Implicit Generative ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2202.10571?context=cs "[2202.10571] Generating Videos with Dynamics-aware Implicit Generative ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel generative adversarial network for video generation, called dynamics-aware implicit generative adversarial network (DIGAN).
- **Why**: The paper aims to overcome the limitations of existing video generation methods that model videos as 3D grids of RGB values, which restricts the scale and quality of generated videos and ignores the continuous dynamics of video signals.
- **How**: The paper leverages implicit neural representations (INRs) of video, which encode a continuous signal into a parameterized neural network. The paper introduces an INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently, and a motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences. The paper demonstrates the superiority of DIGAN on various datasets and tasks, such as long video synthesis, video extrapolation, and non-autoregressive video generation.

## Main Contributions

According to the paper, the main contributions are:

- The paper proposes DIGAN, a novel generative adversarial network for video generation that leverages INRs of video to model the continuous spatio-temporal dynamics of video signals.
- The paper introduces a novel INR-based video generator that improves the motion dynamics by manipulating the space and time coordinates differently, and a novel motion discriminator that efficiently identifies the unnatural motions without observing the entire long frame sequences.
- The paper shows that DIGAN can generate high-quality videos of long duration (up to 128 frames) and high resolution (up to 128x128 pixels), outperforming previous state-of-the-art methods on various datasets and metrics.
- The paper demonstrates that DIGAN has multiple intriguing properties, such as video extrapolation, non-autoregressive video generation, and controllable video synthesis.

## Method Summary

[1]: https://arxiv.org/abs/2202.10571 "[2202.10571] Generating Videos with Dynamics-aware Implicit Generative ..."
[2]: https://arxiv.org/pdf/2202.10571.pdf "arXiv:2202.10571v1 [cs.CV] 21 Feb 2022"
[3]: https://arxiv-export-lb.library.cornell.edu/pdf/2204.10571 "arXiv:2204.10571v1 [quant-ph] 22 Apr 2022"

Here is a summary of the method section of the paper:

- The paper presents the details of the proposed DIGAN framework, which consists of an INR-based video generator and a motion discriminator.
- The paper explains how the video generator decomposes the motion and content features of a video, and incorporates the temporal dynamics into the motion features by regulating their variations with a smaller temporal frequency. The paper also describes how the video generator uses a positional encoding scheme that allows different manipulations of the space and time coordinates to improve the motion dynamics.
- The paper introduces the motion discriminator, which aims to distinguish between real and fake motions without observing the entire long frame sequences. The paper shows how the motion discriminator uses a novel motion encoding scheme that extracts motion features from pairs of frames with different temporal distances, and how it uses a contrastive loss function that encourages the motion discriminator to learn from hard negative samples.
- The paper discusses the training procedure and the loss functions of DIGAN, which include an adversarial loss, a reconstruction loss, a perceptual loss, and a contrastive loss. The paper also provides the implementation details and the hyperparameters of DIGAN.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the INR-based video generator G
G = INRVideoGenerator()

# Define the motion discriminator D
D = MotionDiscriminator()

# Define the loss functions
L_adv = AdversarialLoss()
L_rec = ReconstructionLoss()
L_per = PerceptualLoss()
L_con = ContrastiveLoss()

# Define the optimizers
opt_G = Optimizer(G.parameters())
opt_D = Optimizer(D.parameters())

# Loop over the training data
for video in data:

  # Sample a latent code z
  z = sample_latent_code()

  # Generate a fake video from z
  fake_video = G(z)

  # Compute the motion features of the real and fake videos
  real_motion = D(video)
  fake_motion = D(fake_video)

  # Compute the adversarial loss for G and D
  loss_G_adv = L_adv(fake_motion, real=True)
  loss_D_adv = L_adv(real_motion, real=True) + L_adv(fake_motion, real=False)

  # Compute the reconstruction loss for G
  loss_G_rec = L_rec(fake_video, video)

  # Compute the perceptual loss for G
  loss_G_per = L_per(fake_video, video)

  # Compute the contrastive loss for D
  loss_D_con = L_con(real_motion, fake_motion)

  # Compute the total losses for G and D
  loss_G = loss_G_adv + loss_G_rec + loss_G_per
  loss_D = loss_D_adv + loss_D_con

  # Update the parameters of G and D
  opt_G.zero_grad()
  loss_G.backward()
  opt_G.step()

  opt_D.zero_grad()
  loss_D.backward()
  opt_D.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

# Define the hyperparameters
latent_dim = 256 # the dimension of the latent code z
hidden_dim = 256 # the dimension of the hidden layer of the INR
frame_size = 128 # the size of each frame (assumed to be square)
frame_num = 128 # the number of frames in each video
channel_num = 3 # the number of channels in each frame (RGB)
temporal_freq = 0.1 # the temporal frequency for regulating the motion features
lambda_rec = 10 # the weight for the reconstruction loss
lambda_per = 0.01 # the weight for the perceptual loss
lambda_con = 0.1 # the weight for the contrastive loss
temporal_dist = [1, 2, 4, 8] # the list of temporal distances for motion encoding
batch_size = 32 # the batch size for training
learning_rate = 0.0002 # the learning rate for training
beta1 = 0.5 # the beta1 parameter for Adam optimizer
beta2 = 0.999 # the beta2 parameter for Adam optimizer

# Define the INR-based video generator G
class INRVideoGenerator(nn.Module):

  def __init__(self):
    super().__init__()

    # Define the INR network that maps a latent code z and a spatio-temporal coordinate (x, y, t) to an RGB value
    self.inr = nn.Sequential(
      nn.Linear(latent_dim + 3, hidden_dim),
      nn.ReLU(),
      nn.Linear(hidden_dim, hidden_dim),
      nn.ReLU(),
      nn.Linear(hidden_dim, hidden_dim),
      nn.ReLU(),
      nn.Linear(hidden_dim, channel_num),
      nn.Sigmoid()
    )

    # Define the positional encoding function that maps a spatio-temporal coordinate (x, y, t) to a higher-dimensional vector
    self.pos_enc = PositionalEncoding()

    # Define the motion encoding function that maps a latent code z to a motion feature vector m
    self.mot_enc = MotionEncoding()

    # Define the content encoding function that maps a latent code z to a content feature vector c
    self.con_enc = ContentEncoding()

  def forward(self, z):
    # Encode z into motion and content features
    m = self.mot_enc(z)
    c = self.con_enc(z)

    # Generate a fake video from z by sampling spatio-temporal coordinates and applying INR
    fake_video = []
    for t in range(frame_num):
      fake_frame = []
      for x in range(frame_size):
        for y in range(frame_size):
          # Sample a spatio-temporal coordinate (x, y, t)
          coord = torch.tensor([x, y, t], dtype=torch.float)

          # Apply positional encoding to get a higher-dimensional vector p
          p = self.pos_enc(coord)

          # Concatenate p with m and c to get an input vector for INR
          inr_input = torch.cat([p, m, c], dim=-1)

          # Apply INR to get an RGB value v
          v = self.inr(inr_input)

          # Append v to the fake frame
          fake_frame.append(v)

      # Reshape and normalize the fake frame to have shape (channel_num, frame_size, frame_size) and range [0, 1]
      fake_frame = torch.stack(fake_frame).view(channel_num, frame_size, frame_size)
      fake_frame = (fake_frame - fake_frame.min()) / (fake_frame.max() - fake_frame.min())

      # Append the fake frame to the fake video
      fake_video.append(fake_frame)

    # Stack and return the fake video with shape (frame_num, channel_num, frame_size, frame_size)
    return torch.stack(fake_video)


# Define the positional encoding function that maps a spatio-temporal coordinate (x, y, t) to a higher-dimensional vector p
class PositionalEncoding(nn.Module):

  def __init__(self):
    super().__init__()

    # Define the frequency bands for sinusoidal functions
    self.freq_bands = torch.exp(torch.linspace(0, -5, latent_dim // 2))

  def forward(self, coord):
    # Normalize and scale the coordinate to have range [-pi, pi]
    coord = coord / frame_size * 2 * math.pi

    # Apply sinusoidal functions with different frequencies to get a higher-dimensional vector p
    p = torch.cat([torch.sin(coord * f), torch.cos(coord * f)], dim=-1) for f in self.freq_bands

    # Return p with shape (latent_dim,)
    return p


# Define the motion encoding function that maps a latent code z to a motion feature vector m
class MotionEncoding(nn.Module):

  def __init__(self):
    super().__init__()

    # Define a linear layer that maps z to m
    self.linear = nn.Linear(latent_dim, latent_dim)

  def forward(self, z):
    # Apply the linear layer to get m
    m = self.linear(z)

    # Regulate the variation of m with a smaller temporal frequency
    m = m * temporal_freq

    # Return m with shape (latent_dim,)
    return m


# Define the content encoding function that maps a latent code z to a content feature vector c
class ContentEncoding(nn.Module):

  def __init__(self):
    super().__init__()

    # Define a linear layer that maps z to c
    self.linear = nn.Linear(latent_dim, latent_dim)

  def forward(self, z):
    # Apply the linear layer to get c
    c = self.linear(z)

    # Return c with shape (latent_dim,)
    return c


# Define the motion discriminator D
class MotionDiscriminator(nn.Module):

  def __init__(self):
    super().__init__()

    # Define the motion encoding network that maps a pair of frames to a motion feature vector
    self.motion_enc = MotionEncodingNetwork()

    # Define the classification network that maps a motion feature vector to a scalar score
    self.classifier = ClassificationNetwork()

  def forward(self, video):
    # Compute the motion features of the video by applying motion encoding network to pairs of frames with different temporal distances
    motion_features = []
    for d in temporal_dist:
      for i in range(frame_num - d):
        # Get a pair of frames with temporal distance d
        frame1 = video[i]
        frame2 = video[i + d]

        # Concatenate the frames along the channel dimension
        frame_pair = torch.cat([frame1, frame2], dim=0)

        # Apply motion encoding network to get a motion feature vector
        motion_feature = self.motion_enc(frame_pair)

        # Append the motion feature to the list
        motion_features.append(motion_feature)

    # Stack and return the motion features with shape (len(temporal_dist) * (frame_num - d), hidden_dim)
    return torch.stack(motion_features)


# Define the motion encoding network that maps a pair of frames to a motion feature vector
class MotionEncodingNetwork(nn.Module):

  def __init__(self):
    super().__init__()

    # Define a convolutional network that extracts features from a pair of frames
    self.conv_net = nn.Sequential(
      nn.Conv2d(channel_num * 2, 64, 4, 2, 1),
      nn.LeakyReLU(0.2),
      nn.Conv2d(64, 128, 4, 2, 1),
      nn.BatchNorm2d(128),
      nn.LeakyReLU(0.2),
      nn.Conv2d(128, 256, 4, 2, 1),
      nn.BatchNorm2d(256),
      nn.LeakyReLU(0.2),
      nn.Conv2d(256, 512, 4, 2, 1),
      nn.BatchNorm2d(512),
      nn.LeakyReLU(0.2)
    )

    # Define a linear layer that maps the features to a motion feature vector
    self.linear = nn.Linear(512 * (frame_size // 16) * (frame_size // 16), hidden_dim)

  def forward(self, frame_pair):
    # Apply the convolutional network to get features
    features = self.conv_net(frame_pair)

    # Flatten and apply the linear layer to get a motion feature vector
    motion_feature = self.linear(features.view(features.size(0), -1))

    # Return the motion feature vector with shape (hidden_dim,)
    return motion_feature


# Define the classification network that maps a motion feature vector to a scalar score
class ClassificationNetwork(nn.Module):

  def __init__(self):
    super().__init__()

    # Define a linear layer that maps a motion feature vector to a scalar score
    self.linear = nn.Linear(hidden_dim, 1)

  def forward(self, motion_feature):
    # Apply the linear layer to get a scalar score
    score = self.linear(motion_feature)

    # Return the score with shape (1,)
    return score


# Define the adversarial loss function
def AdversarialLoss(score, real):
  # Use binary cross entropy loss with logits
  if real:
    target = torch.ones(score.size())
  else:
    target = torch.zeros(score.size())
  
  loss = F.binary_cross_entropy_with_logits(score, target