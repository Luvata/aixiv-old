---
title: 1803.10039v1 Learning Depth from Single Images with Deep Neural Network Embedding Focal Length
date: 2018-03-11
---

# [Learning Depth from Single Images with Deep Neural Network Embedding Focal Length](http://arxiv.org/abs/1803.10039v1)

authors: Lei He, Guanghui Wang, Zhanyi Hu


## What, Why and How

[1]: https://arxiv.org/abs/1803.10039 "[1803.10039] Learning Depth from Single Images with Deep ... - arXiv.org"
[2]: https://arxiv.org/pdf/1803.10039 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1712.10039v1 "[1712.10039v1] Local Casimir Effect for a Scalar Field in Presence of a ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel deep neural network to learn depth from single images by embedding focal length information.
- **Why**: The paper aims to solve the inherent ambiguity between the focal length and monocular depth learning, which affects the accuracy of depth recovery from a single image.
- **How**: The paper first proves the ambiguity theoretically and experimentally, then generates a synthetic varying-focal-length dataset from fixed-focal-length datasets using a hole-filling method. The paper then introduces a deep neural network that fuses middle-level information to infer depth from both fixed- and varying-focal-length datasets. The paper evaluates the proposed method on several benchmarks and shows that it outperforms the state-of-the-art methods.

## Main Contributions

[1]: https://arxiv.org/pdf/1803.10039v1.pdf "Learning Depth from Single Images with Deep Neural Network ... - arXiv.org"
[2]: https://arxiv.org/abs/1803.10039 "[1803.10039] Learning Depth from Single Images with Deep ... - arXiv.org"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

According to the paper[^1^][1], the main contributions are:

- **Theoretical and experimental proof of the ambiguity between the focal length and monocular depth learning**, showing that the focal length has a great influence on accurate depth recovery.
- **A method to generate synthetic varying-focal-length dataset from fixed-focal-length datasets**, and a simple and effective method to fill the holes in the newly generated images.
- **A novel deep neural network to infer depth through effectively fusing the middle-level information** on the fixed-focal-length dataset, which outperforms the state-of-the-art methods built on pre-trained VGG.
- **Extensive experiments on the fixed- and varying-focal-length datasets** demonstrating that the learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information.

## Method Summary

[1]: https://arxiv.org/abs/1803.10039v1 "[1803.10039v1] Learning Depth from Single Images with Deep Neural ..."
[2]: https://arxiv.org/pdf/1803.10039v1.pdf "Learning Depth from Single Images with Deep Neural Network ... - arXiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first **proves the ambiguity between the focal length and monocular depth learning** theoretically by deriving a formula that shows the relationship between the focal length, the depth map, and the ground truth depth. The paper then **verifies the result using experiments** on two fixed-focal-length datasets (NYU Depth V2 and Make3D), showing that different focal lengths lead to different depth maps and errors.
- The paper then **proposes a method to generate synthetic varying-focal-length dataset from fixed-focal-length datasets** by applying perspective transformations to the original images and depth maps. The paper also **implements a simple and effective method to fill the holes** in the newly generated images using inpainting techniques based on patch matching and Poisson blending.
- The paper then **introduces a novel deep neural network to infer depth from single images** by effectively fusing the middle-level information from different convolutional layers. The paper adopts a fully convolutional network architecture with skip connections and residual blocks, and uses a multi-scale loss function to optimize the network. The paper also **embeds the focal length information into the network** by adding a focal length layer that takes the focal length as input and adjusts the output depth map accordingly.
- The paper then **evaluates the proposed method on several benchmarks** using both fixed- and varying-focal-length datasets. The paper compares the proposed method with several state-of-the-art methods built on pre-trained VGG networks, and shows that the proposed method achieves better performance in terms of accuracy and visual quality. The paper also **demonstrates that the learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information**.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a single image I and its focal length f
# Output: a depth map D

# Step 1: Prove and verify the ambiguity between the focal length and monocular depth learning
# Formula: D = f * Z / d, where Z is the ground truth depth and d is the depth map
# Experiment: Use two fixed-focal-length datasets (NYU Depth V2 and Make3D) and vary the focal length to show the effect on depth maps and errors

# Step 2: Generate synthetic varying-focal-length dataset from fixed-focal-length datasets
# Method: Apply perspective transformations to the original images and depth maps using different focal lengths
# Hole-filling: Use inpainting techniques based on patch matching and Poisson blending to fill the holes in the transformed images

# Step 3: Design a novel deep neural network to infer depth from single images
# Architecture: A fully convolutional network with skip connections and residual blocks
# Loss function: A multi-scale loss function that measures the difference between the predicted and ground truth depth maps at different scales
# Focal length layer: A layer that takes the focal length as input and adjusts the output depth map accordingly

# Step 4: Evaluate the proposed method on several benchmarks
# Datasets: Both fixed- and varying-focal-length datasets, such as NYU Depth V2, Make3D, KITTI, and Eigen split
# Metrics: Mean absolute error (MAE), root mean squared error (RMSE), relative error (REL), log10 error (LOG10), accuracy under threshold (Î´ < 1.25)
# Baselines: Several state-of-the-art methods built on pre-trained VGG networks, such as Eigen et al. [10], Liu et al. [11], Laina et al. [12], and Kuznietsov et al. [13]
# Results: The proposed method achieves better performance than the baselines in terms of accuracy and visual quality. The learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information.
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.optim as optim

# Define constants
NUM_SCALES = 4 # number of scales for multi-scale loss
ALPHA = 0.8 # weight for L1 loss
BETA = 0.2 # weight for gradient loss
GAMMA = 0.01 # weight for focal length layer loss
LEARNING_RATE = 0.0001 # learning rate for optimizer
NUM_EPOCHS = 50 # number of epochs for training
BATCH_SIZE = 16 # batch size for training

# Define functions

# Function to apply perspective transformation to an image and a depth map using a given focal length
def transform(image, depth, focal_length):
  # Get the original height, width, and focal length of the image and depth map
  height, width = image.shape[:2]
  original_focal_length = width / 2

  # Compute the transformation matrix based on the given focal length
  scale = focal_length / original_focal_length
  matrix = np.array([[scale, 0, (scale - 1) * width / 2],
                     [0, scale, (scale - 1) * height / 2],
                     [0, 0, 1]])

  # Apply the transformation to the image and depth map using bilinear interpolation
  transformed_image = cv2.warpPerspective(image, matrix, (width, height), flags=cv2.INTER_LINEAR)
  transformed_depth = cv2.warpPerspective(depth, matrix, (width, height), flags=cv2.INTER_LINEAR)

  # Return the transformed image and depth map
  return transformed_image, transformed_depth

# Function to fill the holes in a transformed image using inpainting techniques based on patch matching and Poisson blending
def fill_holes(image):
  # Convert the image to grayscale and threshold it to get a binary mask of the holes
  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  _, mask = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)

  # Find the contours of the holes and draw them on a new mask
  contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
  hole_mask = np.zeros_like(mask)
  cv2.drawContours(hole_mask, contours, -1, 255, -1)

  # Inpaint the holes using patch matching and Poisson blending
  inpainted_image = cv2.inpaint(image, hole_mask, 3, cv2.INPAINT_TELEA)

  # Return the inpainted image
  return inpainted_image

# Function to define the network architecture for depth inference
def define_network():
  # Define the encoder part of the network using pre-trained VGG-16 layers
  encoder = nn.Sequential(
    nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2,stride=2),
    nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2,stride=2),
    nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),
    nn.ReLU(inplace=True),
    nn.MaxPool2d(kernel_size=2,stride=2),
    nn.Conv2d(256,512,kernel_size