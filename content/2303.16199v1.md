---
title: 2303.16199v1 LLaMA-Adapter  Efficient Fine-tuning of Language Models with Zero-init Attention
date: 2023-03-17
---

# [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](http://arxiv.org/abs/2303.16199v1)

authors: Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao


## What, Why and How

[1]: https://arxiv.org/abs/2303.16199 "[2303.16199] LLaMA-Adapter: Efficient Fine-tuning of Language Models ..."
[2]: https://arxiv.org/pdf/2303.16199v1.pdf "Shanghai Artiﬁcial Intelligence Laboratory CUHK MMLab arXiv:2303 ..."
[3]: http://export.arxiv.org/abs/2303.16199 "[2303.16199] LLaMA-Adapter: Efficient Fine-tuning of Language Models ..."

Here is a summary of the paper:

- **What**: The paper presents LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model that can handle text and image inputs.
- **Why**: The paper aims to overcome the limitations of existing instruction-following models, such as high development costs, closed-source restriction, multi-modality unsupported and cumbersome to transfer to different downstream scenarios.
- **How**: The paper adopts a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. The paper evaluates LLaMA-Adapter on two benchmarks: ChatGPT and ScienceQA, and shows that it achieves comparable or superior performance to Alpaca and GPT-3.5 with much fewer parameters and training time.

## Main Contributions

[1]: https://arxiv.org/abs/2303.16199 "[2303.16199] LLaMA-Adapter: Efficient Fine-tuning of Language Models ..."
[2]: https://arxiv.org/pdf/2303.16199v1.pdf "Shanghai Artiﬁcial Intelligence Laboratory CUHK MMLab arXiv:2303 ..."
[3]: http://export.arxiv.org/abs/2303.16199 "[2303.16199] LLaMA-Adapter: Efficient Fine-tuning of Language Models ..."

The paper claims the following contributions:

- **A lightweight adaption method** that only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs[^1^][1] [^2^][2].
- **A set of learnable adaption prompts** that are prepended to the input text tokens at higher transformer layers, and can be customized for different downstream expertise[^1^][1] [^2^][2].
- **A zero-init attention mechanism with zero gating** that adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge[^1^][1] [^2^][2].
- **A multi-modal extension** that can handle image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA[^1^][1] [^2^][2].
- **A code release** that makes the method affordable and replicable[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/abs/2303.16199 "[2303.16199] LLaMA-Adapter: Efficient Fine-tuning of Language Models ..."
[2]: https://arxiv.org/pdf/2303.16199v1.pdf "Shanghai Artiﬁcial Intelligence Laboratory CUHK MMLab arXiv:2303 ..."
[3]: http://export.arxiv.org/abs/2303.16199 "[2303.16199] LLaMA-Adapter: Efficient Fine-tuning of Language Models ..."

Here is a summary of the method section of the paper:

- The paper first introduces the **LLaMA model** and its architecture, which consists of 48 transformer layers with 7B parameters[^1^][2].
- The paper then describes the **LLaMA-Adapter method** and its components, which are:
    - **Adaption prompts**: A set of learnable text tokens that are prepended to the input text tokens at higher transformer layers (layer 24 to 47). The adaption prompts are initialized randomly and optimized during fine-tuning. They act as instructional cues that guide LLaMA to generate desired outputs[^1^][2].
    - **Zero-init attention**: A novel attention mechanism that is applied to the adaption prompts and the input text tokens. The zero-init attention has zero initialization and zero gating, which means that it does not interfere with the pre-trained attention weights and does not require additional parameters. The zero-init attention adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge[^1^][2].
- The paper then explains the **training procedure** of LLaMA-Adapter, which involves:
    - **Data preparation**: The paper uses the same 52K instruction-output pairs as Alpaca, which are generated by self-instructing LLaMA using GPT-3.5. The paper also augments the data with image inputs for image-conditioned LLaMA[^1^][2].
    - **Fine-tuning**: The paper fine-tunes LLaMA-Adapter on 8 A100 GPUs for 10 epochs with a batch size of 64 and a learning rate of 1e-4. The paper freezes the parameters of LLaMA except for the adaption prompts and the zero-init attention. The paper uses AdamW optimizer with weight decay of 0.01 and gradient clipping of 1.0[^1^][2].
- The paper finally presents the **evaluation setup** of LLaMA-Adapter, which includes:
    - **ChatGPT**: A benchmark for text-based instruction-following models, which consists of 175 human-written instruction-output pairs covering various domains and tasks[^1^][2].
    - **ScienceQA**: A benchmark for image-conditioned instruction-following models, which consists of 1K science questions with images and answers[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define LLaMA model with 48 transformer layers and 7B parameters
LLaMA = TransformerModel(num_layers=48, num_params=7B)

# Define adaption prompts as learnable text tokens
adaption_prompts = LearnableTokens(num_tokens=16)

# Define zero-init attention as a novel attention mechanism
zero_init_attention = ZeroInitAttention()

# Freeze the parameters of LLaMA except for the adaption prompts and the zero-init attention
freeze_parameters(LLaMA)
unfreeze_parameters(adaption_prompts, zero_init_attention)

# Load the 52K instruction-output pairs as training data
train_data = load_data("52K_instruction_output_pairs")

# Fine-tune LLaMA-Adapter on 8 A100 GPUs for 10 epochs
for epoch in range(10):
    # Shuffle the training data
    train_data = shuffle(train_data)
    # Iterate over mini-batches of size 64
    for batch in get_batches(train_data, batch_size=64):
        # Get the input text tokens and the output text tokens
        input_tokens, output_tokens = batch
        # Prepend the adaption prompts to the input text tokens at higher transformer layers
        input_tokens_with_prompts = prepend_prompts(input_tokens, adaption_prompts, start_layer=24)
        # Apply the zero-init attention to the input text tokens with prompts
        input_tokens_with_attention = apply_attention(input_tokens_with_prompts, zero_init_attention)
        # Feed the input text tokens with attention to LLaMA and get the output logits
        output_logits = LLaMA(input_tokens_with_attention)
        # Compute the cross-entropy loss between the output logits and the output tokens
        loss = cross_entropy(output_logits, output_tokens)
        # Back-propagate the loss and update the parameters of adaption prompts and zero-init attention
        loss.backward()
        optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import transformers
import datasets

# Define LLaMA model with 48 transformer layers and 7B parameters
LLaMA = transformers.GPT2LMHeadModel.from_pretrained("gpt2-xl")

# Define adaption prompts as learnable text tokens
adaption_prompts = torch.nn.Embedding(num_embeddings=16, embedding_dim=2048)

# Define zero-init attention as a novel attention mechanism
class ZeroInitAttention(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # Initialize the attention weights and bias with zeros
        self.attention_weights = torch.nn.Parameter(torch.zeros(2048, 2048))
        self.attention_bias = torch.nn.Parameter(torch.zeros(2048))
        # Initialize the zero gating with zeros
        self.zero_gating = torch.nn.Parameter(torch.zeros(2048))

    def forward(self, input_tokens):
        # Compute the attention scores by multiplying the input tokens and the attention weights
        attention_scores = torch.matmul(input_tokens, self.attention_weights)
        # Add the attention bias to the attention scores
        attention_scores = attention_scores + self.attention_bias
        # Apply the softmax function to the attention scores
        attention_scores = torch.nn.functional.softmax(attention_scores, dim=-1)
        # Compute the attention output by multiplying the attention scores and the input tokens
        attention_output = torch.matmul(attention_scores, input_tokens)
        # Apply the zero gating to the attention output
        attention_output = attention_output * self.zero_gating
        # Return the attention output
        return attention_output

# Instantiate the zero-init attention module
zero_init_attention = ZeroInitAttention()

# Freeze the parameters of LLaMA except for the adaption prompts and the zero-init attention
for param in LLaMA.parameters():
    param.requires_grad = False

for param in adaption_prompts.parameters():
    param.requires_grad = True

for param in zero_init_attention.parameters():
    param.requires_grad = True

# Load the 52K instruction-output pairs as training data
train_data = datasets.load_dataset("alpaca", split="train")

# Define a collate function to process the input and output text tokens
def collate_fn(batch):
    # Get the input and output texts from the batch
    input_texts = [example["input"] for example in batch]
    output_texts = [example["output"] for example in batch]
    # Tokenize the input and output texts using GPT2 tokenizer
    tokenizer = transformers.GPT2Tokenizer.from_pretrained("gpt2-xl")
    input_tokens = tokenizer(input_texts, return_tensors="pt", padding=True)
    output_tokens = tokenizer(output_texts, return_tensors="pt", padding=True)
    # Return the input and output tokens as a dictionary
    return {"input_tokens": input_tokens, "output_tokens": output_tokens}

# Create a data loader for the training data using the collate function
train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)

# Define an AdamW optimizer with weight decay of 0.01 and gradient clipping of 1.0
optimizer = transformers.AdamW([{"params": adaption_prompts.parameters()}, {"params": zero_init_attention.parameters()}], lr=1e-4, weight_decay=0.01)
optimizer.clip_grad_norm_ = 1.0

# Fine-tune LLaMA-Adapter on 8 A100 GPUs for 10 epochs
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
LLaMA.to(device)
adaption_prompts.to(device)
zero_init_attention.to(device)

for epoch in range(10):
    # Iterate over mini-batches of size 64
    for batch in train_loader:
        # Get the input and output tokens from the batch
        input_tokens = batch["input_tokens"].to(device)
        output_tokens = batch["output_tokens"].to(device)
        # Prepend the adaption prompts to the input tokens at higher transformer layers (layer 24 to 47)
        adaption_prompt_ids = torch.randint(0, 16, (input_tokens.size(0), 16)).to(device) # Randomly sample 16 prompt ids for each input sequence
        adaption_prompt_embeddings = adaption_prompts(adaption_prompt_ids) # Get the prompt embeddings from the adaption prompts module
        input_tokens_with_prompts = torch.cat([input_tokens, adaption_prompt_embeddings], dim=1) # Concatenate the prompt embeddings and the input tokens
        input_tokens_with_prompts = torch.nn.functional.pad(input_tokens_with_prompts, (0, 0, 0, 24)) # Pad the input tokens with prompts with 24 zeros at the end
        input_tokens_with_prompts = input_tokens_with_prompts.unsqueeze(1).repeat(1, 48, 1, 1) # Repeat the input tokens with prompts for each transformer layer
        input_tokens_with_prompts[:, :24, :, -24:] = 0 # Mask out the prompt embeddings for the lower transformer layers (layer 0 to 23)
        # Apply the zero-init attention to the input tokens with prompts
        input_tokens_with_attention = zero_init_attention(input_tokens_with_prompts) # Get the attention output from the zero-init attention module
        input_tokens_with_attention = input_tokens_with_attention + input_tokens_with_prompts # Add the attention output and the input tokens with prompts
        # Feed the input tokens with attention to LLaMA and get the output logits
        output_logits = LLaMA(inputs_embeds=input_tokens_with_attention).logits # Get the output logits from LLaMA using the input embeddings
        # Compute the cross-entropy loss between the output logits and the output tokens
        loss = torch.nn.functional.cross_entropy(output_logits.view(-1, output_logits.size(-1)), output_tokens.view(-1), ignore_index=tokenizer.pad_token_id) # Compute the cross-entropy loss and ignore the padding tokens
        # Back-propagate the loss and update the parameters of adaption prompts and zero-init attention
        loss.backward() # Compute the gradients
        optimizer.step() # Update the parameters
        optimizer.zero_grad() # Zero out the gradients
```