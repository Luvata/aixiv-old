---
title: 2105.08222v1 Decorating Your Own Bedroom  Locally Controlling Image Generation with Generative Adversarial Networks
date: 2021-05-09
---

# [Decorating Your Own Bedroom: Locally Controlling Image Generation with Generative Adversarial Networks](http://arxiv.org/abs/2105.08222v1)

authors: Chen Zhang, Yinghao Xu, Yujun Shen


## What, Why and How

[1]: https://arxiv.org/abs/2105.08222 "[2105.08222] Decorating Your Own Bedroom: Locally Controlling Image ..."
[2]: https://arxiv.org/pdf/2105.08222v1.pdf "fxy119, sy116g@ie.cuhk.edu.hk arXiv:2105.08222v1 [cs.CV] 18 May 2021"
[3]: http://export.arxiv.org/abs/2106.08222v1 "[2106.08222v1] Artificial Neural Network Classification of 4FGL Sources"

Here is a summary of the paper:

- **What**: The paper proposes an approach, called LoGAN, to locally control image generation with generative adversarial networks (GANs). It introduces two operators, content modulation and style modulation, and a priority mask to manipulate the intermediate features of a pre-trained GAN model. It demonstrates the ability to edit individual objects and re-decorate a whole bedroom with customized furniture and styles using a StyleGAN2 model[^1^][1].
- **Why**: The paper aims to address the challenge of steering the generation process of a well-trained GAN model and customizing the output image. Existing methods mainly focus on global editing, but local editing is more critical in many practical cases. The paper claims that local editing can provide more flexibility and creativity for image synthesis applications.
- **How**: The paper adopts a feature-level manipulation strategy, which leverages the spatial information encoded in the intermediate feature maps of GANs. It designs two operators to modulate the content and style of the features at different layers of the generator. It also introduces a priority mask to handle the occlusion and overlap between different objects. The paper presents a four-step pipeline to re-decorate a bedroom: clearing out the room based on layout estimation, composing objects with content modulation, rotating objects with style modulation, and changing styles with layer-wise synthesis[^1^][1]. The paper evaluates the proposed method qualitatively and quantitatively on bedroom synthesis and compares it with existing methods.

[^1^][1]: [2105.08222] Decorating Your Own Bedroom: Locally Controlling Image Generation with Generative Adversarial Networks


## Main Contributions

According to the paper, the main contributions are:

- It proposes a novel approach to locally control image generation with GANs, which can edit individual objects and re-decorate a whole room with customized furniture and styles.
- It introduces two operators, content modulation and style modulation, to manipulate the intermediate features of a pre-trained GAN model in a fine-grained manner.
- It designs a priority mask to handle the spatial relationship between different objects and avoid unrealistic occlusion and overlap.
- It demonstrates the effectiveness and flexibility of the proposed method on bedroom synthesis using a StyleGAN2 model and compares it with existing methods.

## Method Summary

The method section of the paper consists of four subsections:

- In subsection 3.1, the paper reviews the background of StyleGAN2 and introduces the notation and terminology used in the paper.
- In subsection 3.2, the paper presents the content modulation operator, which can insert, remove, or shift an object in a local region by modulating the feature map at a specific layer of the generator.
- In subsection 3.3, the paper presents the style modulation operator, which can rotate an object in a local region by modulating the style vector at a specific layer of the generator.
- In subsection 3.4, the paper presents the priority mask, which can control the occlusion and overlap between different objects by assigning different priorities to different regions of the feature map.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a source image x, a set of object queries Q, a target style s
# Output: an edited image y

# Step 1: Clear out the room based on layout estimation
layout = estimate_layout(x) # use a pre-trained layout estimation model
mask = generate_mask(layout) # generate a binary mask for each object in the layout
z = sample_latent_code() # sample a latent code from the prior distribution
f = G(z) # generate the intermediate feature map using the generator G
f_cleared = f * (1 - mask) # clear out the objects by masking out the corresponding regions

# Step 2: Compose objects with content modulation
for each object query q in Q:
  z_q = retrieve_latent_code(q) # retrieve a latent code for the object query using an embedding model
  f_q = G(z_q) # generate the intermediate feature map for the object query
  l_q = select_layer(q) # select the appropriate layer for inserting the object
  r_q = select_region(q) # select the desired region for inserting the object
  f_cleared[l_q][r_q] = f_q[l_q][r_q] # replace the region with the object feature map

# Step 3: Rotate objects with style modulation
for each object query q in Q:
  if q requires rotation:
    theta_q = get_rotation_angle(q) # get the desired rotation angle for the object
    w_q = get_style_vector(theta_q) # get the style vector corresponding to the rotation angle
    l_q = select_layer(q) # select the appropriate layer for rotating the object
    r_q = select_region(q) # select the desired region for rotating the object
    f_cleared[l_q][r_q] = AdaIN(f_cleared[l_q][r_q], w_q) # apply adaptive instance normalization with the style vector

# Step 4: Change styles with layer-wise synthesis
w_s = get_style_vector(s) # get the style vector corresponding to the target style
y = synthesize_image(f_cleared, w_s) # synthesize the final image using layer-wise synthesis with the style vector
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Load the pre-trained StyleGAN2 model for bedroom synthesis
model = torch.hub.load('NVIDIA/StyleGAN2-ADA', 'stylegan2', pretrained=True)

# Define the function to sample a latent code from the prior distribution
def sample_latent_code():
  return torch.randn(1, 512)

# Define the function to retrieve a latent code for an object query using an embedding model
def retrieve_latent_code(query):
  # Use a pre-trained embedding model (e.g., CLIP) to embed the query into a latent code
  # Alternatively, use a pre-defined dictionary to map the query to a latent code
  return embed(query)

# Define the function to estimate the layout of an image using a pre-trained layout estimation model
def estimate_layout(image):
  # Use a pre-trained layout estimation model (e.g., D-Net) to predict the layout of the image
  # Alternatively, use a pre-defined template to approximate the layout of the image
  return predict_layout(image)

# Define the function to generate a binary mask for each object in the layout
def generate_mask(layout):
  # Use OpenCV or other libraries to generate a binary mask for each object in the layout
  # The mask should have the same shape as the feature map at each layer of the generator
  return mask

# Define the function to select the appropriate layer for inserting or rotating an object
def select_layer(query):
  # Use a heuristic or a learned rule to select the layer based on the object query
  # For example, use lower layers for larger objects and higher layers for smaller objects
  return layer

# Define the function to select the desired region for inserting or rotating an object
def select_region(query):
  # Use a heuristic or a learned rule to select the region based on the object query
  # For example, use random sampling or grid search to find a suitable region
  return region

# Define the function to get the rotation angle for an object query
def get_rotation_angle(query):
  # Use a heuristic or a learned rule to get the rotation angle based on the object query
  # For example, use natural language processing or regular expressions to parse the query
  return angle

# Define the function to get the style vector corresponding to a rotation angle or a target style
def get_style_vector(input):
  # Use a heuristic or a learned mapping to get the style vector based on the input
  # For example, use linear interpolation or neural networks to map the input to a style vector
  return style

# Define the function to apply adaptive instance normalization with a style vector
def AdaIN(feature, style):
  # Use PyTorch or other libraries to implement adaptive instance normalization with a style vector
  # The feature and style should have compatible shapes and types
  return normalized_feature

# Define the function to synthesize an image using layer-wise synthesis with a style vector
def synthesize_image(feature, style):
  # Use PyTorch or other libraries to implement layer-wise synthesis with a style vector
  # The feature and style should have compatible shapes and types
  return image

# Input: a source image x, a set of object queries Q, a target style s
x = cv2.imread('source.jpg') # read the source image from file
Q = ['remove bed', 'insert window', 'shift window', 'rotate bed'] # define the set of object queries
s = 'modern' # define the target style

# Output: an edited image y

# Step 1: Clear out the room based on layout estimation
layout = estimate_layout(x) # estimate the layout of the source image
mask = generate_mask(layout) # generate a binary mask for each object in the layout
z = sample_latent_code() # sample a latent code from the prior distribution
f = model.mapping(z) # generate intermediate feature map using mapping network of StyleGAN2 model 
f_cleared = f * (1 - mask) # clear out objects by masking out corresponding regions

# Step 2: Compose objects with content modulation
for each object query q in Q:
  z_q = retrieve_latent_code(q) # retrieve latent code for object query using embedding model 
  f_q = model.mapping(z_q) # generate intermediate feature map for object query using mapping network 
  l_q = select_layer(q) # select appropriate layer for inserting object 
  r_q = select_region(q) # select desired region for inserting object 
  f_cleared[l_q][r_q] = f_q[l_q][r_q] # replace region with object feature map

# Step 3: Rotate objects with style modulation
for each object query q in Q:
  if q requires rotation:
    theta_q = get_rotation_angle(q) # get desired rotation angle for object 
    w_q = get_style_vector(theta_q) # get style vector corresponding to rotation angle 
    l_q = select_layer(q) # select appropriate layer for rotating object 
    r_q = select_region(q) # select desired region for rotating object 
    f_cleared[l_q][r_q] = AdaIN(f_cleared[l_q][r_q], w_q) # apply adaptive instance normalization with style vector

# Step 4: Change styles with layer-wise synthesis
w_s = get_style_vector(s) # get style vector corresponding to target style 
y = synthesize_image(f_cleared, w_s) # synthesize final image using layer-wise synthesis with style vector
cv2.imwrite('edited.jpg', y) # save the edited image to file
```