---
title: 2006.12075v1 MotioNet  3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency
date: 2020-06-13
---

# [MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency](http://arxiv.org/abs/2006.12075v1)

authors: Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen


## What, Why and How

[1]: https://arxiv.org/pdf/2006.12075v1 "MotioNet: 3D Human Motion Reconstruction from Monocular Video with ..."
[2]: https://arxiv.org/abs/2006.12075v1 "[2006.12075v1] MotioNet: 3D Human Motion Reconstruction from Monocular ..."
[3]: http://export.arxiv.org/abs/2006.12075 "[2006.12075] MotioNet: 3D Human Motion Reconstruction from Monocular ..."

Here is a summary of the paper:

- **What**: The paper introduces MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video[^1^][1].
- **Why**: The paper aims to overcome the limitations of previous methods that rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations[^1^][1]. The paper claims that these methods are either data-agnostic or assume an underlying model that may not fit the data well[^1^][1].
- **How**: The paper proposes a data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation[^1^][1]. The paper uses a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels[^1^][1]. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth[^1^][1]. In addition, an adversarial loss is applied to the velocities of the recovered rotations, to ensure that they lie on the manifold of natural joint rotations[^1^][1]. The paper shows that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities[^1^][1].


## Main Contributions

[1]: https://dl.acm.org/doi/10.1145/3407659 "MotioNet: 3D Human Motion Reconstruction from Monocular Video with ..."
[2]: https://arxiv.org/abs/2006.12075 "[2006.12075] MotioNet: 3D Human Motion Reconstruction from Monocular ..."
[3]: https://github.com/Shimingyi/MotioNet "GitHub - Shimingyi/MotioNet: A deep neural network that directly ..."
[4]: https://www.research.ed.ac.uk/en/publications/motionet-3d-human-motion-reconstruction-from-video-with-skeleton- "MotioNet: 3D Human Motion Reconstruction from Video with Skeleton ..."

According to the paper[^1^][1], the main contributions are:

- **A novel data-driven approach** that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation, from monocular video[^1^][1].
- **A deep neural network with embedded kinematic priors**, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels[^1^][1].
- **An integrated forward kinematics (FK) layer** that outputs 3D positions, which are compared to a ground truth[^1^][1].
- **An adversarial loss** that is applied to the velocities of the recovered rotations to ensure that they lie on the manifold of natural joint rotations[^1^][1].
- **A comprehensive evaluation** that demonstrates the effectiveness and robustness of the proposed method on various datasets and scenarios[^1^][1].

## Method Summary

[1]: https://dl.acm.org/doi/10.1145/3407659 "MotioNet: 3D Human Motion Reconstruction from Monocular Video with ..."
[2]: https://eng.libretexts.org/Bookshelves/Mechanical_Engineering/Mechanics_Map_%28Moore_et_al.%29/05%3A_Engineering_Structures/5.05%3A_Method_of_Sections "5.5: Method of Sections - Engineering LibreTexts"
[3]: https://eng.libretexts.org/Bookshelves/Mechanical_Engineering/Engineering_Statics%3A_Open_and_Interactive_%28Baker_and_Haynes%29/06%3A_Equilibrium_of_Structures/6.05%3A_Method_of_Sections "6.5: Method of Sections - Engineering LibreTexts"

Here is a summary of the method section of the paper:

- The method section describes the architecture and training of MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video[^1^][1].
- The method section consists of four subsections: overview, network architecture, training data, and training details[^1^][1].
- The overview subsection introduces the main components and workflow of MotioNet, which are: 2D pose estimation, skeleton estimation, rotation estimation, forward kinematics layer, and adversarial loss[^1^][1].
- The network architecture subsection explains the design and functionality of each component in detail, such as the input and output dimensions, the loss functions, and the kinematic priors[^1^][1].
- The training data subsection describes the datasets and preprocessing steps used to obtain the ground truth 3D poses and skeletons for training MotioNet[^1^][1].
- The training details subsection provides the implementation details and hyperparameters for training MotioNet, such as the optimizer, learning rate, batch size, and number of epochs[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a monocular video of a human motion
# Output: a kinematic skeleton and a sequence of 3D joint rotations

# Step 1: Estimate 2D joint positions from the video frames using a pre-trained pose estimator
2D_poses = pose_estimator(video)

# Step 2: Estimate the skeleton (bone lengths) from the 2D joint positions using a skeleton estimator network
skeleton = skeleton_estimator(2D_poses)

# Step 3: Estimate the 3D joint rotations and global root positions from the 2D joint positions using a rotation estimator network
rotations, root_positions = rotation_estimator(2D_poses)

# Step 4: Apply a forward kinematics layer to convert the rotations and root positions to 3D joint positions
3D_poses = forward_kinematics(rotations, root_positions, skeleton)

# Step 5: Apply an adversarial loss to the velocities of the rotations to ensure they are natural and smooth
loss = adversarial_loss(rotations)

# Step 6: Return the kinematic skeleton and the sequence of 3D joint rotations
return skeleton, rotations
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a monocular video of a human motion
# Output: a kinematic skeleton and a sequence of 3D joint rotations

# Step 1: Estimate 2D joint positions from the video frames using a pre-trained pose estimator
# We use OpenPose as an example pose estimator
import openpose
pose_estimator = openpose.PoseEstimator()
2D_poses = pose_estimator(video) # a tensor of shape (num_frames, num_joints, 2)

# Step 2: Estimate the skeleton (bone lengths) from the 2D joint positions using a skeleton estimator network
# We use a fully connected network with two hidden layers and ReLU activations as an example skeleton estimator
import torch.nn as nn
skeleton_estimator = nn.Sequential(
    nn.Linear(num_joints * 2, 256),
    nn.ReLU(),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, num_bones)
)
skeleton = skeleton_estimator(2D_poses.mean(dim=0)) # a tensor of shape (num_bones)

# Step 3: Estimate the 3D joint rotations and global root positions from the 2D joint positions using a rotation estimator network
# We use a bidirectional LSTM network with residual connections and kinematic priors as an example rotation estimator
import torch.nn as nn
import torch.nn.functional as F
class RotationEstimator(nn.Module):
    def __init__(self):
        super(RotationEstimator, self).__init__()
        self.lstm = nn.LSTM(num_joints * 2, 1024, bidirectional=True)
        self.fc1 = nn.Linear(2048, num_joints * 9)
        self.fc2 = nn.Linear(2048, num_joints * 3)
        self.fc3 = nn.Linear(2048, num_joints)
        self.res1 = nn.Linear(num_joints * 9, num_joints * 9)
        self.res2 = nn.Linear(num_joints * 3, num_joints * 3)
        self.res3 = nn.Linear(num_joints, num_joints)

    def forward(self, x):
        # x is a tensor of shape (num_frames, num_joints, 2)
        x = x.reshape(num_frames, -1) # flatten the joint coordinates
        x = x.unsqueeze(1) # add a batch dimension
        x, _ = self.lstm(x) # pass through the LSTM layer
        x = x.squeeze(1) # remove the batch dimension
        r = self.fc1(x) # predict the rotation matrices
        r = r + self.res1(r) # add a residual connection
        r = r.reshape(num_frames, num_joints, 3, 3) # reshape to matrices
        r = F.normalize(r, dim=-1) # normalize to ensure orthogonality
        p = self.fc2(x) # predict the global root positions
        p = p + self.res2(p) # add a residual connection
        p = p.reshape(num_frames, num_joints, 3) # reshape to vectors
        c = self.fc3(x) # predict the foot contact labels
        c = c + self.res3(c) # add a residual connection
        c = torch.sigmoid(c) # apply sigmoid to get probabilities
        return r, p, c

rotation_estimator = RotationEstimator()
rotations, root_positions, contacts = rotation_estimator(2D_poses) # tensors of shape (num_frames, num_joints, 3, 3), (num_frames, num_joints, 3), and (num_frames, num_joints)

# Step 4: Apply a forward kinematics layer to convert the rotations and root positions to 3D joint positions
# We use a custom layer that implements the forward kinematics equations as an example forward kinematics layer
import torch.nn as nn
class ForwardKinematics(nn.Module):
    def __init__(self):
        super(ForwardKinematics, self).__init__()
        self.parent_indices = [0] + list(range(num_joints)) # define the parent-child relationship of the joints

    def forward(self, r, p, l):
        # r is a tensor of shape (num_frames, num_joints, 3, 3) containing the rotation matrices of each joint
        # p is a tensor of shape (num_frames, num_joints, 3) containing the global root positions of each joint
        # l is a tensor of shape (num_bones) containing the bone lengths of the skeleton
        x = torch.zeros(num_frames, num_joints, 3) # initialize the output tensor
        for i in range(num_joints):
            j = self.parent_indices[i] # get the parent index of the current joint
            if i == 0: # if the joint is the root
                x[:, i, :] = p[:, i, :] # use the global root position as the joint position
            else: # otherwise
                x[:, i, :] = x[:, j, :] + torch.matmul(r[:, j, :, :], l[i-1] * torch.eye(3)) # use the parent joint position plus the rotated bone vector as the joint position
        return x

forward_kinematics = ForwardKinematics()
3D_poses = forward_kinematics(rotations, root_positions, skeleton) # a tensor of shape (num_frames, num_joints, 3)

# Step 5: Apply an adversarial loss to the velocities of the rotations to ensure they are natural and smooth
# We use a Wasserstein GAN with gradient penalty as an example adversarial loss
import torch.nn as nn
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.fc1 = nn.Linear(num_joints * 9, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 1)

    def forward(self, x):
        # x is a tensor of shape (num_frames - 1, num_joints, 3, 3) containing the velocities of the rotation matrices
        x = x.reshape(num_frames - 1, -1) # flatten the rotation matrices
        x = F.leaky_relu(self.fc1(x)) # pass through the first fully connected layer with leaky ReLU activation
        x = F.leaky_relu(self.fc2(x)) # pass through the second fully connected layer with leaky ReLU activation
        x = self.fc3(x) # pass through the third fully connected layer without activation
        return x

discriminator = Discriminator()
real_velocities = get_real_velocities() # a tensor of shape (num_frames - 1, num_joints, 3, 3) containing the velocities of real rotation matrices from a dataset
fake_velocities = rotations[1:] - rotations[:-1] # a tensor of shape (num_frames - 1, num_joints, 3, 3) containing the velocities of predicted rotation matrices
real_scores = discriminator(real_velocities) # a tensor of shape (num_frames - 1, 1) containing the scores for real velocities
fake_scores = discriminator(fake_velocities) # a tensor of shape (num_frames - 1, 1) containing the scores for fake velocities
wasserstein_distance = real_scores.mean() - fake_scores.mean() # a scalar representing the Wasserstein distance between real and fake velocities
gradient_penalty = compute_gradient_penalty(discriminator, real_velocities, fake_velocities) # a scalar representing the gradient penalty for satisfying the Lipschitz constraint
discriminator_loss = -wasserstein_distance + gradient_penalty # a scalar representing the loss for training the discriminator
generator_loss = -fake_scores.mean() # a scalar representing the loss for training the generator (rotation estimator)
loss = generator_loss + discriminator_loss # a scalar representing the total adversarial loss

# Step 6: Return the kinematic skeleton and the sequence of 3D joint rotations
return skeleton, rotations
```