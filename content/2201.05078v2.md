---
title: 2201.05078v2 CLIP-Event  Connecting Text and Images with Event Structures
date: 2022-01-06
---

# [CLIP-Event: Connecting Text and Images with Event Structures](http://arxiv.org/abs/2201.05078v2)

authors: Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, Shih-Fu Chang


## What, Why and How

[1]: https://arxiv.org/abs/2201.05078 "CLIP-Event: Connecting Text and Images with Event Structures"
[2]: https://arxiv.org/pdf/2201.05078v2 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2101.05078v2 "arXiv.org"

Here is a summary of the paper:

- **What**: The paper proposes a contrastive learning framework called **CLIP-Event** that connects text and images with event structures, which are representations of actions and their participants.
- **Why**: The paper aims to improve the vision-language (V+L) pretraining models that can understand the alignments between images and text, especially at the level of events and their argument structures, which are often ignored by existing models.
- **How**: The paper leverages text information extraction technologies to obtain event structural knowledge from sentences, and uses multiple prompt functions to contrast difficult negative descriptions by manipulating event structures. The paper also designs an event graph alignment loss based on optimal transport to capture event argument structures. The paper collects a large event-rich dataset for pretraining, and evaluates the model on various downstream tasks under zero-shot settings.

The paper is titled **CLIP-Event: Connecting Text and Images with Event Structures**[^1^][1], and it is authored by Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and Shih-Fu Chang. It is accepted by CVPR 2022.

## Main Contributions

[1]: https://arxiv.org/abs/2201.05078 "CLIP-Event: Connecting Text and Images with Event Structures"
[2]: https://arxiv.org/pdf/2201.05078v2 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2101.05078v2 "arXiv.org"

According to the paper, the main contributions are:

- **A novel contrastive learning framework** that connects text and images with event structures, which are representations of actions and their participants.
- **A large event-rich dataset** (106,875 images) for pretraining, which provides a more challenging image retrieval benchmark to assess the understanding of complicated lengthy sentences.
- **An event graph alignment loss** based on optimal transport to capture event argument structures.
- **State-of-the-art results** on argument extraction on Multimedia Event Extraction, as well as significant improvements on a variety of downstream tasks under zero-shot settings.

## Method Summary

[1]: https://arxiv.org/abs/2201.05078 "CLIP-Event: Connecting Text and Images with Event Structures"
[2]: https://arxiv.org/pdf/2201.05078v2 "arXiv.org e-Print archive"
[3]: https://arxiv.org/pdf/2101.05078v2 "arXiv.org"

Here is a summary of the method section of the paper:

- The paper adopts the **CLIP** model as the backbone for vision-language pretraining, which consists of a **ViT** encoder for images and a **Transformer** encoder for text.
- The paper introduces three components to enhance the event understanding of the CLIP model: **event structure extraction**, **event structure contrastive learning**, and **event graph alignment**.
- The paper uses an **OpenIE** system to extract event structures from sentences, which are represented as tuples of predicates and arguments. The paper also uses a **NER** system to identify entities in sentences and images, and maps them to event arguments.
- The paper proposes a set of **prompt functions** to generate positive and negative descriptions for images based on event structures. The prompt functions manipulate the event structures by replacing, deleting, or adding predicates or arguments. The paper uses a **contrastive loss** to train the model to distinguish between positive and negative descriptions.
- The paper also proposes an **event graph alignment loss** based on optimal transport to align the event graphs extracted from sentences and images. The event graphs are constructed by connecting event tuples that share common arguments. The optimal transport problem is solved by a **Sinkhorn algorithm**.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the CLIP model with a ViT encoder for images and a Transformer encoder for text
clip_model = CLIP(ViT, Transformer)

# Define the OpenIE system to extract event structures from sentences
openie_system = OpenIE()

# Define the NER system to identify entities in sentences and images
ner_system = NER()

# Define the prompt functions to generate positive and negative descriptions based on event structures
prompt_functions = [replace_predicate, replace_argument, delete_predicate, delete_argument, add_predicate, add_argument]

# Define the contrastive loss function to train the model to distinguish between positive and negative descriptions
contrastive_loss = ContrastiveLoss()

# Define the event graph alignment loss function based on optimal transport to align the event graphs extracted from sentences and images
event_graph_alignment_loss = EventGraphAlignmentLoss()

# Define the Sinkhorn algorithm to solve the optimal transport problem
sinkhorn_algorithm = SinkhornAlgorithm()

# Load the event-rich dataset for pretraining
dataset = load_dataset("event-rich")

# Pretrain the model on the dataset
for image, sentence in dataset:
  # Encode the image and the sentence using the CLIP model
  image_embedding = clip_model.encode_image(image)
  sentence_embedding = clip_model.encode_text(sentence)

  # Extract the event structure from the sentence using the OpenIE system
  event_structure = openie_system.extract(sentence)

  # Identify the entities in the sentence and the image using the NER system
  sentence_entities = ner_system.identify(sentence)
  image_entities = ner_system.identify(image)

  # Map the entities to the event arguments
  entity_mapping = map_entities(event_structure, sentence_entities, image_entities)

  # Generate positive and negative descriptions for the image using the prompt functions
  positive_description = generate_positive_description(event_structure, entity_mapping)
  negative_descriptions = generate_negative_descriptions(event_structure, entity_mapping, prompt_functions)

  # Encode the positive and negative descriptions using the CLIP model
  positive_description_embedding = clip_model.encode_text(positive_description)
  negative_description_embeddings = [clip_model.encode_text(negative_description) for negative_description in negative_descriptions]

  # Compute the contrastive loss between the image embedding and the description embeddings
  loss_contrastive = contrastive_loss(image_embedding, positive_description_embedding, negative_description_embeddings)

  # Construct the event graphs for the sentence and the image by connecting event tuples that share common arguments
  sentence_event_graph = construct_event_graph(event_structure)
  image_event_graph = construct_event_graph(entity_mapping)

  # Compute the event graph alignment loss between the sentence event graph and the image event graph using optimal transport
  loss_event_graph_alignment = event_graph_alignment_loss(sentence_event_graph, image_event_graph, sinkhorn_algorithm)

  # Compute the total loss as a weighted sum of the contrastive loss and the event graph alignment loss
  loss_total = alpha * loss_contrastive + beta * loss_event_graph_alignment

  # Update the model parameters using gradient descent
  clip_model.update(loss_total)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import openie
import ner
import ot

# Define the CLIP model with a ViT encoder for images and a Transformer encoder for text
clip_model = CLIP(ViT, Transformer)

# Define the OpenIE system to extract event structures from sentences
openie_system = OpenIE()

# Define the NER system to identify entities in sentences and images
ner_system = NER()

# Define the prompt functions to generate positive and negative descriptions based on event structures
def replace_predicate(event_structure, entity_mapping):
  # Randomly select a predicate from the event structure
  predicate = random.choice(event_structure.predicates)

  # Randomly select a synonym or an antonym for the predicate
  new_predicate = random.choice(get_synonyms(predicate) + get_antonyms(predicate))

  # Replace the predicate with the new predicate in the event structure
  event_structure.replace(predicate, new_predicate)

  # Generate a description from the modified event structure and the entity mapping
  description = generate_description(event_structure, entity_mapping)

  # Return the description
  return description

def replace_argument(event_structure, entity_mapping):
  # Randomly select an argument from the event structure
  argument = random.choice(event_structure.arguments)

  # Randomly select an entity from the image that is not mapped to any argument
  new_entity = random.choice([entity for entity in image_entities if entity not in entity_mapping.values()])

  # Replace the argument with the new entity in the event structure and the entity mapping
  event_structure.replace(argument, new_entity)
  entity_mapping[argument] = new_entity

  # Generate a description from the modified event structure and the entity mapping
  description = generate_description(event_structure, entity_mapping)

  # Return the description
  return description

def delete_predicate(event_structure, entity_mapping):
  # Randomly select a predicate from the event structure
  predicate = random.choice(event_structure.predicates)

  # Delete the predicate and its arguments from the event structure and the entity mapping
  event_structure.delete(predicate)
  for argument in predicate.arguments:
    entity_mapping.pop(argument)

  # Generate a description from the modified event structure and the entity mapping
  description = generate_description(event_structure, entity_mapping)

  # Return the description
  return description

def delete_argument(event_structure, entity_mapping):
  # Randomly select an argument from the event structure that is not a trigger or a subject of any predicate
  argument = random.choice([argument for argument in event_structure.arguments if not (argument.is_trigger or argument.is_subject)])

  # Delete the argument and its predicate from the event structure and the entity mapping
  predicate = argument.predicate
  event_structure.delete(predicate)
  for argument in predicate.arguments:
    entity_mapping.pop(argument)

  # Generate a description from the modified event structure and the entity mapping
  description = generate_description(event_structure, entity_mapping)

  # Return the description
  return description

def add_predicate(event_structure, entity_mapping):
  # Randomly select a predicate that is not in the event structure
  predicate = random.choice([predicate for predicate in all_predicates if predicate not in event_structure.predicates])

  # Randomly select arguments for the predicate from the image entities that are not mapped to any argument
  arguments = random.sample([entity for entity in image_entities if entity not in entity_mapping.values()], k=predicate.arity)

  # Add the predicate and its arguments to the event structure and the entity mapping
  event_structure.add(predicate, arguments)
  for argument in arguments:
    entity_mapping[argument] = argument

  # Generate a description from the modified event structure and the entity mapping
  description = generate_description(event_structure, entity_mapping)

  # Return the description
  return description

def add_argument(event_structure, entity_mapping):
  # Randomly select a predicate from the event structure that has less than its maximum arity of arguments
  predicate = random.choice([predicate for predicate in event_structure.predicates if len(predicate.arguments) < predicate.max_arity])

  # Randomly select an argument role for the predicate that is not filled by any existing argument
  role = random.choice([role for role in predicate.roles if role not in [argument.role for argument in predicate.arguments]])

  # Randomly select an entity from the image that is not mapped to any argument
  new_entity = random.choice([entity for entity in image_entities if entity not in entity_mapping.values()])

  # Add the argument with its role and its entity to the predicate, the event structure and the entity mapping
  argument = Argument(role, new_entity)
  predicate.add_argument(argument)
  event_structure.add_argument(argument)
  entity_mapping[argument] = new_entity

   # Generate a description from the modified event structure and the entity mapping
  description = generate_description(event_structure, entity_mapping)

  # Return the description
  return description

prompt_functions = [replace_predicate, replace_argument, delete_predicate, delete_argument, add_predicate, add_argument]

# Define the contrastive loss function to train the model to distinguish between positive and negative descriptions
def contrastive_loss(image_embedding, positive_description_embedding, negative_description_embeddings):
  # Compute the cosine similarity between the image embedding and the positive description embedding
  positive_similarity = cosine_similarity(image_embedding, positive_description_embedding)

  # Compute the cosine similarities between the image embedding and the negative description embeddings
  negative_similarities = [cosine_similarity(image_embedding, negative_description_embedding) for negative_description_embedding in negative_description_embeddings]

  # Compute the contrastive loss as the cross entropy between the positive similarity and the softmax of all similarities
  loss = cross_entropy(positive_similarity, softmax([positive_similarity] + negative_similarities))

  # Return the loss
  return loss

# Define the event graph alignment loss function based on optimal transport to align the event graphs extracted from sentences and images
def event_graph_alignment_loss(sentence_event_graph, image_event_graph, sinkhorn_algorithm):
  # Compute the node embeddings for the sentence event graph and the image event graph using the CLIP model
  sentence_node_embeddings = [clip_model.encode_text(node) for node in sentence_event_graph.nodes]
  image_node_embeddings = [clip_model.encode_image(node) for node in image_event_graph.nodes]

  # Compute the edge embeddings for the sentence event graph and the image event graph by averaging the node embeddings of the endpoints
  sentence_edge_embeddings = [(sentence_node_embeddings[u] + sentence_node_embeddings[v]) / 2 for u, v in sentence_event_graph.edges]
  image_edge_embeddings = [(image_node_embeddings[u] + image_node_embeddings[v]) / 2 for u, v in image_event_graph.edges]

  # Compute the node cost matrix as the negative cosine similarities between the sentence node embeddings and the image node embeddings
  node_cost_matrix = -cosine_similarity_matrix(sentence_node_embeddings, image_node_embeddings)

  # Compute the edge cost matrix as the negative cosine similarities between the sentence edge embeddings and the image edge embeddings
  edge_cost_matrix = -cosine_similarity_matrix(sentence_edge_embeddings, image_edge_embeddings)

  # Compute the total cost matrix as a weighted sum of the node cost matrix and the edge cost matrix
  total_cost_matrix = gamma * node_cost_matrix + (1 - gamma) * edge_cost_matrix

  # Solve the optimal transport problem using the Sinkhorn algorithm to obtain a soft alignment matrix
  alignment_matrix = sinkhorn_algorithm.solve(total_cost_matrix)

  # Compute the event graph alignment loss as the dot product between the total cost matrix and the alignment matrix
  loss = torch.dot(total_cost_matrix.flatten(), alignment_matrix.flatten())

  # Return the loss
  return loss

# Define the Sinkhorn algorithm to solve the optimal transport problem
def sinkhorn_algorithm(cost_matrix):
  # Initialize a uniform distribution matrix with a small epsilon value
  distribution_matrix = torch.ones_like(cost_matrix) * epsilon

  # Repeat until convergence or maximum iterations
  for i in range(max_iterations):
    # Normalize each row of the distribution matrix by dividing by its sum
    distribution_matrix = distribution_matrix / torch.sum(distribution_matrix, dim=1, keepdim=True)

    # Normalize each column of the distribution matrix by dividing by its sum
    distribution_matrix = distribution_matrix / torch.sum(distribution_matrix, dim=0, keepdim=True)

    # Break if the distribution matrix is close to a doubly stochastic matrix
    if is_doubly_stochastic(distribution_matrix):
      break

    # Update the distribution matrix by element-wise multiplying with e^(-lambda * cost_matrix)
    distribution_matrix = distribution_matrix * torch.exp(-lambda * cost_matrix)

  # Return the distribution matrix as a soft alignment matrix
  return distribution_matrix

# Load the event-rich dataset for pretraining
dataset = load_dataset("event-rich")

# Pretrain the model on the dataset
for image, sentence in dataset:
  # Encode the image and the sentence using the CLIP model
  image_embedding = clip_model.encode_image(image)
  sentence_embedding = clip_model.encode_text(sentence)

  # Extract the event structure from the sentence using the OpenIE system
  event_structure = openie_system.extract(sentence)

  # Identify the entities in the sentence and the image using the NER system
  sentence_entities = ner_system.identify(sentence)
  image_entities = ner_system.identify(image)

  # Map the entities to the event arguments using a simple string matching heuristic
  entity_mapping = {}
  for argument in event_structure.arguments:
    for entity in sentence_entities:
      if argument.text == entity.text:
        entity_mapping[argument] = entity
        break
    for entity in image_entities:
      if argument.text == entity.text:
        entity_mapping[argument] = entity
        break

   # Generate a