---
title: 2305.19700v2 GaitGS  Temporal Feature Learning in Granularity and Span Dimension for Gait Recognition
date: 2023-05-20
---

# [GaitGS: Temporal Feature Learning in Granularity and Span Dimension for Gait Recognition](http://arxiv.org/abs/2305.19700v2)

authors: Haijun Xiong, Yunze Deng, Xiaohu Huang, Xinggang Wang, Wenyu Liu, Bin Feng


## What, Why and How

[1]: https://arxiv.org/pdf/2305.19700v2.pdf "GaitGS: Temporal Feature Learning in Granularity and Span ... - arXiv.org"
[2]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2305.19700v2 "[2305.19700v2] GaitGS: Temporal Feature Learning in Granularity and ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel framework named GaitGS, which aggregates temporal features in the granularity dimension and span dimension simultaneously for gait recognition[^1^][1].
- **Why**: The paper aims to improve gait recognition performance by capturing both micro-motion and macro-motion information at different temporal scales[^1^][1].
- **How**: The paper introduces two modules: Multi-Granularity Feature Extractor (MGFE) and Multi-Span Feature Learning (MSFL). MGFE extracts temporal features at the frame level and unit level, while MSFL generates global and local temporal representations using a GaitTransformer and a Cross-Span Temporal Learning (CSTL) operation[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- They propose a novel framework named GaitGS, which aggregates temporal features in the granularity dimension and span dimension simultaneously for gait recognition.
- They design a Multi-Granularity Feature Extractor (MGFE) to capture micro-motion and macro-motion information at different temporal granularities.
- They present a Multi-Span Feature Learning (MSFL) module to generate global and local temporal representations using a GaitTransformer and a Cross-Span Temporal Learning (CSTL) operation.
- They conduct extensive experiments on three popular gait datasets and demonstrate the state-of-the-art performance of their method.

## Method Summary

[1]: https://arxiv.org/pdf/2305.19700v2.pdf "GaitGS: Temporal Feature Learning in Granularity and Span ... - arXiv.org"
[2]: https://arxiv.org/abs/2305.19780 "[2305.19780] A technique to jointly estimate depth and depth ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2305.19700v2 "[2305.19700v2] GaitGS: Temporal Feature Learning in Granularity and ..."

Here is a summary of the method section of the paper:

- The paper proposes a novel framework named GaitGS, which consists of two main components: Multi-Granularity Feature Extractor (MGFE) and Multi-Span Feature Learning (MSFL) module[^1^][1].
- MGFE aims to capture temporal features at different granularities: frame level and unit level. Frame level features are extracted by a 3D convolutional network, while unit level features are obtained by dividing a gait period into several units and applying a Unit Temporal Aggregation (UTA) operation[^1^][1].
- MSFL aims to generate temporal features at different spans: global and local. Global span features are obtained by applying a GaitTransformer, which is a self-attention based network, to the unit level features. Local span features are obtained by applying a Cross-Span Temporal Learning (CSTL) operation, which is a cross-attention based network, to the frame level features[^1^][1].
- The final gait representation is obtained by concatenating the global and local span features and passing them through a fully connected layer[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a gait video sequence of T frames
# Output: a gait representation vector of dimension D

# Define the 3D convolutional network
conv3d = Conv3D(...)

# Define the Unit Temporal Aggregation (UTA) operation
uta = UTA(...)

# Define the GaitTransformer network
gait_transformer = GaitTransformer(...)

# Define the Cross-Span Temporal Learning (CSTL) operation
cstl = CSTL(...)

# Define the fully connected layer
fc = Linear(...)

# Extract frame level features using 3D convolutional network
frame_features = conv3d(gait_video) # shape: (T, H, W)

# Divide gait period into N units and apply UTA operation to each unit
unit_features = []
for i in range(N):
  unit = frame_features[i * T // N : (i + 1) * T // N] # shape: (T // N, H, W)
  unit_feature = uta(unit) # shape: (H, W)
  unit_features.append(unit_feature)
unit_features = torch.stack(unit_features) # shape: (N, H, W)

# Apply GaitTransformer to unit features to obtain global span features
global_span_features = gait_transformer(unit_features) # shape: (N, H, W)

# Apply CSTL operation to frame features and global span features to obtain local span features
local_span_features = cstl(frame_features, global_span_features) # shape: (T, H, W)

# Concatenate global and local span features along the temporal dimension
concat_features = torch.cat([global_span_features, local_span_features], dim=0) # shape: (T + N, H, W)

# Apply fully connected layer to obtain final gait representation
gait_representation = fc(concat_features) # shape: (D,)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a gait video sequence of T frames
# Output: a gait representation vector of dimension D

# Define the 3D convolutional network
conv3d = Conv3D(in_channels=3, out_channels=64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))

# Define the Unit Temporal Aggregation (UTA) operation
uta = UTA(reduction='mean')

# Define the GaitTransformer network
gait_transformer = GaitTransformer(d_model=64, nhead=8, num_encoder_layers=6, dim_feedforward=256, dropout=0.1)

# Define the Cross-Span Temporal Learning (CSTL) operation
cstl = CSTL(d_model=64, nhead=8, num_encoder_layers=6, dim_feedforward=256, dropout=0.1)

# Define the fully connected layer
fc = Linear(in_features=(T + N) * H * W * 64, out_features=D)

# Extract frame level features using 3D convolutional network
frame_features = conv3d(gait_video) # shape: (T, H, W, 64)

# Divide gait period into N units and apply UTA operation to each unit
unit_features = []
for i in range(N):
  unit = frame_features[i * T // N : (i + 1) * T // N] # shape: (T // N, H, W, 64)
  unit_feature = uta(unit) # shape: (H, W, 64)
  unit_features.append(unit_feature)
unit_features = torch.stack(unit_features) # shape: (N, H, W, 64)

# Apply GaitTransformer to unit features to obtain global span features
global_span_features = gait_transformer(unit_features) # shape: (N, H, W, 64)

# Apply CSTL operation to frame features and global span features to obtain local span features
local_span_features = cstl(frame_features, global_span_features) # shape: (T, H, W, 64)

# Concatenate global and local span features along the temporal dimension
concat_features = torch.cat([global_span_features, local_span_features], dim=0) # shape: (T + N, H, W, 64)

# Reshape concat features to a vector
concat_features = concat_features.view(-1) # shape: ((T + N) * H * W * 64)

# Apply fully connected layer to obtain final gait representation
gait_representation = fc(concat_features) # shape: (D,)
```