---
title: 2111.15078v1 SketchEdit  Mask-Free Local Image Manipulation with Partial Sketches
date: 2021-11-16
---

# [SketchEdit: Mask-Free Local Image Manipulation with Partial Sketches](http://arxiv.org/abs/2111.15078v1)

authors: Yu Zeng, Zhe Lin, Vishal M. Patel


## What, Why and How

[1]: https://arxiv.org/abs/2111.15078 "[2111.15078] SketchEdit: Mask-Free Local Image Manipulation with ..."
[2]: https://arxiv.org/pdf/2211.15078v1.pdf "Ontheuseofhybridcoarse-levelmodelsin multilevelminimizationmethods"
[3]: http://export.arxiv.org/abs/2011.15078v1 "[2011.15078v1] Detecting Entanglement can be More Effective with ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new paradigm of sketch-based image manipulation, called SketchEdit, which only requires sketch inputs from users and utilizes the entire original image. SketchEdit can automatically predict the target modification region and synthesize the new image content based on the style vector and sketch.
- **Why**: The paper aims to overcome the limitations of existing methods, which typically require users to draw an extra mask indicating the region to modify in addition to sketches. The masked regions are regarded as holes and filled by an inpainting model conditioned on the sketch. This formulation complicates user interaction and discards useful information in masked regions.
- **How**: The paper introduces a novel model architecture that consists of three modules: a region predictor, a style encoder, and a generator. The region predictor takes an image and sketch as inputs and outputs a soft mask indicating the target modification region. The style encoder encodes the masked image region into a structure agnostic style vector. The generator synthesizes the new image content based on the style vector and sketch. The manipulated image is finally produced by blending the generator output into the modification region of the original image. The model can be trained in a self-supervised fashion by learning the reconstruction of an image region from the style vector and sketch.

## Main Contributions

The paper claims to make the following contributions:

- It investigates a new paradigm of sketch-based image manipulation: mask-free local image manipulation, which only requires sketch inputs from users and utilizes the entire original image.
- It introduces a novel model architecture that consists of three modules: a region predictor, a style encoder, and a generator. The region predictor can automatically predict the target modification region from the sketch. The style encoder can extract a structure agnostic style vector from the masked image region. The generator can synthesize the new image content based on the style vector and sketch.
- It proposes a self-supervised training scheme that leverages the reconstruction loss and the adversarial loss to train the model without paired data.
- It demonstrates the effectiveness and superiority of the proposed method over previous approaches on various image manipulation tasks, such as object removal, object replacement, object deformation, and object colorization. It also shows that the proposed method offers simpler and more intuitive user workflows for sketch-based image manipulation.

## Method Summary

The method section of the paper describes the proposed model architecture and the training scheme in detail. The model consists of three modules: a region predictor R, a style encoder E, and a generator G. The region predictor R takes an image I and a sketch S as inputs and outputs a soft mask M indicating the target modification region. The style encoder E takes the masked image region I * M as input and outputs a structure agnostic style vector z. The generator G takes the style vector z and the sketch S as inputs and outputs a synthesized image content C. The manipulated image J is obtained by blending C into the original image I using the mask M.

The paper also proposes a self-supervised training scheme that does not require paired data. The training data consists of images I and sketches S, where S can be either partial or complete sketches. The paper defines two types of losses: the reconstruction loss L_rec and the adversarial loss L_adv. The reconstruction loss L_rec measures the pixel-wise difference between the manipulated image J and the original image I in the masked region M. The adversarial loss L_adv measures the realism of the synthesized image content C using a discriminator D that tries to distinguish C from real image patches. The paper also introduces a gradient penalty term to enforce the Lipschitz constraint on D. The total loss L is a weighted combination of L_rec and L_adv. The paper optimizes L using Adam optimizer with a learning rate of 0.0002 and a batch size of 16.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the model architecture
R = RegionPredictor() # a U-Net with skip connections
E = StyleEncoder() # a ResNet with global average pooling
G = Generator() # a U-Net with skip connections and SPADE blocks
D = Discriminator() # a PatchGAN with spectral normalization

# Define the loss functions
L_rec = L1Loss() # pixel-wise L1 loss
L_adv = HingeLoss() # hinge loss for adversarial training
L_gp = GradientPenalty() # gradient penalty for Lipschitz constraint

# Define the hyperparameters
lambda_rec = 10 # weight for reconstruction loss
lambda_adv = 1 # weight for adversarial loss
lambda_gp = 10 # weight for gradient penalty
lr = 0.0002 # learning rate
beta1 = 0.5 # Adam parameter
beta2 = 0.999 # Adam parameter
bs = 16 # batch size

# Initialize the optimizers
opt_G = Adam(G.parameters(), lr, (beta1, beta2)) # optimizer for generator
opt_D = Adam(D.parameters(), lr, (beta1, beta2)) # optimizer for discriminator

# Train the model
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the image and sketch from the batch
    I, S = batch["image"], batch["sketch"]

    # Predict the mask from the image and sketch
    M = R(I, S)

    # Encode the masked image region into a style vector
    z = E(I * M)

    # Generate the new image content from the style vector and sketch
    C = G(z, S)

    # Blend the generated content into the original image using the mask
    J = I * (1 - M) + C * M

    # Update the discriminator
    opt_D.zero_grad()
    D_real = D(I * M) # discriminator output for real image patches
    D_fake = D(C.detach()) # discriminator output for fake image patches
    L_D_real = L_adv(D_real, True) # adversarial loss for real image patches
    L_D_fake = L_adv(D_fake, False) # adversarial loss for fake image patches
    L_D_gp = L_gp(D, I * M, C) # gradient penalty for discriminator
    L_D = L_D_real + L_D_fake + lambda_gp * L_D_gp # total discriminator loss
    L_D.backward()
    opt_D.step()

    # Update the generator
    opt_G.zero_grad()
    D_fake = D(C) # discriminator output for fake image patches
    L_G_rec = L_rec(J, I, M) # reconstruction loss for manipulated image
    L_G_adv = L_adv(D_fake, True) # adversarial loss for fake image patches
    L_G = lambda_rec * L_G_rec + lambda_adv * L_G_adv # total generator loss
    L_G.backward()
    opt_G.step()

  # Save the model and sample some results at the end of each epoch

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
import os

# Define the model architecture

# Define a convolutional block with spectral normalization and leaky ReLU activation
def ConvBlock(in_channels, out_channels, kernel_size, stride, padding):
  return nn.Sequential(
    nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)),
    nn.LeakyReLU(0.2)
  )

# Define a deconvolutional block with spectral normalization and leaky ReLU activation
def DeconvBlock(in_channels, out_channels, kernel_size, stride, padding):
  return nn.Sequential(
    nn.utils.spectral_norm(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)),
    nn.LeakyReLU(0.2)
  )

# Define a SPADE block that modulates the feature maps using the style vector and sketch
class SPADEBlock(nn.Module):
  def __init__(self, in_channels, out_channels):
    super().__init__()
    self.norm = nn.InstanceNorm2d(in_channels) # instance normalization layer
    self.conv1 = ConvBlock(in_channels + 1 + 256, 128, 3, 1, 1) # convolutional layer that takes the normalized feature maps, sketch and style vector as inputs
    self.conv2 = ConvBlock(128, out_channels * 2, 3, 1, 1) # convolutional layer that outputs two sets of feature maps for scaling and biasing

  def forward(self, x, s, z):
    # x: feature maps of shape [bs, in_channels, h, w]
    # s: sketch of shape [bs, 1, h', w']
    # z: style vector of shape [bs, 256]
    x_norm = self.norm(x) # normalize the feature maps
    s = F.interpolate(s, size=x.shape[2:]) # resize the sketch to match the feature maps size
    z = z.view(z.shape[0], z.shape[1], 1 ,1).expand(-1,-1,x.shape[2],x.shape[3]) # reshape and repeat the style vector to match the feature maps size
    x_s_z = torch.cat([x_norm,s,z], dim=1) # concatenate the normalized feature maps, sketch and style vector along the channel dimension
    x_s_z = self.conv1(x_s_z) # apply the first convolutional layer
    gamma_beta = self.conv2(x_s_z) # apply the second convolutional layer
    gamma = gamma_beta[:, :out_channels] # get the first half of the output feature maps for scaling
    beta = gamma_beta[:, out_channels:] # get the second half of the output feature maps for biasing
    x_out = gamma * x_norm + beta # modulate the normalized feature maps using gamma and beta
    return x_out

# Define a region predictor that takes an image and sketch as inputs and outputs a soft mask indicating the target modification region
class RegionPredictor(nn.Module):
  def __init__(self):
    super().__init__()
    self.down1 = ConvBlock(4 ,64 ,4 ,2 ,1) # downsample by a factor of 2
    self.down2 = ConvBlock(64 ,128 ,4 ,2 ,1) # downsample by a factor of 2
    self.down3 = ConvBlock(128 ,256 ,4 ,2 ,1) # downsample by a factor of 2
    self.down4 = ConvBlock(256 ,512 ,4 ,2 ,1) # downsample by a factor of 2
    self.up4 = DeconvBlock(512 ,256 ,4 ,2 ,1) # upsample by a factor of 2
    self.up3 = DeconvBlock(512 ,128 ,4 ,2 ,1) # upsample by a factor of 2
    self.up2 = DeconvBlock(256 ,64 ,4 ,2 ,1) # upsample by a factor of 2
    self.up1 = DeconvBlock(128 ,32 ,4 ,2 ,1) # upsample by a factor of 2
    self.final = nn.Conv2d(32 ,1 ,3 ,1 ,1) # output a single-channel mask

  def forward(self,I,S):
    # I: image of shape [bs, 3, h, w]
    # S: sketch of shape [bs, 1, h, w]
    x = torch.cat([I,S], dim=1) # concatenate the image and sketch along the channel dimension
    x1 = self.down1(x) # apply the first downsampling block
    x2 = self.down2(x1) # apply the second downsampling block
    x3 = self.down3(x2) # apply the third downsampling block
    x4 = self.down4(x3) # apply the fourth downsampling block
    y4 = self.up4(x4) # apply the fourth upsampling block
    y4 = torch.cat([y4,x3], dim=1) # concatenate the output with the third downsampling block output along the channel dimension
    y3 = self.up3(y4) # apply the third upsampling block
    y3 = torch.cat([y3,x2], dim=1) # concatenate the output with the second downsampling block output along the channel dimension
    y2 = self.up2(y3) # apply the second upsampling block
    y2 = torch.cat([y2,x1], dim=1) # concatenate the output with the first downsampling block output along the channel dimension
    y1 = self.up1(y2) # apply the first upsampling block
    M = self.final(y1) # apply the final convolutional layer to get the mask
    M = torch.sigmoid(M) # apply sigmoid activation to get a soft mask in [0,1]
    return M

# Define a style encoder that takes the masked image region as input and outputs a structure agnostic style vector
class StyleEncoder(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = ConvBlock(3 ,64 ,7 ,2 ,3) # downsample by a factor of 2
    self.res1 = ResBlock(64 ,64 ,3 ,1 ,1) # residual block
    self.res2 = ResBlock(64 ,128 ,3 ,2 ,1) # residual block with downsampling by a factor of 2
    self.res3 = ResBlock(128 ,256 ,3 ,2 ,1) # residual block with downsampling by a factor of 2
    self.res4 = ResBlock(256 ,512 ,3 ,2 ,1) # residual block with downsampling by a factor of 2
    self.gap = nn.AdaptiveAvgPool2d((1,1)) # global average pooling layer
    self.fc = nn.Linear(512, 256) # fully connected layer

  def forward(self,I_M):
    # I_M: masked image region of shape [bs, 3, h, w]
    x = self.conv1(I_M) # apply the first convolutional layer
    x = self.res1(x) # apply the first residual block
    x = self.res2(x) # apply the second residual block
    x = self.res3(x) # apply the third residual block
    x = self.res4(x) # apply the fourth residual block
    x = self.gap(x) # apply the global average pooling layer
    x = x.view(x.shape[0], -1) # flatten the feature maps
    z = self.fc(x) # apply the fully connected layer to get the style vector
    return z

# Define a generator that takes the style vector and sketch as inputs and outputs a synthesized image content
class Generator(nn.Module):
  def __init__(self):
    super().__init__()
    self.down1 = ConvBlock(257 ,64 ,7 ,2 ,3) # downsample by a factor of 2
    self.down2 = ConvBlock(64 ,128 ,4 ,2 ,1) # downsample by a factor of 2
    self.down3 = ConvBlock(128 ,256 ,4 ,2 ,1) # downsample by a factor of 2
    self.down4 = ConvBlock(256 ,512 ,4 ,2 ,1) # downsample by a factor of 2
    self.spade4_0 = SPADEBlock(512, 512)
    self.spade4_1 = SPADEBlock(512, 512)
    self.spade4_2 = SPADEBlock(512, 512)
    self.spade4_3 = SPADEBlock(512, 512)
    self.up4_0 = DeconvBlock(512, 256, 4, 2, 1)
    self.up4_1 = DeconvBlock(512, 256, 4, 2, 1)
    self.up4_2 = DeconvBlock(512, 256, 4, 2, 1)
    self.up4