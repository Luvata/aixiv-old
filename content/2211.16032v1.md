---
title: 2211.16032v1 Dimensionality-Varying Diffusion Process
date: 2022-11-17
---

# [Dimensionality-Varying Diffusion Process](http://arxiv.org/abs/2211.16032v1)

authors: Han Zhang, Ruili Feng, Zhantao Yang, Lianghua Huang, Yu Liu, Yifei Zhang, Yujun Shen, Deli Zhao, Jingren Zhou, Fan Cheng


## What, Why and How

[1]: https://arxiv.org/abs/2211.16032 "[2211.16032] Dimensionality-Varying Diffusion Process - arXiv.org"
[2]: https://arxiv.org/pdf/2211.16032v1.pdf "Dimensionality-Varying Diffusion Process - arXiv.org"
[3]: http://export.arxiv.org/abs/2211.16032 "[2211.16032] Dimensionality-Varying Diffusion Process"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a **Dimensionality-Varying Diffusion Process (DVDP)**, which is a generalization of the diffusion models for image synthesis that allows varying the dimension of the signal at each step of the diffusion process.
- **Why**: The paper argues that conventional diffusion models, which require the signal to have the same dimension throughout the diffusion process, are inefficient and unnecessary, especially in the early generation phase where the signal has high spatial redundancy and can be represented by a lower-dimensional noise without losing much information.
- **How**: The paper introduces a signal decomposition method that splits an image into multiple orthogonal components and controls the attenuation of each component when adding noise to the image. This way, the paper can vary the dimension of the signal by discarding those inconsequential components along with the noise strength increasing. The paper also derives a reverse process that can reconstruct a high-dimensional image from a low-dimensional noise by using a conditional diffusion model. The paper demonstrates that DVDP can reduce the computational cost and improve the synthesis performance compared to baseline methods on various datasets and resolutions.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel diffusion process that can vary the dimension of the signal at each step, which is more flexible and efficient than conventional diffusion models.
- It introduces a signal decomposition method that can split an image into multiple orthogonal components and control their attenuation when adding noise to the image, which enables varying the dimension of the signal without losing much information.
- It derives a reverse process that can reconstruct a high-dimensional image from a low-dimensional noise by using a conditional diffusion model, which facilitates high-resolution image synthesis and improves the quality of the generated samples.
- It conducts extensive experiments on a range of datasets and resolutions, and shows that its approach can substantially reduce the computational cost and achieve on-par or even better synthesis performance compared to baseline methods. It also reports a significant improvement in FID of diffusion model trained on FFHQ at 1024x1024 resolution from 52.40 to 10.46.

## Method Summary

The method section of the paper consists of three parts:

- The first part reviews the background of diffusion models and introduces the notation and formulation of the forward and reverse diffusion processes.
- The second part presents the main idea of dimensionality-varying diffusion process (DVDP), which is to decompose an image into multiple orthogonal components and control their attenuation when adding noise to the image. It also derives the reverse process that can reconstruct a high-dimensional image from a low-dimensional noise by using a conditional diffusion model.
- The third part discusses some implementation details and tricks for training and sampling DVDP, such as using a multi-scale architecture, applying data augmentation, and using a warm-up strategy. It also provides some analysis and comparison of DVDP with conventional diffusion models.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Forward diffusion process
def forward_diffusion(x):
  # x is a high-dimensional image
  # Decompose x into multiple orthogonal components
  x_components = decompose(x)
  # Initialize the noise strength and the dimensionality
  beta = 0
  d = D # D is the original dimension of x
  for t in range(T): # T is the number of diffusion steps
    # Increase the noise strength
    beta = beta + epsilon_t # epsilon_t is a small positive constant
    # Decrease the dimensionality by discarding some components
    d = d - delta_t # delta_t is a small positive integer
    # Attenuate each component by a factor of (1-beta)
    x_components = x_components * (1-beta)
    # Add Gaussian noise to each component
    x_components = x_components + N(0, beta)
    # Keep only the first d components
    x_components = x_components[:d]
  # Return the final low-dimensional noise
  return x_components

# Reverse diffusion process
def reverse_diffusion(z):
  # z is a low-dimensional noise
  # Initialize the noise strength and the dimensionality
  beta = B # B is the final noise strength after T steps
  d = D - Delta # Delta is the total dimension reduction after T steps
  for t in reversed(range(T)): # Reverse the diffusion steps
    # Increase the dimensionality by adding some components
    d = d + delta_t 
    # Sample some new components from a conditional diffusion model
    new_components = sample_from_model(z, t)
    # Concatenate the new components with the existing ones
    z = concatenate(z, new_components)
    # Remove Gaussian noise from each component
    z = z - N(0, beta)
    # Amplify each component by a factor of (1/(1-beta))
    z = z / (1-beta)
    # Decrease the noise strength
    beta = beta - epsilon_t 
  # Return the final high-dimensional image
  return z

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import some libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

# Define some hyperparameters
T = 1000 # Number of diffusion steps
D = 1024 # Original dimension of the image
Delta = 512 # Total dimension reduction after T steps
epsilon = 1e-3 # Small positive constant for noise strength increment
delta = 1 # Small positive integer for dimension reduction increment
B = 0.999 # Final noise strength after T steps
batch_size = 64 # Batch size for training and sampling
lr = 1e-4 # Learning rate for the conditional diffusion model
epochs = 100 # Number of epochs for training the conditional diffusion model

# Define a function to decompose an image into multiple orthogonal components
def decompose(x):
  # x is a batch of high-dimensional images of shape (batch_size, D, D, 3)
  # Convert x to grayscale by averaging the RGB channels
  x_gray = torch.mean(x, dim=-1, keepdim=True) # Shape: (batch_size, D, D, 1)
  # Apply a discrete cosine transform (DCT) to x_gray along the spatial dimensions
  x_dct = torch.fft.dctn(x_gray, dim=(1,2)) # Shape: (batch_size, D, D, 1)
  # Reshape x_dct to a vector of length D*D
  x_dct = torch.reshape(x_dct, (batch_size, -1)) # Shape: (batch_size, D*D)
  # Return the DCT coefficients as the orthogonal components of x
  return x_dct

# Define a function to reconstruct an image from multiple orthogonal components
def reconstruct(x_components):
  # x_components is a batch of orthogonal components of shape (batch_size, d)
  # d is the current dimensionality of the signal
  # Pad x_components with zeros to make it have length D*D
  x_components = F.pad(x_components, (0, D*D-d)) # Shape: (batch_size, D*D)
  # Reshape x_components to a matrix of shape (D, D)
  x_components = torch.reshape(x_components, (batch_size, D, D)) # Shape: (batch_size, D, D)
  # Apply an inverse discrete cosine transform (IDCT) to x_components along the spatial dimensions
  x_idct = torch.fft.idctn(x_components, dim=(1,2)) # Shape: (batch_size, D, D)
  # Expand x_idct to have three channels by repeating the grayscale values
  x_idct = torch.repeat_interleave(x_idct, repeats=3, dim=-1) # Shape: (batch_size, D, D, 3)
  # Return the reconstructed image
  return x_idct

# Define a function to sample some new components from a conditional diffusion model
def sample_from_model(z, t):
  # z is a batch of low-dimensional noises of shape (batch_size, d)
  # t is the current diffusion step index
  # d is the current dimensionality of the signal
  # Use a multi-scale architecture with skip connections and residual blocks as the conditional diffusion model
  model = MultiScaleModel()
  # Feed z and t to the model and get the output distribution parameters
  mu, sigma = model(z,t) # Shape: (batch_size, delta)
  # Sample some new components from a Gaussian distribution with mean mu and standard deviation sigma
  new_components = torch.normal(mu,sigma) # Shape: (batch_size, delta)
  # Return the new components
  return new_components

# Define a class for the multi-scale architecture with skip connections and residual blocks as the conditional diffusion model
class MultiScaleModel(nn.Module):
  
  def __init__(self):
    super(MultiScaleModel,self).__init__()
    # Define some submodules for each scale level
    self.encoder_1 = EncoderBlock() 
    self.encoder_2 = EncoderBlock()
    self.encoder_3 = EncoderBlock()
    self.decoder_3 = DecoderBlock()
    self.decoder_2 = DecoderBlock()
    self.decoder_1 = DecoderBlock()
    self.head = nn.Linear(Delta,delta) 

  
  def forward(self,z,t):
    # z is a batch of low-dimensional noises of shape (batch_size,d)
    # t is the current diffusion step index
    
    # Reshape z to a matrix of shape (d/4,d/4)
    z = torch.reshape(z, (batch_size, d//4, d//4)) # Shape: (batch_size, d/4, d/4)
    # Embed t to a vector of length 64
    t = nn.Embedding(T,64)(t) # Shape: (batch_size, 64)
    # Encode z at the first scale level
    z_1, skip_1 = self.encoder_1(z,t) # Shape: (batch_size, d/8, d/8), (batch_size, d/4, d/4)
    # Encode z at the second scale level
    z_2, skip_2 = self.encoder_2(z_1,t) # Shape: (batch_size, d/16, d/16), (batch_size, d/8, d/8)
    # Encode z at the third scale level
    z_3 = self.encoder_3(z_2,t) # Shape: (batch_size, d/32, d/32)
    # Decode z at the third scale level
    z_3 = self.decoder_3(z_3,t) # Shape: (batch_size, d/16, d/16)
    # Decode z at the second scale level with skip connection
    z_2 = self.decoder_2(z_2 + z_3,t,skip_2) # Shape: (batch_size, d/8, d/8)
    # Decode z at the first scale level with skip connection
    z_1 = self.decoder_1(z_1 + z_2,t,skip_1) # Shape: (batch_size, d/4, d/4)
    # Reshape z to a vector of length D-Delta
    z = torch.reshape(z_1,(batch_size,-1)) # Shape: (batch_size,D-Delta)
    # Apply a linear layer to get the output distribution parameters for delta new components
    mu = self.head(z) # Shape: (batch_size,delta)
    sigma = F.softplus(self.head(z)) + 1e-5 # Shape: (batch_size,delta)
    # Return the output distribution parameters
    return mu,sigma

# Define a class for the encoder block with residual blocks and downsampling
class EncoderBlock(nn.Module):

  def __init__(self):
    super(EncoderBlock,self).__init__()
    # Define some submodules for the encoder block
    self.resblock_1 = ResBlock()
    self.resblock_2 = ResBlock()
    self.downsample = nn.AvgPool2d(2)

  def forward(self,x,t):
    # x is a batch of signals of shape (batch_size,h,w)
    # t is a batch of diffusion step indices of shape (batch_size,)
    # h and w are the height and width of the signal
    # Apply the first residual block with t as a condition
    x = self.resblock_1(x,t) # Shape: (batch_size,h,w)
    # Apply the second residual block with t as a condition
    x = self.resblock_2(x,t) # Shape: (batch_size,h,w)
    # Save a copy of x as a skip connection
    skip = x 
    # Downsample x by a factor of 2
    x = self.downsample(x) # Shape: (batch_size,h/2,w/2)
    # Return x and skip
    return x,skip

# Define a class for the decoder block with residual blocks and upsampling
class DecoderBlock(nn.Module):

  def __init__(self):
    super(DecoderBlock,self).__init__()
    # Define some submodules for the decoder block
    self.resblock_1 = ResBlock()
    self.resblock_2 = ResBlock()
    self.upsample = nn.Upsample(scale_factor=2)

  def forward(self,x,t,skip):
    # x is a batch of signals of shape (batch_size,h,w)
    # t is a batch of diffusion step indices of shape (batch_size,)
    # skip is a batch of signals from the encoder block of shape (batch_size,2h,2w)
    # h and w are the height and width of the signal
    # Upsample x by a factor of 2
    x = self.upsample(x) # Shape: (batch_size,2h,2w)
    # Add skip to x element-wise
    x = x + skip