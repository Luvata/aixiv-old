---
title: 2103.15812v4 LatentKeypointGAN  Controlling GANs via Latent Keypoints
date: 2021-03-16
---

# [LatentKeypointGAN: Controlling GANs via Latent Keypoints](http://arxiv.org/abs/2103.15812v4)

authors: Xingzhe He, Bastian Wandt, Helge Rhodin


## What, Why and How

[1]: https://arxiv.org/abs/2103.15812 "[2103.15812] LatentKeypointGAN: Controlling GANs via Latent Keypoints"
[2]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2103.15812v4 "[2103.15812v4] LatentKeypointGAN: Controlling GANs via Latent Keypoints"
[3]: https://arxiv.org/pdf/2103.15812 "arXiv.org"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper introduces **LatentKeypointGAN**, a two-stage generative adversarial network (GAN) that can generate images with internal conditioning on a set of space keypoints[^1^][1].
- **Why**: The paper aims to address the challenge of controlling the image content of GANs in an interpretable and disentangled way without domain knowledge and supervision signals[^1^][1].
- **How**: The paper proposes a novel network architecture and training scheme that consists of two stages: a keypoint generator that produces a set of keypoints and their associated appearance embeddings from a latent code, and an image generator that synthesizes an image from the keypoints and embeddings[^1^][1]. The paper also introduces a keypoint discriminator and an appearance discriminator to enforce the disentanglement of spatial and appearance factors[^1^][1]. The paper evaluates the proposed method on various datasets and tasks, such as image generation, keypoint detection, image editing, and image interpolation[^1^][1].



## Main Contributions

According to the paper, the main contributions are:

- A novel two-stage GAN framework that generates images with internal conditioning on a set of space keypoints and their appearance embeddings.
- A novel keypoint discriminator and an appearance discriminator that enforce the disentanglement of spatial and appearance factors without supervision signals.
- A new GAN-based method for unsupervised keypoint detection that leverages the generated keypoints and images.
- Extensive experiments and ablation studies that demonstrate the effectiveness and versatility of the proposed method on various datasets and tasks.


## Method Summary

[1]: https://arxiv.org/abs/2103.15812 "[2103.15812] LatentKeypointGAN: Controlling GANs via Latent Keypoints"
[2]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2103.15812v4 "[2103.15812v4] LatentKeypointGAN: Controlling GANs via Latent Keypoints"
[3]: https://arxiv.org/pdf/2103.15812 "arXiv.org"

Here is a summary of the method section of the paper:

- The paper proposes a **two-stage GAN framework** that consists of a **keypoint generator** and an **image generator**[^1^][1].
- The keypoint generator takes a latent code z as input and produces a set of keypoints K and their associated appearance embeddings E[^1^][1]. The keypoints K are represented as 2D coordinates normalized to [-1, 1], and the appearance embeddings E are high-dimensional vectors that encode the style of each keypoint[^1^][1].
- The image generator takes the keypoints K and the appearance embeddings E as input and synthesizes an image X that matches the spatial and appearance factors[^1^][1]. The image generator uses a spatial transformer network to warp a feature map according to the keypoints K, and then applies an adaptive instance normalization layer to modulate the feature map with the appearance embeddings E[^1^][1].
- The paper also introduces two discriminators: a **keypoint discriminator** and an **appearance discriminator**[^1^][1]. The keypoint discriminator takes an image X and a set of keypoints K as input and tries to distinguish between real and fake pairs[^1^][1]. The appearance discriminator takes an image X and an appearance embedding E as input and tries to distinguish between real and fake pairs[^1^][1].
- The paper defines a **total loss function** that combines the classical GAN loss, the cycle-consistency loss, the perceptual loss, and the diversity loss[^1^][1]. The paper trains the model end-to-end on the total loss function using the Adam optimizer[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the keypoint generator G_k
G_k = KeypointGenerator()

# Define the image generator G_x
G_x = ImageGenerator()

# Define the keypoint discriminator D_k
D_k = KeypointDiscriminator()

# Define the appearance discriminator D_e
D_e = AppearanceDiscriminator()

# Define the total loss function L
L = GANLoss + CycleLoss + PerceptualLoss + DiversityLoss

# Define the Adam optimizer
optimizer = Adam(learning_rate, beta1, beta2)

# Train the model end-to-end
for epoch in range(num_epochs):
  for batch in data_loader:
    # Sample a latent code z
    z = sample_z(batch_size, latent_dim)

    # Generate keypoints and embeddings from z
    K, E = G_k(z)

    # Generate an image from keypoints and embeddings
    X = G_x(K, E)

    # Compute the keypoint discriminator output for real and fake pairs
    D_k_real = D_k(X_real, K_real)
    D_k_fake = D_k(X, K)

    # Compute the appearance discriminator output for real and fake pairs
    D_e_real = D_e(X_real, E_real)
    D_e_fake = D_e(X, E)

    # Compute the total loss for generators and discriminators
    L_G = L(G_k, G_x, D_k, D_e, X_real, K_real, E_real, X, K, E)
    L_D = L(D_k, D_e, X_real, K_real, E_real, X, K, E)

    # Update the parameters of generators and discriminators using Adam optimizer
    optimizer.step(L_G)
    optimizer.step(L_D)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
batch_size = 64 # The number of images in a batch
latent_dim = 128 # The dimension of the latent code z
num_keypoints = 10 # The number of keypoints per image
keypoint_dim = 2 # The dimension of each keypoint coordinate
embedding_dim = 64 # The dimension of each appearance embedding
image_size = 64 # The size of the image (assume square image)
num_channels = 3 # The number of channels in the image (RGB)
learning_rate = 0.0002 # The learning rate for Adam optimizer
beta1 = 0.5 # The beta1 parameter for Adam optimizer
beta2 = 0.999 # The beta2 parameter for Adam optimizer
num_epochs = 100 # The number of epochs to train the model

# Define the keypoint generator G_k
class KeypointGenerator(nn.Module):
  def __init__(self):
    super(KeypointGenerator, self).__init__()
    # Define a fully connected layer to map z to a hidden vector h
    self.fc = nn.Linear(latent_dim, latent_dim * 4)
    # Define a batch normalization layer to normalize h
    self.bn = nn.BatchNorm1d(latent_dim * 4)
    # Define a LeakyReLU activation function with negative slope 0.2
    self.lrelu = nn.LeakyReLU(0.2)
    # Define a fully connected layer to map h to keypoints K and embeddings E
    self.fc_k = nn.Linear(latent_dim * 4, num_keypoints * keypoint_dim)
    self.fc_e = nn.Linear(latent_dim * 4, num_keypoints * embedding_dim)

  def forward(self, z):
    # Map z to h using fc layer and apply batch normalization and LeakyReLU activation
    h = self.lrelu(self.bn(self.fc(z)))
    # Map h to keypoints K and embeddings E using fc layers and reshape them accordingly
    K = self.fc_k(h).view(-1, num_keypoints, keypoint_dim)
    E = self.fc_e(h).view(-1, num_keypoints, embedding_dim)
    return K, E

# Define the image generator G_x
class ImageGenerator(nn.Module):
  def __init__(self):
    super(ImageGenerator, self).__init__()
    # Define a convolutional layer to map embeddings E to a feature map F with num_channels channels and image_size spatial size
    self.conv_e = nn.Conv2d(embedding_dim, num_channels, kernel_size=1, stride=1, padding=0)
    # Define a spatial transformer network to warp F according to keypoints K
    self.stn = SpatialTransformerNetwork()
    # Define an adaptive instance normalization layer to modulate F with embeddings E
    self.adain = AdaptiveInstanceNormalization()
    # Define a convolutional layer to map F to an image X with num_channels channels and image_size spatial size
    self.conv_x = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)

  def forward(self, K, E):
    # Map embeddings E to a feature map F using conv_e layer and apply LeakyReLU activation
    F = self.lrelu(self.conv_e(E))
    # Warp F according to keypoints K using stn module and apply LeakyReLU activation
    F = self.lrelu(self.stn(F, K))
    # Modulate F with embeddings E using adain module and apply LeakyReLU activation
    F = self.lrelu(self.adain(F, E))
    # Map F to an image X using conv_x layer and apply tanh activation
    X = torch.tanh(self.conv_x(F))
    return X

# Define the spatial transformer network module
class SpatialTransformerNetwork(nn.Module):
  def __init__(self):
    super(SpatialTransformerNetwork, self).__init__()
  
  def forward(self, F, K):
    # Compute the affine transformation matrix T from keypoints K using a differentiable function (e.g., thin-plate spline interpolation)
    T = compute_affine_matrix(F, K)
    # Apply the affine transformation T to the feature map F using a grid sampler function (e.g., torch.nn.functional.grid_sample)
    F_warped = grid_sample(F, T)
    return F_warped

# Define the adaptive instance normalization module
class AdaptiveInstanceNormalization(nn.Module):
  def __init__(self):
    super(AdaptiveInstanceNormalization, self).__init__()
  
  def forward(self, F, E):
    # Compute the mean and standard deviation of F along the channel dimension
    F_mean = F.mean(dim=1, keepdim=True)
    F_std = F.std(dim=1, keepdim=True)
    # Compute the mean and standard deviation of E along the channel dimension
    E_mean = E.mean(dim=1, keepdim=True)
    E_std = E.std(dim=1, keepdim=True)
    # Normalize F by subtracting F_mean and dividing by F_std
    F_norm = (F - F_mean) / F_std
    # Modulate F_norm by multiplying E_std and adding E_mean
    F_mod = F_norm * E_std + E_mean
    return F_mod

# Define the keypoint discriminator D_k
class KeypointDiscriminator(nn.Module):
  def __init__(self):
    super(KeypointDiscriminator, self).__init__()
    # Define a convolutional layer to map an image X to a feature map H with num_channels channels and image_size spatial size
    self.conv_x = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)
    # Define a convolutional layer to map keypoints K to a feature map G with num_channels channels and image_size spatial size
    self.conv_k = nn.Conv2d(keypoint_dim, num_channels, kernel_size=1, stride=1, padding=0)
    # Define a convolutional layer to map the concatenation of H and G to a feature map J with num_channels channels and image_size spatial size
    self.conv_hg = nn.Conv2d(num_channels * 2, num_channels, kernel_size=3, stride=1, padding=1)
    # Define a fully connected layer to map J to a scalar output S
    self.fc_s = nn.Linear(num_channels * image_size * image_size, 1)

  def forward(self, X, K):
    # Map X to H using conv_x layer and apply LeakyReLU activation
    H = self.lrelu(self.conv_x(X))
    # Map K to G using conv_k layer and apply LeakyReLU activation
    G = self.lrelu(self.conv_k(K))
    # Concatenate H and G along the channel dimension
    HG = torch.cat([H, G], dim=1)
    # Map HG to J using conv_hg layer and apply LeakyReLU activation
    J = self.lrelu(self.conv_hg(HG))
    # Flatten J to a vector
    J_flat = J.view(-1, num_channels * image_size * image_size)
    # Map J_flat to S using fc_s layer and apply sigmoid activation
    S = torch.sigmoid(self.fc_s(J_flat))
    return S

# Define the appearance discriminator D_e
class AppearanceDiscriminator(nn.Module):
  def __init__(self):
    super(AppearanceDiscriminator, self).__init__()
    # Define a convolutional layer to map an image X to a feature map H with num_channels channels and image_size spatial size
    self.conv_x = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)
    # Define a convolutional layer to map embeddings E to a feature map G with num_channels channels and image_size spatial size
    self.conv_e = nn.Conv2d(embedding_dim, num_channels, kernel_size=1, stride=1, padding=0)
    # Define a convolutional layer to map the concatenation of H and G to a feature map J with num_channels channels and image_size spatial size
    self.conv_hg = nn.Conv2d(num_channels * 2, num_channels, kernel_size=3, stride=1, padding=1)
    # Define a fully connected layer to map J to a scalar output S
    self.fc_s = nn.Linear(num_channels * image_size * image_size, 1)

  def forward(self, X, E):
    # Map X to H using conv_x layer and apply LeakyReLU activation
    H = self.lrelu(self.conv_x(X))
    # Map E to G using conv_e layer and apply LeakyReLU activation
    G = self.lrelu(self.conv_e(E))
    # Concatenate H and G along the channel dimension
    HG = torch.cat([H, G], dim=1)
    # Map HG to J using conv_hg layer and apply LeakyReLU activation
    J = self.lrelu(self.conv_hg(HG))
    # Flatten J to a vector
    J_flat = J.view(-1, num_channels * image_size * image_size)
    # Map J_flat to S using fc_s layer and apply sigmoid activation
    S = torch.sigmoid(self.fc_s(J_flat))
    return S

# Define the G