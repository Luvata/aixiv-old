---
title: 2302.05543v1 Adding Conditional Control to Text-to-Image Diffusion Models
date: 2023-02-06
---

# [Adding Conditional Control to Text-to-Image Diffusion Models](http://arxiv.org/abs/2302.05543v1)

authors: Lvmin Zhang, Maneesh Agrawala


## What, Why and How

[1]: https://arxiv.org/abs/2302.05543 "Adding Conditional Control to Text-to-Image Diffusion Models"
[2]: http://export.arxiv.org/abs/2302.05543 "[2302.05543] Adding Conditional Control to Text-to-Image Diffusion Models"
[3]: https://blog.syntha.ai/p/generate-image-from-sketch-with-controlnet "Generate image from sketch with ControlNet - Syntha AI"
[4]: https://arxiv.org/pdf/2302.05543v1.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper presents a neural network structure, **ControlNet**, to control pretrained large diffusion models to support additional input conditions[^1^][1] [^2^][2].
- **Why**: The paper aims to enrich the methods to control large diffusion models and further facilitate related applications, such as text-to-image generation, image editing, and image synthesis[^1^][1] [^2^][2].
- **How**: The paper proposes to train a ControlNet on top of a pretrained diffusion model, such as Stable Diffusion, to learn task-specific conditions in an end-to-end way. The ControlNet can handle various conditional inputs, such as edge maps, segmentation maps, keypoints, etc. The paper also demonstrates that the ControlNet can be trained on a personal device with a small dataset (< 50k), or scale to large amounts of data (millions to billions) with powerful computation clusters[^1^][1] [^2^][2].

## Main Contributions

The paper claims to make the following contributions:

- It introduces a novel neural network structure, ControlNet, to control pretrained large diffusion models to support additional input conditions.
- It shows that the ControlNet can learn task-specific conditions in an end-to-end way, and the learning is robust even when the training dataset is small (< 50k).
- It demonstrates that the ControlNet can be trained on a personal device as fast as fine-tuning a diffusion model, or scale to large amounts of data with powerful computation clusters.
- It reports that the ControlNet can enable conditional inputs like edge maps, segmentation maps, keypoints, etc. for large diffusion models like Stable Diffusion.
- It provides qualitative and quantitative results on various text-to-image generation tasks, such as sketch-to-image, text-to-image, and text+sketch-to-image.

## Method Summary

Here is a summary of the method section of the paper:

- The paper adopts the diffusion model framework, which is a generative model that can produce realistic images from random noise by reversing a diffusion process.
- The paper proposes to train a ControlNet on top of a pretrained diffusion model to learn additional input conditions. The ControlNet is a lightweight neural network that takes the conditional input and the noise level as inputs, and outputs a modulation vector that is used to adjust the parameters of the diffusion model.
- The paper uses Stable Diffusion as the base diffusion model, which is a state-of-the-art diffusion model that can generate high-quality images with fewer steps and faster inference.
- The paper trains the ControlNet using a contrastive loss function, which encourages the ControlNet to produce similar modulation vectors for similar conditional inputs, and different modulation vectors for different conditional inputs.
- The paper applies the ControlNet to various text-to-image generation tasks, such as sketch-to-image, text-to-image, and text+sketch-to-image. The paper uses different types of conditional inputs, such as edge maps, segmentation maps, keypoints, etc. The paper also uses different types of text encoders, such as CLIP and BERT, to encode the text inputs into feature vectors.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion model
diffusion_model = StableDiffusion(pretrained=True)

# Define the ControlNet
control_net = ControlNet(input_dim, output_dim)

# Define the text encoder
text_encoder = CLIP(pretrained=True) # or BERT(pretrained=True)

# Define the contrastive loss function
contrastive_loss = NTXentLoss(temperature)

# Train the ControlNet
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the conditional inputs and the target images
    conditional_inputs, target_images = batch
    
    # Encode the text inputs into feature vectors if any
    if text_inputs in conditional_inputs:
      text_features = text_encoder(text_inputs)
      conditional_inputs.append(text_features)
    
    # Get the noise levels for each diffusion step
    noise_levels = diffusion_model.get_noise_levels()
    
    # Initialize the loss
    loss = 0
    
    # Loop over the diffusion steps
    for t in range(num_steps):
      # Get the current noise level
      noise_level = noise_levels[t]
      
      # Get the modulation vector from the ControlNet
      modulation_vector = control_net(conditional_inputs, noise_level)
      
      # Adjust the parameters of the diffusion model using the modulation vector
      diffusion_model.adjust_parameters(modulation_vector)
      
      # Get the predicted image from the diffusion model
      predicted_image = diffusion_model(target_image, noise_level)
      
      # Compute the contrastive loss between the predicted image and the target image
      loss += contrastive_loss(predicted_image, target_image)
    
    # Update the parameters of the ControlNet using backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Generate images using the ControlNet and the diffusion model
def generate_image(conditional_input):
  # Encode the text input into feature vector if any
  if text_input in conditional_input:
    text_feature = text_encoder(text_input)
    conditional_input.append(text_feature)
  
  # Get the noise levels for each diffusion step
  noise_levels = diffusion_model.get_noise_levels()
  
  # Initialize the image with random noise
  image = torch.randn(3, 256, 256)
  
  # Loop over the diffusion steps in reverse order
  for t in range(num_steps-1, -1, -1):
    # Get the current noise level
    noise_level = noise_levels[t]
    
    # Get the modulation vector from the ControlNet
    modulation_vector = control_net(conditional_input, noise_level)
    
    # Adjust the parameters of the diffusion model using the modulation vector
    diffusion_model.adjust_parameters(modulation_vector)
    
    # Get the denoised image from the diffusion model
    image = diffusion_model.denoise(image, noise_level)
  
  # Return the generated image
  return image

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt

# Define the hyperparameters
batch_size = 32 # the number of images in a batch
num_epochs = 100 # the number of epochs to train the ControlNet
num_steps = 1000 # the number of diffusion steps
input_dim = 512 # the dimension of the conditional input
output_dim = 256 # the dimension of the modulation vector
temperature = 0.1 # the temperature for the contrastive loss function
learning_rate = 0.001 # the learning rate for the optimizer

# Define the diffusion model
class StableDiffusion(nn.Module):
  def __init__(self, pretrained=True):
    super(StableDiffusion, self).__init__()
    # Load the pretrained model from https://github.com/openai/DALL-E/tree/master/dall_e/stable_diffusion.py
    self.model = load_pretrained_stable_diffusion_model()
  
  def forward(self, x, noise_level):
    # Apply the diffusion model to generate a noisy image from a clean image
    return self.model(x, noise_level)
  
  def denoise(self, x, noise_level):
    # Apply the diffusion model to generate a clean image from a noisy image
    return self.model.denoise(x, noise_level)
  
  def get_noise_levels(self):
    # Return the noise levels for each diffusion step
    return self.model.get_noise_levels()
  
  def adjust_parameters(self, modulation_vector):
    # Adjust the parameters of the diffusion model using the modulation vector
    self.model.adjust_parameters(modulation_vector)

# Define the ControlNet
class ControlNet(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(ControlNet, self).__init__()
    # Define a linear layer to project the conditional input into a hidden vector
    self.linear1 = nn.Linear(input_dim, output_dim)
    # Define a ReLU activation function
    self.relu = nn.ReLU()
    # Define a linear layer to project the hidden vector into a modulation vector
    self.linear2 = nn.Linear(output_dim, output_dim)
  
  def forward(self, conditional_input, noise_level):
    # Concatenate the conditional input and the noise level into a single vector
    input_vector = torch.cat([conditional_input, noise_level], dim=-1)
    # Apply the linear layer and the ReLU activation function to get a hidden vector
    hidden_vector = self.relu(self.linear1(input_vector))
    # Apply the linear layer to get a modulation vector
    modulation_vector = self.linear2(hidden_vector)
    # Return the modulation vector
    return modulation_vector

# Define the text encoder
class CLIP(nn.Module):
  def __init__(self, pretrained=True):
    super(CLIP, self).__init__()
    # Load the pretrained model from https://github.com/openai/CLIP/blob/main/clip/model.py
    self.model = load_pretrained_clip_model()
  
  def forward(self, text_input):
    # Encode the text input into a feature vector using CLIP
    return self.model.encode_text(text_input)

# Define the contrastive loss function
class NTXentLoss(nn.Module):
  def __init__(self, temperature):
    super(NTXentLoss, self).__init__()
    # Define the temperature parameter
    self.temperature = temperature
  
  def forward(self, predicted_image, target_image):
    # Normalize the predicted image and the target image to unit vectors
    predicted_image = F.normalize(predicted_image, dim=-1)
    target_image = F.normalize(target_image, dim=-1)
    
    # Compute the cosine similarity matrix between predicted images and target images in a batch
    similarity_matrix = torch.matmul(predicted_image, target_image.t())
    
    # Compute the positive similarity by taking the diagonal elements of the similarity matrix
    positive_similarity = torch.diag(similarity_matrix)
    
    # Compute the negative similarity by masking out the diagonal elements of the similarity matrix and taking the maximum along each row
    mask = torch.eye(batch_size).bool()
    negative_similarity = torch.max(similarity_matrix.masked_fill(mask, -np.inf), dim=-1)[0]
    
    # Compute the contrastive loss using log-sum-exp trick for numerical stability
    loss = -torch.log(torch.exp(positive_similarity / self.temperature) / (torch.exp(positive_similarity / self.temperature) + torch.exp(negative_similarity / self.temperature)))
    
    # Return the mean loss over the batch
    return torch.mean(loss)

# Load the dataset
# Assume the dataset contains pairs of conditional inputs and target images
# For example, conditional inputs can be edge maps, segmentation maps, keypoints, etc.
# Target images can be natural images corresponding to the conditional inputs
# The dataset can be downloaded from https://github.com/openai/DALL-E/tree/master/data
dataset = load_dataset()

# Define the data loader
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Instantiate the diffusion model
diffusion_model = StableDiffusion(pretrained=True)

# Instantiate the ControlNet
control_net = ControlNet(input_dim, output_dim)

# Instantiate the text encoder
text_encoder = CLIP(pretrained=True)

# Instantiate the contrastive loss function
contrastive_loss = NTXentLoss(temperature)

# Instantiate the optimizer
optimizer = optim.Adam(control_net.parameters(), lr=learning_rate)

# Train the ControlNet
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the conditional inputs and the target images
    conditional_inputs, target_images = batch
    
    # Encode the text inputs into feature vectors if any
    if text_inputs in conditional_inputs:
      text_features = text_encoder(text_inputs)
      conditional_inputs.append(text_features)
    
    # Get the noise levels for each diffusion step
    noise_levels = diffusion_model.get_noise_levels()
    
    # Initialize the loss
    loss = 0
    
    # Loop over the diffusion steps
    for t in range(num_steps):
      # Get the current noise level
      noise_level = noise_levels[t]
      
      # Get the modulation vector from the ControlNet
      modulation_vector = control_net(conditional_inputs, noise_level)
      
      # Adjust the parameters of the diffusion model using the modulation vector
      diffusion_model.adjust_parameters(modulation_vector)
      
      # Get the predicted image from the diffusion model
      predicted_image = diffusion_model(target_image, noise_level)
      
      # Compute the contrastive loss between the predicted image and the target image
      loss += contrastive_loss(predicted_image, target_image)
    
    # Update the parameters of the ControlNet using backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  
  # Print the epoch and the loss
  print(f"Epoch {epoch}, Loss {loss.item()}")

# Generate images using the ControlNet and the diffusion model
def generate_image(conditional_input):
  # Encode the text input into feature vector if any
  if text_input in conditional_input:
    text_feature = text_encoder(text_input)
    conditional_input.append(text_feature)
  
  # Get the noise levels for each diffusion step
  noise_levels = diffusion_model.get_noise_levels()
  
  # Initialize the image with random noise
  image = torch.randn(3, 256, 256)
  
  # Loop over the diffusion steps in reverse order
  for t in range(num_steps-1, -1, -1):
    # Get the current noise level
    noise_level = noise_levels[t]
    
    # Get the modulation vector from the ControlNet
    modulation_vector = control_net(conditional_input, noise_level)
    
    # Adjust the parameters of the diffusion model using the modulation vector
    diffusion_model.adjust_parameters(modulation_vector)
    
    # Get the denoised image from the diffusion model
    image = diffusion_model.denoise(image, noise_level)
  
  # Return the generated image
  return image

```