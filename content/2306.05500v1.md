---
title: 2306.05500v1 Word-Level Explanations for Analyzing Bias in Text-to-Image Models
date: 2023-06-06
---

# [Word-Level Explanations for Analyzing Bias in Text-to-Image Models](http://arxiv.org/abs/2306.05500v1)

authors: Alexander Lin, Lucas Monteiro Paes, Sree Harsha Tanneru, Suraj Srinivas, Himabindu Lakkaraju


## What, Why and How

[1]: https://arxiv.org/pdf/2306.05500.pdf "Word-Level Explanations for Analyzing Bias in Text-to-Image Models"
[2]: https://arxiv.org/abs/2306.05500 "[2306.05500] Word-Level Explanations for Analyzing Bias in Text-to ..."
[3]: http://export.arxiv.org/abs/2306.05500 "[2306.05500] Word-Level Explanations for Analyzing Bias in Text-to ..."

Here is a summary of the paper:

- **What**: The paper introduces a method for computing scores for each word in the input prompt of a text-to-image model; these scores represent its influence on biases in the model's output. The paper also performs experiments on Stable Diffusion to demonstrate that the method identifies the replication of societal stereotypes in generated images.
- **Why**: The paper aims to address the problem of underrepresentation of minorities based on race and sex in the images generated by text-to-image models. The paper argues that identifying which word in the input prompt is responsible for bias in generated images can help practitioners to modify prompts to achieve better output representation.
- **How**: The paper follows the principle of *explaining by removing*, leveraging masked language models to calculate the influence scores. The paper defines two metrics: *bias score* and *influence score*. The bias score measures how much a generated image deviates from a desired representation, such as gender or race. The influence score measures how much a word in the prompt affects the bias score when it is masked out. The paper uses these scores to rank the words in the prompt according to their influence on bias. The paper also compares the proposed method with two baselines: random masking and gradient-based attribution. The paper shows that the proposed method outperforms the baselines in identifying words that cause bias in generated images.


## Main Contributions

The paper claims to make the following contributions:

- It proposes a novel method for computing word-level explanations for analyzing bias in text-to-image models, based on the principle of explaining by removing.
- It defines two metrics, bias score and influence score, to quantify the bias and influence of each word in the input prompt on the generated image.
- It evaluates the proposed method on Stable Diffusion, a state-of-the-art text-to-image model, and shows that it can identify words that cause underrepresentation of minorities based on race and sex in generated images.
- It compares the proposed method with two baselines, random masking and gradient-based attribution, and shows that it outperforms them in identifying words that cause bias in generated images.


## Method Summary

The method section of the paper consists of three subsections: problem formulation, explaining by removing, and evaluation metrics. Here is a summary of each subsection:

- Problem formulation: The paper defines the problem of word-level explanations for analyzing bias in text-to-image models. The paper assumes that there is a text-to-image model f that takes a prompt x as input and generates an image y as output. The paper also assumes that there is a desired representation r for the image, such as gender or race. The paper aims to find which word in x has the most influence on the deviation of y from r.
- Explaining by removing: The paper proposes a method for computing word-level explanations based on the principle of explaining by removing. The paper uses a masked language model g to replace each word in x with a special token [MASK]. The paper then feeds the masked prompt x' to the text-to-image model f and obtains a new image y'. The paper compares y and y' to measure the influence of the masked word on the bias in the generated image. The paper repeats this process for each word in x and ranks them according to their influence scores.
- Evaluation metrics: The paper defines two metrics to evaluate the proposed method: bias score and influence score. The bias score measures how much a generated image y deviates from a desired representation r, such as gender or race. The paper uses a pre-trained classifier h to estimate the probability of y belonging to r, and defines the bias score as 1 - h(y). The influence score measures how much a word in x affects the bias score when it is masked out. The paper defines the influence score as the absolute difference between the bias scores of y and y'.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Inputs: text-to-image model f, masked language model g, classifier h, prompt x, desired representation r
# Outputs: ranked list of words in x according to their influence scores

# Initialize an empty list of influence scores
influence_scores = []

# Loop over each word in x
for i in range(len(x)):
  # Mask the i-th word in x using g
  x' = g.mask(x, i)
  # Generate an image y' using f
  y' = f(x')
  # Compute the bias score of y' using h
  bias_score_y' = 1 - h(y', r)
  # Compute the influence score of the i-th word as the absolute difference between the bias scores of y and y'
  influence_score_i = abs(bias_score_y - bias_score_y')
  # Append the influence score and the word to the list
  influence_scores.append((influence_score_i, x[i]))

# Sort the list of influence scores in descending order
influence_scores.sort(reverse=True)

# Return the ranked list of words
return influence_scores
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the required libraries
import torch # for tensor operations
import transformers # for masked language model
import clip # for text-to-image model and classifier

# Load the text-to-image model f
f = clip.load("ViT-B/32", jit=False)[0]

# Load the masked language model g
g = transformers.AutoModelForMaskedLM.from_pretrained("bert-base-uncased")
g_tokenizer = transformers.AutoTokenizer.from_pretrained("bert-base-uncased")

# Load the classifier h
h = clip.load("ViT-B/32", jit=False)[1]

# Define a function to mask a word in a prompt using g
def mask(prompt, index):
  # Tokenize the prompt using g_tokenizer
  tokens = g_tokenizer.tokenize(prompt)
  # Replace the token at the given index with [MASK]
  tokens[index] = "[MASK]"
  # Convert the tokens to ids
  input_ids = g_tokenizer.convert_tokens_to_ids(tokens)
  # Convert the input ids to a tensor
  input_tensor = torch.tensor([input_ids])
  # Generate predictions using g
  predictions = g(input_tensor).logits
  # Get the most probable token id for the masked position
  predicted_id = torch.argmax(predictions[0, index]).item()
  # Convert the predicted id to a token
  predicted_token = g_tokenizer.convert_ids_to_tokens([predicted_id])[0]
  # Replace the masked token with the predicted token
  tokens[index] = predicted_token
  # Convert the tokens back to a string
  masked_prompt = g_tokenizer.convert_tokens_to_string(tokens)
  # Return the masked prompt
  return masked_prompt

# Define a function to generate an image from a prompt using f
def generate(prompt):
  # Encode the prompt using clip.tokenize
  text_input = clip.tokenize(prompt)
  # Generate an image using f
  image_output = f.generate(text_input)
  # Return the image output
  return image_output

# Define a function to compute the bias score of an image using h
def bias_score(image, representation):
  # Encode the representation using clip.tokenize
  text_input = clip.tokenize(representation)
  # Encode the image using f.encode_image
  image_input = f.encode_image(image)
  # Compute the logits using h
  logits = h(image_input, text_input).logits
  # Compute the probability using softmax
  probability = torch.softmax(logits, dim=-1)
  # Return the probability as a scalar value
  return probability.item()

# Define a function to compute the word-level explanations for analyzing bias in text-to-image models
def explain(prompt, representation):
  # Initialize an empty list of influence scores
  influence_scores = []
  
  # Generate an image from the prompt using generate
  image = generate(prompt)
  
  # Compute the bias score of the image using bias_score
  bias_score_image = bias_score(image, representation)

  # Loop over each word in the prompt
  for i in range(len(prompt.split())):
    # Mask the i-th word in the prompt using mask
    masked_prompt = mask(prompt, i)
    # Generate a new image from the masked prompt using generate
    masked_image = generate(masked_prompt)
    # Compute the bias score of the new image using bias_score
    bias_score_masked_image = bias_score(masked_image, representation)
    # Compute the influence score of the i-th word as the absolute difference between the bias scores of image and masked_image
    influence_score_i = abs(bias_score_image - bias_score_masked_image)
    # Append the influence score and the word to the list
    influence_scores.append((influence_score_i, prompt.split()[i]))

  # Sort the list of influence scores in descending order
  influence_scores.sort(reverse=True)

  # Return the ranked list of words
  return influence_scores

```