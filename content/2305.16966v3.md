---
title: 2305.16966v3 Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection
date: 2023-05-17
---

# [Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection](http://arxiv.org/abs/2305.16966v3)

authors: Marc Lafon, Elias Ramzi, Cl√©ment Rambour, Nicolas Thome


## What, Why and How

[1]: https://arxiv.org/pdf/2305.16966v3.pdf "Hybrid Energy Based Model in the Feature Space for Out-of ... - arXiv.org"
[2]: https://arxiv.org/abs/2305.16966 "[2305.16966] Hybrid Energy Based Model in the Feature Space ... - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2305.16966 "[2305.16966] Hybrid Energy Based Model in the Feature Space for Out-of ..."

Here is a summary of the paper:

- **What**: The paper introduces a new method for out-of-distribution (OOD) detection, which is the problem of identifying samples that do not belong to the expected data distribution. OOD detection is important for the safety and reliability of deep learning models in real-world applications.
- **Why**: Existing methods for OOD detection either rely on parametric models that may not capture the complexity of the data, or on non-parametric models that may not generalize well to different types of OOD samples. The paper proposes a hybrid approach that combines both types of models using energy-based models (EBMs), which are flexible and expressive models that can learn from both labeled and unlabeled data.
- **How**: The paper presents the HEAT model, which stands for Hybrid Energy-based model in the feature space for OOD deTection. HEAT uses a pre-trained backbone network to extract features from the input samples, and then estimates the density of in-distribution (ID) samples using a hybrid EBM that consists of several energy terms. These terms include a parametric term based on a Gaussian Mixture Model (GMM), a non-parametric term based on the nearest neighbors distribution, and an energy logits term based on the classifier output. HEAT learns the optimal combination of these terms using an adversarial training scheme, where a discriminator tries to distinguish between ID and OOD samples, and a generator tries to fool the discriminator by producing realistic ID samples. HEAT can detect OOD samples by comparing their energy scores with a threshold learned from validation data.

The paper claims that HEAT achieves state-of-the-art results on several OOD detection benchmarks, such as CIFAR-10 / CIFAR-100 and ImageNet[^1^][1] [^2^][2]. The paper also provides ablation studies and qualitative analysis to demonstrate the effectiveness and robustness of HEAT. The code is available at: github.com/MarcLafon/heatood[^1^][1].


## Main Contributions

The paper makes the following contributions:

- It proposes a new post-hoc OOD detection method based on hybrid EBMs in the feature space of a pre-trained backbone network.
- It introduces a novel way of composing several energy terms to capture different aspects of the ID density, such as parametric, non-parametric, and classifier-based terms.
- It leverages the EBM framework to provide a unified density estimation and an adversarial training scheme for learning the optimal combination of energy terms.
- It sets new state-of-the-art results on several OOD detection benchmarks and provides extensive experiments and analysis to validate its approach.

## Method Summary

The method section of the paper consists of four subsections:

- The first subsection introduces the problem formulation and the notation used in the paper. It defines the OOD detection task as a binary classification problem, where the goal is to assign a low score to OOD samples and a high score to ID samples. It also defines the feature space of a pre-trained backbone network and the energy function of an EBM.
- The second subsection presents the hybrid EBM model for OOD detection, called HEAT. It describes how HEAT combines several energy terms to estimate the ID density in the feature space. These terms include a GMM term, a nearest neighbors term, and an energy logits term. It also explains how HEAT learns the optimal weights for each term using an adversarial training scheme, where a discriminator tries to distinguish between ID and OOD samples, and a generator tries to fool the discriminator by producing realistic ID samples.
- The third subsection details the implementation details of HEAT, such as the backbone network architecture, the number of components in the GMM, the number of neighbors in the nearest neighbors term, and the hyperparameters for the adversarial training. It also describes how HEAT computes the OOD score for a given sample by comparing its energy score with a threshold learned from validation data.
- The fourth subsection discusses some theoretical properties of HEAT, such as its relation to existing methods, its ability to handle different types of OOD samples, and its robustness to adversarial attacks. It also provides some intuition and visualization of how HEAT works in practice.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained backbone network f, a set of ID samples X, a set of OOD samples Y
# Output: an OOD detection model HEAT

# Define the energy function E(x) as a linear combination of several terms
E(x) = w_g * E_g(x) + w_n * E_n(x) + w_l * E_l(x)

# E_g(x) is the GMM term, which computes the log-likelihood of x under a GMM fitted on f(X)
E_g(x) = log(sum(pi_k * N(f(x); mu_k, sigma_k) for k in range(K)))

# E_n(x) is the nearest neighbors term, which computes the negative distance to the k-th nearest neighbor of x in f(X)
E_n(x) = -dist(f(x), nn_k(f(x), f(X)))

# E_l(x) is the energy logits term, which computes the logit output of a classifier trained on f(X) and f(Y)
E_l(x) = logit(f(x))

# Initialize the weights w_g, w_n, w_l randomly
w_g, w_n, w_l = random()

# Define the discriminator D(x) as a binary classifier that outputs the probability of x being ID
D(x) = sigmoid(W * f(x) + b)

# Define the generator G(z) as a function that transforms a random noise z into a feature vector
G(z) = tanh(W' * z + b')

# Train HEAT using an adversarial training scheme
for epoch in range(epochs):
  # Sample a batch of ID samples x_i from X
  x_i = sample(X)
  # Sample a batch of OOD samples y_j from Y
  y_j = sample(Y)
  # Sample a batch of random noise z_k
  z_k = sample(noise)
  # Update the discriminator D by maximizing its accuracy on x_i and y_j
  D_loss = -mean(log(D(x_i)) + log(1 - D(y_j)))
  D_params = D_params - lr * grad(D_loss, D_params)
  # Update the generator G by minimizing its divergence with f(X)
  G_loss = mean(E(G(z_k)))
  G_params = G_params - lr * grad(G_loss, G_params)
  # Update the weights w_g, w_n, w_l by minimizing their divergence with D
  E_loss = mean(D(G(z_k)) * E(G(z_k)) - (1 - D(x_i)) * E(x_i))
  w_g, w_n, w_l = w_g - lr * grad(E_loss, w_g), w_n - lr * grad(E_loss, w_n), w_l - lr * grad(E_loss, w_l)

# Compute the OOD score S(x) for a given sample x by comparing its energy score with a threshold t learned from validation data
S(x) = E(x) - t
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import NearestNeighbors

# Define some hyperparameters
batch_size = 128 # the size of the mini-batch for training
epochs = 100 # the number of epochs for training
lr = 0.01 # the learning rate for optimization
K = 10 # the number of components in the GMM
k = 5 # the number of neighbors in the nearest neighbors term
noise_dim = 100 # the dimension of the random noise for the generator
feature_dim = 2048 # the dimension of the feature space of the backbone network
threshold = 0.5 # the threshold for OOD detection

# Load the pre-trained backbone network f, such as ResNet-50
f = models.resnet50(pretrained=True)
# Freeze its parameters
for param in f.parameters():
  param.requires_grad = False

# Load the ID samples X and the OOD samples Y, such as CIFAR-10 and SVHN
X = load_ID_data()
Y = load_OOD_data()

# Extract the features of X and Y using f and store them in a matrix F
F = f(X + Y)

# Fit a GMM on F[:len(X)] using scikit-learn and store its parameters pi_k, mu_k, sigma_k
gmm = GaussianMixture(n_components=K)
gmm.fit(F[:len(X)])
pi_k = gmm.weights_
mu_k = gmm.means_
sigma_k = gmm.covariances_

# Fit a nearest neighbors model on F[:len(X)] using scikit-learn and store its distances dist_k and indices ind_k
nn = NearestNeighbors(n_neighbors=k)
nn.fit(F[:len(X)])
dist_k, ind_k = nn.kneighbors(F)

# Train a classifier on F and store its output logits logit_k
classifier = nn.Linear(feature_dim, 1)
optimizer = optim.SGD(classifier.parameters(), lr=lr)
criterion = nn.BCEWithLogitsLoss()
for epoch in range(epochs):
  # Shuffle F and its labels L
  F, L = shuffle(F, L)
  # Loop over mini-batches of F and L
  for i in range(0, len(F), batch_size):
    # Get the current batch of features and labels
    F_batch = F[i:i+batch_size]
    L_batch = L[i:i+batch_size]
    # Zero the gradients of the optimizer
    optimizer.zero_grad()
    # Forward pass the batch through the classifier and get the logits
    logit_batch = classifier(F_batch)
    # Compute the loss using the criterion
    loss = criterion(logit_batch, L_batch)
    # Backward pass the loss and update the optimizer
    loss.backward()
    optimizer.step()
# Store the logits of F in a vector logit_k
logit_k = classifier(F)

# Define the energy function E(x) as a linear combination of several terms using PyTorch
def E(x):
  # E_g(x) is the GMM term, which computes the log-likelihood of x under a GMM fitted on f(X)
  E_g = torch.log(torch.sum(pi_k * torch.exp(-0.5 * torch.sum(((f(x) - mu_k) / sigma_k) ** 2, dim=1)) / torch.sqrt(torch.prod(sigma_k, dim=1)), dim=0))
  # E_n(x) is the nearest neighbors term, which computes the negative distance to the k-th nearest neighbor of x in f(X)
  E_n = -torch.norm(f(x) - F[ind_k[-1]], dim=1)
  # E_l(x) is the energy logits term, which computes the logit output of a classifier trained on f(X) and f(Y)
  E_l = classifier(f(x))
  # Return the weighted sum of E_g, E_n, and E_l
  return w_g * E_g + w_n * E_n + w_l * E_l

# Initialize the weights w_g, w_n, w_l randomly using PyTorch
w_g = torch.randn(1, requires_grad=True)
w_n = torch.randn(1, requires_grad=True)
w_l = torch.randn(1, requires_grad=True)

# Define the discriminator D(x) as a binary classifier that outputs the probability of x being ID using PyTorch
D = nn.Sequential(
  nn.Linear(feature_dim, 100),
  nn.ReLU(),
  nn.Linear(100, 1),
  nn.Sigmoid()
)

# Define the generator G(z) as a function that transforms a random noise z into a feature vector using PyTorch
G = nn.Sequential(
  nn.Linear(noise_dim, 100),
  nn.ReLU(),
  nn.Linear(100, feature_dim),
  nn.Tanh()
)

# Define the optimizers for D, G, and w_g, w_n, w_l using PyTorch
D_optimizer = optim.Adam(D.parameters(), lr=lr)
G_optimizer = optim.Adam(G.parameters(), lr=lr)
E_optimizer = optim.Adam([w_g, w_n, w_l], lr=lr)

# Train HEAT using an adversarial training scheme using PyTorch
for epoch in range(epochs):
  # Sample a batch of ID samples x_i from X
  x_i = sample(X, batch_size)
  # Sample a batch of OOD samples y_j from Y
  y_j = sample(Y, batch_size)
  # Sample a batch of random noise z_k
  z_k = torch.randn(batch_size, noise_dim)
  # Update the discriminator D by maximizing its accuracy on x_i and y_j
  D_loss = -torch.mean(torch.log(D(f(x_i))) + torch.log(1 - D(f(y_j))))
  D_optimizer.zero_grad()
  D_loss.backward()
  D_optimizer.step()
  # Update the generator G by minimizing its divergence with f(X)
  G_loss = torch.mean(E(G(z_k)))
  G_optimizer.zero_grad()
  G_loss.backward()
  G_optimizer.step()
  # Update the weights w_g, w_n, w_l by minimizing their divergence with D
  E_loss = torch.mean(D(G(z_k)) * E(G(z_k)) - (1 - D(f(x_i))) * E(f(x_i)))
  E_optimizer.zero_grad()
  E_loss.backward()
  E_optimizer.step()

# Compute the OOD score S(x) for a given sample x by comparing its energy score with a threshold t learned from validation data
def S(x):
  return E(x) - threshold
```