---
title: 2005.00770v2 Exploring and Predicting Transferability across NLP Tasks
date: 2020-05-01
---

# [Exploring and Predicting Transferability across NLP Tasks](http://arxiv.org/abs/2005.00770v2)

authors: Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer


## What, Why and How

[1]: https://arxiv.org/pdf/2005.00770v2.pdf "arXiv:2005.00770v2 [cs.CL] 6 Oct 2020"
[2]: https://arxiv.org/abs/2005.00770 "Exploring and Predicting Transferability across NLP Tasks"
[3]: http://export.arxiv.org/abs/1611.00770v2 "[1611.00770v2] Rare region effects and dynamics near the many-body ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper explores and predicts the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling).
- **Why**: The paper aims to shed light on the conditions for successful transfer learning in NLP, and to develop task embeddings that can be used to select the most beneficial source tasks for a given target task.
- **How**: The paper conducts a large-scale empirical study using BERT as the base model and fine-tuning it on various source and target tasks. The paper also develops a method to compute task embeddings from BERT's layer-wise gradients and uses them to measure task similarity and predict transferability.

## Main Contributions

According to the paper, the main contributions are:

- A large-scale empirical study of the transferability between 33 NLP tasks, covering different problem types, domains, and data sizes.
- A novel method to compute task embeddings that capture the semantic and syntactic characteristics of NLP tasks and can be used to measure task similarity and predict transferability.
- A comprehensive analysis of the factors that affect transferability, such as source data size, task and domain similarity, and task complexity.

## Method Summary

The method section of the paper consists of three subsections:

- **Task Selection and Data Processing**: The paper describes the 33 NLP tasks that are used in the study, covering text classification, question answering, and sequence labeling problems. The paper also explains how the data for each task is processed and split into train, dev, and test sets.
- **Modeling and Fine-tuning**: The paper uses BERT-base as the base model for all tasks and fine-tunes it on each source and target task separately. The paper also describes the hyperparameters and optimization methods that are used for fine-tuning.
- **Task Embedding**: The paper introduces a novel method to compute task embeddings from BERT's layer-wise gradients. The paper explains how to obtain the task-specific gradients for each layer of BERT and how to aggregate them into a single vector representation for each task. The paper also shows how to use the task embeddings to measure task similarity and predict transferability.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load BERT-base model and 33 NLP tasks
bert = load_bert_model()
tasks = load_nlp_tasks()

# Fine-tune BERT on each task separately and save the models
for task in tasks:
  model = fine_tune(bert, task)
  save_model(model, task)

# Compute task embeddings for each task using BERT's layer-wise gradients
task_embeddings = {}
for task in tasks:
  model = load_model(task)
  gradients = get_task_gradients(model, task)
  embedding = aggregate_gradients(gradients)
  task_embeddings[task] = embedding

# Measure task similarity and predict transferability using task embeddings
for target_task in tasks:
  target_embedding = task_embeddings[target_task]
  similarities = {}
  for source_task in tasks:
    source_embedding = task_embeddings[source_task]
    similarity = cosine_similarity(target_embedding, source_embedding)
    similarities[source_task] = similarity
  # Sort source tasks by similarity and select the top-k most similar ones
  sorted_source_tasks = sort_by_value(similarities)
  selected_source_tasks = sorted_source_tasks[:k]
  # Fine-tune BERT on the selected source tasks before fine-tuning on the target task
  model = fine_tune(bert, selected_source_tasks)
  model = fine_tune(model, target_task)
  # Evaluate the performance of the model on the target task
  performance = evaluate(model, target_task)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries and modules
import torch
import transformers
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load BERT-base model and tokenizer
bert = transformers.BertModel.from_pretrained('bert-base-uncased')
tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')

# Load 33 NLP tasks and their data
tasks = load_nlp_tasks()
data = load_nlp_data(tasks)

# Fine-tune BERT on each task separately and save the models
for task in tasks:
  # Create a task-specific model by adding a classification head to BERT
  model = transformers.BertForSequenceClassification(bert, num_labels=task.num_labels)
  # Create a task-specific optimizer and scheduler
  optimizer = transformers.AdamW(model.parameters(), lr=task.learning_rate)
  scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=task.warmup_steps, num_training_steps=task.total_steps)
  # Fine-tune the model on the task data for a fixed number of epochs
  for epoch in range(task.epochs):
    # Iterate over the batches of the task data
    for batch in data[task]:
      # Get the input ids, attention masks, and labels from the batch
      input_ids = batch['input_ids']
      attention_masks = batch['attention_masks']
      labels = batch['labels']
      # Feed the inputs to the model and get the outputs
      outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)
      # Get the loss and logits from the outputs
      loss = outputs.loss
      logits = outputs.logits
      # Backpropagate the loss and update the model parameters
      loss.backward()
      optimizer.step()
      scheduler.step()
      # Zero the gradients for the next batch
      optimizer.zero_grad()
    # Evaluate the model on the dev set of the task data and save the best model
    dev_performance = evaluate(model, data[task]['dev'])
    if dev_performance > best_dev_performance:
      best_dev_performance = dev_performance
      save_model(model, task)
  
# Compute task embeddings for each task using BERT's layer-wise gradients
task_embeddings = {}
for task in tasks:
  # Load the best model for the task
  model = load_model(task)
  # Get the number of layers in BERT
  num_layers = bert.config.num_hidden_layers
  # Initialize an empty list to store the gradients for each layer
  layer_gradients = []
  # Register a hook function to capture the gradients for each layer
  def hook_function(module, grad_input, grad_output):
    layer_gradients.append(grad_output[0])
  for layer in model.bert.encoder.layer:
    layer.register_backward_hook(hook_function)
  # Get a random batch from the task data
  batch = get_random_batch(data[task])
  # Get the input ids, attention masks, and labels from the batch
  input_ids = batch['input_ids']
  attention_masks = batch['attention_masks']
  labels = batch['labels']
  # Feed the inputs to the model and get the outputs
  outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)
  # Get the loss from the outputs
  loss = outputs.loss
  # Backpropagate the loss to get the gradients for each layer
  loss.backward()
  # Compute the task embedding by averaging the gradients across layers and tokens
  task_embedding = torch.mean(torch.stack(layer_gradients), dim=[0,1])
  # Convert the task embedding to a numpy array and store it in a dictionary
  task_embedding = task_embedding.detach().numpy()
  task_embeddings[task] = task_embedding

# Measure task similarity and predict transferability using task embeddings
for target_task in tasks:
  # Get the task embedding for the target task
  target_embedding = task_embeddings[target_task]
  # Initialize an empty dictionary to store the similarities with other tasks
  similarities = {}
  for source_task in tasks:
    # Get the task embedding for the source task
    source_embedding = task_embeddings[source_task]
    # Compute the cosine similarity between the target and source embeddings
    similarity = cosine_similarity(target_embedding.reshape(1,-1), source_embedding.reshape(1,-1))[0][0]
    # Store the similarity in a dictionary with the source task as key
    similarities[source_task] = similarity
  # Sort source tasks by similarity in descending order and select the top-k most similar ones (k can be tuned as a hyperparameter)
  sorted_source_tasks = sorted(similarities.items(), key=lambda x: x[1], reverse=True)
  selected_source_tasks = [x[0] for x in sorted_source_tasks[:k]]
  # Fine-tune BERT on the selected source tasks before fine-tuning on the target task
  model = bert.clone()
  for source_task in selected_source_tasks:
    model = fine_tune(model, source_task)
  model = fine_tune(model, target_task)
  # Evaluate the performance of the model on the target task
  performance = evaluate(model, data[target_task]['test'])
```