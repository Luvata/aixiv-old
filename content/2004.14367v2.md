---
title: 2004.14367v2 Editing in Style  Uncovering the Local Semantics of GANs
date: 2020-04-15
---

# [Editing in Style: Uncovering the Local Semantics of GANs](http://arxiv.org/abs/2004.14367v2)

authors: Edo Collins, Raja Bala, Bob Price, Sabine SÃ¼sstrunk


## What, Why and How

[1]: https://arxiv.org/pdf/2004.14367 "g@parc.com arXiv:2004.14367v2 [cs.CV] 21 May 2020"
[2]: https://arxiv.org/abs/2004.14367 "Title: Editing in Style: Uncovering the Local Semantics of GANs - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2004.14367v2 "[2004.14367v2] Editing in Style: Uncovering the Local Semantics of GANs"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a method for making local, semantically-aware edits to a target image generated by a GAN, such as StyleGAN, by transferring style from a source image, also a GAN output[^1^][1].
- **Why**: The paper aims to improve the ability to control and condition the output of GANs, which is still limited despite the quality of image synthesis[^1^][1]. The paper also leverages the emergent disentanglement of semantic objects that is learned by StyleGAN during its training[^1^][1].
- **How**: The paper introduces a novel manipulation of style vectors that allows for borrowing elements from a source image and applying them to a target image in a spatially-localized manner[^1^][1]. The paper does not require supervision from an external model or complex spatial morphing operations[^1^][1]. The paper demonstrates semantic editing on GANs producing human faces, indoor scenes, cats, and cars[^1^][1]. The paper measures the locality and photorealism of the edits produced by the method and finds that it accomplishes both[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2004.14367 "Editing in Style: Uncovering the Local Semantics of GANs"
[2]: https://ieeexplore.ieee.org/document/9157140 "Editing in Style: Uncovering the Local Semantics of GANs | IEEE ..."
[3]: https://www.arxiv-vanity.com/papers/2004.14367/ "Editing in Style: Uncovering the Local Semantics of GANs"
[4]: https://arxiv-export-lb.library.cornell.edu/abs/2004.14367v2 "Editing in Style: Uncovering the Local Semantics of GANs"

The paper lists the following contributions[^1^][1]:

- A novel method for making local, semantically-aware edits to a target image generated by a GAN, by transferring style from a source image, also a GAN output[^1^][1].
- A demonstration of semantic editing on GANs producing human faces, indoor scenes, cats, and cars[^1^][1] [^2^][2].
- A quantitative and qualitative evaluation of the locality and photorealism of the edits produced by the method[^1^][1] [^2^][2].

## Method Summary

[1]: https://ieeexplore.ieee.org/document/9157140 "Editing in Style: Uncovering the Local Semantics of GANs"
[2]: https://arxiv.org/pdf/2004.14367.pdf "g@parc.com arXiv:2004.14367v2 [cs.CV] 21 May 2020"
[3]: https://www.arxiv-vanity.com/papers/2004.14367/ "Editing in Style: Uncovering the Local Semantics of GANs"
[4]: https://arxiv-export-lb.library.cornell.edu/abs/2004.14367v2 "Editing in Style: Uncovering the Local Semantics of GANs"

Here is a summary of the method section of the paper:

- The paper describes the architecture of StyleGAN and how it uses style vectors to control the appearance of the generated images at different resolutions[^1^][1] [^2^][2].
- The paper introduces a novel operation called **style mixing**, which allows for transferring style from a source image to a target image in a spatially-localized manner[^1^][1] [^2^][2]. Style mixing works by replacing the style vector of the target image with that of the source image at a certain resolution level[^1^][1] [^2^][2].
- The paper explains how to select the resolution level for style mixing based on the semantic object or part that needs to be edited[^1^][1] [^2^][2]. The paper also proposes a simple heuristic for finding the best source image for a given edit[^1^][1] [^2^][2].
- The paper presents some implementation details and limitations of the method, such as the need for alignment and masking, and the difficulty of editing fine-grained details[^1^][1] [^2^][2].

## Pseudo Code - High level

[1]: https://ieeexplore.ieee.org/document/9157140 "Editing in Style: Uncovering the Local Semantics of GANs"
[2]: https://arxiv.org/pdf/2004.14367.pdf "g@parc.com arXiv:2004.14367v2 [cs.CV] 21 May 2020"
[3]: https://www.semanticscholar.org/paper/Editing-in-Style%3A-Uncovering-the-Local-Semantics-of-Collins-Bala/a924535d54bd3417211008e65ead568daba5d525 "Editing in Style: Uncovering the Local Semantics of GANs"
[4]: https://github.com/ikr7/arxiv-survey/issues/9466 "Editing in Style: Uncovering the Local Semantics of GANs #9466 - GitHub"
[5]: https://www.arxiv-vanity.com/papers/2004.14367/ "Editing in Style: Uncovering the Local Semantics of GANs"

Here is the high-level pseudo code for this paper:

```python
# Input: a target image x_t and a source image x_s, both generated by StyleGAN
# Output: an edited image x_e that has the style of x_s transferred to x_t in a local region

# Step 1: Extract the style vectors of x_t and x_s using the mapping network of StyleGAN
w_t = mapping_network(x_t)
w_s = mapping_network(x_s)

# Step 2: Select the resolution level l for style mixing based on the semantic object or part to be edited
l = select_resolution_level()

# Step 3: Replace the style vector of x_t with that of x_s at resolution level l
w_e = w_t.copy()
w_e[l] = w_s[l]

# Step 4: Generate the edited image x_e using the synthesis network of StyleGAN
x_e = synthesis_network(w_e)

# Step 5: Optionally, apply alignment and masking to x_e to improve the quality of the edit
x_e = align_and_mask(x_e)
```

## Pseudo Code - Detail

[1]: https://ieeexplore.ieee.org/document/9157140 "Editing in Style: Uncovering the Local Semantics of GANs"
[2]: https://arxiv.org/pdf/2004.14367.pdf "g@parc.com arXiv:2004.14367v2 [cs.CV] 21 May 2020"
[3]: https://github.com/cyrilzakka/GANLocalEditing "GitHub - cyrilzakka/GANLocalEditing: Editing in Style: Uncovering the ..."

Here is the detailed pseudo code for this paper:

```python
# Input: a target image x_t and a source image x_s, both generated by StyleGAN
# Output: an edited image x_e that has the style of x_s transferred to x_t in a local region

# Step 1: Extract the style vectors of x_t and x_s using the mapping network of StyleGAN
# The mapping network is a fully connected network that maps a latent vector z to a style vector w
# The style vector w is a 512-dimensional vector that controls the appearance of the generated image
z_t = sample_latent_vector() # sample z_t from a standard normal distribution
z_s = sample_latent_vector() # sample z_s from a standard normal distribution
w_t = mapping_network(z_t) # compute w_t from z_t using the mapping network
w_s = mapping_network(z_s) # compute w_s from z_s using the mapping network

# Step 2: Select the resolution level l for style mixing based on the semantic object or part to be edited
# The resolution level l determines which layer of the synthesis network will use the style vector of x_s
# The lower the resolution level, the coarser the edit; the higher the resolution level, the finer the edit
# For example, l = 0 corresponds to 4x4 resolution, l = 1 corresponds to 8x8 resolution, and so on
# The paper proposes a simple heuristic for selecting l based on the size of the bounding box of the object or part to be edited
bbox = get_bounding_box() # get the bounding box of the object or part to be edited in x_t
bbox_size = get_bbox_size(bbox) # get the size (width or height) of the bounding box in pixels
l = log2(bbox_size / 4) # compute l as the logarithm base 2 of the ratio between bbox_size and 4

# Step 3: Replace the style vector of x_t with that of x_s at resolution level l
# The style vector w is split into 18 sub-vectors, each corresponding to a layer of the synthesis network
# The sub-vector at resolution level l is replaced with that of w_s, while keeping the rest unchanged
w_e = w_t.copy() # make a copy of w_t as w_e
w_e[l] = w_s[l] # replace the sub-vector at resolution level l with that of w_s

# Step 4: Generate the edited image x_e using the synthesis network of StyleGAN
# The synthesis network is a convolutional network that generates an image from a style vector w
# The style vector w is used to modulate (scale and shift) the feature maps of each layer via adaptive instance normalization (AdaIN)
x_e = synthesis_network(w_e) # generate x_e from w_e using the synthesis network

# Step 5: Optionally, apply alignment and masking to x_e to improve the quality of the edit
# Alignment ensures that x_e and x_t have the same global pose and orientation
# Masking ensures that only the region corresponding to the object or part to be edited is changed, while keeping the rest unchanged
x_e = align(x_e, x_t) # align x_e with x_t using an affine transformation
mask = get_mask() # get a binary mask that indicates which pixels belong to the object or part to be edited in x_t
x_e = mask * x_e + (1 - mask) * x_t # apply masking by blending x_e and x_t according to mask
```