---
title: 2202.04040v1 Self-Conditioned Generative Adversarial Networks for Image Editing
date: 2022-02-05
---

# [Self-Conditioned Generative Adversarial Networks for Image Editing](http://arxiv.org/abs/2202.04040v1)

authors: Yunzhe Liu, Rinon Gal, Amit H. Bermano, Baoquan Chen, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2202.04040 "[2202.04040] Self-Conditioned Generative Adversarial Networks for Image ..."
[2]: https://arxiv.org/abs/2201.04040 "[2201.04040] PEPit: computer-assisted worst-case analyses of first ..."
[3]: http://export.arxiv.org/abs/2206.04040v1 "[2206.04040v1] An Improved One millisecond Mobile Backbone"

Here is a summary of the paper:

- **What**: The paper proposes a method for mitigating generative bias in Generative Adversarial Networks (GANs) for image editing. Generative bias refers to the tendency of GANs to learn from unbalanced data or mode collapse, resulting in poor generation of rare semantic attributes. The method is called Self-Conditioned Generative Adversarial Networks (SCGANs), and it uses distances in the latent space of a pre-trained generator to provide initial labels for the data. By fine-tuning the generator on a re-sampled distribution drawn from these self-labeled data, SCGANs can better contend with rare semantic attributes and enable more realistic generation of these properties.
- **Why**: The paper aims to address the problem of generative bias in GANs, which limits their ability to perform fine-grained semantic editing and identity preservation. The paper argues that generative bias is responsible for the collapse of latent-traversal editing methods when deviating away from the distribution's core. The paper also claims that existing methods for mitigating generative bias are either limited in scope, require additional supervision, or introduce new hyperparameters.
- **How**: The paper introduces SCGANs, which consist of three steps: 1) self-labeling, where distances in the latent space of a pre-trained generator are used to assign labels to the data based on their rarity; 2) re-sampling, where a new distribution is drawn from the self-labeled data by over-sampling rare samples and under-sampling frequent samples; and 3) fine-tuning, where the generator is fine-tuned on the re-sampled distribution to reduce generative bias. The paper evaluates SCGANs on various image editing tasks, such as facial attribute manipulation, hair color change, and age progression/regression, and compares them with several baselines and state-of-the-art methods. The paper shows that SCGANs achieve finer semantic control and better identity preservation through a wider range of transformations.

## Main Contributions

According to the paper, the main contributions are:

- A novel method for mitigating generative bias in GANs for image editing, called SCGANs, which leverages self-labeling and re-sampling techniques to better contend with rare semantic attributes.
- A comprehensive evaluation of SCGANs on various image editing tasks, showing that they outperform existing methods in terms of semantic control and identity preservation.
- A detailed analysis of the effects of self-labeling and re-sampling on the latent space and the generated images, revealing how SCGANs reduce generative bias and improve diversity.

## Method Summary

[1]: https://arxiv.org/abs/2202.04040 "[2202.04040] Self-Conditioned Generative Adversarial Networks for Image ..."
[2]: https://arxiv.org/abs/2201.04040 "[2201.04040] PEPit: computer-assisted worst-case analyses of first ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2202.04040?context=cs.GR "[2202.04040] Self-Conditioned Generative Adversarial Networks for Image ..."

Here is a summary of the method section of the paper:

- The method section describes the proposed Self-Conditioned Generative Adversarial Networks (SCGANs) for image editing, which consist of three steps: self-labeling, re-sampling, and fine-tuning.
- **Self-labeling**: The paper uses distances in the latent space of a pre-trained generator to assign labels to the data based on their rarity. The paper defines a rarity score for each sample as the average distance to its k nearest neighbors in the latent space. The paper then clusters the samples into different rarity levels using a thresholding scheme. The paper argues that this self-labeling process can capture the semantic attributes that are rare in the data distribution, such as hair color, facial hair, glasses, etc.
- **Re-sampling**: The paper draws a new distribution from the self-labeled data by over-sampling rare samples and under-sampling frequent samples. The paper uses a re-sampling ratio that is inversely proportional to the rarity level of each sample. The paper claims that this re-sampling process can balance the data distribution and reduce generative bias.
- **Fine-tuning**: The paper fine-tunes the generator on the re-sampled distribution using a standard GAN loss. The paper keeps the discriminator fixed during fine-tuning to avoid mode collapse. The paper states that this fine-tuning process can improve the generation quality of rare semantic attributes and enable more realistic image editing.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained generator G and a dataset X
# Output: a fine-tuned generator G'

# Step 1: Self-labeling
# Compute the latent codes Z = G^{-1}(X) using the inverse mapping of G
# Compute the rarity scores R = average_distance(Z, k_nearest_neighbors(Z)) for each sample in Z
# Cluster the samples into different rarity levels L using a thresholding scheme
# Assign labels Y = L(Z) to the samples based on their rarity levels

# Step 2: Re-sampling
# Compute the re-sampling ratio P = 1 / L for each rarity level
# Draw a new distribution X' from X by over-sampling rare samples and under-sampling frequent samples according to P

# Step 3: Fine-tuning
# Fine-tune the generator G on the re-sampled distribution X' using a standard GAN loss
# Keep the discriminator D fixed during fine-tuning to avoid mode collapse
# Return the fine-tuned generator G'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a pre-trained generator G and a dataset X
# Output: a fine-tuned generator G'

# Step 1: Self-labeling
# Compute the latent codes Z = G^{-1}(X) using the inverse mapping of G
# Initialize an empty list R to store the rarity scores
# For each sample z in Z:
  # Find the k nearest neighbors of z in Z using Euclidean distance
  # Compute the average distance d between z and its k nearest neighbors
  # Append d to R
# Sort R in ascending order and divide it into n equal intervals
# Initialize an empty list L to store the rarity levels
# For each interval i in R:
  # Assign a rarity level l = i + 1 to all the samples in the interval
  # Append l to L
# Assign labels Y = L(Z) to the samples based on their rarity levels

# Step 2: Re-sampling
# Compute the re-sampling ratio P = 1 / L for each rarity level
# Initialize an empty list X' to store the re-sampled distribution
# For each sample x in X:
  # Find its corresponding label y in Y
  # Find its corresponding re-sampling ratio p in P
  # Draw a random number r from a uniform distribution between 0 and 1
  # If r < p:
    # Append x to X'
    # Else:
      # Discard x

# Step 3: Fine-tuning
# Initialize the generator G' as a copy of G
# Initialize the discriminator D as a pre-trained discriminator that matches G
# Set the learning rate lr and the number of epochs E for fine-tuning
# For each epoch e in range(E):
  # Shuffle X' randomly
  # Divide X' into batches of size B
  # For each batch x' in X':
    # Generate fake samples z' = G'(x')
    # Compute the real logits D(x')
    # Compute the fake logits D(z')
    # Compute the generator loss L_G = -log(D(z'))
    # Compute the discriminator loss L_D = -log(D(x')) - log(1 - D(z'))
    # Update the generator parameters by gradient descent: G' = G' - lr * grad(L_G, G')
    # Keep the discriminator parameters fixed: D = D
# Return the fine-tuned generator G'
```