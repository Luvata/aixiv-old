---
title: 2306.05422v1 Tracking Everything Everywhere All at Once
date: 2023-06-06
---

# [Tracking Everything Everywhere All at Once](http://arxiv.org/abs/2306.05422v1)

authors: Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, Noah Snavely


## What, Why and How

[1]: https://arxiv.org/abs/2306.05422 "[2306.05422] Tracking Everything Everywhere All at Once - arXiv.org"
[2]: https://arxiv.org/pdf/2306.05422v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/1810.05422v1 "arXiv:1810.05422v1 [eess.SP] 12 Oct 2018"

Here is a summary of the paper[^1^][1]:

- **What**: The paper presents a new method for estimating dense and long-range motion from a video sequence, called OmniMotion. It tracks every pixel in a video using a quasi-3D canonical volume and performs pixel-wise tracking via bijections between local and canonical space.
- **Why**: The paper aims to overcome the limitations of prior optical flow or particle video tracking algorithms, which typically operate within limited temporal windows, struggle to track through occlusions and maintain global consistency of estimated motion trajectories. The paper also aims to model any combination of camera and object motion, which is challenging for existing methods.
- **How**: The paper proposes a test-time optimization framework that takes a video sequence as input and outputs a complete and globally consistent motion representation, dubbed OmniMotion. OmniMotion represents a video using a quasi-3D canonical volume that is aligned with the dominant camera motion. It then performs pixel-wise tracking by finding bijections between local and canonical space using an iterative optimization process. This process ensures global consistency, tracks through occlusions, and models any combination of camera and object motion. The paper evaluates the proposed method on the TAP-Vid benchmark and real-world footage, and shows that it outperforms prior state-of-the-art methods by a large margin both quantitatively and qualitatively.


## Main Contributions

[1]: https://arxiv.org/abs/2306.05422 "[2306.05422] Tracking Everything Everywhere All at Once - arXiv.org"
[2]: https://arxiv.org/pdf/2306.05422v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/1810.05422v1 "arXiv:1810.05422v1 [eess.SP] 12 Oct 2018"

According to the paper[^1^][1], the main contributions are:

- **A new motion representation**: The paper introduces OmniMotion, a complete and globally consistent motion representation that allows for accurate, full-length motion estimation of every pixel in a video. OmniMotion represents a video using a quasi-3D canonical volume that is aligned with the dominant camera motion and performs pixel-wise tracking via bijections between local and canonical space.
- **A test-time optimization framework**: The paper proposes a novel test-time optimization framework that takes a video sequence as input and outputs OmniMotion. The framework consists of three steps: (1) estimating the dominant camera motion and constructing the canonical volume, (2) finding pixel-wise bijections between local and canonical space using an iterative optimization process, and (3) refining the motion estimates using a temporal consistency loss.
- **Extensive evaluations**: The paper evaluates the proposed method on the TAP-Vid benchmark and real-world footage, and shows that it outperforms prior state-of-the-art methods by a large margin both quantitatively and qualitatively. The paper also demonstrates the applicability of OmniMotion for various tasks such as video stabilization, segmentation, inpainting, and synthesis.


## Method Summary

[1]: https://arxiv.org/abs/2306.05422 "[2306.05422] Tracking Everything Everywhere All at Once - arXiv.org"
[2]: https://arxiv.org/pdf/2306.05422v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/1810.05422v1 "arXiv:1810.05422v1 [eess.SP] 12 Oct 2018"

Here is a summary of the method section of the paper[^1^][1]:

- **Overview**: The paper proposes a test-time optimization framework that takes a video sequence as input and outputs OmniMotion, a complete and globally consistent motion representation that allows for accurate, full-length motion estimation of every pixel in a video. The framework consists of three steps: (1) estimating the dominant camera motion and constructing the canonical volume, (2) finding pixel-wise bijections between local and canonical space using an iterative optimization process, and (3) refining the motion estimates using a temporal consistency loss.
- **Dominant camera motion estimation and canonical volume construction**: The paper assumes that the dominant camera motion in a video sequence can be approximated by a homography transformation. The paper uses a pre-trained network to estimate the homography parameters for each frame and then applies them to warp the frames to a common reference frame. The paper then constructs a quasi-3D canonical volume by stacking the warped frames along the depth dimension. The canonical volume serves as a reference for pixel-wise tracking and ensures global consistency of motion estimates.
- **Pixel-wise bijections between local and canonical space**: The paper performs pixel-wise tracking by finding bijections between local and canonical space for each pixel in each frame. A bijection is a mapping that preserves the identity and order of pixels across different views. The paper uses an iterative optimization process to find the optimal bijections that minimize a photometric loss between the local and canonical views. The paper also introduces a regularization term that encourages smoothness and sparsity of motion vectors. The paper models the bijections using 2D affine transformations that can handle any combination of camera and object motion.
- **Motion refinement using temporal consistency loss**: The paper refines the motion estimates by enforcing temporal consistency across consecutive frames. The paper uses a temporal consistency loss that penalizes large changes in motion vectors between adjacent frames. The paper also uses a mask network to identify occluded regions and exclude them from the loss computation. The paper updates the motion estimates by minimizing the temporal consistency loss using gradient descent.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a video sequence V of N frames
# Output: OmniMotion M, a complete and globally consistent motion representation

# Step 1: Estimate the dominant camera motion and construct the canonical volume
H = estimate_homography(V) # a pre-trained network that outputs homography parameters for each frame
V_warp = warp_frames(V, H) # apply homography transformations to warp the frames to a common reference frame
C = stack_frames(V_warp) # construct a quasi-3D canonical volume by stacking the warped frames along the depth dimension

# Step 2: Find pixel-wise bijections between local and canonical space
M = initialize_motion_vectors() # initialize motion vectors with zero or random values
for t in range(N): # for each frame
  for i in range(max_iterations): # for each iteration
    V_proj = project_frame(V[t], M[t]) # project the local frame to the canonical space using motion vectors
    L_photo = compute_photometric_loss(V_proj, C) # compute the photometric loss between the projected and canonical views
    L_reg = compute_regularization_loss(M[t]) # compute the regularization loss that encourages smoothness and sparsity of motion vectors
    L_total = L_photo + lambda * L_reg # compute the total loss as a weighted sum of the two losses
    M[t] = update_motion_vectors(M[t], L_total) # update the motion vectors by minimizing the total loss using gradient descent

# Step 3: Refine the motion estimates using temporal consistency loss
for t in range(1, N): # for each frame except the first one
  O = estimate_occlusion_mask(V[t-1], V[t], M[t-1], M[t]) # estimate the occlusion mask using a pre-trained network
  L_temp = compute_temporal_consistency_loss(M[t-1], M[t], O) # compute the temporal consistency loss that penalizes large changes in motion vectors between adjacent frames, excluding occluded regions
  M[t] = update_motion_vectors(M[t], L_temp) # update the motion vectors by minimizing the temporal consistency loss using gradient descent

return M # return OmniMotion as the output
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: a video sequence V of N frames, each frame has size H x W x 3
# Output: OmniMotion M, a complete and globally consistent motion representation, each motion vector has size 2 x 3

# Step 1: Estimate the dominant camera motion and construct the canonical volume
H = np.zeros((N, 3, 3)) # initialize homography parameters with zero values
net_h = load_pretrained_network("homography_estimator") # load a pre-trained network that outputs homography parameters for each frame
for t in range(N): # for each frame
  H[t] = net_h(V[t]) # estimate the homography parameters using the network
V_warp = np.zeros((N, H, W, 3)) # initialize warped frames with zero values
for t in range(N): # for each frame
  V_warp[t] = warp_frame(V[t], H[t]) # apply homography transformations to warp the frame to a common reference frame
C = np.zeros((H, W, N * 3)) # initialize canonical volume with zero values
for t in range(N): # for each frame
  C[:, :, t * 3 : (t + 1) * 3] = V_warp[t] # stack the warped frame along the depth dimension

# Step 2: Find pixel-wise bijections between local and canonical space
M = np.zeros((N, H, W, 2, 3)) # initialize motion vectors with zero values
for t in range(N): # for each frame
  for i in range(max_iterations): # for each iteration
    V_proj = np.zeros((H, W, N * 3)) # initialize projected frame with zero values
    for x in range(H): # for each row
      for y in range(W): # for each column
        p = np.array([x, y, 1]) # local pixel coordinate in homogeneous form
        q = M[t, x, y] @ p # canonical pixel coordinate in homogeneous form
        q = q / q[2] # normalize canonical pixel coordinate
        V_proj[x, y] = bilinear_interpolate(C, q) # project the local pixel to the canonical space using bilinear interpolation
    L_photo = np.mean(np.abs(V_proj - C), axis=2) # compute the photometric loss as the mean absolute difference between the projected and canonical views along the color dimension
    L_reg = np.mean(np.abs(M[t])) + np.mean(np.abs(np.gradient(M[t]))) # compute the regularization loss as the mean absolute value of motion vectors plus the mean absolute value of motion vector gradients
    L_total = L_photo + lambda * L_reg # compute the total loss as a weighted sum of the two losses
    grad_M = compute_gradient(M[t], L_total) # compute the gradient of motion vectors with respect to the total loss using backpropagation
    M[t] = M[t] - alpha * grad_M # update the motion vectors by minimizing the total loss using gradient descent

# Step 3: Refine the motion estimates using temporal consistency loss
net_o = load_pretrained_network("occlusion_estimator") # load a pre-trained network that outputs occlusion masks for each frame pair
for t in range(1, N): # for each frame except the first one
  O = net_o(V[t-1], V[t], M[t-1], M[t]) # estimate the occlusion mask using the network
  L_temp = np.mean(np.abs(M[t-1] - M[t]) * (1 - O), axis=(2, 3)) # compute the temporal consistency loss as the mean absolute difference between adjacent motion vectors weighted by the inverse occlusion mask along the transformation dimension
  grad_M = compute_gradient(M[t], L_temp) # compute the gradient of motion vectors with respect to the temporal consistency loss using backpropagation
  M[t] = M[t] - beta * grad_M # update the motion vectors by minimizing the temporal consistency loss using gradient descent

return M # return OmniMotion as the output

# Helper functions

def warp_frame(frame, homography):
  """Warp a frame using a homography transformation"""
  height, width, _ = frame.shape # get frame size
  grid_x, grid_y = np.meshgrid(np.arange(width), np.arange(height)) # create a grid of pixel coordinates
  grid_z = np.ones_like(grid_x) # create a grid of ones for homogeneous coordinates
  grid = np.stack([grid_x, grid_y, grid_z], axis=2) # stack the grids along the last dimension
  grid = grid.reshape(-1, 3).T # reshape and transpose the grid to get a 3 x N matrix of pixel coordinates
  grid = homography @ grid # apply the homography transformation to the grid
  grid = grid / grid[2] # normalize the grid by the last row
  grid = grid.T.reshape(height, width, 3) # reshape and transpose the grid to get a H x W x 3 matrix of pixel coordinates
  warped_frame = bilinear_interpolate(frame, grid) # warp the frame using bilinear interpolation
  return warped_frame # return the warped frame

def bilinear_interpolate(frame, coords):
  """Bilinear interpolate a frame using fractional pixel coordinates"""
  height, width, channels = frame.shape # get frame size and number of channels
  x = coords[:, :, 0] # get x coordinates
  y = coords[:, :, 1] # get y coordinates
  x0 = np.floor(x).astype(int) # get left integer x coordinates
  x1 = x0 + 1 # get right integer x coordinates
  y0 = np.floor(y).astype(int) # get top integer y coordinates
  y1 = y0 + 1 # get bottom integer y coordinates
  x0 = np.clip(x0, 0, width - 1) # clip x0 to frame width
  x1 = np.clip(x1, 0, width - 1) # clip x1 to frame width
  y0 = np.clip(y0, 0, height - 1) # clip y0 to frame height
  y1 = np.clip(y1, 0, height - 1) # clip y1 to frame height
  Ia = frame[y0, x0] # get top-left pixel values
  Ib = frame[y1, x0] # get bottom-left pixel values
  Ic = frame[y0, x1] # get top-right pixel values
  Id = frame[y1, x1] # get bottom-right pixel values
  wa = (x1 - x) * (y1 - y) # get top-left pixel weights
  wb = (x1 - x) * (y - y0) # get bottom-left pixel weights
  wc = (x - x0) * (y1 - y) # get top-right pixel weights
  wd = (x - x0) * (y - y0) # get bottom-right pixel weights
  interpolated_frame = wa[..., None] * Ia + wb[..., None] * Ib + wc[..., None] * Ic + wd[..., None] * Id # interpolate the frame using pixel values and weights
  return interpolated_frame # return the interpolated frame

def compute_gradient(x, y):
  """Compute the gradient of x with respect to y using backpropagation"""
  grad_x = np.zeros_like(x) # initialize gradient of x with zero values
  dy_dx = np.ones_like(x) # initialize derivative of y with respect to x with one values
  for i in reversed(range(len(y.shape))): # for each dimension of y in reverse order
    grad_x += dy_dx * np.sum(y, axis=i, keepdims=True) # multiply derivative of y with respect to x by sum of y along that dimension and add to gradient of x
    dy_dx *= np.gradient(x, axis=i) # multiply derivative of y with respect to x by gradient of x along that dimension
  return grad_x # return gradient of x

```