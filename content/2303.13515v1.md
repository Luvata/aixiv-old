---
title: 2303.13515v1 Persistent Nature  A Generative Model of Unbounded 3D Worlds
date: 2023-03-14
---

# [Persistent Nature: A Generative Model of Unbounded 3D Worlds](http://arxiv.org/abs/2303.13515v1)

authors: Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, Noah Snavely


## What, Why and How

[1]: https://arxiv.org/abs/2303.13515 "Persistent Nature: A Generative Model of Unbounded 3D Worlds"
[2]: http://export.arxiv.org/abs/2303.13515 "[2303.13515] Persistent Nature: A Generative Model of Unbounded 3D Worlds"
[3]: https://arxiv.org/pdf/2303.13515.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a generative model of unbounded 3D worlds that can synthesize realistic nature scenes from arbitrary camera poses and motions.
- **Why**: The paper aims to overcome the limitations of existing 3D image generative models that often operate on fixed 3D volumes and support only small camera motions. The paper also seeks to enable a persistent and consistent 3D world representation that is independent of the camera view.
- **How**: The paper uses a scene representation that consists of an extendable, planar scene layout grid and a panoramic skydome. The scene layout grid can be rendered from any camera pose using a 3D decoder and volume rendering. The skydome captures the global illumination and sky appearance. The paper learns the generative model from single-view internet photos using a variational autoencoder framework. The paper demonstrates the ability of the model to synthesize unbounded nature scenes with long flights and global consistency.


## Main Contributions

[1]: https://arxiv.org/abs/2303.13515 "Persistent Nature: A Generative Model of Unbounded 3D Worlds"
[2]: http://export.arxiv.org/abs/2303.13515 "[2303.13515] Persistent Nature: A Generative Model of Unbounded 3D Worlds"
[3]: https://arxiv.org/pdf/2303.13515.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- **A novel scene representation** that consists of an extendable, planar scene layout grid and a panoramic skydome, which can capture unbounded 3D worlds with global illumination and sky appearance.
- **A generative world model** that learns to synthesize realistic nature scenes from single-view internet photos using a variational autoencoder framework, without requiring any 3D supervision or multi-view consistency.
- **A rendering pipeline** that can render the scene representation from arbitrary camera poses and motions using a 3D decoder and volume rendering, while maintaining a persistent and consistent 3D world model.
- **An evaluation** of the proposed model on various metrics and tasks, such as image quality, scene diversity, camera motion range, scene consistency, and scene editing. The paper also compares the model with several baselines and ablations, and shows qualitative results on different types of nature scenes.


## Method Summary

[1]: https://arxiv.org/abs/2303.13515 "Persistent Nature: A Generative Model of Unbounded 3D Worlds"
[2]: http://export.arxiv.org/abs/2303.13515 "[2303.13515] Persistent Nature: A Generative Model of Unbounded 3D Worlds"
[3]: https://arxiv.org/pdf/2303.13515.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a **scene representation** that consists of two components: a **scene layout grid** and a **skydome**. The scene layout grid is a 2D grid of planar patches that can be extended to cover an unbounded 3D world. Each patch has a latent code that encodes its appearance and geometry. The skydome is a spherical texture that captures the global illumination and sky appearance. The paper uses spherical harmonics to represent the skydome in a low-dimensional space.
- The paper proposes a **generative world model** that learns to synthesize realistic nature scenes from single-view internet photos using a variational autoencoder framework. The paper uses a 2D encoder to encode the input image into a latent code, and a 3D decoder to decode the latent code into a scene representation. The paper also uses an adversarial loss to encourage realistic image generation and a perceptual loss to preserve high-frequency details.
- The paper describes a **rendering pipeline** that can render the scene representation from arbitrary camera poses and motions using a 3D decoder and volume rendering. The paper uses ray marching to sample the scene layout grid along each ray, and alpha compositing to blend the samples into a final image. The paper also uses the skydome to modulate the lighting and color of the rendered image.
- The paper discusses some **implementation details** such as the network architectures, the training data, the optimization procedure, and the hyperparameters. The paper also provides some ablation studies and qualitative results on different types of nature scenes, such as mountains, forests, deserts, and oceans.


## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```
# Define the scene representation
scene_layout_grid = 2D grid of planar patches
skydome = spherical texture of sky appearance

# Define the generative world model
VAE = variational autoencoder with 2D encoder and 3D decoder
D = discriminator network for adversarial loss
L_perceptual = perceptual loss function for image quality

# Define the rendering pipeline
ray_march = function to sample scene layout grid along each ray
alpha_composite = function to blend samples into final image
sky_modulate = function to apply skydome lighting and color to image

# Train the generative world model
for each input image I:
  # Encode the input image into a latent code
  z = VAE.encode(I)
  # Decode the latent code into a scene representation
  scene_layout_grid, skydome = VAE.decode(z)
  # Render the scene representation from the same camera pose as input image
  I_hat = render(scene_layout_grid, skydome, camera_pose)
  # Compute the reconstruction loss
  L_recon = L_perceptual(I, I_hat) + KL_divergence(z)
  # Compute the adversarial loss
  L_adv = D(I_hat) - D(I)
  # Update the VAE and D parameters using gradient descent
  VAE.update(L_recon + L_adv)
  D.update(L_adv)

# Synthesize unbounded nature scenes
for each desired camera pose and motion:
  # Sample a random latent code from prior distribution
  z = sample_prior()
  # Decode the latent code into a scene representation
  scene_layout_grid, skydome = VAE.decode(z)
  # Render the scene representation from the desired camera pose and motion
  I_syn = render(scene_layout_grid, skydome, camera_pose)
  # Display the synthesized image
  show(I_syn)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```
# Define the scene representation
scene_layout_grid = 2D grid of planar patches with size S x S
skydome = spherical texture of sky appearance with size H x W

# Define the generative world model
VAE = variational autoencoder with 2D encoder and 3D decoder
  # The 2D encoder takes an input image of size H x W x 3 and outputs a latent code of size D
  encoder = convolutional neural network with residual blocks and downsampling layers
  # The 3D decoder takes a latent code of size D and outputs a scene representation
  decoder = convolutional neural network with residual blocks and upsampling layers
    # The scene representation consists of two components: a scene layout grid and a skydome
    # The scene layout grid is a 2D grid of planar patches, each with a latent code of size C
    scene_layout_grid = reshape(decoder_output[:S*S*C], (S, S, C))
    # The skydome is a spherical texture of sky appearance, represented by spherical harmonics coefficients of size K
    skydome = decoder_output[S*S*C:]

D = discriminator network for adversarial loss
  # The discriminator takes an input image of size H x W x 3 and outputs a scalar score
  D = convolutional neural network with residual blocks and downsampling layers

L_perceptual = perceptual loss function for image quality
  # The perceptual loss function computes the L1 distance between the feature maps of a pretrained VGG network for the input and output images
  L_perceptual(I, I_hat) = sum(L1(VGG(I) - VGG(I_hat)))

# Define the rendering pipeline
ray_march = function to sample scene layout grid along each ray
  # The ray marching function takes a scene layout grid, a camera pose, and a pixel coordinate as inputs and outputs a color and an alpha value for that pixel
  ray_march(scene_layout_grid, camera_pose, pixel) =
    # Compute the ray direction for the pixel using the camera pose
    ray_dir = compute_ray_dir(camera_pose, pixel)
    # Initialize the color and alpha values to zero
    color = [0, 0, 0]
    alpha = 0
    # Loop over the scene layout grid from near to far along the ray direction
    for i in range(S):
      for j in range(S):
        # Get the latent code for the current patch
        patch_code = scene_layout_grid[i][j]
        # Decode the patch code into a patch color and a patch depth using a patch decoder network
        patch_color, patch_depth = patch_decoder(patch_code)
        # Compute the distance from the camera to the patch along the ray direction using bilinear interpolation
        dist = bilinear_interpolate(patch_depth, ray_dir)
        # Compute the weight for the patch using an alpha function that depends on the distance and a sharpness parameter sigma
        weight = alpha_function(dist, sigma)
        # Update the color and alpha values using alpha compositing formula
        color = color * (1 - weight) + patch_color * weight
        alpha = alpha * (1 - weight) + weight
    # Return the color and alpha values for the pixel
    return color, alpha

alpha_composite = function to blend samples into final image
  # The alpha compositing function takes a list of colors and alphas for each pixel and outputs a final image of size H x W x 3
  alpha_composite(colors, alphas) =
    # Initialize the final image to zero
    image = zeros(H, W, 3)
    # Loop over each pixel in the image
    for i in range(H):
      for j in range(W):
        # Get the list of colors and alphas for the current pixel
        pixel_colors = colors[i][j]
        pixel_alphas = alphas[i][j]
        # Initialize the pixel color and alpha values to zero
        pixel_color = [0, 0, 0]
        pixel_alpha = 0
        # Loop over the list of colors and alphas from back to front
        for k in reversed(range(len(pixel_colors))):
          # Get the current color and alpha values
          curr_color = pixel_colors[k]
          curr_alpha = pixel_alphas[k]
          # Update the pixel color and alpha values using alpha compositing formula
          pixel_color = pixel_color * (1 - curr_alpha) + curr_color * curr_alpha
          pixel_alpha = pixel_alpha * (1 - curr_alpha) + curr_alpha
        # Set the final image pixel to the pixel color value
        image[i][j] = pixel_color
    # Return the final image
    return image

sky_modulate = function to apply skydome lighting and color to image
  # The sky modulation function takes a skydome, a camera pose, and an image as inputs and outputs a modulated image of size H x W x 3
  sky_modulate(skydome, camera_pose, image) =
    # Initialize the modulated image to zero
    modulated_image = zeros(H, W, 3)
    # Loop over each pixel in the image
    for i in range(H):
      for j in range(W):
        # Compute the ray direction for the pixel using the camera pose
        ray_dir = compute_ray_dir(camera_pose, pixel)
        # Compute the spherical coordinates for the ray direction
        theta, phi = spherical_coords(ray_dir)
        # Evaluate the skydome at the spherical coordinates using spherical harmonics basis functions
        sky_color = evaluate_skydome(skydome, theta, phi)
        # Modulate the image pixel by the sky color using a modulation function that depends on a modulation parameter gamma
        modulated_image[i][j] = modulation_function(image[i][j], sky_color, gamma)
    # Return the modulated image
    return modulated_image

render = function to render the scene representation from a camera pose
  # The render function takes a scene representation and a camera pose as inputs and outputs a rendered image of size H x W x 3
  render(scene_layout_grid, skydome, camera_pose) =
    # Initialize a list of colors and alphas for each pixel
    colors = list(H, W)
    alphas = list(H, W)
    # Loop over each pixel in the image
    for i in range(H):
      for j in range(W):
        # Sample the scene layout grid along the ray direction for the pixel using ray marching
        color, alpha = ray_march(scene_layout_grid, camera_pose, (i, j))
        # Append the color and alpha values to the list for the pixel
        colors[i][j].append(color)
        alphas[i][j].append(alpha)
    # Blend the samples into a final image using alpha compositing
    image = alpha_composite(colors, alphas)
    # Modulate the image by the skydome lighting and color using sky modulation
    image = sky_modulate(skydome, camera_pose, image)
    # Return the rendered image
    return image

# Train the generative world model
for each input image I:
  # Encode the input image into a latent code using the 2D encoder
  z = VAE.encode(I)
  # Decode the latent code into a scene representation using the 3D decoder
  scene_layout_grid, skydome = VAE.decode(z)
  # Render the scene representation from the same camera pose as input image using the rendering pipeline
  I_hat = render(scene_layout_grid, skydome, camera_pose)
  # Compute the reconstruction loss using perceptual loss and KL divergence
  L_recon = L_perceptual(I, I_hat) + KL_divergence(z)
  # Compute the adversarial loss using discriminator network
  L_adv = D(I_hat) - D(I)
  # Update the VAE and D parameters using gradient descent with Adam optimizer and learning rate lr
  VAE.update(L_recon + L_adv, lr)
  D.update(L_adv, lr)

# Synthesize unbounded nature scenes
for each desired camera pose and motion:
  # Sample a random latent code from prior distribution (standard normal distribution)
  z = sample_prior()
  # Decode the latent code into a scene representation using the 3D decoder
  scene_layout_grid, skydome = VAE.decode(z)
  # Render the scene representation from the desired camera pose and motion using the rendering pipeline
  I_syn = render(scene_layout_grid, skydome, camera_pose)
  # Display the synthesized image on screen or save it to file
  show(I_syn) or save(I_syn)
```