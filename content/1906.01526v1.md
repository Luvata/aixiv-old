---
title: 1906.01526v1 Cross-Domain Cascaded Deep Feature Translation
date: 2019-06-02
---

# [Cross-Domain Cascaded Deep Feature Translation](http://arxiv.org/abs/1906.01526v1)

authors: Oren Katzir, Dani Lischinski, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1906.01526 "[1906.01526] Cross-Domain Cascaded Deep Feature Translation - arXiv.org"
[2]: https://arxiv.org/pdf/1906.01526v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/1806.01526v1 "[1806.01526v1] Leolani: a reference machine with a theory of mind for ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a method for cross-domain cascaded deep feature translation, which can transfer shape and appearance between different image domains, such as cats and dogs.
- **Why**: The paper aims to address the challenge of shape translation, which is difficult for existing image-to-image translation methods that focus on style and appearance transfer. The paper argues that shape translation requires more semantic supervision, which can be obtained from pre-trained classification networks.
- **How**: The paper leverages VGG, a pre-trained classification network, and performs translation in a cascaded, deep-to-shallow, fashion along the deep feature hierarchy. The paper first translates between the deepest layers that encode the higher-level semantic content of the image, then proceeds to translate the shallower layers, conditioned on the deeper ones. The paper uses adversarial training and cycle consistency loss to ensure realistic and consistent translations. The paper evaluates the method both qualitatively and quantitatively and compares it to state-of-the-art image-to-image translation methods.

## Main Contributions

[1]: https://arxiv.org/abs/1906.01526 "[1906.01526] Cross-Domain Cascaded Deep Feature Translation - arXiv.org"
[2]: https://arxiv.org/pdf/1906.01526v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/1806.01526v1 "[1806.01526v1] Leolani: a reference machine with a theory of mind for ..."

According to the paper[^1^][1], the main contributions are:

- **A novel method for cross-domain cascaded deep feature translation**, which can transfer shape and appearance between different image domains, such as cats and dogs, using pre-trained classification networks.
- **A cascaded, deep-to-shallow, translation scheme**, which performs translation along the deep feature hierarchy of VGG, starting from the deepest layers that encode the higher-level semantic content of the image, and ending with the shallowest layers that encode the lower-level details of the image.
- **A comprehensive evaluation and comparison** of the proposed method with state-of-the-art image-to-image translation methods, both qualitatively and quantitatively, on various datasets and tasks. The paper also provides ablation studies and analysis of the method.

## Method Summary

[1]: https://arxiv.org/abs/1906.01526 "[1906.01526] Cross-Domain Cascaded Deep Feature Translation - arXiv.org"
[2]: https://arxiv.org/pdf/1906.01526v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/1806.01526v1 "[1806.01526v1] Leolani: a reference machine with a theory of mind for ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces the concept of **deep feature translation**, which is the process of transforming the deep features of an image from one domain to another, using a pre-trained classification network (VGG) as a feature extractor.
- The paper proposes a **cascaded, deep-to-shallow, translation scheme**, which performs translation along the deep feature hierarchy of VGG, starting from the deepest layers that encode the higher-level semantic content of the image, and ending with the shallowest layers that encode the lower-level details of the image. The paper argues that this scheme allows for better shape translation and preserves more details than existing methods that translate only at a single layer.
- The paper defines a **translation network** for each layer of VGG, which consists of an encoder-decoder architecture with skip connections and residual blocks. The paper also defines a **discriminator network** for each layer of VGG, which is a PatchGAN that classifies whether a patch of deep features is real or fake.
- The paper formulates an **objective function** for the translation network and the discriminator network, which consists of three terms: an adversarial loss, a cycle consistency loss, and a feature matching loss. The paper explains the role and motivation of each term and how they are computed.
- The paper describes the **training procedure** for the proposed method, which involves alternating between updating the translation network and the discriminator network for each layer of VGG. The paper also discusses some implementation details and hyperparameters.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a source image x from domain X and a target image y from domain Y
# Output: a translated image x' from domain Y and a translated image y' from domain X

# Load a pre-trained VGG network as a feature extractor
vgg = load_vgg()

# Define translation networks for each layer of VGG
G_XtoY = [EncoderDecoder(l) for l in vgg.layers]
G_YtoX = [EncoderDecoder(l) for l in vgg.layers]

# Define discriminator networks for each layer of VGG
D_X = [PatchGAN(l) for l in vgg.layers]
D_Y = [PatchGAN(l) for l in vgg.layers]

# Define objective function with adversarial loss, cycle consistency loss, and feature matching loss
L = L_adv + L_cyc + L_fm

# Train the networks in a cascaded, deep-to-shallow, fashion
for l in reversed(vgg.layers):
  # Extract deep features for x and y at layer l
  f_x = vgg.extract(x, l)
  f_y = vgg.extract(y, l)

  # Update translation networks
  f_x' = G_XtoY[l](f_x) # translate x to y at layer l
  f_y' = G_YtoX[l](f_y) # translate y to x at layer l
  G_XtoY[l].update(L(f_x', f_y)) # minimize objective function
  G_YtoX[l].update(L(f_y', f_x)) # minimize objective function

  # Update discriminator networks
  D_X[l].update(L(f_x', f_x)) # maximize objective function
  D_Y[l].update(L(f_y', f_y)) # maximize objective function

# Reconstruct the translated images from the deep features
x' = vgg.reconstruct(f_x', 0) # reconstruct x' from the shallowest layer
y' = vgg.reconstruct(f_y', 0) # reconstruct y' from the shallowest layer

# Return the translated images
return x', y'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define constants
NUM_LAYERS = 5 # number of layers of VGG to use
LAMBDA_ADV = 1 # weight for adversarial loss
LAMBDA_CYC = 10 # weight for cycle consistency loss
LAMBDA_FM = 10 # weight for feature matching loss
BATCH_SIZE = 1 # batch size for training
NUM_EPOCHS = 100 # number of epochs for training
LEARNING_RATE = 0.0002 # learning rate for optimizers
BETA_1 = 0.5 # beta_1 for Adam optimizers

# Load a pre-trained VGG network as a feature extractor and freeze its parameters
vgg = torchvision.models.vgg19(pretrained=True).features[:NUM_LAYERS]
vgg.eval()
for param in vgg.parameters():
  param.requires_grad = False

# Define a function to extract deep features for an image at a given layer of VGG
def extract(image, layer):
  # image: a tensor of shape [B, C, H, W]
  # layer: an integer from 0 to NUM_LAYERS - 1
  # return: a tensor of shape [B, C', H', W']
  features = image
  for i in range(layer + 1):
    features = vgg[i](features)
  return features

# Define a function to reconstruct an image from deep features at a given layer of VGG
def reconstruct(features, layer):
  # features: a tensor of shape [B, C', H', W']
  # layer: an integer from 0 to NUM_LAYERS - 1
  # return: a tensor of shape [B, C, H, W]
  image = features
  for i in reversed(range(layer + 1)):
    if isinstance(vgg[i], nn.Conv2d):
      image = F.conv_transpose2d(image, vgg[i].weight, bias=vgg[i].bias)
    elif isinstance(vgg[i], nn.ReLU):
      image = F.relu(image)
    elif isinstance(vgg[i], nn.MaxPool2d):
      image = F.interpolate(image, scale_factor=vgg[i].kernel_size)
    else:
      raise NotImplementedError("Unknown layer type")
  return image

# Define the encoder-decoder architecture with skip connections and residual blocks for translation network
class EncoderDecoder(nn.Module):
  def __init__(self, layer):
    super(EncoderDecoder, self).__init__()
    self.layer = layer # the layer index of VGG to use as input and output

    # Define the encoder part
    self.enc_conv1 = nn.Conv2d(64 * (2 ** layer), 64 * (2 ** (layer + 1)), kernel_size=3, stride=2, padding=1)
    self.enc_bn1 = nn.BatchNorm2d(64 * (2 ** (layer + 1)))
    self.enc_conv2 = nn.Conv2d(64 * (2 ** (layer + 1)), 64 * (2 ** (layer + 2)), kernel_size=3, stride=2, padding=1)
    self.enc_bn2 = nn.BatchNorm2d(64 * (2 ** (layer + 2)))

    # Define the decoder part
    self.dec_conv1 = nn.ConvTranspose2d(64 * (2 ** (layer + 2)), 64 * (2 ** (layer + 1)), kernel_size=3, stride=2, padding=1, output_padding=1)
    self.dec_bn1 = nn.BatchNorm2d(64 * (2 ** (layer + 1)))
    self.dec_conv2 = nn.ConvTranspose2d(64 * (2 ** (layer + 1)), 64 * (2 ** layer), kernel_size=3, stride=2, padding=1, output_padding=1)
    self.dec_bn2 = nn.BatchNorm2d(64 * (2 ** layer))

    # Define the residual blocks part
    self.res_blocks = nn.Sequential(
      ResBlock(64 * (2 ** (layer + 2))),
      ResBlock(64 * (2 ** (layer + 2))),
      ResBlock(64 * (2 ** (layer + 2))),
      ResBlock(64 * (2 ** (layer + 2)))
    )

    # Define the residual block module
    class ResBlock(nn.Module):
      def __init__(self, channels):
        super(ResBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)

      def forward(self, x):
        # x: a tensor of shape [B, C, H, W]
        # return: a tensor of shape [B, C, H, W]
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = out + x # skip connection
        out = F.relu(out)
        return out

  def forward(self, x):
    # x: a tensor of shape [B, C', H', W']
    # return: a tensor of shape [B, C', H', W']
    # Encode the input features
    enc = F.relu(self.enc_bn1(self.enc_conv1(x)))
    enc = F.relu(self.enc_bn2(self.enc_conv2(enc)))

    # Apply the residual blocks
    res = self.res_blocks(enc)

    # Decode the output features
    dec = F.relu(self.dec_bn1(self.dec_conv1(res)))
    dec = F.relu(self.dec_bn2(self.dec_conv2(dec)))

    # Return the translated features
    return dec

# Define the PatchGAN architecture for discriminator network
class PatchGAN(nn.Module):
  def __init__(self, layer):
    super(PatchGAN, self).__init__()
    self.layer = layer # the layer index of VGG to use as input

    # Define the convolutional layers
    self.conv1 = nn.Conv2d(64 * (2 ** layer), 64 * (2 ** (layer + 1)), kernel_size=4, stride=2, padding=1)
    self.conv2 = nn.Conv2d(64 * (2 ** (layer + 1)), 64 * (2 ** (layer + 2)), kernel_size=4, stride=2, padding=1)
    self.conv3 = nn.Conv2d(64 * (2 ** (layer + 2)), 64 * (2 ** (layer + 3)), kernel_size=4, stride=2, padding=1)
    self.conv4 = nn.Conv2d(64 * (2 ** (layer + 3)), 64 * (2 ** (layer + 4)), kernel_size=4, stride=1, padding=1)
    self.conv5 = nn.Conv2d(64 * (2 ** (layer + 4)), 1, kernel_size=4, stride=1, padding=1)

  def forward(self, x):
    # x: a tensor of shape [B, C', H', W']
    # return: a tensor of shape [B, 1, H'', W'']
    out = F.leaky_relu(self.conv1(x), negative_slope=0.2)
    out = F.leaky_relu(self.conv2(out), negative_slope=0.2)
    out = F.leaky_relu(self.conv3(out), negative_slope=0.2)
    out = F.leaky_relu(self.conv4(out), negative_slope=0.2)
    out = torch.sigmoid(self.conv5(out))
    return out

# Define a function to compute the adversarial loss
def L_adv(f_x', f_y', f_x'', f_y'', D_X[l], D_Y[l]):
  # f_x': a tensor of shape [B, C', H', W'], translated features from X to Y at layer l
  # f_y': a tensor of shape [B, C', H', W'], translated features from Y to X at layer l
  # f_x'': a tensor of shape [B, C', H', W'], reconstructed features from Y to X at layer l
  # f_y'': a tensor of shape [B, C', H', W'], reconstructed features from X to Y at layer l
  # D_X[l]: the discriminator network for domain X at layer l
  # D_Y[l]: the discriminator network for domain Y at layer l
  # return: a scalar tensor

  # Compute the logits for real and fake features
  logit_x_real = D_X[l](f_x) # real features from X
  logit_x_fake = D_X[l](f_y') # fake features from Y to X
  logit_y_real = D_Y[l](f_y) # real features from Y
  logit_y_fake = D_Y[l](f_x') # fake features from X to Y

  # Compute the binary cross entropy loss for real and fake features
  bce_x_real = F.binary_cross_entropy(log