---
title: 2306.00360v1 How Do ConvNets Understand Image Intensity?
date: 2023-06-01
---

# [How Do ConvNets Understand Image Intensity?](http://arxiv.org/abs/2306.00360v1)

authors: Jackson Kaunismaa, Michael Guerzhoy


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00360.pdf "ABSTRACT arXiv:2306.00360v1 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2306.00360 "[2306.00360] How Do ConvNets Understand Image Intensity? - arXiv.org"
[3]: https://arxiv.org/pdf/2305.00360v1.pdf "BSTRACT arXiv:2305.00360v1 [math.PR] 30 Apr 2023"
[4]: https://arxiv-export2.library.cornell.edu/abs/2304.00360v1 "[2304.00360v1] On a conjecture on a series of convergence rate $\\frac{1 ..."

Here is a summary of the paper:

- **What**: The paper explores how a Convolutional Neural Network (ConvNet) classifies images when color/image intensity is important, in addition to shape.
- **Why**: The paper aims to understand how ConvNets use image intensity as an important cue for object recognition, especially in scenarios where intensity is informative or crucial, such as medical imaging, bird species classification, and landscape scene recognition.
- **How**: The paper generates synthetic images that can only be classified by paying attention to intensity and shape, and uses visualization methods to show how the ConvNet relies on intensity information. The paper also compares the performance of the ConvNet with and without shape cues, and with different network architectures.

## Main Contributions

The paper claims to make the following contributions:

- It introduces a synthetic dataset of greyscale images that can only be classified by paying attention to intensity and shape, and shows that a ConvNet can achieve high accuracy on this task.
- It uses visualization methods to demonstrate how the ConvNet uses image intensity as an important cue for object recognition, and how different layers of the network respond to intensity variations.
- It compares the performance of the ConvNet with and without shape cues, and with different network architectures, and discusses the implications for understanding how ConvNets understand image intensity.

## Method Summary

The method section of the paper consists of three parts:

- The first part describes how the synthetic dataset of greyscale images is generated, and how the class labels are assigned based on a non-monotonic function of the intensity of the object in the image.
- The second part explains how the ConvNet is trained and tested on the synthetic dataset, and how different network architectures and hyperparameters are chosen and evaluated.
- The third part presents the visualization methods used to show how the ConvNet uses image intensity as an important cue for object recognition, and how different layers of the network respond to intensity variations. The visualization methods include gradient-based saliency maps, occlusion sensitivity maps, and feature map activations.

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# Define constants
NUM_CLASSES = 3 # Number of classes in the synthetic dataset
IMAGE_SIZE = 64 # Size of the image in pixels
INTENSITY_RANGES = [[0, 30], [120, 150], [210, 240]] # Intensity ranges for each class
NOISE_LEVELS = [0.1, 0.2, 0.4] # Noise levels for each scale
NUM_IMAGES = 250000 # Number of images in the dataset
TRAIN_SPLIT = 0.8 # Fraction of images for training
BATCH_SIZE = 64 # Batch size for training and testing
EPOCHS = 10 # Number of epochs for training
LEARNING_RATE = 0.001 # Learning rate for the optimizer
MODEL_ARCHITECTURE = "large" # Choose between "large" and "small" network architectures

# Define functions
def generate_image():
    # Generate a random image with a single object of uniform intensity and additive noise at multiple scales

    # Choose a random class label
    label = np.random.randint(NUM_CLASSES)

    # Choose a random intensity value within the range of the class label
    intensity = np.random.randint(INTENSITY_RANGES[label][0], INTENSITY_RANGES[label][1] + 1)

    # Create an empty image array
    image = np.zeros((IMAGE_SIZE, IMAGE_SIZE))

    # Choose a random position and size for the object
    x = np.random.randint(IMAGE_SIZE)
    y = np.random.randint(IMAGE_SIZE)
    size = np.random.randint(10, IMAGE_SIZE // 2)

    # Fill the image array with the intensity value within the object region
    image[max(0, x - size):min(IMAGE_SIZE, x + size), max(0, y - size):min(IMAGE_SIZE, y + size)] = intensity

    # Add noise at multiple scales to the image array
    for noise_level in NOISE_LEVELS:
        noise = np.random.normal(0, noise_level * 255, (IMAGE_SIZE, IMAGE_SIZE))
        image += noise

    # Clip the image array to the range [0, 255]
    image = np.clip(image, 0, 255)

    # Normalize the image array to the range [0, 1]
    image = image / 255.0

    return image, label

def generate_dataset(num_images):
    # Generate a dataset of images and labels

    # Create empty arrays for images and labels
    images = np.zeros((num_images, IMAGE_SIZE, IMAGE_SIZE))
    labels = np.zeros((num_images))

    # Loop over the number of images
    for i in range(num_images):
        # Generate an image and a label
        image, label = generate_image()

        # Store the image and the label in the arrays
        images[i] = image
        labels[i] = label

    return images, labels

def create_model(architecture):
    # Create a ConvNet model based on the architecture

    # Create an input layer for the images
    inputs = tf.keras.layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE))

    # Create a convolutional layer with 16 filters and a ReLU activation function
    x = tf.keras.layers.Conv2D(16, (3, 3), padding="same", activation="relu")(inputs)

    if architecture == "large":
        # Create another convolutional layer with 16 filters and a ReLU activation function
        x = tf.keras.layers.Conv2D(16, (3, 3), padding="same", activation="relu")(x)

    # Create a max pooling layer with a pool size of (2, 2)
    x = tf.keras.layers.MaxPooling2D((2, 2))(x)

    if architecture == "large":
        # Create another convolutional layer with 32 filters and a ReLU activation function
        x = tf.keras.layers.Conv2D(32, (3, 3), padding="same", activation="relu")(x)

        # Create another convolutional layer with 32 filters and a ReLU activation function
        x = tf.keras.layers.Conv2D(32, (3, 3), padding="same", activation="relu")(x)

        # Create another max pooling layer with a pool size of (2, 2)
        x = tf.keras.layers.MaxPooling2D((2, 2))(x)

        # Create another convolutional layer with 64 filters and a ReLU activation function
        x = tf.keras.layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)

        # Create another convolutional layer with 64 filters and a ReLU activation function
        x = tf.keras.layers.Conv2D(64, (3, 3), padding="same", activation="relu")(x)

        # Create another max pooling layer with a pool size of (2, 2)
        x = tf.keras.layers.MaxPooling2D((2, 2))(x)

    # Create a flatten layer to convert the feature maps into a vector
    x = tf.keras.layers.Flatten()(x)

    # Create a dense layer with 128 units and a ReLU activation function
    x = tf.keras.layers.Dense(128, activation="relu")(x)

    # Create a dropout layer with a rate of 0.5 to prevent overfitting
    x = tf.keras.layers.Dropout(0.5)(x)

    # Create an output layer with NUM_CLASSES units and a softmax activation function
    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation="softmax")(x)

    # Create a model object with the inputs and outputs
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    return model

def train_model(model, images, labels):
    # Train the model on the images and labels

    # Split the images and labels into training and testing sets
    train_images, test_images, train_labels, test_labels = train_test_split(images, labels, train_size=TRAIN_SPLIT)

    # Compile the model with a categorical crossentropy loss function, an Adam optimizer, and an accuracy metric
    model.compile(loss="sparse_categorical_crossentropy", optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), metrics=["accuracy"])

    # Fit the model on the training set with the batch size and the number of epochs
    model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)

    # Evaluate the model on the testing set and print the results
    test_loss, test_acc = model.evaluate(test_images, test_labels)
    print("Test loss:", test_loss)
    print("Test accuracy:", test_acc)

def visualize_model(model, image):
    # Visualize how the model uses image intensity as an important cue for object recognition

    # Predict the class label for the image
    label = np.argmax(model.predict(image[np.newaxis]))

    # Create a figure with four subplots
    fig, axes = plt.subplots(2, 2)

    # Plot the original image in the first subplot
    axes[0][0].imshow(image, cmap="gray")
    axes[0][0].set_title("Original image")

    # Compute the gradient of the output with respect to the input
    gradient = tf.gradients(model.output[:, label], model.input)[0]

    # Evaluate the gradient for the image
    gradient_value = gradient.eval(session=tf.keras.backend.get_session(), feed_dict={model.input: image[np.newaxis]})

    # Reshape and normalize the gradient value
    gradient_value = gradient_value.reshape((IMAGE_SIZE, IMAGE_SIZE))
    gradient_value = (gradient_value - np.min(gradient_value)) / (np.max(gradient_value) - np.min(gradient_value))

    # Plot the gradient value as a saliency map in the second subplot
    axes[0][1].imshow(gradient_value, cmap="gray")
    axes[0][1].set_title("Saliency map")

    # Define a function to compute the occlusion sensitivity for a given patch size
    def occlusion_sensitivity(patch_size):
        # Create an empty array for the occlusion sensitivity map
        sensitivity_map = np.zeros((IMAGE_SIZE // patch_size, IMAGE_SIZE // patch_size))

        # Loop over the rows and columns of the sensitivity map
        for i in range(sensitivity_map.shape[0]):
            for j in range(sensitivity_map.shape[1]):
                # Create a copy of the image
                occluded_image = image.copy()

                # Replace a patch of the image with zeros
                occluded_image[i * patch_size:(i + 1) * patch_size, j * patch_size:(j + 1) * patch_size] = 0

                # Predict the class probability for the occluded image
                probability = model.predict(occluded_image[np.newaxis])[0][label]

                # Store the probability in the sensitivity map
                sensitivity_map[i][j] = probability

        return sensitivity_map

    # Compute the occlusion sensitivity for a patch size of 8 pixels
    sensitivity_map_8 = occlusion_sensitivity(8)

    # Plot the occlusion sensitivity map in the third subplot
    axes[1][0].imshow(sensitivity_map_8, cmap="gray")
    axes[1][0].set_title("Occlusion sensitivity (patch
```