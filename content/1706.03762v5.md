---
title: 1706.03762v5 Attention Is All You Need
date: 2017-06-04
---

# [Attention Is All You Need](http://arxiv.org/abs/1706.03762v5)

authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin


## What, Why and How

[1]: https://arxiv.org/abs/1706.03762v5 "[1706.03762v5] Attention Is All You Need - arXiv.org"
[2]: https://arxiv.org/pdf/1706.03762v5.pdf "Attention Is All You Need - arXiv.org"
[3]: https://blog.csdn.net/chengshunhang/article/details/110350041 "【论文解读】Attention is All you need - CSDN博客"
[4]: http://export.arxiv.org/abs/1706.03762v5 "[1706.03762v5] Attention Is All You Need - export.arxiv.org"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a new network architecture, called the Transformer, for sequence transduction tasks such as machine translation and parsing. The Transformer is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely[^1^][1].
- **Why**: The paper argues that attention mechanisms are more effective than recurrent or convolutional networks for modeling long-range dependencies and capturing global structure in sequences. Attention mechanisms also allow for more parallelization and faster training[^1^][1].
- **How**: The paper introduces two types of attention mechanisms: scaled dot-product attention and multi-head attention. The former computes the similarity between a query and a set of key-value pairs, and the latter applies multiple attention functions in parallel to different linear projections of the query, keys, and values. The paper also proposes a novel way of encoding the position information of the tokens in a sequence using sinusoidal functions. The paper then describes how to build an encoder-decoder network using these components, and evaluates it on two machine translation tasks and one parsing task[^1^][1].

## Main Contributions

The paper makes the following contributions:

- It proposes a new network architecture, the Transformer, that relies entirely on attention mechanisms for sequence transduction tasks.
- It introduces two novel types of attention mechanisms: scaled dot-product attention and multi-head attention, and shows how they can be used to model both local and global dependencies in sequences.
- It proposes a simple and effective way of encoding the position information of the tokens in a sequence using sinusoidal functions, which avoids the need for learned embeddings or convolutions.
- It achieves state-of-the-art results on two machine translation tasks (WMT 2014 English-to-German and English-to-French) and one parsing task (English constituency parsing), while being more parallelizable and requiring significantly less time to train than previous models.

## Method Summary

[1]: https://arxiv.org/abs/1706.03762 "[1706.03762] Attention Is All You Need - arXiv.org"
[2]: https://arxiv.org/pdf/1706.03762v5.pdf "Attention Is All You Need - arXiv.org"
[3]: http://export.arxiv.org/abs/1706.03762v5 "[1706.03762v5] Attention Is All You Need - export.arxiv.org"

Here is a summary of the method section of the paper:

- The paper describes the Transformer, a network architecture that consists of an encoder and a decoder, each composed of a stack of six identical layers[^1^][2].
- Each layer in the encoder and decoder has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections and layer normalization are applied around each sub-layer[^1^][2].
- The paper introduces scaled dot-product attention, which computes the weighted sum of a set of values based on their relevance to a query. The relevance is measured by the dot product of the query and the corresponding key, scaled by the square root of the dimensionality[^1^][2].
- The paper also introduces multi-head attention, which applies multiple attention functions in parallel to different linear projections of the query, keys, and values. This allows the model to jointly attend to information from different representation subspaces at different positions[^1^][2].
- The paper proposes a novel way of encoding the position information of the tokens in a sequence using sinusoidal functions of different frequencies. This allows the model to learn relative position embeddings without requiring any extra parameters or convolutions[^1^][2].
- The paper describes how to build an encoder-decoder network using these components, and how to apply masking and dropout techniques to prevent overfitting and improve generalization[^1^][2].
- The paper also details the hyperparameters, optimization method, regularization techniques, and evaluation metrics used for training and testing the Transformer on various tasks[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Transformer model
class Transformer(nn.Module):
  def __init__(self, num_layers, num_heads, d_model, d_ff, dropout):
    # Initialize the encoder and decoder stacks
    self.encoder = Encoder(num_layers, num_heads, d_model, d_ff, dropout)
    self.decoder = Decoder(num_layers, num_heads, d_model, d_ff, dropout)
    # Initialize the embeddings for the source and target tokens
    self.src_embed = nn.Embedding(src_vocab_size, d_model)
    self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)
    # Initialize the position encodings using sinusoidal functions
    self.pos_enc = PositionalEncoding(d_model)
    # Initialize the output linear layer
    self.linear = nn.Linear(d_model, tgt_vocab_size)

  def forward(self, src, tgt):
    # Encode the source sequence using the encoder stack
    src_mask = generate_src_mask(src) # Mask out padding tokens
    src_embed = self.src_embed(src) # Embed the source tokens
    src_embed = self.pos_enc(src_embed) # Add position encodings
    encoder_output = self.encoder(src_embed, src_mask) # Apply the encoder layers
    # Decode the target sequence using the decoder stack
    tgt_mask = generate_tgt_mask(tgt) # Mask out padding and future tokens
    tgt_embed = self.tgt_embed(tgt) # Embed the target tokens
    tgt_embed = self.pos_enc(tgt_embed) # Add position encodings
    decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask) # Apply the decoder layers
    # Predict the output probabilities using the linear layer
    output = self.linear(decoder_output) # Project to the target vocabulary size
    output = F.log_softmax(output, dim=-1) # Apply log softmax
    return output

# Define the encoder stack
class Encoder(nn.Module):
  def __init__(self, num_layers, num_heads, d_model, d_ff, dropout):
    # Initialize a list of encoder layers
    self.layers = nn.ModuleList([EncoderLayer(num_heads, d_model, d_ff, dropout) for _ in range(num_layers)])

  def forward(self, x, mask):
    # Apply each encoder layer to the input
    for layer in self.layers:
      x = layer(x, mask)
    return x

# Define the encoder layer
class EncoderLayer(nn.Module):
  def __init__(self, num_heads, d_model, d_ff, dropout):
    # Initialize a multi-head self-attention sub-layer
    self.self_attn = MultiHeadAttention(num_heads, d_model)
    # Initialize a feed-forward sub-layer
    self.feed_forward = FeedForward(d_model, d_ff)
    # Initialize two layer normalization sub-layers
    self.norm1 = LayerNorm(d_model)
    self.norm2 = LayerNorm(d_model)
    # Initialize two dropout sub-layers
    self.dropout1 = nn.Dropout(dropout)
    self.dropout2 = nn.Dropout(dropout)

  def forward(self, x, mask):
    # Apply multi-head self-attention with residual connection and layer normalization
    attn_output = self.self_attn(x, x, x, mask)
    attn_output = self.dropout1(attn_output)
    x = self.norm1(x + attn_output)
    # Apply feed-forward with residual connection and layer normalization
    ff_output = self.feed_forward(x)
    ff_output = self.dropout2(ff_output)
    x = self.norm2(x + ff_output)
    return x

# Define the decoder stack
class Decoder(nn.Module):
  def __init__(self, num_layers, num_heads, d_model, d_ff, dropout):
     # Initialize a list of decoder layers
     self.layers = nn.ModuleList([DecoderLayer(num_heads, d_model,d_ff ,dropout) for _ in range(num_layers)])

  def forward(self,x ,encoder_output ,src_mask ,tgt_mask ):
     # Apply each decoder layer to the input and the encoder output 
     for layer in self.layers:
       x=layer(x ,encoder_output ,src_mask ,tgt_mask )
     return x

# Define the decoder layer 
class DecoderLayer(nn.Module):
  def __init__(self,num_heads ,d_model ,d_ff ,dropout ):
     # Initialize a masked multi-head self-attention sub-layer 
     self.masked_self_attn=MultiHeadAttention(num_heads ,d_model )
     # Initialize a multi-head cross-attention sub-layer 
     self.cross_attn=MultiHeadAttention(num_heads ,d_model )
     # Initialize a feed-forward sub-layer 
     self.feed_forward=FeedForward(d_model ,d_ff )
     # Initialize three layer normalization sub-layers 
     self.norm1=LayerNorm(d_model )
     self.norm2=LayerNorm(d_model )
     self.norm3=LayerNorm(d_model )
     # Initialize three dropout sub-layers 
     self.dropout1=nn.Dropout(dropout )
     self.dropout2=nn.Dropout(dropout )
     self.dropout3=nn.Dropout(dropout )

  def forward(self,x ,encoder_output ,src_mask ,tgt_mask ):
     # Apply masked multi-head self-attention with residual connection and layer normalization 
     attn_output=self.masked_self_attn(x ,x ,x ,tgt_mask )
     attn_output=self.dropout1(attn_output )
     x=self.norm1(x +attn_output )
     # Apply multi-head cross-attention with residual connection and layer normalization 
     attn_output=self.cross_attn(x ,encoder_output ,encoder_output ,src_mask )
     attn_output=self.dropout2(attn_output )
     x=self.norm2(x +attn_output )
     # Apply feed-forward with residual connection and layer normalization 
     ff_output=self.feed_forward(x )
     ff_output=self.dropout3(ff_output )
     x=self.norm3(x +ff_output )
     return x

# Define the multi-head attention sub-layer
class MultiHeadAttention(nn.Module):
  def __init__(self, num_heads, d_model):
    # Initialize the projection matrices for queries, keys, and values
    self.query_proj = nn.Linear(d_model, d_model)
    self.key_proj = nn.Linear(d_model, d_model)
    self.value_proj = nn.Linear(d_model, d_model)
    # Initialize the output projection matrix
    self.output_proj = nn.Linear(d_model, d_model)
    # Initialize the number of heads and the dimension per head
    self.num_heads = num_heads
    self.d_k = d_model // num_heads

  def forward(self, query, key, value, mask):
    # Project the queries, keys, and values using the linear layers
    query = self.query_proj(query) # shape: (batch_size, seq_len, d_model)
    key = self.key_proj(key) # shape: (batch_size, seq_len, d_model)
    value = self.value_proj(value) # shape: (batch_size, seq_len, d_model)
    # Split the projections into multiple heads and transpose
    query = query.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2) # shape: (batch_size, num_heads, seq_len, d_k)
    key = key.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2) # shape: (batch_size, num_heads, seq_len, d_k)
    value = value.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2) # shape: (batch_size, num_heads, seq_len, d_k)
    # Compute the scaled dot-product attention for each head
    scores = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(d_k) # shape: (batch_size,num_heads ,seq_len ,seq_len )
    if mask is not None:
      scores = scores.masked_fill(mask == 0,-1e9 ) # apply the mask to the scores
    weights = F.softmax(scores,dim=-1 ) # shape: (batch_size,num_heads ,seq_len ,seq_len )
    output = torch.matmul(weights,value ) # shape: (batch_size,num_heads ,seq_len ,d_k )
    # Concatenate and project the outputs of the heads
    output = output.transpose(1 ,2 ).contiguous().view(batch_size ,seq_len ,-1 )# shape: (batch_size ,seq_len ,d_model )
    output = self.output_proj(output )# shape: (batch_size ,seq_len ,d_model )
    return output

# Define the feed-forward sub-layer
class FeedForward(nn.Module):
  def __init__(self,d_model,d_ff ):
    # Initialize two linear layers with ReLU activation in between
    self.linear1=nn.Linear(d_model,d_ff )
    self.linear2=nn.Linear(d_ff,d_model )

  def forward(self,x ):
    # Apply the first linear layer with ReLU activation
    x=self.linear1(x )# shape: (batch_size ,seq_len,d_ff )
    x=F.relu(x )# shape: (batch_size ,seq_len,d_ff )
    # Apply the second linear layer
    x=self.linear2(x )# shape: (batch_size ,seq_len,d_model )
    return x

# Define the positional encoding sub-layer
class PositionalEncoding(nn.Module):
  def __init__(self,d_model ):
    # Initialize a buffer to store the pre-computed position encodings
    self.register_buffer("pe",torch.zeros(max_len,d_model ))

  def forward(self

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Define some constants and hyperparameters
max_len = 512 # maximum length of the input and output sequences
src_vocab_size = 50000 # size of the source vocabulary
tgt_vocab_size = 50000 # size of the target vocabulary
d_model = 512 # dimension of the model
d_ff = 2048 # dimension of the feed-forward network
num_layers = 6 # number of layers in the encoder and decoder
num_heads = 8 # number of heads in the multi-head attention
dropout = 0.1 # dropout rate
batch_size = 64 # batch size for training
num_epochs = 20 # number of epochs for training
learning_rate = 0.0001 # learning rate for optimization

# Define the Transformer model
class Transformer(nn.Module):
  def __init__(self, num_layers, num_heads, d_model, d_ff, dropout):
    # Initialize the encoder and decoder stacks
    self.encoder = Encoder(num_layers, num_heads, d_model, d_ff, dropout)
    self.decoder = Decoder(num_layers, num_heads, d_model, d_ff, dropout)
    # Initialize the embeddings for the source and target tokens
    self.src_embed = nn.Embedding(src_vocab_size, d_model)
    self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model)
    # Initialize the position encodings using sinusoidal functions
    self.pos_enc = PositionalEncoding(d_model)
    # Initialize the output linear layer
    self.linear = nn.Linear(d_model, tgt_vocab_size)

  def forward(self, src, tgt):
    # Encode the source sequence using the encoder stack
    src_mask = generate_src_mask(src) # Mask out padding tokens
    src_embed = self.src_embed(src) # Embed the source tokens
    src_embed = self.pos_enc(src_embed) # Add position encodings
    encoder_output = self.encoder(src_embed, src_mask) # Apply the encoder layers
    # Decode the target sequence using the decoder stack
    tgt_mask = generate_tgt_mask(tgt) # Mask out padding and future tokens
    tgt_embed = self.tgt_embed(tgt) # Embed the target tokens
    tgt_embed = self.pos_enc(tgt_embed) # Add position encodings
    decoder_output = self.decoder(tgt_embed, encoder_output, src_mask, tgt_mask) # Apply the decoder layers
    # Predict the output probabilities using the linear layer
    output = self.linear(decoder_output) # Project to the target vocabulary size
    output = F.log_softmax(output, dim=-1) # Apply log softmax
    return output

# Define the encoder stack
class Encoder(nn.Module):
  def __init__(self, num_layers, num_heads, d_model, d_ff, dropout):
    # Initialize a list of encoder layers
    self.layers = nn.ModuleList([EncoderLayer(num_heads, d_model, d_ff, dropout) for _ in range(num_layers)])

  def forward(self, x, mask):
    # Apply each encoder layer to the input
    for layer in self.layers:
      x = layer(x, mask)
    return x

# Define the encoder layer
class EncoderLayer(nn.Module):
  def __init__(self, num_heads, d_model, d_ff, dropout):
    # Initialize a multi-head self-attention sub-layer
    self.self_attn = MultiHeadAttention(num_heads, d_model)
    # Initialize a feed-forward sub-layer
    self.feed_forward = FeedForward(d_model, d_ff)
    # Initialize two layer normalization sub-layers
    self.norm1 = LayerNorm(d_model)
    self.norm2 = LayerNorm(d_model)
    # Initialize two dropout sub-layers
    self.dropout1 = nn.Dropout(dropout)
    self.dropout2 = nn.Dropout(dropout)

  def forward(self, x, mask):
    # Apply multi-head self-attention with residual connection and layer normalization
    attn_output = self.self_attn(x,x,x ,mask )
attn_output=self.dropout1(attn_output )
x=self.norm1(x +attn_output )
# Apply feed-forward with residual connection and layer normalization 
ff_output=self.feed_forward(x )
ff_output=self.dropout2(ff_output )
x=self.norm2(x +ff_output )
return x

# Define the decoder stack 
class Decoder(nn.Module):
def __init__(self,num_layers ,num_heads ,d_model ,d_ff ,dropout ):
# Initialize a list of decoder layers 
self.layers=nn.ModuleList([DecoderLayer(num_heads ,d_model ,d_ff ,dropout )for _in range(num_layers )])

def forward(self,x ,encoder_output ,src_mask ,tgt_mask ):
# Apply each decoder layer to the input and the encoder output 
for layer in self.layers:
x=layer(x ,encoder_output ,src_mask ,tgt_mask )
return x

# Define the decoder layer 
class DecoderLayer(nn.Module):
def __init__(self,num_heads ,d_model ,d_ff ,dropout ):
# Initialize a masked multi-head self-attention sub-layer 
self.masked_self_attn=MultiHeadAttention(num_heads ,d_model )
# Initialize a multi-head cross-attention sub-layer 
self.cross_attn=MultiHeadAttention(num_heads ,d_model )
# Initialize a feed-forward sub-layer 
self.feed_forward=FeedForward(d_model ,d_ff )
# Initialize three layer normalization sub-layers 
self.norm1=LayerNorm(d_model )
self.norm2=LayerNorm(d_model )
self.norm3=LayerNorm(d_model )
# Initialize three dropout sub-layers 
self.dropout1=nn.Dropout(dropout )
self.dropout2=nn.Dropout(dropout )
self.dropout3=nn.Dropout(dropout )

def forward(self,x ,encoder_output ,src_mask ,tgt_mask ):
# Apply masked multi-head self-attention with residual connection and layer normalization 
attn_output=self.masked_self_attn(x ,x ,x ,tgt_mask )
attn_output=self.dropout1(attn_output )
x=self.norm1(x +attn_output )
# Apply multi-head cross-attention with residual connection and layer normalization 
attn_output=self.cross_attn(x ,encoder_output ,encoder_output ,src_mask )
attn_output=self.dropout2(attn_output )
x=self.norm2(x +attn_output )
# Apply feed-forward with residual connection and layer normalization 
ff_output=self.feed_forward(x )
ff_output=self.dropout3(ff_output )
x=self.norm3(x +ff_output )
return x

# Define the multi-head attention sub-layer
class MultiHeadAttention(nn.Module):
  def __init__(self, num_heads, d_model):
    # Initialize the projection matrices for queries, keys, and values
    self.query_proj = nn.Linear(d_model, d_model)
    self.key_proj = nn.Linear(d_model, d_model)
    self.value_proj = nn.Linear(d_model, d_model)
    # Initialize the output projection matrix
    self.output_proj = nn.Linear(d_model, d_model)
    # Initialize the number of heads and the dimension per head
    self.num_heads = num_heads
    self.d_k = d_model // num_heads

  def forward(self, query, key, value, mask):
    # Project the queries, keys, and values using the linear layers
    query = self.query_proj(query) # shape: (batch_size, seq_len, d_model)
    key = self.key_proj(key) # shape: (batch_size, seq_len, d_model)
    value = self.value_proj(value) # shape: (batch_size, seq_len, d_model)
    # Split the projections into multiple heads and transpose
    query = query.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2) # shape: (batch_size, num_heads, seq_len, d_k)
    key = key.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2) # shape: (batch_size, num_heads, seq_len, d_k)
    value = value.view(batch_size, seq_len, num_heads, d_k).transpose(1, 2) # shape: (batch_size,num_heads ,seq_len,d_k )
# Compute the scaled dot-product attention for each head 
scores=torch.matmul(query,key.transpose(-2 ,-1 ))/math.sqrt(d_k )# shape: (batch_size,num_heads ,seq_len ,seq_len )
if mask is not None:
scores=scores.masked_fill(mask ==0 ,-1e9 )# apply the mask to the scores
weights=F.softmax(scores,dim=-1 )# shape: (batch_size,num_heads ,seq_len ,seq_len )
output=torch.matmul(weights,value )# shape: (batch_size,num_heads ,seq_len,d_k )
# Concatenate and project the outputs of the heads 
output=output.transpose(1 ,2 ).contiguous().view(batch_size ,seq_len ,-1 )# shape: (batch_size ,seq_len,d_model )
output=self.output_proj(output )# shape: (batch_size ,seq_len,d_model )
return output

# Define the feed-forward sub-layer
class FeedForward(nn.Module):
  def __init__(self,d_model,d_ff ):
    # Initialize two linear layers with ReLU activation in between
    self.linear1=nn.Linear(d_model,d_ff )
    self.linear2=nn.Linear(d_ff,d_model )

  def forward(self,x ):
    # Apply the first linear layer with ReLU activation
    x=self.linear1(x )# shape: (batch_size ,seq_len,d_ff )
    x=F.relu(x )# shape: (batch_size ,seq_len,d_ff )