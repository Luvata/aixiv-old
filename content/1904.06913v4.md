---
title: 1904.06913v4 Implicit Pairs for Boosting Unpaired Image-to-Image Translation
date: 2019-04-07
---

# [Implicit Pairs for Boosting Unpaired Image-to-Image Translation](http://arxiv.org/abs/1904.06913v4)

authors: Yiftach Ginger, Dov Danon, Hadar Averbuch-Elor, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1904.06913v4 "Implicit Pairs for Boosting Unpaired Image-to-Image Translation"
[2]: https://arxiv.org/abs/1904.06913 "Implicit Pairs for Boosting Unpaired Image-to-Image Translation"
[3]: http://export.arxiv.org/abs/1904.06913 "[1904.06913] Implicit Pairs for Boosting Unpaired Image-to-Image ..."

Here is a summary of the paper:

- **What**: The paper proposes a method for improving unpaired image-to-image translation by injecting implicit pairs into the training data. Implicit pairs are images that are related by some transformation but not explicitly labeled as such. For example, a photo of a person and a sketch of the same person are an implicit pair, but they are not available as a labeled pair in an unpaired dataset. The paper also introduces the concept of pseudo-pairs, which are approximate implicit pairs that can be synthesized in one direction but not in the other. For example, a photo of a person and a cartoon of the same person are a pseudo-pair, but it is easier to generate a cartoon from a photo than vice versa.
- **Why**: The paper aims to address the limitations of existing unpaired image-to-image translation methods, which often suffer from mode collapse, domain gap, and lack of diversity. Mode collapse occurs when the model learns to map all images from one domain to a single image or a few images in the other domain. Domain gap refers to the discrepancy between the distributions of the two domains, which makes it hard for the model to learn a meaningful mapping. Lack of diversity means that the model cannot produce diverse outputs for different inputs or different styles. The paper hypothesizes that implicit pairs can help overcome these challenges by providing additional supervision and regularization for the model, as well as enhancing the compatibility and alignment of the two domains.
- **How**: The paper proposes a framework called Implicit Pairs for Unpaired Image-to-Image Translation (IPUIT), which consists of three components: an implicit pair generator, an implicit pair discriminator, and an image-to-image translator. The implicit pair generator takes an image from one domain and produces an implicit pair in the other domain using a pre-trained model or a heuristic function. The implicit pair discriminator takes an image pair and determines whether it is an implicit pair or not. The image-to-image translator takes an image from one domain and produces an output image in the other domain using a cycle-consistent adversarial network. The paper trains the three components jointly using a combination of losses: cycle-consistency loss, adversarial loss, identity loss, and implicit pair loss. The paper evaluates the proposed method on several image-to-image translation tasks, such as photo-to-sketch, photo-to-cartoon, and day-to-night, and compares it with state-of-the-art baselines. The paper shows that IPUIT outperforms the baselines on various metrics, such as Fr√©chet Inception Distance (FID), Learned Perceptual Image Patch Similarity (LPIPS), Structural Similarity Index (SSIM), and user study[^1^][1]. The paper also demonstrates that pseudo-pairs can further boost the performance of IPUIT by providing more diverse and realistic implicit pairs[^1^][1].


## Main Contributions

The paper claims to make the following contributions:

- It introduces the concept of implicit pairs and shows how they can improve unpaired image-to-image translation by providing additional supervision and regularization for the model, as well as enhancing the compatibility and alignment of the two domains.
- It proposes a novel framework called IPUIT, which consists of an implicit pair generator, an implicit pair discriminator, and an image-to-image translator. It trains the three components jointly using a combination of losses that leverage the implicit pairs.
- It introduces the concept of pseudo-pairs, which are approximate implicit pairs that can be synthesized in one direction but not in the other. It shows that pseudo-pairs can further boost the performance of IPUIT by providing more diverse and realistic implicit pairs.
- It evaluates the proposed method on several image-to-image translation tasks and compares it with state-of-the-art baselines. It shows that IPUIT outperforms the baselines on various metrics and user study.

## Method Summary

Here is a summary of the method section of the paper:

- The paper proposes a framework called IPUIT, which consists of three components: an implicit pair generator G, an implicit pair discriminator D, and an image-to-image translator T. The implicit pair generator G takes an image x from domain X and produces an implicit pair y in domain Y using a pre-trained model or a heuristic function. The implicit pair discriminator D takes an image pair (x,y) and determines whether it is an implicit pair or not. The image-to-image translator T takes an image x from domain X and produces an output image y' in domain Y using a cycle-consistent adversarial network. The paper trains the three components jointly using a combination of losses: cycle-consistency loss L_cyc, adversarial loss L_adv, identity loss L_idt, and implicit pair loss L_imp. The paper defines the total loss as:

$$L_{total} = \lambda_{cyc}L_{cyc} + \lambda_{adv}L_{adv} + \lambda_{idt}L_{idt} + \lambda_{imp}L_{imp}$$

- The cycle-consistency loss L_cyc ensures that the image-to-image translator T can reconstruct the original image x from the output image y'. It is defined as:

$$L_{cyc}(T,G) = E_{x\sim p_{data}(X)}[||T_Y(T_X(x))-x||_1] + E_{y\sim p_{data}(Y)}[||T_X(T_Y(y))-y||_1]$$

where T_X and T_Y are the translation functions from X to Y and from Y to X, respectively.

- The adversarial loss L_adv ensures that the output image y' is indistinguishable from the real images in domain Y by a domain discriminator D_Y. It also ensures that the implicit pair y is indistinguishable from the real images in domain Y by the implicit pair discriminator D. It is defined as:

$$L_{adv}(T,D,D_Y) = E_{y\sim p_{data}(Y)}[\log D_Y(y)] + E_{x\sim p_{data}(X)}[\log(1-D_Y(T_X(x)))] + E_{y\sim p_{data}(Y)}[\log D(y)] + E_{x\sim p_{data}(X)}[\log(1-D(G(x)))]$$

- The identity loss L_idt ensures that the image-to-image translator T preserves the identity of the input image x when it belongs to the target domain Y. It is defined as:

$$L_{idt}(T) = E_{y\sim p_{data}(Y)}[||T_X(y)-y||_1] + E_{x\sim p_{data}(X)}[||T_Y(x)-x||_1]$$

- The implicit pair loss L_imp ensures that the output image y' is consistent with the implicit pair y generated by G. It is defined as:

$$L_{imp}(T,G) = E_{x\sim p_{data}(X)}[||T_X(x)-G(x)||_1]$$

- The paper sets the hyperparameters $\lambda_{cyc}$, $\lambda_{adv}$, $\lambda_{idt}$, and $\lambda_{imp}$ to 10, 1, 0.5, and 0.5, respectively. It uses Adam optimizer with a learning rate of 0.0002 and a batch size of 1. It trains the model for 200 epochs on each dataset.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the implicit pair generator G
G = PretrainedModel() or HeuristicFunction()

# Define the implicit pair discriminator D
D = ConvolutionalNetwork()

# Define the domain discriminator D_Y
D_Y = ConvolutionalNetwork()

# Define the image-to-image translator T
T_X = CycleGAN() # Translation function from X to Y
T_Y = CycleGAN() # Translation function from Y to X

# Define the losses
L_cyc = L1Loss() # Cycle-consistency loss
L_adv = BCELoss() # Adversarial loss
L_idt = L1Loss() # Identity loss
L_imp = L1Loss() # Implicit pair loss

# Define the hyperparameters
lambda_cyc = 10
lambda_adv = 1
lambda_idt = 0.5
lambda_imp = 0.5
lr = 0.0002 # Learning rate
bs = 1 # Batch size
epochs = 200

# Define the optimizer
optimizer = Adam([D.parameters(), D_Y.parameters(), T_X.parameters(), T_Y.parameters()], lr=lr)

# Train the model
for epoch in range(epochs):
  for x in DataLoader(X, batch_size=bs): # Sample a batch of images from domain X
    y = G(x) # Generate an implicit pair for each image in x
    y' = T_X(x) # Translate each image in x to domain Y
    x' = T_Y(y) # Translate each image in y to domain X
    
    # Update D and D_Y by maximizing L_adv
    optimizer.zero_grad()
    L_adv_D_DY = L_adv(T, D, D_Y)
    L_adv_D_DY.backward()
    optimizer.step()
    
    # Update T_X and T_Y by minimizing L_total
    optimizer.zero_grad()
    L_total_TX_TY = lambda_cyc * L_cyc(T, G) + lambda_adv * L_adv(T, D, D_Y) + lambda_idt * L_idt(T) + lambda_imp * L_imp(T, G)
    L_total_TX_TY.backward()
    optimizer.step()
    
  for y in DataLoader(Y, batch_size=bs): # Sample a batch of images from domain Y
    x' = T_Y(y) # Translate each image in y to domain X
    y' = T_X(x') # Translate each image in x' to domain Y
    
    # Update D_Y by maximizing L_adv
    optimizer.zero_grad()
    L_adv_DY = L_adv(T, D, D_Y)
    L_adv_DY.backward()
    optimizer.step()
    
    # Update T_X and T_Y by minimizing L_total
    optimizer.zero_grad()
    L_total_TX_TY = lambda_cyc * L_cyc(T, G) + lambda_adv * L_adv(T, D, D_Y) + lambda_idt * L_idt(T) + lambda_imp * L_imp(T, G)
    L_total_TX_TY.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset

# Define the implicit pair generator G
# Depending on the task, G can be a pre-trained model or a heuristic function
# For example, for photo-to-sketch task, G can be a pre-trained SketchGAN model
# For photo-to-cartoon task, G can be a heuristic function that applies some filters and color adjustments to the input photo
class ImplicitPairGenerator(nn.Module):
  def __init__(self):
    super(ImplicitPairGenerator, self).__init__()
    # Define the model or the function here
  
  def forward(self, x):
    # Generate an implicit pair for each image in x
    y = self.model(x) or self.function(x)
    return y

# Define the implicit pair discriminator D
# D is a convolutional network that takes an image pair and outputs a scalar probability
# D can have a similar architecture as the domain discriminator D_Y
class ImplicitPairDiscriminator(nn.Module):
  def __init__(self):
    super(ImplicitPairDiscriminator, self).__init__()
    # Define the convolutional layers here
    # For example:
    self.conv1 = nn.Conv2d(6, 64, 4, 2, 1) # Input channels: 6 (3 for each image), output channels: 64, kernel size: 4, stride: 2, padding: 1
    self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)
    self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)
    self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)
    self.conv5 = nn.Conv2d(512, 1, 4, 1, 0) # Output channels: 1 (scalar probability)
    
    # Define the activation functions here
    # For example:
    self.leaky_relu = nn.LeakyReLU(0.2)
    self.sigmoid = nn.Sigmoid()
  
  def forward(self, x, y):
    # Concatenate the image pair along the channel dimension
    xy = torch.cat([x,y], dim=1) # Shape: (batch_size, 6, height, width)
    
    # Pass the image pair through the convolutional layers
    # Apply the activation functions after each layer except the last one
    # For example:
    xy = self.leaky_relu(self.conv1(xy)) # Shape: (batch_size, 64, height/2, width/2)
    xy = self.leaky_relu(self.conv2(xy)) # Shape: (batch_size, 128, height/4, width/4)
    xy = self.leaky_relu(self.conv3(xy)) # Shape: (batch_size, 256, height/8, width/8)
    xy = self.leaky_relu(self.conv4(xy)) # Shape: (batch_size, 512, height/16, width/16)
    xy = self.sigmoid(self.conv5(xy)) # Shape: (batch_size, 1 , height/16 -3 , width/16 -3)
    
    # Reshape the output to a scalar probability for each image pair
    xy = xy.view(-1) # Shape: (batch_size,)
    
    return xy

# Define the domain discriminator D_Y
# D_Y is a convolutional network that takes an image and outputs a scalar probability
# D_Y can have a similar architecture as the implicit pair discriminator D but with different input channels
class DomainDiscriminator(nn.Module):
  def __init__(self):
    super(DomainDiscriminator,self).__init__()
    # Define the convolutional layers here
    # For example:
    self.conv1 = nn.Conv2d(3 ,64 ,4 ,2 ,1) # Input channels: 3 (RGB image), output channels: 64 , kernel size: 4 , stride: 2 , padding: 1
    self.conv2 = nn.Conv2d(64 ,128 ,4 ,2 ,1)
    self.conv3 = nn.Conv2d(128 ,256 ,4 ,2 ,1)
    self.conv4 = nn.Conv2d(256 ,512 ,4 ,2 ,1)
    self.conv5 = nn.Conv2d(512 ,1 ,4 ,1 ,0) # Output channels: 1 (scalar probability)
    
    # Define the activation functions here
    # For example:
    self.leaky_relu = nn.LeakyReLU(0.2)
    self.sigmoid = nn.Sigmoid()
  
  def forward(self, x):
    # Pass the image through the convolutional layers
    # Apply the activation functions after each layer except the last one
    # For example:
    x = self.leaky_relu(self.conv1(x)) # Shape: (batch_size, 64, height/2, width/2)
    x = self.leaky_relu(self.conv2(x)) # Shape: (batch_size, 128, height/4, width/4)
    x = self.leaky_relu(self.conv3(x)) # Shape: (batch_size, 256, height/8, width/8)
    x = self.leaky_relu(self.conv4(x)) # Shape: (batch_size, 512, height/16, width/16)
    x = self.sigmoid(self.conv5(x)) # Shape: (batch_size, 1 , height/16 -3 , width/16 -3)
    
    # Reshape the output to a scalar probability for each image
    x = x.view(-1) # Shape: (batch_size,)
    
    return x

# Define the image-to-image translator T
# T is a cycle-consistent adversarial network that consists of two generators and two discriminators
# T_X is the generator that translates images from domain X to domain Y
# T_Y is the generator that translates images from domain Y to domain X
# D_X is the discriminator that distinguishes between real images in domain X and fake images generated by T_Y
# D_Y is the discriminator that distinguishes between real images in domain Y and fake images generated by T_X
# T can have a similar architecture as CycleGAN or other variants
class ImageToImageTranslator(nn.Module):
  def __init__(self):
    super(ImageToImageTranslator,self).__init__()
    # Define the generators and the discriminators here
    # For example:
    self.T_X = ResNetGenerator() # Generator from X to Y
    self.T_Y = ResNetGenerator() # Generator from Y to X
    self.D_X = DomainDiscriminator() # Discriminator for domain X
    self.D_Y = DomainDiscriminator() # Discriminator for domain Y
  
  def forward(self, x, y):
    # Translate the images from one domain to another using the generators
    y' = self.T_X(x) # Translate x to y'
    x' = self.T_Y(y) # Translate y to x'
    
    # Reconstruct the original images using the generators
    x'' = self.T_Y(y') # Reconstruct x from y'
    y'' = self.T_X(x') # Reconstruct y from x'
    
    # Compute the outputs of the discriminators for real and fake images
    D_X_x = self.D_X(x) # Output of D_X for real images in X
    D_X_x' = self.D_X(x') # Output of D_X for fake images in X
    D_Y_y = self.D_Y(y) # Output of D_Y for real images in Y
    D_Y_y' = self.D_Y(y') # Output of D_Y for fake images in Y
    
    return y', x', x'', y'', D_X_x, D_X_x', D_Y_y, D_Y_y'

# Define the losses
L_cyc = nn.L1Loss() # Cycle-consistency loss
L_adv = nn.BCELoss() # Adversarial loss
L_idt = nn.L1Loss() # Identity loss
L_imp = nn.L1Loss() # Implicit pair loss

# Define the hyperparameters
lambda_cyc = 10
lambda_adv = 1
lambda_idt = 0.5
lambda_imp = 0.5
lr = 0.0002 # Learning rate
bs = 1 # Batch size
epochs = 200

# Define the optimizer
G_params = list(G.parameters()) + list(T.parameters()) # Parameters of G and T (generators)
D_params = list(D.parameters()) + list(T.D_X.parameters()) + list(T.D_Y.parameters()) # Parameters of D and T (discriminators)
optimizer_G = optim.Adam(G_params, lr=lr) # Optimizer for G and T (generators)
optimizer_D = optim.Adam(D_params, lr=lr) # Optimizer for D and T (discriminators)

# Load the datasets for domains X and Y
# Apply some transformations such as resizing, cropping, and normalization to the images
# For example:
transform = transforms.Compose([
  transforms.Resize(256),
  transforms.RandomCrop(224),
  transforms.ToTensor(),
  transforms.Normalize((0.5,0.5,0.5),(0.5,0.5