---
title: 2007.12944v1 MRGAN  Multi-Rooted 3D Shape Generation with Unsupervised Part Disentanglement
date: 2020-07-13
---

# [MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part Disentanglement](http://arxiv.org/abs/2007.12944v1)

authors: Rinon Gal, Amit Bermano, Hao Zhang, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2007.12944 "[2007.12944] MRGAN: Multi-Rooted 3D Shape Generation with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2007.12944v1 "MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part ..."
[3]: http://export.arxiv.org/abs/2304.12944v1 "[2304.12944v1] Latent Traversals in Generative Models as Potential Flows"

Here is a summary of the paper:

- **What**: The paper presents MRGAN, a multi-rooted adversarial network that generates 3D point-cloud shapes with unsupervised part disentanglement. The network learns to produce different shape parts from multiple branches of graph convolution layers, and offers control over the shape generation at the part level.
- **Why**: The paper aims to address the challenge of generating realistic and diverse 3D shapes without part-based shape supervision, which is often scarce or expensive to obtain. The paper also aims to enable part-level manipulation of 3D shapes for shape modeling applications.
- **How**: The paper introduces two key ingredients for part-disentangled generation: a root-mixing training strategy that decorrelates the different branches to facilitate disentanglement, and a set of loss terms that encourage convexity, semantic consistency, and root-dropping of the generated parts. The paper evaluates the performance of the network on several 3D shape classes and demonstrates its controllability through part mixing and individual part variation. [^1^][1] [^2^][2]

## Main Contributions

[1]: https://arxiv.org/abs/2007.12944 "[2007.12944] MRGAN: Multi-Rooted 3D Shape Generation with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2007.12944v1 "MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part ..."
[3]: http://export.arxiv.org/abs/2304.12944v1 "[2304.12944v1] Latent Traversals in Generative Models as Potential Flows"

The paper claims the following contributions:

- **A novel multi-rooted adversarial network** that generates 3D point-cloud shapes with unsupervised part disentanglement, without requiring part-based shape supervision or labels.
- **A root-mixing training strategy** that helps decorrelate the different branches of the network to facilitate disentanglement and diversity of the generated parts.
- **A set of loss terms** that encourage convexity, semantic consistency, and root-dropping of the generated parts, as well as adversarial and reconstruction losses for realistic and faithful shape generation.
- **An evaluation of the network** on several 3D shape classes, showing qualitative and quantitative comparisons to previous works and baseline approaches, as well as ablation studies to validate the effectiveness of each component of the network.
- **A demonstration of the controllability** offered by the part-disentangled generation through two applications for shape modeling: part mixing and individual part variation, without receiving segmented shapes as input. [^1^][1] [^2^][2]

## Method Summary

[1]: https://arxiv.org/abs/2007.12944 "[2007.12944] MRGAN: Multi-Rooted 3D Shape Generation with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2007.12944v1 "MRGAN: Multi-Rooted 3D Shape Generation with Unsupervised Part ..."
[3]: http://export.arxiv.org/abs/2304.12944v1 "[2304.12944v1] Latent Traversals in Generative Models as Potential Flows"

Here is a summary of the method section of the paper:

- The paper proposes a **multi-rooted adversarial network** that consists of a generator and a discriminator. The generator has **R** roots, each with a learnable constant input that is modulated by a latent vector drawn from a normal distribution. The roots are grown together into a single object using tree-structured graph convolution layers that produce point clouds. The discriminator is a point-cloud classifier that distinguishes between real and fake shapes.
- The paper introduces a **root-mixing training strategy** that randomly swaps the latent vectors of different roots during training, forcing the network to learn disentangled and diverse parts. The paper also introduces a **root-dropping training strategy** that randomly drops some roots during training, forcing the network to learn robust and complete parts.
- The paper defines a **total loss function** for the network that consists of several terms: an **adversarial loss** that encourages realistic shape generation, a **reconstruction loss** that encourages faithful shape generation, a **convexity loss** that encourages convex part generation, a **semantic consistency loss** that encourages consistent part generation across different shapes, and a **root-dropping loss** that encourages single-part generation per root. [^1^][1] [^2^][2]


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator network with R roots
G = Generator(R)

# Define the discriminator network
D = Discriminator()

# Define the loss functions
L_adv = AdversarialLoss()
L_rec = ReconstructionLoss()
L_conv = ConvexityLoss()
L_sem = SemanticConsistencyLoss()
L_drop = RootDroppingLoss()

# Define the optimizer
optimizer = Adam()

# Define the training loop
for epoch in range(epochs):
  # Sample a batch of real shapes
  X_real = sample_real_shapes()

  # Sample a batch of latent vectors
  Z = sample_latent_vectors(R)

  # Randomly swap some latent vectors across roots
  Z = root_mixing(Z)

  # Randomly drop some roots
  Z, mask = root_dropping(Z)

  # Generate a batch of fake shapes
  X_fake = G(Z)

  # Compute the discriminator outputs for real and fake shapes
  D_real = D(X_real)
  D_fake = D(X_fake)

  # Compute the generator and discriminator losses
  L_G = L_adv(D_fake) + L_rec(X_real, X_fake) + L_conv(X_fake) + L_sem(X_fake) + L_drop(X_fake, mask)
  L_D = L_adv(D_real, D_fake)

  # Update the generator and discriminator parameters
  optimizer.step(L_G, G.parameters())
  optimizer.step(L_D, D.parameters())
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

# Define the hyperparameters
R = 4 # number of roots
N = 2048 # number of points per shape
D = 3 # dimension of points
H = 256 # dimension of hidden features
L = 512 # dimension of latent vectors
B = 32 # batch size
E = 100 # number of epochs
LR = 0.0002 # learning rate
B1 = 0.5 # beta1 for Adam optimizer
B2 = 0.999 # beta2 for Adam optimizer
LAMBDA_REC = 10 # weight for reconstruction loss
LAMBDA_CONV = 0.01 # weight for convexity loss
LAMBDA_SEM = 0.1 # weight for semantic consistency loss
LAMBDA_DROP = 0.1 # weight for root-dropping loss

# Define the generator network with R roots
class Generator(nn.Module):
  def __init__(self, R):
    super(Generator, self).__init__()
    self.R = R # number of roots

    # Define the learnable constants for each root
    self.constants = nn.ParameterList([nn.Parameter(torch.randn(1, H)) for _ in range(R)])

    # Define the AdaIN layers for each root
    self.adains = nn.ModuleList([AdaIN(H, L) for _ in range(R)])

    # Define the graph convolution layers for each level of the tree structure
    self.gconvs_1 = nn.ModuleList([GraphConv(H, H) for _ in range(R)])
    self.gconvs_2 = nn.ModuleList([GraphConv(H, H) for _ in range(R//2)])
    self.gconvs_3 = nn.ModuleList([GraphConv(H, H) for _ in range(R//4)])
    self.gconvs_4 = GraphConv(H, H)

    # Define the point generation layers for each level of the tree structure
    self.pgens_1 = nn.ModuleList([PointGen(H, D) for _ in range(R)])
    self.pgens_2 = nn.ModuleList([PointGen(H, D) for _ in range(R//2)])
    self.pgens_3 = nn.ModuleList([PointGen(H, D) for _ in range(R//4)])
    self.pgens_4 = PointGen(H, D)

  def forward(self, z):
    # z: batch of latent vectors of shape (B, R, L)

    # Initialize the list of point clouds for each level of the tree structure
    pc_1 = []
    pc_2 = []
    pc_3 = []
    pc_4 = None

    # For each root in parallel
    for r in range(self.R):
      # Get the constant input and the latent vector for the current root
      c_r = self.constants[r] # shape (1, H)
      z_r = z[:, r] # shape (B, L)

      # Apply AdaIN to modulate the constant input with the latent vector
      h_r = self.adains[r](c_r, z_r) # shape (B, H)

      # Apply graph convolution to propagate the features to the next level nodes
      h_r = self.gconvs_1[r](h_r) # shape (B, H)

      # Apply point generation to produce a point cloud for the current root
      pc_r = self.pgens_1[r](h_r) # shape (B, N//R, D)

      # Append the point cloud to the list of point clouds for the first level
      pc_1.append(pc_r)

      # If the current root is an even-numbered root (0-based indexing)
      if r % 2 == 0:
        # Get the features of the next root (the sibling of the current root)
        h_s = self.gconvs_1[r+1](self.adains[r+1](self.constants[r+1], z[:, r+1])) # shape (B, H)

        # Concatenate the features of the current root and its sibling along the feature dimension
        h_c = torch.cat([h_r, h_s], dim=1) # shape (B, 2*H)

        # Apply graph convolution to propagate the features to the next level nodes
        h_c = self.gconvs_2[r//2](h_c) # shape (B, H)

        # Apply point generation to produce a point cloud for the current pair of roots
        pc_c = self.pgens_2[r//2](h_c) # shape (B, N//R, D)

        # Append the point cloud to the list of point clouds for the second level
        pc_2.append(pc_c)

        # If the current pair of roots is the first pair
        if r == 0:
          # Get the features of the next pair of roots (the sibling pair of the current pair)
          h_t = self.gconvs_2[1](torch.cat([self.gconvs_1[2](self.adains[2](self.constants[2], z[:, 2])), self.gconvs_1[3](self.adains[3](self.constants[3], z[:, 3]))], dim=1)) # shape (B, H)

          # Concatenate the features of the current pair and its sibling pair along the feature dimension
          h_p = torch.cat([h_c, h_t], dim=1) # shape (B, 2*H)

          # Apply graph convolution to propagate the features to the next level nodes
          h_p = self.gconvs_3[0](h_p) # shape (B, H)

          # Apply point generation to produce a point cloud for the current quartet of roots
          pc_p = self.pgens_3[0](h_p) # shape (B, N//R, D)

          # Append the point cloud to the list of point clouds for the third level
          pc_3.append(pc_p)

          # If the current quartet of roots is the only quartet (R = 4)
          if self.R == 4:
            # Get the features of the next quartet of roots (the sibling quartet of the current quartet)
            h_q = self.gconvs_3[1](torch.cat([self.gconvs_2[2](torch.cat([self.gconvs_1[4](self.adains[4](self.constants[4], z[:, 4])), self.gconvs_1[5](self.adains[5](self.constants[5], z[:, 5]))], dim=1)), self.gconvs_2[3](torch.cat([self.gconvs_1[6](self.adains[6](self.constants[6], z[:, 6])), self.gconvs_1[7](self.adains[7](self.constants[7], z[:, 7]))], dim=1))], dim=1)) # shape (B, H)

            # Concatenate the features of the current quartet and its sibling quartet along the feature dimension
            h_o = torch.cat([h_p, h_q], dim=1) # shape (B, 2*H)

            # Apply graph convolution to propagate the features to the next level nodes
            h_o = self.gconvs_4(h_o) # shape (B, H)

            # Apply point generation to produce a point cloud for the whole object
            pc_o = self.pgens_4(h_o) # shape (B, N//R, D)

            # Set the point cloud for the fourth level as the point cloud for the whole object
            pc_4 = pc_o

    # Concatenate all the point clouds for each level along the point dimension
    pc_1 = torch.cat(pc_1, dim=1) # shape (B, N, D)
    pc_2 = torch.cat(pc_2, dim=1) # shape (B, N, D)
    pc_3 = torch.cat(pc_3, dim=1) # shape (B, N, D)

    # Return all the point clouds for each level
    return pc_1, pc_2, pc_3, pc_4

# Define the AdaIN layer
class AdaIN(nn.Module):
  def __init__(self, in_features, style_dim):
    super(AdaIN, self).__init__()
    self.in_features = in_features # dimension of input features
    self.style_dim = style_dim # dimension of style vector

    # Define the linear layers for computing scale and bias from style vector
    self.fc_scale = nn.Linear(style_dim, in_features)
    self.fc_bias = nn.Linear(style_dim, in_features)

  def forward(self, x, s):
    # x: input features of shape (B, in_features)
    # s: style vector of shape (B, style_dim)

    # Compute scale and bias from style vector
    scale = self.fc_scale(s) # shape (B, in_features)
    bias = self.fc_bias(s) # shape (B, in_features)

    # Normalize input features along feature dimension
    x_norm = F.instance_norm(x.unsqueeze(2), affine