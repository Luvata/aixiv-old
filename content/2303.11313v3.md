---
title: 2303.11313v3 CLIP goes 3D  Leveraging Prompt Tuning for Language Grounded 3D Recognition
date: 2023-03-12
---

# [CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition](http://arxiv.org/abs/2303.11313v3)

authors: Deepti Hegde, Jeya Maria Jose Valanarasu, Vishal M. Patel


## What, Why and How

[1]: https://arxiv.org/pdf/2303.11313v3 "7KHUHLVDVRIDLQ - arXiv.org"
[2]: https://arxiv.org/abs/2303.11313 "[2303.11313] CLIP goes 3D: Leveraging Prompt Tuning for Language ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2303.11313v3 "[2303.11313v3] CLIP goes 3D: Leveraging Prompt Tuning for Language ..."

Here is a summary of the paper:

- **What**: The paper proposes a new framework called CG3D (CLIP Goes 3D) that learns a 3D encoder to perform language grounded 3D recognition tasks using natural language supervision.
- **Why**: The paper aims to address the limitation of CLIP, a vision-language model that can perform zero-shot recognition on images and text, but not on 3D point clouds. The paper argues that 3D recognition is an important and challenging task that requires multimodal understanding of geometry and semantics.
- **How**: The paper trains CG3D using triplets of point clouds, corresponding rendered 2D images, and texts. The paper uses contrastive loss to align the features from the 3D encoder, the visual encoder, and the text encoder in a common embedding space. The paper also employs prompt tuning to adapt CLIP to the 3D pre-training dataset and avoid catastrophic forgetting. The paper evaluates CG3D on various tasks such as zero-shot 3D recognition, 3D point cloud retrieval, scene querying with natural language, and fine-tuning on downstream 3D recognition tasks. The paper shows that CG3D outperforms existing methods and demonstrates impressive zero-shot capabilities.

## Main Contributions

According to the paper, the main contributions are:

- The paper introduces CG3D, a novel framework that leverages natural language supervision to learn a 3D encoder for language grounded 3D recognition tasks.
- The paper proposes a prompt tuning method to adapt CLIP to the 3D pre-training dataset and avoid catastrophic forgetting of the original CLIP knowledge.
- The paper demonstrates the effectiveness of CG3D on various tasks such as zero-shot 3D recognition, 3D point cloud retrieval, scene querying with natural language, and fine-tuning on downstream 3D recognition tasks. The paper shows that CG3D outperforms existing methods and exhibits impressive zero-shot capabilities.

## Method Summary

Here is a summary of the method section:

- The paper presents the details of CG3D, which consists of three components: a 3D encoder, a visual encoder, and a text encoder. The 3D encoder is a point cloud network that extracts features from 3D point clouds. The visual encoder and the text encoder are borrowed from CLIP, which are pre-trained on a large-scale image-text dataset. The paper uses PointMLP  as the 3D encoder and ViT-B/32  as the visual encoder.
- The paper trains CG3D using triplets of point clouds, corresponding rendered 2D images, and texts. The texts are natural language descriptions of the point clouds or the images. The paper uses ShapeNet  and ScanNet  as the 3D pre-training datasets. The paper renders 2D images from the point clouds using PyTorch3D .
- The paper uses contrastive loss to align the features from the 3D encoder, the visual encoder, and the text encoder in a common embedding space. The paper follows the CLIP formulation and computes the cosine similarity between the normalized feature vectors. The paper uses temperature scaling and hard negative mining to improve the contrastive learning.
- The paper also employs prompt tuning to adapt CLIP to the 3D pre-training dataset and avoid catastrophic forgetting of the original CLIP knowledge. The paper introduces learnable parameters in the input space of the visual and text encoders to shift them towards the 3D domain. The paper optimizes these parameters along with the 3D encoder while freezing the rest of CLIP. The paper shows that prompt tuning improves the performance of CG3D on various tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the 3D encoder, the visual encoder, and the text encoder
3D_encoder = PointMLP()
visual_encoder = ViT-B/32()
text_encoder = Transformer()

# Load the pre-trained CLIP model and freeze its parameters
CLIP = load_pretrained_model()
CLIP.freeze()

# Initialize the prompt parameters for the visual and text encoders
visual_prompt = nn.Parameter(torch.randn(1, 3, 224, 224))
text_prompt = nn.Parameter(torch.randn(1, 77))

# Define the contrastive loss function
def contrastive_loss(3D_features, visual_features, text_features):
  # Normalize the feature vectors
  3D_features = normalize(3D_features)
  visual_features = normalize(visual_features)
  text_features = normalize(text_features)
  
  # Compute the cosine similarity matrix
  similarity_matrix = torch.matmul(torch.cat([3D_features, visual_features]), torch.cat([visual_features, text_features]).T)
  
  # Scale the similarity matrix by a temperature factor
  similarity_matrix = similarity_matrix / temperature
  
  # Apply a softmax function to get the probabilities
  probabilities = softmax(similarity_matrix, dim=-1)
  
  # Compute the cross entropy loss between the probabilities and the labels
  labels = get_labels()
  loss = cross_entropy(probabilities, labels)
  
  # Optionally, apply hard negative mining to select hard negatives
  if hard_negative_mining:
    loss = select_hard_negatives(loss)
  
  return loss

# Define the optimizer and the learning rate scheduler
optimizer = Adam([3D_encoder.parameters(), visual_prompt, text_prompt])
scheduler = CosineAnnealingLR(optimizer)

# Loop over the epochs
for epoch in range(num_epochs):
  # Loop over the batches of data
  for batch in data_loader:
    # Get the point clouds, the rendered images, and the texts from the batch
    point_clouds, images, texts = batch
    
    # Apply data augmentation to the point clouds and the images
    point_clouds = augment(point_clouds)
    images = augment(images)
    
    # Add the prompt parameters to the images and the texts
    images = images + visual_prompt
    texts = texts + text_prompt
    
    # Forward pass the point clouds, the images, and the texts through the encoders
    3D_features = 3D_encoder(point_clouds)
    visual_features = visual_encoder(images)
    text_features = text_encoder(texts)
    
    # Compute the contrastive loss
    loss = contrastive_loss(3D_features, visual_features, text_features)
    
    # Backward pass and update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Update the learning rate scheduler
    scheduler.step()
    
    # Print or log the loss value
    print(loss.item())
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
import pytorch3d
from clip import load, tokenize

# Define the PointMLP class for the 3D encoder
class PointMLP(nn.Module):
  def __init__(self, num_classes=512):
    super(PointMLP, self).__init__()
    # Define the MLP layers
    self.mlp1 = nn.Sequential(
      nn.Conv1d(3, 64, 1),
      nn.BatchNorm1d(64),
      nn.ReLU(),
      nn.Conv1d(64, 128, 1),
      nn.BatchNorm1d(128),
      nn.ReLU(),
      nn.Conv1d(128, 1024, 1),
      nn.BatchNorm1d(1024),
      nn.ReLU()
    )
    # Define the global max pooling layer
    self.max_pool = nn.MaxPool1d(1024)
    # Define the MLP layers after pooling
    self.mlp2 = nn.Sequential(
      nn.Linear(1024, 512),
      nn.BatchNorm1d(512),
      nn.ReLU(),
      nn.Linear(512, 256),
      nn.BatchNorm1d(256),
      nn.ReLU(),
      nn.Linear(256, num_classes)
    )
  
  def forward(self, x):
    # x is a tensor of shape (batch_size, num_points, 3)
    # Transpose x to match the expected input shape of Conv1d
    x = x.transpose(1, 2) # x.shape = (batch_size, 3, num_points)
    # Apply the first MLP layers
    x = self.mlp1(x) # x.shape = (batch_size, 1024, num_points)
    # Apply the global max pooling layer
    x = self.max_pool(x) # x.shape = (batch_size, 1024, 1)
    # Squeeze the last dimension
    x = x.squeeze(-1) # x.shape = (batch_size, 1024)
    # Apply the second MLP layers
    x = self.mlp2(x) # x.shape = (batch_size, num_classes)
    return x

# Load the pre-trained CLIP model and freeze its parameters
CLIP_model, preprocess = load("ViT-B/32", device="cuda", jit=False)
CLIP_model.eval()
for param in CLIP_model.parameters():
  param.requires_grad = False

# Initialize the prompt parameters for the visual and text encoders
visual_prompt = nn.Parameter(torch.randn(1, 3, 224, 224).cuda())
text_prompt = nn.Parameter(torch.randn(1, 77).cuda())

# Define the contrastive loss function
def contrastive_loss(3D_features, visual_features, text_features):
  # Normalize the feature vectors
  3D_features = F.normalize(3D_features)
  visual_features = F.normalize(visual_features)
  text_features = F.normalize(text_features)
  
  # Compute the cosine similarity matrix
  similarity_matrix = torch.matmul(torch.cat([3D_features, visual_features]), torch.cat([visual_features, text_features]).T)
  
  # Scale the similarity matrix by a temperature factor
  temperature = 0.07
  similarity_matrix = similarity_matrix / temperature
  
  # Apply a softmax function to get the probabilities
  probabilities = F.softmax(similarity_matrix, dim=-1)
  
  # Compute the cross entropy loss between the probabilities and the labels
  batch_size = len(3D_features)
  labels = torch.arange(batch_size).cuda()
  loss_3D_visual = F.cross_entropy(probabilities[:batch_size, batch_size:], labels)
  loss_3D_text = F.cross_entropy(probabilities[:batch_size, :batch_size], labels)
  
  # Optionally, apply hard negative mining to select hard negatives
  hard_negative_mining = False
  if hard_negative_mining:
    # TODO: implement hard negative mining logic here
    
  
  return (loss_3D_visual + loss_3D_text) / 2

# Define the optimizer and the learning rate scheduler
optimizer = optim.Adam([3D_encoder.parameters(), visual_prompt, text_prompt], lr=0.0005)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer)

# Define the data augmentation transforms for point clouds and images
point_cloud_transforms = transforms.Compose([
  pytorch3d.transforms.RandomRotate(degrees=180.0),
  pytorch3d.transforms.RandomScale(scale_min=0.8, scale_max=1.2),
  pytorch3d.transforms.RandomTranslate(translate_range=0.1)
])
image_transforms = transforms.Compose([
  transforms.RandomHorizontalFlip(),
  transforms.RandomResizedCrop(224),
  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)
])

# Define the data loader for the 3D pre-training dataset
data_loader = get_data_loader() # TODO: implement the data loader logic here

# Loop over the epochs
num_epochs = 100
for epoch in range(num_epochs):
  # Loop over the batches of data
  for batch in data_loader:
    # Get the point clouds, the rendered images, and the texts from the batch
    point_clouds, images, texts = batch
    
    # Move the data to GPU
    point_clouds = point_clouds.cuda()
    images = images.cuda()
    texts = texts.cuda()
    
    # Apply data augmentation to the point clouds and the images
    point_clouds = point_cloud_transforms(point_clouds)
    images = image_transforms(images)
    
    # Add the prompt parameters to the images and the texts
    images = images + visual_prompt
    texts = texts + text_prompt
    
    # Forward pass the point clouds, the images, and the texts through the encoders
    3D_features = 3D_encoder(point_clouds)
    visual_features = CLIP_model.encode_image(images)
    text_features = CLIP_model.encode_text(texts)
    
    # Compute the contrastive loss
    loss = contrastive_loss(3D_features, visual_features, text_features)
    
    # Backward pass and update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Update the learning rate scheduler
    scheduler.step()
    
    # Print or log the loss value
    print(loss.item())
```