---
title: 2303.13516v2 Ablating Concepts in Text-to-Image Diffusion Models
date: 2023-03-14
---

# [Ablating Concepts in Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.13516v2)

authors: Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu


## What, Why and How

[1]: https://arxiv.org/abs/2303.13516 "[2303.13516] Ablating Concepts in Text-to-Image Diffusion Models"
[2]: http://export.arxiv.org/abs/2303.13516v2 "[2303.13516v2] Ablating Concepts in Text-to-Image Diffusion Models"
[3]: https://arxiv.org/pdf/2303.13516.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] [^2^][2] in terms of What, Why, and How:

- **What**: The paper proposes a method of ablating concepts in text-to-image diffusion models, i.e., preventing the generation of a target concept given a text condition.
- **Why**: The paper aims to address the problem of removing unwanted or copyrighted concepts or images from the pretrained model without retraining it from scratch.
- **How**: The paper introduces an algorithm that learns to match the image distribution for a target concept (such as a style, an instance, or a text prompt) to the distribution corresponding to an anchor concept (such as a blank image or a random noise). This way, the model cannot generate the target concept anymore, but can still generate closely related concepts.



## Main Contributions

[1]: https://arxiv.org/abs/2303.13516 "[2303.13516] Ablating Concepts in Text-to-Image Diffusion Models"
[2]: http://export.arxiv.org/abs/2303.13516v2 "[2303.13516v2] Ablating Concepts in Text-to-Image Diffusion Models"
[3]: https://arxiv.org/pdf/2303.13516.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1] [^2^][2], the main contributions are:

- **A novel method of ablating concepts in text-to-image diffusion models** without retraining the model from scratch or modifying its architecture.
- **A general framework for concept ablation** that can handle different types of target concepts, such as styles, instances, or text prompts, and different types of anchor concepts, such as blank images or random noise.
- **A comprehensive evaluation of the proposed method** on various datasets and tasks, such as style ablation, instance ablation, text prompt ablation, and concept preservation.
- **A user study and a perceptual study** to demonstrate the effectiveness and usefulness of the proposed method for removing unwanted concepts and protecting intellectual property rights.


## Method Summary

[1]: https://arxiv.org/abs/2303.13516 "[2303.13516] Ablating Concepts in Text-to-Image Diffusion Models"
[2]: http://export.arxiv.org/abs/2303.13516v2 "[2303.13516v2] Ablating Concepts in Text-to-Image Diffusion Models"
[3]: https://arxiv.org/pdf/2303.13516.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][1] [^2^][2]:

- The paper first introduces the **text-to-image diffusion model** that is used as the base model for concept ablation. This model is based on the diffusion probabilistic model that generates images by reversing a Markov chain of noisy images conditioned on text. The model consists of an encoder network, a decoder network, and a text encoder network. The encoder network maps an image to a latent code, the decoder network maps a latent code and a text embedding to an image, and the text encoder network maps a text to an embedding.
- The paper then describes the **concept ablation algorithm** that modifies the pretrained text-to-image diffusion model to prevent the generation of a target concept. The algorithm consists of two steps: (1) finding an anchor concept that is close to the target concept but does not contain it, and (2) learning a distribution matching function that maps the image distribution of the target concept to the image distribution of the anchor concept. The distribution matching function is implemented as a residual network that is trained with a contrastive loss and an adversarial loss. The contrastive loss encourages the function to map similar images to similar images, and the adversarial loss encourages the function to fool a discriminator network that tries to distinguish between the original and the modified images.
- The paper also explains how to **apply the concept ablation algorithm** to different types of target concepts and anchor concepts. For example, for style ablation, the target concept is a style name (such as "Van Gogh" or "Picasso"), and the anchor concept is a blank image. For instance ablation, the target concept is an image instance (such as a specific person or object), and the anchor concept is a random noise image. For text prompt ablation, the target concept is a text prompt (such as "a cat wearing sunglasses"), and the anchor concept is another text prompt (such as "a cat wearing nothing").


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Load the pretrained text-to-image diffusion model
model = load_model("text-to-image-diffusion")

# Define the target concept and the anchor concept
target_concept = "Van Gogh" # a style name
anchor_concept = "blank" # a blank image

# Define the distribution matching function as a residual network
dist_match = ResNet()

# Define the discriminator network as a convolutional network
discriminator = ConvNet()

# Train the distribution matching function and the discriminator network
for epoch in range(num_epochs):
  # Sample a batch of images and texts from the dataset
  images, texts = sample_batch(dataset)
  # Apply the concept ablation algorithm to the images and texts
  ablated_images = concept_ablation(images, texts, target_concept, anchor_concept, model, dist_match)
  # Compute the contrastive loss between the original and ablated images
  contrastive_loss = compute_contrastive_loss(images, ablated_images)
  # Compute the adversarial loss between the original and ablated images
  adversarial_loss = compute_adversarial_loss(images, ablated_images, discriminator)
  # Update the distribution matching function parameters using gradient descent
  dist_match.update(contrastive_loss + adversarial_loss)
  # Update the discriminator network parameters using gradient ascent
  discriminator.update(-adversarial_loss)

# Define the concept ablation algorithm
def concept_ablation(images, texts, target_concept, anchor_concept, model, dist_match):
  # Encode the images and texts to latent codes and embeddings
  latent_codes = model.encode(images)
  text_embeddings = model.text_encode(texts)
  # Check if the texts contain the target concept
  target_mask = check_target_concept(texts, target_concept)
  # Generate images for the anchor concept using the text embeddings
  anchor_images = model.generate(anchor_concept, text_embeddings)
  # Encode the anchor images to latent codes
  anchor_codes = model.encode(anchor_images)
  # Apply the distribution matching function to the latent codes of the target concept
  modified_codes = dist_match(latent_codes * target_mask + anchor_codes * (1 - target_mask))
  # Decode the modified latent codes and text embeddings to images
  modified_images = model.decode(modified_codes, text_embeddings)
  # Return the modified images
  return modified_images

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import random

# Load the pretrained text-to-image diffusion model from https://github.com/openai/DALL-E
model = torch.hub.load('openai/DALL-E', 'dalle')

# Define the target concept and the anchor concept
target_concept = "Van Gogh" # a style name
anchor_concept = "blank" # a blank image

# Define the distribution matching function as a residual network with 18 layers
dist_match = torchvision.models.resnet18(pretrained=True)
dist_match.fc = torch.nn.Linear(dist_match.fc.in_features, model.latent_dim)

# Define the discriminator network as a convolutional network with 5 layers
discriminator = torch.nn.Sequential(
  torch.nn.Conv2d(3, 64, 4, 2, 1),
  torch.nn.LeakyReLU(0.2),
  torch.nn.Conv2d(64, 128, 4, 2, 1),
  torch.nn.BatchNorm2d(128),
  torch.nn.LeakyReLU(0.2),
  torch.nn.Conv2d(128, 256, 4, 2, 1),
  torch.nn.BatchNorm2d(256),
  torch.nn.LeakyReLU(0.2),
  torch.nn.Conv2d(256, 512, 4, 2, 1),
  torch.nn.BatchNorm2d(512),
  torch.nn.LeakyReLU(0.2),
  torch.nn.Conv2d(512, 1, 4, 1, 0),
)

# Define the hyperparameters
batch_size = 64 # the number of images and texts in a batch
num_epochs = 100 # the number of epochs to train
learning_rate = 0.0002 # the learning rate for gradient descent
beta1 = 0.5 # the beta1 parameter for Adam optimizer
beta2 = 0.999 # the beta2 parameter for Adam optimizer
temperature = 0.9 # the temperature parameter for diffusion model
contrastive_margin = 0.5 # the margin parameter for contrastive loss

# Define the optimizers for the distribution matching function and the discriminator network
dist_match_optimizer = torch.optim.Adam(dist_match.parameters(), lr=learning_rate, betas=(beta1, beta2))
discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta1, beta2))

# Define the loss functions for contrastive loss and adversarial loss
contrastive_loss_fn = torch.nn.MarginRankingLoss(margin=contrastive_margin)
adversarial_loss_fn = torch.nn.BCEWithLogitsLoss()

# Load the dataset of images and texts from https://github.com/openai/CLIP
dataset = load_dataset("clip")

# Train the distribution matching function and the discriminator network
for epoch in range(num_epochs):
  
  # Shuffle the dataset
  random.shuffle(dataset)

  # Loop over the batches of the dataset
  for i in range(0, len(dataset), batch_size):

    # Sample a batch of images and texts from the dataset
    images = dataset[i:i+batch_size]["image"]
    texts = dataset[i:i+batch_size]["text"]

    # Apply the concept ablation algorithm to the images and texts
    ablated_images = concept_ablation(images, texts, target_concept, anchor_concept, model, dist_match)

    # Compute the contrastive loss between the original and ablated images
    contrastive_loss = compute_contrastive_loss(images, ablated_images)

    # Compute the adversarial loss between the original and ablated images
    adversarial_loss = compute_adversarial_loss(images, ablated_images, discriminator)

    # Update the distribution matching function parameters using gradient descent
    dist_match_optimizer.zero_grad()
    (contrastive_loss + adversarial_loss).backward()
    dist_match_optimizer.step()

    # Update the discriminator network parameters using gradient ascent
    discriminator_optimizer.zero_grad()
    (-adversarial_loss).backward()
    discriminator_optimizer.step()

# Define the concept ablation algorithm
def concept_ablation(images, texts, target_concept, anchor_concept, model, dist_match):

  # Encode the images and texts to latent codes and embeddings using diffusion model
  latent_codes = model.get_latent_codes(images)
  text_embeddings = model.get_text_embeddings(texts)

  # Check if the texts contain the target concept using CLIP model
  target_mask = check_target_concept(texts, target_concept)

  # Generate images for the anchor concept using the diffusion model and the text embeddings
  anchor_images = model.generate_images(anchor_concept, text_embeddings, temperature)

  # Encode the anchor images to latent codes using diffusion model
  anchor_codes = model.get_latent_codes(anchor_images)

  # Apply the distribution matching function to the latent codes of the target concept
  modified_codes = dist_match(latent_codes * target_mask + anchor_codes * (1 - target_mask))

  # Decode the modified latent codes and text embeddings to images using diffusion model
  modified_images = model.get_images(modified_codes, text_embeddings, temperature)

  # Return the modified images
  return modified_images

# Define the function to compute the contrastive loss between the original and ablated images
def compute_contrastive_loss(images, ablated_images):

  # Compute the pairwise cosine similarity between the original and ablated images
  similarity_matrix = torch.nn.functional.cosine_similarity(images.view(batch_size, -1), ablated_images.view(batch_size, -1), dim=1)

  # Create a positive mask that indicates which pairs of images have the same text condition
  positive_mask = torch.eye(batch_size)

  # Create a negative mask that indicates which pairs of images have different text conditions
  negative_mask = 1 - positive_mask

  # Compute the positive similarity by masking the similarity matrix with the positive mask
  positive_similarity = similarity_matrix * positive_mask

  # Compute the negative similarity by masking the similarity matrix with the negative mask
  negative_similarity = similarity_matrix * negative_mask

  # Compute the contrastive loss by comparing the positive and negative similarity with a margin
  contrastive_loss = contrastive_loss_fn(positive_similarity.view(-1, 1), negative_similarity.view(-1, 1), torch.ones(batch_size * batch_size))

  # Return the contrastive loss
  return contrastive_loss

# Define the function to compute the adversarial loss between the original and ablated images
def compute_adversarial_loss(images, ablated_images, discriminator):

  # Compute the discriminator output for the original images
  real_output = discriminator(images)

  # Compute the discriminator output for the ablated images
  fake_output = discriminator(ablated_images)

  # Compute the real label as a tensor of ones
  real_label = torch.ones(batch_size)

  # Compute the fake label as a tensor of zeros
  fake_label = torch.zeros(batch_size)

  # Compute the adversarial loss for the real images
  real_loss = adversarial_loss_fn(real_output, real_label)

  # Compute the adversarial loss for the ablated images
  fake_loss = adversarial_loss_fn(fake_output, fake_label)

  # Compute the total adversarial loss as the sum of the real and fake losses
  adversarial_loss = real_loss + fake_loss

  # Return the adversarial loss
  return adversarial_loss

# Define the function to check if the texts contain the target concept using CLIP model
def check_target_concept(texts, target_concept):

  # Load the CLIP model from https://github.com/openai/CLIP
  clip_model = torch.hub.load('openai/CLIP', 'ViT-B/32')

  # Encode the texts to embeddings using CLIP model
  text_embeddings = clip_model.encode_text(texts)

  # Encode the target concept to an embedding using CLIP model
  target_embedding = clip_model.encode_text(target_concept)

  # Compute the cosine similarity between the text embeddings and the target embedding
  similarity_scores = torch.nn.functional.cosine_similarity(text_embeddings, target_embedding, dim=1)

  # Create a mask that indicates which texts have a high similarity score with the target concept
  threshold = 0.5 # a hyperparameter to tune
  target_mask = (similarity_scores > threshold).float()

  # Return the target mask
  return target_mask

```