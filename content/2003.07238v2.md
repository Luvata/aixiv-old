---
title: 2003.07238v2 A Rotation-Invariant Framework for Deep Point Cloud Analysis
date: 2020-03-08
---

# [A Rotation-Invariant Framework for Deep Point Cloud Analysis](http://arxiv.org/abs/2003.07238v2)

authors: Xianzhi Li, Ruihui Li, Guangyong Chen, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng


## What, Why and How

[1]: https://arxiv.org/pdf/2003.07238v2.pdf "A Rotation-Invariant Framework for Deep Point Cloud Analysis"
[2]: https://arxiv.org/abs/2003.07238v2 "A Rotation-Invariant Framework for Deep Point Cloud Analysis"
[3]: http://export.arxiv.org/abs/2208.07238v2 "[2208.07238v2] Multidegrees, prime ideals, and non-standard gradings"

Here is a summary of the paper:

- **What**: The paper introduces a new framework for deep point cloud analysis that is rotation-invariant, meaning that it can handle point clouds in arbitrary orientations without losing performance.
- **Why**: The paper aims to address the common drawback of existing deep neural networks for point cloud analysis, which are not rotation-invariant and thus have poor generalization to arbitrary orientations. This drawback limits the applicability of these networks to real-world scenarios where point clouds may have different poses.
- **How**: The paper proposes a new low-level purely rotation-invariant representation to replace common 3D Cartesian coordinates as the network inputs. This representation encodes the relative positions and orientations of points and their neighbors using spherical harmonics. The paper also presents a network architecture that embeds these representations into features, encoding local and global shape structure. To alleviate the inevitable global information loss caused by the rotation-invariant representations, the paper further introduces a region relation convolution to encode local and non-local information. The paper evaluates the proposed method on multiple point cloud analysis tasks, including shape classification, part segmentation, and shape retrieval, and shows that it achieves consistent and state-of-the-art performance on inputs at arbitrary orientations[^1^][1].

## Main Contributions

The main contributions of this paper are:

- It introduces a new low-level purely rotation-invariant representation for point cloud analysis, which can handle point clouds in arbitrary orientations without losing performance.
- It presents a network architecture that embeds these representations into features, encoding local and global shape structure.
- It proposes a region relation convolution to encode local and non-local information, which can alleviate the global information loss caused by the rotation-invariant representations.
- It evaluates the proposed method on multiple point cloud analysis tasks, and shows that it achieves consistent and state-of-the-art performance on inputs at arbitrary orientations.

## Method Summary

[1]: https://arxiv.org/pdf/2003.07238v2.pdf "A Rotation-Invariant Framework for Deep Point Cloud Analysis - arXiv.org"
[2]: https://arxiv.org/abs/2003.07238v2 "A Rotation-Invariant Framework for Deep Point Cloud Analysis"
[3]: http://export.arxiv.org/abs/1607.07238v2 "[1607.07238v2] Interpolation process between standard diffusion and ..."

Here is a summary of the method section of the paper:

- The method section consists of four subsections: rotation-invariant representation, feature embedding network, region relation convolution, and network architecture.
- In the rotation-invariant representation subsection, the paper explains how to encode the relative positions and orientations of points and their neighbors using spherical harmonics. The paper shows that this representation is purely rotation-invariant and can be computed efficiently using a fast Fourier transform (FFT).
- In the feature embedding network subsection, the paper describes how to embed the rotation-invariant representations into features using a series of convolutional layers. The paper introduces a novel convolutional layer that can handle irregular point clouds and preserve local structure. The paper also introduces a global feature aggregation layer that can capture global shape structure using max pooling and concatenation.
- In the region relation convolution subsection, the paper proposes a new convolutional layer that can encode local and non-local information. The paper defines a region relation matrix that measures the similarity between different regions of a point cloud. The paper then applies a convolutional operation on this matrix to generate region relation features. The paper shows that this layer can alleviate the global information loss caused by the rotation-invariant representations and improve the performance on various tasks.
- In the network architecture subsection, the paper presents the overall network architecture for different point cloud analysis tasks. The paper shows how to combine the feature embedding network and the region relation convolution to form a backbone network. The paper then shows how to add task-specific layers for shape classification, part segmentation, and shape retrieval. The paper also discusses some implementation details and hyperparameters.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a point cloud P with N points
# Output: a task-specific prediction Y

# Compute the rotation-invariant representation R for each point and its K neighbors using spherical harmonics and FFT
R = rotation_invariant_representation(P, K)

# Embed R into features F using a feature embedding network with convolutional layers
F = feature_embedding_network(R)

# Compute the region relation matrix M for F using cosine similarity
M = region_relation_matrix(F)

# Generate region relation features G using a region relation convolution on M
G = region_relation_convolution(M)

# Concatenate F and G to form the backbone features H
H = concatenate(F, G)

# Add task-specific layers to H to get the prediction Y
if task == "shape classification":
  Y = shape_classification_layer(H)
elif task == "part segmentation":
  Y = part_segmentation_layer(H)
elif task == "shape retrieval":
  Y = shape_retrieval_layer(H)
else:
  raise ValueError("Invalid task")

# Return Y
return Y
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a point cloud P with N points and D dimensions
# Output: a task-specific prediction Y

# Define some hyperparameters
K = 16 # number of neighbors for each point
L = 3 # number of spherical harmonics bands
C = 64 # number of channels for feature embedding network
R = 16 # number of regions for region relation convolution
T = 128 # number of channels for region relation convolution

# Define some helper functions
def spherical_harmonics(x, y, z, l, m):
  # Compute the spherical harmonics function Y_l^m(x, y, z) using the formula from https://en.wikipedia.org/wiki/Spherical_harmonics
  # Input: x, y, z are the Cartesian coordinates of a point on the unit sphere
  #        l is the band index (0 <= l < L)
  #        m is the order index (-l <= m <= l)
  # Output: a complex number representing the spherical harmonics value

def fft(x):
  # Compute the fast Fourier transform of a complex vector x using the numpy library
  # Input: x is a complex vector of length L^2
  # Output: a complex vector of length L^2 representing the FFT of x

def conv1d(x, w, b):
  # Compute the one-dimensional convolution of a vector x with a weight vector w and a bias scalar b using the numpy library
  # Input: x is a real vector of length N
  #        w is a real vector of length K
  #        b is a real scalar
  # Output: a real vector of length N - K + 1 representing the convolution of x with w and b

def conv2d(x, w, b):
  # Compute the two-dimensional convolution of a matrix x with a weight matrix w and a bias scalar b using the numpy library
  # Input: x is a real matrix of shape (M, N)
  #        w is a real matrix of shape (P, Q)
  #        b is a real scalar
  # Output: a real matrix of shape (M - P + 1, N - Q + 1) representing the convolution of x with w and b

def max_pooling(x):
  # Compute the max pooling of a matrix x using the numpy library
  # Input: x is a real matrix of shape (M, N)
  # Output: a real scalar representing the maximum value in x

def cosine_similarity(x, y):
  # Compute the cosine similarity between two vectors x and y using the numpy library
  # Input: x and y are real vectors of length D
  # Output: a real scalar representing the cosine similarity between x and y

# Compute the rotation-invariant representation R for each point and its K neighbors using spherical harmonics and FFT
R = np.zeros((N, K, L**2), dtype=np.complex) # initialize R as a complex matrix of shape (N, K, L^2)
for i in range(N): # loop over each point in P
  p = P[i] # get the i-th point in P
  n = P[knn(P, p, K)] - p # get the K nearest neighbors of p in P and subtract p from them to get relative positions
  n = n / np.linalg.norm(n, axis=1, keepdims=True) # normalize n to get unit vectors on the sphere
  for j in range(K): # loop over each neighbor in n
    x, y, z = n[j] # get the Cartesian coordinates of the j-th neighbor in n
    for l in range(L): # loop over each band index l
      for m in range(-l, l+1): # loop over each order index m
        R[i][j][l**2 + m] = spherical_harmonics(x, y, z, l, m) # compute the spherical harmonics function Y_l^m(x, y, z) and store it in R[i][j][l**2 + m]
    R[i][j] = fft(R[i][j]) / np.sqrt(L**2) # compute the FFT of R[i][j] and normalize it by sqrt(L^2)

# Embed R into features F using a feature embedding network with convolutional layers
F = np.zeros((N, C)) # initialize F as a real matrix of shape (N, C)
w1 = np.random.randn(K, L**2) * np.sqrt(2 / (K * L**2)) # initialize the weight vector w1 for the first convolutional layer using He initialization
b1 = np.random.randn() # initialize the bias scalar b1 for the first convolutional layer
w2 = np.random.randn(C, K) * np.sqrt(2 / (C * K)) # initialize the weight vector w2 for the second convolutional layer using He initialization
b2 = np.random.randn() # initialize the bias scalar b2 for the second convolutional layer
for i in range(N): # loop over each point in P
  f = np.abs(R[i]) # get the absolute value of R[i] as a real matrix of shape (K, L^2)
  f = conv1d(f, w1, b1) # apply the first convolutional layer on f to get a real vector of length C
  f = np.maximum(f, 0) # apply the ReLU activation function on f
  f = conv1d(f, w2, b2) # apply the second convolutional layer on f to get a real vector of length C
  f = np.maximum(f, 0) # apply the ReLU activation function on f
  F[i] = f # store f in F[i]

# Compute the region relation matrix M for F using cosine similarity
M = np.zeros((R, R)) # initialize M as a real matrix of shape (R, R)
for i in range(R): # loop over each region index i
  fi = F[region(P, i)] # get the features of the points in the i-th region of P
  for j in range(R): # loop over each region index j
    fj = F[region(P, j)] # get the features of the points in the j-th region of P
    M[i][j] = cosine_similarity(np.mean(fi, axis=0), np.mean(fj, axis=0)) # compute the cosine similarity between the mean features of fi and fj and store it in M[i][j]

# Generate region relation features G using a region relation convolution on M
G = np.zeros((N, T)) # initialize G as a real matrix of shape (N, T)
w3 = np.random.randn(T, R) * np.sqrt(2 / (T * R)) # initialize the weight matrix w3 for the region relation convolution using He initialization
b3 = np.random.randn() # initialize the bias scalar b3 for the region relation convolution
for i in range(N): # loop over each point in P
  p = P[i] # get the i-th point in P
  r = region_index(P, p) # get the region index of p in P
  g = conv2d(M, w3[:, r], b3) # apply the region relation convolution on M using w3[:, r] and b3 to get a real vector of length T
  g = np.maximum(g, 0) # apply the ReLU activation function on g
  G[i] = g # store g in G[i]

# Concatenate F and G to form the backbone features H
H = np.concatenate((F, G), axis=1) # concatenate F and G along the second axis to get a real matrix of shape (N, C + T)

# Add task-specific layers to H to get the prediction Y
if task == "shape classification":
  w4 = np.random.randn(C + T, num_classes) * np.sqrt(2 / (C + T)) # initialize the weight matrix w4 for shape classification using He initialization
  b4 = np.random.randn(num_classes) # initialize the bias vector b4 for shape classification
  Y = softmax(np.dot(H.mean(axis=0), w4) + b4) # compute the softmax of H.mean(axis=0) dot w4 plus b4 to get a probability vector of length num_classes

elif task == "part segmentation":
  w5 = np.random.randn(C + T, num_parts) * np.sqrt(2 / (C + T)) # initialize the weight matrix w5 for part segmentation using He initialization
  b5 = np.random.randn(num_parts) # initialize the bias vector b5 for part segmentation
  Y = softmax(np.dot(H, w5) + b5) # compute the softmax of H dot w5 plus b5 to get a probability matrix of shape (N, num_parts)

elif task == "shape retrieval":
  Y = H.mean(axis=0) / np.linalg.norm(H.mean(axis=0)) # compute the normalized mean feature of H as a real vector of length C + T

else:
  raise ValueError("Invalid task")

# Return Y
return Y
```