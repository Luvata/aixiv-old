---
title: 2104.00670v1 Unconstrained Scene Generation with Locally Conditioned Radiance Fields
date: 2021-04-01
---

# [Unconstrained Scene Generation with Locally Conditioned Radiance Fields](http://arxiv.org/abs/2104.00670v1)

authors: Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, Joshua M. Susskind


## What, Why and How

[1]: https://arxiv.org/abs/2104.00670 "[2104.00670] Unconstrained Scene Generation with Locally Conditioned ..."
[2]: https://arxiv.org/pdf/2104.00670 "arXiv:2104.00670v1 [cs.CV] 1 Apr 2021"
[3]: http://export.arxiv.org/abs/1803.00670v1 "[1803.00670v1] Vacuum energy for Yang-Mills fields in $R^d\\times S^1 ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces **Generative Scene Networks (GSN)**, a model that learns to decompose scenes into a collection of many local radiance fields that can be rendered from a free moving camera.
- **Why**: The paper tackles the challenge of learning a distribution over complex, realistic, indoor scenes. Existing models either lack the representational capacity to capture them, or cannot extrapolate to infer unobserved parts of the scene.
- **How**: The paper proposes a novel decomposition scheme that scales to larger and more complex scenes while preserving details and diversity. The paper also leverages a conditional variational autoencoder framework to learn a scene prior that enables high-quality rendering from viewpoints that are significantly different from observed viewpoints. The paper evaluates the model on several different scene datasets and shows that it produces quantitatively higher-quality scene renderings than existing models.

## Main Contributions

[1]: https://arxiv.org/abs/2104.00670 "[2104.00670] Unconstrained Scene Generation with Locally Conditioned ..."
[2]: https://arxiv.org/pdf/2104.00670 "arXiv:2104.00670v1 [cs.CV] 1 Apr 2021"
[3]: http://export.arxiv.org/abs/1803.00670v1 "[1803.00670v1] Vacuum energy for Yang-Mills fields in $R^d\\times S^1 ..."

The paper claims the following contributions[^1^][1]:

- **A novel decomposition scheme** that splits a scene into many local radiance fields, each conditioned on a latent code and a local coordinate system. This allows the model to scale to larger and more complex scenes than existing methods.
- **A conditional variational autoencoder framework** that learns a scene prior over the latent codes and coordinate systems of the local radiance fields. This enables the model to generate new scenes or complete scenes given sparse observations.
- **A comprehensive evaluation** of the model on several different scene datasets, including synthetic and real-world scenes. The paper shows that the model outperforms existing methods in terms of quantitative metrics and qualitative visualizations. The paper also provides ablation studies and analysis of the model components.

## Method Summary

[1]: https://arxiv.org/abs/2104.00670 "[2104.00670] Unconstrained Scene Generation with Locally Conditioned ..."
[2]: https://arxiv.org/pdf/2104.00670 "arXiv:2104.00670v1 [cs.CV] 1 Apr 2021"
[3]: http://export.arxiv.org/abs/1803.00670v1 "[1803.00670v1] Vacuum energy for Yang-Mills fields in $R^d\\times S^1 ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a **generative model** that learns to decompose a scene into a collection of **local radiance fields**, each represented by a **neural network** that maps a 3D location and a 2D viewing direction to a color and an opacity value.
- The paper defines a **local coordinate system** for each local radiance field, which is parameterized by a **translation vector** and a **rotation matrix**. The paper also assigns a **latent code** to each local radiance field, which encodes its appearance and geometry.
- The paper uses a **conditional variational autoencoder (CVAE)** framework to learn a distribution over the latent codes and coordinate systems of the local radiance fields. The paper defines an **encoder network** that takes a set of 2D observations of a scene and outputs the posterior distribution over the latent codes and coordinate systems. The paper also defines a **decoder network** that takes a latent code, a coordinate system, and a query point and outputs the predicted color and opacity at that point.
- The paper trains the model using a **reconstruction loss** that measures the discrepancy between the observed images and the rendered images from the same viewpoints. The paper also uses a **regularization loss** that encourages the latent codes and coordinate systems to follow a prior distribution, which is defined as a mixture of Gaussians.
- The paper uses an **iterative sampling scheme** to generate new scenes or complete scenes given sparse observations. The paper samples latent codes and coordinate systems from the prior distribution, renders images from different viewpoints, and feeds them back to the encoder network to refine the posterior distribution. The paper repeats this process until convergence.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the local radiance field network
def lrf_network(x, d, z):
  # x: 3D location
  # d: 2D viewing direction
  # z: latent code
  # Returns: color and opacity at x
  h = MLP(concat(x, d, z)) # Multi-layer perceptron
  c = sigmoid(h[:3]) # Color
  sigma = softplus(h[3]) # Opacity
  return c, sigma

# Define the encoder network
def encoder_network(I):
  # I: a set of 2D observations of a scene
  # Returns: the posterior distribution over the latent codes and coordinate systems
  h = CNN(I) # Convolutional neural network
  mu_z = h[:K] # Mean of the latent codes
  logvar_z = h[K:2K] # Log-variance of the latent codes
  mu_t = h[2K:3K] # Mean of the translation vectors
  logvar_t = h[3K:4K] # Log-variance of the translation vectors
  mu_r = h[4K:5K] # Mean of the rotation matrices
  logvar_r = h[5K:] # Log-variance of the rotation matrices
  return mu_z, logvar_z, mu_t, logvar_t, mu_r, logvar_r

# Define the decoder network
def decoder_network(z, t, r, x_q):
  # z: latent code
  # t: translation vector
  # r: rotation matrix
  # x_q: query point
  # Returns: color and opacity at x_q
  x_l = r @ (x_q - t) # Transform query point to local coordinate system
  c, sigma = lrf_network(x_l, x_q / norm(x_q), z) # Call local radiance field network
  return c, sigma

# Define the prior distribution
def prior_distribution(K):
  # K: number of local radiance fields
  # Returns: the prior distribution over the latent codes and coordinate systems
  pi = Dirichlet(alpha) # Mixture weights
  mu_z = Normal(0, I) # Mean of the latent codes
  logvar_z = Normal(0, I) # Log-variance of the latent codes
  mu_t = Normal(0, I) # Mean of the translation vectors
  logvar_t = Normal(0, I) # Log-variance of the translation vectors
  mu_r = Normal(0, I) # Mean of the rotation matrices
  logvar_r = Normal(0, I) # Log-variance of the rotation matrices
  return pi, mu_z, logvar_z, mu_t, logvar_t, mu_r, logvar_r

# Define the reconstruction loss
def reconstruction_loss(I, c_hat):
  # I: observed images
  # c_hat: rendered images from the same viewpoints
  # Returns: the reconstruction loss (mean squared error)
  return mean((I - c_hat)**2)

# Define the regularization loss (KL divergence)
def regularization_loss(mu_z, logvar_z, mu_t, logvar_t, mu_r, logvar_r):
  # mu_z, logvar_z: posterior parameters of the latent codes
  # mu_t, logvar_t: posterior parameters of the translation vectors
  # mu_r, logvar_r: posterior parameters of the rotation matrices
  # Returns: the regularization loss (KL divergence between posterior and prior)
  
  pi, mu_z_prior, logvar_z_prior,
    mu_t_prior, logvar_t_prior,
    mu_r_prior, logvar_r_prior = prior_distribution(K) # Get prior parameters
  
  kl_z = KL(Normal(mu_z, exp(logvar_z)), Normal(mu_z_prior, exp(logvar_z_prior))) # KL for latent codes
  
  kl_t = KL(Normal(mu_t, exp(logvar_t)), Normal(mu_t_prior, exp(logvar_t_prior))) # KL for translation vectors
  
  kl_r = KL(Normal(mu_r, exp(logvar_r)), Normal(mu_r_prior, exp(logvar_r_prior))) # KL for rotation matrices
  
  


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Define the hyperparameters
K = 64 # Number of local radiance fields
N = 256 # Number of samples along each ray
L = 4 # Number of layers in MLP
H = 256 # Number of hidden units in MLP
alpha = 0.1 # Dirichlet concentration parameter
beta = 1.0 # Weight for regularization loss
gamma = 0.01 # Weight for entropy loss
lr = 0.001 # Learning rate
epochs = 100 # Number of training epochs
batch_size = 32 # Batch size

# Define the local radiance field network
def lrf_network(x, d, z):
  # x: 3D location (shape: [batch_size, N, 3])
  # d: 2D viewing direction (shape: [batch_size, N, 2])
  # z: latent code (shape: [batch_size, K])
  # Returns: color and opacity at x (shape: [batch_size, N, 3] and [batch_size, N])
  
  # Concatenate inputs along the last dimension
  inputs = concat(x, d, z[:, None, :]) # Shape: [batch_size, N, K + 5]
  
  # Apply L fully connected layers with ReLU activation
  h = inputs # Shape: [batch_size, N, K + 5]
  for i in range(L):
    h = linear(h, H) # Shape: [batch_size, N, H]
    h = relu(h) # Shape: [batch_size, N, H]
  
  # Apply a final linear layer to get color and opacity
  outputs = linear(h, 4) # Shape: [batch_size, N, 4]
  
  # Apply sigmoid and softplus activation to get color and opacity
  c = sigmoid(outputs[:, :, :3]) # Shape: [batch_size, N, 3]
  sigma = softplus(outputs[:, :, 3]) # Shape: [batch_size, N]
  
  return c, sigma

# Define the encoder network
def encoder_network(I):
  # I: a set of 2D observations of a scene (shape: [batch_size, M, H, W, C])
  # Returns: the posterior distribution over the latent codes and coordinate systems
  
  # Apply a CNN to encode each observation into a feature vector
  h = CNN(I) # Shape: [batch_size, M, F]
  
  # Reduce the feature vectors along the second dimension by mean pooling
  h = mean(h, dim=1) # Shape: [batch_size, F]
  
  # Apply a linear layer to get the posterior parameters
  params = linear(h, K * (6 + dim)) # Shape: [batch_size, K * (6 + dim)]
  
  # Split the parameters into mean and log-variance for each variable
  mu_z = params[:, :K] # Shape: [batch_size, K]
  logvar_z = params[:, K:2K] # Shape: [batch_size, K]