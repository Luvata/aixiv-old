---
title: 2306.00980v1 SnapFusion  Text-to-Image Diffusion Model on Mobile Devices within Two Seconds
date: 2023-06-01
---

# [SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds](http://arxiv.org/abs/2306.00980v1)

authors: Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren


## What, Why and How

[1]: https://arxiv.org/abs/2306.00980 "[2306.00980] SnapFusion: Text-to-Image Diffusion Model on Mobile ..."
[2]: https://arxiv.org/abs/2304.00980 "Long-lived valley states in bilayer graphene quantum dots"
[3]: http://export.arxiv.org/abs/2306.00980 "[2306.00980] SnapFusion: Text-to-Image Diffusion Model on Mobile ..."

Here is a summary of the paper:

- **What**: The paper presents a text-to-image diffusion model that can run on mobile devices within two seconds, generating realistic and diverse images from natural language descriptions.
- **Why**: The paper aims to overcome the challenges of existing text-to-image diffusion models, which are large, complex, slow, and require high-end GPUs or cloud-based inference, making them costly and privacy-sensitive.
- **How**: The paper introduces an efficient UNet architecture that reduces the redundancy and computation of the image decoder, and improves the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. The paper evaluates the model on MS-COCO dataset and shows that it achieves better FID and CLIP scores than Stable Diffusion v 1.5 with fewer denoising steps[^1^][1].

## Main Contributions

The paper claims the following contributions:

- It proposes a generic approach that enables running text-to-image diffusion models on mobile devices in less than 2 seconds, which is orders of magnitude faster than existing methods.
- It introduces an efficient UNet architecture that reduces the model size and computation by 4.5x and 2.5x respectively, without sacrificing the image quality or diversity.
- It enhances the step distillation by exploring different training strategies and introducing regularization from classifier-free guidance, which improves the performance and stability of the model.
- It demonstrates the effectiveness and efficiency of the proposed model on MS-COCO dataset, achieving better FID and CLIP scores than Stable Diffusion v 1.5 with 8 denoising steps versus 50 steps.

## Method Summary

[1]: https://arxiv.org/pdf/2306.00920v1.pdf "arXiv:2306.00920v1 [cs.LG] 1 Jun 2023"
[2]: https://arxiv.org/abs/2106.00980v1 "[2106.00980v1] End-to-End Hierarchical Relation Extraction for Generic ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2105.00980v1 "[2105.00980v1] Growable Realizations: a Powerful Approach to the ..."

Here is a summary of the method section of the paper:

- The paper proposes a two-stage approach to text-to-image diffusion on mobile devices: **efficient UNet** and **improved step distillation**.
- The **efficient UNet** consists of an image encoder and an image decoder, which are both based on the UNet architecture [^1^][2]. The image encoder is identical to the original UNet, but the image decoder is modified to reduce the redundancy and computation. Specifically, the paper uses data distillation to train a smaller decoder that mimics the output of the original decoder, and then replaces the original decoder with the smaller one. The paper also applies quantization and pruning techniques to further compress the model size.
- The **improved step distillation** aims to enhance the performance and stability of the diffusion model with fewer denoising steps. The paper explores different training strategies, such as using different learning rates, loss functions, and data augmentations for different steps. The paper also introduces regularization from classifier-free guidance, which leverages a pre-trained CLIP model [^2^][3] to provide additional supervision for the diffusion model without requiring any extra labels.
- The paper evaluates the proposed model on MS-COCO dataset , using FID and CLIP scores as the main metrics. The paper compares the model with Stable Diffusion v 1.5  and other baselines, and shows that the model achieves better results with 8 denoising steps versus 50 steps for Stable Diffusion v 1.5. The paper also demonstrates that the model can run on mobile devices within two seconds, which is orders of magnitude faster than existing methods.

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define the hyperparameters
num_steps = 8 # number of denoising steps
num_epochs = 100 # number of training epochs
batch_size = 32 # batch size for training
lr = 0.001 # learning rate for the diffusion model
lr_clip = 0.01 # learning rate for the CLIP model
beta1 = 0.9 # beta1 for Adam optimizer
beta2 = 0.999 # beta2 for Adam optimizer
lambda_clip = 0.1 # weight for the CLIP loss
lambda_mse = 0.9 # weight for the MSE loss

# Load the MS-COCO dataset and split into train and test sets
dataset = torchvision.datasets.CocoCaptions(root = 'data/coco', annFile = 'data/coco/annotations/captions_train2017.json', transform = torchvision.transforms.ToTensor())
train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset) - 1000, 1000])

# Load the pre-trained CLIP model and freeze its parameters
clip_model = clip.load('ViT-B/32', device = 'cuda')
clip_model.eval()
for param in clip_model.parameters():
    param.requires_grad = False

# Define the image encoder based on the original UNet architecture
image_encoder = UNetEncoder(in_channels = 3, out_channels = 256)

# Define the image decoder based on the data distillation method
image_decoder = DistilledUNetDecoder(in_channels = 256, out_channels = 3)

# Define the diffusion model that combines the image encoder and decoder
diffusion_model = DiffusionModel(image_encoder, image_decoder)

# Move the models to GPU
clip_model.to('cuda')
diffusion_model.to('cuda')

# Define the loss function as a combination of MSE loss and CLIP loss
def loss_function(x, y):
    # x: the original image tensor of shape (batch_size, 3, height, width)
    # y: the denoised image tensor of shape (batch_size, 3, height, width)
    mse_loss = torch.nn.MSELoss()(x, y) # mean squared error between x and y
    clip_loss = -torch.mean(clip_model.logits_per_image(x, y)) # negative log-likelihood of x and y being in the same class according to CLIP model
    return lambda_mse * mse_loss + lambda_clip * clip_loss

# Define the optimizer as Adam
optimizer = torch.optim.Adam(diffusion_model.parameters(), lr = lr, betas = (beta1, beta2))

# Define the scheduler as a cosine annealing scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs)

# Define the training loop
def train():
    # Set the diffusion model to training mode
    diffusion_model.train()
    # Loop over the epochs
    for epoch in range(num_epochs):
        # Initialize the epoch loss
        epoch_loss = 0.0
        # Create a data loader for the train set
        train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)
        # Loop over the batches
        for batch in train_loader:
            # Get the image and caption tensors from the batch
            image, caption = batch
            # Move the tensors to GPU
            image = image.to('cuda')
            caption = caption.to('cuda')
            # Zero the gradients
            optimizer.zero_grad()
            # Apply data augmentation to the image tensor (e.g. random cropping, flipping, etc.)
            image_augmented = augment(image)
            # Apply noise to the image tensor according to a Gaussian distribution with variance proportional to the step number (e.g. sigma^2 = 0.01 * step / num_steps)
            image_noisy = add_noise(image_augmented)
            # Encode the image tensor using the image encoder
            latent = image_encoder(image_noisy)
            # Decode the latent tensor using the image decoder for each step with different learning rates (e.g. lr * (num_steps - step) / num_steps)
            for step in range(num_steps):
                # Set the learning rate for this step
                optimizer.lr = lr * (num_steps - step) / num_steps 
                # Decode the latent tensor using the image decoder 
                image_denoised = image_decoder(latent)
                # Compute the loss between the original image and the denoised image 
                loss = loss_function(image, image_denoised)
                # Backpropagate the loss and update the parameters
                loss.backward()
                optimizer.step()
                # Add the loss to the epoch loss
                epoch_loss += loss.item()
        # Print the average epoch loss
        print(f'Epoch {epoch}, Loss: {epoch_loss / len(train_loader)}')
        # Update the learning rate using the scheduler
        scheduler.step()

# Define the testing loop
def test():
    # Set the diffusion model to evaluation mode
    diffusion_model.eval()
    # Initialize the test loss and test metrics (e.g. FID, CLIP)
    test_loss = 0.0
    test_fid = 0.0
    test_clip = 0.0
    # Create a data loader for the test set
    test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = False)
    # Loop over the batches
    for batch in test_loader:
        # Get the image and caption tensors from the batch
        image, caption = batch
        # Move the tensors to GPU
        image = image.to('cuda')
        caption = caption.to('cuda')
        # Apply data augmentation to the image tensor (e.g. random cropping, flipping, etc.)
        image_augmented = augment(image)
        # Apply noise to the image tensor according to a Gaussian distribution with variance proportional to the step number (e.g. sigma^2 = 0.01 * step / num_steps)
        image_noisy = add_noise(image_augmented)
        # Encode the image tensor using the image encoder
        latent = image_encoder(image_noisy)
        # Decode the latent tensor using the image decoder for the last step 
        image_denoised = image_decoder(latent)
        # Compute the loss between the original image and the denoised image 
        loss = loss_function(image, image_denoised)
        # Add the loss to the test loss
        test_loss += loss.item()
        # Compute the metrics between the original image and the denoised image (e.g. FID, CLIP)
        fid = compute_fid(image, image_denoised)
        clip = compute_clip(image, image_denoised)
        # Add the metrics to the test metrics
        test_fid += fid
        test_clip += clip
    # Print the average test loss and test metrics
    print(f'Test Loss: {test_loss / len(test_loader)}')
    print(f'Test FID: {test_fid / len(test_loader)}')
    print(f'Test CLIP: {test_clip / len(test_loader)}')

# Run the training and testing loops
train()
test()
```