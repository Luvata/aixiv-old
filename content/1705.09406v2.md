---
title: 1705.09406v2 Multimodal Machine Learning  A Survey and Taxonomy
date: 2017-05-10
---

# [Multimodal Machine Learning: A Survey and Taxonomy](http://arxiv.org/abs/1705.09406v2)

authors: Tadas Baltru≈°aitis, Chaitanya Ahuja, Louis-Philippe Morency


## What, Why and How

[1]: https://arxiv.org/abs/1705.09406 "[1705.09406] Multimodal Machine Learning: A Survey and Taxonomy - arXiv.org"
[2]: https://www.researchgate.net/publication/360112473_Multi-Modal_Instruction_based_Reinforcement_Learning_using_MoME_Transformer "(PDF) Multi-Modal Instruction based Reinforcement Learning using MoME ..."
[3]: https://arxiv.org/pdf/1705.09406v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper is a survey and taxonomy of multimodal machine learning, which is a field that aims to build models that can process and relate information from multiple modalities, such as vision, speech, text, etc.
- **Why**: The paper argues that multimodal machine learning is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential, as it can enable artificial intelligence to better understand the world around us and interact with humans in natural ways.
- **How**: The paper presents a common taxonomy of multimodal machine learning based on five challenges: representation, translation, alignment, fusion, and co-learning. It reviews the recent advances and open problems in each challenge and provides examples of multimodal applications and datasets. It also discusses the future directions and opportunities for multimodal machine learning research.

## Main Contributions

[1]: https://arxiv.org/abs/1705.09406 "[1705.09406] Multimodal Machine Learning: A Survey and Taxonomy - arXiv.org"
[2]: https://www.researchgate.net/publication/360112473_Multi-Modal_Instruction_based_Reinforcement_Learning_using_MoME_Transformer "(PDF) Multi-Modal Instruction based Reinforcement Learning using MoME ..."
[3]: https://arxiv.org/pdf/1705.09406v2.pdf "arXiv.org e-Print archive"

The paper claims to make the following contributions[^1^][1]:

- It provides a comprehensive and up-to-date survey of multimodal machine learning research, covering various modalities, applications, and datasets.
- It proposes a new taxonomy of multimodal machine learning based on five key challenges: representation, translation, alignment, fusion, and co-learning.
- It identifies the current state-of-the-art methods and open problems for each challenge and discusses the future research directions and opportunities.

## Method Summary

[1]: https://arxiv.org/abs/1705.09406 "[1705.09406] Multimodal Machine Learning: A Survey and Taxonomy - arXiv.org"
[2]: https://www.researchgate.net/publication/360112473_Multi-Modal_Instruction_based_Reinforcement_Learning_using_MoME_Transformer "(PDF) Multi-Modal Instruction based Reinforcement Learning using MoME ..."
[3]: https://arxiv.org/pdf/1705.09406v2.pdf "arXiv.org e-Print archive"

The method section of the paper consists of five subsections, each corresponding to one of the challenges in the proposed taxonomy[^1^][1]:

- **Representation**: This subsection reviews the methods for learning multimodal representations, which are compact and meaningful representations of multimodal data that capture the common and complementary information across modalities. It covers three types of representation learning methods: feature extraction, feature transformation, and feature generation.
- **Translation**: This subsection reviews the methods for translating information from one modality to another, such as image captioning, speech recognition, and text-to-speech synthesis. It covers three types of translation methods: direct mapping, intermediate representation, and generative models.
- **Alignment**: This subsection reviews the methods for aligning information across modalities, such as temporal alignment, spatial alignment, and semantic alignment. It covers three types of alignment methods: supervised alignment, weakly supervised alignment, and unsupervised alignment.
- **Fusion**: This subsection reviews the methods for fusing information from multiple modalities, such as early fusion, late fusion, and hybrid fusion. It covers three types of fusion methods: feature-level fusion, decision-level fusion, and model-level fusion.
- **Co-learning**: This subsection reviews the methods for co-learning from multiple modalities, such as multimodal regularization, multimodal distillation, and multimodal curriculum learning. It covers three types of co-learning methods: single-task co-learning, multi-task co-learning, and meta-learning.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the multimodal data and the task
data = load_multimodal_data()
task = define_task()

# Choose the challenge and the method
challenge = choose_from(representation, translation, alignment, fusion, co-learning)
method = choose_from(challenge.methods)

# Apply the method to the data and the task
model = method(data, task)

# Evaluate the model performance
metrics = evaluate(model, data, task)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torchvision
import transformers
import nltk

# Define the multimodal data and the task
data = load_multimodal_data() # e.g. COCO dataset for image captioning
task = define_task() # e.g. generate captions for images

# Choose the challenge and the method
challenge = translation # e.g. translate images to text
method = generative_models # e.g. use a generative model to generate captions

# Apply the method to the data and the task
# Define the encoder and decoder networks
encoder = torchvision.models.resnet50(pretrained=True) # e.g. use ResNet-50 to encode images
decoder = transformers.models.bart.BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn') # e.g. use BART to decode captions

# Define the loss function and optimizer
criterion = torch.nn.CrossEntropyLoss() # e.g. use cross entropy loss for caption generation
optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001) # e.g. use Adam optimizer for decoder

# Define the training loop
for epoch in range(num_epochs):
  for batch in data:
    # Get the image and caption from the batch
    image = batch['image']
    caption = batch['caption']

    # Encode the image using the encoder network
    image_features = encoder(image)

    # Decode the caption using the decoder network
    output = decoder(input_ids=None, decoder_input_ids=caption, encoder_outputs=image_features)

    # Compute the loss using the criterion
    loss = criterion(output.logits, caption)

    # Update the decoder parameters using the optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Evaluate the model performance
metrics = evaluate(model, data, task) # e.g. use BLEU score for caption evaluation
```