---
title: 2304.07193v1 DINOv2  Learning Robust Visual Features without Supervision
date: 2023-04-08
---

# [DINOv2: Learning Robust Visual Features without Supervision](http://arxiv.org/abs/2304.07193v1)

authors: Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski


## What, Why and How

[1]: https://arxiv.org/pdf/2304.07193v1 "DINOv2: LearningRobustVisualFeatures withoutSupervision - arXiv.org"
[2]: https://arxiv.org/abs/2304.07193 "DINOv2: Learning Robust Visual Features without Supervision"
[3]: http://export.arxiv.org/abs/2305.07193v1 "[2305.07193v1] Aggressive Internet-Wide Scanners: Network Impact and ..."

Here is a summary of the paper:

- **What**: The paper presents DINOv2, a method for learning robust visual features without supervision by scaling up existing self-supervised pretraining techniques on a large and diverse image dataset.
- **Why**: The paper aims to produce all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning, similar to foundation models in natural language processing.
- **How**: The paper combines different techniques to scale up the pretraining in terms of data and model size, such as:
  - Building a dedicated, diverse, and curated image dataset using an automatic pipeline instead of uncurated data.
  - Training a ViT model with 1B parameters using a contrastive learning objective with momentum encoder and teacher-student distillation.
  - Distilling the large model into a series of smaller models that surpass the best available all-purpose features, OpenCLIP, on most of the benchmarks at image and pixel levels.

## Main Contributions

[1]: https://arxiv.org/pdf/2304.07193v1 "DINOv2: LearningRobustVisualFeatures withoutSupervision - arXiv.org"
[2]: https://arxiv.org/abs/2304.07193 "DINOv2: Learning Robust Visual Features without Supervision"
[3]: http://export.arxiv.org/abs/2305.07193v1 "[2305.07193v1] Aggressive Internet-Wide Scanners: Network Impact and ..."

The paper claims the following contributions:

- A method for learning robust visual features without supervision by scaling up existing self-supervised pretraining techniques on a large and diverse image dataset.
- A novel dataset construction pipeline that automatically selects and filters images from diverse sources to create a curated and balanced dataset for pretraining.
- A series of technical improvements to accelerate and stabilize the training of large ViT models with contrastive learning and distillation, such as:
  - Using a momentum encoder with a large queue size to increase the number of negative samples.
  - Using a teacher-student distillation scheme with a temperature parameter that adapts to the training progress.
  - Using a warmup phase with a lower learning rate and a smaller batch size to avoid early collapse.
  - Using gradient clipping and weight decay to regularize the training and prevent gradient explosion.
- An extensive evaluation of the pretrained models on various downstream tasks at image and pixel levels, showing that they surpass the state-of-the-art all-purpose features, OpenCLIP, on most of the benchmarks.

## Method Summary

[1]: https://arxiv.org/pdf/2304.07193v1 "DINOv2: LearningRobustVisualFeatures withoutSupervision - arXiv.org"
[2]: https://arxiv.org/abs/2304.07193 "DINOv2: Learning Robust Visual Features without Supervision"
[3]: http://export.arxiv.org/abs/2305.07193v1 "[2305.07193v1] Aggressive Internet-Wide Scanners: Network Impact and ..."

Here is a summary of the method section of the paper:

- The paper uses a self-supervised pretraining method based on contrastive learning, where a model learns to distinguish between different views of the same image and views of different images.
- The paper adopts the DINO framework (Caron et al., 2021), which uses a teacher-student distillation scheme with a momentum encoder and a queue of negative samples to implement the contrastive learning objective.
- The paper modifies the DINO framework in several ways to scale up the pretraining, such as:
  - Using a temperature parameter that adapts to the training progress to control the hardness of the contrastive learning objective.
  - Using a warmup phase with a lower learning rate and a smaller batch size to avoid early collapse of the features.
  - Using gradient clipping and weight decay to regularize the training and prevent gradient explosion.
- The paper uses Vision Transformer (ViT) models (Dosovitskiy et al., 2020) as the backbone architecture for the pretraining, and trains a large model with 1B parameters on a dedicated image dataset.
- The paper distills the large model into a series of smaller models using knowledge distillation (Hinton et al., 2015), where the smaller models learn to mimic the output of the large model on unlabeled data.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Pretraining phase
# Initialize a large ViT model as the teacher and a smaller ViT model as the student
teacher = ViT(large)
student = ViT(small)

# Initialize a momentum encoder for the teacher
momentum_encoder = MomentumEncoder(teacher)

# Initialize a queue of negative samples
queue = Queue()

# Initialize a temperature parameter that adapts to the training progress
temperature = Temperature()

# Initialize a warmup scheduler for the learning rate and the batch size
warmup_scheduler = WarmupScheduler()

# For each batch of images from the pretraining dataset
for images in dataset:

  # Apply data augmentation to create two views of each image
  views = augment(images)

  # Update the learning rate and the batch size according to the warmup scheduler
  lr, bs = warmup_scheduler.update()

  # Forward pass the views through the student and the teacher models
  student_features = student(views)
  teacher_features = teacher(views)

  # Update the momentum encoder with the teacher features
  momentum_encoder.update(teacher_features)

  # Enqueue the teacher features to the queue of negative samples
  queue.enqueue(teacher_features)

  # Compute the contrastive loss between the student and the teacher features using the queue and the temperature
  loss = contrastive_loss(student_features, teacher_features, queue, temperature)

  # Backward pass and update the student model parameters using gradient clipping and weight decay
  loss.backward()
  clip_and_decay_gradients(student.parameters())
  optimizer.step()

# Distillation phase
# Initialize a smaller ViT model as the distillation student
distill_student = ViT(smaller)

# For each batch of images from the distillation dataset
for images in dataset:

  # Forward pass the images through the distillation student and the teacher models
  distill_student_logits = distill_student(images)
  teacher_logits = teacher(images)

  # Compute the distillation loss between the distillation student and the teacher logits using softmax and KL divergence
  loss = distillation_loss(distill_student_logits, teacher_logits)

  # Backward pass and update the distillation student model parameters using gradient clipping and weight decay
  loss.backward()
  clip_and_decay_gradients(distill_student.parameters())
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
num_epochs = 100 # number of epochs for pretraining and distillation
batch_size = 256 # batch size for pretraining and distillation
lr = 0.0001 # initial learning rate for pretraining and distillation
momentum = 0.9 # momentum coefficient for the momentum encoder and the optimizer
weight_decay = 0.01 # weight decay coefficient for the optimizer
clip_value = 1.0 # gradient clipping value for the optimizer
queue_size = 65536 # queue size for the negative samples
temperature_init = 0.07 # initial temperature parameter for the contrastive loss
temperature_final = 0.04 # final temperature parameter for the contrastive loss
warmup_epochs = 10 # number of epochs for the warmup phase
teacher_model_size = "ViT-B/32" # model size for the teacher model
student_model_size = "ViT-B/16" # model size for the student model
distill_student_model_size = "ViT-L/16" # model size for the distillation student model

# Define the data augmentation transforms
# Random resized crop, random horizontal flip, color jitter, grayscale, gaussian blur, solarization
transform = torchvision.transforms.Compose([
  torchvision.transforms.RandomResizedCrop(224),
  torchvision.transforms.RandomHorizontalFlip(),
  torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),
  torchvision.transforms.RandomGrayscale(p=0.2),
  torchvision.transforms.GaussianBlur(23, sigma=(0.1, 2.0)),
  torchvision.transforms.RandomApply([torchvision.transforms.Lambda(lambda x: x * torch.randint(2, (1,), dtype=torch.float).item())], p=0.2)
])

# Define the contrastive loss function
# Input: student_features: a tensor of shape (batch_size, feature_dim)
#        teacher_features: a tensor of shape (batch_size, feature_dim)
#        queue: a tensor of shape (queue_size, feature_dim)
#        temperature: a scalar value
# Output: loss: a scalar value
def contrastive_loss(student_features, teacher_features, queue, temperature):

  # Normalize the student and teacher features along the feature dimension
  student_features = student_features / torch.norm(student_features, dim=1, keepdim=True)
  teacher_features = teacher_features / torch.norm(teacher_features, dim=1, keepdim=True)

  # Concatenate the teacher features and the queue to form the set of negative samples
  negatives = torch.cat([teacher_features, queue], dim=0)

  # Compute the logits between the student features and the negative samples using matrix multiplication
  logits = torch.mm(student_features, negatives.t())

  # Divide the logits by the temperature to control the hardness of the contrastive loss
  logits = logits / temperature

  # Compute the labels for the contrastive loss using the index of the teacher features in the negative samples
  labels = torch.arange(batch_size).to(logits.device)

  # Compute the contrastive loss using cross entropy with the logits and the labels
  loss = torch.nn.functional.cross_entropy(logits, labels)

  # Return the loss value
  return loss

# Define the distillation loss function
# Input: distill_student_logits: a tensor of shape (batch_size, num_classes)
#        teacher_logits: a tensor of shape (batch_size, num_classes)
# Output: loss: a scalar value
def distillation_loss(distill_student_logits, teacher_logits):

  # Apply softmax to the distillation student and teacher logits along the class dimension
  distill_student_probs = torch.nn.functional.softmax(distill_student_logits, dim=1)
  teacher_probs = torch.nn.functional.softmax(teacher_logits, dim=1)

  # Compute the distillation loss using KL divergence between the distillation student and teacher probabilities
  loss = torch.nn.functional.kl_div(distill_student_probs.log(), teacher_probs, reduction="batchmean")

  # Return the loss value
  return loss

# Load the pretraining dataset using torchvision.datasets.ImageFolder with the data augmentation transform
pretrain_dataset = torchvision.datasets.ImageFolder("pretrain_data", transform=transform)

# Load the distillation dataset using torchvision.datasets.ImageFolder with a standard transform (resize and center crop)
distill_dataset = torchvision.datasets.ImageFolder("distill_data", transform=torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224)
]))

# Create data loaders for the pretraining and distillation datasets using torch.utils.data.DataLoader with the batch size
pretrain_loader = torch.utils.data.DataLoader(pretrain_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
distill_loader = torch.utils.data.DataLoader(distill_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

# Load the teacher model using torchvision.models.vision_transformer with the teacher model size and pretrained weights
teacher = torchvision.models.vision_transformer(teacher_model_size, pretrained=True)

# Load the student model using torchvision.models.vision_transformer with the student model size and random weights
student = torchvision.models.vision_transformer(student_model_size, pretrained=False)

# Load the distillation student model using torchvision.models.vision_transformer with the distillation student model size and random weights
distill_student = torchvision.models.vision_transformer(distill_student_model_size, pretrained=False)

# Create a momentum encoder for the teacher model using a copy of the teacher model and the momentum coefficient
momentum_encoder = teacher.copy()
momentum_encoder.load_state_dict(teacher.state_dict())
for param in momentum_encoder.parameters():
  param.requires_grad = False
momentum_update = lambda p1, p2: p1 * momentum + p2 * (1 - momentum)

# Create a queue for the negative samples using a tensor of zeros with the queue size and the feature dimension
queue = torch.zeros(queue_size, teacher.num_features)

# Create a temperature parameter for the contrastive loss using the initial temperature value
temperature = temperature_init

# Create an optimizer for the student model using torch.optim.SGD with the learning rate, momentum, and weight decay
optimizer = torch.optim.SGD(student.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)

# Create an optimizer for the distillation student model using torch.optim.SGD with the learning rate, momentum, and weight decay
distill_optimizer = torch.optim.SGD(distill_student.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)

# Create a learning rate scheduler for the optimizer using torch.optim.lr_scheduler.CosineAnnealingLR with the number of epochs
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)

# Create a learning rate scheduler for the distill optimizer using torch.optim.lr_scheduler.CosineAnnealingLR with the number of epochs
distill_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(distill_optimizer, num_epochs)

# Set the teacher and student models to train mode
teacher.train()
student.train()

# Set the distillation student model to eval mode
distill_student.eval()

# Pretraining loop
# For each epoch from 1 to num_epochs
for epoch in range(1, num_epochs + 1):

  # Initialize the epoch loss to zero
  epoch_loss = 0.0

  # For each batch of images and labels from the pretrain loader
  for images, labels in pretrain_loader:

    # Move the images and labels to the device (CPU or GPU)
    images = images.to(device)
    labels = labels.to(device)

    # Apply data augmentation to create two views of each image
    views = torch.cat([transform(image) for image in images], dim=0)

    # Forward pass the views through the student and teacher models to get their features
    student_features = student(views)
    with torch.no_grad():
      teacher_features = teacher(views)

    # Update the momentum encoder with the teacher features using the momentum update function
    with torch.no_grad():
      for param_q, param_k in zip(teacher.parameters(), momentum_encoder.parameters()):
        param_k.data.copy_(momentum_update(param_k.data, param_q.data))

    # Enqueue the teacher features to the queue of negative samples and dequeue the oldest samples
    with torch.no_grad():
      queue = torch.cat([teacher_features, queue], dim=0)
      queue = queue[:queue_size]

    # Compute the contrastive loss between the student and teacher features using the queue and the temperature parameter
    loss = contrastive_loss(student_features, teacher_features.detach(), queue.detach(), temperature)

    # Backward pass and update the student model parameters using gradient clipping and weight decay
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(student.parameters(), clip_value)
    optimizer.step()

    # Update the temperature parameter linearly from the initial value to the final value
    temperature = temperature_init + (temperature_final - temperature_init) * (epoch / num_epochs)

    # Update the epoch loss with the batch loss
    epoch_loss += loss.item()

  # Update the learning rate scheduler
  scheduler.step()

  # Print the epoch loss
  print(f"Epoch {epoch}, Loss: {epoch_loss / len(pretrain_loader)}")

# Save the student model weights
torch.save(student.state_dict(), "student.pth")