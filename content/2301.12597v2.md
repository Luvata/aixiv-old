---
title: 2301.12597v2 BLIP-2  Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models
date: 2023-01-13
---

# [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](http://arxiv.org/abs/2301.12597v2)

authors: Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi


## What, Why and How

[1]: https://arxiv.org/pdf/2301.12597v2.pdf "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image ..."
[2]: https://arxiv.org/abs/2301.12597 "[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with ..."
[3]: http://export.arxiv.org/abs/2301.12597 "[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with ..."

Here is a summary of the paper:

- **What**: The paper proposes **BLIP-2**, a method for vision-language pre-training that uses **frozen pre-trained image encoders and frozen large language models** as inputs and outputs[^1^][1].
- **Why**: The paper aims to reduce the **computation cost** of vision-language pre-training, which usually requires end-to-end training of large-scale models and datasets[^1^][1].
- **How**: The paper introduces a **lightweight Querying Transformer**, which is pre-trained in two stages: the first stage bootstraps vision-language representation learning from a frozen image encoder, and the second stage bootstraps vision-to-language generative learning from a frozen language model[^1^][1]. The paper evaluates the method on various vision-language tasks and shows that it achieves state-of-the-art performance with significantly fewer trainable parameters than existing methods[^1^][1]. The paper also demonstrates the model's capability of zero-shot image-to-text generation that can follow natural language instructions[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2301.12597v2.pdf "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image ..."
[2]: https://arxiv.org/abs/2301.12597 "[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with ..."
[3]: http://export.arxiv.org/abs/2301.12597 "[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with ..."

The paper claims the following contributions:

- It proposes **BLIP-2**, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models[^1^][1].
- It introduces a **lightweight Querying Transformer**, which is pre-trained in two stages to bridge the modality gap between vision and language[^1^][1].
- It achieves **state-of-the-art performance** on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods[^1^][1].
- It demonstrates the model's **emerging capabilities** of zero-shot image-to-text generation that can follow natural language instructions[^1^][1].

## Method Summary

[1]: https://arxiv.org/pdf/2301.12597v2.pdf "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image ..."
[2]: https://arxiv.org/abs/2301.12597 "[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with ..."
[3]: http://export.arxiv.org/abs/2301.12597 "[2301.12597] BLIP-2: Bootstrapping Language-Image Pre-training with ..."

Here is a summary of the method section of the paper:

- The paper introduces a **lightweight Querying Transformer (Q-Former)**, which consists of a **query encoder** and a **cross-modal attention layer**[^1^][1].
- The query encoder encodes a set of queries that are either natural language sentences or image regions into query embeddings[^1^][1].
- The cross-modal attention layer computes the attention weights between the query embeddings and the frozen pre-trained image encoder or language model outputs[^1^][1].
- The paper pre-trains the Q-Former in two stages: **vision-language representation learning** and **vision-to-language generative learning**[^1^][1].
- In the first stage, the paper uses masked language modeling (MLM) and masked region modeling (MRM) as the pre-training objectives, where the queries are either masked tokens or masked regions from the input image-text pairs[^1^][1].
- In the second stage, the paper uses conditional masked language modeling (CMLM) as the pre-training objective, where the queries are natural language instructions that specify what to generate from the input image[^1^][1].
- The paper uses a frozen large language model (LLM) as the decoder to generate text conditioned on the Q-Former outputs[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Q-Former model
class QFormer(nn.Module):
  def __init__(self):
    # Initialize the query encoder and the cross-modal attention layer
    self.query_encoder = QueryEncoder()
    self.cross_modal_attention = CrossModalAttention()

  def forward(self, queries, image_encoder_outputs, language_model_outputs):
    # Encode the queries into query embeddings
    query_embeddings = self.query_encoder(queries)
    # Compute the cross-modal attention weights
    cross_modal_weights = self.cross_modal_attention(query_embeddings, image_encoder_outputs, language_model_outputs)
    # Return the weighted sum of the image encoder outputs and the language model outputs
    return cross_modal_weights @ (image_encoder_outputs + language_model_outputs)

# Define the pre-training objectives
def MLM(query_embeddings, language_model_outputs, masked_tokens):
  # Predict the masked tokens using the query embeddings and the language model outputs
  return CrossEntropyLoss(query_embeddings @ language_model_outputs.T, masked_tokens)

def MRM(query_embeddings, image_encoder_outputs, masked_regions):
  # Predict the masked regions using the query embeddings and the image encoder outputs
  return CrossEntropyLoss(query_embeddings @ image_encoder_outputs.T, masked_regions)

def CMLM(query_embeddings, image_encoder_outputs, language_model_outputs, instructions):
  # Generate text from the image using the query embeddings, the image encoder outputs, and the language model outputs
  return LLM.generate(query_embeddings @ (image_encoder_outputs + language_model_outputs), instructions)

# Pre-train the Q-Former in two stages
def pre_train():
  # Load the pre-trained image encoder and language model
  image_encoder = load_pretrained_image_encoder()
  LLM = load_pretrained_language_model()
  # Freeze their parameters
  image_encoder.requires_grad = False
  LLM.requires_grad = False
  # Initialize the Q-Former model
  Q_former = QFormer()
  # Initialize the optimizer and the scheduler
  optimizer = Adam(Q_former.parameters())
  scheduler = CosineAnnealingLR(optimizer)
  # Loop over the pre-training epochs
  for epoch in range(num_epochs):
    # Loop over the pre-training batches
    for batch in pre_training_data_loader:
      # Stage 1: vision-language representation learning
      # Get the image-text pairs and the masked tokens and regions from the batch
      images, texts, masked_tokens, masked_regions = batch
      # Get the image encoder outputs and the language model outputs
      image_encoder_outputs = image_encoder(images)
      language_model_outputs = LLM(texts)
      # Get the queries as either masked tokens or masked regions
      queries = random.choice([masked_tokens, masked_regions])
      # Forward pass through the Q-Former model
      Q_former_outputs = Q_former(queries, image_encoder_outputs, language_model_outputs)
      # Compute the MLM or MRM loss
      if queries == masked_tokens:
        loss = MLM(Q_former_outputs, language_model_outputs, masked_tokens)
      else:
        loss = MRM(Q_former_outputs, image_encoder_outputs, masked_regions)
      # Backward pass and update the Q-Former parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      # Stage 2: vision-to-language generative learning
      # Get the images and the instructions from another batch
      images, instructions = another_batch
      # Get the image encoder outputs and the language model outputs
      image_encoder_outputs = image_encoder(images)
      language_model_outputs = LLM(instructions)
      # Get the queries as natural language instructions
      queries = instructions
      # Forward pass through the Q-Former model
      Q_former_outputs = Q_former(queries, image_encoder_outputs, language_model_outputs)
      # Compute the CMLM loss
      loss = CMLM(Q_former_outputs, image_encoder_outputs, language_model_outputs, instructions)
      # Backward pass and update the Q-Former parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    # Update the learning rate scheduler
    scheduler.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import transformers

# Define the hyperparameters
num_epochs = 10 # number of pre-training epochs
batch_size = 32 # batch size for pre-training data loader
lr = 1e-4 # learning rate for optimizer
max_length = 128 # maximum length for language model inputs and outputs

# Define the QueryEncoder class
class QueryEncoder(nn.Module):
  def __init__(self, hidden_size):
    # Initialize the parent class
    super(QueryEncoder, self).__init__()
    # Initialize the query embedding layer
    self.query_embedding = nn.Embedding(num_embeddings=hidden_size, embedding_dim=hidden_size)
    # Initialize the query position embedding layer
    self.query_position_embedding = nn.Embedding(num_embeddings=max_length, embedding_dim=hidden_size)
    # Initialize the query type embedding layer
    self.query_type_embedding = nn.Embedding(num_embeddings=2, embedding_dim=hidden_size) # 0 for text, 1 for image

  def forward(self, queries):
    # Get the query lengths and types from the queries
    query_lengths = queries.ne(0).sum(dim=-1) # non-zero elements are query tokens or regions
    query_types = queries.gt(0).long() # positive elements are text tokens, negative elements are image regions
    # Get the query embeddings from the query embedding layer
    query_embeddings = self.query_embedding(queries.abs()) # absolute value to handle negative image regions
    # Get the query position embeddings from the query position embedding layer
    query_positions = torch.arange(max_length).unsqueeze(0).repeat(batch_size, 1).to(queries.device) # create position indices
    query_position_embeddings = self.query_position_embedding(query_positions)
    # Get the query type embeddings from the query type embedding layer
    query_type_embeddings = self.query_type_embedding(query_types)
    # Add the query embeddings, the query position embeddings, and the query type embeddings
    query_embeddings = query_embeddings + query_position_embeddings + query_type_embeddings
    # Return the query embeddings
    return query_embeddings

# Define the CrossModalAttention class
class CrossModalAttention(nn.Module):
  def __init__(self, hidden_size):
    # Initialize the parent class
    super(CrossModalAttention, self).__init__()
    # Initialize the query projection layer
    self.query_projection = nn.Linear(hidden_size, hidden_size)
    # Initialize the key projection layer
    self.key_projection = nn.Linear(hidden_size, hidden_size)
    # Initialize the value projection layer
    self.value_projection = nn.Linear(hidden_size, hidden_size)
    # Initialize the softmax layer
    self.softmax = nn.Softmax(dim=-1)

  def forward(self, query_embeddings, image_encoder_outputs, language_model_outputs):
    # Project the query embeddings into query vectors using the query projection layer
    query_vectors = self.query_projection(query_embeddings)
    # Project the image encoder outputs and the language model outputs into key vectors using the key projection layer
    key_vectors = self.key_projection(torch.cat([image_encoder_outputs, language_model_outputs], dim=1))
    # Project the image encoder outputs and the language model outputs into value vectors using the value projection layer
    value_vectors = self.value_projection(torch.cat([image_encoder_outputs, language_model_outputs], dim=1))
    # Compute the dot product between the query vectors and the key vectors and scale by square root of hidden size
    attention_scores = torch.bmm(query_vectors, key_vectors.transpose(1, 2)) / torch.sqrt(hidden_size)
    # Apply softmax to get the attention weights
    attention_weights = self.softmax(attention_scores)
    # Compute the weighted sum of the value vectors using the attention weights
    attention_outputs = torch.bmm(attention_weights, value_vectors)
    # Return the attention outputs
    return attention_outputs

# Load the pre-trained image encoder and language model
image_encoder = models.resnet50(pretrained=True) # use ResNet-50 as image encoder
LLM = transformers.AutoModelForCausalLM.from_pretrained("gpt2") # use GPT-2 as language model

# Freeze their parameters
for param in image_encoder.parameters():
  param.requires_grad = False

for param in LLM.parameters():
  param.requires_grad = False

# Initialize the Q-Former model with hidden size equal to LLM's hidden size
Q_former = QFormer(hidden_size=LLM.config.hidden_size)

# Initialize the optimizer and the scheduler for Q-Former parameters only
optimizer = optim.Adam(Q_former.parameters(), lr=lr)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Load the pre-training data
pre_training_data = load_pre_training_data()
pre_training_data_loader = torch.utils.data.DataLoader(pre_training_data, batch_size=batch_size, shuffle=True)

# Loop over the pre-training epochs
for epoch in range(num_epochs):
  # Loop over the pre-training batches
  for batch in pre_training_data_loader:
    # Stage 1: vision-language representation learning
    # Get the image-text pairs and the masked tokens and regions from the batch
    images, texts, masked_tokens, masked_regions = batch
    # Get the image encoder outputs by passing the images through the image encoder
    image_encoder_outputs = image_encoder(images)
    # Get the language model outputs by passing the texts through the language model
    language_model_outputs = LLM(texts).last_hidden_state
    # Get the queries as either masked tokens or masked regions randomly
    queries = random.choice([masked_tokens, masked_regions])
    # Forward pass through the Q-Former model by passing the queries, the image encoder outputs, and the language model outputs
    Q_former_outputs = Q_former(queries, image_encoder_outputs, language_model_outputs)
    # Compute the MLM or MRM loss by passing the Q-Former outputs, the language model outputs, and the masked tokens or regions
    if queries == masked_tokens:
      loss = MLM(Q_former_outputs, language_model_outputs, masked_tokens)
    else:
      loss = MRM(Q_former_outputs, image_encoder_outputs, masked_regions)
    # Backward pass and update the Q-Former parameters using the optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # Stage 2: vision-to-language generative learning
    # Get another batch of images and instructions for image-to-text generation
    images, instructions = get_another_batch()
    # Get the image encoder outputs by passing the images through the image encoder
    image_encoder_outputs = image_encoder(images)
    # Get the language model outputs by passing the instructions through the language model
    language_model_outputs = LLM(instructions).last_hidden_state
    # Get the queries as natural language instructions
    queries = instructions
    # Forward pass through the Q-Former model by passing the queries, the image encoder outputs, and the language model outputs
    Q_former_outputs = Q_former(queries, image_encoder_outputs, language_model_outputs)
    # Compute the CMLM loss by passing the Q-Former outputs, the image encoder outputs, the language model outputs, and the instructions to the LLM's generate method
    loss = CMLM(Q_former_outputs, image_encoder_outputs, language_model_outputs, instructions)
    # Backward pass and update the Q-Former parameters using the optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  # Update the learning rate scheduler
  scheduler.step()
```