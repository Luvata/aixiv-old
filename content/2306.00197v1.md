---
title: 2306.00197v1 SSL-CPCD  Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis
date: 2023-06-01
---

# [SSL-CPCD: Self-supervised learning with composite pretext-class discrimination for improved generalisability in endoscopic image analysis](http://arxiv.org/abs/2306.00197v1)

authors: Ziang Xu, Jens Rittscher, Sharib Ali


## What, Why and How

[1]: https://arxiv.org/abs/2306.00197 "[2306.00197] SSL-CPCD: Self-supervised learning with ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.00975 "[2306.00975] Active Reinforcement Learning under Limited Visual ..."
[3]: http://export.arxiv.org/abs/2306.00197 "[2306.00197] SSL-CPCD: Self-supervised learning with composite pretext ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel self-supervised learning method for endoscopic image analysis, called SSL-CPCD, which stands for Self-supervised learning with composite pretext-class discrimination.
- **Why**: The paper aims to address the challenges of supervised learning methods for endoscopic image analysis, such as the need for large amounts of annotated data and the lack of generalisability to unseen datasets. The paper also aims to improve the performance of existing self-supervised learning methods for medical image domain, which have a considerable gap compared to natural scene data.
- **How**: The paper introduces two novel components to enhance self-supervised learning: patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics. The former enables the model to learn to cluster similar representative patches, while the latter improves the separation between different classes. The paper evaluates the proposed method on three downstream tasks: classification, detection, and segmentation of endoscopic images, and demonstrates significant improvement over the state-of-the-art methods on both same and diverse datasets.

## Main Contributions

The paper claims the following contributions:

- A novel self-supervised learning method for endoscopic image analysis that leverages patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics.
- A comprehensive evaluation of the proposed method on three downstream tasks: classification, detection, and segmentation of endoscopic images, using both same and diverse datasets.
- A demonstration of the superior performance and generalisability of the proposed method over the state-of-the-art methods for self-supervised learning in medical image domain.

## Method Summary

[1]: https://arxiv.org/abs/2306.00197 "[2306.00197] SSL-CPCD: Self-supervised learning with ... - arXiv.org"
[2]: https://arxiv.org/abs/2306.00975 "[2306.00975] Active Reinforcement Learning under Limited Visual ..."
[3]: http://export.arxiv.org/abs/2306.00197 "[2306.00197] SSL-CPCD: Self-supervised learning with composite pretext ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a self-supervised learning method for endoscopic image analysis, which consists of three main components: a backbone network, a projection head, and a classification head.
- The backbone network is a convolutional neural network that extracts features from input images. The paper uses ResNet-50 as the backbone network and pre-trains it on ImageNet.
- The projection head is a two-layer fully connected network that maps the features from the backbone network to a lower-dimensional latent space. The paper uses ReLU activation and batch normalization for the projection head.
- The classification head is a linear layer that predicts the class labels for the input images. The paper uses softmax activation and cross-entropy loss for the classification head.
- The paper introduces two novel components to enhance self-supervised learning: patch-level instance-group discrimination and penalisation of inter-class variation using additive angular margin within the cosine similarity metrics.
- Patch-level instance-group discrimination is a technique that divides an input image into patches and assigns each patch a unique label based on its position and orientation. The paper uses 16 patches per image and 8 orientations per patch. The classification head then learns to discriminate between different patches and different orientations, which encourages the model to learn fine-grained features and spatial relations.
- Penalisation of inter-class variation using additive angular margin is a technique that modifies the cosine similarity metric between the features and the class labels by adding an angular margin term. The paper uses 0.35 as the angular margin parameter. This technique penalizes the features that are close to the decision boundary and improves the separation between different classes, which enhances the generalisability of the model.

## Pseudo Code

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np

# Define hyperparameters
batch_size = 64 # batch size for training and testing
num_epochs = 200 # number of epochs for training
learning_rate = 0.01 # learning rate for optimizer
weight_decay = 1e-4 # weight decay for optimizer
momentum = 0.9 # momentum for optimizer
temperature = 0.07 # temperature parameter for softmax
margin = 0.35 # angular margin parameter for cosine similarity

# Define data augmentation transforms
transform_train = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224), # randomly crop and resize images to 224x224
    torchvision.transforms.RandomHorizontalFlip(), # randomly flip images horizontally
    torchvision.transforms.RandomRotation(360), # randomly rotate images by 360 degrees
    torchvision.transforms.ToTensor(), # convert images to tensors
    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # normalize images using ImageNet mean and std
])

transform_test = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256), # resize images to 256x256
    torchvision.transforms.CenterCrop(224), # crop images to 224x224 from the center
    torchvision.transforms.ToTensor(), # convert images to tensors
    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # normalize images using ImageNet mean and std
])

# Define data loaders
train_loader = torch.utils.data.DataLoader(
    torchvision.datasets.ImageFolder('path/to/train/dataset', transform=transform_train), # load train dataset from folder
    batch_size=batch_size, shuffle=True, num_workers=4) # set batch size, shuffle and num workers

test_loader = torch.utils.data.DataLoader(
    torchvision.datasets.ImageFolder('path/to/test/dataset', transform=transform_test), # load test dataset from folder
    batch_size=batch_size, shuffle=False, num_workers=4) # set batch size, shuffle and num workers

# Define backbone network
backbone = torchvision.models.resnet50(pretrained=True) # use ResNet-50 pre-trained on ImageNet as backbone network
backbone.fc = torch.nn.Identity() # remove the last fully connected layer of ResNet-50

# Define projection head
projection = torch.nn.Sequential(
    torch.nn.Linear(2048, 512), # first linear layer with input size 2048 and output size 512
    torch.nn.ReLU(), # ReLU activation function
    torch.nn.BatchNorm1d(512), # batch normalization layer with input size 512
    torch.nn.Linear(512, 128), # second linear layer with input size 512 and output size 128
)

# Define classification head
classification = torch.nn.Linear(128, 16 * 8) # linear layer with input size 128 and output size 16 * 8 (number of patches * number of orientations)

# Define optimizer and scheduler
optimizer = torch.optim.SGD(list(backbone.parameters()) + list(projection.parameters()) + list(classification.parameters()), lr=learning_rate, weight_decay=weight_decay, momentum=momentum) # use stochastic gradient descent with weight decay and momentum as optimizer for all parameters
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs) # use cosine annealing learning rate scheduler with max epochs as T_max

# Define device (GPU or CPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # use GPU if available else use CPU

# Move models to device
backbone.to(device)
projection.to(device)
classification.to(device)

# Define function to compute cosine similarity with angular margin
def cosine_similarity_with_margin(x, y):
    x_norm = torch.nn.functional.normalize(x, dim=1) # normalize x along dimension 1
    y_norm = torch.nn.functional.normalize(y, dim=1) # normalize y along dimension 1
    cos_sim = torch.matmul(x_norm, y_norm.t()) / temperature # compute cosine similarity between x and y and divide by temperature parameter
    theta = torch.acos(torch.clamp(cos_sim.detach(), -1.0 + 1e-7, 1.0 - 1e-7)) + margin # compute the angle between x and y and add the margin parameter (clamp values to avoid nan)
    cos_sim_margin = torch.cos(theta) * temperature # compute the cosine similarity with margin and multiply by temperature parameter
    return cos_sim_margin # return the cosine similarity with margin

# Define function to compute cross entropy loss with hard negative mining
def cross_entropy_loss_with_hard_negative_mining(logits, labels):
    mask_pos = torch.eye(batch_size, device=device).bool() # create a mask for positive pairs (diagonal elements)
    mask_neg = ~mask_pos # create a mask for negative pairs (non-diagonal elements)
    logits_pos = logits[mask_pos].view(batch_size, 1) # get the logits for positive pairs
    logits_neg = logits[mask_neg].view(batch_size, -1) # get the logits for negative pairs
    neg_hard = torch.max(logits_neg, dim=1)[0] # get the hard negative logits (maximum among negative pairs)
    logits_new = torch.cat((logits_pos, neg_hard.unsqueeze(1)), dim=1) # concatenate the positive and hard negative logits
    labels_new = torch.zeros(batch_size, device=device).long() # create new labels with all zeros
    loss = torch.nn.functional.cross_entropy(logits_new, labels_new) # compute the cross entropy loss
    return loss # return the loss

# Define function to train one epoch
def train_one_epoch(epoch):
    backbone.train() # set backbone to train mode
    projection.train() # set projection to train mode
    classification.train() # set classification to train mode
    train_loss = 0.0 # initialize train loss to zero
    for i, (images, _) in enumerate(train_loader): # loop over batches of images and labels
        images = images.to(device) # move images to device
        batch_size = images.size(0) # get batch size
        images = images.view(batch_size * 2, 3, 224, 224) # reshape images to have two views per image
        features = backbone(images) # get features from backbone network
        features = projection(features) # get features from projection head
        features_1, features_2 = torch.split(features, [batch_size, batch_size], dim=0) # split features into two views
        logits_1 = classification(features_1) # get logits from classification head for view 1
        logits_2 = classification(features_2) # get logits from classification head for view 2
        sim_12 = cosine_similarity_with_margin(features_1, features_2) # get cosine similarity with margin between view 1 and view 2
        sim_21 = cosine_similarity_with_margin(features_2, features_1) # get cosine similarity with margin between view 2 and view 1
        loss_12 = cross_entropy_loss_with_hard_negative_mining(sim_12, torch.arange(batch_size, device=device)) # get cross entropy loss with hard negative mining for view 1 and view 2
        loss_21 = cross_entropy_loss_with_hard_negative_mining(sim_21, torch.arange(batch_size, device=device)) # get cross entropy loss with hard negative mining for view 2 and view 1
        loss = (loss_12 + loss_21) / 2.0 + (torch.nn.functional.cross_entropy(logits_1, torch.arange(batch_size * 8, device=device)) + torch.nn.functional.cross_entropy(logits_2, torch.arange(batch_size * 8, device=device))) / (batch_size * 8.0) # compute the total loss as the average of the two losses plus the classification losses
        optimizer.zero_grad() # zero the gradients of the optimizer
        loss.backward() # perform backpropagation on the loss
        optimizer.step() # update the parameters of the optimizer
        train_loss += loss.item() # update the train loss
        if (i + 1) % 10 == 0: # print the train loss every 10 batches
            print(f'Epoch {epoch}, Batch {i + 1}, Train Loss: {train_loss / (i + 1)}') # print the epoch, batch and train loss

# Define function to test one epoch
def test_one_epoch(epoch):
    backbone.eval() # set backbone to eval mode
    projection.eval() # set projection to eval mode
    classification.eval() # set classification to eval mode
    test_loss = 0.0 # initialize test loss to zero
    test_acc = 0.0 # initialize test accuracy to zero
    with torch.no_grad(): # disable gradient computation
        for i, (images, labels) in enumerate(test_loader): # loop over batches of images and labels
            images = images.to(device) # move images to device
            labels = labels.to(device) # move labels to device
            batch_size = images.size(0) # get batch size
            features = backbone(images) # get features from backbone network
            features = projection(features) # get features from projection head
            logits = classification(features)

```