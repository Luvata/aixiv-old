---
title: 2111.07783v1 FILIP  Fine-grained Interactive Language-Image Pre-Training
date: 2021-11-08
---

# [FILIP: Fine-grained Interactive Language-Image Pre-Training](http://arxiv.org/abs/2111.07783v1)

authors: Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, Chunjing Xu


## What, Why and How

[1]: https://arxiv.org/pdf/2111.07783v1.pdf "FILIP: F INTERACTIVE LANGUAGE P -T - arXiv.org"
[2]: https://arxiv.org/abs/2111.07783v1 "FILIP: Fine-grained Interactive Language-Image Pre-Training"
[3]: http://export.arxiv.org/abs/2111.07783 "[2111.07783] FILIP: Fine-grained Interactive Language-Image Pre ... - arXiv"

Here is a summary of the paper:

- **What**: The paper introduces a large-scale Fine-grained Interactive Language-Image Pre-training (FILIP) model that learns to align visual and textual tokens using a cross-modal late interaction mechanism and a contrastive objective.
- **Why**: The paper aims to achieve finer-level cross-modal interaction than existing methods that either use global features or cross/self-attention, which miss sufficient information or suffer from inferior efficiency.
- **How**: The paper proposes to use a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective, which allows FILIP to leverage the finer-grained expressiveness between image patches and textual words. The paper also constructs a new large-scale image-text pair dataset called FILIP300M for pre-training. The paper evaluates FILIP on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval, and shows that FILIP achieves state-of-the-art performance and promising localization ability.

The summary is based on the abstract[^1^][1] and the introduction[^2^][2] of the paper. You can read the full paper here: https://arxiv.org/pdf/2111.07783v1.pdf


## Main Contributions

[1]: https://arxiv.org/pdf/2111.07783v1.pdf "FILIP: F INTERACTIVE LANGUAGE P -T - arXiv.org"
[2]: https://arxiv.org/abs/2111.07783v1 "FILIP: Fine-grained Interactive Language-Image Pre-Training"
[3]: http://export.arxiv.org/abs/2111.07783 "[2111.07783] FILIP: Fine-grained Interactive Language-Image Pre ... - arXiv"

According to the paper, the main contributions are:

- **A novel cross-modal late interaction mechanism** that uses a token-wise maximum similarity between visual and textual tokens to guide the contrastive objective, which enables finer-grained alignment and expressiveness between image patches and textual words.
- **A new large-scale image-text pair dataset** called FILIP300M for pre-training, which contains 300 million image-text pairs collected from the Internet with high quality and diversity.
- **State-of-the-art performance** on multiple downstream vision-language tasks including zero-shot image classification and image-text retrieval, as well as promising localization ability demonstrated by the visualization on word-patch alignment.

The contributions are stated in the abstract[^1^][1] and the conclusion[^2^][2] of the paper. You can read the full paper here: https://arxiv.org/pdf/2111.07783v1.pdf


## Method Summary

[1]: https://arxiv.org/pdf/2111.07783v1.pdf "FILIP: F INTERACTIVE LANGUAGE P -T - arXiv.org"
[2]: https://arxiv.org/abs/2111.07783v1 "FILIP: Fine-grained Interactive Language-Image Pre-Training"
[3]: http://export.arxiv.org/abs/2111.07783 "[2111.07783] FILIP: Fine-grained Interactive Language-Image Pre ... - arXiv"

Here is a summary of the method section of the paper:

- **Model Architecture**: The paper adopts a dual-stream model architecture that consists of two encoders: a visual encoder and a textual encoder. The visual encoder is based on Vision Transformer (ViT) (Dosovitskiy et al., 2020) that takes image patches as input and outputs visual tokens. The textual encoder is based on BERT (Devlin et al., 2019) that takes word pieces as input and outputs textual tokens. Both encoders share the same hidden size and output dimension.
- **Cross-modal Late Interaction**: The paper proposes a cross-modal late interaction mechanism that computes the token-wise maximum similarity between visual and textual tokens to guide the contrastive objective. Specifically, for each visual token, the paper finds the most similar textual token based on cosine similarity, and vice versa. Then, the paper uses these maximum similarities to form a similarity matrix, which is used to calculate the contrastive loss. The paper argues that this mechanism can achieve finer-level alignment and expressiveness between image patches and textual words, while keeping the inference efficiency of the dual-stream model.
- **Pre-training Dataset**: The paper constructs a new large-scale image-text pair dataset called FILIP300M for pre-training. The dataset contains 300 million image-text pairs collected from the Internet with high quality and diversity. The paper uses various filtering techniques to ensure the relevance and cleanliness of the image-text pairs, such as removing duplicates, low-resolution images, adult content, etc. The paper also applies data augmentation techniques such as random cropping, color jittering, etc. to enhance the robustness of the model.
- **Pre-training Objective**: The paper uses a contrastive learning objective to pre-train the model on FILIP300M. The objective is to maximize the similarity between positive image-text pairs and minimize the similarity between negative image-text pairs. The paper uses a temperature-scaled softmax function to calculate the probability of each pair being positive, and uses cross-entropy loss to optimize the model parameters.

The summary is based on section 3[^1^][1] of the paper. You can read the full paper here: https://arxiv.org/pdf/2111.07783v1.pdf


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the visual encoder based on ViT
visual_encoder = ViT(hidden_size, output_dim)

# Define the textual encoder based on BERT
textual_encoder = BERT(hidden_size, output_dim)

# Define the contrastive learning objective
def contrastive_loss(image, text):
  # Encode the image and text using the encoders
  image_tokens = visual_encoder(image)
  text_tokens = textual_encoder(text)
  
  # Compute the token-wise maximum similarity between image and text tokens
  similarity_matrix = cosine_similarity(image_tokens, text_tokens)
  max_image_sim = max(similarity_matrix, dim=1)
  max_text_sim = max(similarity_matrix, dim=0)
  
  # Concatenate the maximum similarities to form a similarity vector
  similarity_vector = concat(max_image_sim, max_text_sim)
  
  # Apply a temperature-scaled softmax function to get the probability of each pair being positive
  probability_vector = softmax(similarity_vector / temperature)
  
  # Compute the cross-entropy loss between the probability vector and the ground truth labels
  loss = cross_entropy(probability_vector, labels)
  
  return loss

# Load the pre-training dataset FILIP300M
dataset = load_dataset("FILIP300M")

# Train the model using contrastive learning on FILIP300M
for epoch in epochs:
  for batch in dataset:
    # Get the image and text pairs from the batch
    image = batch["image"]
    text = batch["text"]
    
    # Compute the contrastive loss for the batch
    loss = contrastive_loss(image, text)
    
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
hidden_size = 768 # The hidden size of the encoders
output_dim = 768 # The output dimension of the encoders
temperature = 0.07 # The temperature parameter for softmax
batch_size = 256 # The batch size for training
epochs = 100 # The number of epochs for training
learning_rate = 1e-4 # The learning rate for gradient descent

# Define the visual encoder based on ViT
visual_encoder = torchvision.models.vision_transformer.vit_base_patch16_224(pretrained=True)
visual_encoder.fc = torch.nn.Linear(visual_encoder.fc.in_features, output_dim)

# Define the textual encoder based on BERT
textual_encoder = transformers.BertModel.from_pretrained("bert-base-uncased")
textual_encoder.pooler = torch.nn.Linear(textual_encoder.pooler.dense.out_features, output_dim)

# Define the contrastive learning objective
def contrastive_loss(image, text):
  # Encode the image and text using the encoders
  image_tokens = visual_encoder(image) # Shape: (batch_size, output_dim)
  text_tokens = textual_encoder(text)[0][:,0,:] # Shape: (batch_size, output_dim)
  
  # Compute the token-wise maximum similarity between image and text tokens
  similarity_matrix = torch.matmul(image_tokens, text_tokens.t()) # Shape: (batch_size, batch_size)
  max_image_sim = torch.max(similarity_matrix, dim=1)[0] # Shape: (batch_size,)
  max_text_sim = torch.max(similarity_matrix, dim=0)[0] # Shape: (batch_size,)
  
  # Concatenate the maximum similarities to form a similarity vector
  similarity_vector = torch.cat((max_image_sim, max_text_sim), dim=0) # Shape: (2 * batch_size,)
  
  # Apply a temperature-scaled softmax function to get the probability of each pair being positive
  probability_vector = torch.nn.functional.softmax(similarity_vector / temperature, dim=0) # Shape: (2 * batch_size,)
  
  # Compute the cross-entropy loss between the probability vector and the ground truth labels
  labels = torch.arange(2 * batch_size) # Shape: (2 * batch_size,)
  loss = torch.nn.functional.cross_entropy(probability_vector.unsqueeze(0), labels.unsqueeze(0)) # Scalar
  
  return loss

# Load the pre-training dataset FILIP300M
dataset = load_dataset("FILIP300M") # A custom function to load the dataset

# Create a data loader for the dataset
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Create an optimizer for gradient descent
optimizer = torch.optim.Adam(list(visual_encoder.parameters()) + list(textual_encoder.parameters()), lr=learning_rate)

# Train the model using contrastive learning on FILIP300M
for epoch in range(epochs):
  for batch in data_loader:
    # Get the image and text pairs from the batch
    image = batch["image"] # Shape: (batch_size, 3, 224, 224)
    text = batch["text"] # Shape: (batch_size, max_length)
    
    # Compute the contrastive loss for the batch
    loss = contrastive_loss(image, text)
    
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Print the loss value every 100 batches
    if batch % 100 == 0:
      print(f"Epoch {epoch}, Batch {batch}, Loss {loss.item()}")
```