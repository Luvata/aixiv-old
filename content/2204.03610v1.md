---
title: 2204.03610v1 Unified Contrastive Learning in Image-Text-Label Space
date: 2022-04-04
---

# [Unified Contrastive Learning in Image-Text-Label Space](http://arxiv.org/abs/2204.03610v1)

authors: Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, Jianfeng Gao


## What, Why and How

[1]: https://arxiv.org/abs/2204.03610 "Unified Contrastive Learning in Image-Text-Label Space"
[2]: https://arxiv.org/pdf/2204.03610.pdf "Uniï¬ed Contrastive Learning in Image-Text-Label Space - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.03610 "[2204.03610] Unified Contrastive Learning in Image-Text-Label Space"

Here is a summary of the paper:

- **What**: The paper introduces a new learning paradigm called Unified Contrastive Learning (UniCL) that combines supervised learning on image-label data and language-image contrastive learning on image-text data into a common image-text-label space.
- **Why**: The paper aims to leverage the advantages of both data types and learning objectives to learn semantically rich yet discriminative representations for visual recognition tasks, such as zero-shot, linear-probe, fully finetuning and transfer learning scenarios.
- **How**: The paper proposes a single learning objective that maximizes the agreement between image-text-label triplets using a contrastive loss function. The paper also proposes a new data augmentation strategy that generates dense labels from image-text pairs using natural language processing techniques. The paper evaluates the proposed method on various benchmarks and datasets, and compares it with existing methods such as CLIP [^1^][1], ALIGN [^2^][2], Cross-Entropy (CE) [^3^][3] and Supervised Contrastive Learning (SupCon) . The paper shows that UniCL outperforms or rivals the existing methods in most cases, especially in zero-shot and linear-probe settings.

[^1^][1]: Radford et al., "Learning Transferable Visual Models From Natural Language Supervision", 2021
[^2^][2]: Jia et al., "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", 2021
[^3^][3]: Deng et al., "ImageNet: A Large-Scale Hierarchical Image Database", 2009
: Khosla et al., "Supervised Contrastive Learning", 2020


## Main Contributions

According to the paper, the main contributions are:

- A new learning paradigm that unifies supervised learning on image-label data and language-image contrastive learning on image-text data into a common image-text-label space.
- A single learning objective that maximizes the agreement between image-text-label triplets using a contrastive loss function.
- A new data augmentation strategy that generates dense labels from image-text pairs using natural language processing techniques.
- Extensive experiments that demonstrate the effectiveness of the proposed method in various visual recognition tasks and scenarios, and show its superiority or competitiveness over existing methods.

## Method Summary

Here is a summary of the method section:

- The paper formulates the unified contrastive learning problem as maximizing the agreement between image-text-label triplets in a common embedding space, where each modality is encoded by a neural network.
- The paper defines the agreement between triplets as the cosine similarity between their embeddings, and uses a contrastive loss function to optimize the learning objective. The contrastive loss function consists of two terms: one for image-text pairs and one for image-label pairs. The paper also introduces a temperature parameter to control the trade-off between the two terms.
- The paper proposes a new data augmentation strategy that generates dense labels from image-text pairs using natural language processing techniques. The paper uses a pretrained language model to extract noun phrases from the text captions, and then filters them based on their frequency and relevance to the image. The paper also uses a label vocabulary to map the extracted phrases to canonical labels. The paper claims that this strategy can enrich the label information and improve the representation learning.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the neural networks for image, text and label encoding
image_encoder = ImageEncoder()
text_encoder = TextEncoder()
label_encoder = LabelEncoder()

# Define the contrastive loss function
def contrastive_loss(image, text, label):
  # Compute the embeddings for image, text and label
  image_emb = image_encoder(image)
  text_emb = text_encoder(text)
  label_emb = label_encoder(label)

  # Compute the cosine similarities between embeddings
  image_text_sim = cosine_similarity(image_emb, text_emb)
  image_label_sim = cosine_similarity(image_emb, label_emb)

  # Compute the contrastive loss terms for image-text and image-label pairs
  image_text_loss = cross_entropy(image_text_sim, positive_mask)
  image_label_loss = cross_entropy(image_label_sim, positive_mask)

  # Combine the two loss terms with a temperature parameter
  loss = (image_text_loss + image_label_loss) / temperature

  return loss

# Define the data augmentation strategy
def generate_dense_labels(image, text):
  # Extract noun phrases from the text caption using a pretrained language model
  noun_phrases = extract_noun_phrases(text)

  # Filter the noun phrases based on their frequency and relevance to the image
  filtered_phrases = filter_noun_phrases(noun_phrases, image)

  # Map the filtered phrases to canonical labels using a label vocabulary
  dense_labels = map_to_labels(filtered_phrases, label_vocab)

  return dense_labels

# Train the model on image-text-label data
for batch in data_loader:
  # Get the image-text-label triplets from the batch
  image, text, label = batch

  # Generate dense labels from image-text pairs using data augmentation
  dense_label = generate_dense_labels(image, text)

  # Compute the contrastive loss for the triplets
  loss = contrastive_loss(image, text, dense_label)

  # Update the model parameters using gradient descent
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import nltk

# Define the hyperparameters
batch_size = 256 # The batch size for training
image_size = 224 # The image size for resizing and cropping
text_length = 77 # The maximum text length for padding and truncating
label_size = 1000 # The label vocabulary size
embed_size = 512 # The embedding size for image, text and label
temperature = 0.07 # The temperature parameter for contrastive loss
learning_rate = 0.0003 # The learning rate for optimizer
num_epochs = 100 # The number of epochs for training

# Define the neural networks for image, text and label encoding
# Use ResNet-50 for image encoder, initialized with pretrained weights on ImageNet
image_encoder = torchvision.models.resnet50(pretrained=True)
# Replace the last fully connected layer with a linear projection to the embedding size
image_encoder.fc = torch.nn.Linear(image_encoder.fc.in_features, embed_size)
# Use BERT-base for text encoder, initialized with pretrained weights on natural language data
text_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
# Replace the pooler layer with a linear projection to the embedding size
text_encoder.pooler = torch.nn.Linear(text_encoder.pooler.dense.out_features, embed_size)
# Use a simple linear layer for label encoder
label_encoder = torch.nn.Linear(label_size, embed_size)

# Define the contrastive loss function
def contrastive_loss(image, text, label):
  # Compute the embeddings for image, text and label
  image_emb = image_encoder(image) # Shape: (batch_size, embed_size)
  text_emb = text_encoder(text)[1] # Shape: (batch_size, embed_size)
  label_emb = label_encoder(label) # Shape: (batch_size, embed_size)

  # Normalize the embeddings to have unit norm
  image_emb = torch.nn.functional.normalize(image_emb, dim=1)
  text_emb = torch.nn.functional.normalize(text_emb, dim=1)
  label_emb = torch.nn.functional.normalize(label_emb, dim=1)

  # Compute the cosine similarities between embeddings
  image_text_sim = torch.matmul(image_emb, text_emb.t()) # Shape: (batch_size, batch_size)
  image_label_sim = torch.matmul(image_emb, label_emb.t()) # Shape: (batch_size, batch_size)

  # Create a positive mask for the diagonal elements (i.e., matching pairs)
  positive_mask = torch.eye(batch_size).bool().to(device)

  # Compute the contrastive loss terms for image-text and image-label pairs
  # Use the softmax cross entropy with logits as the loss function
  image_text_loss = torch.nn.functional.cross_entropy(image_text_sim / temperature, torch.arange(batch_size).to(device))
  image_label_loss = torch.nn.functional.cross_entropy(image_label_sim / temperature, torch.arange(batch_size).to(device))

  # Combine the two loss terms with a temperature parameter
  loss = (image_text_loss + image_label_loss) / temperature

  return loss

# Define the data augmentation strategy
def generate_dense_labels(image, text):
  # Extract noun phrases from the text caption using NLTK library
  noun_phrases = nltk.chunk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))

  # Filter the noun phrases based on their frequency and relevance to the image
  # Use a simple heuristic: keep the phrases that appear more than once in the caption or have a high confidence score from an object detector
  filtered_phrases = []
  for phrase in noun_phrases:
    if isinstance(phrase, nltk.tree.Tree) and phrase.label() == 'NP':
      phrase_text = ' '.join(word for word, tag in phrase.leaves())
      if text.count(phrase_text) > 1 or object_detector(image, phrase_text) > 0.5:
        filtered_phrases.append(phrase_text)

  # Map the filtered phrases to canonical labels using a label vocabulary
  # Use a simple heuristic: find the closest match in the vocabulary based on string similarity or word overlap
  dense_labels = []
  for phrase in filtered_phrases:
    best_match = None
    best_score = -1
    for label in label_vocab:
      score = string_similarity(phrase, label) + word_overlap(phrase, label)
      if score > best_score:
        best_score = score
        best_match = label
    dense_labels.append(best_match)

  # Convert the dense labels to a one-hot vector of size label_size
  dense_label_vec = torch.zeros(label_size)
  for label in dense_labels:
    dense_label_vec[label_vocab.index(label)] = 1

  return dense_label_vec

# Load the image-text-label data
# Use the Conceptual Captions dataset [4] as an example
# The dataset contains 3.3M image-text pairs crawled from the web
# The label vocabulary is the same as ImageNet [10]
data_loader = load_conceptual_captions(batch_size, image_size, text_length, label_size)

# Move the model to the device (CPU or GPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
image_encoder.to(device)
text_encoder.to(device)
label_encoder.to(device)

# Define the optimizer
# Use Adam with weight decay as the optimizer
optimizer = torch.optim.AdamW([
  {'params': image_encoder.parameters()},
  {'params': text_encoder.parameters()},
  {'params': label_encoder.parameters()}
], lr=learning_rate)

# Train the model on image-text-label data
for epoch in range(num_epochs):
  # Set the model to training mode
  image_encoder.train()
  text_encoder.train()
  label_encoder.train()

  # Loop over the batches
  for batch in data_loader:
    # Get the image-text-label triplets from the batch
    image, text, label = batch
    # Move the data to the device
    image = image.to(device)
    text = text.to(device)
    label = label.to(device)

    # Generate dense labels from image-text pairs using data augmentation
    dense_label = generate_dense_labels(image, text)

    # Compute the contrastive loss for the triplets
    loss = contrastive_loss(image, text, dense_label)

    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  # Print the loss for the epoch
  print(f'Epoch {epoch}, Loss: {loss.item()}')
```

: Sharma et al., "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning", 2018