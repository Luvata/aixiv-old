---
title: 2306.15955v2 Bridging the Gap  Neural Collapse Inspired Prompt Tuning for Generalization under Class Imbalance
date: 2023-06-16
---

# [Bridging the Gap: Neural Collapse Inspired Prompt Tuning for Generalization under Class Imbalance](http://arxiv.org/abs/2306.15955v2)

authors: Didi Zhu, Yinchuan Li, Min Zhang, Junkun Yuan, Jiashuo Liu, Zexi Li, Kun Kuang, Chao Wu


## What, Why and How

[1]: https://arxiv.org/pdf/2306.15955v2.pdf "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[2]: https://arxiv.org/abs/2306.15955 "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[3]: https://arxiv.org/abs/2306.15992 "[2306.15992] Autonomous Drone Racing: Time-Optimal Spatial Iterative ..."

Here is a summary of the paper:

```
What: The paper proposes a novel method called Neural Collapse based Prompt Tuning (NPT) for improving the generalization performance of vision-language models under class imbalance conditions.

Why: The paper argues that existing prompt tuning methods suffer from lower generalization capabilities in the presence of class imbalance due to the lack of constraints on feature structures.

How: The paper introduces two regularization terms, geometric de-biasing and multi-modal isomorphism, to optimize prompts so that both text and image features satisfy the same simplex ETF structure. The paper evaluates NPT on 11 image recognition datasets and shows that it outperforms existing prompt learning techniques.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2306.15955v2.pdf "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[2]: https://arxiv.org/abs/2306.15955 "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[3]: https://arxiv.org/abs/2306.15992 "[2306.15992] Autonomous Drone Racing: Time-Optimal Spatial Iterative ..."

Here are some of the contributions of the paper:

- The paper extends the Neural Collapse phenomenon to vision-language models and reveals the geometric reasons behind the impact of class imbalance on their generalization ability.
- The paper proposes a novel method called Neural Collapse based Prompt Tuning (NPT) that optimizes prompts so that both text and image features satisfy the same simplex ETF structure.
- The paper introduces two regularization terms, geometric de-biasing and multi-modal isomorphism, to enhance the robustness of vision-language models under class imbalance conditions while maintaining their generalization capabilities.
- The paper evaluates NPT on 11 diverse image recognition datasets and shows that it outperforms existing prompt learning techniques across different metrics.

## Method Summary

[1]: https://arxiv.org/pdf/2306.15955v2.pdf "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[2]: https://arxiv.org/abs/2306.15955 "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[3]: https://arxiv.org/abs/2306.15992 "[2306.15992] Autonomous Drone Racing: Time-Optimal Spatial Iterative ..."

Here is a summary of the method section of the paper:

```
The method section consists of four subsections: Preliminaries, Neural Collapse based Prompt Tuning, Geometric De-biasing, and Multi-modal Isomorphism.

- Preliminaries: This subsection introduces the background and notation of vision-language models, prompt tuning, and neural collapse phenomenon.
- Neural Collapse based Prompt Tuning: This subsection presents the main idea of NPT, which is to optimize prompts so that both text and image features satisfy the same simplex ETF structure. It also defines the simplex ETF structure and shows how to measure the distance between two simplex ETF structures.
- Geometric De-biasing: This subsection proposes a regularization term that minimizes the distance between the text and image simplex ETF structures. It also explains how to compute the gradients of this term efficiently.
- Multi-modal Isomorphism: This subsection proposes another regularization term that encourages the text and image features to have similar distributions within each class. It also explains how to compute the gradients of this term using Sinkhorn-Knopp algorithm.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2306.15955v2.pdf "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[2]: https://arxiv.org/abs/2306.15955 "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[3]: https://arxiv.org/abs/2306.15992 "[2306.15992] Autonomous Drone Racing: Time-Optimal Spatial Iterative ..."

Here is a high-level pseudo code for the paper:

```
# Input: a vision-language model V, a dataset D, a prompt template P, a learning rate lr, a number of epochs T
# Output: an optimized prompt P*
# Initialize P with random context words
# For each epoch t = 1, ..., T
  # Shuffle D
  # For each batch B in D
    # Compute text features F_t and image features F_i using V and P
    # Compute simplex ETF structures S_t and S_i using F_t and F_i
    # Compute distance d between S_t and S_i
    # Compute Sinkhorn distance s between F_t and F_i within each class
    # Compute loss L as the sum of cross-entropy loss, d, and s
    # Update P using gradient descent with lr and L
# Return P as P*
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2306.15955v2.pdf "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[2]: https://arxiv.org/abs/2306.15955 "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for ..."
[3]: https://arxiv.org/abs/2306.15992 "[2306.15992] Autonomous Drone Racing: Time-Optimal Spatial Iterative ..."

Here is a detailed pseudo code to implement the paper:

```
# Input: a vision-language model V, a dataset D, a prompt template P, a learning rate lr, a number of epochs T, a number of classes C, a regularization weight lambda
# Output: an optimized prompt P*
# Initialize P with random context words
# For each epoch t = 1, ..., T
  # Shuffle D
  # For each batch B in D
    # Compute text features F_t and image features F_i using V and P
    # Compute simplex ETF structures S_t and S_i using F_t and F_i
      # For each class c = 1, ..., C
        # Compute the mean vector m_t_c and m_i_c of F_t and F_i for class c
        # Compute the covariance matrix C_t_c and C_i_c of F_t and F_i for class c
        # Compute the eigenvalues and eigenvectors of C_t_c and C_i_c
        # Sort the eigenvalues in descending order and select the top C-1 eigenvectors
        # Normalize the eigenvectors to form an orthonormal basis B_t_c and B_i_c
      # Concatenate the mean vectors and bases for all classes to form S_t and S_i
    # Compute distance d between S_t and S_i using Frobenius norm
    # Compute Sinkhorn distance s between F_t and F_i within each class using Sinkhorn-Knopp algorithm
      # Initialize a matrix M of size C x C with zeros
      # For each class c = 1, ..., C
        # Compute the pairwise Euclidean distance matrix D_c between F_t and F_i for class c
        # Normalize D_c by subtracting the row minima and column minima
        # Initialize a matrix K_c of size C x C with exp(-D_c)
        # Initialize two vectors u_c and v_c of size C with ones
        # Repeat until convergence
          # Update u_c by dividing each element by the dot product of K_c and v_c
          # Update v_c by dividing each element by the dot product of K_c.T and u_c
        # Compute the optimal transport matrix P_c by multiplying u_c, K_c, and v_c element-wise
        # Add P_c to M at the corresponding block for class c
      # Compute s as the dot product of M and D
    # Compute loss L as the sum of cross-entropy loss, lambda * d, and lambda * s
    # Update P using gradient descent with lr and L
# Return P as P*
```