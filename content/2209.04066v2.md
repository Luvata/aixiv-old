---
title: 2209.04066v2 TEACH  Temporal Action Composition for 3D Humans
date: 2022-09-05
---

# [TEACH: Temporal Action Composition for 3D Humans](http://arxiv.org/abs/2209.04066v2)

authors: Nikos Athanasiou, Mathis Petrovich, Michael J. Black, GÃ¼l Varol


## What, Why and How

[1]: https://arxiv.org/abs/2209.04066 "[2209.04066] TEACH: Temporal Action Composition for 3D Humans - arXiv.org"
[2]: https://arxiv.org/pdf/2209.04066v2.pdf "TEACH: Temporal Action Composition for 3D Humans - arXiv.org"
[3]: http://export.arxiv.org/abs/2209.04066 "[2209.04066] TEACH: Temporal Action Composition for 3D Humans"

Here is a summary of the paper:

- **What**: The paper presents a new method for generating 3D human motions from natural language descriptions, called TEACH (TEmporal Action Composition for 3D Humans).
- **Why**: The paper aims to enable the synthesis of a series of actions, which is a challenging task that requires handling diverse and complex motions, temporal order, and smooth transitions. The paper also introduces and establishes a new benchmark for temporal action composition on the BABEL dataset, which has rich and varied motion-text annotations.
- **How**: The paper proposes a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation allows the model to generate realistic and diverse motions for each action, while also conditioning on the previous motion to ensure continuity. The paper also applies spherical linear interpolation (Slerp) to smooth out any remaining discontinuities between actions. The paper evaluates the model on the BABEL dataset and compares it with multiple baselines, showing that it produces high-quality motions for a wide variety of actions and temporal compositions from language descriptions.

## Main Contributions

According to the paper, the contributions are:

- Introducing and establishing a new benchmark for temporal action composition of 3D motions on the BABEL dataset
- Designing a new hybrid neural network model, TEACH, that addresses the limitations of previous state of the art by iteratively generating infinitely many actions with smooth transitions
- Obtaining promising results for text-to-motion synthesis from a large-vocabulary of actions


## Method Summary

[1]: https://arxiv.org/pdf/2209.04066v2 "TEACH: Temporal Action Composition for 3D Humans - arXiv.org"
[2]: https://arxiv.org/abs/2209.04066 "[2209.04066] TEACH: Temporal Action Composition for 3D Humans - arXiv.org"
[3]: http://export.arxiv.org/abs/2209.04066 "[2209.04066] TEACH: Temporal Action Composition for 3D Humans"

Here is a summary of the method section of the paper:

- The paper proposes a **Transformer-based approach** that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation allows the model to generate realistic and diverse motions for each action, while also conditioning on the previous motion to ensure continuity.
- The paper uses the **BABEL dataset** as the main source of training data, which has rich and varied motion-text annotations for the AMASS motion capture collection. The paper also uses a pretrained language model (BERT) to encode the text inputs into embeddings.
- The paper designs a **non-autoregressive Transformer-based VAE** to generate one motion per action at a time. The model consists of an encoder that maps the input text and motion into a latent space, and a decoder that samples from the latent space and outputs a motion sequence. The model also uses a discriminator to enforce realism and diversity of the generated motions.
- The paper introduces a **temporal action composition module** that iteratively generates a sequence of actions given a stream of textual prompts. The module encodes the next action conditioned on the last few frames of the previous action, and feeds it to the VAE model to generate the next motion. The module also applies spherical linear interpolation (Slerp) to smooth out any remaining discontinuities between actions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the VAE model
class VAE(nn.Module):
  def __init__(self):
    # Initialize the encoder, decoder and discriminator networks
    self.encoder = Encoder()
    self.decoder = Decoder()
    self.discriminator = Discriminator()

  def forward(self, text, motion):
    # Encode the text and motion into a latent space
    z_mean, z_logvar = self.encoder(text, motion)
    # Sample from the latent space using the reparameterization trick
    z = self.reparameterize(z_mean, z_logvar)
    # Decode the latent vector into a motion sequence
    motion_hat = self.decoder(text, z)
    # Compute the reconstruction loss and the KL divergence
    recon_loss = mse_loss(motion_hat, motion)
    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())
    # Compute the adversarial loss using the discriminator
    adv_loss = bce_loss(self.discriminator(motion_hat), 1) + bce_loss(self.discriminator(motion), 0)
    # Return the outputs and losses
    return motion_hat, recon_loss, kl_loss, adv_loss

# Define the temporal action composition module
class TAC(nn.Module):
  def __init__(self):
    # Initialize the pretrained language model and the VAE model
    self.lm = BERT()
    self.vae = VAE()

  def forward(self, text_stream):
    # Initialize an empty list to store the generated motions
    motion_stream = []
    # Loop over the text stream
    for i in range(len(text_stream)):
      # Encode the current text into an embedding using the language model
      text_emb = self.lm(text_stream[i])
      # If this is the first action, generate a motion from scratch using the VAE model
      if i == 0:
        motion = self.vae(text_emb, None)
      # Else, encode the last few frames of the previous motion and concatenate with the text embedding
      else:
        prev_motion_emb = self.lm(motion_stream[i-1][-k:])
        input_emb = torch.cat([text_emb, prev_motion_emb], dim=-1)
        # Generate a motion conditioned on the previous motion using the VAE model
        motion = self.vae(input_emb, None)
      # Apply Slerp to smooth out any discontinuities between motions
      if i > 0:
        motion = slerp(motion_stream[i-1][-k:], motion[:k], alpha) + motion[k:]
      # Append the motion to the motion stream
      motion_stream.append(motion)
    # Return the motion stream
    return motion_stream
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import transformers
import numpy as np

# Define some hyperparameters
batch_size = 32 # The number of text-motion pairs in a batch
seq_len = 120 # The length of a motion sequence in frames
emb_dim = 768 # The dimension of the text and motion embeddings
latent_dim = 256 # The dimension of the latent space
hidden_dim = 512 # The dimension of the hidden layers in the networks
num_heads = 8 # The number of attention heads in the Transformer layers
num_layers = 6 # The number of Transformer layers in the encoder and decoder networks
dropout = 0.1 # The dropout rate for regularization
lr = 0.0001 # The learning rate for optimization
beta1 = 0.9 # The beta1 parameter for Adam optimizer
beta2 = 0.999 # The beta2 parameter for Adam optimizer
weight_decay = 0.01 # The weight decay parameter for regularization
num_epochs = 100 # The number of epochs for training
k = 10 # The number of frames to encode from the previous motion
alpha = 0.5 # The interpolation factor for Slerp

# Define the Encoder network
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Initialize a Transformer encoder layer
    self.transformer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)
    # Initialize a linear layer to map the output to the latent space
    self.linear = nn.Linear(emb_dim, latent_dim * 2)

  def forward(self, text, motion):
    # Concatenate the text and motion embeddings along the sequence dimension
    input = torch.cat([text, motion], dim=1)
    # Apply the Transformer encoder layer to the input
    output = self.transformer(input)
    # Apply the linear layer to the output and split into mean and log variance vectors
    z_mean, z_logvar = torch.chunk(self.linear(output), 2, dim=-1)
    # Return the mean and log variance vectors
    return z_mean, z_logvar

# Define the Decoder network
class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    # Initialize a linear layer to map the latent vector to the embedding dimension
    self.linear = nn.Linear(latent_dim, emb_dim)
    # Initialize a Transformer decoder layer
    self.transformer = nn.TransformerDecoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)
    # Initialize a linear layer to map the output to the motion dimension (3 joints * 3 coordinates)
    self.output_layer = nn.Linear(emb_dim, 9)

  def forward(self, text, z):
    # Apply the linear layer to the latent vector and repeat it along the sequence dimension
    z = self.linear(z).repeat(1, seq_len, 1)
    # Concatenate the text and latent embeddings along the sequence dimension
    input = torch.cat([text, z], dim=1)
    # Apply the Transformer decoder layer to the input
    output = self.transformer(input)
    # Apply the output layer to the output and reshape it into a motion sequence
    motion_hat = self.output_layer(output).view(-1, seq_len, 3, 3)
    # Return the generated motion sequence
    return motion_hat

# Define the Discriminator network
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Initialize a convolutional layer to reduce the motion dimension (3 joints * 3 coordinates) to a single channel
    self.conv1d = nn.Conv1d(in_channels=9, out_channels=1, kernel_size=1)
    # Initialize a Transformer encoder layer
    self.transformer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)
    # Initialize a linear layer to map the output to a single value (real or fake)
    self.output_layer = nn.Linear(emb_dim, 1)

  def forward(self, motion):
    # Reshape the motion sequence into a feature map (batch_size * channels * seq_len)
    motion = motion.view(-1, 9, seq_len)
    # Apply the convolutional layer to the motion
    motion = self.conv1d(motion)
    # Apply the Transformer encoder layer to the motion
    output = self.transformer(motion)
    # Apply the output layer to the output and squeeze the last dimension
    output = self.output_layer(output).squeeze(-1)
    # Return the output
    return output

# Define the VAE model
class VAE(nn.Module):
  def __init__(self):
    super(VAE, self).__init__()
    # Initialize the encoder, decoder and discriminator networks
    self.encoder = Encoder()
    self.decoder = Decoder()
    self.discriminator = Discriminator()

  def reparameterize(self, z_mean, z_logvar):
    # Compute the standard deviation from the log variance
    z_std = torch.exp(0.5 * z_logvar)
    # Sample a random vector from a standard normal distribution
    epsilon = torch.randn_like(z_std)
    # Compute the latent vector by adding the product of the standard deviation and the random vector to the mean
    z = z_mean + z_std * epsilon
    # Return the latent vector
    return z

  def forward(self, text, motion):
    # Encode the text and motion into a latent space
    z_mean, z_logvar = self.encoder(text, motion)
    # Sample from the latent space using the reparameterization trick
    z = self.reparameterize(z_mean, z_logvar)
    # Decode the latent vector into a motion sequence
    motion_hat = self.decoder(text, z)
    # Compute the reconstruction loss and the KL divergence
    recon_loss = F.mse_loss(motion_hat, motion, reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())
    # Compute the adversarial loss using the discriminator
    adv_loss = F.binary_cross_entropy_with_logits(self.discriminator(motion_hat), torch.ones(batch_size)) + F.binary_cross_entropy_with_logits(self.discriminator(motion), torch.zeros(batch_size))
    # Return the outputs and losses
    return motion_hat, recon_loss, kl_loss, adv_loss

# Define the temporal action composition module
class TAC(nn.Module):
  def __init__(self):
    super(TAC, self).__init__()
    # Initialize the pretrained language model and the VAE model
    self.lm = transformers.BertModel.from_pretrained('bert-base-uncased')
    self.vae = VAE()

  def forward(self, text_stream):
    # Initialize an empty list to store the generated motions
    motion_stream = []
    # Loop over the text stream
    for i in range(len(text_stream)):
      # Encode the current text into an embedding using the language model
      text_emb = self.lm(text_stream[i])[0]
      # If this is the first action, generate a motion from scratch using the VAE model
      if i == 0:
        motion = self.vae(text_emb, None)[0]
      # Else, encode the last few frames of the previous motion and concatenate with the text embedding
      else:
        prev_motion_emb = self.lm(motion_stream[i-1][-k:])[0]
        input_emb = torch.cat([text_emb, prev_motion_emb], dim=1)
        # Generate a motion conditioned on the previous motion using the VAE model
        motion = self.vae(input_emb, None)[0]
      # Apply Slerp to smooth out any discontinuities between motions
      if i > 0:
        motion = slerp(motion_stream[i-1][-k:], motion[:k], alpha) + motion[k:]
      # Append the motion to the motion stream
      motion_stream.append(motion)
    # Return the motion stream
    return motion_stream

# Define a function to perform spherical linear interpolation (Slerp) between two quaternions
def slerp(q1, q2, alpha):
  # Compute the dot product between q1 and q2
  dot = torch.sum(q1 * q2, dim=-1)
  # Clamp the dot product to [-1, 1] to avoid numerical errors
  dot = torch.clamp(dot, -1.0, 1.0)
  # Compute the angle between q1 and q2
  theta = torch.acos(dot) * alpha
  # Compute an orthogonal vector to q1 and q2 using cross product
  q3 = q2 - q1 * dot
  q3 = F.normalize(q3, dim=-1)
  # Compute the interpolated quaternion using trigonometric identities
  q = q1 * torch.cos(theta) + q3 * torch.sin(theta)
  # Return the interpolated quaternion
  return q

# Load the BABEL dataset and split into train and test sets (the details are omitted for