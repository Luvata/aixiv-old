---
title: 2211.12446v2 EDICT  Exact Diffusion Inversion via Coupled Transformations
date: 2022-11-13
---

# [EDICT: Exact Diffusion Inversion via Coupled Transformations](http://arxiv.org/abs/2211.12446v2)

authors: Bram Wallace, Akash Gokul, Nikhil Naik


## What, Why and How

[1]: https://arxiv.org/pdf/2211.12446v2.pdf "EDICT: Exact Diffusion Inversion via Coupled Transformations - arXiv.org"
[2]: https://arxiv.org/abs/2211.12446 "EDICT: Exact Diffusion Inversion via Coupled Transformations"
[3]: http://export.arxiv.org/abs/2011.12446v2 "[2011.12446v2] Strategies for the Detection of ET Probes Within Our Own ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes a new method for inverting denoising diffusion models (DDMs), which are generative models that can create realistic images from noise vectors. The method is called EDICT (Exact Diffusion Inversion via Coupled Transformations) and it can reconstruct real images with high fidelity and enable various image editing tasks.
- **Why**: The paper aims to solve the problem of image editing with DDMs, which requires finding the noise vector that corresponds to a given input image. The existing method for this, called DDIM (Denoising Diffusion Implicit Models), is unstable and prone to errors, leading to poor image reconstruction and content loss. EDICT overcomes these limitations by using a novel inversion technique inspired by affine coupling layers.
- **How**: EDICT maintains two coupled noise vectors that are used to invert each other in an alternating fashion. This way, EDICT can achieve mathematically exact inversion of real and model-generated images without relying on local linearization assumptions. EDICT can be combined with any pretrained DDM and does not require any model training, finetuning, prompt tuning, or extra data. EDICT demonstrates superior performance over DDIM on complex image datasets like MS-COCO and enables a wide range of image edits, such as semantic changes, stylization, and inpainting.

## Main Contributions

The paper claims to make the following contributions:

- It introduces EDICT, a novel method for inverting DDMs that enables exact image reconstruction and editing.
- It shows that EDICT outperforms DDIM on image reconstruction quality and stability across different datasets and models.
- It demonstrates that EDICT can perform various image editing tasks, such as semantic changes, stylization, and inpainting, using noise vectors inverted from real images.
- It provides code and pretrained models for EDICT at https://github.com/salesforce/EDICT.

## Method Summary

[1]: https://arxiv.org/pdf/2211.12446v2.pdf "EDICT: Exact Diffusion Inversion via Coupled Transformations - arXiv.org"
[2]: https://arxiv.org/abs/2211.12446 "EDICT: Exact Diffusion Inversion via Coupled Transformations"
[3]: http://export.arxiv.org/abs/2011.12446v2 "[2011.12446v2] Strategies for the Detection of ET Probes Within Our Own ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper first reviews the background of DDMs and DDIMs, and explains the challenges and limitations of DDIM inversion for real images.
- The paper then introduces EDICT, a new inversion method that uses two coupled noise vectors (z1 and z2) that are initialized randomly and updated alternately using a forward and a backward model.
- The paper describes the forward model, which takes z1 as input and outputs a noisy image x and a noise vector z2. The forward model consists of a diffusion model (such as Stable Diffusion) and an affine coupling layer that splits and mixes z1 into x and z2.
- The paper also describes the backward model, which takes z2 as input and outputs a noise vector z1. The backward model consists of an inverse affine coupling layer that splits and mixes z2 into x and z1, and an inverse diffusion model that denoises x to obtain z1.
- The paper shows that EDICT can invert both real and model-generated images exactly by applying the forward and backward models iteratively until convergence. The paper also shows that EDICT can perform image editing by modifying the inverted noise vectors using text prompts or other guidance signals.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the forward and backward models
forward_model = DiffusionModel + AffineCouplingLayer
backward_model = InverseAffineCouplingLayer + InverseDiffusionModel

# Define the inversion function
def invert(image):
  # Initialize two noise vectors randomly
  z1 = sample_noise()
  z2 = sample_noise()
  # Loop until convergence
  while not converged:
    # Apply the forward model to z1 and get x and z2
    x, z2 = forward_model(z1)
    # Compute the loss between x and image
    loss = mse(x, image)
    # Update z1 by gradient descent
    z1 = z1 - lr * grad(loss, z1)
    # Apply the backward model to z2 and get z1
    z1 = backward_model(z2)
    # Compute the loss between x and image
    loss = mse(x, image)
    # Update z2 by gradient descent
    z2 = z2 - lr * grad(loss, z2)
  # Return the inverted noise vectors
  return z1, z2

# Define the editing function
def edit(z1, z2, prompt):
  # Modify z1 or z2 using the prompt or other guidance signals
  # For example, use CLIP to align z1 or z2 with the prompt
  # Or use other methods such as inpainting or stylization
  z1 = modify(z1, prompt)
  # Apply the forward model to get the edited image
  x, _ = forward_model(z1)
  # Return the edited image
  return x
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import clip

# Define the hyperparameters
num_steps = 1000 # Number of inversion steps
lr = 1e-3 # Learning rate
beta1 = 0.9 # Adam beta1
beta2 = 0.999 # Adam beta2
eps = 1e-8 # Adam epsilon
T = 1000 # Number of diffusion steps
sigma_min = 0.01 # Minimum noise level
sigma_max = 0.99 # Maximum noise level

# Define the diffusion model
# Assume it is a pretrained Stable Diffusion model from https://github.com/salesforce/stablediffusion
diffusion_model = StableDiffusionModel()

# Define the affine coupling layer
class AffineCouplingLayer(nn.Module):
  def __init__(self, in_channels):
    super().__init__()
    # Define the convolutional network that outputs scale and shift parameters
    self.net = nn.Sequential(
      nn.Conv2d(in_channels // 2, in_channels, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(in_channels, in_channels, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(in_channels, in_channels, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(in_channels, in_channels, 3, padding=1)
    )
  
  def forward(self, x):
    # Split x into two halves along the channel dimension
    x1, x2 = torch.chunk(x, 2, dim=1)
    # Apply the network to x1 and get scale and shift parameters
    s, t = torch.chunk(self.net(x1), 2, dim=1)
    # Apply the affine transformation to x2
    y2 = x2 * torch.exp(s) + t
    # Concatenate x1 and y2 along the channel dimension
    y = torch.cat([x1, y2], dim=1)
    # Return y and the log determinant of the Jacobian
    log_det = torch.sum(s, dim=[1, 2, 3])
    return y, log_det
  
  def inverse(self, y):
    # Split y into two halves along the channel dimension
    y1, y2 = torch.chunk(y, 2, dim=1)
    # Apply the network to y1 and get scale and shift parameters
    s, t = torch.chunk(self.net(y1), 2, dim=1)
    # Apply the inverse affine transformation to y2
    x2 = (y2 - t) * torch.exp(-s)
    # Concatenate y1 and x2 along the channel dimension
    x = torch.cat([y1, x2], dim=1)
    # Return x and the log determinant of the Jacobian
    log_det = -torch.sum(s, dim=[1, 2, 3])
    return x, log_det

# Define the forward model
class ForwardModel(nn.Module):
  def __init__(self):
    super().__init__()
    # Define the affine coupling layer with the same number of channels as the diffusion model
    self.coupling_layer = AffineCouplingLayer(diffusion_model.num_channels)
  
  def forward(self, z1):
    # Get the noise level for the last diffusion step
    sigma_T = diffusion_model.get_sigma(T)
    # Apply the diffusion model to z1 and get x_T and q(x_T | z_1)
    x_T, q_x_T_z_1 = diffusion_model(z1)
    # Apply the coupling layer to z_1 and get z_2 and log_det_12
    z_2, log_det_12 = self.coupling_layer(z_1)
    # Compute p(z_2) as a standard normal distribution
    p_z_2 = torch.distributions.Normal(0.0, 1.0).log_prob(z_2).sum(dim=[1, 2 ,3])
    # Compute p(x_T | z_2) as a Gaussian distribution with mean z_2 and variance sigma_T^2 * I 
    p_x_T_z_2 = torch.distributions.Normal(z_2 * sigma_T + (1 - sigma_T) * x_T.mean(), sigma_T).log_prob(x_T).sum(dim=[1 ,2 ,3])
    # Compute p(z_1 | z_2) using the change of variable formula: p(z_1 | z_2) = p(z_2) / |det(dz_2 / dz_1)|
    p_z_1_z_2 = p_z_2 - log_det_12
    # Return x_T, z_2, and the log probabilities
    return x_T, z_2, q_x_T_z_1, p_x_T_z_2, p_z_1_z_2

# Define the inverse coupling layer
class InverseCouplingLayer(nn.Module):
  def __init__(self, in_channels):
    super().__init__()
    # Define the convolutional network that outputs scale and shift parameters
    self.net = nn.Sequential(
      nn.Conv2d(in_channels // 2, in_channels, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(in_channels, in_channels, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(in_channels, in_channels, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(in_channels, in_channels, 3, padding=1)
    )
  
  def forward(self, z2):
    # Split z2 into two halves along the channel dimension
    z1, z2 = torch.chunk(z2, 2, dim=1)
    # Apply the network to z1 and get scale and shift parameters
    s, t = torch.chunk(self.net(z1), 2, dim=1)
    # Apply the inverse affine transformation to z2
    x2 = (z2 - t) * torch.exp(-s)
    # Concatenate z1 and x2 along the channel dimension
    x = torch.cat([z1, x2], dim=1)
    # Return x and the log determinant of the Jacobian
    log_det = -torch.sum(s, dim=[1, 2, 3])
    return x, log_det
  
  def inverse(self, x):
    # Split x into two halves along the channel dimension
    x1, x2 = torch.chunk(x, 2, dim=1)
    # Apply the network to x1 and get scale and shift parameters
    s, t = torch.chunk(self.net(x1), 2, dim=1)
    # Apply the affine transformation to x2
    y2 = x2 * torch.exp(s) + t
    # Concatenate x1 and y2 along the channel dimension
    y = torch.cat([x1, y2], dim=1)
    # Return y and the log determinant of the Jacobian
    log_det = torch.sum(s, dim=[1 , 2 ,3])
    return y , log_det

# Define the inverse diffusion model
class InverseDiffusionModel(nn.Module):
  def __init__(self):
    super().__init__()
  
  def forward(self ,x_T):
    # Get the noise level for the last diffusion step
    sigma_T = diffusion_model.get_sigma(T)
    # Compute z_1 as (x_T - mean(x_T)) / sigma_T + mean(x_T)
    z_1 = (x_T - x_T.mean()) / sigma_T + x_T.mean()
    # Compute p(x_T | z_1) as a Gaussian distribution with mean z_1 and variance sigma_T^2 * I 
    p_x_T_z_1 = torch.distributions.Normal(z_1 * sigma_T + (1 - sigma_T) * x_T.mean(), sigma_T).log_prob(x_T).sum(dim=[1 , 2 ,3])
    # Return z_1 and p(x_T | z_1)
    return z_1 , p_x_T_z_1

# Define the backward model
class BackwardModel(nn.Module):
  def __init__(self):
    super().__init__()
    # Define the inverse coupling layer with the same number of channels as the diffusion model
    self.inverse_coupling_layer = InverseCouplingLayer(diffusion_model.num_channels)
  
  def forward(self ,z_2):
     # Apply the inverse coupling layer to z_2 and get z_1 and log_det_21
     z_1 , log_det_21 = self.inverse_coupling_layer(z_2)
     # Compute p(z_2) as a standard normal distribution
     p_z_2 = torch.distributions.Normal(0.0 , 1.0).log_prob(z_2).sum(dim=[1 , 2 ,3])
     # Compute p(z_1 | z_2) using the change of variable formula: p(z_1 | z_2) = p(z_2) / |det(dz_2 / dz_1)|
     p_z_1_z_2 = p_z_2 - log_det_21
     # Return z_1 and p(z_1 | z_2)
     return z_1 , p_z