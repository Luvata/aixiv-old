---
title: 2105.05233v4 Diffusion Models Beat GANs on Image Synthesis
date: 2021-05-06
---

# [Diffusion Models Beat GANs on Image Synthesis](http://arxiv.org/abs/2105.05233v4)

authors: Prafulla Dhariwal, Alex Nichol


## What, Why and How

[1]: https://arxiv.org/abs/2105.05233 "[2105.05233] Diffusion Models Beat GANs on Image Synthesis - arXiv.org"
[2]: http://export.arxiv.org/abs/2105.05233v4 "[2105.05233v4] Diffusion Models Beat GANs on Image Synthesis"
[3]: https://arxiv.org/pdf/2105.05233v4.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper presents a new method for generating realistic images using diffusion models, which are a type of generative model that reverses the process of adding noise to an image until it becomes unrecognizable.
- **Why**: The paper aims to show that diffusion models can outperform generative adversarial networks (GANs), which are the current state-of-the-art generative models, in terms of image quality, diversity, and efficiency.
- **How**: The paper proposes two main innovations: (1) finding a better architecture for diffusion models through a series of experiments and ablations, and (2) improving the fidelity of conditional image synthesis using classifier guidance, which is a technique that uses gradients from a classifier to adjust the noise level during sampling. The paper evaluates the proposed method on various image datasets and metrics, and demonstrates that it achieves superior or comparable results to GANs.

## Main Contributions

The paper claims to make the following contributions:

- It shows that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models on unconditional image synthesis.
- It introduces classifier guidance, a simple and compute-efficient method for trading off diversity for fidelity in conditional image synthesis using diffusion models.
- It combines classifier guidance with upsampling diffusion models, which further improves the sample quality on high-resolution image datasets.
- It provides extensive ablation studies and analysis of the architecture and hyperparameters of diffusion models, and reveals some of their advantages and limitations.

## Method Summary

The method section of the paper consists of three subsections:

- **Diffusion models**: This subsection reviews the basics of diffusion models, which are generative models that learn to invert a Markov chain of adding noise to an image. The authors explain how to train and sample from diffusion models using denoising score matching and denoising diffusion implicit models (DDIMs).
- **Architecture**: This subsection describes the architecture of the diffusion model used in the paper, which is based on a U-Net with skip connections and attention layers. The authors also discuss some of the design choices and trade-offs involved in choosing the number of layers, channels, and attention heads.
- **Classifier guidance**: This subsection introduces classifier guidance, a technique that uses gradients from a pretrained classifier to adjust the noise level during sampling from a conditional diffusion model. The authors explain how classifier guidance works, how to implement it efficiently, and how to balance the trade-off between diversity and fidelity.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the diffusion model architecture
def diffusion_model(x, y, t):
  # x: image tensor
  # y: class label tensor
  # t: noise level tensor
  # Returns: score tensor
  # Use a U-Net with skip connections and attention layers
  # Concatenate x, y, and t as inputs
  # Apply convolutional layers with residual blocks and normalization
  # Apply attention layers at different scales
  # Apply deconvolutional layers with skip connections and normalization
  # Return the final output layer

# Train the diffusion model using denoising score matching
def train_diffusion_model(data):
  # data: image dataset
  # Returns: trained diffusion model
  # Initialize the diffusion model parameters randomly
  # For each batch of images x and labels y from data:
    # Sample a random noise level t from a predefined schedule
    # Add Gaussian noise to x according to t to get x_tilde
    # Compute the score s = diffusion_model(x_tilde, y, t)
    # Compute the loss L = mean(squared_error(s, -grad_log_normal(x, x_tilde, t)))
    # Update the diffusion model parameters using gradient descent on L
  # Return the trained diffusion model

# Sample from the diffusion model using DDIMs
def sample_diffusion_model(model, y, n):
  # model: trained diffusion model
  # y: class label tensor
  # n: number of samples to generate
  # Returns: generated image tensor
  # Initialize x_0 as Gaussian noise with shape (n, image_size, image_size, channels)
  # For each noise level t from high to low in reverse order:
    # Compute the score s = model(x_0, y, t)
    # Compute the mean m = x_0 - s * variance(t)
    # Sample x_1 from normal(m, variance(t))
    # Set x_0 = x_1
  # Return x_0 as the generated image

# Sample from the conditional diffusion model using classifier guidance
def sample_classifier_guided_diffusion_model(model, classifier, y, n):
  # model: trained diffusion model
  # classifier: pretrained classifier
  # y: class label tensor
  # n: number of samples to generate
  # Returns: generated image tensor
  # Initialize x_0 as Gaussian noise with shape (n, image_size, image_size, channels)
  # For each noise level t from high to low in reverse order:
    # Compute the score s = model(x_0, y, t)
    # Compute the mean m = x_0 - s * variance(t)
    # Compute the classifier gradient g = grad_log_prob(classifier(x_0), y)
    # Adjust the mean m = m + alpha * g * variance(t) where alpha is a hyperparameter
    # Sample x_1 from normal(m, variance(t))
    # Set x_0 = x_1
  # Return x_0 as the generated image

```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torch.nn as nn # for neural network modules
import torch.nn.functional as F # for activation functions
import torch.optim as optim # for optimization algorithms
import torchvision # for image datasets and transformations
import torchvision.models as models # for pretrained classifiers

# Define some hyperparameters
image_size = 256 # size of the image
num_classes = 1000 # number of classes in ImageNet
num_channels = 3 # number of channels in the image
num_layers = 12 # number of layers in the U-Net
num_channels_per_layer = 256 # number of channels per layer in the U-Net
num_attention_heads = 4 # number of attention heads per layer in the U-Net
num_timesteps = 1000 # number of noise levels in the diffusion process
beta = 0.0001 # noise level increment per timestep
alpha = 0.01 # classifier guidance coefficient
batch_size = 64 # batch size for training and sampling
num_epochs = 100 # number of epochs for training
learning_rate = 0.001 # learning rate for training

# Define a helper function to compute the variance of the Gaussian noise at a given timestep
def variance(t):
  # t: timestep tensor
  # Returns: variance tensor
  return (1 - torch.exp(-beta * t)) / (1 - torch.exp(-beta))

# Define a helper function to compute the negative log probability of a Gaussian distribution
def neg_log_normal(x, mean, var):
  # x: input tensor
  # mean: mean tensor
  # var: variance tensor
  # Returns: negative log probability tensor
  return 0.5 * torch.log(2 * math.pi * var) + (x - mean) ** 2 / (2 * var)

# Define a helper function to compute the gradient of the negative log probability of a Gaussian distribution
def grad_neg_log_normal(x, mean, var):
  # x: input tensor
  # mean: mean tensor
  # var: variance tensor
  # Returns: gradient tensor
  return (x - mean) / var

# Define a helper function to compute the gradient of the log probability of a classifier output given a label
def grad_log_prob(output, label):
  # output: output tensor from the classifier
  # label: label tensor
  # Returns: gradient tensor
  return F.one_hot(label, num_classes=num_classes) / output

# Define a convolutional block with residual connection and normalization
class ConvBlock(nn.Module):
  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
    super(ConvBlock, self).__init__()
    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
    self.norm1 = nn.BatchNorm2d(out_channels)
    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)
    self.norm2 = nn.BatchNorm2d(out_channels)
    self.relu = nn.ReLU()
    self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()

  def forward(self, x):
    y = self.relu(self.norm1(self.conv1(x)))
    y = self.norm2(self.conv2(y))
    y = y + self.residual(x)
    y = self.relu(y)
    return y

# Define an attention block with multi-head attention and residual connection
class AttentionBlock(nn.Module):
  def __init__(self, channels, heads):
    super(AttentionBlock, self).__init__()
    self.channels = channels
    self.heads = heads
    self.head_dim = channels // heads
    assert channels % heads == 0, "channels must be divisible by heads"
    self.query_proj = nn.Conv2d(channels, channels, kernel_size=1)
    self.key_proj = nn.Conv2d(channels, channels, kernel_size=1)
    self.value_proj = nn.Conv2d(channels, channels, kernel_size=1)
    self.out_proj = nn.Conv2d(channels, channels, kernel_size=1)
    self.softmax = nn.Softmax(dim=-1)

  def forward(self, x):
    batch_size, _, height, width = x.shape
    query = self.query_proj(x).view(batch_size, self.heads, self.head_dim, height * width).transpose(2,3) # (batch_size, heads, height * width, head_dim)
    key = self.key_proj(x).view(batch_size, self.heads, self.head_dim, height * width) # (batch_size, heads, head_dim, height * width)
    value = self.value_proj(x).view(batch_size, self.heads, self.head_dim, height * width) # (batch_size, heads, head_dim, height * width)
    attention = self.softmax(torch.matmul(query, key) / math.sqrt(self.head_dim)) # (batch_size, heads, height * width, height * width)
    output = torch.matmul(attention, value).transpose(2,3).contiguous().view(batch_size, self.channels, height, width) # (batch_size, channels, height, width)
    output = self.out_proj(output)
    output = output + x
    return output

# Define the diffusion model as a U-Net with skip connections and attention layers
class DiffusionModel(nn.Module):
  def __init__(self):
    super(DiffusionModel, self).__init__()
    # Define the encoder layers
    self.encoder_layers = nn.ModuleList()
    for i in range(num_layers):
      in_channels = num_channels if i == 0 else num_channels_per_layer
      out_channels = num_channels_per_layer
      kernel_size = 3 if i < num_layers - 1 else 1
      stride = 2 if i < num_layers - 1 else 1
      padding = 1 if i < num_layers - 1 else 0
      self.encoder_layers.append(ConvBlock(in_channels + num_classes + 1, out_channels, kernel_size, stride, padding))
      if i % 2 == 0:
        self.encoder_layers.append(AttentionBlock(out_channels, num_attention_heads))
    
    # Define the decoder layers
    self.decoder_layers = nn.ModuleList()
    for i in range(num_layers):
      in_channels = num_channels_per_layer * 2 if i > 0 else num_channels_per_layer
      out_channels = num_channels_per_layer if i < num_layers - 1 else num_channels
      kernel_size = 3 if i < num_layers - 1 else 1
      stride = 1
      padding = 1 if i < num_layers - 1 else 0
      self.decoder_layers.append(ConvBlock(in_channels + num_classes + 1, out_channels, kernel_size, stride, padding))
      if i % 2 == 0:
        self.decoder_layers.append(AttentionBlock(out_channels, num_attention_heads))
    
    # Define the output layer
    self.output_layer = nn.Conv2d(num_channels_per_layer + num_classes + 1, num_channels, kernel_size=3, stride=1, padding=1)

  def forward(self, x, y, t):
    # x: image tensor of shape (batch_size, num_channels, image_size, image_size)
    # y: class label tensor of shape (batch_size,)
    # t: noise level tensor of shape (batch_size,)
    # Returns: score tensor of shape (batch_size, num_channels, image_size, image_size)

    # Embed the class label y into a one-hot vector of shape (batch_size, num_classes)
    y = F.one_hot(y, num_classes=num_classes)

    # Embed the noise level t into a scalar of shape (batch_size,)
    t = torch.log(t / (1 - t))

    # Broadcast y and t to match the spatial dimensions of x
    y = y[:, :, None, None].repeat(1, 1, image_size, image_size)
    t = t[:, None].repeat(1,image_size ** 2).view(-1,image_size,image_size)

    # Concatenate x, y and t along the channel dimension
    xyt = torch.cat([x,y,t], dim=1)

    # Encode xyt using the encoder layers and store the intermediate outputs in a list
    encoder_outputs = []
    for layer in self.encoder_layers:
      xyt = layer(xyt)
      encoder_outputs.append(xyt)
    
    # Decode xyt using the decoder layers and the skip connections from the encoder outputs
    for i in range(num_layers):
      layer = self.decoder_layers[i]
      skip_connection = encoder_outputs[-i-2]
      xyt = F.interpolate(xyt,scale_factor=2) if i > 0 else xyt
      xyt = torch.cat([xyt,y,t], dim=1)
      xyt = layer(xyt)
      xyt = torch.cat([xyt , skip_connection], dim=1) if i < num_layers - 1 else xyt
    
    # Apply the output layer to get the final score
    xyt = torch.cat([xyt,y,t], dim=1)
    score = self.output_layer(xyt)

    return score

# Instantiate the diffusion model and move it