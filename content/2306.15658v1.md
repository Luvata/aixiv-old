---
title: 2306.15658v1 CLIPA-v2  Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy
date: 2023-06-16
---

# [CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy](http://arxiv.org/abs/2306.15658v1)

authors: Xianhang Li, Zeyu Wang, Cihang Xie


## What, Why and How

[1]: https://arxiv.org/abs/2306.15658 "[2306.15658] CLIPA-v2: Scaling CLIP Training with 81.1% ... - arXiv.org"
[2]: https://arxiv.org/pdf/2303.15658v1.pdf "arXiv:2303.15658v1 [hep-th] 28 Mar 2023"
[3]: http://export.arxiv.org/abs/2306.15658 "[2306.15658] CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ..."

Here is a summary of the paper:

```
What: The paper presents CLIPA-v2, a method to scale CLIP training with reduced computations and high zero-shot ImageNet accuracy.

Why: The paper builds upon the previous work CLIPA, which shows an inverse scaling law for CLIP training, meaning that larger image/text encoders can use shorter image/text tokens.

How: The paper extends CLIPA by applying the inverse scaling law to the finetuning stage and exploring CLIP models up to the H/14 size with ~13B image-text pairs. The paper reports that CLIPA-v2 can achieve 81.1% zero-shot ImageNet accuracy within a $10,000 budget and 81.8% accuracy with an extra $4,000 investment.
```

## Main Contributions

The paper claims to have two key contributions:

- Applying the inverse scaling law to the finetuning stage, which enables further reduction in computational needs.
- Exploring CLIPA at scale, extending the experiments up to the H/14 model with ~13B image-text pairs seen during training, and achieving state-of-the-art zero-shot ImageNet accuracy with significantly reduced cost.

## Method Summary

[1]: https://arxiv.org/pdf/2303.15658v1.pdf "arXiv:2303.15658v1 [hep-th] 28 Mar 2023"
[2]: https://arxiv.org/pdf/2306.13650 "arXiv:2306.13650v1 [hep-th] 23 Jun 2023"
[3]: https://arxiv.org/pdf/2106.15658v1 "arXiv:2106.15658v1 [math.AC] 29 Jun 2021"

Here is a summary of the method section:

```
The method section describes how CLIPA-v2 applies the inverse scaling law to the finetuning stage and explores CLIP models at scale. The main steps are:

- Pretraining: The authors use a large-scale image-text dataset with ~13B image-text pairs and train CLIP models with different image/text encoder sizes and sequence lengths. They use a linear warmup and cosine decay learning rate schedule and a contrastive loss function.
- Finetuning: The authors finetune the pretrained CLIP models on ImageNet using a smaller sequence length for both image and text tokens. They use a constant learning rate and a cross-entropy loss function.
- Evaluation: The authors evaluate the finetuned CLIP models on zero-shot ImageNet accuracy, which measures how well the models can classify images without any labels. They also compare the performance and cost of their models with previous CLIP models.
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the image and text encoder sizes and sequence lengths
image_encoder_sizes = [B/16, L/14, H/14]
text_encoder_sizes = [B/64, L/48, H/32]
image_sequence_lengths = [32, 16, 8]
text_sequence_lengths = [64, 32, 16]

# Define the image-text dataset
dataset = load_image_text_dataset()

# Define the contrastive loss function
contrastive_loss = define_contrastive_loss()

# Define the cross-entropy loss function
cross_entropy_loss = define_cross_entropy_loss()

# Define the learning rate schedules
pretrain_lr_schedule = linear_warmup_and_cosine_decay()
finetune_lr_schedule = constant()

# Loop over the image and text encoder sizes and sequence lengths
for i in range(len(image_encoder_sizes)):
  # Initialize the image and text encoders
  image_encoder = ImageEncoder(image_encoder_sizes[i])
  text_encoder = TextEncoder(text_encoder_sizes[i])

  # Pretrain the CLIP model on the image-text dataset
  for batch in dataset:
    # Extract the images and texts from the batch
    images = batch["images"]
    texts = batch["texts"]

    # Encode the images and texts with the encoders
    image_features = image_encoder(images, image_sequence_lengths[i])
    text_features = text_encoder(texts, text_sequence_lengths[i])

    # Compute the contrastive loss
    loss = contrastive_loss(image_features, text_features)

    # Update the encoders with the learning rate schedule
    update_encoders(loss, pretrain_lr_schedule)

  # Finetune the CLIP model on ImageNet
  for batch in ImageNet:
    # Extract the images and labels from the batch
    images = batch["images"]
    labels = batch["labels"]

    # Encode the images and texts with the encoders
    image_features = image_encoder(images, image_sequence_lengths[i] / 2)
    text_features = text_encoder(ImageNet_classes, text_sequence_lengths[i] / 2)

    # Compute the cross-entropy loss
    loss = cross_entropy_loss(image_features, text_features, labels)

    # Update the encoders with the learning rate schedule
    update_encoders(loss, finetune_lr_schedule)

  # Evaluate the CLIP model on zero-shot ImageNet accuracy
  accuracy = evaluate_zero_shot_accuracy(image_encoder, text_encoder)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import datasets

# Define the image and text encoder sizes and sequence lengths
image_encoder_sizes = [B/16, L/14, H/14]
text_encoder_sizes = [B/64, L/48, H/32]
image_sequence_lengths = [32, 16, 8]
text_sequence_lengths = [64, 32, 16]

# Define the image-text dataset
dataset = datasets.load_dataset("openwebtext2", "image")

# Define the contrastive loss function
contrastive_loss = torch.nn.CrossEntropyLoss()

# Define the cross-entropy loss function
cross_entropy_loss = torch.nn.CrossEntropyLoss()

# Define the learning rate schedules
pretrain_lr_schedule = torch.optim.lr_scheduler.CosineAnnealingLR()
finetune_lr_schedule = torch.optim.lr_scheduler.ConstantLR()

# Define the optimizer
optimizer = torch.optim.AdamW()

# Define the ImageNet classes
ImageNet_classes = ["tench", "goldfish", "great white shark", ...]

# Loop over the image and text encoder sizes and sequence lengths
for i in range(len(image_encoder_sizes)):
  # Initialize the image and text encoders
  image_encoder = torchvision.models.vit(image_encoder_sizes[i], pretrained=True)
  text_encoder = transformers.BertModel(text_encoder_sizes[i], pretrained=True)

  # Pretrain the CLIP model on the image-text dataset
  for epoch in range(num_epochs):
    for batch in dataset:
      # Extract the images and texts from the batch
      images = batch["images"]
      texts = batch["texts"]

      # Encode the images and texts with the encoders
      image_features = image_encoder(images, image_sequence_lengths[i])
      text_features = text_encoder(texts, text_sequence_lengths[i])

      # Normalize the features
      image_features = torch.nn.functional.normalize(image_features, dim=-1)
      text_features = torch.nn.functional.normalize(text_features, dim=-1)

      # Compute the logits
      logits = torch.matmul(image_features, text_features.t())

      # Compute the labels
      labels = torch.arange(len(images))

      # Compute the contrastive loss
      loss = contrastive_loss(logits, labels)

      # Zero the gradients
      optimizer.zero_grad()

      # Backpropagate the loss
      loss.backward()

      # Update the encoders with the learning rate schedule
      optimizer.step()
      pretrain_lr_schedule.step()

    # Save the encoders
    torch.save(image_encoder, f"image_encoder_{i}.pt")
    torch.save(text_encoder, f"text_encoder_{i}.pt")

  # Finetune the CLIP model on ImageNet
  for epoch in range(num_epochs):
    for batch in ImageNet:
      # Extract the images and labels from the batch
      images = batch["images"]
      labels = batch["labels"]

      # Encode the images and texts with the encoders
      image_features = image_encoder(images, image_sequence_lengths[i] / 2)
      text_features = text_encoder(ImageNet_classes, text_sequence_lengths[i] / 2)

      # Normalize the features
      image_features = torch.nn.functional.normalize(image_features, dim=-1)
      text_features = torch.nn.functional.normalize(text_features, dim=-1)

      # Compute the logits
      logits = torch.matmul(image_features, text_features.t())

      # Compute the cross-entropy loss
      loss = cross_entropy_loss(logits, labels)

      # Zero the gradients
      optimizer.zero_grad()

      # Backpropagate the loss
      loss.backward()

      # Update the encoders with the learning rate schedule
      optimizer.step()
      finetune_lr_schedule.step()

    # Save the encoders
    torch.save(image_encoder, f"image_encoder_{i}_finetuned.pt")
    torch.save(text_encoder, f"text_encoder_{i}_finetuned.pt")

  # Evaluate the CLIP model on zero-shot ImageNet accuracy
  accuracy = evaluate_zero_shot_accuracy(image_encoder, text_encoder)
```