---
title: 1904.02756v1 Blind Visual Motif Removal from a Single Image
date: 2019-04-03
---

# [Blind Visual Motif Removal from a Single Image](http://arxiv.org/abs/1904.02756v1)

authors: Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/1904.02756 "[1904.02756] Blind Visual Motif Removal from a Single Image - arXiv.org"
[2]: https://arxiv.org/pdf/1904.02756.pdf "arXiv:1904.02756v1 [cs.CV] 4 Apr 2019"
[3]: https://arxiv.org/pdf/1804.02756v1.pdf "arXiv:1804.02756v1 [stat.ML] 8 Apr 2018"
[4]: http://export.arxiv.org/abs/2205.02756v1 "[2205.02756v1] Eigenvalues and Diagonal Elements"

Here is a summary of the paper at http://arxiv.org/pdf/1904.02756v1:

- **What**: The paper proposes a deep learning based technique for blind removal of visual motifs (such as text, symbols or drawings) from a single image, without any user assistance or prior knowledge about the location and geometry of the motif[^1^][1].
- **Why**: The paper aims to address the challenging task of recovering a pristine image from an image that contains overlaid objects that add description or decoration, but also occlude the underlying image. Previous methods have relied on user guidance, multiple images with the same motif, or assumptions about the image content[^1^][1].
- **How**: The paper introduces a network that simultaneously estimates which pixels contain the visual motif, and synthesizes the underlying latent image. The network consists of two branches: a segmentation branch that predicts a binary mask for the motif pixels, and a reconstruction branch that generates a clean image. The network is trained on synthetic data with various types of visual motifs, and is evaluated on both synthetic and real-world images[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/1904.02756 "[1904.02756] Blind Visual Motif Removal from a Single Image - arXiv.org"
[2]: https://arxiv.org/pdf/1904.02756.pdf "arXiv:1904.02756v1 [cs.CV] 4 Apr 2019"
[3]: https://arxiv.org/pdf/1804.02756v1.pdf "arXiv:1804.02756v1 [stat.ML] 8 Apr 2018"
[4]: http://export.arxiv.org/abs/2205.02756v1 "[2205.02756v1] Eigenvalues and Diagonal Elements"

The paper claims the following contributions[^1^][1]:

- A novel network architecture for blind visual motif removal that consists of two branches: a segmentation branch that predicts a binary mask for the motif pixels, and a reconstruction branch that generates a clean image.
- A synthetic dataset with various types of visual motifs, such as text, symbols and drawings, that can be used for training and evaluation of visual motif removal methods.
- State-of-the-art results for blind removal of both opaque and semi-transparent visual motifs on both synthetic and real-world images.

## Method Summary

[1]: https://arxiv.org/abs/1904.02756 "[1904.02756] Blind Visual Motif Removal from a Single Image - arXiv.org"
[2]: https://arxiv.org/pdf/1904.02756.pdf "arXiv:1904.02756v1 [cs.CV] 4 Apr 2019"
[3]: https://arxiv.org/pdf/1804.02756v1.pdf "arXiv:1804.02756v1 [stat.ML] 8 Apr 2018"
[4]: http://export.arxiv.org/abs/2205.02756v1 "[2205.02756v1] Eigenvalues and Diagonal Elements"

Here is a summary of the method section of the paper at http://arxiv.org/pdf/1904.02756v1:

- The paper introduces a network architecture that consists of two branches: a segmentation branch that predicts a binary mask for the motif pixels, and a reconstruction branch that generates a clean image[^1^][2].
- The segmentation branch is based on a U-Net [23] architecture, which takes the input image and outputs a binary mask that indicates which pixels belong to the visual motif. The segmentation branch is trained with a pixel-wise cross-entropy loss[^1^][2].
- The reconstruction branch is based on a conditional generative adversarial network (cGAN) [11] architecture, which takes the input image and the predicted mask from the segmentation branch, and outputs a clean image that does not contain the visual motif. The reconstruction branch is trained with a combination of an adversarial loss, a perceptual loss [12], and an L1 loss[^1^][2].
- The network is trained on synthetic data with various types of visual motifs, such as text, symbols and drawings. The motifs are randomly generated and overlaid on natural images from different datasets, such as COCO [16], Places2 [37], and CelebA [18]. The motifs can be opaque or semi-transparent, and can have different colors, fonts, sizes, and locations[^1^][2].
- The network is evaluated on both synthetic and real-world images with visual motifs. The paper compares the proposed method with several baselines, such as inpainting methods [21, 20], watermark removal methods [5], and text removal methods [24]. The paper also conducts ablation studies to analyze the effect of different components of the network[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image I with a visual motif
# Output: a clean image O without the visual motif

# Define the segmentation branch S
S = UNet(I)

# Define the reconstruction branch R
R = cGAN(I, S)

# Train the network on synthetic data with various visual motifs
for each image-motif pair (I, M) in the synthetic data:
  # Compute the segmentation loss L_s
  L_s = cross_entropy(S(I), M)
  # Compute the reconstruction loss L_r
  L_r = adversarial_loss(R(I, S(I)), I) + perceptual_loss(R(I, S(I)), I) + L1_loss(R(I, S(I)), I)
  # Update the network parameters by minimizing the total loss L
  L = L_s + L_r
  update_network(L)

# Evaluate the network on synthetic and real-world images with visual motifs
for each image I with a visual motif in the test data:
  # Predict the binary mask for the motif pixels
  M_hat = S(I)
  # Generate a clean image without the motif
  O = R(I, M_hat)
  # Compare O with the ground truth image or other baselines
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import random

# Define some hyperparameters
batch_size = 16 # the number of images in a batch
num_epochs = 100 # the number of epochs to train the network
learning_rate = 0.0002 # the learning rate for the optimizer
beta1 = 0.5 # the beta1 parameter for the Adam optimizer
lambda_L1 = 100 # the weight for the L1 loss term
lambda_perceptual = 10 # the weight for the perceptual loss term

# Define the segmentation branch S
# S is a U-Net [23] architecture with 8 down-sampling and up-sampling blocks
# Each block consists of convolution, batch normalization, and activation layers
# The skip connections between the corresponding blocks are implemented by concatenation
# The input and output channels are both 1 (for grayscale images)
S = UNet(in_channels=1, out_channels=1)

# Define the reconstruction branch R
# R is a conditional generative adversarial network (cGAN) [11] architecture with a generator G and a discriminator D
# G is a U-Net [23] architecture with 8 down-sampling and up-sampling blocks
# Each block consists of convolution, batch normalization, and activation layers
# The skip connections between the corresponding blocks are implemented by concatenation
# The input channels are 4 (3 for RGB images and 1 for the mask) and the output channels are 3 (for RGB images)
G = UNet(in_channels=4, out_channels=3)

# D is a PatchGAN [11] architecture with 5 convolutional blocks
# Each block consists of convolution, batch normalization, and activation layers
# The input channels are 6 (3 for RGB images and 3 for the condition) and the output channels are 1 (for real/fake prediction)
D = PatchGAN(in_channels=6, out_channels=1)

# Define the loss functions
# The segmentation loss L_s is a pixel-wise cross-entropy loss between the predicted mask and the ground truth mask
L_s = torch.nn.BCELoss()

# The adversarial loss L_adv is a hinge loss [25] between the discriminator's outputs and the target labels (1 for real and -1 for fake)
L_adv = torch.nn.HingeEmbeddingLoss()

# The perceptual loss L_perceptual is a feature-wise L1 loss between the VGG19 [27] features of the generated image and the ground truth image
L_perceptual = torch.nn.L1Loss()

# The L1 loss L_L1 is a pixel-wise L1 loss between the generated image and the ground truth image
L_L1 = torch.nn.L1Loss()

# Define the optimizers
# The optimizer for S is an Adam [14] optimizer with learning rate learning_rate and beta1 beta1
optimizer_S = torch.optim.Adam(S.parameters(), lr=learning_rate, betas=(beta1, 0.999))

# The optimizer for G is an Adam [14] optimizer with learning rate learning_rate and beta1 beta1
optimizer_G = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=(beta1, 0.999))

# The optimizer for D is an Adam [14] optimizer with learning rate learning_rate and beta1 beta1
optimizer_D = torch.optim.Adam(D.parameters(), lr=learning_rate, betas=(beta1, 0.999))

# Load the synthetic data with various types of visual motifs
# The data consists of pairs of images (I, M), where I is an image with a visual motif and M is a binary mask that indicates the motif pixels
# The motifs are randomly generated and overlaid on natural images from different datasets, such as COCO [16], Places2 [37], and CelebA [18]
# The motifs can be opaque or semi-transparent, and can have different colors, fonts, sizes, and locations
data_loader = load_synthetic_data(batch_size)

# Train the network on synthetic data with various visual motifs
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get a batch of image-mask pairs (I, M) from the data loader
    I, M = batch

    # Predict the binary mask for the motif pixels using S
    M_hat = S(I)

    # Compute the segmentation loss L_s
    L_s_value = L_s(M_hat, M)

    # Update S by minimizing L_s
    optimizer_S.zero_grad()
    L_s_value.backward()
    optimizer_S.step()

    # Generate a clean image without the motif using G
    O = G(torch.cat([I, M_hat], dim=1))

    # Compute the discriminator's outputs for real and fake images
    D_real = D(torch.cat([I, O], dim=1))
    D_fake = D(torch.cat([I, G(torch.cat([I, M_hat], dim=1))], dim=1))

    # Compute the adversarial loss L_adv for G and D
    L_adv_G = L_adv(D_fake, torch.ones_like(D_fake))
    L_adv_D = L_adv(D_real, torch.ones_like(D_real)) + L_adv(D_fake, -torch.ones_like(D_fake))

    # Compute the perceptual loss L_perceptual for G
    L_perceptual_G = L_perceptual(VGG19(O), VGG19(I))

    # Compute the L1 loss L_L1 for G
    L_L1_G = L_L1(O, I)

    # Compute the total reconstruction loss L_r for G
    L_r_G = L_adv_G + lambda_perceptual * L_perceptual_G + lambda_L1 * L_L1_G

    # Update G by minimizing L_r
    optimizer_G.zero_grad()
    L_r_G.backward()
    optimizer_G.step()

    # Update D by minimizing L_adv
    optimizer_D.zero_grad()
    L_adv_D.backward()
    optimizer_D.step()

# Evaluate the network on synthetic and real-world images with visual motifs
for each image I with a visual motif in the test data:
  # Predict the binary mask for the motif pixels using S
  M_hat = S(I)
  # Generate a clean image without the motif using G
  O = G(torch.cat([I, M_hat], dim=1))
  # Compare O with the ground truth image or other baselines using various metrics, such as PSNR, SSIM, and FID
```