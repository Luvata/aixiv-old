---
title: 2104.03960v1 Modulated Periodic Activations for Generalizable Local Functional Representations
date: 2021-04-04
---

# [Modulated Periodic Activations for Generalizable Local Functional Representations](http://arxiv.org/abs/2104.03960v1)

authors: Ishit Mehta, MichaÃ«l Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, Manmohan Chandraker


## What, Why and How

[1]: https://arxiv.org/abs/2104.03960 "[2104.03960] Modulated Periodic Activations for Generalizable Local ..."
[2]: https://arxiv.org/pdf/2108.03960v1.pdf "arXiv:2108.03960v1 [cond-mat.mes-hall] 9 Aug 2021"
[3]: http://export.arxiv.org/abs/2110.03960v1 "[2110.03960v1] Mixability made efficient: Fast online multiclass ..."

Here is a summary of the paper:

- **What**: The paper proposes a new method for creating functional representations of low-dimensional signals such as images, shapes and light fields using multi-layer perceptrons (MLPs) with modulated periodic activations.
- **Why**: The paper aims to improve the generalization and fidelity of functional representations that can capture high-frequency content and handle multiple instances of signals.
- **How**: The paper uses a dual-MLP architecture, where one network synthesizes the output signal from an input coordinate, and another network modulates the periodic activations of the synthesis network with a latent code corresponding to the target signal. The paper also introduces a local-functional representation, where the signal domain is divided into tiles, each with its own latent code. The paper demonstrates the effectiveness of the proposed method on various tasks such as image reconstruction, video compression and shape interpolation.

## Main Contributions

The paper claims the following contributions:

- A novel functional representation based on modulated periodic activations that can generalize to multiple instances of signals and achieve state-of-the-art reconstruction quality.
- A local-functional representation that enables high-fidelity encoding of signals by inferring or optimizing the latent code-book at test time.
- A comprehensive evaluation of the proposed method on various domains and applications, showing its advantages over prior works.

## Method Summary

The method section of the paper describes the proposed functional representation in detail. It consists of three parts:

- The first part introduces the basic idea of using MLPs with periodic activations to represent signals, and explains how to modulate the activations with a latent code to enable generalization and diversity.
- The second part presents the local-functional representation, where the signal domain is partitioned into a regular grid, and each tile is associated with a latent code. It also discusses how to infer or optimize the latent code-book for a given signal at test time.
- The third part provides the implementation details of the proposed method, such as the network architectures, the loss functions, the training procedures and the hyperparameters. It also compares the proposed method with existing methods in terms of memory and computation efficiency.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the synthesis network f and the modulation network g
f = MLP(input_dim, output_dim, periodic_activations)
g = MLP(latent_dim, modulation_dim)

# Define the loss function L
L = reconstruction_loss + regularization_loss

# Define the grid size N and the latent dimension D
N = 16 # number of tiles per dimension
D = 32 # dimension of latent code

# Train the networks on a dataset of signals
for each signal S in the dataset:
  # Initialize the latent code-book Z with random values
  Z = random_tensor(N * N, D)
  
  # Optimize Z with gradient descent to minimize L
  for t in range(num_iterations):
    # Sample a batch of input coordinates x and corresponding output values y from S
    x, y = sample_batch(S)
    
    # Compute the modulation parameters m from Z and x
    m = g(Z[x // N])
    
    # Compute the predicted output values y_hat from f, x and m
    y_hat = f(x, m)
    
    # Compute the loss L from y and y_hat
    loss = L(y, y_hat)
    
    # Update Z with gradient descent
    Z = Z - learning_rate * gradient(loss, Z)
  
  # Save Z as the latent code-book for S
  
# Test the networks on a new signal S*
# Infer or optimize the latent code-book Z* for S*
Z* = infer_or_optimize(f, g, L, S*, N, D)

# Reconstruct S* from Z* using f and g
for each input coordinate x* in S*:
  # Compute the modulation parameters m* from Z* and x*
  m* = g(Z*[x* // N])
  
  # Compute the predicted output value y_hat* from f, x* and m*
  y_hat* = f(x*, m*)
  
  # Save y_hat* as the reconstruction of S*
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensors and neural networks
import numpy as np # for math and arrays
import matplotlib.pyplot as plt # for plotting

# Define the periodic activation function sin_cos
def sin_cos(x):
  # Compute the sine and cosine of x
  sin_x = torch.sin(x)
  cos_x = torch.cos(x)
  
  # Concatenate sin_x and cos_x along the last dimension
  out = torch.cat([sin_x, cos_x], dim=-1)
  
  # Return out
  return out

# Define the synthesis network f
class SynthesisNetwork(torch.nn.Module):
  def __init__(self, input_dim, output_dim, hidden_dim, num_layers):
    # Initialize the parent class
    super().__init__()
    
    # Store the input and output dimensions
    self.input_dim = input_dim
    self.output_dim = output_dim
    
    # Define the linear layers
    self.linear_layers = torch.nn.ModuleList()
    
    # Add the first linear layer with input_dim + modulation_dim inputs and hidden_dim outputs
    self.linear_layers.append(torch.nn.Linear(input_dim + modulation_dim, hidden_dim))
    
    # Add num_layers - 2 linear layers with hidden_dim inputs and outputs
    for i in range(num_layers - 2):
      self.linear_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))
    
    # Add the last linear layer with hidden_dim inputs and output_dim outputs
    self.linear_layers.append(torch.nn.Linear(hidden_dim, output_dim))
  
  def forward(self, x, m):
    # Repeat m along the first dimension to match the batch size of x
    m = m.repeat(x.size(0), 1)
    
    # Concatenate x and m along the last dimension
    x = torch.cat([x, m], dim=-1)
    
    # Apply the linear layers with periodic activations in between
    for i, linear_layer in enumerate(self.linear_layers):
      # Apply the linear layer
      x = linear_layer(x)
      
      # Apply the periodic activation if not the last layer
      if i < len(self.linear_layers) - 1:
        x = sin_cos(x)
    
    # Return x as the output
    return x

# Define the modulation network g
class ModulationNetwork(torch.nn.Module):
  def __init__(self, latent_dim, modulation_dim, hidden_dim, num_layers):
    # Initialize the parent class
    super().__init__()
    
    # Store the latent and modulation dimensions
    self.latent_dim = latent_dim
    self.modulation_dim = modulation_dim
    
    # Define the linear layers
    self.linear_layers = torch.nn.ModuleList()
    
    # Add the first linear layer with latent_dim inputs and hidden_dim outputs
    self.linear_layers.append(torch.nn.Linear(latent_dim, hidden_dim))
    
    # Add num_layers - 2 linear layers with hidden_dim inputs and outputs
    for i in range(num_layers - 2):
      self.linear_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))
    
    # Add the last linear layer with hidden_dim inputs and modulation_dim outputs
    self.linear_layers.append(torch.nn.Linear(hidden_dim, modulation_dim))
  
  def forward(self, z):
    # Apply the linear layers with ReLU activations in between
    for i, linear_layer in enumerate(self.linear_layers):
      # Apply the linear layer
      z = linear_layer(z)
      
      # Apply the ReLU activation if not the last layer
      if i < len(self.linear_layers) - 1:
        z = torch.relu(z)
    
    # Return z as the output
    return z

# Define the reconstruction loss function L_rec as mean squared error (MSE)
def L_rec(y, y_hat):
  return torch.mean((y - y_hat) ** 2)

# Define the regularization loss function L_reg as L2 norm of Z divided by N * N * D
def L_reg(Z):
  return torch.norm(Z) / (N * N * D)

# Define the total loss function L as a weighted sum of L_rec and L_reg
def L(y, y_hat, Z):
  return alpha * L_rec(y, y_hat) + beta * L_reg(Z)

# Define some hyperparameters (can be tuned)
input_dim = 2 # dimension of input coordinate (e.g. pixel position)
output_dim = 3 # dimension of output value (e.g. RGB color)
latent_dim = 32 # dimension of latent code (can be different for different domains)
modulation_dim = 64 # dimension of modulation parameters (can be different for different domains)
hidden_dim = 256 # dimension of hidden units in MLPs
num_layers = 4 # number of layers in MLPs
N = 16 # number of tiles per dimension
alpha = 1.0 # weight of reconstruction loss
beta = 1e-4 # weight of regularization loss
learning_rate = 1e-3 # learning rate for gradient descent
num_iterations = 1000 # number of iterations for optimizing Z
batch_size = 256 # batch size for sampling x and y

# Create the synthesis network f and the modulation network g
f = SynthesisNetwork(input_dim, output_dim, hidden_dim, num_layers)
g = ModulationNetwork(latent_dim, modulation_dim, hidden_dim, num_layers)

# Create an optimizer for Z (e.g. Adam)
optimizer = torch.optim.Adam([Z], lr=learning_rate)

# Train the networks on a dataset of signals
for each signal S in the dataset:
  # Initialize the latent code-book Z with random values
  Z = torch.randn(N * N, latent_dim)
  
  # Optimize Z with gradient descent to minimize L
  for t in range(num_iterations):
    # Sample a batch of input coordinates x and corresponding output values y from S
    x, y = sample_batch(S, batch_size)
    
    # Compute the modulation parameters m from Z and x
    m = g(Z[x // N])
    
    # Compute the predicted output values y_hat from f, x and m
    y_hat = f(x, m)
    
    # Compute the loss L from y, y_hat and Z
    loss = L(y, y_hat, Z)
    
    # Zero the gradients of Z
    optimizer.zero_grad()
    
    # Backpropagate the loss
    loss.backward()
    
    # Update Z with gradient descent
    optimizer.step()
  
  # Save Z as the latent code-book for S
  
# Test the networks on a new signal S*
# Infer or optimize the latent code-book Z* for S*
Z* = infer_or_optimize(f, g, L, S*, N, D)

# Reconstruct S* from Z* using f and g
for each input coordinate x* in S*:
  # Compute the modulation parameters m* from Z* and x*
  m* = g(Z*[x* // N])
  
  # Compute the predicted output value y_hat* from f, x* and m*
  y_hat* = f(x*, m*)
  
  # Save y_hat* as the reconstruction of S*
```