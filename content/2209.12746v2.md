---
title: 2209.12746v2 LSAP  Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space
date: 2022-09-13
---

# [LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space](http://arxiv.org/abs/2209.12746v2)

authors: Pu Cao, Lu Yang, Dongxu Liu, Zhiwei Liu, Shan Li, Qing Song


## What, Why and How

[1]: https://arxiv.org/abs/2209.12746 "[2209.12746] LSAP: Rethinking Inversion Fidelity, Perception and ..."
[2]: http://export.arxiv.org/abs/2209.12746v2 "[2209.12746v2] LSAP: Rethinking Inversion Fidelity, Perception and ..."
[3]: https://arxiv.org/pdf/2209.12746.pdf "arXiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new paradigm for inverting images into latent codes of generative adversarial networks (GANs), which aims to improve the fidelity, perception and editability of the inversion results.
- **Why**: The paper argues that existing inversion methods suffer from a problem of disalignment between the inverse latent codes and the synthetic distribution of GANs, which limits the quality and diversity of the inversion results. The paper also claims that existing evaluation metrics do not capture this problem well.
- **How**: The paper introduces a new evaluation metric called **Cosine Distance in Normalized Style Space (SNCD)**, which measures the degree of alignment between the inverse latent codes and the synthetic distribution. The paper also proposes a new inversion paradigm called **Latent Space Alignment Inversion Paradigm (LSAP)**, which consists of optimizing SNCD in both encoder-based and optimization-based embedding methods. The paper shows that LSAP can achieve state-of-the-art performance in both image embedding and result refinement steps.

## Main Contributions

[1]: https://arxiv.org/abs/2209.12746 "[2209.12746] LSAP: Rethinking Inversion Fidelity, Perception and ..."
[2]: http://export.arxiv.org/abs/2209.12746v2 "[2209.12746v2] LSAP: Rethinking Inversion Fidelity, Perception and ..."
[3]: https://arxiv.org/pdf/2209.12746.pdf "arXiv.org"

According to the paper[^1^][1], the main contributions are:

- **Proposing a new evaluation metric (SNCD)** that measures the degree of alignment between the inverse latent codes and the synthetic distribution of GANs, and showing that it correlates well with perception and editability of the inversion results.
- **Proposing a new inversion paradigm (LSAP)** that consists of optimizing SNCD in both encoder-based and optimization-based embedding methods, and showing that it can improve the fidelity, perception and editability of the inversion results in various domains.
- **Providing extensive experiments and analyses** to demonstrate the effectiveness and superiority of LSAP over existing methods, and to reveal some insights and limitations of current inversion methods.

## Method Summary

[1]: https://arxiv.org/abs/2209.12746 "[2209.12746] LSAP: Rethinking Inversion Fidelity, Perception and ..."
[2]: http://export.arxiv.org/abs/2209.12746v2 "[2209.12746v2] LSAP: Rethinking Inversion Fidelity, Perception and ..."
[3]: https://arxiv.org/pdf/2209.12746.pdf "arXiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first introduces the concept of **Normalized Style Space ( space)**, which is a normalized version of the style space of GANs, where each dimension has zero mean and unit variance. The paper shows that space can better reflect the semantic information and diversity of GANs than the original style space.
- The paper then defines the **Cosine Distance in Normalized Style Space (SNCD)** as a metric to measure the degree of alignment between the inverse latent codes and the synthetic distribution of GANs. The paper argues that SNCD can capture the perception and editability of the inversion results better than existing metrics such as LPIPS and FID.
- The paper then proposes the **Latent Space Alignment Inversion Paradigm (LSAP)**, which consists of optimizing SNCD in both encoder-based and optimization-based embedding methods. The paper describes how to implement LSAP for different GAN architectures and inversion methods, and how to balance SNCD with other objectives such as reconstruction loss and regularization loss.
- The paper also discusses some details and challenges of applying LSAP, such as choosing appropriate hyperparameters, dealing with mode collapse and truncation trick, and extending LSAP to result refinement step.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the normalized style space
def normalize_style_space(style_space):
  # Compute the mean and standard deviation of each dimension
  mean = np.mean(style_space, axis=0)
  std = np.std(style_space, axis=0)
  # Normalize each dimension by subtracting the mean and dividing by the std
  normalized_style_space = (style_space - mean) / std
  return normalized_style_space

# Define the cosine distance in normalized style space
def cosine_distance_in_normalized_style_space(latent_code, style_space):
  # Normalize the style space
  normalized_style_space = normalize_style_space(style_space)
  # Normalize the latent code
  latent_code = latent_code / np.linalg.norm(latent_code)
  # Compute the cosine distance between the latent code and the normalized style space
  cosine_distance = 1 - np.dot(latent_code, normalized_style_space.T)
  return cosine_distance

# Define the latent space alignment inversion paradigm
def latent_space_alignment_inversion_paradigm(image, GAN, embedding_method):
  # Initialize the latent code
  latent_code = embedding_method.initialize_latent_code(image, GAN)
  # Define the reconstruction loss
  reconstruction_loss = embedding_method.compute_reconstruction_loss(image, GAN, latent_code)
  # Define the regularization loss
  regularization_loss = embedding_method.compute_regularization_loss(latent_code)
  # Define the alignment loss
  alignment_loss = cosine_distance_in_normalized_style_space(latent_code, GAN.style_space)
  # Define the total loss
  total_loss = reconstruction_loss + lambda_1 * regularization_loss + lambda_2 * alignment_loss
  # Optimize the total loss with respect to the latent code
  latent_code = embedding_method.optimize_total_loss(total_loss, latent_code)
  # Return the optimized latent code
  return latent_code
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from PIL import Image

# Define the GAN model
class GAN(nn.Module):
  def __init__(self):
    super(GAN, self).__init__()
    # Define the generator network
    self.generator = Generator()
    # Define the style space
    self.style_space = self.get_style_space()

  def forward(self, latent_code):
    # Generate an image from the latent code
    image = self.generator(latent_code)
    return image

  def get_style_space(self):
    # Sample a large number of latent codes from the prior distribution
    latent_codes = torch.randn(10000, 512)
    # Feed them to the generator and get the style vectors
    style_vectors = self.generator.get_style_vectors(latent_codes)
    # Convert the style vectors to numpy array
    style_space = style_vectors.detach().cpu().numpy()
    return style_space

# Define the generator network
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the mapping network
    self.mapping_network = MappingNetwork()
    # Define the synthesis network
    self.synthesis_network = SynthesisNetwork()

  def forward(self, latent_code):
    # Map the latent code to an intermediate latent vector
    intermediate_latent_vector = self.mapping_network(latent_code)
    # Synthesize an image from the intermediate latent vector
    image = self.synthesis_network(intermediate_latent_vector)
    return image

  def get_style_vectors(self, latent_codes):
    # Map the latent codes to intermediate latent vectors
    intermediate_latent_vectors = self.mapping_network(latent_codes)
    # Get the style vectors from the intermediate latent vectors
    style_vectors = self.synthesis_network.get_style_vectors(intermediate_latent_vectors)
    return style_vectors

# Define the mapping network
class MappingNetwork(nn.Module):
  def __init__(self):
    super(MappingNetwork, self).__init__()
    # Define a list of fully connected layers with leaky ReLU activation
    self.layers = nn.ModuleList([nn.Linear(512, 512) for _ in range(8)])
    self.activation = nn.LeakyReLU(0.2)

  def forward(self, latent_code):
    # Apply each layer to the latent code and get the intermediate latent vector
    intermediate_latent_vector = latent_code
    for layer in self.layers:
      intermediate_latent_vector = layer(intermediate_latent_vector)
      intermediate_latent_vector = self.activation(intermediate_latent_vector)
    return intermediate_latent_vector

# Define the synthesis network
class SynthesisNetwork(nn.Module):
  def __init__(self):
    super(SynthesisNetwork, self).__init__()
    # Define a list of convolutional blocks with adaptive instance normalization (AdaIN)
    self.blocks = nn.ModuleList([ConvBlock(512, 512) for _ in range(4)] + [ConvBlock(512, 256) for _ in range(4)])
  
  def forward(self, intermediate_latent_vector):
    # Initialize a constant input tensor
    input_tensor = torch.ones((1, 512, 4, 4))
    # Apply each block to the input tensor and get the output tensor
    output_tensor = input_tensor
    for i, block in enumerate(self.blocks):
      # Get the corresponding style vector from the intermediate latent vector
      style_vector = intermediate_latent_vector[:, i * 512 : (i + 1) * 512]
      # Apply the block to the output tensor and get the updated output tensor
      output_tensor = block(output_tensor, style_vector)