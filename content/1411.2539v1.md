---
title: 1411.2539v1 Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models
date: 2014-11-26
---

# [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](http://arxiv.org/abs/1411.2539v1)

authors: Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel


## What, Why and How

[1]: https://arxiv.org/abs/1411.2539 "[1411.2539] Unifying Visual-Semantic Embeddings with Multimodal Neural ..."
[2]: https://arxiv.org/pdf/1411.2539v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/pdf/0711.2539 "2 5 arXiv:0711.2539v1 [cond-mat.mtrl-sci] 16 Nov 2007"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper introduces an encoder-decoder pipeline that learns a multimodal joint embedding space with images and text and a novel language model for decoding distributed representations from the space.
- **Why**: The paper aims to unify joint image-text embedding models with multimodal neural language models and to enable ranking images and sentences as well as generating novel descriptions from scratch.
- **How**: The paper uses LSTM to encode sentences and a structure-content neural language model to decode them, conditioned on representations produced by the encoder. The paper also uses the 19-layer Oxford convolutional network to extract image features. The paper evaluates the model on Flickr8K and Flickr30K datasets and shows that it matches or surpasses the state-of-the-art performance. The paper also demonstrates that the learned embedding space captures multimodal regularities in terms of vector space arithmetic.

## Main Contributions

[1]: https://arxiv.org/abs/1411.2539 "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models"
[2]: https://arxiv.org/pdf/1411.2539.pdf "Abstract - arXiv.org"
[3]: https://www.researchgate.net/publication/332368602_Unified_Visual-Semantic_Embeddings_Bridging_Vision_and_Language_with_Structured_Meaning_Representations "Unified Visual-Semantic Embeddings: Bridging Vision and Language with ..."
[4]: https://upcommons.upc.edu/bitstream/handle/2117/134582/18__Caption_Retrieval__Special_Issue_on_Semantic_Deep_Learning___Reviewed___postprint_version_.pdf?sequence=3 "STUDYING THE I FULL-NETWORK EMBEDDING MULTIMODAL PIPELINES"

According to the paper at [^1^][1], the main contributions are:

- Introducing an **encoder-decoder pipeline** that learns a **multimodal joint embedding space** with images and text and a **novel language model** for decoding distributed representations from the space.
- Introducing the **structure-content neural language model (SC-NLM)** that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder.
- Matching or surpassing the **state-of-the-art performance** on Flickr8K and Flickr30K without using object detections and setting new best results when using the 19-layer Oxford convolutional network.
- Showing that the learned embedding space captures **multimodal regularities** in terms of vector space arithmetic e.g. *image of a blue car* - "blue" + "red" is near images of red cars.

## Method Summary

[1]: https://arxiv.org/abs/1411.2539 "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models"
[2]: https://arxiv.org/pdf/1411.2539.pdf "Abstract - arXiv.org"
[3]: https://github.com/ryankiros/visual-semantic-embedding "ryankiros/visual-semantic-embedding - GitHub"
[4]: https://ui.adsabs.harvard.edu/abs/2014arXiv1411.2539K/abstract "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models"

Here is a summary of the method section of the paper at [^1^][1]:

- The method section consists of three subsections: **Joint Image-Sentence Embedding**, **Structure-Content Neural Language Model**, and **Learning and Inference**.
- In the **Joint Image-Sentence Embedding** subsection, the paper describes how to encode sentences using LSTM and how to project image features from a convolutional network into the same embedding space as the LSTM hidden states. The paper also defines a pairwise ranking loss function that learns to rank images and sentences based on their similarity in the embedding space.
- In the **Structure-Content Neural Language Model** subsection, the paper introduces a novel language model that decodes distributed representations from the embedding space into natural language sentences. The paper explains how the model disentangles the structure of a sentence (e.g. syntax, grammar) from its content (e.g. semantics, meaning), conditioned on representations produced by the encoder. The paper also shows how the model can generate novel descriptions for images by sampling from the decoder distribution.
- In the **Learning and Inference** subsection, the paper describes how to train the encoder and decoder models jointly using stochastic gradient descent and backpropagation through time. The paper also discusses how to perform inference using beam search or sampling for image caption generation and retrieval tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Encoder: learn a joint image-sentence embedding space
# Input: a set of images and sentences
# Output: a set of image and sentence embeddings

# Define an LSTM network to encode sentences
lstm = LSTM(input_size, hidden_size)

# Define a linear layer to project image features into the embedding space
image_projector = Linear(image_size, hidden_size)

# Define a pairwise ranking loss function
ranking_loss = PairwiseRankingLoss(margin)

# For each image and sentence pair in the input set
for image, sentence in input_set:
  # Extract image features from a convolutional network
  image_features = conv_net(image)
  
  # Project image features into the embedding space
  image_embedding = image_projector(image_features)
  
  # Encode sentence using LSTM
  sentence_embedding = lstm(sentence)
  
  # Compute ranking loss between image and sentence embeddings
  loss = ranking_loss(image_embedding, sentence_embedding)
  
  # Update encoder parameters using gradient descent
  encoder_params -= learning_rate * grad(loss, encoder_params)

# Decoder: learn a structure-content neural language model
# Input: a set of image and sentence embeddings
# Output: a set of generated sentences

# Define a linear layer to project content embeddings into the word space
content_projector = Linear(hidden_size, vocab_size)

# Define a linear layer to project structure embeddings into the word space
structure_projector = Linear(hidden_size, vocab_size)

# Define a softmax function to compute word probabilities
softmax = Softmax()

# For each image and sentence embedding pair in the input set
for image_embedding, sentence_embedding in input_set:
  # Split the sentence embedding into structure and content parts
  structure_embedding, content_embedding = split(sentence_embedding)
  
  # Initialize the decoder hidden state with the image embedding
  decoder_hidden = image_embedding
  
  # Initialize the generated sentence with an empty list
  generated_sentence = []
  
  # While the generated sentence is not complete
  while not end_of_sentence(generated_sentence):
    # Compute the word probabilities using the content projector and softmax
    content_probs = softmax(content_projector(content_embedding))
    
    # Compute the word probabilities using the structure projector and softmax
    structure_probs = softmax(structure_projector(structure_embedding))
    
    # Combine the word probabilities using element-wise multiplication
    word_probs = content_probs * structure_probs
    
    # Sample a word from the word probabilities or use beam search
    word = sample(word_probs) or beam_search(word_probs)
    
    # Append the word to the generated sentence
    generated_sentence.append(word)
    
    # Update the decoder hidden state using LSTM
    decoder_hidden = lstm(word, decoder_hidden)
    
    # Update the content embedding using LSTM
    content_embedding = lstm(word, content_embedding)
  
  # Compute the negative log-likelihood loss between the generated and target sentences
  loss = nll_loss(generated_sentence, target_sentence)
  
  # Update decoder parameters using gradient descent
  decoder_params -= learning_rate * grad(loss, decoder_params)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torch.nn as nn # for neural network modules
import torch.optim as optim # for optimization algorithms
import torchvision.models as models # for convolutional networks
import torchvision.transforms as transforms # for image transformations
import nltk # for natural language processing
import numpy as np # for numerical operations

# Define some hyperparameters
input_size = 300 # the size of the word embeddings
hidden_size = 1024 # the size of the LSTM hidden states
image_size = 4096 # the size of the image features
vocab_size = 10000 # the size of the vocabulary
margin = 0.2 # the margin for the ranking loss
learning_rate = 0.01 # the learning rate for gradient descent
batch_size = 32 # the batch size for training
num_epochs = 50 # the number of epochs for training

# Load the Flickr8K or Flickr30K dataset
dataset = load_dataset('Flickr8K') or load_dataset('Flickr30K')

# Split the dataset into train, validation and test sets
train_set, val_set, test_set = split_dataset(dataset)

# Define a tokenizer to convert sentences into tokens
tokenizer = nltk.tokenize.TreebankWordTokenizer()

# Build a vocabulary from the train set sentences
vocab = build_vocab(train_set.sentences, vocab_size)

# Define a transform to resize and normalize images
transform = transforms.Compose([
  transforms.Resize(256), # resize to 256 x 256 pixels
  transforms.CenterCrop(224), # crop to 224 x 224 pixels
  transforms.ToTensor(), # convert to tensor
  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # normalize using ImageNet statistics
])

# Load a pretrained convolutional network (e.g. VGG19) and freeze its parameters
conv_net = models.vgg19(pretrained=True)
conv_net.eval()
for param in conv_net.parameters():
  param.requires_grad = False

# Define an LSTM network to encode sentences
lstm = nn.LSTM(input_size, hidden_size)

# Define a linear layer to project image features into the embedding space
image_projector = nn.Linear(image_size, hidden_size)

# Define a pairwise ranking loss function
ranking_loss = nn.MarginRankingLoss(margin)

# Define a linear layer to project content embeddings into the word space
content_projector = nn.Linear(hidden_size, vocab_size)

# Define a linear layer to project structure embeddings into the word space
structure_projector = nn.Linear(hidden_size, vocab_size)

# Define a softmax function to compute word probabilities
softmax = nn.Softmax(dim=1)

# Define a negative log-likelihood loss function
nll_loss = nn.NLLLoss()

# Define an optimizer to update encoder and decoder parameters jointly
optimizer = optim.SGD([lstm.parameters(), image_projector.parameters(), content_projector.parameters(), structure_projector.parameters()], lr=learning_rate)

# Define a function to split a sentence embedding into structure and content parts
def split(embedding):
  # Assume that the embedding is a tensor of shape (hidden_size,)
  # Split it into two tensors of shape (hidden_size / 2,)
  structure_embedding = embedding[:hidden_size // 2]
  content_embedding = embedding[hidden_size // 2:]
  return structure_embedding, content_embedding

# Define a function to check if a generated sentence is complete
def end_of_sentence(sentence):
  # Assume that the sentence is a list of tokens (strings)
  # Check if the last token is a period ('.')
  return sentence[-1] == '.'

# Define a function to sample a word from a probability distribution
def sample(probs):
  # Assume that probs is a tensor of shape (vocab_size,) containing word probabilities
  # Sample a word index from probs using multinomial distribution
  index = torch.multinomial(probs, num_samples=1)
  return index

# Define a function to perform beam search for a probability distribution
def beam_search(probs):
  # Assume that probs is a tensor of shape (vocab_size,) containing word probabilities
  # Use beam search algorithm to find the most likely word index (see https://en.wikipedia.org/wiki/Beam_search)
  
  # Initialize a beam size k (e.g. k=5)
  k = 5
  
  # Initialize a list of k candidates, each containing a word index and a score (log probability)
  candidates = [(index, torch.log(probs[index])) for index in torch.topk(probs, k=k).indices]
  
  # Repeat until a candidate is complete or reaches a maximum length (e.g. 20)
  while not end_of_sentence(candidates[0][0]) and len(candidates[0][0]) < 20:
    # Initialize a new list of candidates
    new_candidates = []
    
    # For each candidate in the current list
    for candidate in candidates:
      # Get the last word index and the score of the candidate
      word, score = candidate
      
      # Compute the next word probabilities using the decoder model
      next_probs = decoder(word)
      
      # Add k new candidates to the new list, each with a different next word and an updated score
      new_candidates.extend([(word + [index], score + torch.log(next_probs[index])) for index in torch.topk(next_probs, k=k).indices])
    
    # Sort the new candidates by their scores in descending order
    new_candidates.sort(key=lambda x: x[1], reverse=True)
    
    # Keep only the top k candidates for the next iteration
    candidates = new_candidates[:k]
  
  # Return the first (best) candidate's word index
  return candidates[0][0]

# For each epoch
for epoch in range(num_epochs):
  # Shuffle the train set
  train_set.shuffle()
  
  # Initialize the total loss to zero
  total_loss = 0
  
  # For each batch of image and sentence pairs in the train set
  for image_batch, sentence_batch in train_set.batch(batch_size):
    # Extract image features from the convolutional network
    image_features = conv_net(image_batch)
    
    # Project image features into the embedding space
    image_embeddings = image_projector(image_features)
    
    # Encode sentences using LSTM
    sentence_embeddings = lstm(sentence_batch)
    
    # Compute ranking loss between image and sentence embeddings
    loss = ranking_loss(image_embeddings, sentence_embeddings)
    
    # Update encoder parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Split the sentence embeddings into structure and content parts
    structure_embeddings, content_embeddings = split(sentence_embeddings)
    
    # Initialize the decoder hidden states with the image embeddings
    decoder_hiddens = image_embeddings
    
    # Initialize the generated sentences with empty lists
    generated_sentences = [[] for _ in range(batch_size)]
    
    # While the generated sentences are not complete
    while not all(end_of_sentence(sentence) for sentence in generated_sentences):
      # Compute the word probabilities using the content projector and softmax
      content_probs = softmax(content_projector(content_embeddings))
      
      # Compute the word probabilities using the structure projector and softmax
      structure_probs = softmax(structure_projector(structure_embeddings))
      
      # Combine the word probabilities using element-wise multiplication
      word_probs = content_probs * structure_probs
      
      # Sample a word from the word probabilities or use beam search for each sentence in the batch
      words = [sample(word_prob) or beam_search(word_prob) for word_prob in word_probs]
      
      # Append the words to the generated sentences
      for i in range(batch_size):
        generated_sentences[i].append(words[i])
      
      # Update the decoder hidden states using LSTM
      decoder_hiddens = lstm(words, decoder_hiddens)
      
      # Update the content embeddings using LSTM
      content_embeddings = lstm(words, content_embeddings)
    
    # Compute the negative log-likelihood loss between the generated and target sentences
    loss = nll_loss(generated_sentences, target_sentences)
    
    # Update decoder parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Accumulate the total loss
    total_loss += loss.item()
  
  # Print the average loss per batch for this epoch
  print(f'Epoch {epoch}, Loss: {total_loss / len(train_set)}')
```