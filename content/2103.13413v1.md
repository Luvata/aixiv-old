---
title: 2103.13413v1 Vision Transformers for Dense Prediction
date: 2021-03-14
---

# [Vision Transformers for Dense Prediction](http://arxiv.org/abs/2103.13413v1)

authors: Ren√© Ranftl, Alexey Bochkovskiy, Vladlen Koltun


## What, Why and How

[1]: https://arxiv.org/pdf/2103.13413v1.pdf "arXiv:2103.13413v1 [cs.CV] 24 Mar 2021"
[2]: https://arxiv.org/abs/2103.13413 "[2103.13413] Vision Transformers for Dense Prediction - arXiv.org"
[3]: http://export.arxiv.org/abs/2103.13413v1 "[2103.13413v1] Vision Transformers for Dense Prediction"

Here is a summary of the paper:

- **What**: The paper introduces **dense vision transformers**, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks such as monocular depth estimation and semantic segmentation[^1^][1].
- **Why**: The paper argues that convolutional networks have drawbacks for dense prediction tasks, such as losing feature resolution and granularity in the deeper stages of the model and having a limited receptive field. Vision transformers, on the other hand, process representations at a constant and relatively high resolution and have a global receptive field at every stage, which allows them to provide finer-grained and more globally coherent predictions[^1^][1].
- **How**: The paper proposes to assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The paper also introduces a new training strategy that uses a combination of self-supervised learning and supervised learning with large-scale datasets. The paper evaluates the proposed architecture on several dense prediction tasks and shows that it outperforms state-of-the-art fully-convolutional networks[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- It introduces **dense vision transformers**, a novel architecture that leverages vision transformers as a backbone for dense prediction tasks and shows that it outperforms state-of-the-art fully-convolutional networks on several benchmarks.
- It proposes a new **training strategy** that uses a combination of self-supervised learning and supervised learning with large-scale datasets to effectively train vision transformers for dense prediction tasks.
- It provides a **comprehensive analysis** of the proposed architecture and its components, such as the token assembly, the decoder design, and the transformer configuration. It also studies the impact of different training data sources and sizes on the performance of the model.

## Method Summary

Here is a summary of the method section of the paper:

- The paper describes the **dense vision transformer** architecture, which consists of a vision transformer backbone and a convolutional decoder. The backbone is based on the ViT-B/16 model , which takes patches of size 16x16 as input tokens and applies 12 transformer blocks. The decoder takes tokens from different stages of the backbone and assembles them into image-like representations at various resolutions using bilinear interpolation. The decoder then uses skip connections and convolutional layers to fuse the representations and produce the final dense predictions at the input resolution.
- The paper also introduces a new **training strategy** for dense vision transformers, which combines self-supervised learning and supervised learning with large-scale datasets. The paper uses the ImageNet-21k dataset  for self-supervised learning, where the model is trained to predict masked patches using a contrastive loss. The paper then uses the ADE20K dataset  for supervised learning, where the model is trained to perform semantic segmentation using a cross-entropy loss. The paper also uses data augmentation techniques such as random cropping, scaling, flipping, color jittering, and cutout to improve the generalization of the model.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the vision transformer backbone
backbone = ViT-B/16()

# Define the convolutional decoder
decoder = ConvDecoder()

# Define the self-supervised loss
ssl_loss = ContrastiveLoss()

# Define the supervised loss
sup_loss = CrossEntropyLoss()

# Load the ImageNet-21k dataset for self-supervised learning
ssl_data = ImageNet-21k()

# Load the ADE20K dataset for supervised learning
sup_data = ADE20K()

# Train the model using a two-stage strategy
for epoch in range(num_epochs):
  # Stage 1: Self-supervised learning
  for batch in ssl_data:
    # Apply data augmentation to the batch
    batch = augment(batch)
    # Mask some patches in the batch
    masked_batch, mask = mask_patches(batch)
    # Forward pass through the backbone
    tokens = backbone(masked_batch)
    # Predict the masked patches using a linear projection head
    pred_patches = projection_head(tokens[mask])
    # Compute the self-supervised loss using a contrastive loss
    loss = ssl_loss(pred_patches, batch[mask])
    # Backpropagate and update the backbone parameters
    loss.backward()
    update(backbone.parameters())
  
  # Stage 2: Supervised learning
  for batch in sup_data:
    # Apply data augmentation to the batch
    batch = augment(batch)
    # Get the input images and labels from the batch
    images, labels = batch
    # Forward pass through the backbone and decoder
    tokens = backbone(images)
    outputs = decoder(tokens)
    # Compute the supervised loss using a cross-entropy loss
    loss = sup_loss(outputs, labels)
    # Backpropagate and update both the backbone and decoder parameters
    loss.backward()
    update(backbone.parameters(), decoder.parameters())
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

# Define some hyperparameters
num_epochs = 100 # Number of training epochs
batch_size = 16 # Batch size for training
lr = 1e-4 # Learning rate for optimizer
num_classes = 150 # Number of classes for semantic segmentation
patch_size = 16 # Patch size for vision transformer
num_heads = 12 # Number of attention heads for vision transformer
num_layers = 12 # Number of transformer blocks for vision transformer
hidden_size = 768 # Hidden size for vision transformer
mlp_size = 3072 # MLP size for vision transformer
temperature = 0.07 # Temperature for contrastive loss

# Define the vision transformer backbone
class ViT(nn.Module):
  def __init__(self):
    super(ViT, self).__init__()
    # Define the patch embedding layer
    self.patch_embed = nn.Conv2d(3, hidden_size, kernel_size=patch_size, stride=patch_size)
    # Define the positional embedding layer
    self.pos_embed = nn.Parameter(torch.randn(1, (256 // patch_size) ** 2 + 1, hidden_size))
    # Define the class token
    self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))
    # Define the transformer blocks
    self.blocks = nn.ModuleList([TransformerBlock() for _ in range(num_layers)])
    # Define the layer normalization layer
    self.ln = nn.LayerNorm(hidden_size)

  def forward(self, x):
    # Get the batch size and image size from the input
    b, c, h, w = x.shape
    # Embed the patches using the patch embedding layer
    x = self.patch_embed(x) # shape: (b, hidden_size, h // patch_size, w // patch_size)
    # Reshape and transpose the patches to get a sequence of tokens
    x = x.reshape(b, hidden_size, -1).transpose(1, 2) # shape: (b, (h // patch_size) * (w // patch_size), hidden_size)
    # Add the class token to the beginning of the sequence
    x = torch.cat([self.cls_token.expand(b, -1, -1), x], dim=1) # shape: (b, (h // patch_size) * (w // patch_size) + 1, hidden_size)
    # Add the positional embedding to the tokens
    x = x + self.pos_embed # shape: (b, (h // patch_size) * (w // patch_size) + 1, hidden_size)
    # Pass the tokens through the transformer blocks
    for block in self.blocks:
      x = block(x) # shape: (b, (h // patch_size) * (w // patch_size) + 1, hidden_size)
    # Apply layer normalization to the output tokens
    x = self.ln(x) # shape: (b, (h // patch_size) * (w // patch_size) + 1, hidden_size)
    return x

# Define the transformer block
class TransformerBlock(nn.Module):
  def __init__(self):
    super(TransformerBlock, self).__init__()
    # Define the multi-head attention layer
    self.mha = nn.MultiheadAttention(hidden_size, num_heads)
    # Define the first residual connection and layer normalization layer
    self.ln1 = nn.LayerNorm(hidden_size)
    # Define the MLP layer
    self.mlp = nn.Sequential(
      nn.Linear(hidden_size, mlp_size),
      nn.GELU(),
      nn.Linear(mlp_size, hidden_size)
    )
    # Define the second residual connection and layer normalization layer
    self.ln2 = nn.LayerNorm(hidden_size)

  def forward(self, x):
    # Get the batch size and sequence length from the input
    b, n, _ = x.shape 
    # Transpose the input for multi-head attention layer
    x = x.transpose(0, 1) # shape: (n, b, hidden_size)
    # Apply multi-head attention to the input tokens and add a residual connection
    x = x + self.mha(x, x ,x)[0] # shape: (n ,b ,hidden_size)
    # Apply layer normalization to the output tokens 
    x = self.ln1(x) # shape: (n ,b ,hidden_size)
    # Apply MLP to the output tokens and add a residual connection
    x = x + self.mlp(x.transpose(0, 1)).transpose(0, 1) # shape: (n ,b ,hidden_size)
    # Apply layer normalization to the output tokens
    x = self.ln2(x) # shape: (n ,b ,hidden_size)
    # Transpose the output for the next layer
    x = x.transpose(0, 1) # shape: (b, n, hidden_size)
    return x

# Define the convolutional decoder
class ConvDecoder(nn.Module):
  def __init__(self):
    super(ConvDecoder, self).__init__()
    # Define the token assembly layers
    self.token_assembly_1 = nn.Sequential(
      nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=2, padding=1),
      nn.GELU(),
      nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1),
      nn.GELU()
    )
    self.token_assembly_2 = nn.Sequential(
      nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=2, padding=1),
      nn.GELU(),
      nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1),
      nn.GELU()
    )
    self.token_assembly_3 = nn.Sequential(
      nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=2, padding=1),
      nn.GELU(),
      nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1),
      nn.GELU()
    )
    # Define the skip connection layers
    self.skip_1 = nn.Conv2d(hidden_size * 2, hidden_size * 2, kernel_size=3, stride=1, padding=1)
    self.skip_2 = nn.Conv2d(hidden_size * 4, hidden_size * 4, kernel_size=3, stride=1, padding=1)
    self.skip_3 = nn.Conv2d(hidden_size * 8, hidden_size * 8, kernel_size=3, stride=1, padding=1)
    # Define the upsampling layers
    self.upsample_1 = nn.Sequential(
      nn.Upsample(scale_factor=4),
      nn.Conv2d(hidden_size * 8 ,hidden_size * 4 ,kernel_size=3 ,stride=1 ,padding=1),
      nn.GELU()
    )
    self.upsample_2 = nn.Sequential(
      nn.Upsample(scale_factor=4),
      nn.Conv2d(hidden_size * 4 ,hidden_size * 2 ,kernel_size=3 ,stride=1 ,padding=1),
      nn.GELU()
    )
    self.upsample_3 = nn.Sequential(
      nn.Upsample(scale_factor=4),
      nn.Conv2d(hidden_size * 2 ,hidden_size ,kernel_size=3 ,stride=1 ,padding=1),
      nn.GELU()
    )
    # Define the final convolutional layer
    self.final_conv = nn.Conv2d(hidden_size ,num_classes ,kernel_size=3 ,stride=1 ,padding=1)

  def forward(self, x):
    # Get the batch size from the input
    b = x.shape[0]
    # Get the tokens from different stages of the backbone
    tokens_0 = x[:, :((256 // patch_size) ** 2) + 1] # shape: (b ,((256 // patch_size) ** 2) + 1 ,hidden_size)
    tokens_4 = x[:, ((256 // patch_size) ** 2) + 1:((256 // patch_size) ** 2) + ((128 // patch_size) ** 2) + 5] # shape: (b ,((128 // patch_size) ** 2) + 4 ,hidden_size)
    tokens_8 = x[:, ((256 // patch_size) ** 2) + ((128 // patch_size) ** 2) + 5:((256 // patch_size) ** 2) + ((128 // patch_size) ** 2) + ((64 // patch_size) ** 2) + 9] # shape: (b ,((64 // patch_size) ** 2) + 8 ,hidden_size)
    tokens_12 = x[:, ((256 // patch_size) ** 2) + ((128 // patch_size) ** 2) + ((64 // patch_size) ** 2) + 9:] # shape: (b ,((32 // patch_size) ** 2) + 12 ,hidden_siz
e)
    
    # Reshape and transpose the tokens to get image-like representations