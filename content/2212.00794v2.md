---
title: 2212.00794v2 Scaling Language-Image Pre-training via Masking
date: 2022-12-01
---

# [Scaling Language-Image Pre-training via Masking](http://arxiv.org/abs/2212.00794v2)

authors: Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, Kaiming He


## What, Why and How

[1]: https://arxiv.org/abs/2212.00794 "[2212.00794] Scaling Language-Image Pre-training via Masking - arXiv.org"
[2]: https://arxiv.org/pdf/2212.00794.pdf "arXiv:2212.00794v2 [cs.CV] 30 Mar 2023"
[3]: http://export.arxiv.org/abs/2212.00794 "[2212.00794] Scaling Language-Image Pre-training via Masking"

Here is a summary of the paper:

- **What**: The paper presents Fast Language-Image Pre-training (FLIP), a simple and more efficient method for training CLIP [^1^][1], a model that learns visual representations from natural language supervision.
- **Why**: The paper aims to improve the accuracy and speed of CLIP training by using masking, a technique that randomly removes a large portion of image patches during training. Masking allows the model to learn from more image-text pairs given the same wall-clock time and contrast more samples per iteration with similar memory footprint.
- **How**: The paper evaluates FLIP on 400 million image-text pairs and compares it with the original CLIP method. The paper shows that FLIP outperforms CLIP on a variety of downstream tasks, such as zero-shot transfer, text-to-image generation, and multimodal retrieval. The paper also explores the scaling behavior of FLIP by increasing the model size, data size, or training length, and reports encouraging results and comparisons.

## Main Contributions

[1]: https://arxiv.org/abs/2212.00794 "[2212.00794] Scaling Language-Image Pre-training via Masking - arXiv.org"
[2]: https://arxiv.org/pdf/2212.00794.pdf "arXiv:2212.00794v2 [cs.CV] 30 Mar 2023"
[3]: http://export.arxiv.org/abs/2212.00794 "[2212.00794] Scaling Language-Image Pre-training via Masking"

The paper claims the following contributions:

- **A simple and efficient method for CLIP training**: The paper introduces masking, a technique that randomly removes a large portion of image patches during training, as a way to improve the accuracy and speed of CLIP training. The paper shows that masking leads to a favorable trade-off between accuracy and training time, and that it can be easily implemented with minimal changes to the existing CLIP framework.
- **A comprehensive evaluation of FLIP on various tasks**: The paper evaluates FLIP on 400 million image-text pairs and compares it with the original CLIP method. The paper shows that FLIP outperforms CLIP on a large diversity of downstream tasks, such as zero-shot transfer, text-to-image generation, and multimodal retrieval. The paper also provides ablation studies and analysis to understand the effects of masking on different aspects of the model and the data.
- **A study of scaling behavior of FLIP**: The paper explores the scaling behavior of FLIP by increasing the model size, data size, or training length, and reports encouraging results and comparisons. The paper demonstrates that FLIP can benefit from scaling up along different dimensions, and that it can achieve state-of-the-art results on several benchmarks. The paper also discusses some challenges and limitations of scaling vision-language learning.

## Method Summary

[1]: https://arxiv.org/abs/2212.00794 "[2212.00794] Scaling Language-Image Pre-training via Masking - arXiv.org"
[2]: https://arxiv.org/pdf/2212.00794.pdf "arXiv:2212.00794v2 [cs.CV] 30 Mar 2023"
[3]: http://export.arxiv.org/abs/2212.00794 "[2212.00794] Scaling Language-Image Pre-training via Masking"

Here is a summary of the method section of the paper:

- **Masking**: The paper proposes to randomly mask out and remove a large portion of image patches during training. The paper argues that masking can improve the efficiency and effectiveness of CLIP training by reducing the redundancy and increasing the diversity of image-text pairs. The paper also discusses some implementation details and challenges of masking, such as how to handle different image resolutions, how to avoid trivial solutions, and how to balance the masking ratio and the batch size.
- **Contrastive learning**: The paper adopts the contrastive learning framework of CLIP, which learns a joint embedding space for images and texts by maximizing the agreement between matched image-text pairs and minimizing the agreement between unmatched pairs. The paper uses a temperature-scaled cosine similarity as the contrastive loss function, and applies a large-scale negative sampling strategy to increase the number of negative samples per iteration.
- **Model architecture**: The paper follows the model architecture of CLIP, which consists of a vision encoder and a text encoder. The paper uses Vision Transformer (ViT)  as the vision encoder, which splits an image into patches and processes them with a sequence of transformer layers. The paper uses BERT  as the text encoder, which encodes a text input with a transformer-based language model. The paper also explores different model sizes, such as ViT-B/16, ViT-L/16, ViT-H/14, BERT-Base, and BERT-Large.
- **Data preparation**: The paper uses LAION-400M  as the main data source for pre-training, which contains 400 million image-text pairs collected from various websites. The paper also uses additional data sources, such as Conceptual Captions , Open Images , and ImageNet-21K , to augment the training data. The paper applies some data preprocessing steps, such as resizing images, tokenizing texts, filtering out low-quality pairs, and balancing the data distribution.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the vision encoder (ViT) and the text encoder (BERT)
vision_encoder = ViT(...)
text_encoder = BERT(...)

# Define the contrastive loss function
def contrastive_loss(image_embeddings, text_embeddings):
  # Compute the cosine similarity matrix between image and text embeddings
  similarity_matrix = cosine_similarity(image_embeddings, text_embeddings)
  # Scale the similarity matrix by a temperature factor
  scaled_similarity_matrix = similarity_matrix / temperature
  # Compute the contrastive loss for each image-text pair
  loss = cross_entropy(scaled_similarity_matrix, labels)
  return loss

# Load the pre-training data (LAION-400M + other sources)
data_loader = DataLoader(...)

# Loop over the pre-training data for a number of epochs
for epoch in range(num_epochs):
  # Loop over the batches of image-text pairs
  for batch in data_loader:
    # Get the images and texts from the batch
    images, texts = batch
    # Apply masking to the images by randomly removing a portion of patches
    masked_images = mask(images)
    # Encode the masked images and texts with the vision and text encoders
    image_embeddings = vision_encoder(masked_images)
    text_embeddings = text_encoder(texts)
    # Compute the contrastive loss for the batch
    loss = contrastive_loss(image_embeddings, text_embeddings)
    # Update the model parameters with gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define some hyperparameters
num_epochs = 32 # number of epochs for pre-training
batch_size = 512 # batch size for pre-training
image_size = 224 # image size for resizing
patch_size = 16 # patch size for ViT
num_patches = (image_size // patch_size) ** 2 # number of patches per image
masking_ratio = 0.5 # ratio of patches to be masked out
temperature = 0.07 # temperature factor for scaling similarity matrix
learning_rate = 1e-4 # learning rate for optimizer

# Define the vision encoder (ViT) and the text encoder (BERT)
# Use the pre-defined classes from the transformers library
vision_encoder = transformers.ViTModel(
  image_size=image_size,
  patch_size=patch_size,
  num_hidden_layers=24, # ViT-L/16 has 24 transformer layers
  hidden_size=1024, # ViT-L/16 has hidden size of 1024
  num_attention_heads=16, # ViT-L/16 has 16 attention heads
  intermediate_size=4096, # ViT-L/16 has intermediate size of 4096
  dropout_rate=0.1, # dropout rate for regularization
)

text_encoder = transformers.BertModel(
  vocab_size=30522, # BERT-Base has vocab size of 30522
  hidden_size=768, # BERT-Base has hidden size of 768
  num_hidden_layers=12, # BERT-Base has 12 transformer layers
  num_attention_heads=12, # BERT-Base has 12 attention heads
  intermediate_size=3072, # BERT-Base has intermediate size of 3072
)

# Define the contrastive loss function
def contrastive_loss(image_embeddings, text_embeddings):
  # Compute the cosine similarity matrix between image and text embeddings
  similarity_matrix = torch.matmul(image_embeddings, text_embeddings.t())
  # Scale the similarity matrix by a temperature factor
  scaled_similarity_matrix = similarity_matrix / temperature
  # Compute the contrastive loss for each image-text pair
  labels = torch.arange(batch_size) # labels are the diagonal indices of the matrix
  loss = torch.nn.CrossEntropyLoss()(scaled_similarity_matrix, labels)
  return loss

# Load the pre-training data (LAION-400M + other sources)
# Use a custom data loader that returns batches of image-text pairs
data_loader = CustomDataLoader(...)

# Define the optimizer for updating the model parameters
# Use Adam with weight decay as the optimizer
optimizer = torch.optim.AdamW(
  params=list(vision_encoder.parameters()) + list(text_encoder.parameters()),
  lr=learning_rate,
)

# Define a function to apply masking to a batch of images
def mask(images):
  # Get the shape of the images (batch_size, channels, height, width)
  b, c, h, w = images.shape 
  # Compute the number of patches to be masked out per image
  num_masked_patches = int(num_patches * masking_ratio)
  # Generate random indices for masking out patches
  mask_indices = np.random.choice(num_patches, size=(b, num_masked_patches))
  # Convert the indices to row and column coordinates in the image grid
  mask_rows = mask_indices // (w // patch_size)
  mask_cols = mask_indices % (w // patch_size)
  # Loop over the batch and mask out the patches by setting them to zero
  for i in range(b):
    images[i, :, mask_rows[i] * patch_size : (mask_rows[i] + 1) * patch_size,
                mask_cols[i] * patch_size : (mask_cols[i] + 1) * patch_size] = 0.0 
  return images

# Loop over the pre-training data for a number of epochs
for epoch in range(num_epochs):
  
  # Loop over the batches of image-text pairs
  for batch in data_loader:
    
    # Get the images and texts from the batch
    images, texts = batch
    
    # Resize the images to a fixed size and normalize them 
    images = torchvision.transforms.Resize(image_size)(images)
    images = torchvision.transforms.Normalize(mean=[0.48145466,0.4578275 ,0.40821073], std=[0.26862954 ,0.26130258 ,0.27577711])(images)
    
    # Tokenize the texts and pad them to a fixed length
    texts = transformers.BertTokenizer(texts, padding=True, truncation=True, max_length=77)
    
    # Convert the images and texts to tensors and move them to the device (GPU or CPU)
    images = torch.tensor(images).to(device)
    texts = torch.tensor(texts).to(device)
    
    # Apply masking to the images by randomly removing a portion of patches
    masked_images = mask(images)
    
    # Encode the masked images and texts with the vision and text encoders
    image_embeddings = vision_encoder(masked_images).last_hidden_state[:, 0, :] # use the first token as the image embedding
    text_embeddings = text_encoder(texts).last_hidden_state[:, 0, :] # use the first token as the text embedding
    
    # Compute the contrastive loss for the batch
    loss = contrastive_loss(image_embeddings, text_embeddings)
    
    # Update the model parameters with gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```