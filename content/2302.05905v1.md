---
title: 2302.05905v1 Single Motion Diffusion
date: 2023-02-06
---

# [Single Motion Diffusion](http://arxiv.org/abs/2302.05905v1)

authors: Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H. Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/2302.05905v1.pdf "Single Motion Diffusion - arXiv.org"
[2]: https://arxiv.org/abs/2302.05905 "[2302.05905] Single Motion Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/2104.05905v1 "[2104.05905v1] Center-specific causal inference with multicenter trials ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a Single Motion Diffusion Model (SinMDM), a model that can learn the internal motifs of a single motion sequence with arbitrary topology and synthesize motions of arbitrary length that are faithful to them.
- **Why**: The paper aims to address the challenge of synthesizing realistic animations of humans, animals, and even imaginary creatures, which have limited or no available data for motion modeling. The paper also aims to provide a versatile model that can be applied in various contexts, such as in-betweening, motion expansion, style transfer, and crowd animation.
- **How**: The paper harnesses the power of diffusion models and presents a denoising network designed specifically for the task of learning from a single input motion. The paper uses a transformer-based architecture that avoids overfitting by using local attention layers that narrow the receptive field, and encourages motion diversity by using relative positional embedding. The paper evaluates the model on various datasets and applications and compares it with existing methods.

## Main Contributions

According to the paper, the main contributions are:

- A novel diffusion model that can learn from a single motion sequence with arbitrary topology and synthesize motions of arbitrary length that are faithful to the input sequence.
- A transformer-based denoising network that uses local attention and relative positional embedding to avoid overfitting and encourage diversity.
- A variety of applications that can be facilitated by the model at inference time, such as spatial and temporal in-betweening, motion expansion, style transfer, and crowd animation.
- An extensive evaluation of the model on various datasets and applications, showing that it outperforms existing methods both in quality and time-space efficiency.

## Method Summary

The method section of the paper consists of four subsections:

- **Diffusion models for motion**: The paper reviews the basics of diffusion models and how they can be applied to motion modeling. The paper also introduces the notion of motion motifs, which are the core patterns of a motion sequence that capture its essence and style.
- **Single Motion Diffusion Model**: The paper presents the proposed model, SinMDM, which consists of a denoising network and a diffusion process. The paper describes how the model learns from a single input motion sequence and generates motions of arbitrary length that are faithful to the input sequence.
- **Denoising network**: The paper details the architecture of the denoising network, which is based on a transformer with local attention and relative positional embedding. The paper explains how these components help to avoid overfitting and encourage diversity in the generated motions.
- **Applications**: The paper demonstrates how the model can be used for various applications at inference time, such as spatial and temporal in-betweening, motion expansion, style transfer, and crowd animation. The paper also discusses some implementation details and limitations of the model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a single motion sequence x of length T with arbitrary topology
# Output: a motion sequence y of length L that is faithful to x

# Define the diffusion process parameters
beta = [beta_1, ..., beta_T] # noise levels
alpha = [1 - beta_1, ..., 1 - beta_T] # reverse cumulative product of beta
sigma = [sqrt(beta_1), ..., sqrt(beta_T)] # standard deviations

# Define the denoising network parameters
d_model = 512 # hidden dimension
n_head = 8 # number of attention heads
n_layer = 6 # number of transformer layers
k = 16 # local attention window size
dropout = 0.1 # dropout rate

# Initialize the denoising network
network = Transformer(d_model, n_head, n_layer, k, dropout)

# Train the denoising network on the input motion sequence x
for t in range(1, T + 1):
  # Corrupt x with Gaussian noise
  epsilon_t = normal(0, sigma[t], size=x.shape)
  x_t = sqrt(alpha[t]) * x + epsilon_t
  
  # Predict the noiseless motion from the corrupted motion
  x_hat_t = network(x_t)
  
  # Compute the loss as the mean squared error
  loss_t = mse(x_hat_t, x)
  
  # Update the network parameters using gradient descent
  network.backward(loss_t)

# Generate a motion sequence y of length L from the trained network
y_0 = x[-1] # initialize y with the last frame of x
for t in range(1, L + 1):
  # Sample a noise level from the diffusion process
  beta_t = sample(beta)
  alpha_t = 1 - beta_t
  sigma_t = sqrt(beta_t)
  
  # Corrupt y with Gaussian noise
  epsilon_t = normal(0, sigma_t, size=y_0.shape)
  y_t = sqrt(alpha_t) * y_0 + epsilon_t
  
  # Predict the noiseless motion from the corrupted motion
  y_hat_t = network(y_t)
  
  # Update y with the predicted motion
  y_0 = y_hat_t
  
# Return the generated motion sequence y
return y_0
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define some constants
d_model = 512 # hidden dimension
n_head = 8 # number of attention heads
n_layer = 6 # number of transformer layers
k = 16 # local attention window size
dropout = 0.1 # dropout rate
lr = 0.0001 # learning rate
T = 100 # number of diffusion steps for training
L = 200 # number of diffusion steps for generation

# Define the noise levels for the diffusion process
beta = np.linspace(1e-4, 0.02, T) # linearly spaced noise levels
alpha = 1 - beta # reverse cumulative product of beta
sigma = np.sqrt(beta) # standard deviations

# Define the transformer layer with local attention and relative positional embedding
class TransformerLayer(nn.Module):
  def __init__(self, d_model, n_head, k, dropout):
    super(TransformerLayer, self).__init__()
    self.d_model = d_model # hidden dimension
    self.n_head = n_head # number of attention heads
    self.k = k # local attention window size
    self.dropout = dropout # dropout rate
    
    # Define the multi-head attention layer with local attention
    self.attention = nn.MultiheadAttention(d_model, n_head, dropout=dropout)
    
    # Define the feed-forward layer with two linear transformations and a ReLU activation
    self.ffn = nn.Sequential(
      nn.Linear(d_model, 4 * d_model),
      nn.ReLU(),
      nn.Linear(4 * d_model, d_model)
    )
    
    # Define the layer normalization layers
    self.ln1 = nn.LayerNorm(d_model)
    self.ln2 = nn.LayerNorm(d_model)
    
    # Define the relative positional embedding matrix
    self.pe = nn.Parameter(torch.randn(k * 2 + 1, d_model))
  
  def forward(self, x):
    # x: input tensor of shape (seq_len, batch_size, d_model)
    
    # Get the sequence length and the batch size
    seq_len, batch_size, _ = x.shape
    
    # Compute the attention mask to enforce local attention
    mask = torch.ones((seq_len, seq_len), device=x.device) # full mask of shape (seq_len, seq_len)
    mask = mask.triu(-self.k).tril(self.k) # upper and lower triangular mask of shape (seq_len, seq_len)
    mask = mask.masked_fill(mask == 0, float('-inf')) # replace zeros with negative infinity
    
    # Compute the relative positional embedding for each pair of positions
    pe = self.pe[self.k - torch.arange(seq_len).unsqueeze(0) + torch.arange(seq_len).unsqueeze(1)] # shape (seq_len, seq_len, d_model)
    
    # Add the relative positional embedding to the input tensor
    x = x + pe.transpose(0, 1) # shape (seq_len, batch_size, d_model)
    
    # Apply the multi-head attention layer with local attention and dropout
    x2, _ = self.attention(x, x, x, attn_mask=mask) # shape (seq_len, batch_size, d_model)
    x2 = F.dropout(x2, p=self.dropout, training=self.training) # apply dropout
    
    # Add and normalize the input and output tensors
    x = self.ln1(x + x2) # shape (seq_len, batch_size, d_model)
    
    # Apply the feed-forward layer and dropout
    x2 = self.ffn(x) # shape (seq_len, batch_size, d_model)
    x2 = F.dropout(x2, p=self.dropout, training=self.training) # apply dropout
    
    # Add and normalize the input and output tensors
    x = self.ln2(x + x2) # shape (seq_len, batch_size, d_model)
    
    return x

# Define the transformer model with multiple transformer layers
class Transformer(nn.Module):
  def __init__(self, d_model, n_head, n_layer, k, dropout):
    super(Transformer, self).__init__()
    
    # Define a list of transformer layers
    self.layers = nn.ModuleList([TransformerLayer(d_model, n_head, k ,dropout) for _ in range(n_layer)])
  
  def forward(self,x):
     # x: input tensor of shape (seq_len,batch_size,d_model)

     # Apply each transformer layer to the input tensor
     for layer in self.layers:
       x = layer(x) # shape (seq_len, batch_size, d_model)
     
     return x

# Initialize the denoising network
network = Transformer(d_model, n_head, n_layer, k, dropout)

# Define the optimizer
optimizer = optim.Adam(network.parameters(), lr=lr)

# Load the input motion sequence x of shape (seq_len, batch_size, d_model)
x = torch.load('input_motion.pt')

# Train the denoising network on the input motion sequence x
for epoch in range(1, 11): # train for 10 epochs
  print(f'Epoch {epoch}')
  for t in range(1, T + 1): # iterate over the diffusion steps
    # Corrupt x with Gaussian noise
    epsilon_t = torch.randn_like(x) * sigma[t] # shape (seq_len, batch_size, d_model)
    x_t = torch.sqrt(torch.tensor(alpha[t])) * x + epsilon_t # shape (seq_len, batch_size, d_model)
    
    # Predict the noiseless motion from the corrupted motion
    x_hat_t = network(x_t) # shape (seq_len, batch_size, d_model)
    
    # Compute the loss as the mean squared error
    loss_t = F.mse_loss(x_hat_t, x) # scalar
    
    # Print the loss
    print(f'Step {t}, Loss {loss_t.item():.4f}')
    
    # Update the network parameters using gradient descent
    optimizer.zero_grad() # reset the gradients
    loss_t.backward() # compute the gradients
    optimizer.step() # update the parameters

# Generate a motion sequence y of length L from the trained network
y_0 = x[-1] # initialize y with the last frame of x, shape (1, batch_size, d_model)
for t in range(1, L + 1): # iterate over the diffusion steps
  # Sample a noise level from the diffusion process
  beta_t = np.random.choice(beta) # scalar
  alpha_t = 1 - beta_t # scalar
  sigma_t = np.sqrt(beta_t) # scalar
  
  # Corrupt y with Gaussian noise
  epsilon_t = torch.randn_like(y_0) * sigma_t # shape (1, batch_size, d_model)
  y_t = torch.sqrt(torch.tensor(alpha_t)) * y_0 + epsilon_t # shape (1, batch_size, d_model)
  
  # Predict the noiseless motion from the corrupted motion
  y_hat_t = network(y_t) # shape (1, batch_size, d_model)
  
  # Update y with the predicted motion
  y_0 = y_hat_t # shape (1, batch_size, d_model)

# Return the generated motion sequence y
y = y_0.squeeze(0) # shape (batch_size, d_model)
return y

```