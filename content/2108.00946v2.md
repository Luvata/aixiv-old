---
title: 2108.00946v2 StyleGAN-NADA  CLIP-Guided Domain Adaptation of Image Generators
date: 2021-08-01
---

# [StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators](http://arxiv.org/abs/2108.00946v2)

authors: Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2108.00946 "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators"
[2]: https://arxiv.org/pdf/2107.00946v2 "1 Online Metro Origin-Destination Prediction via Heterogeneous ..."
[3]: http://export.arxiv.org/abs/1602.00946v2 "[1602.00946v2] The Potential in General Linear Electrodynamics: Causal ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- What: The paper proposes a method called **StyleGAN-NADA** that can adapt a generative model to new domains using only text prompts, without any images from the target domain.
- Why: The paper aims to address the challenge of domain adaptation for image generators, which usually requires collecting a large amount of images from the target domain. The paper leverages the semantic power of **CLIP**, a large-scale contrastive language-image pre-training model, to guide the adaptation process using natural language descriptions.
- How: The paper introduces a novel neural network module called **Heterogeneous Information Aggregation Machine (HIAM)**, which exploits heterogeneous information of historical data (e.g., incomplete OD matrices, unﬁnished order vectors, and DO matrices) to jointly learn the evolutionary patterns of OD and DO ridership. The paper also develops a unified Seq2Seq network that uses HIAM to forecast the future OD and DO ridership simultaneously. The paper demonstrates the effectiveness of the proposed method on a wide range of domains and shows that it can produce diverse and realistic images that match the text prompts.

## Main Contributions

According to the paper, the main contributions are:

- The paper presents the first method that can adapt a generative model to new domains using only text prompts, without any images from the target domain.
- The paper introduces a novel neural network module called HIAM that can fully exploit heterogeneous information of historical data to jointly learn the evolutionary patterns of OD and DO ridership.
- The paper develops a unified Seq2Seq network that uses HIAM to forecast the future OD and DO ridership simultaneously.
- The paper conducts extensive experiments and comparisons across a wide range of domains and shows that the proposed method can produce diverse and realistic images that match the text prompts. The paper also shows that the adapted models maintain the latent-space properties that make generative models appealing for downstream tasks.


## Method Summary

[1]: https://arxiv.org/abs/2108.00946 "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators"
[2]: https://arxiv.org/pdf/2104.00946v2.pdf "arXiv:2104.00946v2 [cs.CV] 12 Apr 2021"
[3]: http://export.arxiv.org/abs/1602.00946v2 "[1602.00946v2] The Potential in General Linear Electrodynamics: Causal ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper adopts a pre-trained **StyleGAN2** model as the base generator and uses a **CLIP** model as a semantic guidance for domain adaptation. The paper assumes that the generator can be decomposed into two parts: a mapping network that maps a latent code to an intermediate latent code, and a synthesis network that maps the intermediate latent code to an image.
- The paper proposes to adapt the generator by fine-tuning only the mapping network with a novel loss function that consists of three terms: a **CLIP loss**, a **latent regression loss**, and an **identity loss**.
- The CLIP loss measures the similarity between the generated image and the text prompt using the CLIP model, which is trained to align natural language and images in a large-scale dataset. The CLIP loss encourages the generator to produce images that match the semantic meaning of the text prompt.
- The latent regression loss measures the distance between the intermediate latent code and the output of the mapping network. The latent regression loss encourages the mapping network to preserve the original distribution of the intermediate latent codes, which is important for maintaining the latent-space properties of the generator.
- The identity loss measures the distance between the original image and the generated image when the text prompt is empty. The identity loss encourages the generator to produce images that are close to the original domain when no adaptation is required.
- The paper also introduces a novel neural network module called **Heterogeneous Information Aggregation Machine (HIAM)**, which exploits heterogeneous information of historical data (e.g., incomplete OD matrices, unﬁnished order vectors, and DO matrices) to jointly learn the evolutionary patterns of OD and DO ridership. The paper also develops a unified Seq2Seq network that uses HIAM to forecast the future OD and DO ridership simultaneously.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained StyleGAN2 generator G, a CLIP model C, a text prompt t, a set of historical data H
# Output: an adapted generator G'
# Hyperparameters: learning rate lr, number of iterations n

# Initialize the mapping network M of G as M_0
M = M_0

# Define the loss function L as a combination of CLIP loss, latent regression loss, and identity loss
def L(M, t):
  # Sample a latent code z from a standard normal distribution
  z = sample_normal()
  # Obtain the intermediate latent code w from M
  w = M(z)
  # Obtain the image x from the synthesis network S of G
  x = S(w)
  # Obtain the original image x_0 from G
  x_0 = G(z)
  # Compute the CLIP loss as the negative cosine similarity between x and t using C
  clip_loss = -cosine_similarity(C(x), C(t))
  # Compute the latent regression loss as the L2 distance between w and w_0
  latent_loss = L2_distance(w, w_0)
  # Compute the identity loss as the L2 distance between x and x_0 when t is empty
  identity_loss = L2_distance(x, x_0) if t == "" else 0
  # Return the weighted sum of the three losses
  return alpha * clip_loss + beta * latent_loss + gamma * identity_loss

# Define the HIAM module that aggregates heterogeneous information of historical data to learn OD and DO patterns
def HIAM(H):
  # Extract incomplete OD matrices, unﬁnished order vectors, and DO matrices from H
  OD_incomplete, order_vector, DO = extract(H)
  # Use an OD modeling branch to estimate the potential destinations of unﬁnished orders and complete the OD matrices
  OD_complete = OD_modeling(OD_incomplete, order_vector)
  # Use a DO modeling branch to capture the spatial-temporal distribution of DO ridership
  DO_feature = DO_modeling(DO)
  # Use a Dual Information Transformer to propagate the mutual information among OD features and DO features
  OD_feature, DO_feature = Dual_Transformer(OD_complete, DO_feature)
  # Return the aggregated features for OD and DO ridership
  return OD_feature, DO_feature

# Define the Seq2Seq network that uses HIAM to forecast the future OD and DO ridership
def Seq2Seq(H):
  # Use an encoder to encode the historical data H into a hidden state h
  h = encoder(H)
  # Use HIAM to obtain the aggregated features for OD and DO ridership
  OD_feature, DO_feature = HIAM(H)
  # Concatenate h with OD_feature and DO_feature to form a context vector c
  c = concatenate(h, OD_feature, DO_feature)
  # Use a decoder to generate the future OD and DO matrices based on c
  OD_future, DO_future = decoder(c)
  # Return the predicted matrices for OD and DO ridership
  return OD_future, DO_future

# Fine-tune the mapping network M using gradient descent for n iterations
for i in range(n):
  # Compute the gradient of the loss function L with respect to M
  grad = gradient(L(M, t), M)
  # Update M using gradient descent with learning rate lr
  M = M - lr * grad

# Return the adapted generator G' with the updated mapping network M
G' = G(M)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing
import clip # for CLIP model
import stylegan2 # for StyleGAN2 model
import transformers # for Dual Information Transformer

# Input: a pre-trained StyleGAN2 generator G, a CLIP model C, a text prompt t, a set of historical data H
# Output: an adapted generator G'
# Hyperparameters: learning rate lr, number of iterations n, batch size b, weight coefficients alpha, beta, gamma

# Initialize the mapping network M of G as M_0 and move it to the device (GPU or CPU)
M = M_0.to(device)

# Define the loss function L as a combination of CLIP loss, latent regression loss, and identity loss
def L(M, t):
  # Initialize the loss as zero
  loss = 0
  # Loop over the batch size b
  for i in range(b):
    # Sample a latent code z from a standard normal distribution and move it to the device
    z = torch.randn(1, 512).to(device)
    # Obtain the intermediate latent code w from M
    w = M(z)
    # Obtain the image x from the synthesis network S of G
    x = G.synthesis(w)
    # Obtain the original image x_0 from G
    x_0 = G(z)
    # Preprocess x and x_0 using torchvision transforms (resize, center crop, normalize, etc.)
    x = preprocess(x)
    x_0 = preprocess(x_0)
    # Encode x and t using C and obtain the image and text embeddings
    image_embed = C.encode_image(x)
    text_embed = C.encode_text(t)
    # Compute the CLIP loss as the negative cosine similarity between image_embed and text_embed
    clip_loss = -torch.cosine_similarity(image_embed, text_embed)
    # Compute the latent regression loss as the L2 distance between w and w_0
    latent_loss = torch.dist(w, w_0)
    # Compute the identity loss as the L2 distance between x and x_0 when t is empty
    identity_loss = torch.dist(x, x_0) if t == "" else 0
    # Add the weighted sum of the three losses to the total loss
    loss += alpha * clip_loss + beta * latent_loss + gamma * identity_loss
  # Return the average loss over the batch size
  return loss / b

# Define the HIAM module that aggregates heterogeneous information of historical data to learn OD and DO patterns
def HIAM(H):
  # Extract incomplete OD matrices, unﬁnished order vectors, and DO matrices from H using torch tensors and move them to the device
  OD_incomplete = torch.tensor(H["OD_incomplete"]).to(device)
  order_vector = torch.tensor(H["order_vector"]).to(device)
  DO = torch.tensor(H["DO"]).to(device)
  # Use an OD modeling branch to estimate the potential destinations of unﬁnished orders and complete the OD matrices
  # The OD modeling branch consists of a linear layer followed by a softmax layer
  OD_modeling = torch.nn.Sequential(
    torch.nn.Linear(512, 512),
    torch.nn.Softmax(dim=1)
  ).to(device)
  OD_complete = OD_modeling(OD_incomplete + order_vector)
  # Use a DO modeling branch to capture the spatial-temporal distribution of DO ridership
  # The DO modeling branch consists of a convolutional layer followed by a batch normalization layer and a ReLU activation layer
  DO_modeling = torch.nn.Sequential(
    torch.nn.Conv2d(1, 64, kernel_size=3, padding=1),
    torch.nn.BatchNorm2d(64),
    torch.nn.ReLU()
  ).to(device)
  DO_feature = DO_modeling(DO.unsqueeze(1))
  # Use a Dual Information Transformer to propagate the mutual information among OD features and DO features
  # The Dual Information Transformer consists of two transformer encoders that share parameters and exchange outputs
  Dual_Transformer = transformers.TransformerEncoder(
    transformers.TransformerEncoderLayer(512, 8),
    num_layers=6,
    norm=torch.nn.LayerNorm(512)
  ).to(device)
  OD_feature = Dual_Transformer(OD_complete + DO_feature.permute(0,2,3,1).reshape(b,-1,512))
  DO_feature = Dual_Transformer(DO_feature.permute(0,2,3,1).reshape(b,-1,512) + OD_complete)
  # Return the aggregated features for OD and DO ridership
  return OD_feature, DO_feature

# Define the Seq2Seq network that uses HIAM to forecast the future OD and DO ridership
def Seq2Seq(H):
  # Use an encoder to encode the historical data H into a hidden state h
  # The encoder consists of a GRU layer
  encoder = torch.nn.GRU(512, 512).to(device)
  h = encoder(H)
  # Use HIAM to obtain the aggregated features for OD and DO ridership
  OD_feature, DO_feature = HIAM(H)
  # Concatenate h with OD_feature and DO_feature to form a context vector c
  c = torch.cat([h, OD_feature, DO_feature], dim=1)
  # Use a decoder to generate the future OD and DO matrices based on c
  # The decoder consists of a GRU layer followed by a linear layer
  decoder = torch.nn.Sequential(
    torch.nn.GRU(512, 512),
    torch.nn.Linear(512, 512)
  ).to(device)
  OD_future, DO_future = decoder(c)
  # Return the predicted matrices for OD and DO ridership
  return OD_future, DO_future

# Fine-tune the mapping network M using gradient descent for n iterations
# Use an Adam optimizer with learning rate lr
optimizer = torch.optim.Adam(M.parameters(), lr=lr)

for i in range(n):
  # Compute the loss function L with respect to M
  loss = L(M, t)
  # Compute the gradient of the loss function with respect to M
  loss.backward()
  # Update M using the optimizer
  optimizer.step()
  # Print the loss value every 100 iterations
  if i % 100 == 0:
    print(f"Iteration {i}, Loss {loss.item()}")

# Return the adapted generator G' with the updated mapping network M
G' = G(M)
```