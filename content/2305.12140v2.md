---
title: 2305.12140v2 Movie101  A New Movie Understanding Benchmark
date: 2023-05-13
---

# [Movie101: A New Movie Understanding Benchmark](http://arxiv.org/abs/2305.12140v2)

authors: Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, Qin Jin


## What, Why and How

[1]: https://arxiv.org/pdf/2305.12140v2 "Movie101: A New Movie Understanding Benchmark - arXiv.org"
[2]: https://arxiv.org/abs/2305.12140 "[2305.12140] Movie101: A New Movie Understanding Benchmark - arXiv.org"
[3]: https://arxiv.org/pdf/2007.12140v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: The paper introduces a new movie understanding benchmark, named Movie101, which supports two tasks: Movie Clip Narrating (MCN) and Temporal Narration Grounding (TNG). The paper also proposes a new metric called Movie Narration Score (MNScore) for movie narrating evaluation.

Why: The paper aims to help the visually impaired enjoy movies by generating accurate, coherent, and role-aware narrations for movie clips where no actors are speaking. The paper also aims to narrow the gap between existing movie narrating datasets and real application scenarios.

How: The paper constructs a large-scale Chinese movie benchmark, Movie101, by collecting 101 movies with ADs and aligning them with movie clips. The paper also provides external knowledge, such as role information and movie genres, for better movie understanding. The paper proposes a new metric, MNScore, which combines ngram-based metrics and role-aware metrics to evaluate the quality of movie narrations. The paper also proposes methods for both MCN and TNG tasks, which leverage external knowledge and outperform baselines.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2305.12140v2 "Movie101: A New Movie Understanding Benchmark - arXiv.org"
[2]: https://arxiv.org/abs/2305.12140 "[2305.12140] Movie101: A New Movie Understanding Benchmark - arXiv.org"
[3]: https://arxiv.org/pdf/2007.12140v2.pdf "arXiv.org e-Print archive"

The paper claims to make the following contributions:

- It constructs a large-scale Chinese movie benchmark, Movie101, which contains 101 movies with ADs and movie clips, as well as external knowledge such as role information and movie genres.
- It proposes a new task, Movie Clip Narrating (MCN), which asks models to generate role-aware narration paragraphs for complete movie clips where no actors are speaking, closer to real scenarios.
- It proposes a new metric, Movie Narration Score (MNScore), which combines ngram-based metrics and role-aware metrics to evaluate the quality of movie narrations, and achieves the best correlation with human evaluation.
- It proposes methods for both MCN and TNG tasks, which leverage external knowledge and outperform carefully designed baselines.

## Method Summary

[1]: https://arxiv.org/pdf/2305.12140v2 "Movie101: A New Movie Understanding Benchmark - arXiv.org"
[2]: https://arxiv.org/abs/2305.12140 "[2305.12140] Movie101: A New Movie Understanding Benchmark - arXiv.org"
[3]: https://arxiv.org/pdf/2007.12140v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

```
The paper proposes methods for both MCN and TNG tasks, which leverage external knowledge and outperform baselines.

For the MCN task, the paper proposes a Knowledge-Enhanced Video Transformer (KEVT) model, which consists of three modules: a video encoder, a knowledge encoder, and a text decoder. The video encoder extracts visual features from movie clips using a 3D CNN and a transformer. The knowledge encoder encodes role information and movie genres using an embedding layer and a transformer. The text decoder generates narration paragraphs using a transformer with cross-attention to both video and knowledge encoders. The paper also proposes a multi-task learning framework to jointly optimize the MCN task and the role recognition task, which aims to identify the roles appearing in each movie clip.

For the TNG task, the paper proposes a Knowledge-Enhanced Clip Localizer (KECL) model, which consists of four modules: a video encoder, a text encoder, a knowledge encoder, and a clip localizer. The video encoder extracts visual features from movie clips using a 3D CNN and a transformer. The text encoder encodes text descriptions using an embedding layer and a transformer. The knowledge encoder encodes role information and movie genres using an embedding layer and a transformer. The clip localizer predicts the start and end timestamps of the target clip using a transformer with cross-attention to all three encoders.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2305.12140v2 "Movie101: A New Movie Understanding Benchmark - arXiv.org"
[2]: https://arxiv.org/abs/2305.12140 "[2305.12140] Movie101: A New Movie Understanding Benchmark - arXiv.org"
[3]: https://arxiv.org/pdf/2007.12140v2.pdf "arXiv.org e-Print archive"

Here is a high-level pseudo code for the paper:

```
# Define the KEVT model for the MCN task
class KEVT():
  def __init__():
    # Initialize the video encoder, knowledge encoder, and text decoder
    self.video_encoder = VideoEncoder()
    self.knowledge_encoder = KnowledgeEncoder()
    self.text_decoder = TextDecoder()

  def forward(movie_clip, role_info, movie_genre):
    # Extract visual features from movie clip using video encoder
    video_features = self.video_encoder(movie_clip)
    # Encode role information and movie genre using knowledge encoder
    knowledge_features = self.knowledge_encoder(role_info, movie_genre)
    # Generate narration paragraph using text decoder with cross-attention
    narration = self.text_decoder(video_features, knowledge_features)
    return narration

# Define the KECL model for the TNG task
class KECL():
  def __init__():
    # Initialize the video encoder, text encoder, knowledge encoder, and clip localizer
    self.video_encoder = VideoEncoder()
    self.text_encoder = TextEncoder()
    self.knowledge_encoder = KnowledgeEncoder()
    self.clip_localizer = ClipLocalizer()

  def forward(movie_clip, text_description, role_info, movie_genre):
    # Extract visual features from movie clip using video encoder
    video_features = self.video_encoder(movie_clip)
    # Encode text description using text encoder
    text_features = self.text_encoder(text_description)
    # Encode role information and movie genre using knowledge encoder
    knowledge_features = self.knowledge_encoder(role_info, movie_genre)
    # Predict start and end timestamps of target clip using clip localizer with cross-attention
    start, end = self.clip_localizer(video_features, text_features, knowledge_features)
    return start, end
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2305.12140v2 "Movie101: A New Movie Understanding Benchmark - arXiv.org"
[2]: https://arxiv.org/abs/2305.12140 "[2305.12140] Movie101: A New Movie Understanding Benchmark - arXiv.org"
[3]: https://arxiv.org/pdf/2007.12140v2.pdf "arXiv.org e-Print archive"

Here is a detailed pseudo code for the paper:

```
# Define the VideoEncoder module
class VideoEncoder():
  def __init__():
    # Initialize the 3D CNN and the transformer
    self.cnn = CNN3D()
    self.transformer = Transformer()

  def forward(movie_clip):
    # Extract visual features from movie clip using 3D CNN
    cnn_features = self.cnn(movie_clip)
    # Apply positional encoding to cnn_features
    cnn_features = positional_encoding(cnn_features)
    # Encode visual features using transformer
    video_features = self.transformer(cnn_features)
    return video_features

# Define the KnowledgeEncoder module
class KnowledgeEncoder():
  def __init__():
    # Initialize the embedding layer and the transformer
    self.embedding = Embedding()
    self.transformer = Transformer()

  def forward(role_info, movie_genre):
    # Concatenate role information and movie genre
    knowledge_input = concatenate(role_info, movie_genre)
    # Embed knowledge input using embedding layer
    knowledge_embedded = self.embedding(knowledge_input)
    # Apply positional encoding to knowledge_embedded
    knowledge_embedded = positional_encoding(knowledge_embedded)
    # Encode knowledge input using transformer
    knowledge_features = self.transformer(knowledge_embedded)
    return knowledge_features

# Define the TextDecoder module
class TextDecoder():
  def __init__():
    # Initialize the embedding layer and the transformer
    self.embedding = Embedding()
    self.transformer = Transformer()

  def forward(video_features, knowledge_features):
    # Initialize an empty narration paragraph
    narration = []
    # Initialize a start token
    token = "<START>"
    # Loop until end token or max length is reached
    while token != "<END>" and len(narration) < max_length:
      # Embed token using embedding layer
      token_embedded = self.embedding(token)
      # Apply positional encoding to token_embedded
      token_embedded = positional_encoding(token_embedded)
      # Decode token using transformer with cross-attention to video and knowledge features
      token_output = self.transformer(token_embedded, video_features, knowledge_features)
      # Get the most probable next token from token_output
      token = argmax(token_output)
      # Append token to narration paragraph
      narration.append(token)
    return narration

# Define the TextEncoder module
class TextEncoder():
  def __init__():
    # Initialize the embedding layer and the transformer
    self.embedding = Embedding()
    self.transformer = Transformer()

  def forward(text_description):
    # Embed text description using embedding layer
    text_embedded = self.embedding(text_description)
    # Apply positional encoding to text_embedded
    text_embedded = positional_encoding(text_embedded)
    # Encode text description using transformer
    text_features = self.transformer(text_embedded)
    return text_features

# Define the ClipLocalizer module
class ClipLocalizer():
  def __init__():
    # Initialize the transformer and the linear layer
    self.transformer = Transformer()
    self.linear = Linear()

  def forward(video_features, text_features, knowledge_features):
    # Concatenate video features, text features, and knowledge features along dimension 1
    localizer_input = concatenate(video_features, text_features, knowledge_features, dim=1)
    # Localize clip using transformer with cross-attention to all three features
    localizer_output = self.transformer(localizer_input)
    # Predict start and end timestamps using linear layer with sigmoid activation
    start, end = sigmoid(self.linear(localizer_output))
    return start, end

# Define the KEVT model for the MCN task
class KEVT():
  def __init__():
    # Initialize the video encoder, knowledge encoder, and text decoder
    self.video_encoder = VideoEncoder()
    self.knowledge_encoder = KnowledgeEncoder()
    self.text_decoder = TextDecoder()

  def forward(movie_clip, role_info, movie_genre):
     # Extract visual features from movie clip using video encoder
     video_features = self.video_encoder(movie_clip)
     # Encode role information and movie genre using knowledge encoder
     knowledge_features = self.knowledge_encoder(role_info, movie_genre)
     # Generate narration paragraph using text decoder with cross-attention 
     narration = self.text_decoder(video_features, knowledge_features)
     return narration

# Define the KECL model for the TNG task 
class KECL():
  def __init__():
     # Initialize the video encoder, text encoder, knowledge encoder, and clip localizer 
     self.video_encoder = VideoEncoder()
     self.text_encoder = TextEncoder()
     self.knowledge_encoder = KnowledgeEncoder()
     self.clip_localizer = ClipLocalizer()

  def forward(movie_clip, text_description, role_info, movie_genre):
     # Extract visual features from movie clip using video encoder
     video_features = self.video_encoder(movie_clip)
     # Encode text description using text encoder
     text_features = self.text_encoder(text_description)
     # Encode role information and movie genre using knowledge encoder
     knowledge_features = self.knowledge_encoder(role_info, movie_genre)
     # Predict start and end timestamps of target clip using clip localizer with cross-attention
     start, end = self.clip_localizer(video_features, text_features, knowledge_features)
     return start, end
```