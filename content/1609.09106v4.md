---
title: 1609.09106v4 HyperNetworks
date: 2016-09-10
---

# [HyperNetworks](http://arxiv.org/abs/1609.09106v4)

authors: David Ha, Andrew Dai, Quoc V. Le


## What, Why and How

[1]: https://arxiv.org/abs/1609.09106 "[1609.09106] HyperNetworks - arXiv.org"
[2]: https://arxiv.org/pdf/1609.09106v4.pdf "H YPER N ETWORKS - arXiv.org"
[3]: https://www.researchgate.net/publication/308744285_HyperNetworks "(PDF) HyperNetworks - ResearchGate"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper introduces **hypernetworks**, an approach of using a small network (called a hypernetwork) to generate the weights for a larger network (called a main network).
- **Why**: The paper aims to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as a relaxed form of weight-sharing across layers. The paper also claims that hypernetworks can provide an abstraction that is similar to what is found in nature: the relationship between a genotype (the hypernetwork) and a phenotype (the main network).
- **How**: The paper proposes different ways of generating the input embeddings for the hypernetwork, which can be fixed parameters or dynamically generated by another network. The paper also explores how hypernetworks can mix well with other techniques such as batch normalization and layer normalization. The paper evaluates hypernetworks on a variety of tasks such as character-level language modelling, handwriting generation and neural machine translation, and compares them with state-of-the-art baseline models.

## Main Contributions

[1]: https://arxiv.org/abs/1609.09106 "[1609.09106] HyperNetworks - arXiv.org"
[2]: https://arxiv.org/pdf/1609.09106v4.pdf "H YPER N ETWORKS - arXiv.org"
[3]: https://www.researchgate.net/publication/308744285_HyperNetworks "(PDF) HyperNetworks - ResearchGate"

According to the paper, the main contributions are:

- **Introducing hypernetworks**, an approach of using a small network to generate the weights for a larger network, and showing that they can provide an abstraction that is similar to what is found in nature: the relationship between a genotype and a phenotype[^1^][1].
- **Proposing different ways of generating input embeddings for the hypernetwork**, which can be fixed parameters or dynamically generated by another network, and showing that they can allow approximate weight-sharing within a layer and across layers of the main network[^1^][1] [^2^][2].
- **Evaluating hypernetworks on a variety of tasks** such as character-level language modelling, handwriting generation and neural machine translation, and showing that they can achieve near state-of-the-art results while requiring fewer learnable parameters[^1^][1] [^2^][2].
- **Challenging the weight-sharing paradigm for recurrent networks** by showing that hypernetworks can generate non-shared weights for LSTM that work better than the standard version of LSTM[^1^][1] [^2^][2].
- **Exploring how hypernetworks can mix well with other techniques** such as batch normalization and layer normalization, and showing that they can improve the performance and stability of the main network[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/abs/1609.09106 "[1609.09106] HyperNetworks - arXiv.org"
[2]: https://arxiv.org/pdf/1609.09106v4.pdf "H YPER N ETWORKS - arXiv.org"
[3]: https://www.researchgate.net/publication/308744285_HyperNetworks "(PDF) HyperNetworks - ResearchGate"

Here is a summary of the method section of the paper:

- The paper describes how to construct a **hypernetwork** that takes an input embedding vector and generates the weights for a layer of a **main network**. The hypernetwork can be a feedforward network or a recurrent network, depending on the task and the structure of the main network[^1^][2].
- The paper proposes different ways of generating the input embedding vector for the hypernetwork, which can be either **static** or **dynamic**. Static embeddings are fixed parameters that are learned during end-to-end training, and can allow approximate weight-sharing within a layer and across layers of the main network. Dynamic embeddings are generated by another network that takes some additional inputs, such as the timestep or the input sequence, and can allow the weights of a recurrent network to change over time and adapt to the input sequence[^1^][2].
- The paper also explores how hypernetworks can mix well with other techniques such as **batch normalization** and **layer normalization**, and shows that they can improve the performance and stability of the main network. Batch normalization normalizes the inputs of each layer using statistics computed over mini-batches, whereas layer normalization normalizes the inputs of each layer using statistics computed over all hidden units in a layer[^1^][2].
- The paper evaluates hypernetworks on a variety of tasks such as **character-level language modelling**, **handwriting generation** and **neural machine translation**, and compares them with state-of-the-art baseline models. The paper shows that hypernetworks can generate non-shared weights for LSTM that work better than the standard version of LSTM, and that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks while requiring fewer learnable parameters[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the main network and the hypernetwork
main_network = some_network()
hypernetwork = some_network()

# Define the input embeddings for the hypernetwork
if static_embeddings:
  input_embeddings = learnable_parameters()
elif dynamic_embeddings:
  input_embeddings = another_network(inputs)

# Generate the weights for the main network using the hypernetwork
weights = hypernetwork(input_embeddings)

# Apply batch normalization or layer normalization to the weights if needed
if batch_normalization:
  weights = batch_norm(weights)
elif layer_normalization:
  weights = layer_norm(weights)

# Train the main network and the hypernetwork end-to-end using backpropagation
loss = some_loss_function(main_network(inputs, weights), targets)
loss.backward()
optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import some libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the main network and the hypernetwork
# For simplicity, we assume the main network is a feedforward network with one hidden layer
# and the hypernetwork is a feedforward network with one hidden layer
class MainNetwork(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(MainNetwork, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size

  def forward(self, x, weights):
    # Unpack the weights generated by the hypernetwork
    w1, b1, w2, b2 = weights

    # Apply the first linear layer with activation function
    x = F.relu(F.linear(x, w1, b1))

    # Apply the second linear layer with softmax function
    x = F.softmax(F.linear(x, w2, b2), dim=-1)

    return x

class HyperNetwork(nn.Module):
  def __init__(self, embedding_size, hidden_size):
    super(HyperNetwork, self).__init__()
    self.embedding_size = embedding_size
    self.hidden_size = hidden_size

    # Define the linear layers for the hypernetwork
    self.fc1 = nn.Linear(embedding_size, hidden_size)
    self.fc2 = nn.Linear(hidden_size, hidden_size)

  def forward(self, input_embeddings):
    # Apply the first linear layer with activation function
    x = F.relu(self.fc1(input_embeddings))

    # Apply the second linear layer with activation function
    x = F.relu(self.fc2(x))

    # Reshape the output to match the shape of the weights for the main network
    x = x.view(-1)

    # Split the output into four parts: w1, b1, w2, b2
    w1 = x[:input_size * hidden_size].view(input_size, hidden_size)
    b1 = x[input_size * hidden_size:input_size * hidden_size + hidden_size].view(hidden_size)
    w2 = x[input_size * hidden_size + hidden_size:input_size * hidden_size + hidden_size + hidden_size * output_size].view(hidden_size, output_size)
    b2 = x[input_size * hidden_size + hidden_size + hidden_size * output_size:].view(output_size)

    return w1, b1, w2, b2

# Define the input embeddings for the hypernetwork
# For simplicity, we assume static embeddings that are learned during end-to-end training
input_embeddings = nn.Parameter(torch.randn(embedding_size))

# Define batch normalization or layer normalization if needed
if batch_normalization:
  bn = nn.BatchNorm1d(embedding_size)
elif layer_normalization:
  ln = nn.LayerNorm(embedding_size)

# Instantiate the main network and the hypernetwork
main_network = MainNetwork(input_size, hidden_size, output_size)
hypernetwork = HyperNetwork(embedding_size, hidden_size)

# Define the loss function and the optimizer
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam([input_embeddings] + list(main_network.parameters()) + list(hypernetwork.parameters()))

# Train the main network and the hypernetwork end-to-end using backpropagation
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the inputs and targets from the batch
    inputs, targets = batch

    # Generate the weights for the main network using the hypernetwork
    weights = hypernetwork(input_embeddings)

    # Apply batch normalization or layer normalization to the weights if needed
    if batch_normalization:
      weights = bn(weights)
    elif layer_normalization:
      weights = ln(weights)

    # Get the outputs from the main network using the generated weights
    outputs = main_network(inputs, weights)

    # Compute the loss using the outputs and targets
    loss = loss_function(outputs, targets)

    # Backpropagate the loss and update the parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```