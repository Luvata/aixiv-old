---
title: 2306.04632v1 Designing a Better Asymmetric VQGAN for StableDiffusion
date: 2023-06-05
---

# [Designing a Better Asymmetric VQGAN for StableDiffusion](http://arxiv.org/abs/2306.04632v1)

authors: Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, Gang Hua


## What, Why and How

[1]: https://arxiv.org/pdf/2306.04632v1.pdf "arXiv:2306.04632v1 [cs.CV] 7 Jun 2023"
[2]: https://arxiv.org/abs/2306.04632 "Designing a Better Asymmetric VQGAN for StableDiffusion"
[3]: http://export.arxiv.org/abs/2306.04632 "[2306.04632] Designing a Better Asymmetric VQGAN for StableDiffusion"

Here is a summary of the paper:

- **What**: The paper proposes a new asymmetric VQGAN for StableDiffusion, a text-to-image generator that uses a diffusion model in the latent space via a VQGAN.
- **Why**: The paper aims to address the problem of information loss and distortion artifacts caused by the vanilla VQGAN used in StableDiffusion, especially for image editing tasks such as inpainting and local editing.
- **How**: The paper introduces two simple designs for the asymmetric VQGAN: (1) a conditional branch in the decoder that incorporates information from task-specific priors, such as the unmasked image region in inpainting; and (2) a heavier decoder than the encoder, allowing for more detailed recovery while only slightly increasing the total inference cost. The paper shows that the asymmetric VQGAN can significantly improve the inpainting and editing performance, while maintaining the original text-to-image capability.


## Main Contributions

[1]: https://arxiv.org/pdf/2306.04632v1.pdf "arXiv:2306.04632v1 [cs.CV] 7 Jun 2023"
[2]: https://arxiv.org/abs/2306.04632 "Designing a Better Asymmetric VQGAN for StableDiffusion"
[3]: http://export.arxiv.org/abs/2306.04632 "[2306.04632] Designing a Better Asymmetric VQGAN for StableDiffusion"

According to the paper[^1^][1], the main contributions are:

- They propose a new asymmetric VQGAN with a conditional branch in the decoder and a heavier decoder than the encoder, which can reduce information loss and distortion artifacts in StableDiffusion-based image editing tasks.
- They show that the training cost of their asymmetric VQGAN is cheap, and they only need to retrain a new asymmetric decoder while keeping the vanilla VQGAN encoder and StableDiffusion unchanged.
- They demonstrate that their asymmetric VQGAN can significantly improve the inpainting and editing performance, while maintaining the original text-to-image capability of StableDiffusion.
- They provide code and models for their asymmetric VQGAN and StableDiffusion-based methods at https://github.com/buxiangzhiren/Asymmetric_VQGAN.


## Method Summary

[1]: https://arxiv.org/pdf/2306.04632v1.pdf "arXiv:2306.04632v1 [cs.CV] 7 Jun 2023"
[2]: https://arxiv.org/abs/2306.04632 "Designing a Better Asymmetric VQGAN for StableDiffusion"
[3]: http://export.arxiv.org/abs/2306.04632 "[2306.04632] Designing a Better Asymmetric VQGAN for StableDiffusion"

Here is a summary of the method section of the paper:

- The paper first reviews the background of StableDiffusion, a text-to-image generator that uses a diffusion model in the latent space via a VQGAN. The paper identifies two limitations of the vanilla VQGAN used in StableDiffusion: (1) it causes information loss and distortion artifacts in image editing tasks; and (2) it does not leverage task-specific priors, such as the unmasked image region in inpainting.
- The paper then proposes a new asymmetric VQGAN with two simple designs: (1) a conditional branch in the decoder that incorporates information from task-specific priors; and (2) a heavier decoder than the encoder, allowing for more detailed recovery while only slightly increasing the total inference cost. The paper explains the motivation and implementation details of each design, and shows how they can improve the inpainting and editing performance.
- The paper also describes how to train the asymmetric VQGAN with minimal cost. The paper only needs to retrain a new asymmetric decoder while keeping the vanilla VQGAN encoder and StableDiffusion unchanged. The paper provides the training details and hyperparameters for the asymmetric decoder.
- The paper finally shows how to use the asymmetric VQGAN in StableDiffusion-based inpainting and local editing methods. The paper illustrates how to modify the existing methods to incorporate the conditional branch and the heavier decoder of the asymmetric VQGAN. The paper also provides code and models for their methods at https://github.com/buxiangzhiren/Asymmetric_VQGAN.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the asymmetric VQGAN with a conditional branch and a heavier decoder
class AsymmetricVQGAN(nn.Module):
  def __init__(self):
    # Initialize the encoder and the decoder
    self.encoder = VanillaVQGANEncoder()
    self.decoder = AsymmetricVQGANDecoder()

  def forward(self, x, prior=None):
    # Encode the input image x to a latent code z
    z = self.encoder(x)
    # Decode the latent code z to a reconstructed image x_hat
    # If prior is given, use it as a conditional input to the decoder
    x_hat = self.decoder(z, prior)
    return x_hat

# Define the asymmetric VQGAN decoder with a conditional branch and a heavier structure
class AsymmetricVQGANDecoder(nn.Module):
  def __init__(self):
    # Initialize the decoder layers
    self.layers = nn.Sequential(
      # A conditional branch that takes the prior as input and produces a feature map
      ConditionalBranch(),
      # A series of upsampling and convolutional blocks that increase the resolution and channels
      UpsampleConvBlock(),
      UpsampleConvBlock(),
      UpsampleConvBlock(),
      # A final convolutional layer that produces the output image
      ConvLayer()
    )

  def forward(self, z, prior=None):
    # If prior is given, concatenate it with z along the channel dimension
    if prior is not None:
      z = torch.cat([z, prior], dim=1)
    # Apply the decoder layers to z and return the output image
    x_hat = self.layers(z)
    return x_hat

# Train the asymmetric VQGAN decoder with minimal cost
# Keep the vanilla VQGAN encoder and StableDiffusion unchanged
def train_asymmetric_vqgan_decoder():
  # Initialize the asymmetric VQGAN with a pretrained vanilla VQGAN encoder
  model = AsymmetricVQGAN()
  model.encoder.load_state_dict(pretrained_vqgan_encoder_state_dict)
  # Freeze the encoder parameters
  for param in model.encoder.parameters():
    param.requires_grad = False
  # Define the optimizer and the loss function for the decoder
  optimizer = Adam(model.decoder.parameters(), lr=1e-4)
  loss_fn = L1Loss()
  # Loop over the training data
  for epoch in range(num_epochs):
    for batch in dataloader:
      # Get the input image and the target image
      x = batch["input"]
      y = batch["target"]
      # Forward pass the model and get the reconstructed image
      x_hat = model(x)
      # Compute the loss between x_hat and y
      loss = loss_fn(x_hat, y)
      # Backward pass and update the decoder parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    # Print the epoch and the loss
    print(f"Epoch {epoch}, Loss {loss.item()}")

# Use the asymmetric VQGAN in StableDiffusion-based inpainting and local editing methods
def inpaint_or_edit_image(x, mask, text):
  # Initialize the asymmetric VQGAN with a pretrained vanilla VQGAN encoder and a trained asymmetric VQGAN decoder
  model = AsymmetricVQGAN()
  model.encoder.load_state_dict(pretrained_vqgan_encoder_state_dict)
  model.decoder.load_state_dict(trained_asymmetric_vqgan_decoder_state_dict)
  # Initialize StableDiffusion with a pretrained diffusion model in latent space
  stable_diffusion = StableDiffusion()
  stable_diffusion.load_state_dict(pretrained_diffusion_model_state_dict)
  # Encode the input image x to a latent code z
  z = model.encoder(x)
  # Mask out the latent code z according to the mask
  z_masked = z * mask
  # Generate a new latent code z_new from StableDiffusion conditioned on text and z_masked
  z_new = stable_diffusion.generate(text, z_masked)
  # Decode the new latent code z_new to a new image x_new
  # Use x as a prior for the conditional branch of the decoder
  x_new = model.decoder(z_new, x)
  return x_new

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Define the asymmetric VQGAN with a conditional branch and a heavier decoder
class AsymmetricVQGAN(nn.Module):
  def __init__(self):
    # Initialize the encoder and the decoder
    self.encoder = VanillaVQGANEncoder()
    self.decoder = AsymmetricVQGANDecoder()

  def forward(self, x, prior=None):
    # Encode the input image x to a latent code z
    z = self.encoder(x)
    # Decode the latent code z to a reconstructed image x_hat
    # If prior is given, use it as a conditional input to the decoder
    x_hat = self.decoder(z, prior)
    return x_hat

# Define the vanilla VQGAN encoder with a quantization layer
class VanillaVQGANEncoder(nn.Module):
  def __init__(self):
    # Initialize the encoder layers
    self.layers = nn.Sequential(
      # A series of downsampling and convolutional blocks that reduce the resolution and increase the channels
      DownsampleConvBlock(),
      DownsampleConvBlock(),
      DownsampleConvBlock(),
      # A quantization layer that maps the continuous features to discrete codes
      QuantizeLayer()
    )

  def forward(self, x):
    # Apply the encoder layers to x and return the latent code z
    z = self.layers(x)
    return z

# Define the asymmetric VQGAN decoder with a conditional branch and a heavier structure
class AsymmetricVQGANDecoder(nn.Module):
  def __init__(self):
    # Initialize the decoder layers
    self.layers = nn.Sequential(
      # A conditional branch that takes the prior as input and produces a feature map
      ConditionalBranch(),
      # A series of upsampling and convolutional blocks that increase the resolution and channels
      UpsampleConvBlock(),
      UpsampleConvBlock(),
      UpsampleConvBlock(),
      # A final convolutional layer that produces the output image
      ConvLayer()
    )

  def forward(self, z, prior=None):
    # If prior is given, concatenate it with z along the channel dimension
    if prior is not None:
      z = torch.cat([z, prior], dim=1)
    # Apply the decoder layers to z and return the output image
    x_hat = self.layers(z)
    return x_hat

# Define the conditional branch that takes the prior as input and produces a feature map
class ConditionalBranch(nn.Module):
  def __init__(self):
    # Initialize the conditional branch layers
    self.layers = nn.Sequential(
      # A convolutional layer that reduces the channels of the prior
      ConvLayer(in_channels=3, out_channels=64),
      # A residual block that preserves the resolution and channels of the prior
      ResidualBlock(in_channels=64, out_channels=64),
      # A convolutional layer that increases the channels of the prior to match the latent code dimension
      ConvLayer(in_channels=64, out_channels=256)
    )

  def forward(self, prior):
    # Apply the conditional branch layers to prior and return the feature map
    feature = self.layers(prior)
    return feature

# Define a downsampling convolutional block that reduces the resolution and increases the channels
class DownsampleConvBlock(nn.Module):
  def __init__(self, in_channels=None, out_channels=None):
    # Initialize the block layers
    self.layers = nn.Sequential(
      # A convolutional layer with stride 2 that halves the resolution and changes the channels
      ConvLayer(in_channels=in_channels, out_channels=out_channels, stride=2),
      # A residual block that preserves the resolution and channels
      ResidualBlock(in_channels=out_channels, out_channels=out_channels),
      # An attention block that enhances the feature representation
      AttentionBlock(in_channels=out_channels)
    )

  def forward(self, x):
    # Apply the block layers to x and return the output feature map
    y = self.layers(x)
    return y

# Define an upsampling convolutional block that increases the resolution and reduces the channels
class UpsampleConvBlock(nn.Module):
  def __init__(self, in_channels=None, out_channels=None):
    # Initialize the block layers
    self.layers = nn.Sequential(
      # An upsampling layer with scale factor 2 that doubles the resolution
      UpsampleLayer(scale_factor=2),
      # A convolutional layer that changes the channels
      ConvLayer(in_channels=in_channels, out_channels=out_channels),
      # A residual block that preserves the resolution and channels
      ResidualBlock(in_channels=out_channels, out_channels=out_channels),
      # An attention block that enhances the feature representation
      AttentionBlock(in_channels=out_channels)
    )

  def forward(self, x):
    # Apply the block layers to x and return the output feature map
    y = self.layers(x)
    return y

# Define a convolutional layer with optional parameters
class ConvLayer(nn.Module):
  def __init__(self, in_channels=None, out_channels=None, kernel_size=3, stride=1, padding=1, activation="relu", normalization="instance"):
    # Initialize the layer
    self.layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)
    # Add an activation function if specified
    if activation == "relu":
      self.layer = nn.Sequential(self.layer, nn.ReLU())
    elif activation == "tanh":
      self.layer = nn.Sequential(self.layer, nn.Tanh())
    # Add a normalization layer if specified
    if normalization == "instance":
      self.layer = nn.Sequential(self.layer, nn.InstanceNorm2d(out_channels))
    elif normalization == "batch":
      self.layer = nn.Sequential(self.layer, nn.BatchNorm2d(out_channels))

  def forward(self, x):
    # Apply the layer to x and return the output feature map
    y = self.layer(x)
    return y

# Define a residual block that preserves the resolution and channels
class ResidualBlock(nn.Module):
  def __init__(self, in_channels=None, out_channels=None):
    # Initialize the block layers
    self.layers = nn.Sequential(
      # A convolutional layer that preserves the resolution and channels
      ConvLayer(in_channels=in_channels, out_channels=out_channels),
      # A convolutional layer that preserves the resolution and channels
      ConvLayer(in_channels=out_channels, out_channels=out_channels),
      # A skip connection that adds the input to the output
      SkipConnection()
    )

  def forward(self, x):
    # Apply the block layers to x and return the output feature map
    y = self.layers(x)
    return y

# Define an attention block that enhances the feature representation
class AttentionBlock(nn.Module):
  def __init__(self, in_channels=None):
    # Initialize the block layers
    self.layers = nn.Sequential(
      # A self-attention layer that computes the attention weights
      SelfAttentionLayer(in_channels=in_channels),
      # A convolutional layer that preserves the resolution and channels
      ConvLayer(in_channels=in_channels, out_channels=in_channels),
      # A skip connection that adds the input to the output
      SkipConnection()
    )

  def forward(self, x):
    # Apply the block layers to x and return the output feature map
    y = self.layers(x)
    return y

# Define a quantization layer that maps the continuous features to discrete codes
class QuantizeLayer(nn.Module):
  def __init__(self, num_embeddings=512, embedding_dim=256):
    # Initialize the layer
    self.layer = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)
    # Initialize the embeddings with random values from a normal distribution
    self.layer.weight.data.normal_()

  def forward(self, x):
    # Flatten x to a 2D tensor of shape (batch_size * height * width, channels)
    x = x.view(-1, x.shape[-1])
    # Compute the L2 distance between x and each embedding vector
    distances = torch.cdist(x.unsqueeze(0), self.layer.weight.unsqueeze(0))[0]
    # Find the index of the nearest embedding vector for each x
    indices = torch.argmin(distances, dim=1)
    # Quantize x by replacing it with the nearest embedding vector
    z = torch.index_select(self.layer.weight, dim=0, index=indices)
    # Reshape z to match the original shape of x
    z = z.view(*x.shape[:-1], -1)
    return z

# Define an upsampling layer with a scale factor
class UpsampleLayer(nn.Module):
  def __init__(self, scale_factor=None):
    # Initialize the layer
    self.layer = nn.Upsample(scale_factor=scale_factor)

  def forward(self, x):
    # Apply the layer to x and return the output feature map
    y = self.layer(x)
    return y

# Define a self-attention layer that computes the attention weights
class SelfAttentionLayer(nn.Module):
  def __init__(self, in_channels=None):
    # Initialize the layer parameters
    self.query_conv = ConvLayer(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, padding=0)
    self.key_conv = ConvLayer(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, padding=0)
    self.value_conv = ConvLayer(in_channels=in_channels, out_channels=in_channels//8*4 , kernel_size=1,padding=0)

  def forward