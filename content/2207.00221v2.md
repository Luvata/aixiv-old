---
title: 2207.00221v2 VL-CheckList  Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations
date: 2022-07-01
---

# [VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations](http://arxiv.org/abs/2207.00221v2)

authors: Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, Jianwei Yin


## What, Why and How

[1]: https://arxiv.org/pdf/2207.00221v2.pdf "arXiv:2207.00221v2 [cs.CV] 22 Jun 2023"
[2]: https://arxiv.org/abs/2207.00221 "[2207.00221] VL-CheckList: Evaluating Pre-trained Vision-Language ..."
[3]: https://arxiv.org/pdf/2207.00221 "PDF for 2207.00221 - arXiv.org"

Here is a summary of the paper:

- **What**: The paper introduces VL-CheckList, a novel framework to evaluate pre-trained vision-language models with objects, attributes and relations.
- **Why**: The paper argues that existing methods of evaluating vision-language models based on downstream tasks are limited in interpretability, comparability and coverage. The paper proposes to use a more fine-grained and comprehensive approach to test the image-texting ability of vision-language models across different categories and subcategories.
- **How**: The paper divides the image-texting ability of a vision-language model into three categories: objects, attributes and relations, and uses a novel taxonomy to further break down these three aspects. The paper then generates test cases for each subcategory using existing datasets or human annotation. The paper applies the proposed framework to seven popular vision-language models and analyzes their strengths and weaknesses. The paper also discusses some potential directions for improving vision-language models based on the findings.


## Main Contributions

According to the paper, the contributions are:

- The paper proposes VL-CheckList, a novel framework to evaluate pre-trained vision-language models with objects, attributes and relations, which is more fine-grained and comprehensive than existing methods based on downstream tasks.
- The paper introduces a novel taxonomy to categorize the image-texting ability of vision-language models into three categories and 18 subcategories, which covers a wide range of cross-modal phenomena and challenges.
- The paper generates test cases for each subcategory using existing datasets or human annotation, and provides a large-scale test suite with over 100K test cases for vision-language models.
- The paper applies the proposed framework to seven popular vision-language models and reveals their strengths and weaknesses in different aspects of image-texting ability. The paper also discusses some potential directions for improving vision-language models based on the findings.


## Method Summary

Here is a summary of the method section of the paper:

- The method section describes the proposed VL-CheckList framework, which consists of three steps: taxonomy construction, test case generation and model evaluation.
- The first step is to construct a taxonomy that categorizes the image-texting ability of vision-language models into three categories: objects, attributes and relations. Each category is further divided into six subcategories based on different types of cross-modal phenomena and challenges. For example, the objects category includes subcategories such as object detection, object counting, object recognition and object localization. The paper provides definitions and examples for each subcategory in the taxonomy.
- The second step is to generate test cases for each subcategory using existing datasets or human annotation. The paper leverages various sources of data, such as COCO, Visual Genome, Conceptual Captions and Flickr30K, to create test cases that cover different aspects of image-texting ability. The paper also uses human annotation to generate test cases for some subcategories that are not well covered by existing datasets, such as attribute comparison, relation detection and relation reasoning. The paper provides details on how to select or create test cases for each subcategory in the taxonomy.
- The third step is to evaluate vision-language models using the generated test cases. The paper applies the proposed framework to seven popular vision-language models: CLIP, DALL-E, VIT-B/32, UNITER-base, OSCAR-base, ViLBERT-base and LXMERT. The paper measures the accuracy of each model on each subcategory and reports the results in a table. The paper also analyzes the results and discusses some insights and implications for improving vision-language models.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the taxonomy of image-texting ability
taxonomy = {
  "objects": ["object detection", "object counting", "object recognition", "object localization", "object comparison", "object generation"],
  "attributes": ["attribute detection", "attribute recognition", "attribute comparison", "attribute localization", "attribute generation", "attribute reasoning"],
  "relations": ["relation detection", "relation recognition", "relation comparison", "relation localization", "relation generation", "relation reasoning"]
}

# Generate test cases for each subcategory
test_cases = {}
for category in taxonomy:
  for subcategory in taxonomy[category]:
    # Use existing datasets or human annotation to create test cases
    test_cases[subcategory] = generate_test_cases(subcategory)

# Evaluate vision-language models using test cases
models = ["CLIP", "DALL-E", "VIT-B/32", "UNITER-base", "OSCAR-base", "ViLBERT-base", "LXMERT"]
results = {}
for model in models:
  # Load the pre-trained model
  model = load_model(model)
  results[model] = {}
  for subcategory in test_cases:
    # Compute the accuracy of the model on the test cases
    results[model][subcategory] = evaluate_model(model, test_cases[subcategory])

# Analyze and discuss the results
analyze_results(results)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the taxonomy of image-texting ability
taxonomy = {
  "objects": ["object detection", "object counting", "object recognition", "object localization", "object comparison", "object generation"],
  "attributes": ["attribute detection", "attribute recognition", "attribute comparison", "attribute localization", "attribute generation", "attribute reasoning"],
  "relations": ["relation detection", "relation recognition", "relation comparison", "relation localization", "relation generation", "relation reasoning"]
}

# Define the datasets and annotation sources for each subcategory
datasets = {
  "object detection": ["COCO"],
  "object counting": ["COCO"],
  "object recognition": ["COCO", "Visual Genome"],
  "object localization": ["COCO"],
  "object comparison": ["Human Annotation"],
  "object generation": ["Conceptual Captions"],
  "attribute detection": ["Visual Genome"],
  "attribute recognition": ["Visual Genome"],
  "attribute comparison": ["Human Annotation"],
  "attribute localization": ["Visual Genome"],
  "attribute generation": ["Conceptual Captions"],
  "attribute reasoning": ["Human Annotation"],
  "relation detection": ["Visual Genome"],
  "relation recognition": ["Visual Genome"],
  "relation comparison": ["Human Annotation"],
  "relation localization": ["Visual Genome"],
  "relation generation": ["Conceptual Captions"],
  "relation reasoning": ["Human Annotation"]
}

# Define the functions to load and preprocess the datasets
def load_coco():
  # Load the COCO dataset using torchvision
  coco_train = torchvision.datasets.CocoDetection(root="coco/train2017",
                                                  annFile="coco/annotations/instances_train2017.json")
  coco_val = torchvision.datasets.CocoDetection(root="coco/val2017",
                                                annFile="coco/annotations/instances_val2017.json")
  return coco_train, coco_val

def load_visual_genome():
  # Load the Visual Genome dataset using pandas
  vg_images = pd.read_json("visual_genome/images.json")
  vg_objects = pd.read_json("visual_genome/objects.json")
  vg_attributes = pd.read_json("visual_genome/attributes.json")
  vg_relationships = pd.read_json("visual_genome/relationships.json")
  return vg_images, vg_objects, vg_attributes, vg_relationships

def load_conceptual_captions():
  # Load the Conceptual Captions dataset using pandas
  cc_train = pd.read_csv("conceptual_captions/train.tsv", sep="\t", names=["caption", "image_url"])
  cc_val = pd.read_csv("conceptual_captions/validation.tsv", sep="\t", names=["caption", "image_url"])
  return cc_train, cc_val

def load_human_annotation():
  # Load the human annotation data using pandas
  ha_data = pd.read_csv("human_annotation/data.csv")
  return ha_data

def preprocess_image(image):
  # Preprocess the image using torchvision transforms
  transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize(224),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
    ])
  
  image = transform(image)
  
  return image

def preprocess_text(text):
  # Preprocess the text using transformers tokenizer
  tokenizer = transformers.AutoTokenizer.from_pretrained("bert-base-uncased")
  
  text = tokenizer(text,
                   padding="max_length",
                   truncation=True,
                   max_length=64,
                   return_tensors="pt")
  
  return text

# Define the functions to generate test cases for each subcategory
def generate_test_cases(subcategory):
  
  test_cases = []
  
  if subcategory == "object detection":
    # Use COCO dataset to generate test cases for object detection
    coco_train, coco_val = load_coco()
    
    # Sample a random image and its annotations from the validation set
    image, annotations = coco_val[np.random.randint(len(coco_val))]
    
    # Preprocess the image and extract the object labels and bounding boxes from the annotations
    image = preprocess_image(image)
    labels = [coco_val.coco.cats[ann["category_id"]]["name"] for ann in annotations]
    boxes = [ann["bbox"] for ann in annotations]
    
    # Create a test case with the image, labels and boxes as inputs and outputs
    test_case = {
      "input": {
        "image": image
      },
      "output": {
        "labels": labels,
        "boxes": boxes
      }
    }
    
    # Append the test case to the list of test cases
    test_cases.append(test_case)
  
  elif subcategory == "object counting":
    # Use COCO dataset to generate test cases for object counting
    coco_train, coco_val = load_coco()
    
    # Sample a random image and its annotations from the validation set
    image, annotations = coco_val[np.random.randint(len(coco_val))]
    
    # Preprocess the image and extract the object labels and counts from the annotations
    image = preprocess_image(image)
    labels = [coco_val.coco.cats[ann["category_id"]]["name"] for ann in annotations]
    counts = {label: labels.count(label) for label in set(labels)}
    
    # Create a test case with the image and counts as inputs and outputs
    test_case = {
      "input": {
        "image": image
      },
      "output": {
        "counts": counts
      }
    }
    
    # Append the test case to the list of test cases
    test_cases.append(test_case)
  
  elif subcategory == "object recognition":
    # Use COCO or Visual Genome dataset to generate test cases for object recognition
    dataset = np.random.choice(["COCO", "Visual Genome"])
    
    if dataset == "COCO":
      # Use COCO dataset to generate test cases for object recognition
      coco_train, coco_val = load_coco()
      
      # Sample a random image and its annotations from the validation set
      image, annotations = coco_val[np.random.randint(len(coco_val))]
      
      # Preprocess the image and extract the object label from the annotations
      image = preprocess_image(image)
      label = coco_val.coco.cats[annotations[0]["category_id"]]["name"]
      
      # Create a test case with the image and label as inputs and outputs
      test_case = {
        "input": {
          "image": image
        },
        "output": {
          "label": label
        }
      }
      
      # Append the test case to the list of test cases
      test_cases.append(test_case)
    
    else:
      # Use Visual Genome dataset to generate test cases for object recognition
      vg_images, vg_objects, vg_attributes, vg_relationships = load_visual_genome()
      
      # Sample a random image and its objects from the dataset
      image_id = np.random.choice(vg_images["image_id"])
      image_url = vg_images[vg_images["image_id"] == image_id]["url"].iloc[0]
      objects = vg_objects[vg_objects["image_id"] == image_id]["objects"].iloc[0]
      
      # Download and preprocess the image and extract a random object name from the objects
      image = torchvision.io.read_image(image_url)
      image = preprocess_image(image)
      object_name = np.random.choice(objects)["names"]
      
      # Create a test case with the image and object name as inputs and outputs
      test_case = {
        "input": {
          "image": image
        },
        "output": {
          "object_name": object_name
        }
      }
      
      # Append the test case to the list of test cases
      test_cases.append(test_case)
  
  elif subcategory == "object localization":
    # Use COCO dataset to generate test cases for object localization
    coco_train, coco_val = load_coco()
    
    # Sample a random image and its annotations from the validation set
    image, annotations = coco_val[np.random.randint(len(coco_val))]
    
    # Preprocess the image and extract a random object label and bounding box from the annotations
    image = preprocess_image(image)
    annotation = np.random.choice(annotations)
    label = coco_val.coco.cats[annotation["category_id"]]["name"]
    box = annotation["bbox"]
    
    # Create a test case with the image and label as inputs and box as output
    test_case = {
      "input": {
        "image": image,
        "label": label
      },
      "output": {
        "box": box
      }
    }
    
    # Append the test case to the list of test cases
    test_cases.append(test_case)
  
  elif subcategory == "object comparison":
    # Use human annotation data to generate test cases for object comparison
    ha_data = load_human_annotation()
    
    # Sample a random row from the data that contains an object comparison question and answer
    row = ha_data[ha_data["question"].str.contains("compare")].sample(1).iloc[0]
    
    # Download and preprocess the images and extract the question and answer from the row
    image1