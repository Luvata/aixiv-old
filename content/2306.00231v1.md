---
title: 2306.00231v1 A Universal Latent Fingerprint Enhancer Using Transformers
date: 2023-06-01
---

# [A Universal Latent Fingerprint Enhancer Using Transformers](http://arxiv.org/abs/2306.00231v1)

authors: Andre Brasil Vieira Wyzykowski, Anil K. Jain


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00231 "A U L F E U T - arXiv.org"
[2]: https://arxiv.org/abs/2306.00039 "[2306.00039] Sensing and control scheme for the inteferometer ..."
[3]: http://export.arxiv.org/abs/2306.00231 "[2306.00231] A Universal Latent Fingerprint Enhancer Using Transformers"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a novel method called ULPrint to enhance latent fingerprints using transformers.
- **Why**: The paper aims to improve the identification and analysis of latent fingerprints, which are often-invisible residues on surfaces that provide crucial evidence in criminal investigations. Latent fingerprints are challenging to identify due to various factors such as noise, overlapping prints, contamination, and distortions.
- **How**: The paper develops a two-step latent fingerprint enhancement method that combines Ridge Segmentation with UNet and Mix Visual Transformer (MiT) SegFormer-B5 encoder architecture. The method also implements multiple dilated convolutions in the UNet architecture to capture intricate, non-local patterns better and enhance ridge segmentation. The method also uses guided blending of the predicted ridge mask with the latent fingerprint. The paper evaluates the method on three latent fingerprint databases and shows that it improves the performance of the MSU-AFIS system[^1^][1].

## Main Contributions

The paper lists the following contributions:

- The development of a two-step latent fingerprint enhancement method that combines Ridge Segmentation with UNet and Mix Visual Transformer (MiT) SegFormer-B5 encoder architecture.
- The implementation of multiple dilated convolutions in the UNet architecture to capture intricate, non-local patterns better and enhance ridge segmentation.
- The guided blending of the predicted ridge mask with the latent fingerprint.
- The streamlining of the enhancement process, addressing challenges across diverse latent fingerprint types to improve forensic investigations and criminal justice outcomes.

## Method Summary

[1]: https://arxiv.org/pdf/2302.00231v1.pdf "arXiv:2302.00231v1 [math.FA] 1 Feb 2023"
[2]: https://arxiv.org/ "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2304.00231v1 "[2304.00231v1] Using Overlap Weights to Address Extreme Propensity ..."

Here is a summary of the method section of the paper:

- The method consists of two steps: ridge segmentation and ridge enhancement.
- Ridge segmentation aims to separate the genuine ridges from the background noise and artifacts. The method uses a modified UNet architecture with multiple dilated convolutions to capture non-local patterns and generate a ridge mask. The method also uses a Mix Visual Transformer (MiT) SegFormer-B5 encoder to extract high-level features and refine the ridge mask.
- Ridge enhancement aims to improve the contrast and clarity of the ridges. The method uses guided blending to combine the original latent fingerprint with the predicted ridge mask. The method also applies histogram equalization and contrast-limited adaptive histogram equalization to further enhance the ridges.
- The method is trained on a synthetic dataset of 100,000 latent fingerprints generated by applying various distortions and noises to rolled fingerprints. The method is evaluated on three real latent fingerprint databases: NIST SD27, MSP Latent, and NIST SD302. The method is compared with the MSU-AFIS system and several state-of-the-art enhancement methods.

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```
# Define the UNet architecture with multiple dilated convolutions
function UNet(input):
  # input is a latent fingerprint image of size 256 x 256
  # downsample the input using convolutional layers with stride 2 and ReLU activation
  # apply batch normalization and dropout after each layer
  # use dilated convolutions with different dilation rates to capture non-local patterns
  # store the intermediate feature maps for skip connections
  down1 = Conv2D(64, kernel_size=3, stride=2, dilation_rate=1)(input)
  down1 = BatchNorm()(down1)
  down1 = ReLU()(down1)
  down1 = Dropout(0.1)(down1)
  
  down2 = Conv2D(128, kernel_size=3, stride=2, dilation_rate=2)(down1)
  down2 = BatchNorm()(down2)
  down2 = ReLU()(down2)
  down2 = Dropout(0.1)(down2)

  down3 = Conv2D(256, kernel_size=3, stride=2, dilation_rate=4)(down2)
  down3 = BatchNorm()(down3)
  down3 = ReLU()(down3)
  down3 = Dropout(0.1)(down3)

  down4 = Conv2D(512, kernel_size=3, stride=2, dilation_rate=8)(down3)
  down4 = BatchNorm()(down4)
  down4 = ReLU()(down4)
  down4 = Dropout(0.1)(down4)

  # bottleneck layer with a convolutional layer and a dilated convolutional layer
  bottleneck = Conv2D(1024, kernel_size=3, stride=1)(down4)
  bottleneck = BatchNorm()(bottleneck)
  bottleneck = ReLU()(bottleneck)
  bottleneck = Dropout(0.1)(bottleneck)

  bottleneck = Conv2D(1024, kernel_size=3, stride=1, dilation_rate=16)(bottleneck)
  bottleneck = BatchNorm()(bottleneck)
  bottleneck = ReLU()(bottleneck)
  bottleneck = Dropout(0.1)(bottleneck)

  # upsample the bottleneck using transposed convolutional layers with stride 2 and ReLU activation
  # apply batch normalization and dropout after each layer
  # concatenate the upsampled feature maps with the corresponding skip connections from the downsampling path
  # use convolutional layers to reduce the number of channels and refine the feature maps
  up4 = Conv2DTranspose(512, kernel_size=3, stride=2)(bottleneck)
  up4 = BatchNorm()(up4)
  up4 = ReLU()(up4)
  up4 = Dropout(0.1)(up4)

  up4 = Concatenate()([up4, down4])
  up4 = Conv2D(512, kernel_size=3, stride=1)(up4)
  up4 = BatchNorm()(up4)
  up4 = ReLU()(up4)

  up3 = Conv2DTranspose(256, kernel_size=3, stride=2)(up4)
  up3 = BatchNorm()(up3)
  up3 = ReLU()(up3)
  up3 = Dropout(0.1)(up3)

  up3 = Concatenate()([up3, down3])
  up3 = Conv2D(256, kernel_size=3, stride=1)(up3)
  up3 = BatchNorm()(up3)
  up3 = ReLU()(up3)
  
```