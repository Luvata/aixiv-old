---
title: 2010.02502v4 Denoising Diffusion Implicit Models
date: 2020-10-03
---

# [Denoising Diffusion Implicit Models](http://arxiv.org/abs/2010.02502v4)

authors: Jiaming Song, Chenlin Meng, Stefano Ermon


## What, Why and How

[1]: https://arxiv.org/abs/2010.02502 "[2010.02502] Denoising Diffusion Implicit Models - arXiv.org"
[2]: https://web7.arxiv.org/abs/2010.02502v4 "[2010.02502v4] Denoising Diffusion Implicit Models"
[3]: https://arxiv.org/pdf/2010.02502v4 "arXiv.org e-Print archive"
[4]: https://arxiv-export-lb.library.cornell.edu/abs/2010.02502 "[2010.02502] Denoising Diffusion Implicit Models"

Here is a summary of the paper:

- **What**: The paper proposes a new class of generative models called **denoising diffusion implicit models (DDIMs)** that can produce high quality images faster than existing methods[^1^][1].
- **Why**: The paper aims to overcome the limitations of **denoising diffusion probabilistic models (DDPMs)**, which are state-of-the-art generative models that do not require adversarial training, but are slow to sample from because they require simulating a Markov chain for many steps[^1^][1].
- **How**: The paper constructs a class of **non-Markovian diffusion processes** that lead to the same training objective as DDPMs, but whose reverse process can be much faster to sample from. The paper also introduces a novel sampling algorithm that leverages the implicit nature of the models and allows trading off computation for sample quality[^1^][1]. The paper evaluates the proposed models on several image datasets and shows that they can produce high quality samples up to 50 times faster than DDPMs, and can also perform semantically meaningful image interpolation in the latent space[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- It introduces a new class of generative models called **denoising diffusion implicit models (DDIMs)** that can produce high quality images faster than existing methods.
- It constructs a class of **non-Markovian diffusion processes** that lead to the same training objective as DDPMs, but whose reverse process can be much faster to sample from.
- It proposes a novel sampling algorithm that leverages the implicit nature of the models and allows trading off computation for sample quality.
- It empirically demonstrates that DDIMs can produce high quality samples up to 50 times faster than DDPMs, and can also perform semantically meaningful image interpolation in the latent space.

## Method Summary

Here is a summary of the method section of the paper:

- The paper starts by reviewing the background of **denoising diffusion probabilistic models (DDPMs)**, which are generative models that define a Markovian diffusion process that gradually adds noise to the data, and a reverse process that denoises the data using a neural network. The paper also reviews the training and sampling procedures of DDPMs, and their connections to score-based generative models and variational autoencoders.
- The paper then introduces **denoising diffusion implicit models (DDIMs)**, which are a more general class of generative models that do not assume a Markovian diffusion process, but instead define a non-Markovian diffusion process that can have arbitrary noise levels and transitions. The paper shows that DDIMs can be trained with the same objective as DDPMs, but can have faster and more flexible sampling procedures.
- The paper then proposes a novel sampling algorithm for DDIMs, which leverages the implicit nature of the models and allows trading off computation for sample quality. The algorithm uses a gradient-based optimization method to find the most likely latent variables given the current observation and the previous noise level. The algorithm also uses a stochastic approximation technique to estimate the gradient of the implicit model using finite differences.
- The paper then discusses some theoretical properties of DDIMs, such as their connections to ordinary differential equations (ODEs) and stochastic differential equations (SDEs), and their relation to other implicit generative models such as generative adversarial networks (GANs) and normalizing flows. The paper also provides some analysis on the trade-off between computation and sample quality in DDIMs.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Training DDIMs
# Input: data x, noise schedule beta, denoising network f
# Output: trained f
for each iteration:
  sample a minibatch of data x
  sample a noise level t from beta
  add Gaussian noise z to x to get noisy data x_t
  compute the loss L(f(x_t, t), x) using the reverse KL divergence
  update f using gradient descent on L

# Sampling DDIMs
# Input: noise schedule beta, denoising network f, number of steps T, step size alpha
# Output: sampled data x
initialize x_0 with Gaussian noise
for t in range(T):
  sample a noise level t from beta
  update x_t using gradient descent on log p(x_t | t) with step size alpha
  # log p(x_t | t) is approximated by f(x_t, t)
return x_T as the final sample
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Training DDIMs
# Input: data x, noise schedule beta, denoising network f
# Output: trained f
# Hyperparameters: learning rate lr, batch size bs
initialize f randomly
create an optimizer opt for f with learning rate lr
for each iteration:
  sample a minibatch of data x of size bs
  sample a noise level t from beta uniformly
  compute the noise variance sigma_t^2 = (1 - t) / t
  sample Gaussian noise z of size bs with mean 0 and variance sigma_t^2
  compute the noisy data x_t = sqrt(1 - t) * x + sqrt(t) * z
  compute the predicted data x_hat = f(x_t, t)
  compute the loss L(f(x_t, t), x) = 0.5 * sigma_t^2 * ||x_hat - x||^2
  compute the gradient of L with respect to f
  update f using opt

# Sampling DDIMs
# Input: noise schedule beta, denoising network f, number of steps T, step size alpha
# Output: sampled data x
# Hyperparameters: epsilon for finite difference approximation
sample a noise level T from beta uniformly
sample Gaussian noise z of size 1 with mean 0 and variance T
initialize x_0 = z / sqrt(T)
for t in reversed(range(T)):
  sample a noise level t from beta uniformly
  compute the noise variance sigma_t^2 = (1 - t) / t
  sample Gaussian noise e of size 1 with mean 0 and variance epsilon^2
  compute the noisy data x_t = sqrt(1 - t) * x_{t+1} + sqrt(t) * z
  compute the predicted data x_hat = f(x_t, t)
  compute the gradient of log p(x_t | t) with respect to x_t using finite difference approximation:
    grad_log_p = (f(x_t + e, t) - f(x_t - e, t)) / (2 * epsilon)
  update x_t using gradient ascent on log p(x_t | t) with step size alpha:
    x_t = x_t + alpha * grad_log_p
return x_0 as the final sample
```