---
title: 2304.07090v1 Delta Denoising Score
date: 2023-04-08
---

# [Delta Denoising Score](http://arxiv.org/abs/2304.07090v1)

authors: Amir Hertz, Kfir Aberman, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2304.07090 "[2304.07090] Delta Denoising Score - arXiv.org"
[2]: https://arxiv-export3.library.cornell.edu/abs/2304.07090v1 "[2304.07090v1] Delta Denoising Score"
[3]: https://arxiv.org/pdf/2304.07090.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces **Delta Denoising Score (DDS)**, a novel scoring function for text-based image editing that guides minimal modifications of an input image towards the content described in a target prompt.
- **Why**: The paper aims to address the limitations of existing methods for text-based image editing, such as producing non-detailed and blurry outputs due to noisy gradients, or requiring large amounts of training data and domain-specific models.
- **How**: The paper leverages the rich generative prior of text-to-image diffusion models and uses the Score Distillation Sampling (SDS) mechanism for the purpose of image editing. DDS uses a prompt that matches the input image to identify and remove undesired erroneous directions of SDS. DDS can be used as a loss term in an optimization problem to steer an image towards a desired direction dictated by a text. The paper also shows that DDS can be used to train an effective zero-shot image translation model. The paper evaluates DDS on various text-based image editing tasks and compares it with existing methods in terms of stability and quality.

## Main Contributions

The paper claims to make the following contributions:

- It proposes **Delta Denoising Score (DDS)**, a novel scoring function for text-based image editing that guides minimal modifications of an input image towards the content described in a target prompt.
- It demonstrates the competence of DDS for text based image-to-image translation and shows that it outperforms existing methods in terms of stability and quality.
- It shows that DDS can be used to train an effective zero-shot image translation model that can handle diverse domains and styles without requiring any paired data.

## Method Summary

[1]: https://arxiv.org/abs/2304.07090 "[2304.07090] Delta Denoising Score - arXiv.org"
[2]: https://arxiv.org/pdf/2304.07090.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2301.07090v1 "[2301.07090v1] Kostant's problem for parabolic Verma modules"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses a text-to-image diffusion model as the backbone for text-based image editing. A diffusion model is a generative model that reverses the process of adding Gaussian noise to an image until it becomes a pure noise sample. The model learns to denoise the image conditioned on a text prompt at each step of the diffusion process.
- The paper introduces **Score Distillation Sampling (SDS)**, a mechanism that allows sampling from a diffusion model without running the full diffusion process. SDS uses a score network that estimates the gradient of the log-density of the diffusion model at any given noise level. SDS samples an image by starting from a pure noise sample and applying Langevin dynamics guided by the score network and the text prompt.
- The paper proposes **Delta Denoising Score (DDS)**, a novel scoring function for text-based image editing that guides minimal modifications of an input image towards the content described in a target prompt. DDS is defined as the difference between two SDS scores: one calculated with the target prompt and one calculated with a matching prompt. A matching prompt is a text that describes the content of the input image accurately. DDS measures how well an image matches a target prompt relative to a matching prompt, and can be used as a loss term in an optimization problem to steer an image towards a desired direction dictated by a text.
- The paper also shows how to use DDS to train a zero-shot image translation model that can handle diverse domains and styles without requiring any paired data. The paper uses a cycle-consistency loss that enforces that an image translated from one domain to another and back to the original domain should be similar to the original image. The paper uses DDS as the translation loss for both directions of the cycle. The paper also uses an identity loss that prevents changing an image when translating it to its own domain.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a text-to-image diffusion model
diffusion_model = DiffusionModel()

# Define a score network that estimates the gradient of the log-density of the diffusion model
score_network = ScoreNetwork(diffusion_model)

# Define a function that calculates the Score Distillation Sampling (SDS) score for a given image and prompt
def SDS_score(image, prompt):
  # Start from a pure noise sample
  noise = sample_gaussian_noise()
  # Apply Langevin dynamics guided by the score network and the prompt
  for t in range(num_steps):
    noise = noise + step_size * score_network(noise, prompt) + sqrt(2 * step_size) * sample_gaussian_noise()
  # Return the final noise sample as the SDS score
  return noise

# Define a function that calculates the Delta Denoising Score (DDS) for a given image, target prompt and matching prompt
def DDS_score(image, target_prompt, matching_prompt):
  # Calculate the SDS score with the target prompt
  target_score = SDS_score(image, target_prompt)
  # Calculate the SDS score with the matching prompt
  matching_score = SDS_score(image, matching_prompt)
  # Return the difference between the two scores as the DDS score
  return target_score - matching_score

# Define a function that performs text-based image editing using DDS as a loss term
def edit_image(image, target_prompt, matching_prompt):
  # Initialize an optimizer
  optimizer = Optimizer()
  # Optimize the image to minimize the DDS score
  for i in range(num_iterations):
    # Calculate the DDS score
    loss = DDS_score(image, target_prompt, matching_prompt)
    # Update the image using the optimizer
    image = optimizer.step(loss)
  # Return the edited image
  return image

# Define a function that trains a zero-shot image translation model using DDS and cycle-consistency loss
def train_model(source_domain, target_domain):
  # Initialize a model
  model = Model()
  # Loop over the training data
  for source_image, target_image in data_loader:
    # Generate a target prompt for the source image
    target_prompt = generate_prompt(source_image, target_domain)
    # Generate a source prompt for the target image
    source_prompt = generate_prompt(target_image, source_domain)
    # Translate the source image to the target domain using DDS
    translated_image = edit_image(source_image, target_prompt, source_prompt)
    # Translate the translated image back to the source domain using DDS
    reconstructed_image = edit_image(translated_image, source_prompt, target_prompt)
    # Calculate the cycle-consistency loss
    cycle_loss = reconstruction_loss(source_image, reconstructed_image)
    # Calculate the identity loss for both domains
    identity_loss_source = identity_loss(source_image, edit_image(source_image, source_prompt, source_prompt))
    identity_loss_target = identity_loss(target_image, edit_image(target_image, target_prompt, target_prompt))
    # Calculate the total loss
    total_loss = cycle_loss + identity_loss_source + identity_loss_target
    # Update the model parameters using the total loss
    model.update(total_loss)
  # Return the trained model
  return model

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import random

# Define some hyperparameters
image_size = 256 # The size of the input and output images
num_channels = 3 # The number of channels in the images
num_classes = 10 # The number of classes in the text prompts
vocab_size = 1000 # The size of the vocabulary for the text prompts
embed_size = 128 # The size of the word embeddings for the text prompts
hidden_size = 256 # The size of the hidden state for the text encoder
num_layers = 4 # The number of layers for the text encoder and the score network
num_steps = 1000 # The number of steps for the diffusion process
beta_start = 1e-4 # The initial value of the noise variance for the diffusion process
beta_end = 0.02 # The final value of the noise variance for the diffusion process
step_size = 0.01 # The step size for the Langevin dynamics
num_iterations = 100 # The number of iterations for the image editing optimization
batch_size = 32 # The batch size for the training and inference
learning_rate = 1e-4 # The learning rate for the optimizer

# Define a function that calculates the beta schedule for the diffusion process
def beta_schedule(t):
  # Use a cosine annealing schedule from beta_start to beta_end
  return beta_start + (beta_end - beta_start) * (1 - np.cos(np.pi * t / num_steps)) / 2

# Define a function that adds Gaussian noise to an image according to a given noise level
def add_noise(image, noise_level):
  # Sample a Gaussian noise tensor with the same shape as the image
  noise = torch.randn_like(image)
  # Scale the noise by the square root of the noise level
  noise = noise * torch.sqrt(noise_level)
  # Add the noise to the image and clip the values to [0, 1]
  noisy_image = torch.clamp(image + noise, 0, 1)
  # Return the noisy image and the noise
  return noisy_image, noise

# Define a function that encodes a text prompt into a fixed-length vector using an LSTM encoder
def encode_text(prompt):
  # Initialize an LSTM encoder with vocab_size, embed_size, hidden_size and num_layers
  encoder = torch.nn.LSTM(vocab_size, embed_size, hidden_size, num_layers)
  # Convert the prompt into a sequence of word indices
  indices = torch.tensor([word_to_index(word) for word in prompt.split()])
  # Embed the indices into a sequence of word vectors
  embeddings = torch.nn.Embedding(vocab_size, embed_size)(indices)
  # Pass the embeddings through the encoder and get the final hidden state
  _, (hidden, _) = encoder(embeddings)
  # Concatenate the hidden states from all layers and return it as the encoded vector
  encoded = torch.cat(hidden, dim=-1)
  return encoded

# Define a text-to-image diffusion model that generates an image conditioned on a text prompt at each step of the diffusion process
class DiffusionModel(torch.nn.Module):
  
  def __init__(self):
    # Initialize the base class
    super(DiffusionModel, self).__init__()
    # Initialize a convolutional network that takes an image and a text vector as inputs and outputs an image
    self.network = ConvNet(num_channels + hidden_size * num_layers, num_channels)

  def forward(self, image, prompt, t):
    # Encode the prompt into a vector
    prompt_vector = encode_text(prompt)
    # Add Gaussian noise to the image according to the beta schedule
    noisy_image, noise = add_noise(image, beta_schedule(t))
    # Concatenate the noisy image and the prompt vector along the channel dimension
    input = torch.cat([noisy_image, prompt_vector], dim=-1)
    # Pass the input through the network and get an output image
    output = self.network(input)
    # Calculate and return the negative log-likelihood loss between the output and the noise
    loss = torch.nn.MSELoss()(output, noise)
    return loss

# Define a score network that estimates the gradient of the log-density of the diffusion model at any given noise level
class ScoreNetwork(torch.nn.Module):

  def __init__(self, diffusion_model):
    # Initialize the base class
    super(ScoreNetwork, self).__init__()
    # Store the diffusion model as an attribute
    self.diffusion_model = diffusion_model
    # Initialize a convolutional network that takes an image and a text vector as inputs and outputs a score vector
    self.network = ConvNet(num_channels + hidden_size * num_layers, num_channels)

  def forward(self, image, prompt, noise_level):
    # Encode the prompt into a vector
    prompt_vector = encode_text(prompt)
    # Concatenate the image and the prompt vector along the channel dimension
    input = torch.cat([image, prompt_vector], dim=-1)
    # Pass the input through the network and get a score vector
    score = self.network(input)
    # Scale the score by the inverse of the noise level
    score = score / noise_level
    # Return the score
    return score

# Define a function that calculates the Score Distillation Sampling (SDS) score for a given image and prompt
def SDS_score(image, prompt):
  # Start from a pure noise sample
  noise = torch.randn_like(image)
  # Apply Langevin dynamics guided by the score network and the prompt
  for t in range(num_steps):
    # Calculate the noise level for the current step
    noise_level = beta_schedule(t)
    # Calculate the score for the current noise sample and prompt
    score = score_network(noise, prompt, noise_level)
    # Update the noise sample using the score and the step size
    noise = noise + step_size * score + torch.sqrt(2 * step_size) * torch.randn_like(noise)
  # Return the final noise sample as the SDS score
  return noise

# Define a function that calculates the Delta Denoising Score (DDS) for a given image, target prompt and matching prompt
def DDS_score(image, target_prompt, matching_prompt):
  # Calculate the SDS score with the target prompt
  target_score = SDS_score(image, target_prompt)
  # Calculate the SDS score with the matching prompt
  matching_score = SDS_score(image, matching_prompt)
  # Return the difference between the two scores as the DDS score
  return target_score - matching_score

# Define a function that performs text-based image editing using DDS as a loss term
def edit_image(image, target_prompt, matching_prompt):
  # Initialize an optimizer
  optimizer = torch.optim.Adam([image], lr=learning_rate)
  # Optimize the image to minimize the DDS score
  for i in range(num_iterations):
    # Zero out the gradients
    optimizer.zero_grad()
    # Calculate the DDS score
    loss = DDS_score(image, target_prompt, matching_prompt)
    # Backpropagate the loss
    loss.backward()
    # Update the image using the optimizer
    optimizer.step()
  # Return the edited image
  return image

# Define a function that trains a zero-shot image translation model using DDS and cycle-consistency loss
def train_model(source_domain, target_domain):
  # Initialize a model
  model = DiffusionModel()
  # Initialize an optimizer
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  # Loop over the training data
  for source_image, target_image in data_loader:
    # Zero out the gradients
    optimizer.zero_grad()
    # Generate a target prompt for the source image
    target_prompt = generate_prompt(source_image, target_domain)
    # Generate a source prompt for the target image
    source_prompt = generate_prompt(target_image, source_domain)
    # Translate the source image to the target domain using DDS
    translated_image = edit_image(source_image, target_prompt, source_prompt)
    # Translate the translated image back to the source domain using DDS
    reconstructed_image = edit_image(translated_image, source_prompt, target_prompt)
    # Calculate the cycle-consistency loss between the source image and the reconstructed image
    cycle_loss = torch.nn.MSELoss()(source_image, reconstructed_image)
    # Calculate the identity loss for both domains by preventing changing an image when translating it to its own domain
    identity_loss_source = torch.nn.MSELoss()(source_image, edit_image(source_image, source_prompt, source_prompt))
    identity_loss_target = torch.nn.MSELoss()(target_image, edit_image(target_image, target_prompt, target_prompt))
    # Calculate the total loss as a weighted sum of the cycle-consistency loss and the identity loss
    total_loss = cycle_loss + identity_loss_source + identity_loss_target
    # Backpropagate the total loss
    total_loss.backward()
    # Update the model parameters using the optimizer
    optimizer.step()
  # Return the trained model
  return model

```