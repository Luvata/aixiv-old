---
title: 2203.11926v3 Focal Modulation Networks
date: 2022-03-12
---

# [Focal Modulation Networks](http://arxiv.org/abs/2203.11926v3)

authors: Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao


## What, Why and How

[1]: https://arxiv.org/abs/2203.11926 "[2203.11926] Focal Modulation Networks - arXiv.org"
[2]: https://arxiv.org/pdf/2203.11926v3.pdf "arXiv:2203.11926v3 [cs.CV] 5 Nov 2022"
[3]: http://export.arxiv.org/abs/2203.11926 "[2203.11926] Focal Modulation Networks - export.arxiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes focal modulation networks (FocalNets), where self-attention is replaced by a focal modulation module for modeling token interactions in vision.
- **Why**: The paper aims to address the limitations of self-attention, such as high computational cost, lack of interpretability, and sensitivity to noise and outliers.
- **How**: The paper introduces focal modulation, which consists of three components: (i) hierarchical contextualization, to encode visual contexts from short to long ranges using depth-wise convolutional layers, (ii) gated aggregation, to selectively gather contexts for each query token based on its content, and (iii) element-wise affine transformation, to inject the aggregated context into the query. The paper shows that focal modulation can achieve better performance and interpretability than self-attention on various vision tasks.

## Main Contributions

The paper claims the following contributions:

- It proposes focal modulation networks (FocalNets), a novel vision architecture that replaces self-attention with focal modulation for modeling token interactions.
- It demonstrates that focal modulation can encode rich and adaptive visual contexts with lower computational cost and higher interpretability than self-attention.
- It shows that FocalNets can outperform the state-of-the-art self-attention counterparts on image classification, object detection, and segmentation tasks, and achieve new records on COCO detection and ADE20K segmentation.

## Method Summary

The method section of the paper describes the focal modulation module and how it is integrated into the FocalNet architecture. The focal modulation module consists of three steps: focal contextualization, gated aggregation, and element-wise affine transformation. Focal contextualization encodes the visual context of each token at different levels of granularity using a stack of depth-wise convolutional layers. Gated aggregation computes a modulator for each query token by selectively gathering contexts from other tokens based on their similarity. Element-wise affine transformation injects the modulator into the query token by applying a learned scale and bias. The FocalNet architecture follows a hierarchical design similar to Swin Transformer, but replaces the self-attention layers with focal modulation layers. The paper also introduces some techniques to improve the performance and efficiency of FocalNets, such as patch merging, patch splitting, and patch embedding. The paper provides the details of the FocalNet variants and their configurations for different vision tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the focal modulation module
def focal_modulation(query, target):
  # Focal contextualization
  context = depthwise_conv(target) # Apply a stack of depth-wise convolutional layers to target tokens
  # Gated aggregation
  gate = softmax(query @ context.T) # Compute the similarity between query and context tokens
  modulator = gate @ context # Aggregate the context tokens weighted by the gate
  # Element-wise affine transformation
  scale = linear(modulator) # Learn a scale parameter for each modulator
  bias = linear(modulator) # Learn a bias parameter for each modulator
  output = scale * query + bias # Inject the modulator into the query token
  return output

# Define the FocalNet architecture
def FocalNet(input):
  # Patch embedding
  x = patch_embedding(input) # Divide the input image into patches and embed them into vectors
  # Patch merging and splitting
  x = patch_merging(x) # Merge neighboring patches into larger ones at certain stages
  x = patch_splitting(x) # Split larger patches into smaller ones at certain stages
  # Focal modulation layers
  for i in range(num_layers):
    x = focal_modulation(x, x) # Apply focal modulation to each token with itself as target
    x = mlp(x) # Apply a multi-layer perceptron to each token
    x = layer_norm(x) # Apply layer normalization to each token
  # Output head
  x = global_average_pooling(x) # Pool the tokens into a single vector
  x = linear(x) # Apply a linear layer to the vector
  return x
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # For tensor operations
import torch.nn as nn # For neural network modules
import torch.nn.functional as F # For activation functions and other utilities

# Define the focal modulation module
class FocalModulation(nn.Module):
  def __init__(self, dim, num_contexts):
    super().__init__()
    self.dim = dim # The dimension of the token embeddings
    self.num_contexts = num_contexts # The number of context levels
    self.context_layers = nn.ModuleList() # A list of depth-wise convolutional layers for encoding contexts
    self.scale_layers = nn.ModuleList() # A list of linear layers for learning scale parameters
    self.bias_layers = nn.ModuleList() # A list of linear layers for learning bias parameters
    for i in range(num_contexts):
      # Create a depth-wise convolutional layer with kernel size 2^(i+1) and stride 2^i
      self.context_layers.append(nn.Conv1d(dim, dim, kernel_size=2**(i+1), stride=2**i, groups=dim))
      # Create a linear layer for learning scale parameter
      self.scale_layers.append(nn.Linear(dim, dim))
      # Create a linear layer for learning bias parameter
      self.bias_layers.append(nn.Linear(dim, dim))

  def forward(self, query, target):
    # Focal contextualization
    context_list = [] # A list of context tensors at different levels
    for i in range(self.num_contexts):
      # Apply the i-th depth-wise convolutional layer to the target tokens along the sequence dimension
      context = self.context_layers[i](target.transpose(1, 2)).transpose(1, 2)
      # Append the context tensor to the list
      context_list.append(context)
    # Concatenate the context tensors along the sequence dimension
    context = torch.cat(context_list, dim=1)
    # Gated aggregation
    gate = F.softmax(query @ context.transpose(1, 2), dim=-1) # Compute the similarity between query and context tokens and apply softmax along the context dimension
    modulator = gate @ context # Aggregate the context tokens weighted by the gate
    # Element-wise affine transformation
    scale = torch.ones_like(query) # Initialize the scale tensor with ones
    bias = torch.zeros_like(query) # Initialize the bias tensor with zeros
    for i in range(self.num_contexts):
      # Apply the i-th linear layer to the modulator and add it to the scale tensor
      scale += self.scale_layers[i](modulator)
      # Apply the i-th linear layer to the modulator and add it to the bias tensor
      bias += self.bias_layers[i](modulator)
    output = scale * query + bias # Inject the modulator into the query token by applying element-wise multiplication and addition
    return output

# Define the patch embedding module
class PatchEmbedding(nn.Module):
  def __init__(self, image_size, patch_size, dim):
    super().__init__()
    self.image_size = image_size # The size of the input image (assumed to be square)
    self.patch_size = patch_size # The size of each patch (assumed to be square)
    self.dim = dim # The dimension of the patch embeddings
    self.num_patches = (image_size // patch_size) ** 2 # The number of patches in each image
    self.proj = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size) # A convolutional layer for projecting patches into vectors

  def forward(self, x):
    x = self.proj(x) # Apply the convolutional layer to the input image
    x = x.flatten(2).transpose(1, 2) # Reshape and transpose the output tensor to have shape (batch_size, num_patches, dim)
    return x

# Define the patch merging module
class PatchMerging(nn.Module):
  def __init__(self, dim):
    super().__init__()
    self.dim = dim # The dimension of the patch embeddings before merging
    self.proj = nn.Linear(2 * dim, 2 * dim) # A linear layer for projecting merged patches

  def forward(self, x):
    b, n, c = x.shape # Get the batch size, number of patches, and dimension from the input tensor
    assert n % 2 == 0 and c == self.dim # Check that n is even and c matches the expected dimension
    x = x.reshape(b, n // 2, 2 * c) # Reshape the input tensor to have shape (batch_size, num_patches // 2, 2 * dim)
    x = self.proj(x) # Apply the linear layer to the reshaped tensor
    return x

# Define the patch splitting module
class PatchSplitting(nn.Module):
  def __init__(self, dim):
    super().__init__()
    self.dim = dim # The dimension of the patch embeddings before splitting
    self.proj = nn.Linear(dim, 4 * dim) # A linear layer for projecting split patches

  def forward(self, x):
    b, n, c = x.shape # Get the batch size, number of patches, and dimension from the input tensor
    assert c == self.dim # Check that c matches the expected dimension
    x = self.proj(x) # Apply the linear layer to the input tensor
    x = x.reshape(b, n * 4, c // 4) # Reshape the output tensor to have shape (batch_size, num_patches * 4, dim // 4)
    return x

# Define the multi-layer perceptron module
class MLP(nn.Module):
  def __init__(self, dim, hidden_dim, dropout):
    super().__init__()
    self.fc1 = nn.Linear(dim, hidden_dim) # The first linear layer
    self.act = nn.GELU() # The activation function
    self.fc2 = nn.Linear(hidden_dim, dim) # The second linear layer
    self.drop = nn.Dropout(dropout) # The dropout layer

  def forward(self, x):
    x = self.fc1(x) # Apply the first linear layer
    x = self.act(x) # Apply the activation function
    x = self.drop(x) # Apply the dropout layer
    x = self.fc2(x) # Apply the second linear layer
    x = self.drop(x) # Apply the dropout layer
    return x

# Define the FocalNet architecture
class FocalNet(nn.Module):
  def __init__(self, image_size=224, patch_size=4, num_classes=1000, dim=96, depth=12, num_contexts=3, hidden_dim=384, dropout=0.1):
    super().__init__()
    self.num_classes = num_classes # The number of output classes
    self.num_layers = depth # The number of focal modulation layers
    self.patch_embed = PatchEmbedding(image_size, patch_size, dim) # The patch embedding module
    self.patch_merging_indices = [3, 7] # The indices of layers where patch merging is applied
    self.patch_splitting_indices = [9] # The indices of layers where patch splitting is applied
    self.focal_modulation_layers = nn.ModuleList() # A list of focal modulation modules
    self.mlp_layers = nn.ModuleList() # A list of multi-layer perceptron modules
    self.norm_layers = nn.ModuleList() # A list of layer normalization modules
    for i in range(depth):
      if i in self.patch_merging_indices:
        dim *= 2 # Double the dimension when patch merging is applied
      if i in self.patch_splitting_indices:
        dim //= 4 # Quarter the dimension when patch splitting is applied
      self.focal_modulation_layers.append(FocalModulation(dim, num_contexts)) # Create a focal modulation module with the current dimension and number of contexts
      self.mlp_layers.append(MLP(dim, hidden_dim, dropout)) # Create a multi-layer perceptron module with the current dimension and hidden dimension
      self.norm_layers.append(nn.LayerNorm(dim)) # Create a layer normalization module with the current dimension
    self.head = nn.Linear(dim, num_classes) # The output head for classification

  def forward(self, x):
    x = self.patch_embed(x) # Apply the patch embedding module to the input image
    for i in range(self.num_layers):
      if i in self.patch_merging_indices:
        x = PatchMerging(x.dim)(x) # Apply patch merging at certain layers
      if i in self.patch_splitting_indices:
        x = PatchSplitting(x.dim)(x) # Apply patch splitting at certain layers
      x = x + self.focal_modulation_layers[i](x, x) # Apply focal modulation to each token with itself as target and add residual connection
      x = x + self.mlp_layers[i](x) # Apply multi-layer perceptron to each token and add residual connection
      x = self.norm_layers[i](x) # Apply layer normalization to each token
    x = x.mean(dim=1) # Pool the tokens into a single vector by taking mean along the sequence dimension
    x = self.head(x) # Apply the output head to the vector
    return x

```