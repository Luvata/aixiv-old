---
title: 2306.06088v1 SENS  Sketch-based Implicit Neural Shape Modeling
date: 2023-06-07
---

# [SENS: Sketch-based Implicit Neural Shape Modeling](http://arxiv.org/abs/2306.06088v1)

authors: Alexandre Binninger, Amir Hertz, Olga Sorkine-Hornung, Daniel Cohen-Or, Raja Giryes


## What, Why and How

[1]: https://arxiv.org/abs/2306.06088v1 "[2306.06088v1] SENS: Sketch-based Implicit Neural Shape Modeling"
[2]: https://arxiv.org/abs/2306.05688 "[2306.05688] ModeT: Learning Deformable Image Registration via Motion ..."

Here is a summary of the paper:

- **What**: The paper presents SENS, a novel method for generating and editing 3D models from hand-drawn sketches, including those of an abstract nature.
- **Why**: The paper aims to provide a user-friendly and expressive way to create and manipulate 3D shapes using sketches, which can capture the user's intent and imagination better than existing methods.
- **How**: The paper proposes a part-aware neural implicit shape architecture that maps sketches into the latent space of 3D shapes. SENS analyzes the sketch and encodes its parts into ViT patch encoding, then feeds them into a transformer decoder that converts them to shape embeddings. SENS can then generate and edit 3D neural implicit shapes from the embeddings. The paper evaluates SENS on various datasets and compares it with state-of-the-art methods using objective metrics and a user study.[^1^][1]


## Main Contributions

According to the paper, the main contributions are:

- A novel sketch-based implicit neural shape modeling method that can generate and edit 3D shapes from hand-drawn sketches, even those of an abstract nature.
- A part-aware neural implicit shape architecture that leverages ViT patch encoding and a transformer decoder to map sketches into the latent space of 3D shapes.
- A comprehensive evaluation of SENS on various datasets and tasks, showing its superiority over state-of-the-art methods in terms of sketch-based generation and editing, as well as user preference and satisfaction.


## Method Summary

The method section of the paper describes the proposed SENS framework in detail. It consists of three main components: sketch analysis, sketch-to-shape mapping, and shape generation and editing.

- Sketch analysis: This component takes a hand-drawn sketch as input and extracts its parts using a pre-trained part segmentation network. Then, it encodes each part into a ViT patch encoding, which is a fixed-length vector representation that captures the local and global features of the part.
- Sketch-to-shape mapping: This component takes the ViT patch encodings of the sketch parts and feeds them into a transformer decoder, which outputs shape embeddings for each part. The shape embeddings are learned representations that encode the 3D shape information of the corresponding parts. The transformer decoder is trained to map sketches to shapes using a large-scale dataset of paired sketches and shapes.
- Shape generation and editing: This component takes the shape embeddings and uses them to generate and edit 3D neural implicit shapes. A neural implicit shape is a continuous function that maps a 3D point to an occupancy value, indicating whether the point is inside or outside the shape. The paper uses a part-aware neural implicit shape architecture that consists of a global shape network and multiple part networks. The global shape network takes the shape embeddings as input and outputs a global occupancy value for each point. The part networks take the shape embeddings and the point coordinates as input and output part-specific occupancy values for each point. The final occupancy value is obtained by combining the global and part occupancy values using a competitive weighting module (CWM), which learns to assign weights to different parts based on their relevance to the point. The paper also introduces a sketch-based editing interface that allows users to manipulate the shape embeddings using sketch strokes, and then update the 3D shapes accordingly.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a hand-drawn sketch S
# Output: a 3D neural implicit shape F

# Sketch analysis
P = part_segmentation(S) # segment S into parts P
E = [] # initialize an empty list of ViT patch encodings
for p in P:
  e = ViT_patch_encoding(p) # encode p into a ViT patch encoding e
  E.append(e) # add e to E

# Sketch-to-shape mapping
Z = transformer_decoder(E) # map E to shape embeddings Z

# Shape generation and editing
F = part_aware_neural_implicit_shape(Z) # generate a neural implicit shape F from Z
while user wants to edit F:
  S' = sketch_editing(S, Z) # get a new sketch S' from user based on S and Z
  P' = part_segmentation(S') # segment S' into parts P'
  E' = [] # initialize an empty list of ViT patch encodings
  for p' in P':
    e' = ViT_patch_encoding(p') # encode p' into a ViT patch encoding e'
    E'.append(e') # add e' to E'
  Z' = transformer_decoder(E') # map E' to shape embeddings Z'
  F' = part_aware_neural_implicit_shape(Z') # generate a new neural implicit shape F' from Z'
  F = F' # update F to F'
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a hand-drawn sketch S
# Output: a 3D neural implicit shape F

# Sketch analysis
P = part_segmentation(S) # segment S into parts P using a pre-trained U-Net
E = [] # initialize an empty list of ViT patch encodings
for p in P:
  e = ViT_patch_encoding(p) # encode p into a ViT patch encoding e using a pre-trained ViT model
  E.append(e) # add e to E

# Sketch-to-shape mapping
Z = transformer_decoder(E) # map E to shape embeddings Z using a transformer decoder with 6 layers and 8 heads
# The transformer decoder is trained on a large-scale dataset of paired sketches and shapes using a reconstruction loss and a perceptual loss

# Shape generation and editing
F = part_aware_neural_implicit_shape(Z) # generate a neural implicit shape F from Z using a part-aware neural implicit shape architecture
# The part-aware neural implicit shape architecture consists of a global shape network and multiple part networks
# The global shape network takes Z as input and outputs a global occupancy value for each point using a fully connected network with ReLU activations
# The part networks take Z and the point coordinates as input and output part-specific occupancy values for each point using fully connected networks with ReLU activations
# The final occupancy value is obtained by combining the global and part occupancy values using a competitive weighting module (CWM), which learns to assign weights to different parts based on their relevance to the point using softmax
# The part-aware neural implicit shape architecture is trained on the same dataset as the transformer decoder using a binary cross-entropy loss and an L2 regularization term

while user wants to edit F:
  S' = sketch_editing(S, Z) # get a new sketch S' from user based on S and Z using a sketch-based editing interface
  # The sketch-based editing interface allows users to draw sketch strokes on S to manipulate Z
  # The sketch strokes are classified into three types: add, remove, and move
  # The add strokes are used to add new parts or details to the shape
  # The remove strokes are used to erase existing parts or details from the shape
  # The move strokes are used to translate, rotate, or scale existing parts of the shape
  # The sketch strokes are processed by a stroke analysis module, which extracts their features and labels using convolutional neural networks and random forests
  # The stroke features and labels are then used to update Z using an embedding update module, which applies different operations to Z based on the stroke types using fully connected networks with ReLU activations
  
  P' = part_segmentation(S') # segment S' into parts P' using the same U-Net as before
  E' = [] # initialize an empty list of ViT patch encodings
  for p' in P':
    e' = ViT_patch_encoding(p') # encode p' into a ViT patch encoding e' using the same ViT model as before
    E'.append(e') # add e' to E'
  Z' = transformer_decoder(E') # map E' to shape embeddings Z' using the same transformer decoder as before
  F' = part_aware_neural_implicit_shape(Z') # generate a new neural implicit shape F' from Z' using the same part-aware neural implicit shape architecture as before
  F = F' # update F to F'
```