---
title: 2108.01073v2 SDEdit  Guided Image Synthesis and Editing with Stochastic Differential Equations
date: 2021-08-02
---

# [SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations](http://arxiv.org/abs/2108.01073v2)

authors: Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon


## What, Why and How

[1]: https://arxiv.org/abs/2108.01073 "[2108.01073] SDEdit: Guided Image Synthesis and Editing with Stochastic ..."
[2]: https://arxiv.org/pdf/2108.01073v2 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2201.01073v2 "[2201.01073v2] Towards Unsupervised Open World Semantic Segmentation"

Here is a summary of the paper at [^1^][1]:

**What**: The paper introduces a new image synthesis and editing method, **Stochastic Differential Editing (SDEdit)**, based on a diffusion model generative prior.

**Why**: The paper aims to address the key challenge of balancing **faithfulness** to the user input (e.g., hand-drawn colored strokes) and **realism** of the synthesized image. Existing GAN-based methods are challenging and often require additional training data or loss functions for individual applications.

**How**: The paper proposes to synthesize realistic images by iteratively denoising through a **stochastic differential equation (SDE)**. Given an input image with user guide of any type, SDEdit first adds noise to the input, then subsequently denoises the resulting image through the SDE prior to increase its realism. SDEdit does not require task-specific training or inversions and can naturally achieve the balance between realism and faithfulness. SDEdit significantly outperforms state-of-the-art GAN-based methods on multiple tasks, according to a human perception study.

## Main Contributions

[1]: https://arxiv.org/abs/2108.01073 "[2108.01073] SDEdit: Guided Image Synthesis and Editing with Stochastic ..."
[2]: https://arxiv.org/pdf/2108.01073v2 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2201.01073v2 "[2201.01073v2] Towards Unsupervised Open World Semantic Segmentation"

According to the paper at [^1^][1], the main contributions are:

- They propose **SDEdit**, a new image synthesis and editing method based on a diffusion model generative prior, which synthesizes realistic images by iteratively denoising through a stochastic differential equation (SDE).
- They show that SDEdit can handle various types of user guides, such as strokes, colors, masks, and sketches, and can perform multiple tasks, such as stroke-based image synthesis and editing, image compositing, and image inpainting.
- They conduct extensive experiments and a human perception study to demonstrate that SDEdit significantly outperforms state-of-the-art GAN-based methods on realism and overall satisfaction scores.

## Method Summary

[1]: https://arxiv.org/abs/2108.01073 "[2108.01073] SDEdit: Guided Image Synthesis and Editing with Stochastic ..."
[2]: https://arxiv.org/pdf/2108.01073v2 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2201.01073v2 "[2201.01073v2] Towards Unsupervised Open World Semantic Segmentation"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first reviews the diffusion model generative prior, which is a generative model that learns to synthesize images by reversing a stochastic differential equation (SDE) that gradually adds noise to the data distribution.
- The paper then introduces the Stochastic Differential Editing (SDEdit) framework, which consists of two steps: noise injection and denoising. In the noise injection step, SDEdit adds noise to the input image with user guide to match the noise level of a target timestep in the SDE. In the denoising step, SDEdit applies the diffusion model generative prior to denoise the noisy image and increase its realism.
- The paper also describes how SDEdit can handle different types of user guides, such as strokes, colors, masks, and sketches, by using different strategies to inject noise and denoise. For example, for stroke-based image synthesis and editing, SDEdit uses a stroke mask to inject noise only to the regions outside the strokes and uses a conditional diffusion model to denoise while preserving the stroke information. For image compositing, SDEdit uses a foreground mask to inject noise only to the foreground region and uses an unconditional diffusion model to denoise while blending the foreground and background seamlessly.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: an image x with user guide g of any type
# Output: a realistic image y that satisfies the user guide g

# Step 1: Noise injection
# Find the target timestep T in the SDE that matches the noise level of x
T = find_timestep(x)
# Add noise to x according to the type of g and T
x_noisy = add_noise(x, g, T)

# Step 2: Denoising
# Apply the diffusion model generative prior to denoise x_noisy
y = denoise(x_noisy, g, T)
# Return y as the output image
return y
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: an image x with user guide g of any type
# Output: a realistic image y that satisfies the user guide g

# Step 1: Noise injection
# Find the target timestep T in the SDE that matches the noise level of x
# Use the mean squared error (MSE) between x and x_noisy as the criterion
T = 0
min_mse = inf
for t in range(1, N): # N is the total number of timesteps in the SDE
  # Sample a noisy image x_noisy from the SDE at timestep t
  x_noisy = sample_from_SDE(x, t)
  # Compute the MSE between x and x_noisy
  mse = compute_MSE(x, x_noisy)
  # Update T and min_mse if mse is smaller
  if mse < min_mse:
    T = t
    min_mse = mse

# Add noise to x according to the type of g and T
# Use different strategies for different types of g
if g is stroke: # g is a hand-drawn colored stroke
  # Use a stroke mask m to indicate the regions inside and outside the stroke
  m = create_stroke_mask(g)
  # Add noise only to the regions outside the stroke
  x_noisy = x * m + sample_from_SDE(x, T) * (1 - m)
elif g is color: # g is a color palette or a color histogram
  # Use a color transfer method to transfer the color of g to x
  x_color = color_transfer(x, g)
  # Add noise to the whole image
  x_noisy = sample_from_SDE(x_color, T)
elif g is mask: # g is a foreground mask for image compositing
  # Use a foreground mask m to indicate the regions inside and outside the foreground
  m = g
  # Add noise only to the foreground region
  x_noisy = x * (1 - m) + sample_from_SDE(x, T) * m
elif g is sketch: # g is a hand-drawn sketch
  # Use a sketch mask m to indicate the regions inside and outside the sketch
  m = create_sketch_mask(g)
  # Add noise only to the regions outside the sketch
  x_noisy = x * m + sample_from_SDE(x, T) * (1 - m)

# Step 2: Denoising
# Apply the diffusion model generative prior to denoise x_noisy
# Use different diffusion models for different types of g
if g is stroke or g is color: # use a conditional diffusion model that takes g as input
  y = conditional_denoise(x_noisy, g, T)
elif g is mask or g is sketch: # use an unconditional diffusion model that does not take g as input
  y = unconditional_denoise(x_noisy, T)
# Return y as the output image
return y

```