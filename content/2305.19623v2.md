---
title: 2305.19623v2 Point-GCC  Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast
date: 2023-05-20
---

# [Point-GCC: Universal Self-supervised 3D Scene Pre-training via Geometry-Color Contrast](http://arxiv.org/abs/2305.19623v2)

authors: Guofan Fan, Zekun Qi, Wenkai Shi, Kaisheng Ma


## What, Why and How

[1]: https://arxiv.org/pdf/2305.19623v2.pdf "arXiv:2305.19623v2 [cs.CV] 1 Jun 2023"
[2]: https://arxiv-export-lb.library.cornell.edu/abs/2305.19623v2 "[2305.19623v2] Point-GCC: Universal Self-supervised 3D Scene Pre ..."
[3]: https://arxiv-export3.library.cornell.edu/abs/2305.19623 "[2305.19623] Point-GCC: Universal Self-supervised 3D Scene Pre-training ..."

Here is a summary of the paper:

- **What**: The paper proposes a universal self-supervised 3D scene pre-training framework via Geometry-Color Contrast (Point-GCC), which aligns geometry and color information using a Siamese network.
- **Why**: The paper aims to better utilize the relations of point cloud information, which are crucial for 3D scene understanding, but existing methods lack an elaborate design for the discrimination and relevance of geometry and color information.
- **How**: The paper designs (i) hierarchical supervision with point-level contrast and reconstruct and object-level contrast based on the novel deep clustering module to close the gap between pre-training and downstream tasks; (ii) architecture-agnostic backbone to adapt for various downstream models. The paper evaluates the model performance on a wide range of tasks and shows consistent improvements across all datasets. e.g., new state-of-the-art object detection results on SUN RGB-D and S3DIS datasets.

## Main Contributions

The paper claims the following contributions:

- A novel 3D self-supervised paradigm that can better utilize the relations of point cloud information via Geometry-Color Contrast (Point-GCC).
- A hierarchical supervision scheme that combines point-level contrast and reconstruct and object-level contrast based on the novel deep clustering module to close the gap between pre-training and downstream tasks.
- An architecture-agnostic backbone that can adapt for various downstream models and achieve state-of-the-art results on multiple 3D scene understanding tasks.

## Method Summary

[1]: https://arxiv.org/pdf/2305.19623v2.pdf "arXiv:2305.19623v2 [cs.CV] 1 Jun 2023"
[2]: https://arxiv-export-lb.library.cornell.edu/abs/2305.19623v2 "[2305.19623v2] Point-GCC: Universal Self-supervised 3D Scene Pre ..."
[3]: https://arxiv-export3.library.cornell.edu/abs/2305.19623 "[2305.19623] Point-GCC: Universal Self-supervised 3D Scene Pre-training ..."

Here is a summary of the method section of the paper:

- The paper proposes a universal 3D scene pre-training framework via Geometry-Color Contrast (Point-GCC), which aligns geometry and color information using a Siamese network.
- The paper designs a hierarchical supervision scheme that consists of three losses: point-level contrast loss, point-level reconstruct loss, and object-level contrast loss.
- The point-level contrast loss encourages the model to learn discriminative representations for each point by contrasting positive and negative pairs of points based on their geometry and color information.
- The point-level reconstruct loss encourages the model to learn complementary representations for each point by reconstructing its geometry and color information from the other branch of the Siamese network.
- The object-level contrast loss encourages the model to learn cluster-aware representations for each object by contrasting positive and negative pairs of objects based on their semantic labels.
- The paper introduces a novel deep clustering module that can automatically generate pseudo labels for objects based on their point-level representations and cluster centers.
- The paper adopts an architecture-agnostic backbone that can be easily plugged into various downstream models without changing their architectures or hyperparameters. The backbone consists of two branches: one for geometry encoding and one for color encoding. Each branch has four stages: feature extraction, feature aggregation, feature transformation, and feature fusion.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a point cloud P with N points
# Output: a point-level representation Z with N vectors

# Initialize the Siamese network with two branches: G and C
# Initialize the deep clustering module with K cluster centers

# Encode the geometry and color information of P using G and C
X_g = G(P) # X_g is a matrix of N x D_g
X_c = C(P) # X_c is a matrix of N x D_c

# Concatenate X_g and X_c to get the point-level representation Z
Z = concatenate(X_g, X_c) # Z is a matrix of N x (D_g + D_c)

# Compute the point-level contrast loss L_p using Z and a contrastive learning objective
L_p = contrastive_loss(Z)

# Compute the point-level reconstruct loss L_r using X_g and X_c and a reconstruction objective
L_r = reconstruction_loss(X_g, X_c)

# Generate pseudo labels for objects using the deep clustering module and Z
O = deep_clustering(Z) # O is a vector of N integers in [1, K]

# Compute the object-level contrast loss L_o using O and a contrastive learning objective
L_o = contrastive_loss(O)

# Compute the total loss L as a weighted sum of L_p, L_r, and L_o
L = alpha * L_p + beta * L_r + gamma * L_o

# Update the Siamese network and the deep clustering module using gradient descent on L
update(G, C, deep_clustering, L)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the hyperparameters
D_g = 64 # the dimension of geometry features
D_c = 64 # the dimension of color features
K = 16 # the number of clusters
alpha = 1.0 # the weight of point-level contrast loss
beta = 0.5 # the weight of point-level reconstruct loss
gamma = 0.1 # the weight of object-level contrast loss
tau = 0.07 # the temperature parameter for contrastive learning
batch_size = 32 # the batch size for training
num_epochs = 100 # the number of epochs for training
learning_rate = 0.01 # the learning rate for training

# Define the Siamese network with two branches: G and C
class SiameseNetwork(nn.Module):
    def __init__(self):
        super(SiameseNetwork, self).__init__()
        # Define the geometry branch G
        self.G = nn.Sequential(
            # Feature extraction stage: use PointNet [1] to extract local and global features from point coordinates
            PointNetEncoder(D_g),
            # Feature aggregation stage: use max pooling to aggregate global features
            nn.AdaptiveMaxPool1d(1),
            # Feature transformation stage: use a fully connected layer to transform global features
            nn.Linear(D_g, D_g),
            # Feature fusion stage: use a fully connected layer to fuse global and local features
            nn.Linear(2 * D_g, D_g)
        )
        # Define the color branch C
        self.C = nn.Sequential(
            # Feature extraction stage: use PointNet [1] to extract local and global features from point colors
            PointNetEncoder(D_c),
            # Feature aggregation stage: use max pooling to aggregate global features
            nn.AdaptiveMaxPool1d(1),
            # Feature transformation stage: use a fully connected layer to transform global features
            nn.Linear(D_c, D_c),
            # Feature fusion stage: use a fully connected layer to fuse global and local features
            nn.Linear(2 * D_c, D_c)
        )

    def forward(self, P):
        # Input: a point cloud P with N points and 6 channels (x, y, z, r, g, b)
        # Output: a point-level representation Z with N vectors of (D_g + D_c) dimensions

        # Split P into point coordinates and point colors
        P_xyz = P[:, :3] # P_xyz is a matrix of N x 3
        P_rgb = P[:, 3:] # P_rgb is a matrix of N x 3

        # Encode the geometry and color information of P using G and C
        X_g = self.G(P_xyz) # X_g is a matrix of N x D_g
        X_c = self.C(P_rgb) # X_c is a matrix of N x D_c

        # Concatenate X_g and X_c to get the point-level representation Z
        Z = torch.cat((X_g, X_c), dim=1) # Z is a matrix of N x (D_g + D_c)

        return Z, X_g, X_c

# Define the deep clustering module with K cluster centers
class DeepClustering(nn.Module):
    def __init__(self):
        super(DeepClustering, self).__init__()
        # Initialize the cluster centers randomly from a standard normal distribution
        self.cluster_centers = nn.Parameter(torch.randn(K, D_g + D_c))

    def forward(self, Z):
        # Input: a point-level representation Z with N vectors of (D_g + D_c) dimensions
        # Output: a pseudo label vector O with N integers in [1, K]

        # Compute the cosine similarity between Z and cluster centers
        S = F.cosine_similarity(Z.unsqueeze(1), self.cluster_centers.unsqueeze(0), dim=2) # S is a matrix of N x K

        # Assign each point to the nearest cluster center based on similarity
        O = torch.argmax(S, dim=1) + 1 # O is a vector of N integers in [1, K]

        return O

# Define the contrastive learning objective using InfoNCE [2]
def contrastive_loss(X):
    # Input: a matrix X with B x N vectors of (D_g + D_c) dimensions, where B is the batch size and N is the number of points per point cloud
    # Output: a scalar loss value

    # Normalize X to have unit norm along the last dimension
    X = F.normalize(X, dim=-1)

    # Compute the dot product between all pairs of vectors in X
    P = torch.matmul(X, X.transpose(1, 2)) # P is a matrix of B x N x N

    # Divide P by the temperature parameter tau
    P = P / tau

    # Compute the numerator of the InfoNCE loss as the sum of the diagonal elements of P
    num = torch.sum(torch.diagonal(P, dim1=1, dim2=2)) # num is a scalar

    # Compute the denominator of the InfoNCE loss as the sum of the exponential of all elements of P
    den = torch.sum(torch.exp(P)) # den is a scalar

    # Compute the InfoNCE loss as the negative log of the ratio of num and den
    loss = -torch.log(num / den) # loss is a scalar

    return loss

# Define the reconstruction objective using mean squared error (MSE)
def reconstruction_loss(X_g, X_c):
    # Input: two matrices X_g and X_c with N vectors of D_g and D_c dimensions, respectively
    # Output: a scalar loss value

    # Reconstruct X_g from X_c using a fully connected layer
    X_g_hat = nn.Linear(D_c, D_g)(X_c) # X_g_hat is a matrix of N x D_g

    # Reconstruct X_c from X_g using a fully connected layer
    X_c_hat = nn.Linear(D_g, D_c)(X_g) # X_c_hat is a matrix of N x D_c

    # Compute the MSE loss between X_g and X_g_hat
    loss_g = F.mse_loss(X_g, X_g_hat) # loss_g is a scalar

    # Compute the MSE loss between X_c and X_c_hat
    loss_c = F.mse_loss(X_c, X_c_hat) # loss_c is a scalar

    # Compute the total reconstruction loss as the sum of loss_g and loss_c
    loss = loss_g + loss_c # loss is a scalar

    return loss

# Create an instance of the Siamese network
siamese_network = SiameseNetwork()

# Create an instance of the deep clustering module
deep_clustering = DeepClustering()

# Create an optimizer for updating the parameters of the Siamese network and the deep clustering module
optimizer = optim.Adam(list(siamese_network.parameters()) + list(deep_clustering.parameters()), lr=learning_rate)

# Loop over the number of epochs
for epoch in range(num_epochs):
    # Loop over the batches of point clouds
    for batch in dataloader:
        # Get a batch of point clouds with B x N x 6 dimensions
        P = batch["point_cloud"]

        # Encode the point clouds using the Siamese network and get the point-level representation Z and the intermediate features X_g and X_c
        Z, X_g, X_c = siamese_network(P)

        # Compute the point-level contrast loss L_p using Z and the contrastive learning objective
        L_p = contrastive_loss(Z)

        # Compute the point-level reconstruct loss L_r using X_g and X_c and the reconstruction objective
        L_r = reconstruction_loss(X_g, X_c)

        # Generate pseudo labels for objects using the deep clustering module and Z
        O = deep_clustering(Z)

        # Compute the object-level contrast loss L_o using O and the contrastive learning objective
        L_o = contrastive_loss(O)

        # Compute the total loss L as a weighted sum of L_p, L_r, and L_o
        L = alpha * L_p + beta * L_r + gamma * L_o

        # Update the Siamese network and the deep clustering module using gradient descent on L
        optimizer.zero_grad()
        L.backward()
        optimizer.step()

# References:
# [1] Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660).
# [2] Oord, A. V. D., Li, Y., & Vinyals, O. (2018). Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748.
```