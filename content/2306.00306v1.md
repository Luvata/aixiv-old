---
title: 2306.00306v1 Low-Light Image Enhancement with Wavelet-based Diffusion Models
date: 2023-06-01
---

# [Low-Light Image Enhancement with Wavelet-based Diffusion Models](http://arxiv.org/abs/2306.00306v1)

authors: Hai Jiang, Ao Luo, Songchen Han, Haoqiang Fan, Shuaicheng Liu


## What, Why and How

[1]: https://arxiv.org/abs/2306.00306 "Low-Light Image Enhancement with Wavelet-based Diffusion Models"
[2]: https://arxiv.org/abs/2306.00068 "[2306.00068] A Candidate Dual QSO at Cosmic Noon - arXiv.org"
[3]: http://export.arxiv.org/pdf/2306.00306 "arXiv:2306.00306v1 [cs.CV] 1 Jun 2023"

Here is a summary of the paper:

- **What**: The paper proposes a novel method for low-light image enhancement based on wavelet-based conditional diffusion models (WCDM) and high-frequency restoration module (HFRM).
- **Why**: The paper aims to address the issues of existing diffusion-based methods, such as time-consuming inference, excessive computational resource consumption, and unstable restoration.
- **How**: The paper leverages the generative power of diffusion models and the advantages of wavelet transformation to produce high-quality results with satisfactory perceptual fidelity. The paper also performs both forward diffusion and reverse denoising in the training phase of WCDM to achieve stable denoising and reduce randomness during inference. Moreover, the paper designs a HFRM that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration.

The summary is based on the abstract and introduction of the paper[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2306.00306 "Low-Light Image Enhancement with Wavelet-based Diffusion Models"
[2]: https://arxiv.org/abs/2306.00068 "[2306.00068] A Candidate Dual QSO at Cosmic Noon - arXiv.org"
[3]: http://export.arxiv.org/pdf/2306.00306 "arXiv:2306.00306v1 [cs.CV] 1 Jun 2023"

According to the paper, the main contributions are:

- **A wavelet-based conditional diffusion model (WCDM)** that combines the advantages of diffusion models and wavelet transformation to enhance low-light images with high perceptual quality and low computational cost.
- **A high-frequency restoration module (HFRM)** that exploits the vertical and horizontal details of the image to improve the fine-grained restoration of the diagonal information.
- **A training scheme** that performs both forward diffusion and reverse denoising to stabilize the denoising process and reduce the randomness during inference.
- **Extensive experiments** that demonstrate the superiority of the proposed method over existing state-of-the-art methods in terms of quality, efficiency, and practical value.

The contributions are based on section 1.2 of the paper[^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/2306.00306 "Low-Light Image Enhancement with Wavelet-based Diffusion Models"
[2]: https://arxiv.org/abs/2306.00068 "[2306.00068] A Candidate Dual QSO at Cosmic Noon - arXiv.org"
[3]: http://export.arxiv.org/pdf/2306.00306 "arXiv:2306.00306v1 [cs.CV] 1 Jun 2023"

Here is a summary of the method section of the paper:

- The paper introduces the **wavelet-based conditional diffusion model (WCDM)**, which consists of two components: a wavelet transformation module and a conditional diffusion module.
- The wavelet transformation module applies the discrete wavelet transformation (DWT) to decompose the input image into four sub-bands: low-low (LL), low-high (LH), high-low (HL), and high-high (HH). The LL sub-band contains the coarse information of the image, while the other three sub-bands contain the fine details in different directions.
- The conditional diffusion module takes the LL sub-band as input and generates a series of intermediate images with increasing quality by applying a series of stochastic transitions. The transitions are conditioned on a noise schedule and a learned score network that predicts the gradient of the log-likelihood of the data distribution. The score network is trained by minimizing the reverse KL divergence between the model distribution and the data distribution.
- The paper also introduces the **high-frequency restoration module (HFRM)**, which aims to restore the fine details of the image that are lost or distorted by the wavelet transformation and diffusion process. The HFRM takes the LH, HL, and HH sub-bands as input and applies a series of convolutional layers to enhance their contrast and sharpness. The HFRM also uses a skip connection to preserve the original information of the sub-bands. The output of the HFRM is then combined with the output of the WCDM by applying the inverse discrete wavelet transformation (IDWT) to obtain the final enhanced image.
- The paper also describes the **training scheme** of the proposed method, which involves both forward diffusion and reverse denoising. Forward diffusion is used to generate noisy images from clean images by applying random noise injections according to a noise schedule. Reverse denoising is used to train the score network to predict the score function that can guide the diffusion process from noisy images to clean images. The paper uses an annealed noise schedule that gradually reduces the noise level during training. The paper also uses a data augmentation technique that randomly crops and flips the images to increase their diversity.

The summary is based on section 3 of the paper[^1^][1].

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import pywt # for wavelet transformation
import numpy as np

# Define the hyperparameters
T = 1000 # number of diffusion steps
beta_min = 1e-5 # minimum noise level
beta_max = 0.02 # maximum noise level
sigma_min = np.sqrt(beta_min) # minimum standard deviation of noise
sigma_max = np.sqrt(beta_max) # maximum standard deviation of noise
alpha = 0.001 # learning rate

# Define the wavelet-based conditional diffusion model (WCDM)
class WCDM(nn.Module):
    def __init__(self):
        super(WCDM, self).__init__()
        # Define the score network
        self.score_net = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 1, 3, padding=1)
        )
    
    def forward(self, x):
        # Apply the discrete wavelet transformation (DWT) to decompose x into four sub-bands
        coeffs = pywt.dwt2(x, 'haar')
        LL, (LH, HL, HH) = coeffs
        
        # Initialize the intermediate image z_0 as LL
        z = LL
        
        # Loop over the diffusion steps from t = 0 to t = T-1
        for t in range(T):
            # Compute the noise level beta_t according to the annealed noise schedule
            beta_t = beta_min + (beta_max - beta_min) * (t / (T - 1)) ** 2
            
            # Compute the standard deviation of noise sigma_t
            sigma_t = np.sqrt(beta_t)
            
            # Sample a random noise epsilon_t from N(0, I)
            epsilon_t = torch.randn_like(z)
            
            # Compute the score function s_t(z_t) using the score network
            s_t = self.score_net(z)
            
            # Compute the next intermediate image z_{t+1} using the reverse diffusion process
            z = (z - alpha * s_t + np.sqrt(2 * alpha) * epsilon_t) / (1 + alpha)
            
            # Clip z_{t+1} to the range [0, 1]
            z = torch.clamp(z, 0, 1)
        
        # Return the final intermediate image z_T as the output of WCDM
        return z

# Define the high-frequency restoration module (HFRM)
class HFRM(nn.Module):
    def __init__(self):
        super(HFRM, self).__init__()
        # Define the convolutional layers
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv3 = nn.Conv2d(64, 3, 3, padding=1)
    
    def forward(self, LH, HL, HH):
        # Concatenate the three sub-bands along the channel dimension
        x = torch.cat([LH, HL, HH], dim=1)
        
        # Apply the first convolutional layer and ReLU activation
        x = self.conv1(x)
        x = nn.ReLU()(x)
        
        # Apply the second convolutional layer and ReLU activation
        x = self.conv2(x)
        x = nn.ReLU()(x)
        
        # Apply the third convolutional layer and skip connection
        x = self.conv3(x) + torch.cat([LH, HL, HH], dim=1)
        
        # Return the output of HFRM as a tuple of three sub-bands
        return torch.split(x, 1, dim=1)

# Define the loss function as the mean squared error (MSE) between the enhanced image and the ground truth image
def loss_function(y_pred, y_true):
    return torch.mean((y_pred - y_true) ** 2)

# Instantiate the WCDM and HFRM models
wcdm = WCDM()
hfrm = HFRM()

# Define an optimizer for updating the parameters of both models
optimizer = torch.optim.Adam(list(wcdm.parameters()) + list(hfrm.parameters()), lr=alpha)

# Loop over the training epochs
for epoch in range(num_epochs):
    # Loop over the training batches
    for x, y in train_loader:
        # Apply data augmentation by randomly cropping and flipping x and y
        x = random_crop_and_flip(x)
        y = random_crop_and_flip(y)
        
        # Apply forward diffusion to generate noisy images x_t from clean images x
        x_t = forward_diffusion(x)
        
        # Feed the noisy images x_t to the WCDM model and get the output z_T
        z_T = wcdm(x_t)
        
        # Apply the inverse discrete wavelet transformation (IDWT) to reconstruct the enhanced image y_pred from z_T and the sub-bands of x
        coeffs_x = pywt.dwt2(x, 'haar')
        LL_x, (LH_x, HL_x, HH_x) = coeffs_x
        y_pred = pywt.idwt2((z_T, (LH_x, HL_x, HH_x)), 'haar')
        
        # Feed the sub-bands of x to the HFRM model and get the output (LH_y, HL_y, HH_y)
        LH_y, HL_y, HH_y = hfrm(LH_x, HL_x, HH_x)
        
        # Apply the IDWT again to reconstruct the enhanced image y_pred_hf from z_T and the output of HFRM
        y_pred_hf = pywt.idwt2((z_T, (LH_y, HL_y, HH_y)), 'haar')
        
        # Compute the loss as the sum of the MSE between y_pred and y and the MSE between y_pred_hf and y
        loss = loss_function(y_pred, y) + loss_function(y_pred_hf, y)
        
        # Perform backpropagation and update the parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    # Print the epoch and the loss
    print(f"Epoch {epoch}, Loss {loss.item()}")
```