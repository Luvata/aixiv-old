---
title: 2303.14302v2 VILA  Learning Image Aesthetics from User Comments with Vision-Language Pretraining
date: 2023-03-15
---

# [VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining](http://arxiv.org/abs/2303.14302v2)

authors: Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milanfar, Feng Yang


## What, Why and How

[1]: https://arxiv.org/pdf/2303.14302v2.pdf "VILA: Learning Image Aesthetics from User Comments with ... - arXiv.org"
[2]: https://arxiv.org/abs/2303.14302 "[2303.14302] VILA: Learning Image Aesthetics from User Comments with ..."
[3]: https://arxiv.org/pdf/2203.14302v2.pdf "arXiv:2203.14302v2 [quant-ph] 25 Aug 2022"
[4]: http://arxiv-export2.library.cornell.edu/abs/2303.14302v2 "[2303.14302v2] VILA: Learning Image Aesthetics from User Comments with ..."

Here is a summary of the paper:

- **What**: The paper proposes a vision-language aesthetics learning framework called VILA, which learns image aesthetics from user comments with vision-language pretraining.
- **Why**: The paper aims to address the limitations of existing image aesthetic assessment (IAA) methods that rely on human-labeled rating scores, which oversimplify the visual aesthetic information that humans perceive. User comments offer more comprehensive and natural information about image aesthetics, and can be leveraged to learn multimodal aesthetic representations without human labels.
- **How**: The paper pretrains an image-text encoder-decoder model with image-comment pairs, using contrastive and generative objectives to learn rich and generic aesthetic semantics. The paper also proposes a lightweight rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept for downstream IAA tasks. The paper evaluates the model on image aesthetic captioning, zero-shot style classification, zero-shot IAA, and IAA with minimal finetuning, and shows that it outperforms prior works and achieves state-of-the-art results on the AVA dataset[^1^][1].


## Main Contributions

According to the paper, the main contributions are:

- They propose learning image aesthetics from user comments, and explore vision-language pretraining methods to learn multimodal aesthetic representations without human labels.
- They pretrain an image-text encoder-decoder model with image-comment pairs, using contrastive and generative objectives to learn rich and generic aesthetic semantics.
- They propose a lightweight rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept for downstream IAA tasks.
- They show that their pretrained aesthetic vision-language model outperforms prior works on image aesthetic captioning over the AVA-Captions dataset, and has powerful zero-shot capability for aesthetic tasks such as zero-shot style classification and zero-shot IAA, surpassing many supervised baselines.
- They achieve state-of-the-art IAA performance over the AVA dataset with only minimal finetuning parameters using the proposed adapter module.


## Method Summary

Here is a summary of the method section of the paper:

- The paper adopts a vision-language encoder-decoder model that consists of an image encoder, a text encoder, and a text decoder. The image encoder is a ResNet-50 [12] pretrained on ImageNet [8], and the text encoder and decoder are BERT [9] models pretrained on large-scale text corpora.
- The paper pretrains the model with image-comment pairs from Flickr [2], using two objectives: a contrastive objective and a generative objective. The contrastive objective aims to learn image-text alignment by maximizing the similarity between an image and its corresponding comment, while minimizing the similarity between an image and other comments. The generative objective aims to learn image-text generation by reconstructing the comment given the image as input.
- The paper proposes a rank-based adapter that employs text as an anchor to learn the aesthetic ranking concept for downstream IAA tasks. The adapter consists of a text encoder that encodes a textual description of an aesthetic level (e.g., "very low", "low", "medium", "high", "very high"), and a rank classifier that predicts the relative ranking between an image and the text anchor. The adapter is trained with pairwise ranking loss on the AVA dataset [16], which contains human-labeled rating scores for each image. The adapter can be applied to any pretrained vision-language model with minimal finetuning parameters.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the vision-language encoder-decoder model
image_encoder = ResNet50(pretrained=True)
text_encoder = BERT(pretrained=True)
text_decoder = BERT(pretrained=True)

# Pretrain the model with image-comment pairs
for image, comment in flickr_data:
  # Compute the contrastive objective
  contrastive_loss = contrastive(image, comment, text_encoder, image_encoder)
  # Compute the generative objective
  generative_loss = generative(image, comment, text_decoder, image_encoder)
  # Optimize the model parameters
  loss = contrastive_loss + generative_loss
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

# Define the rank-based adapter
text_anchor_encoder = BERT(pretrained=True)
rank_classifier = LinearLayer()

# Finetune the adapter with image-rating pairs
for image, rating in ava_data:
  # Encode the image and the rating text anchor
  image_feature = image_encoder(image)
  rating_text = rating_to_text(rating) # e.g., "very low", "low", etc.
  rating_feature = text_anchor_encoder(rating_text)
  # Predict the relative ranking between the image and the rating
  rank_score = rank_classifier(image_feature, rating_feature)
  # Compute the pairwise ranking loss
  ranking_loss = ranking(rank_score, rating)
  # Optimize the adapter parameters
  optimizer.zero_grad()
  ranking_loss.backward()
  optimizer.step()
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
batch_size = 256 # the batch size for pretraining and finetuning
image_size = 224 # the image size for ResNet-50
text_length = 64 # the maximum text length for BERT
hidden_size = 768 # the hidden size for BERT and LinearLayer
temperature = 0.07 # the temperature for contrastive loss
vocab_size = 30522 # the vocabulary size for BERT
learning_rate = 1e-4 # the learning rate for optimizer

# Define the vision-language encoder-decoder model
image_encoder = torchvision.models.resnet50(pretrained=True)
image_encoder.fc = torch.nn.Identity() # remove the final layer of ResNet-50
text_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
text_decoder = transformers.BertForMaskedLM.from_pretrained('bert-base-uncased')

# Define the contrastive objective
def contrastive(image, comment, text_encoder, image_encoder):
  # Encode the image and the comment with the image encoder and the text encoder
  image_feature = image_encoder(image) # shape: (batch_size, hidden_size)
  comment_feature = text_encoder(comment)[1] # shape: (batch_size, hidden_size)
  # Compute the cosine similarity between the image and the comment features
  similarity_matrix = torch.matmul(image_feature, comment_feature.t()) # shape: (batch_size, batch_size)
  similarity_matrix = similarity_matrix / temperature # apply temperature scaling
  # Compute the contrastive loss with cross entropy
  labels = torch.arange(batch_size).to(similarity_matrix.device) # create labels for positive pairs
  contrastive_loss = torch.nn.CrossEntropyLoss()(similarity_matrix, labels) # compute cross entropy loss
  return contrastive_loss

# Define the generative objective
def generative(image, comment, text_decoder, image_encoder):
  # Encode the image with the image encoder
  image_feature = image_encoder(image) # shape: (batch_size, hidden_size)
  # Mask some tokens in the comment randomly
  masked_comment, masked_indices = mask_tokens(comment) # shape: (batch_size, text_length)
  # Decode the masked comment with the text decoder, using the image feature as an additional input
  logits = text_decoder(masked_comment, encoder_hidden_states=image_feature.unsqueeze(1).repeat(1, text_length, 1))[0] # shape: (batch_size, text_length, vocab_size)
  # Compute the generative loss with cross entropy
  labels = comment.masked_select(masked_indices) # select the original tokens at masked positions
  logits = logits.masked_select(masked_indices.unsqueeze(-1)).view(-1, vocab_size) # select the logits at masked positions
  generative_loss = torch.nn.CrossEntropyLoss()(logits, labels) # compute cross entropy loss
  return generative_loss

# Define a function to mask tokens randomly
def mask_tokens(comment):
  # Create a mask probability matrix with 15% chance of masking a token
  mask_prob = torch.full(comment.shape, 0.15)
  mask_prob[comment == 0] = 0 # do not mask padding tokens
  mask_prob[comment == 101] = 0 # do not mask [CLS] token
  mask_prob[comment == 102] = 0 # do not mask [SEP] token
  masked_indices = torch.bernoulli(mask_prob).bool() # sample masked indices from mask probability matrix

  # Replace masked tokens with [MASK] token (103) or random tokens from vocabulary
  masked_comment = comment.clone() # copy the original comment tensor
  masked_comment[masked_indices] = 103 # replace masked tokens with [MASK] token
  random_indices = torch.bernoulli(torch.full(comment.shape, 0.8)).bool() & masked_indices # sample random indices from masked indices with 80% chance 
  random_tokens = torch.randint(vocab_size, comment.shape, dtype=torch.long).to(comment.device) # sample random tokens from vocabulary 
  masked_comment[random_indices] = random_tokens[random_indices] # replace random tokens at random indices

  return masked_comment, masked_indices

# Pretrain the model with image-comment pairs from Flickr dataset [2]
flickr_data_loader = DataLoader(flickr_data, batch_size=batch_size, shuffle=True) # create a data loader for Flickr dataset 
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # create an optimizer for model parameters
for epoch in range(num_epochs): # loop over the epochs
  for image, comment in flickr_data_loader: # loop over the batches
    # Resize and normalize the image
    image = torchvision.transforms.Resize((image_size, image_size))(image) # resize the image to 224 x 224
    image = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image) # normalize the image with mean and std
    # Tokenize and pad the comment
    comment = transformers.BertTokenizer.from_pretrained('bert-base-uncased').encode(comment, padding='max_length', max_length=text_length, truncation=True) # tokenize and pad the comment to 64 tokens
    # Move the image and the comment to the device (CPU or GPU)
    image = image.to(device)
    comment = comment.to(device)
    # Compute the contrastive objective
    contrastive_loss = contrastive(image, comment, text_encoder, image_encoder)
    # Compute the generative objective
    generative_loss = generative(image, comment, text_decoder, image_encoder)
    # Optimize the model parameters
    loss = contrastive_loss + generative_loss # combine the two losses
    optimizer.zero_grad() # reset the gradients
    loss.backward() # compute the gradients
    optimizer.step() # update the parameters

# Define the rank-based adapter
text_anchor_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
rank_classifier = torch.nn.Linear(hidden_size * 2, 1)

# Define the pairwise ranking loss
def ranking(rank_score, rating):
  # Create a rating matrix where each element is the difference between two ratings in a batch
  rating_matrix = rating.unsqueeze(1) - rating.unsqueeze(0) # shape: (batch_size, batch_size)
  # Create a label matrix where each element is 1 if the rating difference is positive, -1 if negative, and 0 if zero
  label_matrix = torch.sign(rating_matrix) # shape: (batch_size, batch_size)
  # Create a score matrix where each element is the difference between two rank scores in a batch
  score_matrix = rank_score.unsqueeze(1) - rank_score.unsqueeze(0) # shape: (batch_size, batch_size)
  # Compute the pairwise ranking loss with hinge loss
  ranking_loss = torch.nn.ReLU()(1 - label_matrix * score_matrix) # apply hinge loss function element-wise
  ranking_loss = torch.mean(ranking_loss) # average over all elements
  return ranking_loss

# Finetune the adapter with image-rating pairs from AVA dataset [16]
ava_data_loader = DataLoader(ava_data, batch_size=batch_size, shuffle=True) # create a data loader for AVA dataset 
optimizer = torch.optim.Adam(adapter.parameters(), lr=learning_rate) # create an optimizer for adapter parameters
for epoch in range(num_epochs): # loop over the epochs
  for image, rating in ava_data_loader: # loop over the batches
    # Resize and normalize the image
    image = torchvision.transforms.Resize((image_size, image_size))(image) # resize the image to 224 x 224
    image = torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image) # normalize the image with mean and std
    # Convert the rating to a textual description of an aesthetic level (e.g., "very low", "low", etc.)
    rating_text = rating_to_text(rating) 
    # Tokenize and pad the rating text
    rating_text = transformers.BertTokenizer.from_pretrained('bert-base-uncased').encode(rating_text, padding='max_length', max_length=text_length, truncation=True) # tokenize and pad the rating text to 64 tokens
    # Move the image and the rating text to the device (CPU or GPU)
    image = image.to(device)
    rating_text = rating_text.to(device)
    # Encode the image and the rating text with the image encoder and the text anchor encoder
    image_feature = image_encoder(image) # shape: (batch_size, hidden_size)
    rating_feature = text_anchor_encoder(rating_text)[1] # shape: (batch_size, hidden_size)
    # Concatenate the image feature and the rating feature 
    feature = torch.cat([image_feature, rating_feature], dim=-1) # shape: (batch_size, hidden_size * 2)
    # Predict the relative ranking between the image and the rating text with the rank classifier
    rank_score = rank_classifier(feature).squeeze(-1) # shape: (batch_size,)
    # Compute the pairwise ranking loss 
    ranking_loss = ranking(rank_score, rating)
    # Optimize the adapter parameters