---
title: 2009.07823v4 GOCor  Bringing Globally Optimized Correspondence Volumes into Your Neural Network
date: 2020-09-08
---

# [GOCor: Bringing Globally Optimized Correspondence Volumes into Your Neural Network](http://arxiv.org/abs/2009.07823v4)

authors: Prune Truong, Martin Danelljan, Luc Van Gool, Radu Timofte


## What, Why and How

[1]: https://arxiv.org/pdf/2009.07823v4.pdf "Abstract arXiv:2009.07823v4 [cs.CV] 5 Apr 2021"
[2]: https://arxiv.org/abs/2009.07823 "[2009.07823] GOCor: Bringing Globally Optimized ... - arXiv.org"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes GOCor, a novel dense matching module that can replace the feature correlation layer in neural networks that deal with dense correspondences between image pairs.
- **Why**: The feature correlation layer is insufficient when disambiguating multiple similar regions in an image, which affects the performance of tasks such as geometric matching, optical flow, and dense semantic matching.
- **How**: GOCor generates a correspondence volume that is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene. It also learns spatial matching priors to resolve further matching ambiguities. GOCor outperforms the feature correlation layer on various benchmarks and tasks.

## Main Contributions

[1]: https://arxiv.org/pdf/2009.07823v4.pdf "Abstract arXiv:2009.07823v4 [cs.CV] 5 Apr 2021"
[2]: https://arxiv.org/abs/2009.07823 "[2009.07823] GOCor: Bringing Globally Optimized ... - arXiv.org"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

According to the paper[^1^][1], the main contributions are:

- **GOCor**: A novel dense matching module that can replace the feature correlation layer in neural networks that deal with dense correspondences between image pairs. GOCor generates a correspondence volume that is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene.
- **Spatial matching priors**: A method to learn spatial matching priors from data and incorporate them into GOCor to resolve further matching ambiguities. The priors are learned as convolutional kernels that are applied to the correspondence volume.
- **Extensive experiments**: A comprehensive evaluation of GOCor on various benchmarks and tasks, including geometric matching, optical flow, and dense semantic matching. GOCor outperforms the feature correlation layer and other baselines on all tasks.

## Method Summary

[1]: https://arxiv.org/pdf/2009.07823v4.pdf "Abstract arXiv:2009.07823v4 [cs.CV] 5 Apr 2021"
[2]: https://arxiv.org/abs/2009.07823 "[2009.07823] GOCor: Bringing Globally Optimized ... - arXiv.org"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces GOCor, a dense matching module that can be used as a drop-in replacement for the feature correlation layer in neural networks that deal with dense correspondences between image pairs.
- GOCor consists of two main components: a global optimization module and a spatial matching prior module.
- The global optimization module generates a correspondence volume by solving an optimization problem that maximizes the similarity between feature vectors while minimizing the similarity between similar regions in the scene. The optimization problem is formulated as a quadratic program and solved using a differentiable projected gradient descent algorithm.
- The spatial matching prior module learns convolutional kernels that are applied to the correspondence volume to incorporate spatial matching priors. The kernels are learned from data using a self-supervised loss function that encourages consistent matches across different scales and rotations.
- The paper evaluates GOCor on three tasks: geometric matching, optical flow, and dense semantic matching. The paper shows that GOCor outperforms the feature correlation layer and other baselines on various benchmarks and datasets.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: reference image Ir, query image Iq
# Output: correspondence volume C

# Extract feature maps Fr and Fq from Ir and Iq using a CNN
Fr = CNN(Ir)
Fq = CNN(Iq)

# Initialize correspondence volume C as zero matrix
C = zeros(Fr.shape[0], Fq.shape[0])

# Global optimization module
for k in range(K): # K is the number of iterations
  # Compute similarity matrix S between Fr and Fq
  S = Fr @ Fq.T # @ is matrix multiplication
  
  # Update correspondence volume C by adding S
  C = C + S
  
  # Apply softmax to each row of C
  C = softmax(C, axis=1)
  
  # Project C to the probability simplex using Euclidean projection
  C = project_simplex(C)
  
  # Update feature map Fr by multiplying C and Fq
  Fr = C @ Fq

# Spatial matching prior module
# Learn convolutional kernels K from data using self-supervised loss
K = learn_kernels()

# Apply convolutional kernels K to correspondence volume C
C = convolve(C, K)

# Return correspondence volume C
return C
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# Define constants
K = 10 # number of iterations for global optimization
L = 8 # number of convolutional kernels for spatial matching prior
M = 3 # size of convolutional kernels for spatial matching prior
N = 256 # size of feature maps for reference and query images
T = 1e-3 # threshold for Euclidean projection

# Define CNN model to extract feature maps
class CNN(nn.Module):
  def __init__(self):
    super(CNN, self).__init__()
    # Define layers
    self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
    self.conv2 = nn.Conv2d(64, 64, 3, padding=1)
    self.conv3 = nn.Conv2d(64, N, 3, padding=1)
    self.pool = nn.MaxPool2d(2)
    self.relu = nn.ReLU()
  
  def forward(self, x):
    # Forward pass
    x = self.relu(self.conv1(x))
    x = self.pool(x)
    x = self.relu(self.conv2(x))
    x = self.pool(x)
    x = self.relu(self.conv3(x))
    x = self.pool(x)
    # Reshape x to (N, H*W)
    x = x.view(N, -1)
    # Normalize x by L2 norm
    x = F.normalize(x, p=2, dim=0)
    return x

# Define GOCor module
class GOCor(nn.Module):
  def __init__(self):
    super(GOCor, self).__init__()
    # Define CNN model
    self.cnn = CNN()
    # Define convolutional kernels for spatial matching prior
    self.kernels = nn.Parameter(torch.randn(L, M, M))
  
  def forward(self, Ir, Iq):
    # Extract feature maps Fr and Fq from Ir and Iq using CNN model
    Fr = self.cnn(Ir)
    Fq = self.cnn(Iq)

    # Initialize correspondence volume C as zero matrix
    C = torch.zeros(Fr.shape[1], Fq.shape[1])

    # Global optimization module
    for k in range(K):
      # Compute similarity matrix S between Fr and Fq
      S = torch.matmul(Fr.T, Fq) # matrix multiplication
      
      # Update correspondence volume C by adding S
      C = C + S
      
      # Apply softmax to each row of C
      C = F.softmax(C, dim=1)
      
      # Project C to the probability simplex using Euclidean projection
      C = self.project_simplex(C)
      
      # Update feature map Fr by multiplying C and Fq
      Fr = torch.matmul(C, Fq)

    # Spatial matching prior module
    # Apply convolutional kernels to correspondence volume C
    C = F.conv2d(C.unsqueeze(0).unsqueeze(0), self.kernels).squeeze()

    # Return correspondence volume C
    return C
  
  def project_simplex(self, C):
    # Sort C along each row in descending order and compute cumulative sum
    u = torch.sort(C, dim=1, descending=True)[0]
    v = torch.cumsum(u, dim=1)

    # Compute the number of elements that are greater than the threshold T
    w = (u * torch.arange(1, u.shape[1] + 1)).gt(T).float()
    
    # Compute the optimal lambda for each row
    lambda_ = (v - T) / torch.sum(w, dim=1).unsqueeze(1)

    # Compute the projection of C onto the probability simplex
    C_hat = torch.clamp(C - lambda_, min=0)

    return C_hat

# Input: reference image Ir, query image Iq
# Output: correspondence volume C

# Create GOCor module
gocor = GOCor()

# Forward pass of GOCor module
C = gocor(Ir, Iq)

# Return correspondence volume C
return C

```