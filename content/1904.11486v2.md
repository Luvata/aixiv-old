---
title: 1904.11486v2 Making Convolutional Networks Shift-Invariant Again
date: 2019-04-12
---

# [Making Convolutional Networks Shift-Invariant Again](http://arxiv.org/abs/1904.11486v2)

authors: Richard Zhang


## What, Why and How

[1]: https://arxiv.org/pdf/1904.11486v2.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"
[2]: https://arxiv.org/abs/1904.11486 "[1904.11486] Making Convolutional Networks Shift-Invariant Again"
[3]: https://arxiv.org/pdf/1904.11486v1.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"

Here is a summary of the paper:

```
What: The paper proposes a method to make convolutional networks shift-invariant by inserting low-pass filtering before downsampling operations, such as max-pooling and strided-convolution.

Why: The paper argues that modern convolutional networks are not shift-invariant, meaning that small input shifts can cause drastic changes in the output, which can degrade performance and generalization. The paper claims that this is because commonly used downsampling methods ignore the sampling theorem, which requires anti-aliasing by low-pass filtering before downsampling.

How: The paper shows that low-pass filtering can be integrated with existing architectural components, such as max-pooling and strided-convolution, by separating them into two operations: evaluating the operator densely and naive subsampling. The paper demonstrates that this method improves accuracy, stability and robustness on ImageNet classification across several popular architectures, such as ResNet, DenseNet and MobileNet.
```


## Main Contributions

[1]: https://arxiv.org/pdf/1904.11486v2.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"
[2]: https://arxiv.org/abs/1904.11486 "[1904.11486] Making Convolutional Networks Shift-Invariant Again"
[3]: https://arxiv.org/pdf/1904.11486v1.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"

Here is a list of the contributions of the paper:

```
- The paper proposes a simple and effective method to make convolutional networks shift-invariant by inserting low-pass filtering before downsampling operations, such as max-pooling and strided-convolution.
- The paper shows that low-pass filtering is compatible with existing architectural components, such as max-pooling and strided-convolution, by separating them into two operations: evaluating the operator densely and naive subsampling.
- The paper demonstrates that this method improves accuracy, stability and robustness on ImageNet classification across several popular architectures, such as ResNet, DenseNet and MobileNet, indicating effective regularization.
- The paper provides code and anti-aliased versions of popular networks for easy adoption and experimentation.
```


## Method Summary

[1]: https://arxiv.org/pdf/1904.11486v2.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"
[2]: https://arxiv.org/abs/1904.11486 "[1904.11486] Making Convolutional Networks Shift-Invariant Again"
[3]: https://arxiv.org/pdf/1904.11486v1.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"

Here is a summary of the method section of the paper:

```
The method section of the paper consists of four subsections: (1) Shift-Equivariance and Shift-Invariance, (2) Anti-Aliasing, (3) Implementation Details, and (4) Experiments.

- In the first subsection, the paper defines shift-equivariance and shift-invariance for convolutional networks, and shows that commonly used downsampling methods, such as max-pooling and strided-convolution, break these properties. The paper also introduces a metric to measure the degree of shift-invariance for a network, based on the probability of correct class prediction under input shifts.

- In the second subsection, the paper reviews the sampling theorem and explains how anti-aliasing by low-pass filtering before downsampling can prevent aliasing artifacts. The paper proposes to insert low-pass filtering between two operations: evaluating the operator densely and naive subsampling. The paper shows that this method can be applied to both max-pooling and strided-convolution, and that it preserves shift-equivariance and improves shift-invariance.

- In the third subsection, the paper describes the implementation details of the proposed method, such as the choice of low-pass filter, the placement of anti-aliasing modules, and the training procedure. The paper also provides code and anti-aliased versions of popular networks for easy adoption and experimentation.

- In the fourth subsection, the paper presents the experimental results of the proposed method on ImageNet classification across several architectures, such as ResNet, DenseNet and MobileNet. The paper reports increased accuracy, stability and robustness to input corruptions for anti-aliased networks compared to baseline networks. The paper also analyzes the effect of anti-aliasing on different layers and different downsampling methods.
```


## Pseudo Code - High level

[1]: https://arxiv.org/pdf/1904.11486v2.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"
[2]: https://arxiv.org/abs/1904.11486 "[1904.11486] Making Convolutional Networks Shift-Invariant Again"
[3]: https://arxiv.org/pdf/1904.11486v1.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"

Here is a high-level pseudo code for the paper:

```
# Define a low-pass filter, such as a Gaussian filter
filter = GaussianFilter(sigma)

# Define a downsampling method, such as max-pooling or strided-convolution
downsample = MaxPool(kernel_size, stride) or StridedConv(kernel_size, stride)

# Define an anti-aliased downsampling module
def AntiAliasedDownsample(input):
  # Evaluate the downsampling operator densely
  output = downsample(input, stride=1)
  # Low-pass filter before naive subsampling
  output = filter(output)
  # Naive subsampling by slicing
  output = output[:, ::stride, ::stride, :]
  return output

# Define a convolutional network architecture, such as ResNet, DenseNet or MobileNet
network = ResNet() or DenseNet() or MobileNet()

# Replace the downsampling layers in the network with anti-aliased downsampling modules
network.replace_downsample(AntiAliasedDownsample)

# Train and test the network on ImageNet classification
network.train(ImageNet.train)
network.test(ImageNet.test)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/1904.11486v2.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"
[2]: https://arxiv.org/abs/1904.11486 "[1904.11486] Making Convolutional Networks Shift-Invariant Again"
[3]: https://arxiv.org/pdf/1904.11486v1.pdf "Making Convolutional Networks Shift-Invariant Again - arXiv.org"

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.datasets as datasets
import torchvision.transforms as transforms

# Define a low-pass filter, such as a Gaussian filter
class GaussianFilter(nn.Module):
  def __init__(self, sigma):
    super(GaussianFilter, self).__init__()
    # Compute the kernel size and standard deviation
    kernel_size = int(4 * sigma + 0.5)
    std = sigma / kernel_size
    # Create a 1D Gaussian kernel
    kernel_1d = torch.exp(-torch.arange(-kernel_size // 2, kernel_size // 2 + 1, dtype=torch.float32) ** 2 / (2 * std ** 2))
    # Normalize the kernel
    kernel_1d = kernel_1d / torch.sum(kernel_1d)
    # Create a 2D Gaussian kernel by outer product
    kernel_2d = torch.outer(kernel_1d, kernel_1d).unsqueeze(0).unsqueeze(0)
    # Register the kernel as a buffer
    self.register_buffer('kernel', kernel_2d)
  
  def forward(self, input):
    # Apply the Gaussian filter to each channel of the input using convolution
    output = nn.functional.conv2d(input, self.kernel, groups=input.shape[1], padding=self.kernel.shape[-1] // 2)
    return output

# Define an anti-aliased downsampling module
class AntiAliasedDownsample(nn.Module):
  def __init__(self, sigma, method):
    super(AntiAliasedDownsample, self).__init__()
    # Initialize the low-pass filter with the given sigma
    self.filter = GaussianFilter(sigma)
    # Initialize the downsampling method with the given method name and parameters
    if method == 'max_pool':
      self.downsample = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
    elif method == 'strided_conv':
      self.downsample = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1)
  
  def forward(self, input):
    # Evaluate the downsampling operator densely with stride 1
    output = self.downsample(input, stride=1)
    # Low-pass filter before naive subsampling
    output = self.filter(output)
    # Naive subsampling by slicing every other pixel
    output = output[:, :, ::2, ::2]
    return output

# Define a convolutional network architecture, such as ResNet-50
network = models.resnet50(pretrained=True)

# Replace the downsampling layers in the network with anti-aliased downsampling modules with sigma=0.5 and different methods
network.layer1[0].conv1 = AntiAliasedDownsample(sigma=0.5, method='strided_conv')
network.layer2[0].conv1 = AntiAliasedDownsample(sigma=0.5, method='strided_conv')
network.layer3[0].conv1 = AntiAliasedDownsample(sigma=0.5, method='strided_conv')
network.layer4[0].conv1 = AntiAliasedDownsample(sigma=0.5, method='strided_conv')
network.maxpool = AntiAliasedDownsample(sigma=0.5, method='max_pool')

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(network.parameters(), lr=0.01, momentum=0.9)

# Define the data transforms for training and testing
train_transform = transforms.Compose([
  transforms.RandomResizedCrop(224),
  transforms.RandomHorizontalFlip(),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
  transforms.Resize(256),
  transforms.CenterCrop(224),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the ImageNet dataset for training and testing
train_dataset = datasets.ImageNet(root='./data', split='train', transform=train_transform)
test_dataset = datasets.ImageNet(root='./data', split='val', transform=test_transform)

# Create data loaders for training and testing
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256)

# Train and test the network on ImageNet classification for a number of epochs
for epoch in range(10):
  # Set the network to training mode
  network.train()
  # Loop over the training batches
  for inputs, labels in train_loader:
    # Move the inputs and labels to the device
    inputs = inputs.to(device)
    labels = labels.to(device)
    # Zero the parameter gradients
    optimizer.zero_grad()
    # Forward pass
    outputs = network(inputs)
    # Compute the loss
    loss = criterion(outputs, labels)
    # Backward pass and optimize
    loss.backward()
    optimizer.step()
  
  # Set the network to evaluation mode
  network.eval()
  # Initialize the accuracy and the number of samples
  accuracy = 0.0
  total = 0.0
  # Loop over the test batches
  with torch.no_grad():
    for inputs, labels in test_loader:
      # Move the inputs and labels to the device
      inputs = inputs.to(device)
      labels = labels.to(device)
      # Forward pass
      outputs = network(inputs)
      # Get the predicted classes
      _, preds = torch.max(outputs, 1)
      # Update the accuracy and the number of samples
      accuracy += torch.sum(preds == labels).item()
      total += inputs.size(0)
  
  # Print the epoch, loss and accuracy
  print(f'Epoch {epoch}, Loss {loss.item():.4f}, Accuracy {accuracy / total:.4f}')
```