---
title: 2006.10569v2 Towards a Neural Graphics Pipeline for Controllable Image Generation
date: 2020-06-11
---

# [Towards a Neural Graphics Pipeline for Controllable Image Generation](http://arxiv.org/abs/2006.10569v2)

authors: Xuelin Chen, Daniel Cohen-Or, Baoquan Chen, Niloy J. Mitra


## What, Why and How

[1]: https://arxiv.org/abs/2006.10569v2 "[2006.10569v2] Towards a Neural Graphics Pipeline for Controllable ..."
[2]: https://arxiv.org/pdf/2006.10569.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2204.10569v2 "[2204.10569v2] Comparison of different sensor thicknesses and substrate ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a hybrid generative model called Neural Graphics Pipeline (NGP) that combines neural and traditional image formation models for controllable image generation.
- **Why**: The paper aims to leverage advances in neural networks to bypass the need for detailed modeling in conventional graphics pipeline and to provide direct control handles for manipulating shape, appearance, illumination and camera parameters of the generated images.
- **How**: The paper proposes to decompose the image into a set of interpretable appearance feature maps that are generated by neural rendering modules from coarse 3D models. The paper also proposes an unsupervised training scheme that links generated 3D models with unpaired real images via neural and traditional rendering functions, without establishing an explicit correspondence between them. The paper evaluates the proposed approach on single-object scenes and compares it with neural-only generation methods.

## Main Contributions

According to the paper, the main contributions are:

- A hybrid generative model that brings together neural and traditional image formation models for controllable image generation.
- A novel image decomposition scheme that uncovers direct control handles for manipulating shape, appearance, illumination and camera parameters of the generated images.
- An unsupervised training scheme that links generated coarse 3D models with unpaired real images via neural and traditional rendering functions, without establishing an explicit correspondence between them.
- An extensive evaluation of the proposed approach on single-object scenes and a comparison with neural-only generation methods.

## Method Summary

[1]: https://arxiv.org/abs/2006.10569 "[2006.10569] Towards a Neural Graphics Pipeline for Controllable Image ..."
[2]: https://arxiv.org/pdf/2006.10569.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export2.library.cornell.edu/abs/2303.10569v2 "[2303.10569v2] Scattering for wave equations with sources close to the ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a hybrid generative model called Neural Graphics Pipeline (NGP) that consists of three main components: a **3D generator**, a **neural rendering module**, and a **traditional image formation model**.
- The **3D generator** takes a random latent vector as input and outputs a coarse 3D model of the object, represented by a voxel grid and a normal map. The 3D generator is trained with a 3D adversarial loss and a 3D reconstruction loss to encourage realistic and diverse shape generation.
- The **neural rendering module** takes the coarse 3D model and some control parameters (such as illumination and camera) as input and outputs a set of interpretable 2D maps, such as albedo, specular, normal, depth, and mask. The neural rendering module is composed of several sub-modules that are designed to mimic the traditional rendering pipeline, such as projection, shading, and texturing. The neural rendering module is trained with a 2D adversarial loss and a 2D reconstruction loss to encourage realistic and consistent appearance generation.
- The **traditional image formation model** takes the 2D maps as input and composites them into the final output image using a Blinn-Phong shading model. The traditional image formation model is fixed and does not require training.
- The paper also proposes an unsupervised training scheme that links generated coarse 3D models with unpaired real images via neural and traditional rendering functions, without establishing an explicit correspondence between them. The paper introduces a cycle consistency loss that enforces that the generated 3D models can be rendered back to the real images, and vice versa. The paper also introduces a perceptual loss that measures the similarity between the generated images and the real images in a feature space learned by a pre-trained VGG network.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the 3D generator network G
# Define the neural rendering module R
# Define the traditional image formation model F
# Define the 3D discriminator network D_3D
# Define the 2D discriminator network D_2D
# Define the VGG network V

# Load a set of unpaired real images I_real
# Initialize the network parameters randomly

# Repeat until convergence:
  # Sample a batch of random latent vectors z
  # Generate a batch of coarse 3D models (V, N) = G(z)
  # Sample a batch of control parameters (L, C)
  # Generate a batch of 2D maps M = R(V, N, L, C)
  # Generate a batch of output images I_gen = F(M)
  
  # Compute the 3D adversarial loss L_adv_3D = -log(D_3D(V, N))
  # Compute the 2D adversarial loss L_adv_2D = -log(D_2D(I_gen))
  # Compute the 3D reconstruction loss L_rec_3D = ||G(R(V, N)) - (V, N)||_1
  # Compute the 2D reconstruction loss L_rec_2D = ||R(G(I_gen)) - M||_1
  # Compute the cycle consistency loss L_cyc = ||F(R(G(I_real))) - I_real||_1 + ||G(R(F(M))) - (V, N)||_1
  # Compute the perceptual loss L_per = ||V(I_gen) - V(I_real)||_2
  
  # Compute the total generator loss L_G = L_adv_3D + L_adv_2D + L_rec_3D + L_rec_2D + L_cyc + L_per
  # Update the parameters of G and R to minimize L_G
  
  # Compute the 3D discriminator loss L_D_3D = -log(D_3D(G(I_real))) - log(1 - D_3D(G(z)))
  # Update the parameters of D_3D to minimize L_D_3D
  
  # Compute the 2D discriminator loss L_D_2D = -log(D_2D(I_real)) - log(1 - D_2D(I_gen))
  # Update the parameters of D_2D to minimize L_D_2D
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np
import random

# Define some hyperparameters
batch_size = 16 # The number of images in a batch
latent_dim = 256 # The dimension of the latent vector
voxel_dim = 64 # The dimension of the voxel grid
map_dim = 128 # The dimension of the 2D maps
image_dim = 256 # The dimension of the output image
num_epochs = 100 # The number of training epochs
lr = 0.0002 # The learning rate
beta1 = 0.5 # The beta1 parameter for Adam optimizer
beta2 = 0.999 # The beta2 parameter for Adam optimizer
lambda_3D = 10 # The weight for the 3D reconstruction loss
lambda_2D = 10 # The weight for the 2D reconstruction loss
lambda_cyc = 10 # The weight for the cycle consistency loss
lambda_per = 1 # The weight for the perceptual loss

# Define the 3D generator network G
class Generator3D(nn.Module):
  def __init__(self):
    super(Generator3D, self).__init__()
    # Define the network architecture as a series of transposed convolutional layers with batch normalization and ReLU activation
    self.net = nn.Sequential(
      nn.ConvTranspose3d(latent_dim, 512, kernel_size=4, stride=1, padding=0, bias=False),
      nn.BatchNorm3d(512),
      nn.ReLU(True),
      nn.ConvTranspose3d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm3d(256),
      nn.ReLU(True),
      nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm3d(128),
      nn.ReLU(True),
      nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm3d(64),
      nn.ReLU(True),
      nn.ConvTranspose3d(64, 2, kernel_size=4, stride=2, padding=1, bias=False),
      nn.Tanh()
    )
  
  def forward(self, z):
    # Take a batch of latent vectors z as input and output a batch of coarse 3D models (V, N)
    # where V is a voxel grid and N is a normal map
    out = self.net(z) # Shape: (batch_size, 2, voxel_dim, voxel_dim, voxel_dim)
    V = out[:, :1] # Shape: (batch_size, 1, voxel_dim, voxel_dim, voxel_dim)
    N = out[:, 1:] # Shape: (batch_size, 1, voxel_dim, voxel_dim, voxel_dim)
    return V, N

# Define the neural rendering module R
class Renderer(nn.Module):
  def __init__(self):
    super(Renderer, self).__init__()
    # Define the sub-modules for projection (P), shading (S), and texturing (T)
    self.P = Projector()
    self.S = Shader()
    self.T = Texturer()
  
  def forward(self, V, N, L, C):
    # Take a batch of coarse 3D models (V,N), a batch of illumination parameters L,
    # and a batch of camera parameters C as input and output a batch of interpretable 2D maps M
    P_V = self.P(V) # Project the voxel grid to a depth map using ray casting
    P_N = self.P(N) # Project the normal map to a normal map using ray casting and interpolation
    S_M = self.S(P_N) # Shade the normal map to an albedo map and a specular map using neural networks
    T_M = self.T(P_V) # Texture the depth map to a mask map using neural networks
    M = torch.cat([S_M,T_M], dim=1) # Concatenate the albedo map (A), specular map (S), normal map (N), depth map (D), and mask map (M) to form the interpretable 2D maps M 
    return M

# Define the projector sub-module P
class Projector(nn.Module):
  def __init__(self):
    super(Projector,self).__init__()
  
  def forward(self,X):
    # Take a batch of voxel grids X as input and output a batch of depth maps using ray casting
    # Assume the camera parameters are fixed and known
    # Use the ray casting algorithm described in the paper
    # Return the depth map as a 2D tensor of shape (batch_size, 1, map_dim, map_dim)
    pass

# Define the shader sub-module S
class Shader(nn.Module):
  def __init__(self):
    super(Shader,self).__init__()
    # Define the network architecture as a series of convolutional layers with batch normalization and ReLU activation
    self.net = nn.Sequential(
      nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(64),
      nn.ReLU(True),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(True),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(256),
      nn.ReLU(True),
      nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(True),
      nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(64),
      nn.ReLU(True),
      nn.ConvTranspose2d(64, 6, kernel_size=4, stride=2, padding=1, bias=False),
      nn.Tanh()
    )
  
  def forward(self,N):
    # Take a batch of normal maps N as input and output a batch of albedo maps and specular maps using neural networks
    out = self.net(N) # Shape: (batch_size, 6, map_dim, map_dim)
    A = out[:, :3] # Shape: (batch_size, 3, map_dim, map_dim)
    S = out[:, 3:] # Shape: (batch_size, 3, map_dim, map_dim)
    return torch.cat([A,S], dim=1) # Shape: (batch_size, 6, map_dim, map_dim)

# Define the texturer sub-module T
class Texturer(nn.Module):
  def __init__(self):
    super(Texturer,self).__init__()
    # Define the network architecture as a series of convolutional layers with batch normalization and ReLU activation
    self.net = nn.Sequential(
      nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(64),
      nn.ReLU(True),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(True),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
      nn.BatchNorm2d(256),
      nn.ReLU(True),
      nn.ConvTranspose2d(256, 128,kernel_size=4,stride=2,padding=1,bias=False),
      nn.BatchNorm2d(128),
      nn.ReLU(True),
      nn.ConvTranspose2d(128 ,64,kernel_size =4,stride =2,padding =1,bias =False ),
      nn.BatchNorm2d(64 ),
      nn.ReLU(True ),
      nn.ConvTranspose2d(64 ,1,kernel_size =4,stride =2,padding =1,bias =False ),
      torch.sigmoid()
    )
  
  def forward(self,D):
    # Take a batch of depth maps D as input and output a batch of mask maps using neural networks
    M = self.net(D) # Shape: (batch_size ,1,map_dim ,map_dim )
    return M

# Define the traditional image formation model F
def image_formation(M,L,C):
  # Take a batch of interpretable 2D maps M ,a batch of illumination parameters L,
  # and a batch of camera parameters C as input and output a batch of output images using a Blinn-Phong shading model
  # Assume the illumination parameters L are fixed and known
  # Use the Blinn-Phong shading model described in the paper
  # Return the output image as a 3D tensor of shape (batch_size ,3,image_dim ,image_dim )
  pass

# Define the 3D discriminator network D_3D
class Discriminator3D(nn.Module):
  def __init__(self):
    super(Discriminator3D,self).__init__()
    # Define the network architecture as a series of convolutional layers with batch normalization and LeakyReLU