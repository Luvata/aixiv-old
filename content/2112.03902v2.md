---
title: 2112.03902v2 MS-TCT  Multi-Scale Temporal ConvTransformer for Action Detection
date: 2021-12-04
---

# [MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection](http://arxiv.org/abs/2112.03902v2)

authors: Rui Dai, Srijan Das, Kumara Kahatapitiya, Michael S. Ryoo, Francois Bremond


## What, Why and How

[1]: https://arxiv.org/pdf/2112.03902v2 "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection"
[2]: https://arxiv.org/abs/2112.03902v2 "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection"
[3]: https://arxiv.org/pdf/2104.03902v2.pdf "The Autodidactic Universe - arXiv.org"
[4]: http://export.arxiv.org/abs/2112.03902v2 "[2112.03902v2] MS-TCT: Multi-Scale Temporal ConvTransformer for Action ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel network for action detection in untrimmed videos, called MS-TCT (Multi-Scale Temporal ConvTransformer).
- **Why**: Action detection is a challenging task that requires capturing both short-term and long-term temporal relations among actions, which are complex and diverse in densely-labelled datasets. Existing methods based on temporal convolutions have limitations in accessing global information and modeling long-range dependencies.
- **How**: The paper introduces three main components in MS-TCT: (1) a Temporal Encoder module that uses convolutional layers and transformer layers to explore global and local temporal relations at multiple resolutions, (2) a Temporal Scale Mixer module that fuses multi-scale features to create a unified feature representation, and (3) a Classification module that learns a center-relative position of each action instance and predicts frame-level classification scores. The paper evaluates MS-TCT on three datasets (Charades, TSU and MultiTHUMOS) and shows that it outperforms the state-of-the-art methods on all of them.

## Main Contributions

According to the paper at , the contributions are:

- The paper proposes a novel ConvTransformer network for action detection in untrimmed videos, which can capture both short-term and long-term temporal relations among actions.
- The paper introduces a Temporal Encoder module that combines convolutional layers and transformer layers to explore global and local temporal information at multiple resolutions.
- The paper designs a Temporal Scale Mixer module that effectively fuses multi-scale features to create a unified feature representation for action detection.
- The paper develops a Classification module that learns a center-relative position of each action instance and predicts frame-level classification scores, which can handle complex action distributions and temporal overlaps.
- The paper demonstrates the effectiveness of the proposed method on three challenging datasets (Charades, TSU and MultiTHUMOS), where it outperforms the state-of-the-art methods on all of them.

## Method Summary

[1]: https://arxiv.org/abs/2112.03902 "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection"
[2]: https://arxiv.org/pdf/2112.03902 "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection"
[3]: https://www.researchgate.net/publication/362606221_MS-TCT_Multi-Scale_Temporal_ConvTransformer_for_Action_Detection "MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a novel network for action detection in untrimmed videos, called MS-TCT (Multi-Scale Temporal ConvTransformer), which inherits a transformer encoder architecture while also gaining benefits from temporal convolution.
- The paper introduces three main components in MS-TCT: (1) a Temporal Encoder module that uses convolutional layers and transformer layers to explore global and local temporal relations at multiple resolutions, (2) a Temporal Scale Mixer module that effectively fuses multi-scale features to create a unified feature representation, and (3) a Classification module that learns a center-relative position of each action instance and predicts frame-level classification scores.
- The paper describes the details of each component, such as the input representation, the convolutional layers, the transformer layers, the feature fusion strategy, the position encoding scheme, the classification loss function, and the inference procedure.
- The paper also discusses some design choices and ablation studies of MS-TCT, such as the number of temporal resolutions, the number of transformer layers, the effect of different convolutional layers, and the comparison with other ConvTransformer variants.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a video clip of T frames
# Output: a list of action instances with class labels and temporal boundaries

# Define the network parameters
N = number of temporal resolutions
M = number of transformer layers
K = number of convolutional layers

# Initialize the network modules
Temporal_Encoder = ConvTransformer(N, M, K)
Temporal_Scale_Mixer = Feature_Fusion(N)
Classification = Position_Prediction(N)

# Extract the input features
X = extract_features(video_clip)

# Encode the temporal relations at multiple resolutions
Z = Temporal_Encoder(X)

# Fuse the multi-scale features
Y = Temporal_Scale_Mixer(Z)

# Predict the action instances
instances = Classification(Y)

# Return the action instances
return instances
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: a video clip of T frames
# Output: a list of action instances with class labels and temporal boundaries

# Define the network parameters
N = number of temporal resolutions
M = number of transformer layers
K = number of convolutional layers
D = feature dimension
C = number of action classes
P = number of position bins

# Initialize the network modules
Temporal_Encoder = ConvTransformer(N, M, K)
Temporal_Scale_Mixer = Feature_Fusion(N)
Classification = Position_Prediction(N)

# Define the convolutional layers
Conv1D = 1D convolution with kernel size 3 and padding 1
Conv3D = 3D convolution with kernel size (3, 3, 3) and padding (1, 1, 1)

# Define the transformer layers
MultiHeadAttention = multi-head self-attention with D heads
FeedForward = feed-forward network with two linear layers and ReLU activation
LayerNorm = layer normalization
Dropout = dropout with probability p

# Define the feature fusion strategy
Concatenate = concatenate features along the channel dimension
Linear = linear projection with D output channels

# Define the position encoding scheme
Sinusoidal = sinusoidal encoding with D channels and P bins
Learnable = learnable encoding with D channels and P bins

# Define the classification loss function
BCEWithLogitsLoss = binary cross-entropy with logits loss

# Extract the input features
X = extract_features(video_clip) # X.shape = (T, D)

# Encode the temporal relations at multiple resolutions
Z = [] # Z is a list of N tensors of shape (T_i, D)
for i in range(N):
  # Downsample the input features by a factor of 2^i
  X_i = Conv1D(X) if i == 0 else Conv3D(X_i) # X_i.shape = (T_i, D)
  
  # Apply K convolutional layers to capture local information
  for k in range(K):
    X_i = Conv1D(X_i) + X_i # residual connection
  
  # Apply M transformer layers to capture global information
  for m in range(M):
    # Multi-head self-attention sub-layer
    A_i = MultiHeadAttention(X_i) # A_i.shape = (T_i, D)
    A_i = Dropout(A_i) + X_i # residual connection and dropout
    A_i = LayerNorm(A_i) # layer normalization
    
    # Feed-forward sub-layer
    F_i = FeedForward(A_i) # F_i.shape = (T_i, D)
    F_i = Dropout(F_i) + A_i # residual connection and dropout
    F_i = LayerNorm(F_i) # layer normalization
    
    # Update X_i with F_i
    X_i = F_i
  
  # Append X_i to Z
  Z.append(X_i)

# Fuse the multi-scale features
Y = [] # Y is a list of N tensors of shape (T_i, D)
for i in range(N):
  # Concatenate the features from all resolutions
  C_i = Concatenate(Z[0], Z[1], ..., Z[N-1]) # C_i.shape = (T_i, N * D)
  
  # Project the concatenated features to D channels
  P_i = Linear(C_i) # P_i.shape = (T_i, D)
  
  # Append P_i to Y
  Y.append(P_i)

# Predict the action instances
instances = [] # instances is a list of action instances with class labels and temporal boundaries
for i in range(N):
  # Apply sinusoidal or learnable position encoding to Y[i]
  E_i = Sinusoidal(Y[i]) or Learnable(Y[i]) # E_i.shape = (T_i, P * D)
  
  # Predict the frame-level classification scores and position scores for each action class
  S_c, S_p = Classification(E[i]) # S_c.shape = (T_i, C), S_p.shape = (T_i, C * P)
  
  # Apply a threshold to S_c to get the action labels for each frame
  L_c = S_c > threshold # L_c.shape = (T_i, C)
  
  # Apply softmax to S_p to get the position probabilities for each frame and action class
  P_p = softmax(S_p) # P_p.shape = (T_i, C * P)
  
  # For each action class c that has at least one positive label in L_c:
  for c in range(C):
    if L_c[:, c].any():
      # Get the start and end frames of each action instance for class c
      start_frames, end_frames = get_instance_frames(L_c[:, c]) # start_frames and end_frames are lists of integers
      
      # For each pair of start and end frames:
      for s, e in zip(start_frames, end_frames):
        # Get the position probabilities for the action instance
        P_i = P_p[s:e+1, c * P : (c + 1) * P] # P_i.shape = (e - s + 1, P)
        
        # Compute the expected position bin for the action instance
        bin_i = sum(P_i * range(P)) # bin_i is a scalar
        
        # Compute the center-relative position for the action instance
        pos_i = (bin_i - P / 2) / (P / 2) # pos_i is a scalar in [-1, 1]
        
        # Compute the absolute temporal boundaries for the action instance
        t_s = s / T_i + pos_i / (2 * T_i) # t_s is a scalar in [0, 1]
        t_e = e / T_i + pos_i / (2 * T_i) # t_e is a scalar in [0, 1]
        
        # Create an action instance with class label c and temporal boundaries t_s and t_e
        instance = (c, t_s, t_e)
        
        # Append the action instance to instances
        instances.append(instance)

# Return the action instances
return instances
```