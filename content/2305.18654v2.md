---
title: 2305.18654v2 Faith and Fate  Limits of Transformers on Compositionality
date: 2023-05-19
---

# [Faith and Fate: Limits of Transformers on Compositionality](http://arxiv.org/abs/2305.18654v2)

authors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, Yejin Choi


## What, Why and How

[1]: https://arxiv.org/pdf/2305.18654v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2305.18654 "Faith and Fate: Limits of Transformers on Compositionality"
[3]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper investigates the limits of Transformer large language models (LLMs) on compositional tasks that require multi-step reasoning, such as multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem.
- **Why**: The paper aims to demystify the mixed capabilities of Transformers, which show impressive performance on some complex tasks but fail on some simple ones. The paper also seeks to understand how Transformers solve compositional tasks and whether they can develop systematic problem-solving skills.
- **How**: The paper formulates compositional tasks as computation graphs to quantify the level of complexity and break down reasoning steps into intermediate sub-procedures. The paper evaluates several Transformer LLMs on these tasks and analyzes their errors and limitations. The paper also provides theoretical arguments on abstract multi-step reasoning problems that highlight how Transformers' performance will decay with increased task complexity.


## Main Contributions

[1]: https://arxiv.org/pdf/2305.18654v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2305.18654 "Faith and Fate: Limits of Transformers on Compositionality"
[3]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."

According to the paper at [^1^][1], the main contributions are:

- **A new framework for formulating and evaluating compositional tasks as computation graphs**, which allows for systematic quantification of task complexity and intermediate reasoning steps.
- **A comprehensive empirical study of Transformer LLMs on three representative compositional tasks**, which reveals their strengths and weaknesses, as well as the types of errors they make.
- **A theoretical analysis of abstract multi-step reasoning problems**, which shows how Transformers' performance will rapidly decay with increased task complexity and why they cannot generalize beyond linearized subgraph matching.


## Method Summary

[1]: https://arxiv.org/pdf/2305.18654v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2305.18654 "Faith and Fate: Limits of Transformers on Compositionality"
[3]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper defines **compositional tasks** as tasks that require breaking down a problem into sub-steps and synthesizing these steps into a precise answer.
- The paper introduces **computation graphs** as a way to represent compositional tasks, where nodes are sub-procedures and edges are dependencies between them. The paper also defines metrics to measure the complexity of computation graphs, such as depth, width, and fan-out.
- The paper selects three representative compositional tasks that vary in their computation graph complexity: **multi-digit multiplication**, **logic grid puzzles**, and **a classic dynamic programming problem**. The paper constructs datasets for these tasks and provides details on their formats and properties.
- The paper evaluates several Transformer LLMs on these tasks, including **ChatGPT**, **GPT-4**, **T5**, and **BART**. The paper uses different methods to prompt the models, such as natural language questions, cloze-style queries, and special tokens. The paper also fine-tunes some models on the task-specific data to test their generalization ability.
- The paper analyzes the performance of the models on the tasks, as well as their errors and limitations. The paper uses various techniques to probe the models' reasoning skills, such as ablation studies, attention visualization, and counterfactual analysis. The paper also compares the models' performance with human baselines and other baselines such as rule-based systems and neural networks.
- The paper provides theoretical arguments on abstract multi-step reasoning problems that show how Transformers' performance will decay with increased task complexity. The paper also discusses why Transformers cannot generalize beyond linearized subgraph matching, which is their main strategy for solving compositional tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# Define a compositional task as a computation graph
# A computation graph is a directed acyclic graph where nodes are sub-procedures and edges are dependencies
# A sub-procedure is a function that takes some inputs and produces some outputs
# A dependency is a relation that indicates which inputs of a sub-procedure depend on which outputs of another sub-procedure

# Define metrics to measure the complexity of a computation graph
# Depth: the longest path from the root node to any leaf node
# Width: the maximum number of nodes at any level of the graph
# Fan-out: the maximum number of outgoing edges from any node

# Select three representative compositional tasks that vary in their computation graph complexity
# Multi-digit multiplication: given two numbers with n digits each, compute their product
# Logic grid puzzles: given a set of clues and a grid with categories and items, fill in the grid with logical deductions
# A classic dynamic programming problem: given a set of items with weights and values, and a knapsack with a capacity, find the maximum value that can be put in the knapsack

# Construct datasets for these tasks and provide details on their formats and properties
# Multi-digit multiplication: generate random pairs of numbers with n digits each, where n ranges from 1 to 5, and compute their product using standard arithmetic rules
# Logic grid puzzles: collect existing puzzles from various sources and convert them into a standard format with clues, categories, items, and grids
# A classic dynamic programming problem: generate random sets of items with weights and values, where the number of items ranges from 5 to 15, and the knapsack capacity ranges from 10 to 50, and compute the optimal solution using a recursive formula

# Evaluate several Transformer LLMs on these tasks, including ChatGPT, GPT-4, T5, and BART
# Use different methods to prompt the models, such as natural language questions, cloze-style queries, and special tokens
# Fine-tune some models on the task-specific data to test their generalization ability

# Analyze the performance of the models on the tasks, as well as their errors and limitations
# Use various techniques to probe the models' reasoning skills, such as ablation studies, attention visualization, and counterfactual analysis
# Compare the models' performance with human baselines and other baselines such as rule-based systems and neural networks

# Provide theoretical arguments on abstract multi-step reasoning problems that show how Transformers' performance will decay with increased task complexity
# Discuss why Transformers cannot generalize beyond linearized subgraph matching, which is their main strategy for solving compositional tasks
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Define a compositional task as a computation graph
# A computation graph is a directed acyclic graph where nodes are sub-procedures and edges are dependencies
# A sub-procedure is a function that takes some inputs and produces some outputs
# A dependency is a relation that indicates which inputs of a sub-procedure depend on which outputs of another sub-procedure

# Define metrics to measure the complexity of a computation graph
# Depth: the longest path from the root node to any leaf node
# Width: the maximum number of nodes at any level of the graph
# Fan-out: the maximum number of outgoing edges from any node

# Define a class for computation graphs
class ComputationGraph:
  # Initialize a computation graph with a list of nodes and a list of edges
  def __init__(self, nodes, edges):
    self.nodes = nodes # a list of sub-procedures
    self.edges = edges # a list of tuples (source, target, dependency)

  # Compute the depth of the computation graph
  def depth(self):
    # Use a breadth-first search algorithm to find the longest path from the root node to any leaf node
    # Initialize a queue with the root node and its distance from the root (0)
    queue = [(self.nodes[0], 0)]
    # Initialize a variable to store the maximum distance
    max_dist = 0
    # Loop until the queue is empty
    while queue:
      # Dequeue a node and its distance from the queue
      node, dist = queue.pop(0)
      # Update the maximum distance if needed
      max_dist = max(max_dist, dist)
      # Find all the nodes that depend on the current node
      for edge in self.edges:
        if edge[0] == node:
          # Enqueue the dependent node and its distance from the root (one more than the current node)
          queue.append((edge[1], dist + 1))
    # Return the maximum distance as the depth of the computation graph
    return max_dist

  # Compute the width of the computation graph
  def width(self):
    # Use a breadth-first search algorithm to find the maximum number of nodes at any level of the graph
    # Initialize a queue with the root node and its level (0)
    queue = [(self.nodes[0], 0)]
    # Initialize a dictionary to store the number of nodes at each level
    level_count = {}
    # Loop until the queue is empty
    while queue:
      # Dequeue a node and its level from the queue
      node, level = queue.pop(0)
      # Increment the number of nodes at the current level by one
      level_count[level] = level_count.get(level, 0) + 1
      # Find all the nodes that depend on the current node
      for edge in self.edges:
        if edge[0] == node:
          # Enqueue the dependent node and its level (one more than the current node)
          queue.append((edge[1], level + 1))
    # Return the maximum value in the dictionary as the width of the computation graph
    return max(level_count.values())

  # Compute the fan-out of the computation graph
  def fan_out(self):
    # Use a loop to find the maximum number of outgoing edges from any node in the graph
    # Initialize a variable to store the maximum fan-out
    max_fan_out = 0
    # Loop through all the nodes in the graph
    for node in self.nodes:
      # Count how many edges have this node as their source
      fan_out = 0
      for edge in self.edges:
        if edge[0] == node:
          fan_out += 1
      # Update the maximum fan-out if needed
      max_fan_out = max(max_fan_out, fan_out)
    # Return the maximum fan-out as the fan-out of the computation graph
    return max_fan_out

# Select three representative compositional tasks that vary in their computation graph complexity

# Multi-digit multiplication: given two numbers with n digits each, compute their product

# Define a sub-procedure for single-digit multiplication with carry-over
def single_digit_mul(x, y, c):
  # x and y are single digits, c is an optional carry-over digit (default to zero)
  # Compute x times y plus c and return two digits: one for result and one for carry-over
  p = x * y + c 
  r = p % 10 # result digit is remainder of p divided by 10 
  c = p // 10 # carry-over digit is quotient of p divided by 10
  return r, c

# Define a sub-procedure for single-digit addition with carry-over
def single_digit_add(x, y, c):
  # x and y are single digits, c is an optional carry-over digit (default to zero)
  # Compute x plus y plus c and return two digits: one for result and one for carry-over
  s = x + y + c 
  r = s % 10 # result digit is remainder of s divided by 10 
  c = s // 10 # carry-over digit is quotient of s divided by 10
  return r, c

# Define a sub-procedure for appending a digit to a number
def append_digit(n, d):
  # n is a number, d is a single digit
  # Append d to the right of n and return the new number
  return n * 10 + d

# Define a sub-procedure for multiplying a number by 10 to the power of k
def mul_by_power_of_10(n, k):
  # n is a number, k is a non-negative integer
  # Multiply n by 10 to the power of k and return the new number
  return n * (10 ** k)

# Define a sub-procedure for adding two numbers
def add_numbers(x, y):
  # x and y are numbers
  # Add x and y and return the sum
  return x + y

# Define a computation graph for multi-digit multiplication
def multi_digit_mul(a, b):
  # a and b are numbers with n digits each
  # Compute their product using the sub-procedures defined above and return the result

  # Initialize an empty list to store the partial products
  partials = []

  # Loop through the digits of b from right to left
  for i in range(n):
    # Get the i-th digit of b
    bi = b % 10 
    # Initialize a variable to store the carry-over for multiplication
    cm = 0 
    # Initialize a variable to store the partial product
    pp = 0 
    # Loop through the digits of a from right to left
    for j in range(n):
      # Get the j-th digit of a
      aj = a % 10 
      # Multiply aj and bi using single_digit_mul sub-procedure and update cm
      r, cm = single_digit_mul(aj, bi, cm) 
      # Append r to pp using append_digit sub-procedure
      pp = append_digit(pp, r) 
      # Move to the next digit of a by dividing by 10
      a = a // 10 
    # If there is any remaining carry-over, append it to pp
    if cm > 0:
      pp = append_digit(pp, cm) 
    # Multiply pp by 10 to the power of i using mul_by_power_of_10 sub-procedure
    pp = mul_by_power_of_10(pp, i) 
    # Add pp to the list of partial products
    partials.append(pp) 
    # Move to the next digit of b by dividing by 10
    b = b // 10 

  # Initialize a variable to store the final product
  p = 0 
  # Loop through the partial products and add them using add_numbers sub-procedure
  for pp in partials:
    p = add_numbers(p, pp) 

  # Return the final product as the result of multi-digit multiplication
  return p

# Logic grid puzzles: given a set of clues and a grid with categories and items, fill in the grid with logical deductions

# Define a sub-procedure for parsing clues into logical expressions
def parse_clue(clue):
  # clue is a natural language sentence that provides some information about the grid
  # Parse clue into a logical expression that can be evaluated on the grid and return it

# Define a sub-procedure for evaluating logical expressions on the grid
def eval_expr(expr, grid):
  # expr is a logical expression that involves categories and items from the grid
  # grid is a two-dimensional matrix that represents the possible associations between categories and items
  # Evaluate expr on the grid and update the grid with new information if any and return it

# Define a sub-procedure for checking if the grid is complete or inconsistent
def check_grid(grid):
  # grid is a two-dimensional matrix that represents the possible associations between categories and items
  # Check if the grid is complete (all cells are filled with either true or false) or inconsistent (some cells are empty or contradictory) and return a boolean value

# Define a computation graph for logic grid puzzles
def logic_grid_puzzle(clues, grid):
  
```