---
title: 2201.11903v6 Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
date: 2022-01-12
---

# [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](http://arxiv.org/abs/2201.11903v6)

authors: Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou


## What, Why and How

[1]: https://arxiv.org/pdf/2201.11903v6 "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
[2]: https://arxiv.org/abs/2201.11903 "[2201.11903] Chain-of-Thought Prompting Elicits Reasoning in Large ..."
[3]: https://info.arxiv.org/about/index.html "About arXiv - arXiv info"

Here is a summary of the paper:

- **What**: The paper explores how generating a chain of thought - a series of intermediate reasoning steps - significantly improves the ability of large language models to perform complex reasoning tasks such as arithmetic, commonsense, and symbolic reasoning[^1^][1] [^2^][2].
- **Why**: The paper argues that chain of thought prompting enables large language models to leverage their implicit knowledge and learn from a few exemplars, without requiring fine-tuning or additional supervision[^1^][1] [^2^][2]. The paper also claims that chain of thought prompting can help humans understand the reasoning process of large language models and identify potential errors or biases[^1^][1] [^2^][2].
- **How**: The paper proposes a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. For example, for an arithmetic word problem, the prompt would include not only the question and the answer, but also the natural language rationale that leads to the answer[^1^][1] [^2^][2]. The paper evaluates chain of thought prompting on three large language models (GPT-3 175B, PaLM 540B, and GPT-Neo 2.7B) and shows that it improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The paper also analyzes the effect of chain length, model size, and domain adaptation on chain of thought prompting[^1^][1] [^2^][2].


## Main Contributions

According to the paper, the main contributions are:

- The paper introduces chain of thought prompting, a simple method that elicits reasoning in large language models by providing a few chain of thought demonstrations as exemplars in prompting .
- The paper shows that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks, and achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems .
- The paper provides empirical analysis and qualitative examples to understand how chain of thought prompting works and what are the limitations and challenges .

## Method Summary

[1]: https://arxiv.org/pdf/2201.11903v6.pdf "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
[2]: https://arxiv.org/abs/2201.11903 "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
[3]: https://www.promptengineering.org/master-prompting-concepts-chain-of-thought-prompting/ "Master Prompting Concepts: Chain of Thought Prompting"
[4]: https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html "Language Models Perform Reasoning via Chain of Thought"

Here is a summary of the method section of the paper:

- The paper proposes a simple method called **chain of thought prompting**, where a few chain of thought demonstrations are provided as exemplars in prompting[^1^][1] [^2^][2] [^3^][3] [^4^][4].
- A chain of thought demonstration consists of a question, an answer, and a natural language rationale that explains how the answer is derived from the question using intermediate reasoning steps[^1^][1] [^2^][2] [^3^][3] [^4^][4].
- The paper uses three large language models (GPT-3 175B, PaLM 540B, and GPT-Neo 2.7B) and evaluates them on a range of arithmetic, commonsense, and symbolic reasoning tasks using chain of thought prompting[^1^][1] [^2^][2] [^4^][4].
- The paper also analyzes the effect of chain length, model size, and domain adaptation on chain of thought prompting, and provides qualitative examples to illustrate the strengths and weaknesses of the method[^1^][1] [^2^][2] [^4^][4].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a set of chain of thought demonstrations for a given reasoning task
# Each demonstration consists of a question, an answer, and a natural language rationale
demonstrations = [
  {
    "question": "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?",
    "answer": "The answer is 11.",
    "rationale": "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11."
  },
  # More demonstrations ...
]

# Load a large language model
model = load_model("PaLM-540B")

# For each test question in the reasoning task
for question in test_questions:
  # Generate a prompt by concatenating the question with the demonstrations
  prompt = question + "\n\n" + "\n\n".join(demonstrations)
  
  # Generate a response from the model using the prompt
  response = model.generate(prompt)
  
  # Evaluate the response using some metric
  metric.evaluate(response)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import transformers # For loading and using language models
import datasets # For loading and processing reasoning tasks
import numpy as np # For numerical operations

# Define a function to generate a chain of thought prompt
def generate_prompt(question, demonstrations):
  # Initialize an empty prompt
  prompt = ""
  
  # Add the question to the prompt
  prompt += "Q: " + question + "\n\n"
  
  # For each demonstration in the demonstrations
  for demo in demonstrations:
    # Add the demonstration question, answer, and rationale to the prompt
    prompt += "Q: " + demo["question"] + "\n\n"
    prompt += "A: " + demo["answer"] + "\n\n"
    prompt += demo["rationale"] + "\n\n"
  
  # Return the prompt
  return prompt

# Define a function to extract the answer from the model response
def extract_answer(response):
  # Split the response by newline characters
  lines = response.split("\n")
  
  # Find the line that starts with "A: "
  for line in lines:
    if line.startswith("A: "):
      # Return the rest of the line as the answer
      return line[3:]
  
  # If no answer is found, return an empty string
  return ""

# Define a function to evaluate the answer using some metric
def evaluate_answer(answer, gold_answer, metric):
  # Convert both answer and gold_answer to lower case
  answer = answer.lower()
  gold_answer = gold_answer.lower()
  
  # Compute and return the metric score between answer and gold_answer
  return metric.score(answer, gold_answer)

# Load a large language model and its tokenizer
model_name = "PaLM-540B"
model = transformers.AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

# Load a reasoning task dataset
dataset_name = "gsm8k" # Math word problems
dataset = datasets.load_dataset(dataset_name)

# Define a set of chain of thought demonstrations for the reasoning task
# Each demonstration consists of a question, an answer, and a natural language rationale
demonstrations = [
  {
    "question": "Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?",
    "answer": "The answer is 11.",
    "rationale": "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11."
  },
  # More demonstrations ...
]

# Define a metric to evaluate the answers
metric = datasets.load_metric("accuracy")

# Initialize an empty list to store the scores
scores = []

# For each test question in the reasoning task
for question in dataset["test"]:
  # Generate a chain of thought prompt by concatenating the question with the demonstrations
  prompt = generate_prompt(question["question"], demonstrations)
  
  # Encode the prompt using the tokenizer
  input_ids = tokenizer.encode(prompt, return_tensors="pt")
  
  # Generate a response from the model using the input_ids
  output_ids = model.generate(input_ids)
  
  # Decode the output_ids using the tokenizer
  response = tokenizer.decode(output_ids[0])
  
  # Extract the answer from the response
  answer = extract_answer(response)
  
  # Evaluate the answer using the metric and the gold answer from the question
  score = evaluate_answer(answer, question["answer"], metric)
  
  # Append the score to the scores list
  scores.append(score)

# Compute and print the average score over all test questions
average_score = np.mean(scores)
print(f"Average score: {average_score}")
```