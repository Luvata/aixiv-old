---
title: 2208.01626v1 Prompt-to-Prompt Image Editing with Cross Attention Control
date: 2022-08-02
---

# [Prompt-to-Prompt Image Editing with Cross Attention Control](http://arxiv.org/abs/2208.01626v1)

authors: Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2208.01626 "Prompt-to-Prompt Image Editing with Cross Attention Control"
[2]: http://export.arxiv.org/abs/2208.01626 "Prompt-to-Prompt Image Editing with Cross Attention Control"
[3]: https://arxiv.org/pdf/2208.01626v1 "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1]:

- **What**: The paper presents a method for text-driven image editing using large-scale text-conditioned generative models. The method allows users to edit images by modifying the text prompts that describe them, without requiring any spatial masks or manual interventions.
- **Why**: The paper aims to overcome the limitations of existing text-driven image synthesis methods, which often produce completely different images when the text prompts are slightly changed, and which ignore the original structure and content of the images when editing them. The paper also aims to provide an intuitive and natural way for humans to control the image synthesis process by using verbal descriptions of their intent.
- **How**: The paper analyzes the cross-attention layers of a text-conditioned model and observes that they are the key to controlling the relation between the spatial layout of the image and each word in the prompt. Based on this observation, the paper proposes several applications that monitor the image synthesis by editing the textual prompt only, such as localized editing, global editing, and fine-grained editing. The paper demonstrates high-quality synthesis and fidelity to the edited prompts on diverse images and prompts.

## Main Contributions

The paper claims to make the following contributions:

- It introduces a novel prompt-to-prompt image editing framework that leverages the cross-attention layers of a text-conditioned model to control the image synthesis by modifying the text prompts only.
- It presents several applications of the framework that enable localized, global, and fine-grained editing of images without requiring any spatial masks or manual interventions.
- It demonstrates high-quality synthesis and fidelity to the edited prompts on diverse images and prompts, and compares favorably with state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/abs/2208.01626 "Prompt-to-Prompt Image Editing with Cross Attention Control"
[2]: http://export.arxiv.org/abs/2208.01626 "Prompt-to-Prompt Image Editing with Cross Attention Control"
[3]: https://arxiv.org/abs/2211.01626v1 "[2211.01626v1] FEniCS implementation of the Virtual Fields Method (VFM ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses a text-conditioned model called **DALL-E**[^2^][2], which is a large-scale transformer-based model that can generate diverse images from text prompts using a discrete variational autoencoder (VAE).
- The paper focuses on the **cross-attention layers** of DALL-E, which are responsible for aligning the text tokens with the image tokens. The paper shows that by manipulating the cross-attention weights, one can control the image synthesis process by editing the text prompts only.
- The paper proposes several applications of the prompt-to-prompt editing framework, such as:
    - **Localized editing**: replacing a word in the prompt with another word to edit a specific region in the image, e.g., changing "a cat on a couch" to "a dog on a couch".
    - **Global editing**: adding a specification to the prompt to edit the whole image, e.g., adding "in black and white" to "a cat on a couch".
    - **Fine-grained editing**: adjusting the cross-attention weights to control the extent to which a word is reflected in the image, e.g., changing the size or color of an object.
- The paper evaluates the quality and fidelity of the edited images using human ratings and quantitative metrics. The paper also compares the results with state-of-the-art methods that require spatial masks or manual interventions for image editing.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load the DALL-E model and the text prompt
model = DALL_E()
prompt = "a cat on a couch"

# Generate an image from the prompt
image = model.generate(prompt)

# Edit the prompt by replacing a word
new_prompt = "a dog on a couch"

# Get the cross-attention weights between the text and image tokens
weights = model.cross_attention(prompt, image)

# Modify the weights to reflect the new prompt
new_weights = model.edit_weights(weights, prompt, new_prompt)

# Generate a new image from the new prompt and weights
new_image = model.generate(new_prompt, new_weights)

# Display the original and edited images
show(image, new_image)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt

# Define some constants
TEXT_VOCAB_SIZE = 8192 # The size of the text vocabulary
IMAGE_VOCAB_SIZE = 8192 # The size of the image vocabulary
TEXT_LENGTH = 256 # The maximum length of the text prompt
IMAGE_SIZE = 256 # The size of the image
NUM_LAYERS = 12 # The number of transformer layers
NUM_HEADS = 16 # The number of attention heads
HIDDEN_SIZE = 1024 # The hidden size of the transformer
LATENT_SIZE = 64 # The latent size of the VAE

# Define a function to encode text tokens into embeddings
def text_encoder(tokens):
  # tokens: a tensor of shape [batch_size, text_length] containing the text tokens
  # returns: a tensor of shape [batch_size, text_length, hidden_size] containing the text embeddings

  # Initialize an embedding layer with TEXT_VOCAB_SIZE and HIDDEN_SIZE
  embedding = torch.nn.Embedding(TEXT_VOCAB_SIZE, HIDDEN_SIZE)

  # Embed the tokens using the embedding layer
  embeddings = embedding(tokens)

  # Return the embeddings
  return embeddings

# Define a function to encode image tokens into embeddings
def image_encoder(tokens):
  # tokens: a tensor of shape [batch_size, image_size, image_size] containing the image tokens
  # returns: a tensor of shape [batch_size, image_size * image_size, hidden_size] containing the image embeddings

  # Initialize an embedding layer with IMAGE_VOCAB_SIZE and HIDDEN_SIZE
  embedding = torch.nn.Embedding(IMAGE_VOCAB_SIZE, HIDDEN_SIZE)

  # Reshape the tokens to [batch_size, image_size * image_size]
  tokens = tokens.reshape(-1, IMAGE_SIZE * IMAGE_SIZE)

  # Embed the tokens using the embedding layer
  embeddings = embedding(tokens)

  # Return the embeddings
  return embeddings

# Define a function to decode image embeddings into pixels
def image_decoder(embeddings):
  # embeddings: a tensor of shape [batch_size, image_size * image_size, hidden_size] containing the image embeddings
  # returns: a tensor of shape [batch_size, image_size, image_size] containing the image pixels

  # Initialize a linear layer with HIDDEN_SIZE and IMAGE_VOCAB_SIZE
  linear = torch.nn.Linear(HIDDEN_SIZE, IMAGE_VOCAB_SIZE)

  # Apply the linear layer to the embeddings
  logits = linear(embeddings)

  # Reshape the logits to [batch_size, image_size, image_size]
  logits = logits.reshape(-1, IMAGE_SIZE, IMAGE_SIZE)

  # Apply softmax to get probabilities over IMAGE_VOCAB_SIZE
  probs = torch.nn.functional.softmax(logits, dim=-1)

  # Sample pixels from the probabilities using argmax or multinomial
  pixels = torch.argmax(probs, dim=-1) # or torch.multinomial(probs, num_samples=1)

  # Return the pixels
  return pixels

# Define a function to compute cross-attention between text and image embeddings
def cross_attention(text_embeddings, image_embeddings):
  # text_embeddings: a tensor of shape [batch_size, text_length, hidden_size] containing the text embeddings
  # image_embeddings: a tensor of shape [batch_size, image_size * image_size, hidden_size] containing the image embeddings
  # returns: a tensor of shape [batch_size, num_heads, text_length, image_size * image_size] containing the cross-attention weights

  # Initialize three linear layers for query, key and value projections with HIDDEN_SIZE and HIDDEN_SIZE / NUM_HEADS each
  query_proj = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE // NUM_HEADS)
  key_proj = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE // NUM_HEADS)
  value_proj = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE // NUM_HEADS)

  # Project the text embeddings using the query projection layer
  query = query_proj(text_embeddings) # shape: [batch_size, text_length, hidden_size // num_heads]

  # Project the image embeddings using the key and value projection layers
  key = key_proj(image_embeddings) # shape: [batch_size, image_size * image_size, hidden_size // num_heads]
  value = value_proj(image_embeddings) # shape: [batch_size, image_size * image_size, hidden_size // num_heads]

  # Transpose and reshape the query and key tensors to [batch_size, num_heads, text_length, hidden_size // num_heads] and [batch_size, num_heads, hidden_size // num_heads, image_size * image_size] respectively
  query = query.transpose(1, 2).reshape(-1, NUM_HEADS, TEXT_LENGTH, HIDDEN_SIZE // NUM_HEADS)
  key = key.transpose(1, 2).reshape(-1, NUM_HEADS, HIDDEN_SIZE // NUM_HEADS, IMAGE_SIZE * IMAGE_SIZE)

  # Compute the dot product between query and key tensors
  scores = torch.matmul(query, key) # shape: [batch_size, num_heads, text_length, image_size * image_size]

  # Apply softmax to get attention weights
  weights = torch.nn.functional.softmax(scores, dim=-1) # shape: [batch_size, num_heads, text_length, image_size * image_size]

  # Transpose and reshape the value tensor to [batch_size, num_heads, image_size * image_size, hidden_size // num_heads]
  value = value.transpose(1, 2).reshape(-1, NUM_HEADS, IMAGE_SIZE * IMAGE_SIZE, HIDDEN_SIZE // NUM_HEADS)

  # Compute the weighted sum of value tensors using the attention weights
  output = torch.matmul(weights, value) # shape: [batch_size, num_heads, text_length, hidden_size // num_heads]

  # Reshape and transpose the output tensor to [batch_size, text_length, hidden_size]
  output = output.reshape(-1, TEXT_LENGTH, HIDDEN_SIZE).transpose(1, 2) # shape: [batch_size, text_length, hidden_size]

  # Return the output and the weights
  return output, weights

# Define a function to encode text and image embeddings into latent codes using a VAE
def vae_encoder(text_embeddings, image_embeddings):
  # text_embeddings: a tensor of shape [batch_size, text_length, hidden_size] containing the text embeddings
  # image_embeddings: a tensor of shape [batch_size, image_size * image_size, hidden_size] containing the image embeddings
  # returns: a tensor of shape [batch_size, latent_size] containing the latent codes

  # Concatenate the text and image embeddings along the second dimension
  embeddings = torch.cat([text_embeddings, image_embeddings], dim=1) # shape: [batch_size, text_length + image_size * image_size, hidden_size]

  # Initialize a transformer encoder with NUM_LAYERS and HIDDEN_SIZE
  transformer = torch.nn.TransformerEncoder(HIDDEN_SIZE)

  # Apply the transformer encoder to the embeddings
  encoded = transformer(embeddings) # shape: [batch_size, text_length + image_size * image_size, hidden_size]

  # Initialize two linear layers for mean and log variance projections with HIDDEN_SIZE and LATENT_SIZE each
  mean_proj = torch.nn.Linear(HIDDEN_SIZE, LATENT_SIZE)
  logvar_proj = torch.nn.Linear(HIDDEN_SIZE, LATENT_SIZE)

  # Apply the mean and log variance projection layers to the encoded embeddings
  mean = mean_proj(encoded) # shape: [batch_size, text_length + image_size * image_size, latent_size]
  logvar = logvar_proj(encoded) # shape: [batch_size, text_length + image_size * image_size, latent_size]

  # Compute the KL divergence between the prior and posterior distributions
  kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp(), dim=-1) # shape: [batch_size]

  # Sample latent codes from the posterior distribution using the reparameterization trick
  std = torch.exp(0.5 * logvar) # shape: [batch_size]
  eps = torch.randn_like(std) # shape: [batch_size]
  z = mean + eps * std # shape: [batch_size]

  # Return the latent codes and the KL loss
  return z, kl_loss

# Define a function to decode latent codes into text and image embeddings using a VAE
def vae_decoder(z):
  # z: a tensor of shape [batch_size] containing the latent codes
  # returns: a tensor of shape [batch_size] containing the decoded tokens