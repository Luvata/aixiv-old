---
title: 2110.04544v1 CLIP-Adapter  Better Vision-Language Models with Feature Adapters
date: 2021-10-05
---

# [CLIP-Adapter: Better Vision-Language Models with Feature Adapters](http://arxiv.org/abs/2110.04544v1)

authors: Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, Yu Qiao


## What, Why and How

[1]: https://arxiv.org/pdf/2110.04544v1 "CLIP-Adapter: Better Vision-Language Models with Feature Adapters"
[2]: https://arxiv.org/abs/2110.04544 "[2110.04544] CLIP-Adapter: Better Vision-Language Models ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2104.04544v1 "[2104.04544v1] A note on pencils of norm-form equations"

Here is a summary of the paper:

- **What**: The paper proposes CLIP-Adapter, a method to fine-tune vision-language models with feature adapters on either visual or language branch.
- **Why**: The paper aims to improve the performance of vision-language models on downstream tasks without relying on hand-crafted text prompts or context optimization.
- **How**: The paper introduces an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pre-trained features. The paper evaluates CLIP-Adapter on various visual classification tasks and shows that it outperforms context optimization and naive classification[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/2110.04544v1 "CLIP-Adapter: Better Vision-Language Models with Feature Adapters"
[2]: https://arxiv.org/abs/2110.04544 "[2110.04544] CLIP-Adapter: Better Vision-Language Models ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2104.04544v1 "[2104.04544v1] A note on pencils of norm-form equations"

According to the paper[^1^][1], the main contributions are:

- The paper proposes **CLIP-Adapter**, a novel method to fine-tune vision-language models with feature adapters on either visual or language branch, which can improve the performance on downstream tasks without relying on hand-crafted text prompts or context optimization.
- The paper introduces an **additional bottleneck layer** to learn new features and performs **residual-style feature blending** with the original pre-trained features, which can preserve the generalization ability of the pre-trained model and adapt to new tasks effectively.
- The paper conducts **experiments and extensive ablation studies** on various visual classification tasks, such as ImageNet, CIFAR-10, CIFAR-100, and Oxford Flowers 102, and shows that CLIP-Adapter can outperform context optimization and naive classification by a large margin.

## Method Summary

[1]: https://arxiv.org/pdf/2110.04544v1 "CLIP-Adapter: Better Vision-Language Models with Feature Adapters"
[2]: https://arxiv.org/abs/2110.04544 "[2110.04544] CLIP-Adapter: Better Vision-Language Models ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2104.04544v1 "[2104.04544v1] A note on pencils of norm-form equations"

Here is a summary of the method section of the paper:

- The paper describes the **CLIP model** as a baseline, which consists of a visual encoder (a CNN or a Transformer) and a language encoder (a BERT model), and learns to align image-text pairs with a contrastive loss.
- The paper introduces the **CLIP-Adapter** method, which adds an additional bottleneck layer after the visual or language encoder, and fine-tunes it with a small number of labeled examples. The bottleneck layer learns new features that are blended with the original pre-trained features through a residual connection.
- The paper explains the **training details** of CLIP-Adapter, such as the learning rate, optimizer, batch size, and data augmentation. The paper also discusses some implementation issues, such as the choice of adapter position and size, and the effect of freezing or unfreezing the pre-trained model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the CLIP model with a visual encoder and a language encoder
clip_model = CLIP(visual_encoder, language_encoder)

# Load the pre-trained weights of the CLIP model
clip_model.load_pretrained_weights()

# Add a bottleneck layer after the visual or language encoder
bottleneck_layer = BottleneckLayer(input_size, output_size)

# Define a residual connection to blend the original and new features
residual_connection = ResidualConnection()

# Define a contrastive loss function to align image-text pairs
contrastive_loss = ContrastiveLoss()

# Define an optimizer to update the parameters of the bottleneck layer
optimizer = Optimizer(bottleneck_layer.parameters())

# Loop over the labeled examples
for image, text, label in data_loader:

  # Forward pass the image and text through the CLIP model
  image_features = clip_model.visual_encoder(image)
  text_features = clip_model.language_encoder(text)

  # Forward pass the image or text features through the bottleneck layer
  new_features = bottleneck_layer(image_features or text_features)

  # Blend the original and new features with the residual connection
  blended_features = residual_connection(image_features or text_features, new_features)

  # Compute the contrastive loss between the blended features and the label
  loss = contrastive_loss(blended_features, label)

  # Backward propagate the loss and update the parameters of the bottleneck layer
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers

# Define the hyperparameters
input_size = 512 # the size of the input features
output_size = 128 # the size of the output features
learning_rate = 1e-4 # the learning rate for the optimizer
batch_size = 32 # the batch size for the data loader
num_epochs = 10 # the number of epochs for training

# Define the CLIP model with a visual encoder and a language encoder
# The visual encoder can be a CNN (e.g., ResNet-50) or a Transformer (e.g., ViT)
# The language encoder is a BERT model with 12 layers and 768 hidden units
clip_model = CLIP(visual_encoder, language_encoder)

# Load the pre-trained weights of the CLIP model from https://github.com/openai/CLIP
clip_model.load_pretrained_weights()

# Add a bottleneck layer after the visual or language encoder
# The bottleneck layer is a linear layer that reduces the dimensionality of the features
bottleneck_layer = torch.nn.Linear(input_size, output_size)

# Define a residual connection to blend the original and new features
# The residual connection is a simple addition operation
residual_connection = lambda x, y: x + y

# Define a contrastive loss function to align image-text pairs
# The contrastive loss is computed as the cross entropy between the cosine similarity scores and the labels
contrastive_loss = lambda x, y: torch.nn.CrossEntropyLoss()(torch.matmul(x, y.t()), torch.arange(batch_size))

# Define an optimizer to update the parameters of the bottleneck layer
# The optimizer can be any gradient-based algorithm (e.g., Adam, SGD)
optimizer = torch.optim.Adam(bottleneck_layer.parameters(), lr=learning_rate)

# Load the labeled examples from a dataset (e.g., ImageNet, CIFAR-10)
# The dataset should contain image-text pairs and their corresponding labels
dataset = Dataset()

# Create a data loader to iterate over the dataset in batches
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Loop over the number of epochs
for epoch in range(num_epochs):

  # Loop over the batches in the data loader
  for image, text, label in data_loader:

    # Move the data to the device (e.g., CPU, GPU)
    image = image.to(device)
    text = text.to(device)
    label = label.to(device)

    # Forward pass the image and text through the CLIP model
    # The output features are normalized to have unit length
    image_features = clip_model.visual_encoder(image)
    image_features = torch.nn.functional.normalize(image_features, dim=-1)
    text_features = clip_model.language_encoder(text)
    text_features = torch.nn.functional.normalize(text_features, dim=-1)

    # Forward pass the image or text features through the bottleneck layer
    # The output features are also normalized to have unit length
    new_features = bottleneck_layer(image_features or text_features)
    new_features = torch.nn.functional.normalize(new_features, dim=-1)

    # Blend the original and new features with the residual connection
    # The blended features are also normalized to have unit length
    blended_features = residual_connection(image_features or text_features, new_features)
    blended_features = torch.nn.functional.normalize(blended_features, dim=-1)

    # Compute the contrastive loss between the blended features and the label
    loss = contrastive_loss(blended_features, label)

    # Backward propagate the loss and update the parameters of the bottleneck layer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  # Print the epoch and loss information
  print(f"Epoch {epoch}, Loss {loss.item()}")
```