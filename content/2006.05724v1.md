---
title: 2006.05724v1 Real-time single image depth perception in the wild with handheld devices
date: 2020-06-06
---

# [Real-time single image depth perception in the wild with handheld devices](http://arxiv.org/abs/2006.05724v1)

authors: Filippo Aleotti, Giulio Zaccaroni, Luca Bartolomei, Matteo Poggi, Fabio Tosi, Stefano Mattoccia


## What, Why and How

[1]: https://arxiv.org/abs/2006.05724v1 "[2006.05724v1] Real-time single image depth perception in the wild with ..."
[2]: https://arxiv.org/pdf/2006.05724v1.pdf "1 Real-time single image depth perception in the wild with ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.05724v1 "[2305.05724v1] Lower semicontinuity of pullback attractors for a non ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method for real-time single image depth perception in the wild with handheld devices, such as smartphones or tablets.
- **Why**: The paper aims to address two main challenges for single image depth estimation: i) the low reliability when deployed in-the-wild and ii) the demanding resource requirements to achieve real-time performance, often not compatible with such devices.
- **How**: The paper proposes a framework that combines appropriate network design and training strategies to achieve fast and robust depth estimation from a single image. The paper also shows how to map the resulting networks on handheld devices to achieve real-time performance. The paper evaluates the method on various datasets and demonstrates its applications in real-time depth-aware augmented reality and image blurring with smartphones in-the-wild.

## Main Contributions

[1]: https://arxiv.org/abs/2006.05724v1 "[2006.05724v1] Real-time single image depth perception in the wild with ..."
[2]: https://arxiv.org/pdf/2006.05724v1.pdf "1 Real-time single image depth perception in the wild with ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.05724v1 "[2305.05724v1] Lower semicontinuity of pullback attractors for a non ..."

According to the paper[^1^][1], the main contributions are:

- **A novel framework** for training fast and robust monocular depth estimation models that can generalize well to new environments and achieve real-time performance on handheld devices.
- **A comprehensive evaluation** of the proposed framework on various datasets, showing its effectiveness in terms of accuracy, speed, and generalization compared to state-of-the-art methods.
- **A demonstration** of the practical applications of the proposed method in real-time depth-aware augmented reality and image blurring with smartphones in-the-wild.

## Method Summary

[1]: https://arxiv.org/abs/2006.05724v1 "[2006.05724v1] Real-time single image depth perception in the wild with ..."
[2]: https://arxiv.org/pdf/2006.05724v1.pdf "1 Real-time single image depth perception in the wild with ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.05724v1 "[2305.05724v1] Lower semicontinuity of pullback attractors for a non ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a **framework** that consists of three main components: i) a **network design** that adopts a lightweight encoder-decoder architecture with skip connections and a multi-scale loss function, ii) a **training strategy** that leverages synthetic and real data with domain adaptation and data augmentation techniques, and iii) a **mapping strategy** that optimizes the network for deployment on handheld devices using quantization and pruning methods.
- The paper describes the details of each component and explains the rationale behind the design choices. The paper also provides the implementation details of the framework, such as the network architecture, the loss function, the datasets, the domain adaptation method, the data augmentation method, the quantization method, and the pruning method.
- The paper evaluates the performance of the framework on various datasets, such as KITTI, Make3D, NYU Depth v2, and DIW. The paper also demonstrates the applications of the framework in real-time depth-aware augmented reality and image blurring with smartphones in-the-wild.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the network architecture
network = EncoderDecoder(
    encoder = ResNet18(pretrained=True),
    decoder = UpConvolutionalBlocks(),
    skip_connections = True
)

# Define the loss function
loss_function = MultiScaleLoss(
    scales = [1/4, 1/8, 1/16, 1/32],
    weights = [0.5, 0.25, 0.125, 0.125]
)

# Define the synthetic and real datasets
synthetic_dataset = VirtualKITTI()
real_dataset = KITTI()

# Define the domain adaptation method
domain_adaptation = AdaIN()

# Define the data augmentation method
data_augmentation = RandomCropAndFlip()

# Train the network on synthetic and real data
for epoch in range(num_epochs):
    for batch in synthetic_dataset:
        # Apply domain adaptation to synthetic images
        adapted_images = domain_adaptation(batch.images, real_dataset.style_statistics)
        # Forward pass through the network
        outputs = network(adapted_images)
        # Compute the loss
        loss = loss_function(outputs, batch.depths)
        # Backward pass and update the network parameters
        loss.backward()
        optimizer.step()
    for batch in real_dataset:
        # Apply data augmentation to real images
        augmented_images, augmented_depths = data_augmentation(batch.images, batch.depths)
        # Forward pass through the network
        outputs = network(augmented_images)
        # Compute the loss
        loss = loss_function(outputs, augmented_depths)
        # Backward pass and update the network parameters
        loss.backward()
        optimizer.step()

# Define the quantization and pruning methods
quantization = PostTrainingDynamicRangeQuantization()
pruning = MagnitudePruning()

# Map the network to handheld devices
# Apply quantization to reduce the model size and latency
quantized_network = quantization(network)
# Apply pruning to remove redundant weights and improve efficiency
pruned_network = pruning(quantized_network)

# Deploy the network on handheld devices and run applications
device = Smartphone()
device.load_model(pruned_network)
device.run_application(DepthAwareAugmentedReality())
device.run_application(ImageBlurring())
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the network architecture
class EncoderDecoder(torch.nn.Module):
    def __init__(self, encoder, decoder, skip_connections):
        super(EncoderDecoder, self).__init__()
        # Initialize the encoder and decoder modules
        self.encoder = encoder
        self.decoder = decoder
        # Initialize the skip connections flag
        self.skip_connections = skip_connections
    
    def forward(self, x):
        # Encode the input image into a feature map
        features = self.encoder(x)
        # Decode the feature map into a depth map
        outputs = self.decoder(features)
        # If skip connections are enabled, add the outputs from different scales
        if self.skip_connections:
            outputs = torch.sum(outputs, dim=1)
        # Return the final output
        return outputs

class ResNet18(torch.nn.Module):
    def __init__(self, pretrained):
        super(ResNet18, self).__init__()
        # Load the pretrained ResNet18 model from torchvision
        resnet18 = torchvision.models.resnet18(pretrained=pretrained)
        # Remove the last fully connected layer and the average pooling layer
        self.layers = torch.nn.Sequential(*list(resnet18.children())[:-2])
    
    def forward(self, x):
        # Forward pass through the ResNet18 layers
        features = self.layers(x)
        # Return the feature map
        return features

class UpConvolutionalBlocks(torch.nn.Module):
    def __init__(self):
        super(UpConvolutionalBlocks, self).__init__()
        # Define the number of output channels for each scale
        out_channels = [64, 32, 16, 1]
        # Define the list of up-convolutional blocks
        self.blocks = torch.nn.ModuleList()
        # For each scale, create an up-convolutional block
        for i in range(len(out_channels)):
            # Define the number of input channels as the double of the output channels of the previous scale
            in_channels = out_channels[i-1] * 2 if i > 0 else 512
            # Define the up-convolutional block as a sequence of:
            # - a transpose convolution layer with a stride of 2 and a kernel size of 3
            # - a batch normalization layer
            # - a ReLU activation layer
            block = torch.nn.Sequential(
                torch.nn.ConvTranspose2d(in_channels, out_channels[i], kernel_size=3, stride=2),
                torch.nn.BatchNorm2d(out_channels[i]),
                torch.nn.ReLU()
            )
            # Append the block to the list
            self.blocks.append(block)
    
    def forward(self, x):
        # Initialize an empty list to store the outputs from different scales
        outputs = []
        # For each up-convolutional block, apply it to the input feature map
        for block in self.blocks:
            x = block(x)
            # Append the output to the list
            outputs.append(x)
        # Return the list of outputs
        return outputs

# Define the loss function
class MultiScaleLoss(torch.nn.Module):
    def __init__(self, scales, weights):
        super(MultiScaleLoss, self).__init__()
        # Initialize the list of scales and weights
        self.scales = scales
        self.weights = weights
    
    def forward(self, outputs, targets):
        # Initialize an empty list to store the losses from different scales
        losses = []
        # For each scale and weight, compute the loss between the output and target depth maps at that scale
        for scale, weight in zip(self.scales, self.weights):
            # Resize the output and target depth maps to match the scale factor using bilinear interpolation
            output = torch.nn.functional.interpolate(outputs[-1], scale_factor=scale, mode='bilinear')
            target = torch.nn.functional.interpolate(targets[-1], scale_factor=scale, mode='bilinear')
            # Compute the mean absolute error (MAE) loss between the output and target depth maps
            loss = torch.nn.functional.l1_loss(output, target)
            # Multiply the loss by the weight factor and append it to the list
            losses.append(loss * weight)
        
        


# Sum up all the losses from different scales and return the final loss value

return sum(losses)