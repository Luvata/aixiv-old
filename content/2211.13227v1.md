---
title: 2211.13227v1 Paint by Example  Exemplar-based Image Editing with Diffusion Models
date: 2022-11-14
---

# [Paint by Example: Exemplar-based Image Editing with Diffusion Models](http://arxiv.org/abs/2211.13227v1)

authors: Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen


## What, Why and How

[1]: https://arxiv.org/abs/2211.13227 "[2211.13227] Paint by Example: Exemplar-based Image Editing with ..."
[2]: https://arxiv.org/pdf/2211.13227v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2111.13227v1 "[2111.13227v1] Spectral analysis and stabilization of the dissipative ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel method for exemplar-guided image editing with diffusion models, which allows users to edit a scene by painting with a conditional image.
- **Why**: The paper aims to achieve more precise control over image editing than language-guided methods, and to avoid fusing artifacts and trivial solutions that may arise from naive approaches.
- **How**: The paper leverages self-supervised training to disentangle and re-organize the source image and the exemplar, and introduces an information bottleneck and strong augmentations to prevent copying and pasting the exemplar image. The paper also designs an arbitrary shape mask for the exemplar image and uses classifier-free guidance to increase the similarity to the exemplar image. The paper demonstrates that the method can produce high-quality and controllable editing results on in-the-wild images with a single forward of the diffusion model[^1^][1].

## Main Contributions

According to the paper, the main contributions are:

- The first exemplar-guided image editing framework based on diffusion models, which enables precise control over image editing with conditional images.
- A self-supervised training scheme that can disentangle and re-organize the source image and the exemplar image in a latent space.
- An information bottleneck and strong augmentations that can avoid the trivial solution of directly copying and pasting the exemplar image.
- An arbitrary shape mask and a classifier-free guidance that can ensure the controllability of the editing process and increase the similarity to the exemplar image.
- Extensive experiments and ablation studies that demonstrate the effectiveness and superiority of the proposed method over existing methods.

## Method Summary

[1]: https://arxiv.org/abs/2211.13227 "[2211.13227] Paint by Example: Exemplar-based Image Editing with ..."
[2]: https://arxiv.org/pdf/2211.13227v1.pdf "arXiv.org e-Print archive"
[3]: https://scholar.archive.org/work/c3s5wkynirgpzbh7na3acvc43y "Paint by Example: Exemplar-based Image Editing with Diffusion Models"

Here is a summary of the method section:

- The paper adopts a diffusion model as the backbone for image editing, which can generate realistic images by reversing a Markov chain that gradually adds Gaussian noise to the original image.
- The paper introduces a self-supervised training scheme that can learn to disentangle and re-organize the source image and the exemplar image in a latent space, without requiring any paired data or annotations.
- The paper proposes an information bottleneck and strong augmentations for the diffusion model, which can prevent the model from directly copying and pasting the exemplar image and encourage it to learn meaningful features from both images.
- The paper designs an arbitrary shape mask for the exemplar image, which allows users to specify the region of interest for editing. The paper also uses a classifier-free guidance term in the loss function, which can increase the similarity between the edited image and the exemplar image in the masked region.
- The paper implements the editing process as a single forward pass of the diffusion model, which takes the source image, the exemplar image, and the mask as inputs, and outputs the edited image. The paper also provides an interactive interface for users to adjust the editing parameters and preview the results.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion model
model = DiffusionModel()

# Define the self-supervised training scheme
train_data = load_unpaired_images()
for source_image, exemplar_image in train_data:
  # Apply random augmentations to both images
  source_image = augment(source_image)
  exemplar_image = augment(exemplar_image)
  # Encode both images into latent vectors
  source_latent = model.encode(source_image)
  exemplar_latent = model.encode(exemplar_image)
  # Swap the latent vectors of the masked regions
  mask = generate_random_mask()
  swapped_latent = swap_latent(source_latent, exemplar_latent, mask)
  # Decode the swapped latent vector into an edited image
  edited_image = model.decode(swapped_latent)
  # Compute the loss function with an information bottleneck and a classifier-free guidance
  loss = compute_loss(source_image, exemplar_image, edited_image, mask)
  # Update the model parameters
  model.update(loss)

# Define the editing process
def edit(source_image, exemplar_image, mask):
  # Encode both images into latent vectors
  source_latent = model.encode(source_image)
  exemplar_latent = model.encode(exemplar_image)
  # Swap the latent vectors of the masked regions
  swapped_latent = swap_latent(source_latent, exemplar_latent, mask)
  # Decode the swapped latent vector into an edited image
  edited_image = model.decode(swapped_latent)
  return edited_image
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
image_size = 256 # The size of the input images
latent_size = 1024 # The size of the latent vectors
num_layers = 12 # The number of layers in the diffusion model
num_channels = 256 # The number of channels in each layer
num_classes = 1000 # The number of classes for the classifier-free guidance
beta_min = 0.0001 # The minimum value of the noise level
beta_max = 0.02 # The maximum value of the noise level
num_timesteps = 1000 # The number of timesteps in the diffusion process
batch_size = 16 # The batch size for training
learning_rate = 0.0001 # The learning rate for training
num_epochs = 100 # The number of epochs for training

# Define the diffusion model
class DiffusionModel(torch.nn.Module):
  def __init__(self):
    super(DiffusionModel, self).__init__()
    # Define the encoder network
    self.encoder = torchvision.models.resnet50(pretrained=True)
    self.encoder.fc = torch.nn.Linear(self.encoder.fc.in_features, latent_size)
    # Define the decoder network
    self.decoder = torchvision.models.resnet50(pretrained=True)
    self.decoder.fc = torch.nn.Linear(self.decoder.fc.in_features, image_size * image_size * 3)
    # Define the timestep embedding network
    self.timestep_embed = torch.nn.Embedding(num_timesteps, latent_size)
    # Define the noise level schedule
    self.betas = torch.linspace(beta_min, beta_max, num_timesteps)

  def encode(self, x):
    # Encode an image into a latent vector
    x = x.view(-1, 3, image_size, image_size) # Reshape the input to match the encoder input shape
    z = self.encoder(x) # Apply the encoder network
    return z

  def decode(self, z):
    # Decode a latent vector into an image
    x = self.decoder(z) # Apply the decoder network
    x = x.view(-1, 3, image_size, image_size) # Reshape the output to match the image shape
    return x

  def forward(self, x, t):
    # Compute the output of the diffusion model for a given input and timestep
    z = self.encode(x) # Encode the input into a latent vector
    z += self.timestep_embed(t) # Add the timestep embedding to the latent vector
    x_hat = self.decode(z) # Decode the latent vector into an output image
    return x_hat

# Define the self-supervised training scheme
def train(model):
  # Load the unpaired images from a dataset (e.g., ImageNet)
  train_loader = torch.utils.data.DataLoader(torchvision.datasets.ImageNet(root='./data', split='train', transform=torchvision.transforms.ToTensor()), batch_size=batch_size, shuffle=True)
  # Define the optimizer (e.g., Adam)
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  # Define the loss function (e.g., L2 loss)
  criterion = torch.nn.MSELoss()
  # Define the classifier-free guidance network (e.g., ResNet-50 with pretrained weights)
  guidance = torchvision.models.resnet50(pretrained=True)
  guidance.eval() # Set the guidance network to evaluation mode
  for epoch in range(num_epochs):
    for source_image, _ in train_loader:
      exemplar_image, _ = next(iter(train_loader)) # Sample another batch of images as exemplars
      source_image = source_image.to('cuda') # Move the source image to GPU
      exemplar_image = exemplar_image.to('cuda') # Move the exemplar image to GPU
      optimizer.zero_grad() # Zero out the gradients
      loss = 0.0 # Initialize the loss to zero
      
      for t in range(num_timesteps):
        beta_t = model.betas[t] # Get the noise level at timestep t
        alpha_t = 1 - beta_t / (1 - beta_min) # Get the reverse coefficient at timestep t
        
        if t == 0:
          epsilon_t = torch.randn_like(source_image) * np.sqrt(beta_t / (1 - beta_min)) # Sample Gaussian noise at timestep t
          noised_source_image = source_image * np.sqrt(1 - beta_min) + epsilon_t * np.sqrt(1 - alpha_t) # Add noise to source image at timestep t
          noised_exemplar_image = exemplar_image * np.sqrt(1 - beta_min) + epsilon_t * np.sqrt(1 - alpha_t) # Add noise to exemplar image at timestep t
        else:
          epsilon_t = torch.randn_like(source_image) * np.sqrt(beta_t) # Sample Gaussian noise at timestep t
          noised_source_image = noised_source_image * np.sqrt(1 - beta_t) + epsilon_t # Add noise to source image at timestep t
          noised_exemplar_image = noised_exemplar_image * np.sqrt(1 - beta_t) + epsilon_t # Add noise to exemplar image at timestep t
        
        # Apply random augmentations to both images
        noised_source_image = augment(noised_source_image)
        noised_exemplar_image = augment(noised_exemplar_image)
        
        # Encode both images into latent vectors
        source_latent = model.encode(noised_source_image)
        exemplar_latent = model.encode(noised_exemplar_image)
        
        # Swap the latent vectors of the masked regions
        mask = generate_random_mask() # Generate a random mask with an arbitrary shape
        swapped_latent = swap_latent(source_latent, exemplar_latent, mask) # Swap the latent vectors according to the mask
        
        # Decode the swapped latent vector into an edited image
        edited_image = model.decode(swapped_latent)
        
        # Compute the reconstruction loss
        recon_loss = criterion(edited_image, noised_source_image)
        
        # Compute the information bottleneck loss
        info_loss = torch.mean(torch.norm(source_latent - exemplar_latent, dim=1))
        
        # Compute the classifier-free guidance loss
        guidance_loss = criterion(guidance(edited_image * mask), guidance(exemplar_image * mask))
        
        # Compute the total loss as a weighted sum of the three losses
        total_loss = recon_loss + 0.01 * info_loss + 0.1 * guidance_loss
        
        # Accumulate the loss over timesteps
        loss += total_loss
      
      # Backpropagate the loss and update the model parameters
      loss.backward()
      optimizer.step()
      
      # Print the loss for every 100 batches
      if batch_idx % 100 == 0:
        print(f'Epoch {epoch}, Batch {batch_idx}, Loss {loss.item()}')
  
  # Save the model
  torch.save(model, 'model.pth')

# Define the editing process
def edit(model, source_image, exemplar_image, mask):
  # Load the model from a file
  model = torch.load('model.pth')
  model.eval() # Set the model to evaluation mode
  # Move the images and mask to GPU
  source_image = source_image.to('cuda')
  exemplar_image = exemplar_image.to('cuda')
  mask = mask.to('cuda')
  # Encode both images into latent vectors
  source_latent = model.encode(source_image)
  exemplar_latent = model.encode(exemplar_image)
  # Swap the latent vectors of the masked regions
  swapped_latent = swap_latent(source_latent, exemplar_latent, mask)
  # Decode the swapped latent vector into an edited image
  edited_image = model.decode(swapped_latent)
  return edited_image

# Define some helper functions

def augment(x):
  # Apply random augmentations to an image tensor, such as cropping, flipping, rotating, etc.
  x = torchvision.transforms.RandomCrop(image_size)(x)
  x = torchvision.transforms.RandomHorizontalFlip()(x)
  x = torchvision.transforms.RandomRotation(30)(x)
  return x

def generate_random_mask():
  # Generate a random mask tensor with an arbitrary shape, such as a circle, a rectangle, a polygon, etc.
  mask = torch.zeros(1, image_size, image_size)
  shape = np.random.choice(['circle', 'rectangle', 'polygon'])
  if shape == 'circle':
    center_x = np.random.randint(image_size // 4, image_size * 3 // 4)
    center_y = np.random.randint(image_size // 4, image_size * 3 // 4)
    radius = np.random.randint(image_size // 8, image_size // 4)
    for i in range(image_size):
      for j in range(image_size):
        if (i - center_x) ** 2 + (j - center_y) ** 2 <= radius ** 2:
          mask[0, i, j] = 1.0
  elif shape == 'rectangle':
    left_x = np.random.randint(0, image_size // 2)
    right_x = np.random.randint(image_size // 2, image_size)
    top_y = np.random.randint(0, image_size // 2)
    bottom_y = np.random.randint(image_size // 2, image_size)
    for i in range(left_x, right_x):