---
title: 2001.03640v2 Unsupervised multi-modal Styled Content Generation
date: 2020-01-04
---

# [Unsupervised multi-modal Styled Content Generation](http://arxiv.org/abs/2001.03640v2)

authors: Omry Sendik, Dani Lischinski, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/2001.03640v2.pdf "Unsupervised Multi-modal Styled Content Generation - arXiv.org"
[2]: https://arxiv.org/abs/2001.03640 "[2001.03640] Unsupervised multi-modal Styled Content Generation - arXiv.org"
[3]: http://export.arxiv.org/abs/1504.03640v2 "[1504.03640v2] Lessons from the operation of the \"Penning-Fluorescent ..."

Here is a summary of the paper:

- **What**: The paper introduces a novel architecture called **uMM-GAN** (unsupervised Multi-modal Styled Content Generation) that can generate graphical content with multiple modes and styles in an unsupervised manner[^1^][1].
- **Why**: The paper aims to address the limitations of existing generative models, such as StyleGAN, that are not well-suited for scenarios where the distribution of the training data exhibits a multi-modal behavior and that cannot control the mode of the generated samples independently of the other visual attributes[^1^][1].
- **How**: The paper builds upon the StyleGAN architecture and adds a new component called **mode selector**, which learns multiple modes from the data and combines them using a set of learned weights. The paper also introduces a new loss function called **mode consistency loss**, which encourages the disentanglement between modes and styles. The paper demonstrates the effectiveness of uMM-GAN on various datasets and applications, such as generating images of cats and dogs, faces with different expressions, and 3D shapes with different categories[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- A novel generative model that can learn and control multiple modes and styles in an unsupervised fashion.
- A new mode selector component that can effectively approximate a complex distribution as a superposition of multiple simple ones.
- A new mode consistency loss function that can enforce the disentanglement between modes and styles.
- A comprehensive evaluation of uMM-GAN on various datasets and applications, showing its advantages over existing methods.

## Method Summary

The method section of the paper describes the details of the uMM-GAN architecture and the mode consistency loss function. The uMM-GAN consists of three main components: a generator, a discriminator, and a mode selector. The generator is based on the StyleGAN architecture, which uses adaptive instance normalization (AdaIN) to inject style parameters into the network. The discriminator is a standard convolutional network that tries to distinguish between real and fake samples. The mode selector is a novel component that learns to assign a mode to each sample and to combine multiple modes using a set of learned weights. The mode selector consists of two sub-networks: a mode encoder and a mode mixer. The mode encoder takes an input sample and outputs a one-hot vector indicating the mode of the sample. The mode mixer takes a latent vector and outputs a set of weights that are used to linearly combine multiple modes in the generator. The paper also introduces a new loss function called mode consistency loss, which aims to ensure that the mode selector and the generator are consistent with each other. The mode consistency loss consists of two terms: a mode reconstruction term and a style reconstruction term. The mode reconstruction term penalizes the discrepancy between the mode predicted by the mode encoder and the mode generated by the generator. The style reconstruction term penalizes the discrepancy between the style parameters used by the generator and the style parameters reconstructed by the discriminator. The paper shows that by optimizing the mode consistency loss, uMM-GAN can effectively disentangle between modes and styles and generate diverse and realistic samples.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator, discriminator, and mode selector networks
generator = StyleGAN()
discriminator = ConvNet()
mode_selector = ModeSelector()

# Define the mode consistency loss function
def mode_consistency_loss(real_sample, fake_sample, latent_vector):
  # Get the mode and style parameters of the real sample
  real_mode = mode_selector.mode_encoder(real_sample)
  real_style = discriminator.style_reconstructor(real_sample)
  # Get the mode and style parameters of the fake sample
  fake_mode = mode_selector.mode_encoder(fake_sample)
  fake_style = discriminator.style_reconstructor(fake_sample)
  # Get the mode weights of the latent vector
  mode_weights = mode_selector.mode_mixer(latent_vector)
  # Compute the mode reconstruction term
  mode_reconstruction_loss = cross_entropy(real_mode, fake_mode) + cross_entropy(mode_weights, fake_mode)
  # Compute the style reconstruction term
  style_reconstruction_loss = l2_norm(real_style - fake_style)
  # Return the total loss
  return mode_reconstruction_loss + style_reconstruction_loss

# Train the uMM-GAN model
for epoch in range(num_epochs):
  for batch in data_loader:
    # Sample a batch of real samples from the data
    real_samples = batch
    # Sample a batch of latent vectors from a normal distribution
    latent_vectors = sample_normal(num_samples, latent_dim)
    # Generate a batch of fake samples using the generator and the latent vectors
    fake_samples = generator(latent_vectors)
    # Compute the adversarial loss using the discriminator
    adversarial_loss = discriminator_loss(real_samples, fake_samples) + generator_loss(fake_samples)
    # Compute the mode consistency loss using the mode selector
    consistency_loss = mode_consistency_loss(real_samples, fake_samples, latent_vectors)
    # Update the parameters of the generator, discriminator, and mode selector using gradient descent
    update_parameters(generator, discriminator, mode_selector, adversarial_loss + consistency_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
num_epochs = 100 # Number of training epochs
batch_size = 64 # Batch size
latent_dim = 512 # Dimension of the latent vector
num_modes = 4 # Number of modes to learn
style_dim = 512 # Dimension of the style parameters
image_size = 128 # Size of the input image
image_channels = 3 # Number of channels of the input image
lr = 0.0002 # Learning rate
beta1 = 0.5 # Beta1 parameter for Adam optimizer
beta2 = 0.999 # Beta2 parameter for Adam optimizer

# Define the generator network based on StyleGAN architecture
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the mapping network that maps the latent vector to an intermediate latent vector
    self.mapping = nn.Sequential(
      nn.Linear(latent_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU(),
      nn.Linear(style_dim, style_dim),
      nn.ReLU()
    )
    # Define the synthesis network that generates the image from the intermediate latent vector and the mode weights
    self.synthesis = nn.ModuleList([
      # Input block: 4 x 4 x (style_dim * num_modes)
      nn.Conv2d(style_dim * num_modes, style_dim * num_modes, kernel_size=3, padding=1),
      nn.LeakyReLU(0.2),
      # First AdaIN block: 8 x 8 x (style_dim * num_modes)
      nn.Upsample(scale_factor=2),
      nn.Conv2d(style_dim * num_modes, style_dim * num_modes, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes, style_dim),
      nn.LeakyReLU(0.2),
      nn.Conv2d(style_dim * num_modes, style_dim * num_modes, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes, style_dim),
      nn.LeakyReLU(0.2),
      # Second AdaIN block: 16 x 16 x (style_dim * num_modes)
      nn.Upsample(scale_factor=2),
      nn.Conv2d(style_dim * num_modes, style_dim * num_modes // 2, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes // 2, style_dim),
      nn.LeakyReLU(0.2),
      nn.Conv2d(style_dim * num_modes // 2, style_dim * num_modes // 2, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes // 2, style_dim),
      nn.LeakyReLU(0.2),
      # Third AdaIN block: 32 x 32 x (style_dim * num_modes // 4)
      nn.Upsample(scale_factor=2),
      nn.Conv2d(style_dim * num_modes // 2, style_dim * num_modes // 4, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes // 4, style_dim),
      nn.LeakyReLU(0.2),
      nn.Conv2d(style_dim * num_modes // 4, style_dim * num_modes // 4, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes // 4, style_dim),
      nn.LeakyReLU(0.2),
      # Fourth AdaIN block: 64 x 64 x (style_dim * num_modes // 8)
      nn.Upsample(scale_factor=2),
      nn.Conv2d(style_dim * num_modes // 4, style_dim * num_modes // 8, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes // 8, style_dim),
      nn.LeakyReLU(0.2),
      nn.Conv2d(style_dim * num_modes // 8, style_dim * num_modes // 8, kernel_size=3, padding=1),
      AdaIN(style_dim * num_modes // 8, style_dim),
      nn.LeakyReLU(0.2),
      # Output block: 128 x 128 x image_channels
      nn.Upsample(scale_factor=2),
      nn.Conv2d(style_dim * num_modes // 8, image_channels, kernel_size=1),
      nn.Tanh()
    ])

  def forward(self, latent_vector, mode_weights):
    # Map the latent vector to an intermediate latent vector
    intermediate_latent_vector = self.mapping(latent_vector)
    # Repeat the intermediate latent vector num_modes times
    intermediate_latent_vector = intermediate_latent_vector.unsqueeze(1).repeat(1, num_modes, 1)
    # Multiply the intermediate latent vector by the mode weights
    intermediate_latent_vector = intermediate_latent_vector * mode_weights.unsqueeze(2)
    # Reshape the intermediate latent vector to a 4 x 4 x (style_dim * num_modes) tensor
    intermediate_latent_vector = intermediate_latent_vector.view(-1, style_dim * num_modes, 4, 4)
    # Pass the intermediate latent vector through the synthesis network
    output = intermediate_latent_vector
    for layer in self.synthesis:
      if isinstance(layer, AdaIN):
        output = layer(output, intermediate_latent_vector)
      else:
        output = layer(output)
    # Return the generated image
    return output

# Define the AdaIN layer that injects style parameters into the synthesis network
class AdaIN(nn.Module):
  def __init__(self, channels, style_dim):
    super(AdaIN, self).__init__()
    # Define the affine transformation parameters for each channel
    self.affine_transform = nn.Linear(style_dim, channels * 2)
    # Initialize the parameters
    self.affine_transform.weight.data[:, :channels] = 1.0
    self.affine_transform.weight.data[:, channels:] = 0.0

  def forward(self, input, style):
    # Compute the mean and standard deviation of the input along the spatial dimensions
    mean = input.mean([2, 3], keepdim=True)
    std = input.std([2, 3], keepdim=True) + 1e-8
    # Normalize the input
    normalized_input = (input - mean) / std
    # Compute the affine transformation parameters from the style
    style_transform = self.affine_transform(style).view(-1, 2, input.size(1), 1, 1)
    scale = style_transform[:, 0]
    bias = style_transform[:, 1]
    # Apply the affine transformation to the normalized input
    output = normalized_input * scale + bias
    # Return the output
    return output

# Define the discriminator network based on a standard convolutional network with an additional style reconstructor component
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Define the convolutional network that classifies real and fake samples
    self.conv_net = nn.Sequential(
      # Input block: 128 x 128 x image_channels
      nn.Conv2d(image_channels, style_dim * num_modes // 8, kernel_size=4, stride=2, padding=1),
      nn.LeakyReLU(0.2),
      # First convolutional block: 64 x 64 x (style_dim * num_modes // 8)
      nn.Conv2d(style_dim * num_modes // 8, style_dim * num_modes // 4, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(style_dim * num_modes // 4),
      nn.LeakyReLU(0.2),
      # Second convolutional block: 32 x 32 x (style_dim * num_modes // 4)
      nn.Conv2d(style_dim * num_modes // 4, style_dim * num_modes // 2, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(style_dim * num_modes // 2),
      nn.LeakyReLU(0.2),
      # Third convolutional block: 16 x 16 x (style_dim * num_modes // 2)
      nn.Conv2d(style_dim * num_modes // 2, style_dim * num_modes, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(style_dim * num_modes),
      nn.LeakyReLU(0.2),
      # Fourth convolutional block: 8 x 8 x (style_dim * num_modes)
      nn.Conv2d(style_dim * num_modes, style_dim * num_modes * 2, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(style_dim * num_modes * 2),
      nn.LeakyReLU(0.2),
      # Output block: a single value indicating real or fake