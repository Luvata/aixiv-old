---
title: 2204.09222v2 K-LITE  Learning Transferable Visual Models with External Knowledge
date: 2022-04-10
---

# [K-LITE: Learning Transferable Visual Models with External Knowledge](http://arxiv.org/abs/2204.09222v2)

authors: Sheng Shen, Chunyuan Li, Xiaowei Hu, Jianwei Yang, Yujia Xie, Pengchuan Zhang, Zhe Gan, Lijuan Wang, Lu Yuan, Ce Liu, Kurt Keutzer, Trevor Darrell, Anna Rohrbach, Jianfeng Gao


## What, Why and How

[1]: https://arxiv.org/pdf/2204.09222v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2204.09222 "[2204.09222] K-LITE: Learning Transferable Visual Models ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.09222v2 "[2204.09222v2] K-LITE: Learning Transferable Visual Models with ..."

Here is a summary of the paper:

- **What**: The paper proposes K-LITE, a strategy to leverage external knowledge for building transferable visual models that can perform image classification and object detection on various datasets.
- **Why**: The paper argues that learning with external knowledge is a promising way to achieve sample efficiency and broad concept coverage, compared to learning from natural language supervision alone.
- **How**: The paper enriches entities in text with WordNet and Wiktionary knowledge, and uses them to train and evaluate visual models that can reference or describe visual concepts using knowledge-augmented text. The paper shows that K-LITE improves transfer learning performance over existing methods on 20 image classification datasets and 13 object detection datasets[^1^][2].

## Main Contributions

[1]: https://arxiv.org/pdf/2204.09222v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2204.09222 "[2204.09222] K-LITE: Learning Transferable Visual Models ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.09222v2 "[2204.09222v2] K-LITE: Learning Transferable Visual Models with ..."

According to the paper[^1^][1], the main contributions are:

- **A simple and scalable strategy to leverage external knowledge for building transferable visual models**: The paper proposes K-LITE, which enriches entities in text with WordNet and Wiktionary knowledge, and uses them to train and evaluate visual models that can reference or describe visual concepts using knowledge-augmented text.
- **A comprehensive study of the impact of external knowledge on two important computer vision problems**: The paper evaluates K-LITE on image classification and object detection, benchmarking on 20 and 13 different existing datasets, respectively.
- **A significant improvement in transfer learning performance over existing methods**: The paper shows that K-LITE outperforms state-of-the-art language-augmented visual models, such as CLIP, ALIGN and Florence, on various zero-shot and few-shot transfer learning tasks.

## Method Summary

[1]: https://arxiv.org/pdf/2204.09222v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2204.09222 "[2204.09222] K-LITE: Learning Transferable Visual Models ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.09222v2 "[2204.09222v2] K-LITE: Learning Transferable Visual Models with ..."

Here is a summary of the method section of the paper:

- **Data preparation**: The paper collects a large-scale dataset of image-text pairs from the Internet, and enriches the text with external knowledge from WordNet and Wiktionary. The paper also creates a knowledge-augmented vocabulary that covers both natural language and external knowledge tokens.
- **Model architecture**: The paper adopts a vision-language transformer model that consists of a convolutional neural network (CNN) encoder for images and a transformer encoder for text. The paper also introduces a knowledge-aware attention mechanism that allows the model to attend to relevant knowledge tokens based on the input text.
- **Training objective**: The paper trains the model with a contrastive learning objective that maximizes the cosine similarity between image and text embeddings from matched pairs, and minimizes it for unmatched pairs. The paper also applies data augmentation techniques such as random cropping, color jittering, and text shuffling to increase the diversity of the training data.
- **Evaluation protocol**: The paper evaluates the model on two computer vision tasks: image classification and object detection. For image classification, the paper uses zero-shot and few-shot settings, where the model is given a knowledge-augmented text query that describes a visual concept, and is asked to retrieve or classify images that match the query. For object detection, the paper uses a zero-shot setting, where the model is given an image and a set of knowledge-augmented text queries that describe object categories, and is asked to localize and label the objects in the image. The paper uses various metrics such as top-k accuracy, mean average precision (mAP), and normalized discounted cumulative gain (NDCG) to measure the performance of the model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Data preparation
image_text_pairs = collect_image_text_pairs_from_internet()
knowledge_tokens = extract_knowledge_tokens_from_wordnet_and_wiktionary()
knowledge_augmented_text = enrich_text_with_knowledge_tokens(image_text_pairs, knowledge_tokens)
knowledge_augmented_vocab = create_vocab_from_knowledge_augmented_text()

# Model architecture
image_encoder = CNN_Encoder()
text_encoder = Transformer_Encoder(knowledge_augmented_vocab)
knowledge_aware_attention = Knowledge_Aware_Attention(text_encoder, knowledge_tokens)

# Training objective
contrastive_loss = Contrastive_Loss()
for image, text in image_text_pairs:
  # Apply data augmentation
  image = augment_image(image)
  text = augment_text(text)
  # Encode image and text
  image_embedding = image_encoder(image)
  text_embedding = text_encoder(text)
  # Compute contrastive loss
  loss = contrastive_loss(image_embedding, text_embedding)
  # Update model parameters
  update_model_parameters(loss)

# Evaluation protocol
for task in [image_classification, object_detection]:
  for setting in [zero_shot, few_shot]:
    for dataset in task.datasets:
      for query in dataset.queries:
        # Augment query with knowledge tokens
        query = augment_query_with_knowledge_tokens(query, knowledge_tokens)
        # Encode query
        query_embedding = text_encoder(query)
        if task == image_classification:
          # Retrieve or classify images based on query embedding
          images = retrieve_or_classify_images(query_embedding, dataset.images)
          # Compute metrics such as top-k accuracy and NDCG
          metrics = compute_metrics(images, query, dataset.labels)
        elif task == object_detection:
          # Localize and label objects in image based on query embeddings
          objects = localize_and_label_objects(query_embedding, dataset.image)
          # Compute metrics such as mAP and NDCG
          metrics = compute_metrics(objects, query, dataset.labels)
      # Report metrics for each dataset and setting
      report_metrics(metrics, dataset, setting)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import transformers
import nltk
import wikipedia

# Data preparation
def collect_image_text_pairs_from_internet():
  # Use a web crawler to scrape image-text pairs from various sources such as Wikipedia, Flickr, etc.
  # Filter out low-quality or irrelevant pairs based on image resolution, text length, etc.
  # Return a list of image-text pairs
  pass

def extract_knowledge_tokens_from_wordnet_and_wiktionary():
  # Use nltk to access WordNet and Wiktionary databases
  # For each word in the vocabulary, find its synonyms, hypernyms, hyponyms, definitions, etc. from WordNet and Wiktionary
  # Format the knowledge tokens as [word]_[type]_[knowledge], e.g., dog_synonym_canine, dog_hypernym_animal, dog_definition_a_domesticated_carnivorous_mammal, etc.
  # Return a list of knowledge tokens
  pass

def enrich_text_with_knowledge_tokens(image_text_pairs, knowledge_tokens):
  # For each image-text pair, tokenize the text and identify the entities (nouns, proper nouns, etc.)
  # For each entity, find the matching knowledge tokens from the list and append them to the text
  # Return a list of image-knowledge_augmented_text pairs
  pass

def create_vocab_from_knowledge_augmented_text():
  # Use transformers to create a vocabulary from the knowledge-augmented text
  # Use a subword tokenizer such as Byte-Pair Encoding (BPE) or SentencePiece to handle out-of-vocabulary words
  # Return a vocabulary object
  pass

# Model architecture
def CNN_Encoder():
  # Use torchvision to create a CNN encoder such as ResNet or EfficientNet
  # Remove the last classification layer and return the output of the penultimate layer as the image embedding
  # Return a CNN encoder object
  pass

def Transformer_Encoder(vocab):
  # Use transformers to create a transformer encoder such as BERT or GPT-3
  # Use the vocab object as the input vocabulary for the encoder
  # Return the output of the last layer as the text embedding
  # Return a transformer encoder object
  pass

def Knowledge_Aware_Attention(text_encoder, knowledge_tokens):
  # Use torch to create a knowledge-aware attention mechanism that modifies the self-attention layer of the text encoder
  # For each input text token, compute its attention score with each knowledge token based on their embeddings
  # Normalize the attention scores using softmax and use them to weight the knowledge token embeddings
  # Add the weighted knowledge token embeddings to the input text token embeddings and pass them to the next layer of the text encoder
  # Return a knowledge-aware attention object
  pass

# Training objective
def Contrastive_Loss():
  # Use torch to create a contrastive loss function that takes image and text embeddings as inputs
  # Compute the cosine similarity between image and text embeddings for matched and unmatched pairs
  # Use a margin-based hinge loss to maximize the similarity for matched pairs and minimize it for unmatched pairs
  # Return a contrastive loss object
  pass

def augment_image(image):
  # Use torchvision to apply data augmentation techniques to the image such as random cropping, color jittering, horizontal flipping, etc.
  # Return an augmented image
  pass

def augment_text(text):
  # Use nltk to apply data augmentation techniques to the text such as word replacement, word insertion, word deletion, word swapping, etc.
  # Return an augmented text
  pass

def update_model_parameters(loss):
  # Use torch to create an optimizer such as Adam or SGD that updates the model parameters based on the loss value and a learning rate schedule
  # Perform a backward pass to compute the gradients of the loss with respect to the model parameters
  # Perform an optimizer step to update the model parameters using the gradients
  pass

# Evaluation protocol
def augment_query_with_knowledge_tokens(query, knowledge_tokens):
  # Tokenize the query and identify the entities (nouns, proper nouns, etc.)
  # For each entity, find the matching knowledge tokens from the list and append them to the query
  # Return a knowledge-augmented query
  pass

def retrieve_or_classify_images(query_embedding, images):
  # Encode each image in the images list using the image encoder and obtain their embeddings
  # Compute the cosine similarity between query embedding and each image embedding 
  # Sort the images based on their similarity scores in descending order and return them as a ranked list for retrieval task
  # Alternatively, assign the label of the most similar image to the query and return it as a classification task
  pass

def localize_and_label_objects(query_embedding, image):
  # Encode the image using the image encoder and obtain its embedding
  # Use a region proposal network (RPN) such as Faster R-CNN or YOLO to generate candidate bounding boxes for objects in the image
  # For each bounding box, extract its image features using a region of interest (ROI) pooling layer and concatenate them with the image embedding
  # Use a transformer decoder to generate text embeddings for each bounding box based on the concatenated features
  # Compute the cosine similarity between query embedding and each text embedding
  # For each query, find the bounding box with the highest similarity score and assign it as the object localization and label
  # Return a list of objects with their localizations and labels
  pass

def compute_metrics(predictions, query, labels):
  # Use torch or sklearn to compute various metrics based on the predictions, query, and labels
  # For image classification, use metrics such as top-k accuracy and NDCG
  # For object detection, use metrics such as mAP and NDCG
  # Return a dictionary of metrics
  pass

def report_metrics(metrics, dataset, setting):
  # Print or plot the metrics for each dataset and setting
  # Compare the metrics with existing methods such as CLIP, ALIGN, and Florence
  pass
```