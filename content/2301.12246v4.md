---
title: 2301.12246v4 A Closer Look at Few-shot Classification Again
date: 2023-01-13
---

# [A Closer Look at Few-shot Classification Again](http://arxiv.org/abs/2301.12246v4)

authors: Xu Luo, Hao Wu, Ji Zhang, Lianli Gao, Jing Xu, Jingkuan Song


## What, Why and How

[1]: https://arxiv.org/abs/2301.12246 "[2301.12246] A Closer Look at Few-shot Classification Again - arXiv.org"
[2]: https://arxiv.org/pdf/2301.12246.pdf "A Closer Look at Few-shot Classification Again - arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2301.12246v4 "[2301.12246v4] A Closer Look at Few-shot Classification Again"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper is about few-shot classification, which is the problem of learning a model that can adapt to new classification tasks with only a few labeled samples.
- **Why**: The paper aims to better understand the key aspects of few-shot classification and the connections with other fields such as visual representation learning and transfer learning. The paper also reveals some interesting insights and research challenges that may inspire future work in related directions.
- **How**: The paper empirically proves that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. The paper conducts a meta-analysis for each phase and compares different methods based on various criteria such as performance, robustness, efficiency, and interpretability. The paper also proposes a simple yet effective adaptation algorithm called Meta-Logistic Regression (MLR) that outperforms existing methods on several benchmarks.

## Main Contributions

[1]: https://arxiv.org/abs/2301.12246v2 "[2301.12246v2] A Closer Look at Few-shot Classification Again - arXiv.org"
[2]: http://arxiv-export2.library.cornell.edu/abs/2301.12246v4 "[2301.12246v4] A Closer Look at Few-shot Classification Again"
[3]: https://arxiv.org/pdf/2301.04191.pdf "arXiv.org"

According to the paper[^1^][1], the main contributions are:

- **Empirically proving** that the training algorithm and the adaptation algorithm can be completely disentangled for few-shot classification, which enables independent analysis and design for each phase.
- **Conducting a meta-analysis** for each phase and comparing different methods based on various criteria such as performance, robustness, efficiency, and interpretability.
- **Proposing a simple yet effective adaptation algorithm** called Meta-Logistic Regression (MLR) that outperforms existing methods on several benchmarks.

## Method Summary

[1]: https://arxiv.org/abs/2301.02060 "[2301.02060] A first-order augmented Lagrangian method for constrained ..."
[2]: https://arxiv.org/abs/2301.12246v2 "[2301.12246v2] A Closer Look at Few-shot Classification Again - arXiv.org"
[3]: https://apastyle.apa.org/instructional-aids/student-paper-setup-guide.pdf "Student Paper Setup Guide, APA Style 7th Edition"

Here is a summary of the method section of the paper[^1^][1]:

- The paper divides the few-shot classification problem into two phases: the training phase and the adaptation phase. The paper defines a general formulation for each phase and introduces some notation and terminology.
- The paper proposes a simple adaptation algorithm called Meta-Logistic Regression (MLR) that learns a linear classifier for each task using the learned feature embeddings and the task-specific labeled samples. The paper derives the closed-form solution for MLR and shows that it can be efficiently computed using matrix operations.
- The paper conducts a meta-analysis for each phase and compares different methods based on various criteria such as performance, robustness, efficiency, and interpretability. The paper uses several benchmarks and evaluation protocols to ensure fair and comprehensive comparisons. The paper also discusses some limitations and challenges of existing methods and suggests some possible future directions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Training phase
Input: a training set D_train with N classes and M samples per class
Output: a feature embedding function f_theta

# Initialize f_theta randomly
f_theta = random_init()

# Loop until convergence or maximum iterations
while not converged or max_iter:

  # Sample a batch of tasks from D_train
  tasks = sample_tasks(D_train)

  # For each task in the batch
  for task in tasks:

    # Get the support set and the query set for the task
    support_set = task.support_set
    query_set = task.query_set

    # Compute the feature embeddings for the support set and the query set
    support_features = f_theta(support_set)
    query_features = f_theta(query_set)

    # Compute the loss for the task using a suitable loss function (e.g., cross-entropy)
    task_loss = loss(support_features, query_features)

    # Accumulate the gradients for f_theta
    gradients += compute_gradients(task_loss, f_theta)

  # Update f_theta using a suitable optimizer (e.g., SGD)
  f_theta = update(f_theta, gradients)

# Return f_theta
return f_theta

# Adaptation phase
Input: a feature embedding function f_theta, a test set D_test with K classes and S samples per class
Output: the accuracy on D_test

# Initialize the accuracy to zero
accuracy = 0

# Loop over all tasks in D_test
for task in D_test:

  # Get the support set and the query set for the task
  support_set = task.support_set
  query_set = task.query_set

  # Compute the feature embeddings for the support set and the query set
  support_features = f_theta(support_set)
  query_features = f_theta(query_set)

  # Apply Meta-Logistic Regression (MLR) to learn a linear classifier for the task
  classifier = MLR(support_features, support_labels)

  # Predict the labels for the query set using the classifier
  query_pred = classifier(query_features)

  # Compute the accuracy for the task and add it to the total accuracy
  task_accuracy = accuracy(query_pred, query_labels)
  accuracy += task_accuracy

# Return the average accuracy over all tasks in D_test
return accuracy / len(D_test)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import some libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Define some hyperparameters
N = 64 # number of classes in the training set
M = 16 # number of samples per class in the training set
K = 5 # number of classes in the test set
S = 5 # number of samples per class in the test set
B = 32 # batch size of tasks
D = 512 # dimension of feature embeddings
L = 1000 # maximum number of iterations

# Define a feature embedding function f_theta using a convolutional neural network (CNN)
f_theta = nn.Sequential(
  nn.Conv2d(3, 64, 3, padding=1),
  nn.BatchNorm2d(64),
  nn.ReLU(),
  nn.MaxPool2d(2),
  nn.Conv2d(64, 128, 3, padding=1),
  nn.BatchNorm2d(128),
  nn.ReLU(),
  nn.MaxPool2d(2),
  nn.Conv2d(128, 256, 3, padding=1),
  nn.BatchNorm2d(256),
  nn.ReLU(),
  nn.MaxPool2d(2),
  nn.Conv2d(256, D, 3, padding=1),
  nn.BatchNorm2d(D),
  nn.ReLU(),
  nn.AdaptiveAvgPool2d((1,1)),
)

# Define a loss function using cross-entropy
loss = nn.CrossEntropyLoss()

# Define an optimizer using stochastic gradient descent (SGD)
optimizer = optim.SGD(f_theta.parameters(), lr=0.01, momentum=0.9)

# Load the training set D_train and the test set D_test
D_train = load_data('train')
D_test = load_data('test')

# Training phase
# Loop until convergence or maximum iterations
for iter in range(L):

  # Sample a batch of tasks from D_train
  tasks = sample_tasks(D_train, B)

  # Initialize the gradients to zero
  optimizer.zero_grad()

  # For each task in the batch
  for task in tasks:

    # Get the support set and the query set for the task
    support_set = task.support_set
    query_set = task.query_set

    # Compute the feature embeddings for the support set and the query set
    support_features = f_theta(support_set) # shape: (N*M, D)
    query_features = f_theta(query_set) # shape: (N*M, D)

    # Reshape and transpose the feature embeddings to match the labels
    support_features = support_features.view(N, M, D).mean(dim=1) # shape: (N, D)
    query_features = query_features.view(N*M, D).transpose(0,1) # shape: (D, N*M)

    # Compute the logits for the query set using a linear classifier with support features as weights
    logits = torch.matmul(support_features, query_features) # shape: (N, N*M)

    # Compute the loss for the task using cross-entropy
    task_loss = loss(logits, query_labels)

    # Accumulate the gradients for f_theta
    task_loss.backward()

  # Update f_theta using SGD
  optimizer.step()

# Adaptation phase
# Initialize the accuracy to zero
accuracy = 0

# Loop over all tasks in D_test
for task in D_test:

  # Get the support set and the query set for the task
  support_set = task.support_set
  query_set = task.query_set

  # Compute the feature embeddings for the support set and the query set
  support_features = f_theta(support_set) # shape: (K*S, D)
  query_features = f_theta(query_set) # shape: (K*S, D)

  # Reshape and transpose the feature embeddings to match the labels
  support_features = support_features.view(K, S, D).mean(dim=1) # shape: (K, D)
  query_features = query_features.view(K*S, D).transpose(0,1) # shape: (D, K*S)

  # Apply Meta-Logistic Regression (MLR) to learn a linear classifier for the task

  # Define a function to compute the softmax function along a given dimension
  def softmax(x, dim):
    e_x = torch.exp(x - x.max(dim=dim)[0].unsqueeze(dim))
    return e_x / e_x.sum(dim=dim).unsqueeze(dim)

  # Define a function to compute the log-sum-exp function along a given dimension
  def logsumexp(x, dim):
    max_x = x.max(dim=dim)[0].unsqueeze(dim)
    return max_x + torch.log(torch.exp(x - max_x).sum(dim=dim).unsqueeze(dim))

  # Define the MLR objective function
  def MLR_objective(beta):
    # beta: shape (K, D)
    # Compute the log-likelihood for the support set
    log_likelihood = torch.sum(beta * support_features * support_labels) - torch.sum(logsumexp(beta * support_features, dim=0))
    # Compute the regularization term using the query set
    regularization = torch.sum(logsumexp(beta * query_features, dim=0))
    # Return the negative of the objective function
    return - (log_likelihood - regularization)

  # Initialize beta randomly
  beta = torch.randn(K, D)

  # Loop until convergence or maximum iterations
  while not converged or max_iter:

    # Compute the gradient of the MLR objective function
    gradient = - (support_labels - softmax(beta * support_features, dim=0)) @ support_features + softmax(beta * query_features, dim=0) @ query_features

    # Update beta using gradient descent with a small step size
    beta = beta - 0.01 * gradient

  # Predict the labels for the query set using the classifier
  query_pred = torch.argmax(beta @ query_features, dim=0)

  # Compute the accuracy for the task and add it to the total accuracy
  task_accuracy = torch.mean((query_pred == query_labels).float())
  accuracy += task_accuracy

# Print the average accuracy over all tasks in D_test
print(accuracy / len(D_test))
```