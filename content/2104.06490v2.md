---
title: 2104.06490v2 DatasetGAN  Efficient Labeled Data Factory with Minimal Human Effort
date: 2021-04-07
---

# [DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort](http://arxiv.org/abs/2104.06490v2)

authors: Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio Torralba, Sanja Fidler


## What, Why and How

[1]: https://arxiv.org/abs/2104.06490 "[2104.06490] DatasetGAN: Efficient Labeled Data Factory with Minimal ..."
[2]: https://arxiv.org/pdf/2104.06490.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2104.06490 "[2104.06490] DatasetGAN: Efficient Labeled Data Factory with Minimal ..."

Here is a summary of the paper:

- **What**: The paper introduces DatasetGAN, an automatic procedure to generate massive datasets of high-quality semantically segmented images requiring minimal human effort.
- **Why**: The paper aims to address the problem of data-hungry deep networks that need large-scale annotated datasets, which are time consuming and expensive to create manually.
- **How**: The paper leverages the power of recent GANs to generate realistic images and shows how the GAN latent code can be decoded to produce a semantic segmentation of the image. The paper trains a decoder on a few labeled examples and then uses it to generate an infinite annotated dataset generator. The paper evaluates the approach on seven image segmentation tasks and shows that it outperforms semi-supervised baselines and is on par with fully supervised methods.

## Main Contributions

[1]: https://arxiv.org/abs/2104.06490 "[2104.06490] DatasetGAN: Efficient Labeled Data Factory with Minimal ..."
[2]: https://arxiv.org/pdf/2104.06490.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2104.06490 "[2104.06490] DatasetGAN: Efficient Labeled Data Factory with Minimal ..."

According to the paper[^1^][1], the main contributions are:

- **DatasetGAN**: A novel framework that leverages StyleGAN and a latent mapping network to generate realistic images and their corresponding semantic segmentations with minimal human effort.
- **Decoder training**: A simple yet effective method to train a decoder on a few labeled examples and generalize to the rest of the latent space, resulting in an infinite annotated dataset generator.
- **Detailed annotations**: A demonstration of how DatasetGAN can produce datasets with rich object and part segmentations, such as 34 human face parts and 32 car parts, by annotating images in extreme detail.
- **Empirical evaluation**: A comprehensive evaluation of DatasetGAN on seven image segmentation tasks, showing that it outperforms semi-supervised baselines significantly and is on par with fully supervised methods.

## Method Summary

[1]: https://arxiv.org/abs/2104.06490 "[2104.06490] DatasetGAN: Efficient Labeled Data Factory with Minimal ..."
[2]: https://arxiv.org/pdf/2104.06490.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2104.06490 "[2104.06490] DatasetGAN: Efficient Labeled Data Factory with Minimal ..."

Here is a summary of the method section of the paper:

- The paper proposes a framework that consists of three components: a StyleGAN generator, a latent mapping network, and a decoder network.
- The StyleGAN generator is trained on a large collection of real images and can produce realistic images given a latent code.
- The latent mapping network is trained to map the latent code to a semantic label vector that represents the class of each pixel in the image.
- The decoder network is trained to decode the semantic label vector into a semantic segmentation mask that matches the image generated by the StyleGAN.
- The paper trains the decoder network on a few labeled examples and then uses it to generate an infinite annotated dataset by sampling from the latent space of the StyleGAN.
- The paper evaluates the quality of the generated datasets by training various computer vision models on them and testing them on real images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the StyleGAN generator G
G = StyleGAN()

# Define the latent mapping network M
M = MLP()

# Define the decoder network D
D = UNet()

# Train G on real images
G.train(real_images)

# Generate a few synthetic images and manually annotate them
synthetic_images = G.sample(n)
synthetic_labels = manual_annotate(synthetic_images)

# Train M and D on the synthetic images and labels
M.train(synthetic_images, synthetic_labels)
D.train(M(synthetic_images), synthetic_labels)

# Generate a large synthetic dataset by sampling from the latent space of G
large_synthetic_images = G.sample(N)
large_synthetic_labels = D(M(large_synthetic_images))

# Train any computer vision model on the large synthetic dataset
model = VisionModel()
model.train(large_synthetic_images, large_synthetic_labels)

# Test the model on real images
model.test(real_images)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt

# Define the StyleGAN generator G
# Use the official implementation from https://github.com/NVlabs/stylegan2-ada-pytorch
G = torch.hub.load('NVlabs/stylegan2-ada-pytorch', 'generator', pretrained=True)

# Define the latent mapping network M
# Use a multi-layer perceptron with ReLU activations and a linear output layer
M = torch.nn.Sequential(
    torch.nn.Linear(512, 256),
    torch.nn.ReLU(),
    torch.nn.Linear(256, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, C), # C is the number of classes
)

# Define the decoder network D
# Use a U-Net architecture with skip connections and softmax output layer
D = torchvision.models.segmentation.fcn_resnet50(pretrained=False, num_classes=C)

# Define the loss function for M and D
# Use cross entropy loss for semantic segmentation
loss_fn = torch.nn.CrossEntropyLoss()

# Define the optimizer for M and D
# Use Adam optimizer with learning rate 0.001
optimizer = torch.optim.Adam(list(M.parameters()) + list(D.parameters()), lr=0.001)

# Train G on real images
# Use the official code from https://github.com/NVlabs/stylegan2-ada-pytorch
G.train(real_images)

# Generate a few synthetic images and manually annotate them
synthetic_images = G.sample(n) # n is a small number, e.g. 10
synthetic_labels = manual_annotate(synthetic_images) # use any annotation tool

# Train M and D on the synthetic images and labels for K epochs
for epoch in range(K):
    # Forward pass
    synthetic_logits = M(synthetic_images)
    synthetic_masks = D(synthetic_logits)
    
    # Compute loss
    loss = loss_fn(synthetic_masks, synthetic_labels)
    
    # Backward pass and update parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Print loss and accuracy
    print(f"Epoch {epoch}, Loss: {loss.item()}, Accuracy: {accuracy(synthetic_masks, synthetic_labels)}")

# Generate a large synthetic dataset by sampling from the latent space of G
large_synthetic_images = G.sample(N) # N is a large number, e.g. 10000
large_synthetic_labels = D(M(large_synthetic_images))

# Train any computer vision model on the large synthetic dataset for L epochs
model = VisionModel() # use any vision model, e.g. ResNet, VGG, etc.
model.train(large_synthetic_images, large_synthetic_labels, epochs=L)

# Test the model on real images
model.test(real_images)
```