---
title: 2305.10579v1 MultiPlaneNeRF  Neural Radiance Field with Non-Trainable Representation
date: 2023-05-11
---

# [MultiPlaneNeRF: Neural Radiance Field with Non-Trainable Representation](http://arxiv.org/abs/2305.10579v1)

authors: Dominik Zimny, Jacek Tabor, Maciej Zięba, Przemysław Spurek


## What, Why and How

[1]: https://arxiv.org/abs/2305.10579 "[2305.10579] MultiPlaneNeRF: Neural Radiance Field with Non-Trainable ..."
[2]: https://arxiv.org/pdf/2305.10579v1 "arXiv:2305.10579v1 [cs.CV] 17 May 2023"
[3]: http://export.arxiv.org/abs/2305.10579 "[2305.10579] MultiPlaneNeRF: Neural Radiance Field with Non-Trainable ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a new model called **MultiPlaneNeRF** that can efficiently represent 3D objects from 2D images using non-trainable representations.
- **Why**: The paper aims to overcome the limitations of vanilla NeRF, which requires training on each object separately, has a long training time, and does not generalize well to unseen data.
- **How**: The paper proposes to project 3D points on 2D images to produce non-trainable representations that encode the object's shape and color. The projection step is not parametrized, and a very shallow decoder can process the representation. The paper also shows how to train MultiPlaneNeRF on a large data set and generalize across many objects by replacing the 2D images. The paper evaluates MultiPlaneNeRF on several benchmarks and compares it with state-of-the-art models for synthesizing new views.

## Main Contributions

The paper claims to make the following contributions:

- It introduces **MultiPlaneNeRF**, a first model that can efficiently represent 3D objects from 2D images using non-trainable representations.
- It shows how to train MultiPlaneNeRF on a large data set and generalize across many objects by replacing the 2D images.
- It demonstrates that MultiPlaneNeRF achieves comparable results to state-of-the-art models for synthesizing new views and has generalization properties.

## Method Summary

[1]: https://arxiv.org/abs/2305.10579 "[2305.10579] MultiPlaneNeRF: Neural Radiance Field with Non-Trainable ..."
[2]: https://arxiv.org/pdf/2305.10579v1 "arXiv:2305.10579v1 [cs.CV] 17 May 2023"
[3]: http://export.arxiv.org/abs/2305.10579 "[2305.10579] MultiPlaneNeRF: Neural Radiance Field with Non-Trainable ..."

Here is a summary of the method section of the paper[^1^][2]:

- The paper proposes a new model called **MultiPlaneNeRF** that consists of two components: a **projection module** and an **implicit decoder**.
- The projection module takes as input a 3D point and a set of 2D images of the same object from different viewpoints. It projects the 3D point onto each image plane and extracts the corresponding pixel values. It then concatenates the pixel values along with the 3D point coordinates to form a non-trainable representation of the 3D point.
- The implicit decoder is a shallow multilayer perceptron (MLP) that takes as input the non-trainable representation and predicts the RGB color and volume density of the 3D point. The implicit decoder is trained on a large data set of objects and can generalize across different objects by replacing the 2D images in the projection module.
- The paper also introduces two variants of MultiPlaneNeRF: **SinglePlaneNeRF** and **TriPlaneNeRF**. SinglePlaneNeRF uses only one image plane for projection, while TriPlaneNeRF uses three orthogonal image planes for projection. The paper compares the performance and efficiency of these variants with vanilla NeRF and other baselines.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the projection module
def project(point, images):
  # point: a 3D point in the scene
  # images: a set of 2D images of the same object from different viewpoints
  # return: a non-trainable representation of the point

  # Initialize an empty list to store the pixel values
  pixels = []

  # For each image in the set
  for image in images:
    # Project the point onto the image plane using the camera parameters
    pixel = project_point(point, image.camera)
    # Extract the pixel value from the image
    value = image[pixel]
    # Append the value to the list
    pixels.append(value)

  # Concatenate the pixel values and the point coordinates
  representation = concatenate(pixels, point)

  # Return the representation
  return representation

# Define the implicit decoder
def decode(representation):
  # representation: a non-trainable representation of a 3D point
  # return: the RGB color and volume density of the point

  # Pass the representation through a shallow MLP
  output = MLP(representation)

  # Split the output into color and density
  color = output[:3]
  density = output[3]

  # Return the color and density
  return color, density

# Define the MultiPlaneNeRF model
def MultiPlaneNeRF(point, images):
  # point: a 3D point in the scene
  # images: a set of 2D images of the same object from different viewpoints
  # return: the RGB color and volume density of the point

  # Project the point onto the image planes to get the non-trainable representation
  representation = project(point, images)

  # Decode the representation to get the color and density
  color, density = decode(representation)

  # Return the color and density
  return color, density

# Train the implicit decoder on a large data set of objects and images
train_decoder(data_set)

# Synthesize new views of a given object by replacing the images in the projection module
synthesize_views(object, images)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from PIL import Image

# Define some constants
NUM_IMAGES = 8 # The number of images to use for projection
NUM_PLANES = 3 # The number of planes to use for TriPlaneNeRF
IMAGE_SIZE = 256 # The size of the input images
HIDDEN_SIZE = 128 # The size of the hidden layer in the MLP
OUTPUT_SIZE = 4 # The size of the output layer in the MLP (3 for color, 1 for density)
LEARNING_RATE = 0.001 # The learning rate for the optimizer
BATCH_SIZE = 1024 # The batch size for training
EPOCHS = 100 # The number of epochs for training
EPSILON = 1e-6 # A small constant to avoid division by zero

# Define a function to load an image and resize it
def load_image(path):
  # path: the path to the image file
  # return: a torch tensor of shape (3, IMAGE_SIZE, IMAGE_SIZE)

  # Load the image using PIL
  image = Image.open(path)

  # Resize the image to IMAGE_SIZE x IMAGE_SIZE
  image = image.resize((IMAGE_SIZE, IMAGE_SIZE))

  # Convert the image to a torch tensor and normalize it to [0, 1]
  image = transforms.ToTensor()(image)

  # Return the image tensor
  return image

# Define a function to project a point onto an image plane using the camera parameters
def project_point(point, camera):
  # point: a torch tensor of shape (3,) representing a 3D point in the scene
  # camera: a dictionary containing the camera parameters, such as focal length, principal point, rotation matrix, and translation vector
  # return: a torch tensor of shape (2,) representing the pixel coordinates on the image plane

  # Extract the camera parameters from the dictionary
  f = camera["focal_length"] # A torch tensor of shape (2,) representing the focal length in x and y directions
  c = camera["principal_point"] # A torch tensor of shape (2,) representing the principal point in x and y directions
  R = camera["rotation_matrix"] # A torch tensor of shape (3, 3) representing the rotation matrix from world coordinates to camera coordinates
  t = camera["translation_vector"] # A torch tensor of shape (3,) representing the translation vector from world origin to camera origin

  # Apply the rotation and translation to the point to get its coordinates in camera frame
  point = R @ point + t

  # Apply the perspective projection to get its coordinates on the image plane
  point = f * point[:2] / (point[2] + EPSILON) + c

  # Return the pixel coordinates
  return point

# Define a function to extract the pixel value from an image given its coordinates
def extract_pixel(image, pixel):
  # image: a torch tensor of shape (3, IMAGE_SIZE, IMAGE_SIZE) representing an image
  # pixel: a torch tensor of shape (2,) representing the pixel coordinates on the image plane
  # return: a torch tensor of shape (3,) representing the pixel value

  # Round the pixel coordinates to integers and clamp them to [0, IMAGE_SIZE - 1]
  pixel = torch.round(pixel).clamp(0, IMAGE_SIZE - 1).long()

  # Extract the pixel value from the image using indexing
  value = image[:, pixel[1], pixel[0]]

  # Return the pixel value
  return value

# Define a function to project a point onto a set of images and get its non-trainable representation
def project(point, images):
  # point: a torch tensor of shape (3,) representing a 3D point in the scene
  # images: a list of tuples containing an image tensor and a camera dictionary for each image
  # return: a torch tensor of shape (NUM_IMAGES * 3 + 3,) representing the non-trainable representation of the point

  # Initialize an empty list to store the pixel values
  pixels = []

  # For each image in the list
  for image, camera in images:
    # Project the point onto the image plane using the camera parameters
    pixel = project_point(point, camera)

    # Extract the pixel value from the image using indexing
    value = extract_pixel(image, pixel)

    # Append the value to the list as a numpy array
    pixels.append(value.numpy())

  # Convert the list of pixel values to a numpy array and flatten it
  pixels = np.concatenate(pixels)

  # Convert the point coordinates to a numpy array
  point = point.numpy()

  # Concatenate the pixel values and the point coordinates
  representation = np.concatenate([pixels, point])

  # Convert the representation to a torch tensor
  representation = torch.from_numpy(representation)

  # Return the representation
  return representation

# Define a class for the implicit decoder
class ImplicitDecoder(nn.Module):
  # A shallow MLP that takes as input a non-trainable representation and predicts the RGB color and volume density of the point

  def __init__(self):
    # Initialize the base class
    super(ImplicitDecoder, self).__init__()

    # Define the input size as NUM_IMAGES * 3 + 3
    self.input_size = NUM_IMAGES * 3 + 3

    # Define the hidden layer with HIDDEN_SIZE units and ReLU activation
    self.hidden = nn.Linear(self.input_size, HIDDEN_SIZE)
    self.relu = nn.ReLU()

    # Define the output layer with OUTPUT_SIZE units and sigmoid activation
    self.output = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)
    self.sigmoid = nn.Sigmoid()

  def forward(self, representation):
    # representation: a torch tensor of shape (batch_size, NUM_IMAGES * 3 + 3) representing a batch of non-trainable representations
    # return: a torch tensor of shape (batch_size, OUTPUT_SIZE) representing a batch of predicted colors and densities

    # Pass the representation through the hidden layer and apply ReLU activation
    x = self.relu(self.hidden(representation))

    # Pass the output through the output layer and apply sigmoid activation
    x = self.sigmoid(self.output(x))

    # Return the output
    return x

# Define a function to compute the loss function for training the implicit decoder
def compute_loss(output, target):
  # output: a torch tensor of shape (batch_size, OUTPUT_SIZE) representing a batch of predicted colors and densities
  # target: a torch tensor of shape (batch_size, OUTPUT_SIZE) representing a batch of ground truth colors and densities
  # return: a scalar torch tensor representing the loss value

  # Compute the mean squared error between the output and target
  mse = nn.MSELoss()(output, target)

  # Return the mse as the loss value
  return mse

# Define a function to train the implicit decoder on a data set of objects and images
def train_decoder(data_set):
  # data_set: a list of tuples containing an object name, a list of image paths, and a list of camera dictionaries for each object
  # return: None

  # Instantiate an implicit decoder object
  decoder = ImplicitDecoder()

  # Move the decoder to GPU if available
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  decoder.to(device)

  # Instantiate an optimizer object using Adam with LEARNING_RATE
  optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)

  # For each epoch in EPOCHS
  for epoch in range(EPOCHS):

    # Initialize an empty list to store the loss values for this epoch
    losses = []

    # Shuffle the data set randomly
    np.random.shuffle(data_set)

    # For each object in the data set
    for object_name, image_paths, camera_dicts in data_set:

      # Load the images from the paths and store them in a list along with their cameras
      images = []
      for image_path, camera_dict in zip(image_paths, camera_dicts):
        image = load_image(image_path)
        images.append((image, camera_dict))

      # Sample BATCH_SIZE random points from the object using its bounding box
      points = sample_points(object_name)

      # Project the points onto the images and get their non-trainable representations
      representations = []
      for point in points:
        representation = project(point, images)
        representations.append(representation)

      # Stack the representations into a batch tensor of shape (BATCH_SIZE, NUM_IMAGES * 3 + 3)
      representations = torch.stack(representations)

      # Get the ground truth colors and densities for the points using ray marching on vanilla NeRF
      targets = ray_march(points, object_name)

      # Stack the targets into a batch tensor of shape (BATCH_SIZE, OUTPUT_SIZE)
      targets = torch.stack(targets)

      # Move the representations and targets to GPU if available
      representations = representations.to(device)
      targets = targets.to(device)

      # Zero out the gradients of the decoder parameters
      optimizer.zero_grad()

      # Pass the representations through the decoder and get the outputs
      outputs = decoder(representations)

      # Compute the loss value using the outputs and targets
      loss = compute_loss(outputs, targets)