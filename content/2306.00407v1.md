---
title: 2306.00407v1 Towards Interactive Image Inpainting via Sketch Refinement
date: 2023-06-01
---

# [Towards Interactive Image Inpainting via Sketch Refinement](http://arxiv.org/abs/2306.00407v1)

authors: Chang Liu, Shunxin Xu, Jialun Peng, Kaidong Zhang, Dong Liu


## What, Why and How

[1]: https://arxiv.org/abs/2306.00407 "Towards Interactive Image Inpainting via Sketch Refinement"
[2]: https://arxiv.org/abs/2306.00763 "[2306.00763] Learning Disentangled Prompts for Compositional Image ..."
[3]: http://export.arxiv.org/abs/2306.00407 "[2306.00407] Towards Interactive Image Inpainting via Sketch Refinement"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a two-stage image inpainting method called SketchRefiner that can restore complex structures in corrupted regions with the help of user-provided sketches.
- **Why**: The paper aims to address the problem of interactive image inpainting, which is challenging because sketches are simple and intuitive but also have free forms with much randomness that may confuse the inpainting models and cause artifacts in the completed images.
- **How**: The paper uses a cross-correlation loss function to calibrate and refine the sketches in a coarse-to-fine fashion in the first stage, and then extracts informative features from the abstracted sketches and modulates the inpainting process in the second stage. The paper also proposes an algorithm to simulate real sketches automatically and builds a test protocol with different applications. The paper evaluates the method on public datasets and shows that it outperforms the state-of-the-art methods both qualitatively and quantitatively.

## Main Contributions

The paper claims the following contributions:

- A novel sketch refinement module that can robustly calibrate and abstract the user-provided sketches in a coarse-to-fine fashion using a cross-correlation loss function.
- A sketch feature modulation module that can learn to extract informative features from the refined sketches and modulate the inpainting process in the feature space.
- An automatic sketch simulation algorithm that can generate realistic sketches from images with different levels of complexity and randomness.
- A test protocol that can evaluate the performance of interactive image inpainting methods on different applications such as object removal, structure completion, and style transfer.

## Method Summary

The method section of the paper consists of four subsections:

- **Sketch Refinement Module**: This module takes an image with a corrupted region and a user-provided sketch as inputs and outputs a refined sketch that is more consistent with the image content and structure. The module consists of two subnetworks: a sketch calibration network and a sketch abstraction network. The sketch calibration network learns to align the sketch with the image using a cross-correlation loss function that measures the similarity between the sketch and the image features. The sketch abstraction network learns to remove the noise and redundancy in the sketch and generate a more abstract representation that can guide the inpainting process. The module operates in a coarse-to-fine fashion with multiple scales to handle different levels of details.
- **Sketch Feature Modulation Module**: This module takes the refined sketch and the image as inputs and outputs a completed image that fills in the corrupted region with realistic content. The module consists of two subnetworks: a sketch feature extraction network and an inpainting network. The sketch feature extraction network learns to extract informative features from the refined sketch that can capture the semantic and structural information. The inpainting network learns to generate realistic content in the corrupted region by modulating its features with the sketch features using adaptive instance normalization (AdaIN) layers. The module also uses a global-local discriminator to enforce the consistency between the completed region and the whole image.
- **Automatic Sketch Simulation Algorithm**: This algorithm takes an image as input and outputs a simulated sketch that mimics the user-provided sketches. The algorithm consists of three steps: edge detection, edge thinning, and edge perturbation. The edge detection step uses a pretrained edge detection model to extract edges from the image. The edge thinning step uses morphological operations to thin the edges and make them more like sketches. The edge perturbation step randomly adds or removes pixels along the edges to introduce noise and randomness in the sketches. The algorithm can control the level of complexity and randomness by adjusting the parameters of each step.
- **Test Protocol**: This protocol evaluates the performance of interactive image inpainting methods on different applications such as object removal, structure completion, and style transfer. The protocol uses two types of sketches: real sketches drawn by human users and simulated sketches generated by the automatic sketch simulation algorithm. The protocol also uses two types of metrics: quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and Fr√©chet inception distance (FID), and qualitative metrics such as user study and visual comparison. The protocol compares the proposed method with several state-of-the-art methods on public datasets such as CelebA-HQ, Places2, Paris StreetView, and ImageNet.


## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np
import cv2
import matplotlib.pyplot as plt

# Define hyperparameters
num_scales = 3 # number of scales for sketch refinement and inpainting
num_channels = 64 # number of channels for sketch refinement and inpainting networks
num_blocks = 8 # number of residual blocks for sketch refinement and inpainting networks
num_classes = 10 # number of classes for class-conditional generation
lambda_cc = 10 # weight for cross-correlation loss
lambda_fm = 10 # weight for feature matching loss
lambda_adv = 1 # weight for adversarial loss
lr = 0.0002 # learning rate
beta1 = 0.5 # beta1 for Adam optimizer
beta2 = 0.999 # beta2 for Adam optimizer
batch_size = 16 # batch size
num_epochs = 100 # number of epochs

# Define models
class SketchCalibrationNetwork(torch.nn.Module):
    def __init__(self, num_channels):
        super(SketchCalibrationNetwork, self).__init__()
        # Define the network architecture as a sequence of convolutional layers with leaky ReLU activation and instance normalization
        self.conv1 = torch.nn.Conv2d(4, num_channels, kernel_size=7, stride=1, padding=3)
        self.in1 = torch.nn.InstanceNorm2d(num_channels)
        self.lrelu1 = torch.nn.LeakyReLU(0.2)
        self.conv2 = torch.nn.Conv2d(num_channels, num_channels * 2, kernel_size=4, stride=2, padding=1)
        self.in2 = torch.nn.InstanceNorm2d(num_channels * 2)
        self.lrelu2 = torch.nn.LeakyReLU(0.2)
        self.conv3 = torch.nn.Conv2d(num_channels * 2, num_channels * 4, kernel_size=4, stride=2, padding=1)
        self.in3 = torch.nn.InstanceNorm2d(num_channels * 4)
        self.lrelu3 = torch.nn.LeakyReLU(0.2)
        self.conv4 = torch.nn.ConvTranspose2d(num_channels * 4, num_channels * 2, kernel_size=4, stride=2, padding=1)
        self.in4 = torch.nn.InstanceNorm2d(num_channels * 2)
        self.lrelu4 = torch.nn.LeakyReLU(0.2)
        self.conv5 = torch.nn.ConvTranspose2d(num_channels * 2, num_channels, kernel_size=4, stride=2, padding=1)
        self.in5 = torch.nn.InstanceNorm2d(num_channels)
        self.lrelu5 = torch.nn.LeakyReLU(0.2)
        self.conv6 = torch.nn.ConvTranspose2d(num_channels, 1, kernel_size=7, stride=1, padding=3)

    def forward(self, x):
        # x is a tensor of shape (batch_size, 4, height, width), where the first three channels are the image and the last channel is the sketch
        x = self.lrelu1(self.in1(self.conv1(x))) # (batch_size, num_channels, height, width)
        x = self.lrelu2(self.in2(self.conv2(x))) # (batch_size, num_channels * 2, height / 2, width / 2)
        x = self.lrelu3(self.in3(self.conv3(x))) # (batch_size, num_channels * 4, height / 4, width / 4)
        x = self.lrelu4(self.in4(self.conv4(x))) # (batch_size, num_channels * 2, height / 2, width / 2)
        x = self.lrelu5(self.in5(self.conv5(x))) # (batch_size, num_channels, height , width )
        x = torch.sigmoid(self.conv6(x)) # (batch_size , 1 , height , width )
        return x

class SketchAbstractionNetwork(torch.nn.Module):
    def __init__(self , num_channels , num_blocks):
        super(SketchAbstractionNetwork , self).__init__()
        # Define the network architecture as a sequence of convolutional layers with leaky ReLU activation and instance normalization , followed by residual blocks and transposed convolutional layers
        self.conv1 = torch.nn.Conv2d(5 , num_channels , kernel_size=7 , stride=1 , padding=3) 
        self.in1 = torch.nn.InstanceNorm2d(num_channels) 
        self.lrelu1 = torch.nn.LeakyReLU(0.2) 
        self.conv2 = torch.nn.Conv2d(num_channels , num_channels * 2 , kernel_size=4 , stride=2 , padding=1) 
        self.in2 = torch.nn.InstanceNorm2d(num_channels * 2) 
        self.lrelu2 = torch.nn.LeakyReLU(0.2) 
        self.conv3 = torch.nn.Conv2d(num_channels * 2 , num_channels * 4 , kernel_size=4 , stride=2 , padding=1) 
        self.in3 = torch.nn.InstanceNorm2d(num_channels * 4) 
        self.lrelu3 = torch.nn.LeakyReLU(0.2) 
        # Define the residual blocks as a list of modules
        self.res_blocks = torch.nn.ModuleList()
        for i in range(num_blocks):
            self.res_blocks.append(ResidualBlock(num_channels * 4))
        self.conv4 = torch.nn.ConvTranspose2d(num_channels * 4 , num_channels * 2 , kernel_size=4 , stride=2 , padding=1) 
        self.in4 = torch.nn.InstanceNorm2d(num_channels * 2) 
        self.lrelu4 = torch.nn.LeakyReLU(0.2) 
        self.conv5 = torch.nn.ConvTranspose2d(num_channels * 2 , num_channels , kernel_size=4 , stride=2 , padding=1) 
        self.in5 = torch.nn.InstanceNorm2d(num_channels) 
        self.lrelu5 = torch.nn.LeakyReLU(0.2) 
        self.conv6 = torch.nn.ConvTranspose2d(num_channels , 1 , kernel_size=7 , stride=1 , padding=3)

    def forward(self, x):
        # x is a tensor of shape (batch_size, 5, height, width), where the first three channels are the image, the fourth channel is the sketch, and the last channel is the mask
        x = self.lrelu1(self.in1(self.conv1(x))) # (batch_size, num_channels, height, width)
        x = self.lrelu2(self.in2(self.conv2(x))) # (batch_size, num_channels * 2, height / 2, width / 2)
        x = self.lrelu3(self.in3(self.conv3(x))) # (batch_size, num_channels * 4, height / 4, width / 4)
        # Apply the residual blocks
        for res_block in self.res_blocks:
            x = res_block(x) # (batch_size, num_channels * 4, height / 4, width / 4)
        x = self.lrelu4(self.in4(self.conv4(x))) # (batch_size, num_channels * 2, height / 2, width / 2)
        x = self.lrelu5(self.in5(self.conv5(x))) # (batch_size, num_channels, height , width )
        x = torch.sigmoid(self.conv6(x)) # (batch_size , 1 , height , width )
        return x

class ResidualBlock(torch.nn.Module):
    def __init__(self, num_channels):
        super(ResidualBlock, self).__init__()
        # Define the residual block as a sequence of convolutional layers with ReLU activation and instance normalization
        self.conv1 = torch.nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)
        self.in1 = torch.nn.InstanceNorm2d(num_channels)
        self.relu1 = torch.nn.ReLU()
        self.conv2 = torch.nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)
        self.in2 = torch.nn.InstanceNorm2d(num_channels)
    
    def forward(self, x):
        # x is a tensor of shape (batch_size, num_channels, height / 4, width / 4)
        residual = x # save the input as residual
        x = self.relu1(self.in1(self.conv1(x))) # (batch_size, num_channels, height / 4, width / 4)
        x = self.in2(self.conv2(x)) # (batch_size, num_channels, height / 4, width / 4)
        x += residual # add the residual to the output
        return x

class SketchFeatureExtractionNetwork(torch.nn.Module):
    def __init__(self , num_classes):
        super(SketchFeatureExtractionNetwork , self).__init__()
        # Define the network architecture as a sequence of convolutional layers with leaky ReLU activation and instance normalization
        self.conv1 = torch.nn.Conv2d(1 + num_classes , num_classes , kernel_size=7 , stride=1 , padding=3) 
        self.in1 = torch.nn.InstanceNorm2d(num_classes) 
        self.lrelu1 = torch

```