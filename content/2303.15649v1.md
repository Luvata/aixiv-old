---
title: 2303.15649v1 StyleDiffusion  Prompt-Embedding Inversion for Text-Based Editing
date: 2023-03-16
---

# [StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing](http://arxiv.org/abs/2303.15649v1)

authors: Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, Jian Yang


## What, Why and How

[1]: https://arxiv.org/abs/2303.15649 "StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing"
[2]: https://arxiv.org/pdf/2303.15649v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2010.15649v1 "[2010.15649v1] Efficient computation of two-loop amplitudes for Higgs ..."

Here is a summary of the paper:

- **What**: The paper proposes a method for text-based editing of images using pretrained diffusion models and attention regularization.
- **Why**: The paper aims to address two problems of existing methods: unsatisfying results for selected regions and unexpected changes in nonselected regions, and the need for careful text prompt editing that includes all visual objects in the input image.
- **How**: The paper introduces two improvements: only optimizing the input of the value linear network in the cross-attention layers, which is sufficient to reconstruct a real image, and applying attention regularization to preserve the object-like attention maps after editing, which enables accurate style editing without significant structural changes. The paper also improves the editing technique for both unconditional and conditional branches of classifier-free guidance. The paper evaluates the method on a variety of images and shows its superior editing capabilities than existing and concurrent works.

## Main Contributions

The paper claims the following contributions:

- A novel prompt-embedding inversion method for text-based editing of images using pretrained diffusion models, which only optimizes the input of the value linear network in the cross-attention layers.
- A novel attention regularization technique for preserving the object-like attention maps after editing, which enables accurate style editing without invoking significant structural changes.
- An improved editing technique for both unconditional and conditional branches of classifier-free guidance, which can handle complex text prompts and diverse image styles.
- Extensive experimental results on a variety of images, demonstrating the superior editing capabilities of the proposed method than existing and concurrent works.

## Method Summary

[1]: https://arxiv.org/abs/2303.15649 "StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing"
[2]: https://arxiv.org/pdf/2303.15649v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/pdf/2305.15649 "arXiv:2305.15649v1 [quant-ph] 25 May 2023"

Here is a summary of the method section of the paper:

- The paper introduces a novel prompt-embedding inversion method for text-based editing of images using pretrained diffusion models, which consists of three steps: (1) embedding the text prompt into a latent vector using a pretrained CLIP model, (2) optimizing the input of the value linear network in the cross-attention layers of a pretrained diffusion model to reconstruct the input image and match the text prompt, and (3) applying attention regularization to preserve the object-like attention maps after editing.
- The paper also introduces a novel attention regularization technique, which penalizes the difference between the attention maps of the original image and the edited image, and encourages the attention maps to be sparse and localized. The paper argues that this technique can help to obtain accurate style editing without invoking significant structural changes, and can also handle complex text prompts and diverse image styles.
- The paper further improves the editing technique for both unconditional and conditional branches of classifier-free guidance, which are used to guide the diffusion process towards the desired output. The paper proposes to use a weighted combination of different loss functions, such as L2 loss, perceptual loss, style loss, and CLIP loss, to balance between reconstruction quality and style transfer. The paper also proposes to use a patch-based discriminator to enhance the realism and diversity of the edited images.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load a pretrained diffusion model and a pretrained CLIP model
diffusion_model = load_diffusion_model()
clip_model = load_clip_model()

# Define the text prompt and the input image
text_prompt = "a blue sky with clouds"
input_image = load_image("input.jpg")

# Embed the text prompt into a latent vector using CLIP
text_vector = clip_model.encode_text(text_prompt)

# Optimize the input of the value linear network in the cross-attention layers to reconstruct the input image and match the text prompt
value_input = initialize_value_input()
optimizer = Adam(value_input)
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the image vector using CLIP
  image_vector = clip_model.encode_image(noisy_image)
  # Compute the loss function as the cosine distance between the text vector and the image vector
  loss = cosine_distance(text_vector, image_vector)
  # Update the value input using gradient descent
  optimizer.step(loss)

# Apply attention regularization to preserve the object-like attention maps after editing
attention_maps = diffusion_model.get_attention_maps(value_input)
original_maps = diffusion_model.get_attention_maps(input_image)
regularizer = L1Loss()
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the attention maps using diffusion model
  attention_maps = diffusion_model.get_attention_maps(value_input)
  # Compute the regularization loss as the L1 distance between the original attention maps and the edited attention maps
  reg_loss = regularizer(original_maps, attention_maps)
  # Update the value input using gradient descent
  optimizer.step(reg_loss)

# Use classifier-free guidance to guide the diffusion process towards the desired output
loss_weights = {"L2": 0.1, "perceptual": 0.5, "style": 0.2, "CLIP": 0.2}
discriminator = PatchDiscriminator()
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the L2 loss between the noisy image and the input image
  l2_loss = L2Loss(noisy_image, input_image)
  # Compute the perceptual loss between the noisy image and the input image using a pretrained VGG network
  perceptual_loss = PerceptualLoss(noisy_image, input_image)
  # Compute the style loss between the noisy image and the input image using a pretrained VGG network
  style_loss = StyleLoss(noisy_image, input_image)
  # Compute the CLIP loss between the noisy image and the text prompt using CLIP
  clip_loss = CLIPLoss(noisy_image, text_prompt)
  # Compute the total loss as a weighted combination of different losses
  total_loss = loss_weights["L2"] * l2_loss + loss_weights["perceptual"] * perceptual_loss + loss_weights["style"] * style_loss + loss_weights["CLIP"] * clip_loss
  # Update the value input using gradient descent
  optimizer.step(total_loss)

# Use a patch-based discriminator to enhance the realism and diversity of the edited images
discriminator_optimizer = Adam(discriminator.parameters())
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the discriminator loss as the hinge loss between real and fake patches
  disc_loss = HingeLoss(discriminator(input_image), discriminator(noisy_image))
  # Update the discriminator parameters using gradient descent
  discriminator_optimizer.step(disc_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import diffusion

# Load a pretrained diffusion model and a pretrained CLIP model
diffusion_model = diffusion.load_model("diffusion.pt")
clip_model = clip.load_model("clip.pt")

# Define the text prompt and the input image
text_prompt = "a blue sky with clouds"
input_image = torchvision.io.read_image("input.jpg")

# Embed the text prompt into a latent vector using CLIP
text_vector = clip_model.encode_text(text_prompt)

# Define the optimizer and the learning rate for the value input
optimizer = torch.optim.Adam(lr=0.01)
learning_rate = 0.01

# Initialize the value input as a random tensor with the same shape as the input image
value_input = torch.randn_like(input_image)

# Define the maximum number of iterations for the optimization loop
max_iterations = 1000

# Define a function to compute the cosine distance between two vectors
def cosine_distance(x, y):
  # Normalize the vectors to have unit norm
  x_norm = x / torch.norm(x)
  y_norm = y / torch.norm(y)
  # Compute the dot product between the normalized vectors
  dot_product = torch.dot(x_norm, y_norm)
  # Return the negative of the dot product as the cosine distance
  return -dot_product

# Optimize the input of the value linear network in the cross-attention layers to reconstruct the input image and match the text prompt
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the image vector using CLIP
  image_vector = clip_model.encode_image(noisy_image)
  # Compute the loss function as the cosine distance between the text vector and the image vector
  loss = cosine_distance(text_vector, image_vector)
  # Print the loss value and save the noisy image for debugging purposes
  print(f"Iteration {iteration}: Loss = {loss}")
  torchvision.io.write_image(noisy_image, f"noisy_{iteration}.jpg")
  # Zero out the gradients of the optimizer
  optimizer.zero_grad()
  # Compute the gradients of the loss with respect to the value input
  loss.backward()
  # Update the value input using gradient descent
  optimizer.step()
  # Clip the value input to be between -1 and 1
  value_input = torch.clamp(value_input, -1, 1)

# Define a function to compute the L1 distance between two tensors
def L1Loss(x, y):
  # Compute the absolute difference between x and y
  diff = torch.abs(x - y)
  # Sum up all the elements of diff and return it as the L1 distance
  return torch.sum(diff)

# Apply attention regularization to preserve the object-like attention maps after editing
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the attention maps using diffusion model for both original and edited images
  attention_maps = diffusion_model.get_attention_maps(value_input)
  original_maps = diffusion_model.get_attention_maps(input_image)
  # Compute the regularization loss as the L1 distance between the original attention maps and the edited attention maps
  reg_loss = L1Loss(original_maps, attention_maps)
  # Print the regularization loss value and save the attention maps for debugging purposes
  print(f"Iteration {iteration}: Reg Loss = {reg_loss}")
  torchvision.io.write_image(attention_maps, f"attention_{iteration}.jpg")
  # Zero out the gradients of the optimizer
  optimizer.zero_grad()
  # Compute the gradients of the regularization loss with respect to the value input
  reg_loss.backward()
  # Update the value input using gradient descent
  optimizer.step()
  # Clip the value input to be between -1 and 1
  value_input = torch.clamp(value_input, -1, 1)

# Define a function to compute the L2 distance between two tensors
def L2Loss(x, y):
  # Compute the squared difference between x and y
  diff = (x - y) ** 2
  # Sum up all the elements of diff and return it as half of L2 distance (for simplicity)
  return torch.sum(diff) / 2

# Define a function to compute perceptual loss using a pretrained VGG network (https://pytorch.org/hub/pytorch_vision_vgg/)
def PerceptualLoss(x, y):
    vgg_model = torchvision.models.vgg16(pretrained=True)
    # Extract the features of x and y using the VGG network
    x_features = vgg_model.features(x)
    y_features = vgg_model.features(y)
    # Compute the L2 distance between the features of x and y
    return L2Loss(x_features, y_features)

# Define a function to compute style loss using a pretrained VGG network (https://pytorch.org/hub/pytorch_vision_vgg/)
def StyleLoss(x, y):
    vgg_model = torchvision.models.vgg16(pretrained=True)
    # Extract the features of x and y using the VGG network
    x_features = vgg_model.features(x)
    y_features = vgg_model.features(y)
    # Compute the Gram matrices of the features of x and y
    x_gram = torch.matmul(x_features, x_features.transpose(1, 2))
    y_gram = torch.matmul(y_features, y_features.transpose(1, 2))
    # Compute the L2 distance between the Gram matrices of x and y
    return L2Loss(x_gram, y_gram)

# Define a function to compute CLIP loss using CLIP
def CLIPLoss(x, y):
  # Embed the image x and the text y into latent vectors using CLIP
  x_vector = clip_model.encode_image(x)
  y_vector = clip_model.encode_text(y)
  # Compute the cosine distance between the image vector and the text vector
  return cosine_distance(x_vector, y_vector)

# Define the loss weights for different loss functions
loss_weights = {"L2": 0.1, "perceptual": 0.5, "style": 0.2, "CLIP": 0.2}

# Use classifier-free guidance to guide the diffusion process towards the desired output
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the L2 loss between the noisy image and the input image
  l2_loss = L2Loss(noisy_image, input_image)
  # Compute the perceptual loss between the noisy image and the input image using a pretrained VGG network
  perceptual_loss = PerceptualLoss(noisy_image, input_image)
  # Compute the style loss between the noisy image and the input image using a pretrained VGG network
  style_loss = StyleLoss(noisy_image, input_image)
  # Compute the CLIP loss between the noisy image and the text prompt using CLIP
  clip_loss = CLIPLoss(noisy_image, text_prompt)
  # Compute the total loss as a weighted combination of different losses
  total_loss = loss_weights["L2"] * l2_loss + loss_weights["perceptual"] * perceptual_loss + loss_weights["style"] * style_loss + loss_weights["CLIP"] * clip_loss
  # Print the total loss value and save the noisy image for debugging purposes
  print(f"Iteration {iteration}: Total Loss = {total_loss}")
  torchvision.io.write_image(noisy_image, f"total_{iteration}.jpg")
  # Zero out the gradients of the optimizer
  optimizer.zero_grad()
  # Compute the gradients of the total loss with respect to the value input
  total_loss.backward()
  # Update the value input using gradient descent
  optimizer.step()
  # Clip the value input to be between -1 and 1
  value_input = torch.clamp(value_input, -1, 1)

# Define a patch-based discriminator model (https://arxiv.org/abs/1611.07004)
discriminator = PatchDiscriminator()

# Define an optimizer and a learning rate for the discriminator parameters
discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)
learning_rate = 0.0002

# Define a function to compute hinge loss for discriminator training (https://arxiv.org/abs/1705.02894)
def HingeLoss(real_output, fake_output):
    # Compute hinge loss for real output as max(0, 1 - real_output)
    real_loss = torch.max(torch.zeros_like(real_output), torch.ones_like(real_output) - real_output)
    # Compute hinge loss for fake output as max(0, 1 + fake_output)
    fake_loss = torch.max(torch.zeros_like(fake_output), torch.ones_like(fake_output) + fake_output)
    # Sum up real loss and fake loss and return it as hinge loss
    return torch.sum(real_loss + fake_loss)

# Use a patch-based discriminator to enhance the realism and diversity of the edited images
for iteration in range(max_iterations):
  # Generate a noisy image from the value input using diffusion model
  noisy_image = diffusion_model.generate(value_input)
  # Compute the discriminator output for both real and fake images
  real_output = discriminator(input_image)
  fake_output =