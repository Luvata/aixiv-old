---
title: 2201.12086v2 BLIP  Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
date: 2022-01-13
---

# [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](http://arxiv.org/abs/2201.12086v2)

authors: Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi


## What, Why and How

[1]: https://arxiv.org/pdf/2201.12086v2.pdf "Abstract arXiv:2201.12086v2 [cs.CV] 15 Feb 2022"
[2]: https://arxiv.org/abs/2201.12086 "[2201.12086] BLIP: Bootstrapping Language-Image Pre-training for ..."
[3]: https://arxiv.org/pdf/2201.12086v1.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper proposes BLIP, a new framework for vision-language pre-training (VLP) that can transfer to both understanding and generation tasks.
- **Why**: The paper aims to overcome the limitations of existing VLP methods, which either use suboptimal noisy web data or adopt models that are not flexible for different tasks.
- **How**: The paper introduces two innovations: (1) Multimodal mixture of Encoder-Decoder (MED), a new model architecture that can operate as an encoder, an encoder-decoder, or a decoder, depending on the task. (2) Captioning and Filtering (CapFilt), a new dataset bootstrapping method that uses a captioner to generate synthetic captions for web images and a filter to remove noisy captions from both the original and synthetic texts.

The paper claims to achieve state-of-the-art results on various vision-language tasks, such as image-text retrieval, image captioning, and visual question answering. It also shows strong generalization ability when transferred to video-language tasks in a zero-shot manner.

Source: [^1^][1]

## Main Contributions

[1]: https://arxiv.org/pdf/2201.12086v2.pdf "Abstract arXiv:2201.12086v2 [cs.CV] 15 Feb 2022"
[2]: https://arxiv.org/abs/2201.12086 "[2201.12086] BLIP: Bootstrapping Language-Image Pre-training for ..."
[3]: https://arxiv.org/pdf/2201.12086v1.pdf "arXiv.org e-Print archive"

The paper has the following contributions:

- It proposes a new model architecture called Multimodal mixture of Encoder-Decoder (MED) that can operate as an encoder, an encoder-decoder, or a decoder, depending on the task. This enables effective multi-task pre-training and flexible transfer learning for vision-language tasks.
- It proposes a new dataset bootstrapping method called Captioning and Filtering (CapFilt) that uses a captioner to generate synthetic captions for web images and a filter to remove noisy captions from both the original and synthetic texts. This improves the quality of the pre-training data and reduces the noise from the web sources.
- It achieves state-of-the-art results on various vision-language tasks, such as image-text retrieval, image captioning, and visual question answering. It also shows strong generalization ability when transferred to video-language tasks in a zero-shot manner.

Source: [^1^][1]

## Method Summary

[1]: https://arxiv.org/pdf/2201.12086v2.pdf "Abstract arXiv:2201.12086v2 [cs.CV] 15 Feb 2022"
[2]: https://arxiv.org/abs/2201.12086 "[2201.12086] BLIP: Bootstrapping Language-Image Pre-training for ..."
[3]: https://arxiv.org/pdf/2201.12086v1.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper presents the Multimodal mixture of Encoder-Decoder (MED) model, which consists of three components: a unimodal encoder, an image-grounded text encoder, and an image-grounded text decoder. The unimodal encoder encodes either an image or a text into a shared latent space. The image-grounded text encoder encodes both an image and a text into a joint representation. The image-grounded text decoder generates a text conditioned on an image.
- The paper describes the pre-training objectives for the MED model, which are: (1) image-text contrastive learning, which maximizes the cosine similarity between the unimodal encodings of matched image-text pairs and minimizes it for unmatched pairs. (2) image-text matching, which predicts whether an image and a text are matched or not using the image-grounded text encoder. (3) image-conditioned language modeling, which generates captions for images using the image-grounded text decoder.
- The paper introduces the Captioning and Filtering (CapFilt) method, which is a dataset bootstrapping technique that leverages noisy web data for pre-training. The method consists of two steps: (1) captioning, which uses a pre-trained MED model as a captioner to generate synthetic captions for web images. (2) filtering, which uses another pre-trained MED model as a filter to remove noisy captions from both the original web texts and the synthetic texts. The filter assigns a score to each caption based on its relevance to the image and its fluency, and keeps only the captions with high scores.

Source: [^1^][1]

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the MED model
med_model = MultimodalMixtureOfEncoderDecoder()

# Define the pre-training objectives
contrastive_loss = ImageTextContrastiveLoss()
matching_loss = ImageTextMatchingLoss()
language_modeling_loss = ImageConditionedLanguageModelingLoss()

# Pre-train the MED model on a large-scale vision-language dataset
for batch in vision_language_dataset:
  image, text = batch
  # Compute the contrastive loss
  image_embedding = med_model.unimodal_encoder(image)
  text_embedding = med_model.unimodal_encoder(text)
  loss1 = contrastive_loss(image_embedding, text_embedding)
  # Compute the matching loss
  joint_embedding = med_model.image_text_encoder(image, text)
  loss2 = matching_loss(joint_embedding)
  # Compute the language modeling loss
  generated_text = med_model.image_text_decoder(image)
  loss3 = language_modeling_loss(generated_text, text)
  # Update the model parameters
  loss = loss1 + loss2 + loss3
  med_model.backward(loss)
  med_model.update()

# Fine-tune the MED model as a captioner on a clean image-caption dataset
for batch in image_caption_dataset:
  image, caption = batch
  # Generate synthetic captions for images
  synthetic_caption = med_model.image_text_decoder(image)
  # Compute the language modeling loss
  loss = language_modeling_loss(synthetic_caption, caption)
  # Update the model parameters
  med_model.backward(loss)
  med_model.update()

# Fine-tune another MED model as a filter on a noisy web image-text dataset
for batch in web_image_text_dataset:
  image, text = batch
  # Encode the image and text into a joint representation
  joint_embedding = med_model.image_text_encoder(image, text)
  # Predict a score for the image-text pair based on its relevance and fluency
  score = med_model.score(joint_embedding)
  # Compute the binary cross-entropy loss with a threshold
  label = score > threshold
  loss = binary_cross_entropy_loss(score, label)
  # Update the model parameters
  med_model.backward(loss)
  med_model.update()

# Bootstrap the pre-training dataset using CapFilt method
new_dataset = []
for image in web_image_dataset:
  # Generate synthetic captions using the captioner model
  synthetic_captions = captioner_model.image_text_decoder(image, num_captions)
  # Filter out noisy captions using the filter model
  filtered_captions = []
  for caption in synthetic_captions:
    score = filter_model.score(filter_model.image_text_encoder(image, caption))
    if score > threshold:
      filtered_captions.append(caption)
  # Add the filtered captions to the new dataset
  new_dataset.extend([(image, caption) for caption in filtered_captions])

# Pre-train the MED model again on the new dataset using the same objectives as before

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
batch_size = 256
learning_rate = 1e-4
num_epochs = 10
num_captions = 5
threshold = 0.5

# Define the MED model
class MultimodalMixtureOfEncoderDecoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a ResNet-50 model as the unimodal image encoder
    self.image_encoder = torchvision.models.resnet50(pretrained=True)
    # Use a BERT model as the unimodal text encoder
    self.text_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
    # Use a linear layer to project the image and text embeddings to a shared latent space of dimension 768
    self.projection = torch.nn.Linear(2048, 768)
    # Use a BERT model as the image-grounded text encoder
    self.image_text_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
    # Use a BERT model as the image-grounded text decoder
    self.image_text_decoder = transformers.BertForConditionalGeneration.from_pretrained('bert-base-uncased')
    # Use a linear layer to predict a score for an image-text pair
    self.score = torch.nn.Linear(768, 1)

  def forward(self, image, text=None):
    # Encode the image using the image encoder and the projection layer
    image_embedding = self.projection(self.image_encoder(image))
    if text is None:
      # If no text is given, return the image embedding
      return image_embedding
    else:
      # Encode the text using the text encoder and the projection layer
      text_embedding = self.projection(self.text_encoder(text).last_hidden_state)
      # Concatenate the [CLS] token of the text embedding with the image embedding
      joint_embedding = torch.cat([text_embedding[:,0,:], image_embedding], dim=1)
      # Encode the joint embedding using the image-text encoder
      joint_embedding = self.image_text_encoder(joint_embedding).last_hidden_state[:,0,:]
      if self.training:
        # If in training mode, generate captions for the image using the image-text decoder and teacher forcing
        generated_text = self.image_text_decoder(input_ids=text.input_ids, decoder_input_ids=text.input_ids, encoder_outputs=(joint_embedding,))
        return joint_embedding, generated_text.logits
      else:
        # If in inference mode, generate captions for the image using the image-text decoder and beam search
        generated_text = self.image_text_decoder.generate(input_ids=text.input_ids, encoder_outputs=(joint_embedding,), num_beams=num_captions)
        return joint_embedding, generated_text

# Define the pre-training objectives
contrastive_loss = torch.nn.CosineEmbeddingLoss()
matching_loss = torch.nn.BCEWithLogitsLoss()
language_modeling_loss = torch.nn.CrossEntropyLoss()

# Load the pre-training datasets
vision_language_dataset = load_vision_language_dataset()
image_caption_dataset = load_image_caption_dataset()
web_image_text_dataset = load_web_image_text_dataset()
web_image_dataset = load_web_image_dataset()

# Initialize the MED model and the optimizer
med_model = MultimodalMixtureOfEncoderDecoder()
optimizer = torch.optim.Adam(med_model.parameters(), lr=learning_rate)

# Pre-train the MED model on a large-scale vision-language dataset
for epoch in range(num_epochs):
  for batch in vision_language_dataset:
    image, text = batch
    # Compute the contrastive loss
    image_embedding = med_model(image)
    text_embedding = med_model(None, text)
    loss1 = contrastive_loss(image_embedding, text_embedding, torch.ones(batch_size))
    # Compute the matching loss
    joint_embedding, _ = med_model(image, text)
    loss2 = matching_loss(med_model.score(joint_embedding), torch.ones(batch_size))
    # Compute the language modeling loss
    _, generated_text_logits = med_model(image, text)
    loss3 = language_modeling_loss(generated_text_logits.view(-1, generated_text_logits.size(-1)), text.view(-1))
    # Update the model parameters
    loss = loss1 + loss2 + loss3
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Fine-tune the MED model as a captioner on a clean image-caption dataset
for epoch in range(num_epochs):
  for batch in image_caption_dataset:
    image, caption = batch
    # Generate synthetic captions for images and compute the language modeling loss
    _, generated_caption_logits = med_model(image, caption)
    loss = language_modeling_loss(generated_caption_logits.view(-1, generated_caption_logits.size(-1)), caption.view(-1))
    # Update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Save the captioner model
torch.save(med_model, 'captioner_model.pth')

# Fine-tune another MED model as a filter on a noisy web image-text dataset
med_model = MultimodalMixtureOfEncoderDecoder()
optimizer = torch.optim.Adam(med_model.parameters(), lr=learning_rate)
for epoch in range(num_epochs):
  for batch in web_image_text_dataset:
    image, text = batch
    # Encode the image and text into a joint representation and predict a score for the image-text pair
    joint_embedding, _ = med_model(image, text)
    score = med_model.score(joint_embedding)
    # Compute the binary cross-entropy loss with a threshold
    label = score > threshold
    loss = binary_cross_entropy_loss(score, label)
    # Update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Save the filter model
torch.save(med_model, 'filter_model.pth')

# Load the captioner and filter models
captioner_model = torch.load('captioner_model.pth')
filter_model = torch.load('filter_model.pth')

# Bootstrap the pre-training dataset using CapFilt method
new_dataset = []
for image in web_image_dataset:
  # Generate synthetic captions using the captioner model
  _, synthetic_captions = captioner_model(image)
  # Filter out noisy captions using the filter model
  filtered_captions = []
  for caption in synthetic_captions:
    score = filter_model.score(filter_model.image_text_encoder(image, caption))
    if score > threshold:
      filtered_captions.append(caption)
  # Add the filtered captions to the new dataset
  new_dataset.extend([(image, caption) for caption in filtered_captions])

# Pre-train the MED model again on the new dataset using the same objectives as before

```