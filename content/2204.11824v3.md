---
title: 2204.11824v3 Semi-Parametric Neural Image Synthesis
date: 2022-04-12
---

# [Semi-Parametric Neural Image Synthesis](http://arxiv.org/abs/2204.11824v3)

authors: Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, Björn Ommer


## What, Why and How

[1]: https://arxiv.org/abs/2204.11824 "[2204.11824] Semi-Parametric Neural Image Synthesis - arXiv.org"
[2]: https://arxiv.org/abs/2204.11824v1 "[2204.11824v1] Retrieval-Augmented Diffusion Models - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.11824v1 "[2204.11824v1] Retrieval-Augmented Diffusion Models"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a semi-parametric approach for generative image synthesis that combines a small diffusion or autoregressive model with an external image database and a retrieval strategy.
- **Why**: The paper aims to question the paradigm of compressing large training data into ever growing parametric representations and to demonstrate the benefits of using external memory and informative samples for image synthesis.
- **How**: The paper trains the generative model with nearest neighbors retrieved from the external database for each training instance using CLIP's joint image-text embedding space. The paper evaluates the model on various tasks such as class-conditional synthesis, zero-shot stylization, text-to-image synthesis and unconditional generation. The paper shows that the model can outperform the state-of-the-art with significantly fewer parameters and can be easily transferred to novel domains by swapping the database.

## Main Contributions

According to the paper, the main contributions are:

- Introducing a semi-parametric approach for generative image synthesis that leverages an external image database and a retrieval strategy to condition the generative model on informative samples.
- Showing that the proposed approach can significantly reduce the parameter count of the generative model and still achieve competitive or superior performance on various tasks such as class-conditional synthesis, zero-shot stylization, text-to-image synthesis and unconditional generation.
- Demonstrating that the proposed approach can easily transfer a trained model to novel domains by simply swapping the database without retraining or fine-tuning.

## Method Summary

Here is a summary of the method section of the paper:

- The paper uses a diffusion or autoregressive model as the base generative model that can synthesize images from a latent code.
- The paper introduces an external image database that contains a large and diverse collection of images from various domains and sources.
- The paper uses CLIP as a retrieval model that can embed both images and text into a common semantic space and measure their similarity.
- The paper defines a retrieval function that takes an image or text query and returns a set of nearest neighbors from the external database based on CLIP's similarity scores.
- The paper trains the generative model with a semi-parametric objective that consists of two terms: a parametric term that measures the reconstruction loss between the generated image and the original image, and a non-parametric term that measures the retrieval loss between the generated image and the retrieved images.
- The paper conditions the generative model on both the latent code and the retrieved images during training and inference. The paper uses different conditioning strategies for different tasks such as class-conditional synthesis, zero-shot stylization, text-to-image synthesis and unconditional generation.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load the base generative model (diffusion or autoregressive)
model = load_model("base_model")

# Load the external image database
database = load_database("external_database")

# Load the retrieval model (CLIP)
retrieval = load_model("CLIP")

# Define the retrieval function
def retrieve(query, k):
  # Embed the query into the CLIP space
  query_embedding = retrieval.embed(query)
  # Embed the database images into the CLIP space
  database_embeddings = retrieval.embed(database)
  # Compute the similarity scores between the query and the database images
  scores = query_embedding @ database_embeddings.T
  # Sort the scores and get the top-k indices
  indices = argsort(scores)[-k:]
  # Return the top-k nearest neighbors from the database
  return database[indices]

# Define the semi-parametric objective function
def objective(x, x_hat, z):
  # Compute the parametric term (reconstruction loss)
  parametric_loss = mse(x, x_hat)
  # Retrieve k nearest neighbors for x using CLIP
  x_neighbors = retrieve(x, k)
  # Retrieve k nearest neighbors for x_hat using CLIP
  x_hat_neighbors = retrieve(x_hat, k)
  # Compute the non-parametric term (retrieval loss)
  non_parametric_loss = mse(x_neighbors, x_hat_neighbors)
  # Return the weighted sum of the two terms
  return alpha * parametric_loss + beta * non_parametric_loss

# Train the generative model with the semi-parametric objective
for epoch in epochs:
  for batch in batches:
    # Sample a batch of images and latent codes
    x, z = sample_batch(batch_size)
    # Generate images from latent codes and retrieved images
    x_hat = model.generate(z, retrieve(z, k))
    # Compute the semi-parametric objective
    loss = objective(x, x_hat, z)
    # Update the model parameters using gradient descent
    model.update(loss)

# Inference with the generative model
def inference(query):
  # Embed the query into the latent space using CLIP
  z = retrieval.embed(query)
  # Generate an image from the latent code and retrieved images
  x_hat = model.generate(z, retrieve(z, k))
  # Return the generated image
  return x_hat

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Load the base generative model (diffusion or autoregressive)
# For example, using a pretrained diffusion model from https://github.com/openai/guided-diffusion
model = torch.hub.load('openai/guided-diffusion', 'cifar10')

# Load the external image database
# For example, using a subset of ImageNet
database = torchvision.datasets.ImageNet(root="data", split="train")

# Load the retrieval model (CLIP)
# For example, using a pretrained CLIP model from https://github.com/openai/CLIP
retrieval = clip.load("ViT-B/32", device="cuda")

# Define the retrieval function
def retrieve(query, k):
  # Embed the query into the CLIP space
  query_embedding = retrieval.encode_image(query)
  # Embed the database images into the CLIP space
  database_embeddings = retrieval.encode_image(database)
  # Compute the similarity scores between the query and the database images
  scores = query_embedding @ database_embeddings.T
  # Sort the scores and get the top-k indices
  indices = torch.argsort(scores, dim=-1, descending=True)[:,:k]
  # Return the top-k nearest neighbors from the database
  return database[indices]

# Define the semi-parametric objective function
def objective(x, x_hat, z):
  # Compute the parametric term (reconstruction loss)
  parametric_loss = torch.nn.functional.mse_loss(x, x_hat)
  # Retrieve k nearest neighbors for x using CLIP
  x_neighbors = retrieve(x, k)
  # Retrieve k nearest neighbors for x_hat using CLIP
  x_hat_neighbors = retrieve(x_hat, k)
  # Compute the non-parametric term (retrieval loss)
  non_parametric_loss = torch.nn.functional.mse_loss(x_neighbors, x_hat_neighbors)
  # Return the weighted sum of the two terms
  return alpha * parametric_loss + beta * non_parametric_loss

# Define the optimizer for the generative model
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the generative model with the semi-parametric objective
for epoch in range(epochs):
  for batch in dataloader:
    # Sample a batch of images and latent codes
    x, z = batch["image"], batch["latent"]
    # Generate images from latent codes and retrieved images
    x_hat = model.sample(z, cond_fn=lambda t: retrieve(z, k))
    # Compute the semi-parametric objective
    loss = objective(x, x_hat, z)
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Inference with the generative model
def inference(query):
  # Embed the query into the latent space using CLIP
  z = retrieval.encode_image(query)
  # Generate an image from the latent code and retrieved images
  x_hat = model.sample(z, cond_fn=lambda t: retrieve(z, k))
  # Return the generated image
  return x_hat

```