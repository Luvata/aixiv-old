---
title: 2306.12624v1 DreamEdit  Subject-driven Image Editing
date: 2023-06-13
---

# [DreamEdit: Subject-driven Image Editing](http://arxiv.org/abs/2306.12624v1)

authors: Tianle Li, Max Ku, Cong Wei, Wenhu Chen


## What, Why and How

[1]: https://arxiv.org/pdf/2306.12624v1.pdf "arXiv:2306.12624v1 [cs.CV] 22 Jun 2023"
[2]: https://arxiv.org/pdf/2306.02424v1.pdf "arXiv:2306.02424v1 [cs.CV] 4 Jun 2023"
[3]: https://arxiv-export1.library.cornell.edu/abs/2306.12624 "[2306.12624] DreamEdit: Subject-driven Image Editing"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes two novel subject-driven image editing sub-tasks, namely Subject Replacement and Subject Addition, which aim to generate realistic images containing customized subjects with precise control over the background and position.
- **Why**: The paper argues that the existing subject-driven image generation methods cannot achieve fine-grained control over the target subject and its context, and thus introduces new challenges and opportunities for subject-driven image editing.
- **How**: The paper presents a new dataset called DreamEditBench, which contains 22 different types of subjects and 440 source images with diverse scenarios and difficulty levels. The paper also devises a new method called DreamEditor, which performs iterative generation to adapt to the customized subject and blend it smoothly with the background. The paper evaluates DreamEditor and several baselines on DreamEditBench using automatic and human evaluations, and shows that the new tasks are challenging for the existing models.


## Main Contributions

[1]: https://arxiv.org/pdf/2306.12624v1.pdf "arXiv:2306.12624v1 [cs.CV] 22 Jun 2023"
[2]: https://arxiv.org/pdf/2306.02424v1.pdf "arXiv:2306.02424v1 [cs.CV] 4 Jun 2023"
[3]: https://arxiv-export1.library.cornell.edu/abs/2306.12624 "[2306.12624] DreamEdit: Subject-driven Image Editing"

According to the paper at [^1^][1], the main contributions are:

- **Proposing two novel subject-driven image editing sub-tasks**: Subject Replacement and Subject Addition, which require generating realistic images containing customized subjects with precise control over the background and position.
- **Curating a new dataset called DreamEditBench**: which contains 22 different types of subjects and 440 source images with diverse scenarios and difficulty levels, and planning to host it and hire trained evaluators for standardized human evaluation.
- **Devising a new method called DreamEditor**: which performs iterative generation to adapt to the customized subject and blend it smoothly with the background, and evaluating it and several baselines on DreamEditBench using automatic and human evaluations.


## Method Summary

[1]: https://arxiv.org/pdf/2306.12624v1.pdf "arXiv:2306.12624v1 [cs.CV] 22 Jun 2023"
[2]: https://arxiv.org/pdf/2306.02424v1.pdf "arXiv:2306.02424v1 [cs.CV] 4 Jun 2023"
[3]: https://arxiv-export1.library.cornell.edu/abs/2306.12624 "[2306.12624] DreamEdit: Subject-driven Image Editing"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces two novel subject-driven image editing sub-tasks: Subject Replacement and Subject Addition. Subject Replacement aims to replace the original subject in a source image with a customized subject, while preserving the background and the context. Subject Addition aims to add a customized subject to a designated position in a source image, while generating a reasonable posture and blending it with the background.
- The paper presents a new dataset called DreamEditBench, which contains 22 different types of subjects and 440 source images with diverse scenarios and difficulty levels. The subjects include animals, plants, vehicles, furniture, etc. The source images are collected from various online sources and manually annotated with bounding boxes and masks for the original subjects. The dataset also provides some examples of Subject Replacement and Subject Addition for each source image.
- The paper proposes a new method called DreamEditor, which performs iterative generation to adapt to the customized subject and blend it smoothly with the background. DreamEditor consists of three modules: a subject encoder, a scene encoder, and an image generator. The subject encoder encodes the customized subject into a latent vector. The scene encoder encodes the source image into a feature map. The image generator takes the latent vector and the feature map as inputs and generates an intermediate image containing the customized subject. The intermediate image is then fed back to the scene encoder to update the feature map, and the process is repeated until convergence. The final output is the last intermediate image generated by DreamEditor.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# Define the subject encoder, scene encoder, and image generator modules
subject_encoder = SubjectEncoder()
scene_encoder = SceneEncoder()
image_generator = ImageGenerator()

# Define the loss function and the optimizer
loss_function = L1Loss()
optimizer = Adam()

# Define the number of iterations
num_iterations = 10

# Define the input customized subject and the source image
customized_subject = InputSubject()
source_image = InputImage()

# Encode the customized subject into a latent vector
subject_vector = subject_encoder(customized_subject)

# Encode the source image into a feature map
feature_map = scene_encoder(source_image)

# Initialize the output image as None
output_image = None

# Iterate until convergence
for i in range(num_iterations):

  # Generate an intermediate image containing the customized subject
  intermediate_image = image_generator(subject_vector, feature_map)

  # Update the output image as the intermediate image
  output_image = intermediate_image

  # Compute the loss between the intermediate image and the source image
  loss = loss_function(intermediate_image, source_image)

  # Update the parameters of the modules using backpropagation
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  # Update the feature map using the intermediate image
  feature_map = scene_encoder(intermediate_image)

# Return the output image as the final result
return output_image
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Define the subject encoder module
class SubjectEncoder(nn.Module):

  # Initialize the module with a convolutional neural network
  def __init__(self):
    super(SubjectEncoder, self).__init__()
    self.cnn = nn.Sequential(
      nn.Conv2d(3, 64, 4, 2, 1),
      nn.LeakyReLU(0.2),
      nn.Conv2d(64, 128, 4, 2, 1),
      nn.BatchNorm2d(128),
      nn.LeakyReLU(0.2),
      nn.Conv2d(128, 256, 4, 2, 1),
      nn.BatchNorm2d(256),
      nn.LeakyReLU(0.2),
      nn.Conv2d(256, 512, 4, 2, 1),
      nn.BatchNorm2d(512),
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 1024, 4, 2, 1),
      nn.BatchNorm