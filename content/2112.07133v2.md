---
title: 2112.07133v2 CLIP-Lite  Information Efficient Visual Representation Learning with Language Supervision
date: 2021-12-08
---

# [CLIP-Lite: Information Efficient Visual Representation Learning with Language Supervision](http://arxiv.org/abs/2112.07133v2)

authors: Aman Shrivastava, Ramprasaath R. Selvaraju, Nikhil Naik, Vicente Ordonez


## What, Why and How

[1]: https://arxiv.org/pdf/2112.07133v2.pdf "arXiv:2112.07133v2 [cs.CV] 11 May 2023"
[2]: https://arxiv.org/abs/2112.07133 "[2112.07133] CLIP-Lite: Information Efficient Visual Representation ..."
[3]: http://export.arxiv.org/abs/2112.07133 "[2112.07133] CLIP-Lite: Information Efficient Visual Representation ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes CLIP-Lite, an information efficient method for visual representation learning by feature alignment with textual annotations.
- **Why**: The paper aims to overcome the limitations of previous methods such as CLIP, which require a large number of negative image-text pairs and a large-scale dataset for contrastive learning between vision and language modalities.
- **How**: The paper leverages an information efficient lower-bound to maximize the mutual information between the two modalities, which allows CLIP-Lite to learn from only one negative image-text pair for every positive pair. The paper evaluates CLIP-Lite on various downstream tasks such as image and text retrieval, zero-shot classification, and visual grounding, and shows that it outperforms or matches CLIP and other text-supervised models. The paper also demonstrates that CLIP-Lite can encourage bias-free visual representations by using language semantics.

## Main Contributions

[1]: https://arxiv.org/pdf/2112.07133v2.pdf "arXiv:2112.07133v2 [cs.CV] 11 May 2023"
[2]: https://arxiv.org/abs/2112.07133 "[2112.07133] CLIP-Lite: Information Efficient Visual Representation ..."
[3]: http://export.arxiv.org/abs/2112.07133 "[2112.07133] CLIP-Lite: Information Efficient Visual Representation ..."

The paper[^1^][1] lists the following contributions:

- **A novel information efficient method for visual representation learning by feature alignment with textual annotations, called CLIP-Lite, which requires only one negative image-text pair for every positive pair.**
- **An empirical evaluation of CLIP-Lite on various downstream tasks such as image and text retrieval, zero-shot classification, and visual grounding, showing that it outperforms or matches CLIP and other text-supervised models.**
- **A demonstration of how CLIP-Lite can leverage language semantics to encourage bias-free visual representations that can be used in downstream tasks.**

## Method Summary

[1]: https://arxiv.org/pdf/2112.07133v2.pdf "arXiv:2112.07133v2 [cs.CV] 11 May 2023"
[2]: https://arxiv.org/abs/2112.07133 "[2112.07133] CLIP-Lite: Information Efficient Visual Representation ..."
[3]: http://export.arxiv.org/abs/2112.07133 "[2112.07133] CLIP-Lite: Information Efficient Visual Representation ..."

Here is a summary of the method section of the paper[^1^][1]:

- **The paper introduces CLIP-Lite, a method that learns visual representations by aligning image and text features in a common latent space using contrastive learning.**
- **The paper adopts the InfoNCE loss (Oord et al., 2018) as the contrastive learning objective, which maximizes the mutual information between image and text features.**
- **The paper proposes an information efficient lower-bound to the InfoNCE loss, which reduces the number of negative image-text pairs required for contrastive learning from n(n-1) to n, where n is the batch size.**
- **The paper implements CLIP-Lite using a ResNet-50 (He et al., 2016) encoder for images and a Transformer (Vaswani et al., 2017) encoder for texts, and uses cosine similarity as the feature alignment metric.**
- **The paper pretrains CLIP-Lite on the COCO-Captions dataset (Lin et al., 2014), which contains 123k images with five captions each, and uses a batch size of 256.**

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define image and text encoders
image_encoder = ResNet50()
text_encoder = Transformer()

# Define contrastive learning objective
def contrastive_loss(image_features, text_features):
  # Compute cosine similarity between image and text features
  similarities = cosine_similarity(image_features, text_features)
  # Get the diagonal elements as the positive similarities
  positives = torch.diag(similarities)
  # Get the maximum of the off-diagonal elements as the negative similarities
  negatives = torch.max(similarities - torch.eye(similarities.size(0)), dim=1)[0]
  # Compute the InfoNCE lower-bound loss
  loss = -torch.mean(torch.log(positives / (positives + negatives)))
  return loss

# Pretrain CLIP-Lite on COCO-Captions dataset
for batch in COCO_Captions:
  # Get image-caption pairs from batch
  images, captions = batch
  # Encode images and captions into features
  image_features = image_encoder(images)
  text_features = text_encoder(captions)
  # Compute contrastive loss
  loss = contrastive_loss(image_features, text_features)
  # Update parameters using gradient descent
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import libraries
import torch
import torchvision
import transformers
import numpy as np

# Define image and text encoders
image_encoder = torchvision.models.resnet50(pretrained=True)
image_encoder.fc = torch.nn.Identity() # Remove the final classification layer
text_encoder = transformers.AutoModel.from_pretrained('bert-base-uncased') # Use BERT as the text encoder
text_encoder.pooler = torch.nn.Identity() # Remove the final pooling layer

# Define contrastive learning objective
def contrastive_loss(image_features, text_features):
  # Normalize image and text features to unit length
  image_features = image_features / torch.norm(image_features, dim=1, keepdim=True)
  text_features = text_features / torch.norm(text_features, dim=1, keepdim=True)
  # Compute cosine similarity between image and text features
  similarities = torch.matmul(image_features, text_features.t())
  # Get the diagonal elements as the positive similarities
  positives = torch.diag(similarities)
  # Get the maximum of the off-diagonal elements as the negative similarities
  negatives = torch.max(similarities - torch.eye(similarities.size(0)), dim=1)[0]
  # Compute the InfoNCE lower-bound loss
  loss = -torch.mean(torch.log(positives / (positives + negatives)))
  return loss

# Define optimizer and learning rate scheduler
optimizer = torch.optim.Adam(list(image_encoder.parameters()) + list(text_encoder.parameters()), lr=3e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100000)

# Pretrain CLIP-Lite on COCO-Captions dataset
for epoch in range(100):
  for batch in COCO_Captions:
    # Get image-caption pairs from batch
    images, captions = batch
    # Encode images and captions into features
    image_features = image_encoder(images) # Shape: (batch_size, 2048)
    text_features = text_encoder(captions)['last_hidden_state'][:,0,:] # Shape: (batch_size, 768)
    # Compute contrastive loss
    loss = contrastive_loss(image_features, text_features)
    # Update parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    scheduler.step()
    # Print loss every 100 steps
    if step % 100 == 0:
      print(f'Epoch {epoch}, Step {step}, Loss {loss.item()}')
```