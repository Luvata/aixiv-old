---
title: 1511.06434v2 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
date: 2015-11-07
---

# [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](http://arxiv.org/abs/1511.06434v2)

authors: Alec Radford, Luke Metz, Soumith Chintala


## What, Why and How

[1]: https://arxiv.org/abs/1511.06434 "[1511.06434] Unsupervised Representation Learning with Deep ... - arXiv.org"
[2]: https://arxiv.org/pdf/1511.06434v2.pdf "arXiv:1511.06434v2 [cs.LG] 7 Jan 2016"
[3]: https://github.com/kochlisGit/Generative-Adversarial-Networks "GitHub - kochlisGit/Generative-Adversarial-Networks: Generation of ..."
[4]: https://github.com/khushhallchandra/GAN "GitHub - khushhallchandra/GAN: This repo will contains code for ..."

Here is a summary of the paper:

```
What: The paper introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.

Why: The paper aims to bridge the gap between the success of CNNs for supervised learning and unsupervised learning, and to show that GANs can learn good image representations that can be reused for other tasks.

How: The paper trains DCGANs on various image datasets, and shows that they learn a hierarchy of representations from object parts to scenes in both the generator and discriminator. The paper also uses the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms. The paper also visualizes the filters learned by GANs and shows that they have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.
```


## Main Contributions

The contributions of this paper are:

- It proposes and evaluates a set of constraints on the architectural topology of Convolutional GANs that make them stable to train in most settings.
- It uses the trained discriminators for image classification tasks, showing competitive performance with other unsupervised algorithms.
- It visualizes the filters learnt by GANs and empirically shows that specific filters have learned to draw specific objects.
- It shows that the generators have interesting vector arithmetic properties allowing for easy manipulation of many semantic qualities of generated samples.

## Method Summary

[1]: https://arxiv.org/abs/1511.06434 "[1511.06434] Unsupervised Representation Learning with Deep ... - arXiv.org"
[2]: https://arxiv.org/pdf/1511.06434v2.pdf "arXiv:1511.06434v2 [cs.LG] 7 Jan 2016"
[3]: https://arxiv.org/format/1511.06434 "Format selector for 1511.06434 - arXiv.org"
[4]: https://bing.com/search?q=http%3a%2f%2farxiv.org%2fpdf%2f1511.06434v2+method+section "arXiv.org e-Print archive"

Here is a summary of the method section:

```
The method section describes the details of the DCGAN architecture and training procedure. The authors list four architectural constraints that they found to be important for stable training:

- Replacing any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).
- Using batch normalization in both the generator and the discriminator.
- Removing fully connected hidden layers for deeper architectures.
- Using ReLU activation in the generator for all layers except for the output, which uses Tanh. Using LeakyReLU activation in the discriminator for all layers.

The authors also provide some tips for choosing hyperparameters and optimizing the GAN objective function. They use mini-batch stochastic gradient descent with a mini-batch size of 128. They use Adam optimizer with a learning rate of 0.0002 and momentum term Î²1 of 0.5. They use a fixed noise vector to track the progress of the generator during training.

The authors train DCGANs on three datasets: Large-scale Scene Understanding (LSUN) (Yu et al., 2015), Imagenet-1k and a newly assembled Faces dataset. They use 64x64 images for LSUN and Faces, and 32x32 images for Imagenet-1k. They show samples generated by DCGANs on these datasets, and compare them with samples generated by other GAN variants.
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the generator network G
G = Convolutional network with fractional-strided convolutions, batch normalization and ReLU activations
G.output = Tanh(G.last_layer)

# Define the discriminator network D
D = Convolutional network with strided convolutions, batch normalization and LeakyReLU activations
D.output = Sigmoid(D.last_layer)

# Define the GAN objective function
L_GAN(G,D) = E_x[log(D(x))] + E_z[log(1 - D(G(z)))]

# Define the optimizer
optimizer = Adam(lr=0.0002, beta1=0.5)

# Define the noise vector
z = random vector of size 100

# Define the number of epochs and batches
epochs = 20
batches = number of training images / 128

# Train the GAN
for epoch in range(epochs):
  for batch in range(batches):
    # Sample a mini-batch of real images x from the dataset
    x = sample_images(128)
    
    # Sample a mini-batch of noise vectors z
    z = sample_noise(128)
    
    # Generate fake images G(z) using the generator
    G_z = G(z)
    
    # Compute the discriminator outputs for real and fake images
    D_x = D(x)
    D_G_z = D(G_z)
    
    # Compute the generator and discriminator losses using the GAN objective function
    L_G = -log(D_G_z)
    L_D = -log(D_x) - log(1 - D_G_z)
    
    # Update the generator and discriminator parameters using the optimizer and the gradients of the losses
    G_params = G_params - lr * grad(L_G, G_params)
    D_params = D_params - lr * grad(L_D, D_params)
    
  # Track the progress of the generator by sampling a fixed noise vector and saving the generated image
  G_z_fixed = G(z_fixed)
  save_image(G_z_fixed, epoch)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Define the generator network G
def generator(z, reuse=False):
  with tf.variable_scope('generator', reuse=reuse):
    # First layer: project and reshape the noise vector z
    layer1 = tf.layers.dense(z, 4*4*1024)
    layer1 = tf.reshape(layer1, [-1, 4, 4, 1024])
    layer1 = tf.layers.batch_normalization(layer1, training=True)
    layer1 = tf.nn.relu(layer1)
    
    # Second layer: fractional-strided convolution with kernel size 5x5 and stride 2
    layer2 = tf.layers.conv2d_transpose(layer1, 512, 5, 2, padding='same')
    layer2 = tf.layers.batch_normalization(layer2, training=True)
    layer2 = tf.nn.relu(layer2)
    
    # Third layer: fractional-strided convolution with kernel size 5x5 and stride 2
    layer3 = tf.layers.conv2d_transpose(layer2, 256, 5, 2, padding='same')
    layer3 = tf.layers.batch_normalization(layer3, training=True)
    layer3 = tf.nn.relu(layer3)
    
    # Fourth layer: fractional-strided convolution with kernel size 5x5 and stride 2
    layer4 = tf.layers.conv2d_transpose(layer3, 128, 5, 2, padding='same')
    layer4 = tf.layers.batch_normalization(layer4, training=True)
    layer4 = tf.nn.relu(layer4)
    
    # Fifth layer: fractional-strided convolution with kernel size 5x5 and stride 2
    logits = tf.layers.conv2d_transpose(layer4, 3, 5, 2, padding='same')
    
    # Output layer: apply tanh activation to get the generated image
    output = tf.tanh(logits)
    
    return output

# Define the discriminator network D
def discriminator(x, reuse=False):
  with tf.variable_scope('discriminator', reuse=reuse):
    # First layer: strided convolution with kernel size 5x5 and stride 2
    layer1 = tf.layers.conv2d(x, 128, 5, 2, padding='same')
    layer1 = tf.layers.batch_normalization(layer1, training=True)
    layer1 = tf.nn.leaky_relu(layer1)
    
    # Second layer: strided convolution with kernel size 5x5 and stride 2
    layer2 = tf.layers.conv2d(layer1, 256, 5, 2, padding='same')
    layer2 = tf.layers.batch_normalization(layer2, training=True)
    layer2 = tf.nn.leaky_relu(layer2)
    
    # Third layer: strided convolution with kernel size 5x5 and stride 2
    layer3 = tf.layers.conv2d(layer2, 512, 5, 2, padding='same')
    layer3 = tf.layers.batch_normalization(layer3, training=True)
    layer3 = tf.nn.leaky_relu(layer3)
    
    # Fourth layer: strided convolution with kernel size 5x5 and stride 2
    layer4 = tf.layers.conv2d(layer3, 1024, 5, 2, padding='same')
    layer4 = tf.layers.batch_normalization(layer4, training=True)
    layer4 = tf.nn.leaky_relu(layer4)
    
    # Flatten the output of the last layer
    flatten = tf.reshape(layer4, [-1, 4*4*1024])
    
    # Output layer: apply a dense layer and sigmoid activation to get the probability of real or fake
    logits = tf.layers.dense(flatten, 1)
    output = tf.sigmoid(logits)
    
    return output

# Define the GAN objective function
def gan_loss(D_real_output, D_fake_output):
  
   # Define the real label as 1 and the fake label as 0
   real_label = tf.ones_like(D_real_output) * (0.9) # use label smoothing for stability
   fake_label = tf.zeros_like(D_fake_output)

   # Define the binary cross entropy loss function
   bce = tf.keras.losses.BinaryCrossentropy()

   # Compute the discriminator loss as the sum of the losses for real and fake images
   D_loss_real = bce(real_label, D_real_output)
   D_loss_fake = bce(fake_label, D_fake_output)
   D_loss = D_loss_real + D_loss_fake

   # Compute the generator loss as the loss for fake images with flipped labels
   G_loss = bce(real_label, D_fake_output)

   return D_loss, G_loss

# Define the optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)

# Define the noise vector
z = tf.placeholder(tf.float32, shape=[None, 100])

# Define the real image placeholder
x = tf.placeholder(tf.float32, shape=[None, 64, 64, 3])

# Define the generator output
G_output = generator(z)

# Define the discriminator outputs for real and fake images
D_real_output = discriminator(x)
D_fake_output = discriminator(G_output, reuse=True)

# Define the generator and discriminator losses using the GAN objective function
D_loss, G_loss = gan_loss(D_real_output, D_fake_output)

# Get the generator and discriminator variables
G_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')
D_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='discriminator')

# Define the generator and discriminator training operations using the optimizer and the gradients of the losses
G_train_op = optimizer.minimize(G_loss, var_list=G_vars)
D_train_op = optimizer.minimize(D_loss, var_list=D_vars)

# Define the number of epochs and batches
epochs = 20
batches = number of training images / 128

# Create a TensorFlow session
sess = tf.Session()

# Initialize the variables
sess.run(tf.global_variables_initializer())

# Train the GAN
for epoch in range(epochs):
  for batch in range(batches):
    # Sample a mini-batch of real images x from the dataset
    x_batch = sample_images(128)
    
    # Sample a mini-batch of noise vectors z
    z_batch = np.random.uniform(-1, 1, size=(128, 100))
    
    # Run the generator and discriminator training operations with the feed_dict
    sess.run(G_train_op, feed_dict={z: z_batch})
    sess.run(D_train_op, feed_dict={x: x_batch, z: z_batch})
    
  # Track the progress of the generator by sampling a fixed noise vector and saving the generated image
  z_fixed = np.random.uniform(-1, 1, size=(16, 100))
  G_z_fixed = sess.run(G_output, feed_dict={z: z_fixed})
  save_image(G_z_fixed, epoch)
```