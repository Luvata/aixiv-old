---
title: 2306.05682v1 Lightweight Monocular Depth Estimation via Token-Sharing Transformer
date: 2023-06-06
---

# [Lightweight Monocular Depth Estimation via Token-Sharing Transformer](http://arxiv.org/abs/2306.05682v1)

authors: Dong-Jae Lee, Jae Young Lee, Hyounguk Shon, Eojindl Yi, Yeong-Hun Park, Sung-Sik Cho, Junmo Kim


## What, Why and How

[1]: https://arxiv.org/pdf/2306.05682v1.pdf "Lightweight Monocular Depth Estimation via Token-Sharing Transformer"
[2]: https://arxiv.org/abs/2306.05682 "[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing ..."
[3]: http://export.arxiv.org/abs/2306.05682 "[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper presents a Token-Sharing Transformer (TST), an architecture using the Transformer for monocular depth estimation, optimized especially for embedded devices.
- **Why**: Monocular depth estimation is an important task in various robotics systems and applications, but existing methods based on convolutional neural networks or Transformers are either inaccurate or computationally expensive. TST aims to achieve accurate and efficient depth estimation using a single RGB camera.
- **How**: TST utilizes global token sharing, which enables the model to learn the global information with fewer parameters and lower complexity than the Vision Transformer. TST also employs a multi-scale feature fusion module and a depth refinement module to enhance the depth prediction quality. TST outperforms the existing lightweight monocular depth estimation methods on several benchmarks and achieves real-time performance on embedded devices[^1^][1] [^2^][2].


## Main Contributions

[1]: https://arxiv.org/pdf/2306.05682v1.pdf "Lightweight Monocular Depth Estimation via Token-Sharing Transformer"
[2]: https://arxiv.org/abs/2306.05682 "[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing ..."
[3]: http://export.arxiv.org/abs/2306.05682 "[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing ..."

The paper claims the following contributions:

- **A novel architecture for monocular depth estimation using the Transformer**, which is optimized for embedded devices and achieves high accuracy and efficiency.
- **A global token sharing scheme**, which reduces the number of parameters and the complexity of attention computation in the Transformer, while preserving the global information.
- **A multi-scale feature fusion module and a depth refinement module**, which enhance the depth prediction quality by fusing features from different scales and refining the depth map with edge information.
- **Extensive experiments on several benchmarks**, which demonstrate that TST outperforms the existing lightweight monocular depth estimation methods and achieves real-time performance on embedded devices[^1^][1] [^2^][2].


## Method Summary

[1]: https://arxiv.org/pdf/2306.05682v1.pdf "Lightweight Monocular Depth Estimation via Token-Sharing Transformer"
[2]: https://arxiv.org/abs/2306.05682 "[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing ..."
[3]: http://export.arxiv.org/abs/2306.05682 "[2306.05682] Lightweight Monocular Depth Estimation via Token-Sharing ..."

Here is a summary of the method section of the paper:

- The paper proposes a Token-Sharing Transformer (TST), which consists of three main modules: a tokenization module, a token-sharing module, and a depth prediction module.
- The tokenization module converts an input RGB image into a sequence of tokens using a convolutional layer and a patch embedding layer. The tokens are then fed into the token-sharing module, which is the core of TST.
- The token-sharing module applies the Transformer to the tokens, but instead of using a separate Transformer block for each layer, it shares the same Transformer block across all layers. This reduces the number of parameters and the complexity of attention computation, while preserving the global information. The token-sharing module also employs residual connections and layer normalization to facilitate the training process.
- The depth prediction module takes the output tokens from the token-sharing module and fuses them with multi-scale features extracted from the tokenization module using a multi-scale feature fusion module. The fused features are then passed through a depth refinement module, which refines the depth map with edge information using a convolutional layer and an edge-aware loss function. The final output is a depth map that matches the resolution of the input image.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an RGB image I
# Output: a depth map D

# Tokenization module
C = Convolution(I) # apply a convolutional layer to the image
T = PatchEmbedding(C) # apply a patch embedding layer to the convolutional features

# Token-sharing module
for i in range(L): # repeat for L layers
  T = Transformer(T) # apply the same Transformer block to the tokens
  T = Residual(T) # add residual connections
  T = LayerNorm(T) # apply layer normalization

# Depth prediction module
F = MultiScaleFeatureFusion(T, C) # fuse the tokens with multi-scale features from the tokenization module
D = DepthRefinement(F) # refine the depth map with edge information
D = Upsample(D) # upsample the depth map to match the input resolution

return D # return the depth map
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: an RGB image I of size H x W x 3
# Output: a depth map D of size H x W x 1

# Hyperparameters
K = 16 # number of tokens
L = 12 # number of layers
H = 256 # hidden dimension
M = 4 # number of heads
N = 8 # number of scales

# Tokenization module
C = Convolution(I, filters=H, kernel_size=3, stride=2) # apply a convolutional layer to the image with H filters, 3x3 kernel and stride 2
T = PatchEmbedding(C, patches=K) # apply a patch embedding layer to the convolutional features with K patches

# Token-sharing module
Wq = Linear(H, H) # initialize a linear layer for query projection
Wk = Linear(H, H) # initialize a linear layer for key projection
Wv = Linear(H, H) # initialize a linear layer for value projection
Wo = Linear(H, H) # initialize a linear layer for output projection
W1 = Linear(H, H) # initialize a linear layer for feed-forward network
W2 = Linear(H, H) # initialize a linear layer for feed-forward network

for i in range(L): # repeat for L layers
  Q = Wq(T) # project the tokens to query space
  K = Wk(T) # project the tokens to key space
  V = Wv(T) # project the tokens to value space
  Q = Split(Q, M) # split the query into M heads
  K = Split(K, M) # split the key into M heads
  V = Split(V, M) # split the value into M heads
  A = Attention(Q, K, V) # compute the scaled dot-product attention for each head
  A = Concat(A) # concatenate the attention outputs from each head
  A = Wo(A) # project the attention output to output space
  A = Residual(A, T) # add residual connections with the input tokens
  A = LayerNorm(A) # apply layer normalization
  F = W1(A) # apply the first linear layer of the feed-forward network
  F = ReLU(F) # apply ReLU activation function
  F = W2(F) # apply the second linear layer of the feed-forward network
  F = Residual(F, A) # add residual connections with the attention output
  F = LayerNorm(F) # apply layer normalization
  T = F # update the tokens

# Depth prediction module
F_list = [] # initialize an empty list for multi-scale features
for n in range(N): # repeat for N scales
  F_n = Convolution(C, filters=H/2**n, kernel_size=3, stride=2**n) # apply a convolutional layer to the convolutional features with H/2^n filters, 3x3 kernel and stride 2^n 
  F_list.append(F_n) # append the multi-scale feature to the list

D_list = [] # initialize an empty list for depth maps
for k in range(K): # repeat for K tokens
  T_k = T[k] # get the k-th token
  T_k = Repeat(T_k, H/4 x W/4 x H/4) # repeat the token to match the size of the smallest multi-scale feature (H/4 x W/4 x H/4)
  D_k_list = [] # initialize an empty list for depth maps from different scales for the k-th token
  for n in range(N): # repeat for N scales 
    F_n_k = Concat(T_k, F_list[n]) # concatenate the token with the n-th multi-scale feature 
    D_n_k = Convolution(F_n_k, filters=1, kernel_size=3, stride=1) # apply a convolutional layer to the concatenated feature with one filter, 3x3 kernel and stride 1 
    D_n_k = Upsample(D_n_k, scale_factor=2**n) # upsample the depth map to match the input resolution (H x W x 1)
    D_k_list.append(D_n_k) # append the depth map from the n-th scale to the list 
  D_k = Average(D_k_list) # average the depth maps from different scales for the k-th token 
  D_list.append(D_k) # append the depth map for the k-th token to the list 

D = Average(D_list) # average the depth maps from different tokens
E = EdgeDetection(I) # detect the edges in the input image using a Sobel filter
D = Convolution(D, filters=1, kernel_size=3, stride=1) # apply a convolutional layer to the depth map with one filter, 3x3 kernel and stride 1
D = EdgeAwareLoss(D, E) # apply the edge-aware loss function to the depth map using the edge information

return D # return the depth map
```