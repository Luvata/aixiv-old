---
title: 2306.16527v1 OBELISC  An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents
date: 2023-06-17
---

# [OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](http://arxiv.org/abs/2306.16527v1)

authors: Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh


## What, Why and How

[1]: https://arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2303.16527v1 "[2303.16527v1] Understanding and Improving Features Learned in ... - arXiv"
[3]: http://export.arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of Interleaved ..."

Here is a summary of the paper:

```
What: The paper introduces OBELISC, an open web-scale filtered dataset of interleaved image-text documents, and shows its viability for training large multimodal models.

Why: The paper aims to address the lack of publicly available datasets for natural documents, which interleave images and text, and which are more suitable for multimodal tasks than image-text pairs.

How: The paper describes the dataset creation process, which involves extracting web pages from Common Crawl, filtering them based on various criteria, and processing them to obtain image-text documents. The paper also provides an analysis of the dataset's content, and demonstrates its usefulness by training an 80 billion parameters vision and language model on it and evaluating it on various multimodal benchmarks.
```

## Main Contributions

[1]: https://arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2303.16527v1 "[2303.16527v1] Understanding and Improving Features Learned in ... - arXiv"
[3]: http://export.arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of Interleaved ..."

The paper claims the following contributions:

- The creation and release of OBELISC, an open web-scale filtered dataset of interleaved image-text documents, comprising 141 million web pages, 353 million images, and 115 billion text tokens.
- The description and analysis of the dataset creation process and the content of the dataset, including comprehensive filtering rules and statistics.
- The demonstration of the viability of OBELISC for training large multimodal models, by training an 80 billion parameters vision and language model on it and obtaining competitive performance on various multimodal benchmarks.

## Method Summary

[1]: https://arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2303.16527v1 "[2303.16527v1] Understanding and Improving Features Learned in ... - arXiv"
[3]: http://export.arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of Interleaved ..."

Here is a summary of the method section of the paper:

```
The method section of the paper consists of three parts: dataset creation, dataset analysis, and model training and evaluation.

Dataset creation: The authors describe how they extracted web pages from Common Crawl, a publicly available web archive, and filtered them based on various criteria, such as language, content type, domain, and image quality. They also explain how they processed the web pages to obtain image-text documents, which are segments of text that are associated with one or more images. They provide details on the filtering rules and the data processing pipeline, and justify their choices based on previous work and empirical observations.

Dataset analysis: The authors provide an analysis of the content and the statistics of the dataset, such as the number of documents, images, and tokens, the distribution of languages, domains, and topics, and the diversity and quality of the images and the text. They also compare their dataset with existing multimodal datasets, such as Conceptual Captions and CC12M, and highlight the advantages and challenges of their dataset.

Model training and evaluation: The authors demonstrate the viability of their dataset for training large multimodal models by training an 80 billion parameters vision and language model on it using Megatron-LM. They evaluate their model on various multimodal benchmarks that require reasoning over one or multiple images to generate a text, such as ImageNet-21K-1K Zero-Shot Classification, ImageNet-21K-1K Few-Shot Classification, COCO Captions Generation, VizWiz Captions Generation, NLVR2 Visual Reasoning, and GQA Visual Question Answering. They compare their model with existing models trained on image-text pairs or natural documents, and show that their model achieves competitive or superior performance on most tasks.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2303.16527v1 "[2303.16527v1] Understanding and Improving Features Learned in ... - arXiv"
[3]: http://export.arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of Interleaved ..."

Here is a high-level pseudo code for the paper:

```
# Dataset creation
# Input: Common Crawl web archive
# Output: OBELISC dataset of image-text documents

# Define filtering rules based on language, content type, domain, and image quality
filtering_rules = ...

# Define data processing pipeline to extract image-text documents from web pages
data_processing_pipeline = ...

# Initialize empty dataset
dataset = []

# For each web page in Common Crawl
for web_page in common_crawl:

  # Apply filtering rules to web page
  if filtering_rules(web_page):

    # Apply data processing pipeline to web page
    image_text_document = data_processing_pipeline(web_page)

    # Add image-text document to dataset
    dataset.append(image_text_document)

# Return dataset
return dataset

# Dataset analysis
# Input: OBELISC dataset of image-text documents
# Output: Statistics and comparisons of the dataset

# Define metrics to measure the content and the statistics of the dataset
metrics = ...

# Compute metrics on the dataset
statistics = compute_metrics(dataset, metrics)

# Compare the dataset with existing multimodal datasets
comparisons = compare_datasets(dataset, existing_datasets)

# Return statistics and comparisons
return statistics, comparisons

# Model training and evaluation
# Input: OBELISC dataset of image-text documents, multimodal benchmarks
# Output: Trained model, performance scores

# Define model architecture and parameters
model = Megatron-LM(80 billion parameters)

# Train model on the dataset using self-supervised learning
model.train(dataset)

# Evaluate model on multimodal benchmarks that require reasoning over one or multiple images to generate a text
performance_scores = model.evaluate(benchmarks)

# Return trained model and performance scores
return model, performance_scores
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2303.16527v1 "[2303.16527v1] Understanding and Improving Features Learned in ... - arXiv"
[3]: http://export.arxiv.org/abs/2306.16527 "[2306.16527] OBELISC: An Open Web-Scale Filtered Dataset of Interleaved ..."

Here is a detailed pseudo code to implement the paper:

```
# Dataset creation
# Input: Common Crawl web archive
# Output: OBELISC dataset of image-text documents

# Define filtering rules based on language, content type, domain, and image quality
filtering_rules = {
  # Language: keep only English web pages
  language: "en",

  # Content type: keep only web pages with HTML content
  content_type: "text/html",

  # Domain: exclude web pages from domains that are not relevant or trustworthy, such as social media, adult, or spam domains
  domain: exclude(["facebook.com", "twitter.com", "instagram.com", ...]),

  # Image quality: keep only web pages with images that have a minimum resolution and size, and are not corrupted or duplicated
  image_quality: {
    min_resolution: (300, 300),
    min_size: 10 KB,
    max_duplicates: 1,
    corrupted: False
  }
}

# Define data processing pipeline to extract image-text documents from web pages
data_processing_pipeline = {
  # Parse HTML content of web page and extract text and image elements
  parse_html: ...

  # Remove text elements that are not relevant or informative, such as navigation bars, footers, ads, or scripts
  remove_irrelevant_text: ...

  # Remove image elements that are not relevant or informative, such as logos, icons, banners, or ads
  remove_irrelevant_images: ...

  # Segment text elements into paragraphs based on HTML tags or punctuation marks
  segment_text: ...

  # Associate each paragraph with one or more images that are visually or semantically related to it
  associate_images: ...

  # Return a list of image-text documents, each consisting of a paragraph and its associated images
  return_image_text_documents: ...
}

# Initialize empty dataset
dataset = []

# For each web page in Common Crawl
for web_page in common_crawl:

  # Apply filtering rules to web page
  if filtering_rules(web_page):

    # Apply data processing pipeline to web page
    image_text_documents = data_processing_pipeline(web_page)

    # Add image-text documents to dataset
    dataset.extend(image_text_documents)

# Return dataset
return dataset

# Dataset analysis
# Input: OBELISC dataset of image-text documents
# Output: Statistics and comparisons of the dataset

# Define metrics to measure the content and the statistics of the dataset
metrics = {
  # Number of documents, images, and tokens in the dataset
  count: ...

  # Distribution of languages, domains, and topics in the dataset
  distribution: ...

  # Diversity and quality of the images and the text in the dataset
  diversity_quality: ...
}

# Compute metrics on the dataset
statistics = compute_metrics(dataset, metrics)

# Compare the dataset with existing multimodal datasets, such as Conceptual Captions and CC12M, based on their size, diversity, quality, and naturalness
comparisons = compare_datasets(dataset, existing_datasets)

# Return statistics and comparisons
return statistics, comparisons

# Model training and evaluation
# Input: OBELISC dataset of image-text documents, multimodal benchmarks
# Output: Trained model, performance scores

# Define model architecture and parameters using Megatron-LM framework (https://github.com/NVIDIA/Megatron-LM)
model = Megatron-LM(80 billion parameters)

# Train model on the dataset using self-supervised learning with masked language modeling and masked region modeling objectives
model.train(dataset)

# Evaluate model on multimodal benchmarks that require reasoning over one or multiple images to generate a text, such as ImageNet-21K-1K Zero-Shot Classification, ImageNet-21K-1K Few-Shot Classification, COCO Captions Generation, VizWiz Captions Generation, NLVR2 Visual Reasoning, and GQA Visual Question Answering
performance_scores = model.evaluate(benchmarks)

# Return trained model and performance scores
return model, performance_scores
```