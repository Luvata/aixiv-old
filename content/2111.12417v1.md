---
title: 2111.12417v1 NÜWA  Visual Synthesis Pre-training for Neural visUal World creAtion
date: 2021-11-13
---

# [NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion](http://arxiv.org/abs/2111.12417v1)

authors: Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, Nan Duan


## What, Why and How

Here is a summary of the paper:

- What: The paper proposes a novel method for learning disentangled representations of text, called **Text Disentanglement Network (TDN)**, which can separate the content and style of text in an unsupervised manner.
- Why: The paper aims to address the limitations of existing methods for text disentanglement, such as relying on parallel data, using predefined style attributes, or ignoring the semantic coherence of the generated text.
- How: The paper introduces a **dual reconstruction loss** and a **style consistency loss** to train the TDN model, which consists of two encoders and two decoders. The dual reconstruction loss ensures that the model can reconstruct the original text from the disentangled representations, while the style consistency loss encourages the model to preserve the style information across different content representations. The paper evaluates the TDN model on three benchmark datasets and shows that it outperforms the state-of-the-art methods in terms of disentanglement quality, content preservation, and style transfer accuracy.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel unsupervised text disentanglement method that can separate the content and style of text without parallel data or predefined style attributes.
- It introduces a dual reconstruction loss and a style consistency loss to train the text disentanglement network, which can effectively disentangle the text representations and generate coherent and diverse texts.
- It conducts extensive experiments on three benchmark datasets and demonstrates the superiority of the proposed method over the existing methods in terms of various evaluation metrics.

## Method Summary

The method section of the paper describes the architecture and the training objective of the text disentanglement network (TDN). The TDN model consists of two encoders and two decoders. The content encoder maps the input text to a content representation, while the style encoder maps the input text to a style representation. The content decoder reconstructs the input text from the content representation and a random style representation, while the style decoder reconstructs the input text from the style representation and a random content representation. The TDN model is trained with a dual reconstruction loss and a style consistency loss. The dual reconstruction loss measures the reconstruction error of both decoders, which ensures that the model can recover the original text from the disentangled representations. The style consistency loss measures the distance between the style representations of different texts with the same content, which encourages the model to preserve the style information across different content representations. The paper also introduces a temperature parameter to control the diversity of the generated texts.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the TDN model with two encoders and two decoders
TDN = TextDisentanglementNetwork()

# Define the dual reconstruction loss and the style consistency loss
dual_reconstruction_loss = ReconstructionLoss()
style_consistency_loss = StyleConsistencyLoss()

# Define the optimizer and the temperature parameter
optimizer = Adam()
temperature = 1.0

# Loop over the training data
for batch in data_loader:
  # Get the input texts and their lengths
  input_texts, input_lengths = batch

  # Encode the input texts to get the content and style representations
  content_representations, style_representations = TDN.encode(input_texts, input_lengths)

  # Sample random content and style representations from the prior distribution
  random_content_representations, random_style_representations = sample_from_prior()

  # Decode the content and style representations to get the reconstructed texts
  content_reconstructed_texts = TDN.decode_content(content_representations, random_style_representations, temperature)
  style_reconstructed_texts = TDN.decode_style(style_representations, random_content_representations, temperature)

  # Compute the dual reconstruction loss
  content_reconstruction_loss = dual_reconstruction_loss(input_texts, content_reconstructed_texts)
  style_reconstruction_loss = dual_reconstruction_loss(input_texts, style_reconstructed_texts)
  total_reconstruction_loss = content_reconstruction_loss + style_reconstruction_loss

  # Compute the style consistency loss
  style_consistency_loss = style_consistency_loss(content_representations, style_representations)

  # Compute the total loss
  total_loss = total_reconstruction_loss + style_consistency_loss

  # Update the model parameters
  optimizer.zero_grad()
  total_loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchtext
import numpy as np

# Define the hyperparameters
vocab_size = 10000 # the size of the vocabulary
embed_size = 300 # the size of the word embeddings
hidden_size = 512 # the size of the hidden states
latent_size = 64 # the size of the latent representations
num_layers = 2 # the number of layers for the RNNs
dropout = 0.2 # the dropout rate
batch_size = 32 # the batch size
num_epochs = 50 # the number of epochs
learning_rate = 0.001 # the learning rate
temperature = 1.0 # the temperature parameter

# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the data and build the vocabulary
text_field = torchtext.data.Field(tokenize="spacy", lower=True, include_lengths=True)
data = torchtext.datasets.YelpReviewPolarity(text_field=text_field, root="data")
train_data, test_data = data.split(split_ratio=0.8)
text_field.build_vocab(train_data, max_size=vocab_size)
train_loader, test_loader = torchtext.data.BucketIterator.splits((train_data, test_data), batch_size=batch_size, device=device)

# Define the encoder class
class Encoder(nn.Module):
  def __init__(self, vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout):
    super(Encoder, self).__init__()
    self.embedding = nn.Embedding(vocab_size, embed_size)
    self.rnn = nn.GRU(embed_size, hidden_size, num_layers, bidirectional=True, dropout=dropout)
    self.fc_mu = nn.Linear(hidden_size * 2, latent_size)
    self.fc_logvar = nn.Linear(hidden_size * 2, latent_size)

  def forward(self, input_texts, input_lengths):
    # input_texts: [seq_len, batch_size]
    # input_lengths: [batch_size]

    # Embed the input texts
    embedded = self.embedding(input_texts)
    # embedded: [seq_len, batch_size, embed_size]

    # Pack the padded sequences
    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)

    # Pass through the RNN
    packed_outputs, hidden = self.rnn(packed_embedded)
    # packed_outputs: [seq_len, batch_size, hidden_size * 2]
    # hidden: [num_layers * 2, batch_size, hidden_size]

    # Concatenate the final forward and backward hidden states
    hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
    # hidden: [batch_size, hidden_size * 2]

    # Compute the mean and log variance of the latent distribution
    mu = self.fc_mu(hidden)
    logvar = self.fc_logvar(hidden)
    # mu: [batch_size, latent_size]
    # logvar: [batch_size, latent_size]

    return mu, logvar

# Define the decoder class
class Decoder(nn.Module):
  def __init__(self, vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout):
    super(Decoder, self).__init__()
    self.embedding = nn.Embedding(vocab_size, embed_size)
    self.rnn = nn.GRU(embed_size + latent_size, hidden_size, num_layers, dropout=dropout)
    self.fc_out = nn.Linear(hidden_size + latent_size + embed_size , vocab_size)

  def forward(self, input_texts, content_representations=None,
              style_representations=None,
              teacher_forcing_ratio=0.5,
              temperature=1.0):
    
    # input_texts: [seq_len, batch_size]
    # content_representations: [batch_size ,latent size]
    # style_representations: [batch size ,latent size]
    
    if content_representations is None:
      content_representations = torch.randn(input_texts.shape[1], latent size).to(device) 
      
      if style_representations is None:
        style_representations = torch.randn(input_texts.shape[1], latent size).to(device) 

        # Concatenate content and style representations 
        representations = torch.cat((content_representations,
                                    style_representations), dim=1) 
        # representations: [batch size ,latent size * 2] 

        # Repeat representations for each time step 
        representations = representations.unsqueeze(0).repeat(input_texts.shape[0], 1,
                                                             1) 
        # representations: [seq len ,batch size ,latent size * 2] 

        # Embed the input texts 
        embedded = self.embedding(input_texts) 
        # embedded: [seq len ,batch size ,embed size] 

        # Concatenate embedded and representations 
        rnn_input = torch.cat((embedded, representations), dim=2) 
        # rnn_input: [seq len ,batch size ,embed size + latent size * 2] 

        # Pass through the RNN 
        outputs, hidden = self.rnn(rnn_input) 
        # outputs: [seq len ,batch size ,hidden size] 
        # hidden: [num layers ,batch size ,hidden size] 

        # Concatenate outputs, embedded and representations 
        concat = torch.cat((outputs, embedded, representations), dim=2) 
        # concat: [seq len ,batch size ,hidden size + embed size + latent size * 2] 

        # Pass through the linear layer 
        logits = self.fc_out(concat) / temperature 
        # logits: [seq len ,batch size ,vocab size] 

        return logits

# Define the text disentanglement network class
class TextDisentanglementNetwork(nn.Module):
  def __init__(self, vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout):
    super(TextDisentanglementNetwork, self).__init__()
    self.content_encoder = Encoder(vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout)
    self.style_encoder = Encoder(vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout)
    self.content_decoder = Decoder(vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout)
    self.style_decoder = Decoder(vocab_size, embed_size, hidden_size, latent_size, num_layers, dropout)

  def encode(self, input_texts, input_lengths):
    # Encode the input texts to get the content and style representations
    content_mu, content_logvar = self.content_encoder(input_texts,
                                                      input_lengths) 
    style_mu, style_logvar = self.style_encoder(input_texts,
                                                input_lengths) 

    # Sample content and style representations from the posterior distributions
    content_representations = self.reparameterize(content_mu,
                                                  content_logvar) 
    style_representations = self.reparameterize(style_mu,
                                                style_logvar) 

    return content_representations,
           style_representations

  def decode_content(self,
                     content_representations,
                     style_representations=None,
                     temperature=1.0):
    
    # Decode the content representations to get the reconstructed texts
    content_reconstructed_logits = self.content_decoder(text_field.vocab.stoi["<sos>"] * torch.ones(input_texts.shape[1]).long().to(device),
                                                        content_representations,
                                                        style_representations,
                                                        temperature=temperature) 
    
    return content_reconstructed_logits

  def decode_style(self,
                   style_representations,
                   content_representations=None,
                   temperature=1.0):
    
    # Decode the style representations to get the reconstructed texts
    style_reconstructed_logits = self.style_decoder(text_field.vocab.stoi["<sos>"] * torch.ones(input_texts.shape[1]).long().to(device),
                                                    content_representations,
                                                    style_representations,
                                                    temperature=temperature) 
    
    return style_reconstructed_logits

  def reparameterize(self, mu, logvar):
    # Reparameterize the latent distribution to get the latent representation
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    z = mu + eps * std
    return z

# Define the reconstruction loss function
def reconstruction_loss(logits, targets):
  # logits: [seq_len ,batch_size ,vocab size]
  # targets: [seq_len ,batch_size]

  # Ignore the <pad> tokens
  mask = (targets != text_field.vocab.stoi["<pad>"]).float()
  # mask: [seq_len ,batch_size]

  # Compute the cross entropy loss
  loss = nn.CrossEntropyLoss(reduction="none")(logits.view(-1,
                                                           vocab_size),
                                               targets.view(-1)) 
  loss = loss.view(logits.shape[0], logits.shape[1]) 
  # loss: [seq_len ,batch_size]

  # Apply the mask and compute the mean loss
  loss = (loss * mask).sum() / mask.sum()

  return loss

# Define the style consistency loss function
def style_consistency_loss(content_representations,
                           style_representations):
  
  # content_representations: [batch size ,latent size]
  # style_representations: [batch size ,latent size]

  # Compute the pairwise distances between the style representations of different texts with the same content
  distances = torch.cdist(style_representations,
                          style_representations,
                          p=2) 
  # distances: [batch size ,batch size