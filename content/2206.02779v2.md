---
title: 2206.02779v2 Blended Latent Diffusion
date: 2022-06-03
---

# [Blended Latent Diffusion](http://arxiv.org/abs/2206.02779v2)

authors: Omri Avrahami, Ohad Fried, Dani Lischinski


## What, Why and How

[1]: https://arxiv.org/abs/2206.02779 "[2206.02779] Blended Latent Diffusion - arXiv.org"
[2]: https://arxiv.org/pdf/2206.02779v2.pdf "Blended Latent Diffusion - arXiv.org"
[3]: http://export.arxiv.org/abs/2006.02779v2 "[2006.02779v2] Systematic uncertainty of standard sirens from the ..."

Here is a summary of the paper:

- **What**: The paper presents a method for local text-driven editing of generic images using a latent diffusion model (LDM) that operates in a lower-dimensional latent space and blends the latents at each step.
- **Why**: The paper aims to overcome the limitations of existing diffusion models, which are slow, inaccurate, and prone to artifacts when performing local edits on diverse images using text prompts.
- **How**: The paper leverages a text-to-image LDM that speeds up diffusion by eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. The paper also proposes an optimization-based solution for improving the reconstruction quality of LDM and a technique for handling thin masks. The paper evaluates the method against the available baselines both qualitatively and quantitatively and demonstrates its advantages.

## Main Contributions

The paper claims the following contributions:

- A novel method for local text-driven editing of generic images using a latent diffusion model (LDM) that blends the latents at each step.
- An optimization-based solution for improving the reconstruction quality of LDM by minimizing the discrepancy between the input and output images in the masked region.
- A technique for handling thin masks by applying a Gaussian blur to the mask and adjusting the blending weights accordingly.
- A comprehensive evaluation of the method against the available baselines on various tasks and datasets, showing its superiority in terms of speed, precision, and diversity.

## Method Summary

The method section of the paper consists of four subsections:

- **Latent Diffusion Model (LDM)**: This subsection reviews the LDM proposed by , which is a text-to-image diffusion model that operates in a lower-dimensional latent space instead of the pixel space. The LDM consists of an encoder, a decoder, and a text encoder. The encoder maps an input image to a latent code, the decoder maps a latent code to an output image, and the text encoder maps a text prompt to a text embedding. The LDM performs diffusion by sampling a noise vector from a Gaussian distribution and adding it to the latent code at each step. The LDM also uses a CLIP model  to compute the similarity between the output image and the text prompt at each step and guide the diffusion process towards the desired result.
- **Blended Latent Diffusion**: This subsection introduces the main contribution of the paper, which is a method for performing local text-driven editing of generic images using LDM. The method takes as input an image, a mask, and a text prompt, and outputs an edited image that satisfies the text prompt in the masked region. The method works by blending the latents at each diffusion step using the mask as a weight. The method also applies a Gaussian blur to the mask to smooth the transition between the edited and unedited regions.
- **Reconstruction Optimization**: This subsection proposes an optimization-based solution for improving the reconstruction quality of LDM, which suffers from inherent inaccuracies due to operating in a lower-dimensional latent space. The solution works by minimizing the discrepancy between the input and output images in the masked region using an L2 loss function. The solution also uses a perceptual loss function  to preserve the semantic content of the input image in the unmasked region.
- **Thin Mask Handling**: This subsection addresses the scenario of performing local edits using thin masks, which pose a challenge for LDM due to its coarse resolution. The solution works by applying a Gaussian blur to the mask and adjusting the blending weights accordingly. The solution also uses a Laplacian pyramid  to refine the output image at multiple scales and reduce artifacts.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: image x, mask m, text prompt t
# Output: edited image y

# Encode the image and the text to latent codes
z = encoder(x)
w = text_encoder(t)

# Apply Gaussian blur to the mask
m = blur(m)

# Initialize the noise vector
epsilon = sample_noise()

# Perform diffusion steps
for i in range(T):
  # Compute the noise level
  beta = get_beta(i)

  # Blend the latents using the mask
  z_hat = m * (z + sqrt(beta) * epsilon) + (1 - m) * z

  # Decode the blended latent to an image
  y_hat = decoder(z_hat)

  # Compute the CLIP similarity between the image and the text
  s = clip_similarity(y_hat, w)

  # Update the noise vector using gradient ascent
  epsilon = epsilon + alpha * grad(epsilon, s)

# Perform reconstruction optimization
y_opt = optimize(y_hat, x, m)

# Perform thin mask handling using Laplacian pyramid
y_final = refine(y_opt, x, m)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: image x, mask m, text prompt t
# Output: edited image y

# Define the encoder, decoder, and text encoder networks
encoder = Encoder()
decoder = Decoder()
text_encoder = TextEncoder()

# Define the CLIP model
clip_model = CLIP()

# Define the L2 loss function
l2_loss = L2Loss()

# Define the perceptual loss function
perceptual_loss = PerceptualLoss()

# Define the Laplacian pyramid
laplacian_pyramid = LaplacianPyramid()

# Define the diffusion parameters
T = 100 # number of diffusion steps
alpha = 0.01 # learning rate for gradient ascent
sigma = 0.01 # standard deviation for noise sampling
beta_1 = 0.0001 # initial noise level
beta_T = 0.02 # final noise level

# Encode the image and the text to latent codes
z = encoder(x)
w = text_encoder(t)

# Apply Gaussian blur to the mask with a kernel size of 5 and a sigma of 1
m = blur(m, 5, 1)

# Initialize the noise vector with a random Gaussian noise with mean 0 and standard deviation sigma
epsilon = np.random.normal(0, sigma, z.shape)

# Perform diffusion steps
for i in range(T):
  # Compute the noise level using a cosine schedule
  beta = beta_1 + (beta_T - beta_1) * (1 - np.cos(np.pi * i / T)) / 2

  # Blend the latents using the mask
  z_hat = m * (z + np.sqrt(beta) * epsilon) + (1 - m) * z

  # Decode the blended latent to an image
  y_hat = decoder(z_hat)

  # Compute the CLIP similarity between the image and the text using a softmax function
  s = clip_model(y_hat, w)
  s = softmax(s)

  # Update the noise vector using gradient ascent with respect to s
  epsilon = epsilon + alpha * grad(epsilon, s)

# Perform reconstruction optimization by minimizing the L2 loss between y_hat and x in the masked region and the perceptual loss between y_hat and x in the unmasked region for 100 iterations using Adam optimizer with a learning rate of 0.001
y_opt = y_hat.clone()
optimizer = Adam(y_opt, lr=0.001)
for i in range(100):
  optimizer.zero_grad()
  loss = m * l2_loss(y_opt, x) + (1 - m) * perceptual_loss(y_opt, x)
  loss.backward()
  optimizer.step()

# Perform thin mask handling using Laplacian pyramid by refining y_opt at multiple scales and blending it with x using the mask for each scale
y_final = y_opt.clone()
for scale in laplacian_pyramid.scales:
  # Downsample y_opt and x to the current scale
  y_opt_scale = downsample(y_opt, scale)
  x_scale = downsample(x, scale)

  # Refine y_opt_scale by minimizing the L2 loss between y_opt_scale and x_scale in the masked region for 10 iterations using Adam optimizer with a learning rate of 0.001
  optimizer = Adam(y_opt_scale, lr=0.001)
  for i in range(10):
    optimizer.zero_grad()
    loss = m * l2_loss(y_opt_scale, x_scale)
    loss.backward()
    optimizer.step()

  # Blend y_opt_scale with x_scale using the mask
  y_final_scale = m * y_opt_scale + (1 - m) * x_scale

  # Upsample y_final_scale to the original scale and add it to y_final
  y_final_scale = upsample(y_final_scale, scale)
  y_final += y_final_scale

# Return the final edited image
return y_final
```