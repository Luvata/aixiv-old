---
title: 2306.00180v1 FlowCam  Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow
date: 2023-06-01
---

# [FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow](http://arxiv.org/abs/2306.00180v1)

authors: Cameron Smith, Yilun Du, Ayush Tewari, Vincent Sitzmann


## What, Why and How

[1]: https://arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."
[2]: https://arxiv.org/abs/2306.00186 "[2306.00186] Factually Consistent Summarization via Reinforcement ..."
[3]: http://export.arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a method that jointly reconstructs camera poses and 3D neural scene representations from unposed video frames, without relying on structure-from-motion or other external pose estimation techniques.
- **Why**: The paper aims to overcome the limitations of existing methods that require precise camera poses for 3D neural scene learning, which are costly and unreliable on large-scale video data.
- **How**: The paper introduces FlowCam, a system that estimates poses by lifting frame-to-frame optical flow to 3D scene flow via differentiable rendering, and then performing SE (3) camera pose estimation via a weighted least-squares fit to the scene flow field. FlowCam also learns a generalizable neural scene representation that can be supervised via re-rendering the input video. FlowCam is trained end-to-end and fully self-supervised on real-world video datasets.

## Main Contributions

[1]: https://arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."
[2]: https://arxiv.org/abs/2306.00186 "[2306.00186] Factually Consistent Summarization via Reinforcement ..."
[3]: http://export.arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."

According to the paper at [^1^][1], the main contributions are:

- **A novel method for online and self-supervised pose estimation and 3D neural scene reconstruction from unposed video frames**, without relying on structure-from-motion or other external pose estimation techniques.
- **A differentiable rendering module that lifts optical flow to scene flow**, preserving locality and shift-equivariance of the image processing backbone.
- **A weighted least-squares fit to the scene flow field for SE (3) camera pose estimation**, which is robust to outliers and noise.
- **A generalizable neural scene representation that can be supervised via re-rendering the input video**, enabling end-to-end and fully self-supervised training on real-world video datasets.
- **Extensive experiments on diverse and challenging video sequences**, demonstrating the effectiveness and robustness of the proposed method compared to existing approaches.

## Method Summary

[1]: https://arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."
[2]: https://arxiv.org/abs/2306.00186 "[2306.00186] Factually Consistent Summarization via Reinforcement ..."
[3]: http://export.arxiv.org/abs/2306.00180 "[2306.00180] FlowCam: Training Generalizable 3D Radiance Fields without ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper presents FlowCam, a system that consists of four main components: an image processing backbone, a differentiable rendering module, a pose estimation module, and a neural scene representation module.
- The image processing backbone takes two consecutive video frames as input and computes their optical flow using a pre-trained network. The optical flow is then fed into the differentiable rendering module, which lifts it to 3D scene flow by projecting it onto a spherical surface and applying the inverse of the neural scene representation's ray-marching operation. The scene flow is then used by the pose estimation module, which performs a weighted least-squares fit to the SE (3) group to estimate the relative camera pose between the two frames. The pose estimation module also outputs a confidence score for each pixel based on the scene flow consistency and visibility. The neural scene representation module takes the estimated pose and the first frame as input and renders a novel view of the scene using a neural radiance field model. The rendered view is compared with the second frame to compute a reconstruction loss, which is used to update the neural scene representation parameters. The system is trained end-to-end and fully self-supervised using only video data, without any ground-truth poses or depth maps.

## Pseudo Code

Here is a possible pseudo code to implement the paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np

# Define hyperparameters
num_epochs = 100
batch_size = 16
learning_rate = 0.001
sphere_radius = 1.0
num_samples = 64

# Load video data
video_dataset = torchvision.datasets.VideoFolder("path/to/video/data")
video_dataloader = torch.utils.data.DataLoader(video_dataset, batch_size=batch_size, shuffle=True)

# Define image processing backbone
backbone = torchvision.models.flowNet2(pretrained=True)

# Define neural scene representation module
class NeuralRadianceField(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # Define a multi-layer perceptron with 8 hidden layers and ReLU activations
        self.mlp = torch.nn.Sequential(
            torch.nn.Linear(3, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 4)
        )

    def forward(self, x):
        # x is a batch of 3D points of shape (B, N, 3)
        # Return a batch of RGB colors and densities of shape (B, N, 4)
        return self.mlp(x)

# Initialize neural scene representation module
neural_scene = NeuralRadianceField()

# Define differentiable rendering module
def differentiable_rendering(optical_flow, neural_scene):
    # optical_flow is a batch of optical flow maps of shape (B, 2, H, W)
    # neural_scene is a neural radiance field model
    # Return a batch of scene flow maps and confidence scores of shape (B, 4, H, W)
    
    # Compute the spherical coordinates of the pixels in the first frame
    theta_1 = np.linspace(-np.pi/2, np.pi/2, H) # shape (H,)
    phi_1 = np.linspace(-np.pi, np.pi, W) # shape (W,)
    theta_1_grid, phi_1_grid = np.meshgrid(theta_1, phi_1) # shape (H, W)
    theta_1_grid = theta_1_grid.reshape(-1) # shape (H*W,)
    phi_1_grid = phi_1_grid.reshape(-1) # shape (H*W,)
    
    # Compute the spherical coordinates of the pixels in the second frame using the optical flow
    u_flow = optical_flow[:, 0] # shape (B, H*W)
    v_flow = optical_flow[:, 1] # shape (B, H*W)
    theta_2_grid = theta_1_grid + v_flow / sphere_radius # shape (B, H*W)
    phi_2_grid = phi_1_grid + u_flow / sphere_radius # shape (B, H*W)
    
    # Compute the cartesian coordinates of the pixels in the first and second frames on the spherical surface
    x_1_grid = sphere_radius * np.sin(theta_1_grid) * np.cos(phi_1_grid) # shape (H*W,)
    y_1_grid = sphere_radius * np.sin(theta_1_grid) * np.sin(phi_1_grid) # shape (H*W,)
    z_1_grid = sphere_radius * np.cos(theta_1_grid) # shape (H*W,)
    x_2_grid = sphere_radius * np.sin(theta_2_grid) * np.cos(phi_2_grid) # shape (B, H*W)
    y_2_grid = sphere_radius * np.sin(theta_2_grid) * np.sin(phi_2_grid) # shape (B, H*W)
    z_2_grid = sphere_radius * np.cos(theta_2_grid) # shape (B, H*W)
    
    # Compute the scene flow vectors from the first to the second frame
    scene_flow_x = x_2_grid - x_1_grid # shape (B, H*W)
    scene_flow_y = y_2_grid - y_1_grid # shape (B, H*W)
    scene_flow_z = z_2_grid - z_1_grid # shape (B, H*W)
    
    # Compute the confidence scores based on the scene flow consistency and visibility
    # Scene flow consistency: the scene flow vectors should be aligned with the ray directions from the first frame
    # Visibility: the scene flow vectors should not cross the spherical surface
    ray_dir_x = x_1_grid / sphere_radius # shape (H*W,)
    ray_dir_y = y_1_grid / sphere_radius # shape (H*W,)
    ray_dir_z = z_1_grid / sphere_radius # shape (H*W,)
    dot_product = scene_flow_x * ray_dir_x + scene_flow_y * ray_dir_y + scene_flow_z * ray_dir_z # shape (B, H*W)
    consistency_score = torch.clamp(dot_product, 0, 1) # shape (B, H*W)
    visibility_score = torch.clamp(1 - torch.norm(scene_flow_x, scene_flow_y, scene_flow_z, dim=1) / sphere_radius, 0, 1) # shape (B, H*W)
    confidence_score = consistency_score * visibility_score # shape (B, H*W)
    
    # Reshape the scene flow and confidence score to the original image size
    scene_flow_x = scene_flow_x.reshape(B, H, W) # shape (B, H, W)
    scene_flow_y = scene_flow_y.reshape(B, H, W) # shape (B, H, W)
    scene_flow_z = scene_flow_z.reshape(B, H, W) # shape (B, H, W)
    confidence_score = confidence_score.reshape(B, H, W) # shape (B, H, W)
    
    # Concatenate the scene flow and confidence score along the channel dimension
    scene_flow = torch.cat([scene_flow_x, scene_flow_y, scene_flow_z, confidence_score], dim=1) # shape (B, 4, H, W)
    
    return scene_flow

# Define pose estimation module
def pose_estimation(scene_flow):
    # scene_flow is a batch of scene flow maps of shape (B, 4, H, W)
    # Return a batch of relative camera poses of shape (B, 4, 4) and a batch of confidence scores of shape (B,)
    
    # Extract the scene flow vectors and confidence scores from the scene flow maps
    scene_flow_x = scene_flow[:, 0] # shape (B, H*W)
    scene_flow_y = scene_flow[:, 1] # shape (B, H*W)
    scene_flow_z = scene_flow[:, 2] # shape (B, H*W)
    confidence_score = scene_flow[:, 3] # shape (B, H*W)
    
    # Compute the spherical coordinates of the pixels in the first frame
    theta_1 = np.linspace(-np.pi/2, np.pi/2, H) # shape (H,)
    phi_1 = np.linspace(-np.pi, np.pi, W) # shape (W,)
    theta_1_grid, phi_1_grid = np.meshgrid(theta_1, phi_1) # shape (H, W)
    theta_1_grid = theta_1_grid.reshape(-1) # shape (H*W,)
    phi_1_grid = phi_1_grid.reshape(-1) # shape (H*W,)
    
    # Compute the cartesian coordinates of the pixels in the first frame on the spherical surface
    x_1_grid = sphere_radius * np.sin(theta_1_grid) * np.cos(phi_1_grid) # shape (H*W,)
    y_1_grid = sphere_radius * np.sin(theta_1_grid) * np.sin(phi_1_grid) # shape (H*W,)
    z_1_grid = sphere_radius * np.cos(theta_1_grid) # shape (H*W,)
    
    # Compute the cartesian coordinates of the pixels in the second frame using the scene flow vectors
    x_2_grid = x_1_grid + scene_flow_x # shape (B, H*W)
    y_2_grid = y_1_grid + scene_flow_y # shape (B, H*W)
    z_2_grid = z_1_grid + scene_flow_z # shape (B, H*W)
    
    # Stack the coordinates of the first and second frames along the channel dimension
    X_1 = torch.stack([x_1_grid, y_1_grid, z_1_grid], dim=2) # shape (B, H*W ,3)
    X_2 = torch.stack([x_2_grid ,y_2_grid ,z_2_grid], dim=2) # shape (B ,H*W ,3)
    
    # Compute the weighted mean of the coordinates using the confidence scores
    X_mean_1 = torch.sum(X_1 * confidence_score.unsqueeze(2), dim=1) /

```