---
title: 2103.03243v1 Anycost GANs for Interactive Image Synthesis and Editing
date: 2021-03-04
---

# [Anycost GANs for Interactive Image Synthesis and Editing](http://arxiv.org/abs/2103.03243v1)

authors: Ji Lin, Richard Zhang, Frieder Ganz, Song Han, Jun-Yan Zhu


## What, Why and How

[1]: https://arxiv.org/pdf/2103.03243 "arXiv:2103.03243v1 [cs.CV] 4 Mar 2021"
[2]: https://arxiv.org/abs/2103.03243v1 "Anycost GANs for Interactive Image Synthesis and Editing"
[3]: https://arxiv.org/abs/2103.14030 "[2103.14030] Swin Transformer: Hierarchical Vision Transformer using ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- What: The paper proposes **Anycost GAN**, a generative adversarial network that can support **elastic resolutions and channels** for faster image generation and editing at versatile speeds.
- Why: The paper aims to enable **interactive natural image editing** by providing perceptually similar previews at lower computational costs, while maintaining high-quality outputs at full costs. The paper takes inspirations from modern rendering software that offer quick preview features.
- How: The paper trains the Anycost GAN using **sampling-based multi-resolution training**, **adaptive-channel training**, and a **generator-conditioned discriminator**. The paper also develops new encoder training and latent code optimization techniques to encourage **consistency** between the different sub-generators during image projection. The paper evaluates the Anycost GAN on various image synthesis and editing tasks and shows that it can achieve up to 10x computation reduction and 6-12x speedup compared to existing models.


## Main Contributions

[1]: https://arxiv.org/pdf/2103.03243 "arXiv:2103.03243v1 [cs.CV] 4 Mar 2021"
[2]: https://arxiv.org/abs/2103.03243v1 "Anycost GANs for Interactive Image Synthesis and Editing"
[3]: https://arxiv.org/abs/2103.14030 "[2103.14030] Swin Transformer: Hierarchical Vision Transformer using ..."

The paper claims the following contributions:

- It proposes **Anycost GAN**, a novel generative adversarial network that can support **elastic resolutions and channels** for faster image generation and editing at versatile speeds.
- It introduces **sampling-based multi-resolution training**, **adaptive-channel training**, and a **generator-conditioned discriminator** to train the Anycost GAN and achieve better image quality compared to separately trained models.
- It develops new encoder training and latent code optimization techniques to encourage **consistency** between the different sub-generators during image projection, making them a good proxy for preview.
- It demonstrates that Anycost GAN can be executed at various cost budgets (up to 10x computation reduction) and adapt to a wide range of hardware and latency requirements, enabling interactive image editing on desktop CPUs and edge devices. It also shows that Anycost GAN can perform various image synthesis and editing tasks with high-quality outputs.


## Method Summary

[1]: https://arxiv.org/pdf/2103.03243 "arXiv:2103.03243v1 [cs.CV] 4 Mar 2021"
[2]: https://arxiv.org/abs/2103.03243v1 "Anycost GANs for Interactive Image Synthesis and Editing"
[3]: https://arxiv.org/abs/2103.14030 "[2103.14030] Swin Transformer: Hierarchical Vision Transformer using ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first introduces the **Anycost GAN** architecture, which consists of a generator and a discriminator. The generator is composed of multiple sub-generators with different resolutions and channels, and can be executed at different configurations by selecting a subset of sub-generators. The discriminator is conditioned on the generator configuration and can adapt to different input resolutions.
- The paper then describes the **sampling-based multi-resolution training** scheme, which randomly samples a resolution for each sub-generator during training and uses nearest-neighbor upsampling or average pooling to match the input and output resolutions. This scheme allows the sub-generators to learn from diverse resolutions and improves the image quality.
- The paper also presents the **adaptive-channel training** technique, which randomly drops out channels for each sub-generator during training and scales up the remaining channels accordingly. This technique enables the sub-generators to learn from diverse channel capacities and improves the robustness and efficiency.
- The paper further explains the **generator-conditioned discriminator** design, which takes the generator configuration as an additional input and uses conditional batch normalization to modulate the discriminator features. This design allows the discriminator to provide better feedback to the generator and improves the stability and performance.
- The paper then discusses the **encoder training** and **latent code optimization** methods for image projection, which aim to find a latent code that reconstructs a given natural image using Anycost GAN. The paper proposes to train an encoder network that maps an image to a latent code using perceptual loss and feature matching loss, and then refine the latent code using gradient-based optimization with consistency loss. The consistency loss encourages the latent code to produce similar outputs across different sub-generators, making them a good proxy for preview.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# Define the generator and discriminator networks
generator = Anycost_Generator()
discriminator = Generator_Conditioned_Discriminator()

# Define the loss functions
perceptual_loss = Perceptual_Loss()
feature_matching_loss = Feature_Matching_Loss()
consistency_loss = Consistency_Loss()
adversarial_loss = Adversarial_Loss()

# Define the optimizer
optimizer = Adam()

# Train the generator and discriminator
for epoch in range(num_epochs):
  for batch in data_loader:
    # Sample a generator configuration
    config = sample_generator_config()
    # Generate images from random latent codes
    z = sample_latent_codes(batch_size)
    fake_images = generator(z, config)
    # Compute the discriminator outputs for fake and real images
    fake_outputs = discriminator(fake_images, config)
    real_outputs = discriminator(batch, config)
    # Compute the generator and discriminator losses
    g_loss = adversarial_loss(fake_outputs, 1) # generator tries to fool discriminator
    d_loss = adversarial_loss(fake_outputs, 0) + adversarial_loss(real_outputs, 1) # discriminator tries to distinguish fake and real images
    # Update the generator and discriminator parameters
    optimizer.zero_grad()
    g_loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    d_loss.backward()
    optimizer.step()

# Train the encoder network for image projection
encoder = Encoder_Network()
for epoch in range(num_epochs):
  for batch in data_loader:
    # Encode the images to latent codes
    z = encoder(batch)
    # Generate images from latent codes using a random generator configuration
    config = sample_generator_config()
    recon_images = generator(z, config)
    # Compute the perceptual loss and feature matching loss
    p_loss = perceptual_loss(recon_images, batch)
    f_loss = feature_matching_loss(recon_images, batch, discriminator, config)
    # Update the encoder parameters
    optimizer.zero_grad()
    (p_loss + f_loss).backward()
    optimizer.step()

# Refine the latent codes using gradient-based optimization
for image in test_images:
  # Encode the image to a latent code
  z = encoder(image)
  # Optimize the latent code using a fixed generator configuration
  config = fixed_generator_config()
  for step in range(num_steps):
    # Generate an image from the latent code
    recon_image = generator(z, config)
    # Compute the perceptual loss, feature matching loss, and consistency loss
    p_loss = perceptual_loss(recon_image, image)
    f_loss = feature_matching_loss(recon_image, image, discriminator, config)
    c_loss = consistency_loss(z, generator) # compute the output difference across different sub-generators
    # Update the latent code
    optimizer.zero_grad()
    (p_loss + f_loss + c_loss).backward()
    optimizer.step()
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Define the generator network
class Anycost_Generator(nn.Module):
  def __init__(self):
    super().__init__()
    # Define the sub-generators with different resolutions and channels
    self.sub_generators = nn.ModuleList([
      Sub_Generator(4, 512), # 4x4 resolution with 512 channels
      Sub_Generator(8, 512), # 8x8 resolution with 512 channels
      Sub_Generator(16, 512), # 16x16 resolution with 512 channels
      Sub_Generator(32, 256), # 32x32 resolution with 256 channels
      Sub_Generator(64, 128), # 64x64 resolution with 128 channels
      Sub_Generator(128, 64), # 128x128 resolution with 64 channels
      Sub_Generator(256, 32), # 256x256 resolution with 32 channels
      Sub_Generator(512, 16), # 512x512 resolution with 16 channels
    ])
    # Define the output layer that converts the final feature map to RGB image
    self.to_rgb = nn.Conv2d(16, 3, kernel_size=1)

  def forward(self, z, config):
    # z: latent code of shape [batch_size, latent_dim]
    # config: generator configuration of shape [num_sub_generators]
    # returns: generated image of shape [batch_size, 3, output_resolution, output_resolution]
    
    # Initialize the feature map from the latent code
    x = self.sub_generators[0](z) # x shape: [batch_size, 512, 4, 4]
    
    # Loop over the sub-generators according to the config
    for i in range(1, len(self.sub_generators)):
      # Check if the current sub-generator is selected
      if config[i] == 1:
        # Upsample the feature map to match the sub-generator resolution
        x = F.interpolate(x, size=(2 ** (i + 2), 2 ** (i + 2)), mode='nearest')
        # Apply the sub-generator to the feature map
        x = self.sub_generators[i](x) # x shape: [batch_size, channels[i], 2 ** (i + 2), 2 ** (i + 2)]
    
    # Convert the feature map to RGB image
    x = self.to_rgb(x) # x shape: [batch_size, 3, output_resolution, output_resolution]
    
    return x

# Define the sub-generator network
class Sub_Generator(nn.Module):
  def __init__(self, resolution, channels):
    super().__init__()
    # resolution: the output resolution of the sub-generator
    # channels: the number of output channels of the sub-generator
    
    # Define the convolutional layers
    self.conv1 = nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1)
    self.conv2 = nn.Conv2d(channels // 2, channels // 4, kernel_size=3, padding=1)
    self.conv3 = nn.Conv2d(channels // 4, channels // 8, kernel_size=3, padding=1)
    
    # Define the residual blocks
    self.res_blocks = nn.ModuleList([
      Res_Block(channels // 8) for _ in range(int(math.log2(resolution) - 1))
    ])
    
    # Define the normalization layers
    self.norm1 = nn.BatchNorm2d(channels)
    self.norm2 = nn.BatchNorm2d(channels // 2)
    self.norm3 = nn.BatchNorm2d(channels // 4)
    
    # Define the activation function
    self.act = nn.ReLU()
  
  def forward(self, x):
    # x: input feature map of shape [batch_size, channels, resolution, resolution]
    
    # Apply the convolutional layers
    x = self.act(self.norm1(x))
    x = self.act(self.norm2(self.conv1(x)))
    x = self.act(self.norm3(self.conv2(x)))
    x = self.conv3(x) # x shape: [batch_size, channels // 8, resolution, resolution]
    
    # Apply the residual blocks
    for res_block in self.res_blocks:
      x = res_block(x)
    
    return x

# Define the residual block network
class Res_Block(nn.Module):
  def __init__(self, channels):
    super().__init__()
    # channels: the number of input and output channels of the residual block
    
    # Define the convolutional layers
    self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
    self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
    
    # Define the normalization layers
    self.norm1 = nn.BatchNorm2d(channels)
    self.norm2 = nn.BatchNorm2d(channels)
    
    # Define the activation function
    self.act = nn.ReLU()
  
  def forward(self, x):
    # x: input feature map of shape [batch_size, channels, resolution, resolution]
    
    # Save the input for residual connection
    identity = x
    
    # Apply the convolutional layers
    x = self.act(self.norm1(x))
    x = self.act(self.norm2(self.conv1(x)))
    x = self.conv2(x)
    
    # Add the residual connection
    x = x + identity
    
    return x

# Define the discriminator network
class Generator_Conditioned_Discriminator(nn.Module):
  def __init__(self):
    super().__init__()
    # Define the convolutional layers
    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)
    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)
    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
    self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
    
    # Define the conditional batch normalization layers
    self.cbn1 = Conditional_BatchNorm(16)
    self.cbn2 = Conditional_BatchNorm(32)
    self.cbn3 = Conditional_BatchNorm(64)
    self.cbn4 = Conditional_BatchNorm(128)
    self.cbn5 = Conditional_BatchNorm