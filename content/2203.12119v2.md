---
title: 2203.12119v2 Visual Prompt Tuning
date: 2022-03-13
---

# [Visual Prompt Tuning](http://arxiv.org/abs/2203.12119v2)

authors: Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim


## What, Why and How

[1]: https://arxiv.org/pdf/2203.12119v2.pdf "Visual Prompt Tuning arXiv:2203.12119v2 [cs.CV] 20 Jul 2022"
[2]: https://arxiv.org/abs/2203.12119 "[2203.12119] Visual Prompt Tuning - arXiv.org"
[3]: http://export.arxiv.org/pdf/2204.12119 "export.arxiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces Visual Prompt Tuning (VPT), a method for adapting large-scale Transformer models in vision to downstream tasks by adding trainable parameters in the input space while keeping the model backbone frozen.
- **Why**: The paper aims to address the challenges of full fine-tuning, which requires storing and deploying a separate copy of the backbone parameters for every task and may not be optimal for some tasks or data scales. VPT is inspired by recent advances in efficiently tuning large language models using prompts.
- **How**: The paper proposes to add a visual prompt layer before the Transformer backbone, which consists of a set of learnable tokens and a linear projection. The visual prompt layer can be seen as a task-specific input transformation that guides the model to focus on relevant features. The paper evaluates VPT on various recognition tasks, such as image classification, object detection, semantic segmentation, and instance segmentation. The paper shows that VPT achieves significant performance gains compared to other parameter efficient tuning protocols and even outperforms full fine-tuning in many cases.

## Main Contributions

According to the paper, the main contributions are:

- The paper introduces Visual Prompt Tuning (VPT), a novel and effective method for adapting large-scale Transformer models in vision to downstream tasks by adding trainable parameters in the input space while keeping the model backbone frozen.
- The paper provides a comprehensive analysis of VPT and its variants, such as different prompt initialization strategies, prompt token types, and prompt token positions.
- The paper demonstrates the effectiveness and efficiency of VPT on a wide range of downstream recognition tasks, such as image classification, object detection, semantic segmentation, and instance segmentation. The paper shows that VPT outperforms or matches full fine-tuning across model capacities and training data scales, while reducing per-task storage cost. The paper also shows that VPT can be combined with other parameter efficient tuning protocols to further improve performance.

## Method Summary

Here is a summary of the method section of the paper:

- The paper assumes a pre-trained Transformer model in vision, such as ViT [19] or DeiT [62], which consists of a patch embedding layer, a Transformer backbone, and a classification head. The paper proposes to add a visual prompt layer before the Transformer backbone, which consists of a set of learnable tokens and a linear projection. The visual prompt layer can be seen as a task-specific input transformation that guides the model to focus on relevant features.
- The paper defines the visual prompt layer as follows: given an input image x, it is first divided into non-overlapping patches and each patch is embedded into a d-dimensional vector using the patch embedding layer. Then, the visual prompt layer adds k learnable tokens to the patch embeddings and projects them to the same dimension d using a linear projection. The output of the visual prompt layer is then fed into the Transformer backbone, which is kept frozen during tuning. The classification head is replaced by a task-specific head, such as a linear classifier for image classification or an object detector for object detection.
- The paper discusses different choices for the visual prompt layer, such as the number and type of prompt tokens, the initialization strategy, and the position of the prompt tokens. The paper also compares VPT with other parameter efficient tuning protocols, such as partial head MLP [6], sidetune [27], backbone adapter [36], and bias prompt [34]. The paper shows that VPT has several advantages over these methods, such as being more flexible, more expressive, and more robust.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a pre-trained Transformer model in vision (patch_embedding, transformer_backbone, classification_head)
# Output: a tuned model for a downstream task (visual_prompt_layer, task_head)

# Define the visual prompt layer
visual_prompt_layer = VisualPromptLayer(num_tokens, token_type, projection_dim)

# Initialize the prompt tokens
visual_prompt_layer.init_tokens(init_strategy)

# Replace the classification head with a task-specific head
task_head = TaskHead(task_type)

# Freeze the transformer backbone
transformer_backbone.requires_grad = False

# Tune the model on the downstream task
for epoch in range(num_epochs):
  for batch in dataloader:
    # Get the input image and the label
    x, y = batch

    # Divide the image into patches and embed them
    x = patch_embedding(x)

    # Add the prompt tokens and project them
    x = visual_prompt_layer(x)

    # Feed the input into the transformer backbone
    x = transformer_backbone(x)

    # Feed the output into the task head
    y_pred = task_head(x)

    # Compute the loss and update the parameters
    loss = loss_function(y_pred, y)
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import timm

# Define the hyperparameters
num_tokens = 16 # the number of prompt tokens
token_type = "patch" # the type of prompt tokens (patch or class)
projection_dim = 768 # the dimension of the linear projection
init_strategy = "random" # the initialization strategy for prompt tokens
task_type = "classification" # the downstream task type
num_classes = 1000 # the number of classes for classification
num_epochs = 10 # the number of epochs for tuning
batch_size = 64 # the batch size for tuning
learning_rate = 1e-3 # the learning rate for tuning

# Load the pre-trained Transformer model in vision
model_name = "vit_base_patch16_224" # the name of the pre-trained model
model = timm.create_model(model_name, pretrained=True) # load the model using timm library

# Get the patch embedding layer, the transformer backbone, and the classification head from the model
patch_embedding = model.patch_embed # a convolutional layer that divides the image into patches and embeds them
transformer_backbone = model.blocks # a sequence of transformer blocks that process the patch embeddings
classification_head = model.head # a linear layer that predicts the class probabilities

# Define the visual prompt layer as a custom module
class VisualPromptLayer(torch.nn.Module):
  def __init__(self, num_tokens, token_type, projection_dim):
    super().__init__()
    self.num_tokens = num_tokens # store the number of prompt tokens
    self.token_type = token_type # store the type of prompt tokens
    self.projection_dim = projection_dim # store the dimension of the linear projection

    # Create a parameter tensor for the prompt tokens
    self.tokens = torch.nn.Parameter(torch.randn(num_tokens, projection_dim)) # initialize randomly

    # Create a linear layer for projecting the patch embeddings and prompt tokens to the same dimension
    self.projection = torch.nn.Linear(projection_dim, projection_dim)

  def forward(self, x):
    # x is a batch of patch embeddings with shape (batch_size, num_patches, projection_dim)

    # Add the prompt tokens to each batch element
    x = torch.cat([self.tokens.unsqueeze(0).repeat(x.size(0), 1, 1), x], dim=1) # shape: (batch_size, num_tokens + num_patches, projection_dim)

    # Project the patch embeddings and prompt tokens to the same dimension
    x = self.projection(x) # shape: (batch_size, num_tokens + num_patches, projection_dim)

    return x

# Create an instance of the visual prompt layer
visual_prompt_layer = VisualPromptLayer(num_tokens, token_type, projection_dim)

# Replace the classification head with a task-specific head
if task_type == "classification":
  task_head = torch.nn.Linear(projection_dim, num_classes) # a linear layer for classification
else:
  raise NotImplementedError("Other task types are not implemented yet")

# Freeze the transformer backbone
for param in transformer_backbone.parameters():
  param.requires_grad = False

# Move the model to device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
visual_prompt_layer.to(device)
task_head.to(device)

# Define the loss function and optimizer for tuning
loss_function = torch.nn.CrossEntropyLoss() # use cross entropy loss for classification
optimizer = torch.optim.Adam([{"params": visual_prompt_layer.parameters()}, {"params": task_head.parameters()}], lr=learning_rate) # use Adam optimizer for tuning

# Load the dataset for tuning (use ImageNet as an example)
train_dataset = torchvision.datasets.ImageNet(root="data", split="train", transform=torchvision.transforms.ToTensor()) # load the train split of ImageNet and convert images to tensors
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # create a data loader for train split

# Tune the model on the downstream task
for epoch in range(num_epochs):
  for batch in train_dataloader:
    # Get the input image and the label
    x, y = batch

    # Move the input and label to device
    x = x.to(device)
    y = y.to(device)

    # Divide the image into patches and embed them
    x = patch_embedding(x) # shape: (batch_size, num_patches, projection_dim)

    # Add the prompt tokens and project them
    x = visual_prompt_layer(x) # shape: (batch_size, num_tokens + num_patches, projection_dim)

    # Feed the input into the transformer backbone
    x = transformer_backbone(x) # shape: (batch_size, num_tokens + num_patches, projection_dim)

    # Get the output of the last token (the class token)
    x = x[:, 0] # shape: (batch_size, projection_dim)

    # Feed the output into the task head
    y_pred = task_head(x) # shape: (batch_size, num_classes)

    # Compute the loss and update the parameters
    loss = loss_function(y_pred, y)
    loss.backward()
    optimizer.step()

    # Print the loss
    print(f"Epoch {epoch}, Loss {loss.item()}")
```