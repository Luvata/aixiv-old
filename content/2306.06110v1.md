---
title: 2306.06110v1 Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings
date: 2023-06-07
---

# [Surrogate Modeling of Car Drag Coefficient with Depth and Normal Renderings](http://arxiv.org/abs/2306.06110v1)

authors: Binyang Song, Chenyang Yuan, Frank Permenter, Nikos Arechiga, Faez Ahmed


## What, Why and How

[1]: https://arxiv.org/pdf/2306.06110 "A template for the arxiv style"
[2]: https://arxiv.org/abs/2306.06110 "[2306.06110] Surrogate Modeling of Car Drag Coefficient with Depth and ..."
[3]: http://export.arxiv.org/abs/2306.06110 "[2306.06110] Surrogate Modeling of Car Drag Coefficient with Depth and ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new two-dimensional (2D) representation of three-dimensional (3D) shapes based on depth and normal renderings, and develops a surrogate drag model based on this representation to predict 3D car drag coefficients.
- **Why**: The paper aims to make generative AI models performance-aware and enable them to create high-performing car designs, which requires surrogate modeling of engineering metrics such as drag coefficient. However, the currently used representations of 3D shapes either require extensive computational resources to learn or suffer from significant information loss, which impairs their effectiveness in surrogate modeling.
- **How**: The paper constructs a diverse dataset of 9,070 high-quality 3D car meshes labeled by drag coefficients computed from computational fluid dynamics (CFD) simulations, and trains a deep neural network to map the 2D depth and normal renderings of the car shapes to their drag coefficients. The paper evaluates the accuracy and efficiency of the proposed model on various car categories, and demonstrates its compatibility with recent AI image generation tools (such as Stable Diffusion).


## Main Contributions

According to the paper, the main contributions are:

- A new 2D representation of 3D shapes based on depth and normal renderings, which preserves more geometric information than existing methods and requires less computational resources to learn.
- A surrogate drag model based on the proposed representation, which can accurately and efficiently predict 3D car drag coefficients from 2D images.
- A large-scale dataset of 3D car meshes with drag coefficients computed from CFD simulations, which can facilitate future research on performance-aware generative design of cars and other products.
- A demonstration of the compatibility of the proposed model with recent AI image generation tools, which enables the automatic generation of drag-optimized car designs.


## Method Summary

Here is a summary of the method section of the paper:

- The paper introduces the concept of depth and normal renderings, which are 2D images that encode the distance and orientation of each pixel on the surface of a 3D shape.
- The paper describes how to generate depth and normal renderings from 3D car meshes using Blender, a free and open-source 3D creation software.
- The paper explains how to train a deep neural network to map the depth and normal renderings of a car shape to its drag coefficient, using a dataset of 9,070 car meshes with drag labels obtained from CFD simulations.
- The paper details the network architecture, loss function, optimization algorithm, and hyperparameters used for training the surrogate drag model.
- The paper also discusses how to use the trained model in conjunction with recent AI image generation tools (such as Stable Diffusion) to generate novel car shapes with low drag coefficients.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the network architecture
network = ConvolutionalNeuralNetwork(input_size=256x256x6, output_size=1)

# Define the loss function
loss = MeanSquaredError()

# Define the optimization algorithm
optimizer = Adam(learning_rate=0.001)

# Load the dataset of car meshes and drag coefficients
dataset = load_dataset("car_meshes_and_drag_coefficients.csv")

# Generate depth and normal renderings for each car mesh
for car_mesh in dataset:
  depth_rendering = blender.render_depth(car_mesh)
  normal_rendering = blender.render_normal(car_mesh)
  car_image = concatenate(depth_rendering, normal_rendering)

# Train the network on the dataset
for epoch in range(100):
  for car_image, drag_coefficient in dataset:
    # Forward pass
    predicted_drag_coefficient = network(car_image)
    # Compute loss
    current_loss = loss(predicted_drag_coefficient, drag_coefficient)
    # Backward pass
    network.backward(current_loss)
    # Update parameters
    optimizer.step(network.parameters)

# Use the trained network to predict drag coefficients for new car images
for new_car_image in test_set:
  predicted_drag_coefficient = network(new_car_image)
  print(predicted_drag_coefficient)

# Use the trained network in conjunction with Stable Diffusion to generate novel car images with low drag coefficients
for target_drag_coefficient in desired_range:
  # Initialize a random car image
  car_image = random_noise()
  # Apply Stable Diffusion to refine the car image
  for diffusion_step in range(1000):
    car_image = stable_diffusion(car_image)
    # Apply the network to predict the drag coefficient for the current car image
    predicted_drag_coefficient = network(car_image)
    # Adjust the car image to reduce the drag coefficient
    if predicted_drag_coefficient > target_drag_coefficient:
      car_image = car_image - gradient(predicted_drag_coefficient, car_image)
  # Save the generated car image with low drag coefficient
  save_image(car_image)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import blender
import stable_diffusion

# Define the network architecture
class ConvolutionalNeuralNetwork(nn.Module):
  def __init__(self, input_size, output_size):
    super(ConvolutionalNeuralNetwork, self).__init__()
    # Input size is 256x256x6 (depth and normal renderings concatenated along the channel dimension)
    # Output size is 1 (drag coefficient)
    # Use four convolutional layers with ReLU activation and batch normalization
    # Use max pooling after the first two convolutional layers
    # Use a fully connected layer at the end with linear activation
    self.conv1 = nn.Conv2d(in_channels=6, out_channels=32, kernel_size=3, padding=1)
    self.bn1 = nn.BatchNorm2d(32)
    self.relu1 = nn.ReLU()
    self.pool1 = nn.MaxPool2d(2)
    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)
    self.bn2 = nn.BatchNorm2d(64)
    self.relu2 = nn.ReLU()
    self.pool2 = nn.MaxPool2d(2)
    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
    self.bn3 = nn.BatchNorm2d(128)
    self.relu3 = nn.ReLU()
    self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)
    self.bn4 = nn.BatchNorm2d(256)
    self.relu4 = nn.ReLU()
    self.fc = nn.Linear(in_features=256*64*64, out_features=output_size)

  def forward(self, x):
    # Forward pass of the network
    # x is a tensor of shape (batch_size, 6, 256, 256)
    x = self.conv1(x) # shape: (batch_size, 32, 256, 256)
    x = self.bn1(x) # shape: (batch_size, 32, 256, 256)
    x = self.relu1(x) # shape: (batch_size, 32, 256, 256)
    x = self.pool1(x) # shape: (batch_size, 32, 128, 128)
    x = self.conv2(x) # shape: (batch_size, 64, 128, 128)
    x = self.bn2(x) # shape: (batch_size, 64, 128, 128)
    x = self.relu2(x) # shape: (batch_size, 64, 128, 128)
    x = self.pool2(x) # shape: (batch_size, 64, 64, 64)
    x = self.conv3(x) # shape: (batch_size, 128, 64, 64)
    x = self.bn3(x) # shape: (batch_size, 128, 64, 64)
    x = self.relu3(x) # shape: (batch_size, 128, 64, 64)
    x = self.conv4(x) # shape: (batch_size, 256, 64 ,64)
    x = self.bn4(x) # shape: (batch_size ,256 ,64 ,64)
    x = self.relu4(x) # shape: (batch_size ,256 ,64 ,64)
    x = x.view(-1 ,256*64*64) # flatten the tensor to feed into the fully connected layer
                             # shape: (batch_size ,256*64*64)
    x = self.fc(x) # shape: (batch_size ,1)

# Define the loss function
loss = nn.MSELoss() # mean squared error loss

# Define the optimization algorithm
optimizer = optim.Adam(network.parameters(), lr=0.001) # Adam optimizer with learning rate of 0.001

# Load the dataset of car meshes and drag coefficients
dataset = pd.read_csv("car_meshes_and_drag_coefficients.csv") 
# The dataset is a csv file with two columns: car_mesh and drag_coefficient
# car_mesh is the file name of the car mesh stored as a .obj file
# drag_coefficient is a float value representing the drag coefficient of the car mesh

# Generate depth and normal renderings for each car mesh
for index, row in dataset.iterrows():
  car_mesh = row["car_mesh"] # get the file name of the car mesh
  depth_rendering = blender.render_depth(car_mesh) # use blender to render the depth image of the car mesh
  normal_rendering = blender.render_normal(car_mesh) # use blender to render the normal image of the car mesh
  car_image = np.concatenate((depth_rendering, normal_rendering), axis=2) # concatenate the depth and normal images along the channel dimension
  car_image = torch.from_numpy(car_image) # convert the numpy array to a torch tensor
  dataset.loc[index, "car_image"] = car_image # store the car image tensor in the dataset

# Train the network on the dataset
for epoch in range(100): # train for 100 epochs
  for index, row in dataset.iterrows(): # iterate over each row in the dataset
    car_image = row["car_image"] # get the car image tensor
    drag_coefficient = row["drag_coefficient"] # get the drag coefficient value
    drag_coefficient = torch.tensor(drag_coefficient) # convert the float value to a torch tensor
    # Forward pass
    predicted_drag_coefficient = network(car_image) # feed the car image to the network and get the predicted drag coefficient
    # Compute loss
    current_loss = loss(predicted_drag_coefficient, drag_coefficient) # compute the mean squared error loss between the predicted and true drag coefficients
    # Backward pass
    network.zero_grad() # clear the gradients of the network parameters
    current_loss.backward() # compute the gradients of the network parameters with respect to the loss
    # Update parameters
    optimizer.step() # update the network parameters using the Adam optimizer

# Use the trained network to predict drag coefficients for new car images
for new_car_mesh in test_set: # iterate over each new car mesh in the test set
  new_depth_rendering = blender.render_depth(new_car_mesh) # use blender to render the depth image of the new car mesh
  new_normal_rendering = blender.render_normal(new_car_mesh) # use blender to render the normal image of the new car mesh
  new_car_image = np.concatenate((new_depth_rendering, new_normal_rendering), axis=2) # concatenate the depth and normal images along the channel dimension
  new_car_image = torch.from_numpy(new_car_image) # convert the numpy array to a torch tensor
  predicted_drag_coefficient = network(new_car_image) # feed the new car image to the network and get the predicted drag coefficient
  print(predicted_drag_coefficient) # print the predicted drag coefficient

# Use the trained network in conjunction with Stable Diffusion to generate novel car images with low drag coefficients
for target_drag_coefficient in desired_range: # iterate over each target drag coefficient in a desired range (e.g. 0.2 to 0.4)
  # Initialize a random car image
  car_image = torch.randn(6, 256, 256) # generate a random noise tensor of shape (6, 256, 256)
  # Apply Stable Diffusion to refine the car image
  for diffusion_step in range(1000): # apply Stable Diffusion for 1000 steps
    car_image = stable_diffusion(car_image) # use stable_diffusion function to refine the car image using Stable Diffusion algorithm (see Nichol et al. [2021] for details)
    # Apply the network to predict the drag coefficient for the current car image
    predicted_drag_coefficient = network(car_image) # feed the current car image to the network and get the predicted drag coefficient
    # Adjust the car image to reduce the drag coefficient
    if predicted_drag_coefficient > target_drag_coefficient: # if the predicted drag coefficient is higher than the target drag coefficient
      gradient = torch.autograd.grad(predicted_drag_coefficient, car_image)[0] # compute the gradient of the predicted drag coefficient with respect to the car image using PyTorch autograd function 
      car_image = car_image - gradient * 0.01 # subtract a small fraction of the gradient from the car image to reduce its drag coefficient (gradient descent)
  # Save the generated car image with low drag coefficient
  save_image(car_image) # save or display or return or do whatever you want with this generated low-drag car image

```