---
title: 2009.02216v1 SketchPatch  Sketch Stylization via Seamless Patch-level Synthesis
date: 2020-09-03
---

# [SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis](http://arxiv.org/abs/2009.02216v1)

authors: Noa Fish, Lilach Perry, Amit Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2009.02216 "SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis"
[2]: https://arxiv.org/pdf/2009.02216.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2009.02216 "[2009.02216] SketchPatch: Sketch Stylization via Seamless Patch-level ..."

Here is a summary of the paper:

- **What**: The paper proposes a method for sketch stylization via seamless patch-level synthesis, using image-to-image translation and a small set of stylized sketches as reference.
- **Why**: The paper aims to address the challenge of sketch stylization, which is the process of adding geometric textural details to plain sketches, such as hatching, stippling, or cross-hatching. Existing methods either require large amounts of paired data, which are scarce and expensive to obtain, or produce inconsistent and unnatural results across different sketches.
- **How**: The paper leverages the idea that sketches can be decomposed into patches containing basic geometric primitives, such as lines, curves, or circles. The paper uses a handful of stylized sketches as a source of patches, and trains a seamless translation (ST) generator to transfer the style from the patches to plain sketches. The ST generator takes as input a hybrid patch that combines a plain patch with a styled border, and outputs a fully styled patch that matches the border style. The paper also uses an adversarial loss to encourage the ST generator to produce realistic and diverse stylized patches. The paper demonstrates the effectiveness of the method on various styles and sketches, and compares it with existing methods.

## Main Contributions

The paper claims to make the following contributions:

- A novel method for sketch stylization via seamless patch-level synthesis, which can transfer geometric textural details from a small set of stylized sketches to plain sketches.
- A simple and effective way to create hybrid patches that combine plain and styled elements, which serve as the input for the seamless translation (ST) generator.
- A seamless translation (ST) generator that can produce fully styled patches that match the border style and are realistic and diverse.
- A comprehensive evaluation of the proposed method on various styles and sketches, and a comparison with existing methods.

## Method Summary

[1]: https://arxiv.org/abs/2009.02216 "SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis"
[2]: https://arxiv.org/pdf/2009.02216.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export3.library.cornell.edu/pdf/1912.02216 "Electrons in the Young Solar Wind: First Results from the Parker Solar ..."

Here is a summary of the method section of the paper:

- The paper describes the main components of the proposed method, which are: **patch extraction**, **hybrid patch creation**, **seamless translation (ST) generator**, and **adversarial loss**.
- **Patch extraction**: The paper extracts patches of size 64x64 pixels from a small set of stylized sketches, which serve as the source of style transfer. The paper also extracts patches from plain sketches, which serve as the target of style transfer. The paper assumes that both stylized and plain sketches are composed of basic geometric primitives, such as lines, curves, or circles, and that there is a rough alignment between them.
- **Hybrid patch creation**: The paper creates hybrid patches by combining a plain patch with a styled border, which are used as the input for the ST generator. The paper uses a simple algorithm to find the best matching styled patch for a given plain patch, based on the distance between their edge maps. The paper then blends the two patches using a Gaussian mask, such that the center of the hybrid patch is plain and the border is styled.
- **Seamless translation (ST) generator**: The paper trains a ST generator to produce fully styled patches from hybrid patches, using a U-Net architecture with skip connections. The paper uses a reconstruction loss to ensure that the output patch matches the border style of the input patch, and a perceptual loss to encourage the output patch to have similar features as the styled patch. The paper also uses an identity loss to prevent style transfer when the input patch is already fully styled.
- **Adversarial loss**: The paper adds an adversarial loss to improve the realism and diversity of the output patches, using a PatchGAN discriminator that classifies patches as real or fake. The paper also uses a feature matching loss to reduce mode collapse and stabilize training. The paper trains the ST generator and the discriminator alternately using Adam optimizer.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a set of stylized sketches S and a plain sketch P
# Output: a stylized sketch P'

# Extract patches from S and P
S_patches = extract_patches(S)
P_patches = extract_patches(P)

# Initialize ST generator G and discriminator D
G = UNet()
D = PatchGAN()

# Train G and D using hybrid patches and adversarial loss
for epoch in epochs:
  for p in P_patches:
    # Find the best matching styled patch s for p
    s = find_best_match(p, S_patches)
    # Create a hybrid patch h by blending p and s
    h = create_hybrid(p, s)
    # Generate a styled patch p' using G
    p' = G(h)
    # Compute reconstruction loss L_rec, perceptual loss L_per, identity loss L_id
    L_rec = MSE(p'.border, s.border)
    L_per = VGG(p', s)
    L_id = MSE(G(s), s)
    # Compute adversarial loss L_adv and feature matching loss L_fm using D
    L_adv = BCE(D(p'), 1) + BCE(D(s), 0)
    L_fm = MSE(D(p').features, D(s).features)
    # Update G using the total loss L_G
    L_G = L_rec + L_per + L_id + L_adv + L_fm
    G.backward(L_G)
    G.update()
    # Update D using the adversarial loss L_D
    L_D = BCE(D(p'), 0) + BCE(D(s), 1)
    D.backward(L_D)
    D.update()

# Stylize P using G
P' = empty_sketch()
for p in P_patches:
  # Find the best matching styled patch s for p
  s = find_best_match(p, S_patches)
  # Create a hybrid patch h by blending p and s
  h = create_hybrid(p, s)
  # Generate a styled patch p' using G
  p' = G(h)
  # Paste p' to P' at the same location as p
  P'.paste(p', p.location)

# Return P'
return P'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

# Define constants
PATCH_SIZE = 64 # patch size in pixels
PATCH_STRIDE = 32 # patch stride in pixels
BORDER_SIZE = 16 # border size in pixels
MASK_SIGMA = 5 # Gaussian mask sigma in pixels
LAMBDA_REC = 10 # weight for reconstruction loss
LAMBDA_PER = 1 # weight for perceptual loss
LAMBDA_ID = 1 # weight for identity loss
LAMBDA_ADV = 0.01 # weight for adversarial loss
LAMBDA_FM = 10 # weight for feature matching loss
EPOCHS = 100 # number of training epochs
BATCH_SIZE = 16 # batch size for training
LR_G = 0.0002 # learning rate for generator
LR_D = 0.0002 # learning rate for discriminator
BETA1 = 0.5 # beta1 for Adam optimizer
BETA2 = 0.999 # beta2 for Adam optimizer

# Define helper functions

# Extract patches from a sketch using a sliding window
def extract_patches(sketch):
  patches = []
  height, width = sketch.shape[:2]
  for i in range(0, height - PATCH_SIZE + 1, PATCH_STRIDE):
    for j in range(0, width - PATCH_SIZE + 1, PATCH_STRIDE):
      patch = sketch[i:i+PATCH_SIZE, j:j+PATCH_SIZE]
      patches.append(patch)
  return patches

# Find the best matching styled patch for a plain patch based on edge distance
def find_best_match(plain_patch, styled_patches):
  best_match = None
  best_dist = float('inf')
  plain_edge = cv2.Canny(plain_patch, 100, 200) # compute edge map of plain patch
  for styled_patch in styled_patches:
    styled_edge = cv2.Canny(styled_patch, 100, 200) # compute edge map of styled patch
    dist = np.sum(np.abs(plain_edge - styled_edge)) # compute L1 distance between edge maps
    if dist < best_dist:
      best_dist = dist
      best_match = styled_patch
  return best_match

# Create a hybrid patch by blending a plain patch and a styled border using a Gaussian mask
def create_hybrid(plain_patch, styled_patch):
  mask = np.zeros((PATCH_SIZE, PATCH_SIZE)) # create a mask of zeros
  mask[BORDER_SIZE:-BORDER_SIZE, BORDER_SIZE:-BORDER_SIZE] = 1 # set the center of the mask to ones
  mask = cv2.GaussianBlur(mask, (BORDER_SIZE*2+1, BORDER_SIZE*2+1), MASK_SIGMA) # blur the mask with a Gaussian kernel
  mask = np.expand_dims(mask, axis=2) # add a channel dimension to the mask
  hybrid_patch = plain_patch * mask + styled_patch * (1 - mask) # blend the patches using the mask
  return hybrid_patch

# Define network architectures

# Define the U-Net generator with skip connections
class UNet(nn.Module):
  def __init__(self):
    super(UNet, self).__init__()
    # Define the encoder layers
    self.enc1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1) # output size: (batch_size, 64, PATCH_SIZE/2, PATCH_SIZE/2)
    self.enc2 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 128, PATCH_SIZE/4, PATCH_SIZE/4)
      nn.BatchNorm2d(128)
    )
    self.enc3 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 256, PATCH_SIZE/8, PATCH_SIZE/8)
      nn.BatchNorm2d(256)
    )
    self.enc4 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/16, PATCH_SIZE/16)
      nn.BatchNorm2d(512)
    )
    self.enc5 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/32, PATCH_SIZE/32)
      nn.BatchNorm2d(512)
    )
    self.enc6 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/64, PATCH_SIZE/64)
      nn.BatchNorm2d(512)
    )
    self.enc7 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/128, PATCH_SIZE/128)
      nn.BatchNorm2d(512)
    )
    self.enc8 = nn.Sequential(
      nn.LeakyReLU(0.2),
      nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1) # output size: (batch_size, 512, PATCH_SIZE/256, PATCH_SIZE/256)
    )
    # Define the decoder layers
    self.dec1 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/128, PATCH_SIZE/128)
      nn.BatchNorm2d(512),
      nn.Dropout(0.5)
    )
    self.dec2 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/64, PATCH_SIZE/64)
      nn.BatchNorm2d(512),
      nn.Dropout(0.5)
    )
    self.dec3 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 512, PATCH_SIZE/32, PATCH_SIZE/32)
      nn.BatchNorm2d(512),
      nn.Dropout(0.5)
    )
    self.dec4 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 256, PATCH_SIZE/16, PATCH_SIZE/16)
      nn.BatchNorm2d(256)
    )
    self.dec5 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(768, 128, kernel_size=4, stride=2, padding=1), # output size: (batch_size, 128, PATCH_SIZE/8 ,PATCH_SIZE/8)
      nn.BatchNorm2d(128)
    )
    self.dec6 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(384 ,64 ,kernel_size=4 ,stride=2 ,padding=1), # output size: (batch_size ,64 ,PATCH_SIZE/4 ,PATCH_SIZE/4)
      nn.BatchNorm2d(64)
    )
    self.dec7 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(192 ,3 ,kernel_size=4 ,stride=2 ,padding=1), # output size: (batch_size ,3 ,PATCH_SIZE/2 ,PATCH_SIZE/2)
      nn.BatchNorm2d(3)
    )
    self.dec8 = nn.Sequential(
      nn.ReLU(),
      nn.ConvTranspose2d(96 ,3 ,kernel_size=4 ,stride=2 ,padding=1), # output size: (batch_size ,3 ,PATCH_SIZE ,PATCH_SIZE)
      torch.tanh()
    )

  def forward(self,x):
    # Encode x
    e1 = self.enc1(x) # e1 shape: (batch_size ,64 ,PATCH_SIZE/2 ,PATCH_SIZE/2)
    e2 = self.enc2(e1) # e2 shape: (batch_size ,128 ,PATCH_SIZE/4 ,PATCH_SIZE/4)
    e3 = self.enc3(e2) # e3 shape: (batch_size ,256 ,PATCH_SIZE/8 ,PATCH_SIZE/8)
    e4 = self.enc4(e3) # e4 shape: (batch_size ,512 ,PATCH_SIZE/16 ,PATCH_SIZE/16)
    e5 = self.enc5(e4) # e5 shape: (batch_size ,512 ,PATCH_SIZE