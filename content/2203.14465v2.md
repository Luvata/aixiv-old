---
title: 2203.14465v2 STaR  Bootstrapping Reasoning With Reasoning
date: 2022-03-15
---

# [STaR: Bootstrapping Reasoning With Reasoning](http://arxiv.org/abs/2203.14465v2)

authors: Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D. Goodman


## What, Why and How

[1]: https://arxiv.org/abs/2203.14465 "[2203.14465] STaR: Bootstrapping Reasoning With Reasoning - arXiv.org"
[2]: https://arxiv.org/pdf/2203.14465v2.pdf "Bootstrapping Reasoning With Reasoning - arXiv.org"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the paper at [^1^][2] in terms of What, Why, and How:

- **What**: The paper proposes a technique called STaR (Self-Taught Reasoner) that can bootstrap the ability of a language model to perform complex reasoning by generating and learning from its own rationales.
- **Why**: The paper argues that generating step-by-step rationales can improve language model performance on tasks like mathematics or commonsense question-answering, but existing methods for inducing rationale generation either require large datasets of rationales or sacrifice accuracy by using only few-shot inference.
- **How**: The paper describes a simple loop that STaR follows: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. The paper shows that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30 times larger state-of-the-art language model on CommensenseQA.

## Main Contributions

[1]: https://arxiv.org/abs/2203.14465 "[2203.14465] STaR: Bootstrapping Reasoning With Reasoning - arXiv.org"
[2]: https://arxiv.org/pdf/2203.14465v2.pdf "Bootstrapping Reasoning With Reasoning - arXiv.org"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

The paper lists the following contributions at the end of the introduction section[^1^][2]:

- **A novel technique for bootstrapping reasoning with reasoning**, called STaR, that can iteratively leverage a small number of rationale examples and a large dataset without rationales to improve language model performance on complex reasoning tasks.
- **Empirical evidence that STaR improves performance on multiple datasets** compared to a model fine-tuned to directly predict final answers, including arithmetic, symbolic algebra, and commonsense question-answering.
- **A demonstration that STaR can perform comparably to fine-tuning a 30 times larger state-of-the-art language model** on CommensenseQA, while using only a fraction of the compute resources.

## Method Summary

[1]: https://arxiv.org/abs/2203.14465 "[2203.14465] STaR: Bootstrapping Reasoning With Reasoning - arXiv.org"
[2]: https://arxiv.org/pdf/2203.14465v2.pdf "Bootstrapping Reasoning With Reasoning - arXiv.org"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the method section of the paper at [^1^][2]:

- The paper describes the STaR technique in detail, which consists of three main components: a **rationale generator**, a **reasoning evaluator**, and a **reasoning learner**.
- The **rationale generator** is a language model that takes as input a question and a few rationale examples, and outputs a rationale for answering the question. The rationale is a sequence of natural language sentences that explain the reasoning steps leading to the final answer. The rationale generator can also take as input the correct answer, in which case it tries to generate a rationale that is consistent with the answer.
- The **reasoning evaluator** is a classifier that takes as input a question, an answer, and a rationale, and outputs a binary label indicating whether the answer is correct or not. The reasoning evaluator can be trained on any dataset that has questions and answers, without requiring rationales.
- The **reasoning learner** is a fine-tuning procedure that updates the parameters of the rationale generator based on the feedback from the reasoning evaluator. The reasoning learner uses two types of data: **rationale data**, which consists of questions, answers, and rationales; and **answer data**, which consists of questions and answers only. The reasoning learner fine-tunes the rationale generator on both types of data, but only on those instances where the reasoning evaluator agrees with the ground truth labels. This way, the reasoning learner can filter out noisy or incorrect rationales generated by the rationale generator.
- The paper also describes how STaR can be applied to different datasets and tasks, such as arithmetic, symbolic algebra, and commonsense question-answering. The paper provides details on how to construct rationale examples, how to format the input and output for the rationale generator, and how to train and evaluate the reasoning evaluator for each task.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Initialize the rationale generator G and the reasoning evaluator E
G = load_pretrained_language_model()
E = train_classifier_on_answer_data()

# Loop until convergence or maximum iterations
while not converged and iterations < max_iterations:

  # Generate rationales for a batch of questions from answer data
  questions = sample_batch_from_answer_data()
  rationale_examples = sample_rationale_examples()
  rationales, answers = G.generate_rationales(questions, rationale_examples)

  # Evaluate the generated answers and rationales
  labels = E.evaluate_reasoning(questions, answers, rationales)

  # Filter out the instances where the labels disagree with the ground truth
  rationale_data = filter_by_labels(questions, answers, rationales, labels)

  # Fine-tune the rationale generator on the filtered rationale data
  G.fine_tune_on_rationale_data(rationale_data)

  # Optionally, generate rationales for a batch of questions from rationale data
  questions, answers = sample_batch_from_rationale_data()
  rationale_examples = sample_rationale_examples()
  rationales = G.generate_rationales(questions, rationale_examples, answers)

  # Evaluate the generated rationales
  labels = E.evaluate_reasoning(questions, answers, rationales)

  # Filter out the instances where the labels disagree with the ground truth
  rationale_data = filter_by_labels(questions, answers, rationales, labels)

  # Fine-tune the rationale generator on the filtered rationale data
  G.fine_tune_on_rationale_data(rationale_data)

  # Update the convergence criterion and the iteration counter
  converged = check_convergence(G)
  iterations += 1

# Return the final rationale generator
return G
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import transformers
import sklearn

# Define some hyperparameters
batch_size = 32
max_iterations = 10
learning_rate = 1e-4
convergence_threshold = 0.01

# Load the answer data and the rationale data
answer_data = load_answer_data()
rationale_data = load_rationale_data()

# Load the pretrained language model and the tokenizer
model_name = "gpt2"
model = transformers.AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

# Define the rationale generator class
class RationaleGenerator:

  def __init__(self, model, tokenizer):
    self.model = model
    self.tokenizer = tokenizer

  def generate_rationales(self, questions, rationale_examples, answers=None):
    # Concatenate the questions, the rationale examples, and optionally the answers
    inputs = []
    for i in range(len(questions)):
      input = questions[i] + "\n\n"
      input += "Rationale examples:\n"
      for example in rationale_examples[i]:
        input += example + "\n"
      if answers is not None:
        input += "\nAnswer: " + answers[i] + "\n\n"
      input += "Rationale:\n"
      inputs.append(input)

    # Tokenize and encode the inputs
    inputs = self.tokenizer(inputs, return_tensors="pt", padding=True)

    # Generate the outputs using beam search
    outputs = self.model.generate(inputs["input_ids"], num_beams=5)

    # Decode and split the outputs into rationales and answers
    outputs = self.tokenizer.batch_decode(outputs)
    rationales = []
    answers = []
    for output in outputs:
      output = output.split("\n")
      rationale = output[-1].replace("Rationale: ", "")
      answer = output[-3].replace("Answer: ", "")
      rationales.append(rationale)
      answers.append(answer)

    # Return the rationales and answers
    return rationales, answers

  def fine_tune_on_rationale_data(self, rationale_data):
    # Shuffle and split the rationale data into batches
    rationale_data = sklearn.utils.shuffle(rationale_data)
    batches = torch.utils.data.DataLoader(rationale_data, batch_size=batch_size)

    # Define the optimizer and the loss function
    optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
    loss_fn = torch.nn.CrossEntropyLoss()

    # Loop over the batches
    for batch in batches:
      # Extract the questions, answers, and rationales from the batch
      questions, answers, rationales = batch

      # Concatenate the questions and the rationales
      inputs = []
      for i in range(len(questions)):
        input = questions[i] + "\n\n"
        input += "Rationale:\n" + rationales[i] + "\n\n"
        inputs.append(input)

      # Tokenize and encode the inputs and the answers
      inputs = self.tokenizer(inputs, return_tensors="pt", padding=True)
      answers = self.tokenizer(answers, return_tensors="pt", padding=True)

      # Forward pass through the model
      outputs = self.model(inputs["input_ids"], labels=answers["input_ids"])

      # Compute the loss and the gradients
      loss = outputs.loss
      loss.backward()

      # Update the model parameters
      optimizer.step()
      optimizer.zero_grad()

# Define the reasoning evaluator class
class ReasoningEvaluator:

  def __init__(self):
    self.model = None

  def train_classifier_on_answer_data(self):
    # Extract the features and labels from the answer data
    features = answer_data["question"] + " " + answer_data["answer"]
    labels = answer_data["label"]

    # Vectorize the features using TF-IDF
    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()
    features = vectorizer.fit_transform(features)

    # Train a logistic regression classifier on the features and labels
    classifier = sklearn.linear_model.LogisticRegression()
    classifier.fit(features, labels)

    # Save the vectorizer and the classifier as attributes of the evaluator
    self.vectorizer = vectorizer
    self.classifier = classifier

  def evaluate_reasoning(self, questions, answers, rationales):
    # Concatenate the questions, answers, and rationales
    features = []
    for i in range(len(questions)):
      feature = questions[i] + " " + answers[i] + " " + rationales[i]
      features.append(feature)

    # Vectorize the features using the saved vectorizer
    features = self.vectorizer.transform(features)

    # Predict the labels using the saved classifier
    labels = self.classifier.predict(features)

    # Return the labels
    return labels

# Initialize the rationale generator and the reasoning evaluator
G = RationaleGenerator(model, tokenizer)
E = ReasoningEvaluator()
E.train_classifier_on_answer_data()

# Initialize the convergence criterion and the iteration counter
converged = False
iterations = 0

# Loop until convergence or maximum iterations
while not converged and iterations < max_iterations:

  # Generate rationales for a batch of questions from answer data
  questions, answers, labels = sample_batch_from_answer_data(answer_data)
  rationale_examples = sample_rationale_examples(rationale_data)
  rationales, answers = G.generate_rationales(questions, rationale_examples)

  # Evaluate the generated answers and rationales
  labels = E.evaluate_reasoning(questions, answers, rationales)

  # Filter out the instances where the labels disagree with the ground truth
  rationale_data = filter_by_labels(questions, answers, rationales, labels)

  # Fine-tune the rationale generator on the filtered rationale data
  G.fine_tune_on_rationale_data(rationale_data)

  # Optionally, generate rationales for a batch of questions from rationale data
  questions, answers, labels = sample_batch_from_rationale_data(rationale_data)
  rationale_examples = sample_rationale_examples(rationale_data)
  rationales = G.generate_rationales(questions, rationale_examples, answers)

  # Evaluate the generated rationales
  labels = E.evaluate_reasoning(questions, answers, rationales)

  # Filter out the instances where the labels disagree with the ground truth
  rationale_data = filter_by_labels(questions, answers, rationales, labels)

  # Fine-tune the rationale generator on the filtered rationale data
  G.fine_tune_on_rationale_data(rationale_data)

  # Update the convergence criterion and the iteration counter
  converged = check_convergence(G)
  iterations += 1

# Return the final rationale generator
return G
```