---
title: 2104.05764v1 Domain Adaptive Monocular Depth Estimation With Semantic Information
date: 2021-04-06
---

# [Domain Adaptive Monocular Depth Estimation With Semantic Information](http://arxiv.org/abs/2104.05764v1)

authors: Fei Lu, Hyeonwoo Yu, Jean Oh


## What, Why and How

[1]: https://arxiv.org/abs/2104.05764v1 "[2104.05764v1] Domain Adaptive Monocular Depth Estimation With Semantic ..."
[2]: https://arxiv.org/pdf/2104.05764v1 "arXiv:2104.05764v1 [cs.CV] 12 Apr 2021"
[3]: http://export.arxiv.org/abs/2305.05764v1 "[2305.05764v1] Case A or Case B? The effective recombination ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a domain adaptive monocular depth estimation model that uses semantic information to align the source and target domains.
- **Why**: The paper aims to address the problem of domain discrepancy between different datasets for monocular depth estimation, which affects the generalization ability of the trained models. The paper also argues that traditional adversarial domain alignment methods are insufficient for conditional tasks like depth estimation, as they do not consider the semantic information of the images.
- **How**: The paper introduces a semantic-aware adversarial training framework that consists of three components: a depth estimation network, a semantic segmentation network, and a domain discriminator network. The depth estimation network predicts the depth map from a single RGB image, the semantic segmentation network predicts the semantic labels of the image, and the domain discriminator network tries to distinguish the source and target domains based on the feature maps of both networks. The paper uses an adversarial loss and a cycle-consistency loss to encourage the domain alignment at both pixel-level and feature-level. The paper evaluates the proposed model on two datasets: KITTI and Cityscapes, and shows that it achieves state-of-the-art performance compared to existing methods. The paper also demonstrates that the proposed model can produce more accurate and consistent depth maps on boundaries and objects at far distances.

## Main Contributions

The paper claims to make the following contributions:

- It proposes a novel domain adaptive monocular depth estimation model that leverages semantic information to align the source and target domains in an adversarial manner.
- It introduces a cycle-consistency loss that enforces the consistency between the predicted depth maps and the original images in both domains.
- It achieves state-of-the-art results on two benchmark datasets for monocular depth estimation: KITTI and Cityscapes, and shows the effectiveness of the proposed model on challenging scenarios such as boundaries and far objects.

## Method Summary

The method section of the paper describes the proposed semantic-aware adversarial training framework for domain adaptive monocular depth estimation. The framework consists of three networks: a depth estimation network (D), a semantic segmentation network (S), and a domain discriminator network (F). The depth estimation network takes a single RGB image as input and outputs a depth map. The semantic segmentation network takes the same RGB image as input and outputs a semantic label map. The domain discriminator network takes the feature maps of both D and S as input and outputs a domain label, indicating whether the input belongs to the source or target domain. The paper uses two datasets for training and testing: KITTI as the source domain and Cityscapes as the target domain. The paper defines four losses to train the framework: an adversarial loss (L_adv), a cycle-consistency loss (L_cyc), a depth reconstruction loss (L_depth), and a semantic segmentation loss (L_seg). The adversarial loss encourages the domain alignment by fooling the domain discriminator network with the feature maps of D and S. The cycle-consistency loss enforces the consistency between the predicted depth maps and the original images in both domains, by using an inverse warping technique. The depth reconstruction loss measures the difference between the predicted depth maps and the ground truth depth maps in the source domain, by using an L1 norm. The semantic segmentation loss measures the difference between the predicted semantic label maps and the ground truth semantic label maps in both domains, by using a cross-entropy loss. The paper optimizes the total loss (L_total) by minimizing it with respect to D and S, and maximizing it with respect to F. The paper also introduces a curriculum learning strategy to gradually increase the difficulty of the domain adaptation task, by using different subsets of Cityscapes images with different levels of complexity. The paper provides the details of the network architectures, the training procedure, and the hyperparameters in this section.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the depth estimation network D
# Define the semantic segmentation network S
# Define the domain discriminator network F
# Define the adversarial loss L_adv
# Define the cycle-consistency loss L_cyc
# Define the depth reconstruction loss L_depth
# Define the semantic segmentation loss L_seg
# Define the total loss L_total
# Define the curriculum learning strategy CL

# Initialize the networks D, S, and F with random weights
# Load the source domain dataset KITTI
# Load the target domain dataset Cityscapes
# Set the learning rate and the number of epochs

for epoch in range(num_epochs):
  # Shuffle the source and target domain images
  # Divide the target domain images into subsets according to CL
  for batch in range(num_batches):
    # Sample a batch of source domain images and their depth and semantic labels
    # Sample a batch of target domain images and their semantic labels
    # Feed the source and target domain images to D and S, and get the depth maps and semantic label maps
    # Feed the feature maps of D and S to F, and get the domain labels
    # Compute L_adv, L_cyc, L_depth, and L_seg using the predictions and the ground truth labels
    # Compute L_total as a weighted sum of L_adv, L_cyc, L_depth, and L_seg
    # Update the weights of D and S by minimizing L_total using gradient descent
    # Update the weights of F by maximizing L_total using gradient ascent
  
  # Evaluate the performance of D on the target domain test set using standard metrics
  # Save the model checkpoints if the performance improves

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import cv2

# Define the depth estimation network D
# D is based on ResNet-50 with skip connections and up-convolution layers
class DepthEstimationNetwork(nn.Module):
  def __init__(self):
    super(DepthEstimationNetwork, self).__init__()
    # Load the pretrained ResNet-50 model
    self.resnet = models.resnet50(pretrained=True)
    # Remove the last layer of ResNet-50
    self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])
    # Define the up-convolution layers with skip connections
    self.upconv1 = nn.ConvTranspose2d(2048, 1024, kernel_size=3, stride=2, padding=1, output_padding=1)
    self.upconv2 = nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1)
    self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)
    self.upconv4 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)
    self.upconv5 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)
    self.upconv6 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)
    self.upconv7 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1)
    # Define the final convolution layer to output a depth map
    self.conv_final = nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1)
    # Define the activation functions
    self.relu = nn.ReLU(inplace=True)
    self.sigmoid = nn.Sigmoid()

  def forward(self, x):
    # Get the feature maps of ResNet-50 at different levels
    x = self.relu(self.resnet.conv1(x))
    x = self.resnet.bn1(x)
    x = self.resnet.relu(x)
    x = self.resnet.maxpool(x)
    x1 = self.resnet.layer1(x) # 256 channels
    x2 = self.resnet.layer2(x1) # 512 channels
    x3 = self.resnet.layer3(x2) # 1024 channels
    x4 = self.resnet.layer4(x3) # 2048 channels
    # Apply the up-convolution layers with skip connections
    x5 = self.relu(self.upconv1(x4)) # 1024 channels
    x5 = torch.cat((x5,x3), dim=1) # concatenate with x3
    x6 = self.relu(self.upconv2(x5)) # 512 channels
    x6 = torch.cat((x6,x2), dim=1) # concatenate with x2
    x7 = self.relu(self.upconv3(x6)) # 256 channels
    x7 = torch.cat((x7,x1), dim=1) # concatenate with x1
    x8 = self.relu(self.upconv4(x7)) # 128 channels
    x9 = self.relu(self.upconv5(x8)) # 64 channels
    x10 = self.relu(self.upconv6(x9)) # 32 channels
    x11 = self.relu(self.upconv7(x10)) # 16 channels
    # Apply the final convolution layer to output a depth map
    out = self.sigmoid(self.conv_final(x11)) # 1 channel

    return out

# Define the semantic segmentation network S
# S is based on DeepLabv3+ with ResNet-50 as backbone and Atrous Spatial Pyramid Pooling (ASPP) module
class SemanticSegmentationNetwork(nn.Module):
  def __init__(self):
    super(SemanticSegmentationNetwork,self).__init__()
    # Load the pretrained ResNet-50 model
    self.resnet = models.resnet50(pretrained=True)
    # Remove the last layer of ResNet-50
    self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])
    # Define the ASPP module with four parallel branches
    self.aspp1 = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)
    self.aspp2 = nn.Conv2d(2048, 256, kernel_size=3, stride=1, padding=6, dilation=6)
    self.aspp3 = nn.Conv2d(2048, 256, kernel_size=3, stride=1, padding=12, dilation=12)
    self.aspp4 = nn.Conv2d(2048, 256, kernel_size=3, stride=1, padding=18, dilation=18)
    # Define the global average pooling branch
    self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),
                                         nn.Conv2d(2048, 256, kernel_size=1, stride=1))
    # Define the convolution layer to concatenate the ASPP outputs
    self.conv1 = nn.Conv2d(1280, 256, kernel_size=1, stride=1)
    # Define the decoder module with skip connection and bilinear upsampling
    self.decoder = nn.Sequential(nn.Conv2d(256, 48, kernel_size=1, stride=1),
                                 nn.Upsample(scale_factor=4),
                                 nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1),
                                 nn.Upsample(scale_factor=4),
                                 nn.Conv2d(256, 19, kernel_size=3, stride=1, padding=1))
    # Define the activation functions
    self.relu = nn.ReLU(inplace=True)

  def forward(self,x):
    # Get the feature maps of ResNet-50 at different levels
    x = self.relu(self.resnet.conv1(x))
    x = self.resnet.bn1(x)
    x = self.resnet.relu(x)
    x = self.resnet.maxpool(x)
    x_low = self.resnet.layer1(x) # 256 channels
    x_high = self.resnet.layer4(self.resnet.layer3(self.resnet.layer2(x_low))) # 2048 channels
    # Apply the ASPP module to the high-level feature map
    x1 = self.aspp1(x_high) # 256 channels
    x2 = self.aspp2(x_high) # 256 channels
    x3 = self.aspp3(x_high) # 256 channels
    x4 = self.aspp4(x_high) # 256 channels
    # Apply the global average pooling branch to the high-level feature map
    x5 = self.global_avg_pool(x_high) # 256 channels
    x5 = nn.Upsample(size=x4.size()[2:])(x5) # upsample to match the size of x4
    # Concatenate the ASPP outputs and apply a convolution layer
    x6 = torch.cat((x1,x2,x3,x4,x5), dim=1) # 1280 channels
    x6 = self.conv1(x6) # 256 channels
    # Apply the decoder module with skip connection and bilinear upsampling
    x7 = torch.cat((x6,self.decoder[0](x_low)), dim=1) # 304 channels
    out = self.decoder[2](self.decoder[1](x7)) # 19 channels

    return out

# Define the domain discriminator network F
# F is based on a fully convolutional network with LeakyReLU activation and spectral normalization
class DomainDiscriminatorNetwork(nn.Module):
  def __init__(self):
    super(DomainDiscriminatorNetwork,self).__init__()
    # Define the convolution layers with spectral normalization and LeakyReLU activation
    self.conv1 = nn.utils.spectral_norm(nn.Conv2d(2307,64,kernel_size=4,stride=2,padding=1)) # input channel is 2307 (2048+256+3)
    self.conv2 = nn.utils.spectral_norm(nn.Conv2d(64,128,kernel_size=4,stride=2,padding=1))
    self.conv3 = nn.utils.spectral_norm(nn.Conv2d(128,256,kernel_size=4,stride=2,padding=1))
    self.conv4 = nn.utils.spectral_norm(nn.Conv2d(256,512,kernel_size=4,stride=2,padding=1))
    self.conv5 = nn.utils.spectral_norm(nn.Conv2d(512,1024,kernel_size=4,stride=2,padding=1))
    self.conv6 = nn.utils.spectral_norm(nn.Conv2d(1024,2048,kernel_size=4,stride=2,padding=1))
    # Define the final convolution layer to output