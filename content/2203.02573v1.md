---
title: 2203.02573v1 Show Me What and Tell Me How  Video Synthesis via Multimodal Conditioning
date: 2022-03-03
---

# [Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning](http://arxiv.org/abs/2203.02573v1)

authors: Ligong Han, Jian Ren, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski, Shervin Minaee, Dimitris Metaxas, Sergey Tulyakov


## What, Why and How

[1]: https://arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.02573v1 "Show Me What and Tell Me How: Video Synthesis via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis via ..."

Here is a summary of the paper:

```
What: A multimodal video generation framework that can use text and images as conditions to synthesize videos with desired content and motion.

Why: To overcome the limitations of existing methods that use only one modality as the condition, and to enable more flexible and diverse video synthesis.

How: By leveraging quantized representations for videos and applying a bidirectional transformer with multiple modalities as inputs to predict a discrete video representation. The paper also proposes a new video token, an improved mask-prediction algorithm, and text augmentation to enhance the video quality, consistency, and diversity.
```

## Main Contributions

[1]: https://arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.02573v1 "Show Me What and Tell Me How: Video Synthesis via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis via ..."

The paper claims the following contributions:

- A multimodal video generation framework that can use text and images as conditions to synthesize videos with desired content and motion.
- A new video token trained with self-learning and an improved mask-prediction algorithm for sampling video tokens.
- Text augmentation to improve the robustness of the textual representation and diversity of generated videos.
- State-of-the-art generation results on four datasets, including a newly collected dataset labeled with facial attributes.

## Method Summary

[1]: https://arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.02573v1 "Show Me What and Tell Me How: Video Synthesis via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis via ..."

Here is a summary of the method section:

```
The method consists of four main components: a multimodal encoder, a video decoder, a video token, and a text augmentation module.

- The multimodal encoder takes text and image inputs and encodes them into latent representations using separate transformers. The text encoder uses BERT [11] and the image encoder uses ViT [14]. The latent representations are then concatenated and fed into a cross-modal transformer to learn the joint representation of text and image modalities.
- The video decoder takes the joint representation from the multimodal encoder and predicts a sequence of discrete video tokens using a unidirectional transformer. The video tokens are then mapped to continuous video frames using a codebook learned by vector quantization [44].
- The video token is a new discrete representation for videos that is trained with self-learning and an improved mask-prediction algorithm. The self-learning objective encourages the video token to be consistent across different modalities and temporal scales. The mask-prediction algorithm improves the sampling efficiency and diversity of video tokens by masking out multiple tokens in a sequence and predicting them from the context.
- The text augmentation module enhances the robustness and diversity of the textual representation by applying various transformations to the input text, such as paraphrasing, insertion, deletion, and replacement. The augmented texts are used as additional inputs to the multimodal encoder.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.02573v1 "Show Me What and Tell Me How: Video Synthesis via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis via ..."

Here is a high-level pseudo code for the paper:

```
# Define the multimodal encoder, video decoder, video token, and text augmentation modules
multimodal_encoder = MultimodalEncoder()
video_decoder = VideoDecoder()
video_token = VideoToken()
text_augmentation = TextAugmentation()

# Define the input text and image
text = "A person is walking in a park"
image = "an image of a park"

# Augment the input text with various transformations
augmented_texts = text_augmentation(text)

# Encode the input text and image into latent representations
text_latents = multimodal_encoder.encode_text(augmented_texts)
image_latents = multimodal_encoder.encode_image(image)

# Concatenate the text and image latents and feed them into a cross-modal transformer
joint_latents = multimodal_encoder.cross_modal_transformer(text_latents, image_latents)

# Decode the joint latents into a sequence of discrete video tokens
video_tokens = video_decoder(joint_latents)

# Map the video tokens to continuous video frames using a codebook
video_frames = video_token.codebook(video_tokens)

# Output the synthesized video
video = video_frames.to_video()
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.02573v1 "Show Me What and Tell Me How: Video Synthesis via ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.02573 "[2203.02573] Show Me What and Tell Me How: Video Synthesis via ..."

Here is a detailed pseudo code to implement the paper:

```
# Define the hyperparameters and constants
num_layers = 12 # number of transformer layers
num_heads = 16 # number of attention heads
hidden_size = 768 # hidden size of transformer
vocab_size = 30522 # vocabulary size of BERT
codebook_size = 8192 # size of the video token codebook
frame_size = 64 # size of the video frame
max_length = 32 # maximum length of video sequence
mask_ratio = 0.15 # ratio of tokens to be masked
temperature = 0.07 # temperature for softmax
learning_rate = 1e-4 # learning rate for optimization

# Define the multimodal encoder, video decoder, video token, and text augmentation modules
multimodal_encoder = MultimodalEncoder(num_layers, num_heads, hidden_size, vocab_size)
video_decoder = VideoDecoder(num_layers, num_heads, hidden_size, codebook_size)
video_token = VideoToken(codebook_size, frame_size)
text_augmentation = TextAugmentation()

# Define the loss functions and optimizer
cross_entropy_loss = CrossEntropyLoss()
l2_loss = L2Loss()
optimizer = Adam(learning_rate)

# Define the training loop
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the input text and image from the batch
    text = batch["text"]
    image = batch["image"]

    # Augment the input text with various transformations
    augmented_texts = text_augmentation(text)

    # Encode the input text and image into latent representations
    text_latents = multimodal_encoder.encode_text(augmented_texts)
    image_latents = multimodal_encoder.encode_image(image)

    # Concatenate the text and image latents and feed them into a cross-modal transformer
    joint_latents = multimodal_encoder.cross_modal_transformer(text_latents, image_latents)

    # Decode the joint latents into a sequence of discrete video tokens
    video_tokens = video_decoder(joint_latents)

    # Map the video tokens to continuous video frames using a codebook
    video_frames = video_token.codebook(video_tokens)

    # Get the ground truth video frames from the batch
    gt_frames = batch["video"]

    # Compute the reconstruction loss between the generated and ground truth frames
    recon_loss = l2_loss(video_frames, gt_frames)

    # Mask out some tokens in the video token sequence randomly
    masked_tokens, mask_indices, mask_labels = mask_tokens(video_tokens, mask_ratio)

    # Predict the masked tokens from the context using the video decoder
    pred_tokens = video_decoder(masked_tokens, mask_indices)

    # Compute the prediction loss between the predicted and masked tokens
    pred_loss = cross_entropy_loss(pred_tokens, mask_labels, temperature)

    # Compute the total loss as a weighted sum of reconstruction and prediction losses
    total_loss = recon_loss + lambda * pred_loss

    # Backpropagate the gradients and update the parameters
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

# Save the trained model
save_model(multimodal_encoder, video_decoder, video_token)
```