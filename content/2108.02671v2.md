---
title: 2108.02671v2 Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware
date: 2021-08-03
---

# [Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware](http://arxiv.org/abs/2108.02671v2)

authors: Julia Hornauer, Lazaros Nalpantidis, Vasileios Belagiannis


## What, Why and How

[1]: https://arxiv.org/abs/2108.02671v2 "[2108.02671v2] Visual Domain Adaptation for Monocular Depth Estimation ..."
[2]: https://arxiv.org/pdf/2108.02671v2 "arXiv:2108.02671v2 [cs.CV] 5 May 2022"
[3]: http://export.arxiv.org/abs/2108.02671 "[2108.02671] Visual Domain Adaptation for Monocular Depth Estimation on ..."

Here is a summary of the paper:

- **What**: The paper presents a method for visual domain adaptation for monocular depth estimation on resource-constrained hardware, such as embedded devices. Monocular depth estimation is the task of predicting the depth of a scene from a single image. Visual domain adaptation is the process of adapting a model trained on one domain (e.g., synthetic images) to another domain (e.g., real images) without using labels from the target domain.
- **Why**: The paper addresses the problem of deploying deep neural networks on resource-constrained hardware in diverse operation environments, where model adaptation is required due to the domain shift between the source and target domains. The paper claims that visual domain adaptation has not been previously explored for resource-constrained hardware, and that it is relevant only for efficient network architectures and small training sets.
- **How**: The paper proposes an adversarial learning approach that is adapted for training on the device with limited resources. The approach consists of a generator network that predicts the depth map from an input image, and a discriminator network that classifies the depth map as either source or target domain. The generator network is pre-trained on the source domain with labels, and then fine-tuned on the target domain without labels using the adversarial loss from the discriminator network. The paper also introduces some modifications to the standard adversarial learning framework, such as using a lightweight discriminator architecture, applying gradient clipping and normalization, and using a balanced sampling strategy. The paper evaluates the proposed method on two datasets: KITTI  and Virtual KITTI , which simulate real and synthetic driving scenes respectively. The paper compares the proposed method with several baselines and state-of-the-art methods, and shows that it achieves competitive results in terms of depth estimation accuracy and visual quality. The paper also conducts an ablation study to analyze the impact of different components of the proposed method.

## Main Contributions

According to the paper, the main contributions are:

- The first feasibility study of visual domain adaptation for monocular depth estimation on resource-constrained hardware.
- An adversarial learning approach that is adapted for training on the device with limited resources, with several modifications to the standard framework.
- An extensive experimental evaluation on two datasets, showing competitive results with state-of-the-art methods and ablation analysis.

## Method Summary

The method section of the paper describes the proposed adversarial learning approach for visual domain adaptation for monocular depth estimation on resource-constrained hardware. The approach consists of two main components: a generator network and a discriminator network. The generator network is a convolutional neural network that takes an input image and predicts a depth map. The discriminator network is a lightweight convolutional neural network that takes a depth map and classifies it as either source or target domain. The generator network is pre-trained on the source domain with labels, and then fine-tuned on the target domain without labels using the adversarial loss from the discriminator network. The adversarial loss aims to minimize the domain discrepancy between the source and target depth maps, while preserving the depth accuracy. The paper also introduces some modifications to the standard adversarial learning framework, such as:

- Using a lightweight discriminator architecture that has fewer parameters and operations than the generator network, to reduce the computational cost and memory consumption on the device.
- Applying gradient clipping and normalization to the generator network, to prevent gradient explosion and instability during training.
- Using a balanced sampling strategy that ensures that the source and target domains have equal representation in each mini-batch, to avoid biasing the discriminator network towards one domain.

The paper also describes some implementation details, such as the network architectures, the loss functions, the optimization algorithms, and the hyperparameters used in the experiments.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator network G and the discriminator network D
G = GeneratorNetwork()
D = DiscriminatorNetwork()

# Pre-train G on the source domain with labels using depth loss
for epoch in range(pre_train_epochs):
  for batch in source_data_loader:
    # Get the input image and the ground truth depth map from the source domain
    image, depth = batch
    # Predict the depth map using G
    pred_depth = G(image)
    # Compute the depth loss between the predicted and ground truth depth maps
    depth_loss = DepthLoss(pred_depth, depth)
    # Update G parameters using gradient descent
    G.backward(depth_loss)
    G.update()

# Fine-tune G and D on the target domain without labels using adversarial loss
for epoch in range(fine_tune_epochs):
  for batch in target_data_loader:
    # Get the input image from the target domain
    image = batch
    # Predict the depth map using G
    pred_depth = G(image)
    # Get a random depth map from the source domain
    source_depth = source_data_loader.sample()
    # Classify the source and target depth maps using D
    source_label = D(source_depth)
    target_label = D(pred_depth)
    # Compute the adversarial loss for D and G
    adv_loss_D = AdversarialLoss(source_label, target_label, real_label, fake_label)
    adv_loss_G = AdversarialLoss(target_label, real_label)
    # Update D parameters using gradient descent
    D.backward(adv_loss_D)
    D.update()
    # Clip and normalize the gradients of G
    G.clip_gradients()
    G.normalize_gradients()
    # Update G parameters using gradient descent
    G.backward(adv_loss_G)
    G.update()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from PIL import Image

# Define the generator network G
class GeneratorNetwork(nn.Module):
  def __init__(self):
    super(GeneratorNetwork, self).__init__()
    # Use the MobileNetV2 architecture as the backbone
    self.backbone = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)
    # Replace the last convolutional layer with a 1x1 convolution that outputs a single channel depth map
    self.backbone.features[-1][0] = nn.Conv2d(320, 1, kernel_size=1, stride=1, padding=0)
    # Add a bilinear upsampling layer to match the input image resolution
    self.upsample = nn.UpsamplingBilinear2d(size=(256, 512))

  def forward(self, x):
    # Pass the input image through the backbone network
    x = self.backbone.features(x)
    # Upsample the output depth map
    x = self.upsample(x)
    # Return the depth map
    return x

# Define the discriminator network D
class DiscriminatorNetwork(nn.Module):
  def __init__(self):
    super(DiscriminatorNetwork, self).__init__()
    # Use a lightweight convolutional network with four layers
    self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)
    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)
    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
    # Add batch normalization and leaky ReLU activation after each convolutional layer
    self.bn1 = nn.BatchNorm2d(16)
    self.bn2 = nn.BatchNorm2d(32)
    self.bn3 = nn.BatchNorm2d(64)
    self.bn4 = nn.BatchNorm2d(128)
    self.lrelu = nn.LeakyReLU(0.2)
    # Add a global average pooling layer and a fully connected layer that outputs a single value
    self.gap = nn.AdaptiveAvgPool2d((1, 1))
    self.fc = nn.Linear(128, 1)

  def forward(self, x):
    # Pass the input depth map through the convolutional layers
    x = self.lrelu(self.bn1(self.conv1(x)))
    x = self.lrelu(self.bn2(self.conv2(x)))
    x = self.lrelu(self.bn3(self.conv3(x)))
    x = self.lrelu(self.bn4(self.conv4(x)))
    # Apply global average pooling and flatten the output
    x = self.gap(x).view(-1, 128)
    # Pass the output through the fully connected layer and return it
    x = self.fc(x)
    return x

# Define the depth loss function
def DepthLoss(pred_depth, gt_depth):
  # Use the L1 norm between the predicted and ground truth depth maps as the depth loss
  return torch.mean(torch.abs(pred_depth - gt_depth))

# Define the adversarial loss function
def AdversarialLoss(pred_label, target_label):
  # Use the binary cross entropy with logits loss between the predicted and target labels as the adversarial loss
  return nn.BCEWithLogitsLoss()(pred_label, target_label)

# Define some hyperparameters
pre_train_epochs = 10 # Number of epochs for pre-training G on the source domain with labels
fine_tune_epochs = 20 # Number of epochs for fine-tuning G and D on the target domain without labels
batch_size = 8 # Batch size for training and testing
lr_G = 0.00001 # Learning rate for G
lr_D = 0.00001 # Learning rate for D
beta1_G = 0.9 # Beta1 parameter for Adam optimizer for G
beta1_D = 0.9 # Beta1 parameter for Adam optimizer for D
beta2_G = 0.999 # Beta2 parameter for Adam optimizer for G
beta2_D = 0.999 # Beta2 parameter for Adam optimizer for D
clip_value_G = 0.01 # Clip value for the gradients of G
norm_value_G = 0.01 # Normalization value for the gradients of G
real_label = 1 # Label for the real depth maps from the source domain
fake_label = 0 # Label for the fake depth maps from the target domain

# Define some data paths
source_data_path = "path/to/source/data" # Path to the source domain data with labels
target_data_path = "path/to/target/data" # Path to the target domain data without labels

# Define some data transformations
data_transforms = transforms.Compose([
  # Resize the images to 256x512 pixels
  transforms.Resize((256, 512)),
  # Convert the images to tensors and normalize them
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Define a custom dataset class for the source domain data
class SourceDataset(torch.utils.data.Dataset):
  def __init__(self, source_data_path, data_transforms):
    super(SourceDataset, self).__init__()
    # Get the list of image and depth map files from the source data path
    self.image_files = sorted(glob.glob(source_data_path + "/images/*.png"))
    self.depth_files = sorted(glob.glob(source_data_path + "/depth/*.png"))
    # Apply the data transformations
    self.data_transforms = data_transforms

  def __len__(self):
    # Return the number of samples in the dataset
    return len(self.image_files)

  def __getitem__(self, index):
    # Get the image and depth map files at the given index
    image_file = self.image_files[index]
    depth_file = self.depth_files[index]
    # Load the image and depth map as PIL images
    image = Image.open(image_file)
    depth = Image.open(depth_file)
    # Apply the data transformations to the image and depth map
    image = self.data_transforms(image)
    depth = self.data_transforms(depth)
    # Return the image and depth map as tensors
    return image, depth

# Define a custom dataset class for the target domain data
class TargetDataset(torch.utils.data.Dataset):
  def __init__(self, target_data_path, data_transforms):
    super(TargetDataset, self).__init__()
    # Get the list of image files from the target data path
    self.image_files = sorted(glob.glob(target_data_path + "/images/*.png"))
    # Apply the data transformations
    self.data_transforms = data_transforms

  def __len__(self):
    # Return the number of samples in the dataset
    return len(self.image_files)

  def __getitem__(self, index):
    # Get the image file at the given index
    image_file = self.image_files[index]
    # Load the image as a PIL image
    image = Image.open(image_file)
    # Apply the data transformations to the image
    image = self.data_transforms(image)
    # Return the image as a tensor
    return image

# Create the source and target datasets using the custom dataset classes
source_dataset = SourceDataset(source_data_path, data_transforms)
target_dataset = TargetDataset(target_data_path, data_transforms)

# Create the source and target data loaders using the datasets and batch size
source_data_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)
target_data_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)

# Create an instance of G and D using the network classes
G = GeneratorNetwork()
D = DiscriminatorNetwork()

# Create an instance of Adam optimizer for G and D using the learning rates and beta parameters
optimizer_G = optim.Adam(G.parameters(), lr=lr_G, betas=(beta1_G, beta2_G))
optimizer_D = optim.Adam(D.parameters(), lr=lr_D, betas=(beta1_D, beta2_D))

# Pre-train G on the source domain with labels using depth loss
for epoch in range(pre_train_epochs):
  for batch in source_data_loader:
    # Get the input image and the ground truth depth map from the source domain
    image, depth = batch
    # Predict the depth map using G
    pred_depth = G(image)
    # Compute the depth loss between the predicted and ground truth depth maps
    depth_loss = DepthLoss(pred_depth, depth)
    # Zero out any previous gradients in G parameters
    optimizer_G.zero_grad()
    # Backpropagate through G to compute gradients with respect to depth loss
    depth_loss.backward()
    # Update G parameters using gradient descent with Adam optimizer
    optimizer_G.step()

# Fine-tune G and D on the target domain without labels using adversarial loss
for epoch in range(fine_tune_epochs):
  for batch in target_data_loader:
    # Get a balanced mini-batch of source and target images by sampling