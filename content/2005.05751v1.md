---
title: 2005.05751v1 Unpaired Motion Style Transfer from Video to Animation
date: 2020-05-06
---

# [Unpaired Motion Style Transfer from Video to Animation](http://arxiv.org/abs/2005.05751v1)

authors: Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen


## What, Why and How

[1]: https://arxiv.org/abs/2005.05751v1 "Unpaired Motion Style Transfer from Video to Animation"
[2]: https://arxiv.org/pdf/2005.05751v1 "Unpaired Motion Style Transfer from Video to Animation - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.05751v1 "[2305.05751v1] What is mature and what is still emerging in the ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- What: The paper presents a novel data-driven framework for motion style transfer, which can transfer the motion style from one animation clip to another, while preserving the motion content of the latter. The framework can also extract motion styles directly from videos and apply them to 3D input motions.
- Why: The paper aims to address the limitations of existing data-driven approaches for motion style transfer, which are supervised and rely on paired data, where motions with the same content are performed in different styles. These approaches are also limited to transfer of styles that were seen during training. The paper proposes an unsupervised approach that can learn from an unpaired collection of motions with style labels, and enable transferring motion styles not observed during training. The paper also demonstrates the ability to extract motion styles from videos, which extends the set of style examples far beyond motions captured by MoCap systems.
- How: The paper's style transfer network encodes motions into two latent codes, for content and for style, each of which plays a different role in the decoding (synthesis) process. While the content code is decoded into the output motion by several temporal convolutional layers, the style code modifies deep features via temporally invariant adaptive instance normalization (AdaIN). Moreover, while the content code is encoded from 3D joint rotations, the paper learns a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos. The paper evaluates the results using quantitative and qualitative measures, and compares them with state-of-the-art methods. The paper also shows some applications of the framework, such as video-driven animation and style interpolation.


## Main Contributions

[1]: https://arxiv.org/abs/2005.05751v1 "Unpaired Motion Style Transfer from Video to Animation"
[2]: https://arxiv.org/pdf/2005.05751v1 "Unpaired Motion Style Transfer from Video to Animation - arXiv.org"
[3]: http://export.arxiv.org/abs/1605.05751v1 "[1605.05751v1] A Holographic Dual of the Quantum Inequalities"

According to the paper[^1^][1], the main contributions are:

- The paper proposes a novel data-driven framework for motion style transfer, which learns from an unpaired collection of motions with style labels, and enables transferring motion styles not observed during training.
- The paper introduces a style transfer network that encodes motions into two latent codes, for content and for style, and decodes them into the output motion using temporal convolutional layers and adaptive instance normalization (AdaIN).
- The paper learns a common embedding for style from either 3D or 2D joint positions, enabling style extraction from videos and bypassing 3D reconstruction.
- The paper demonstrates the ability to transfer motion styles directly from videos to 3D animations, which extends the set of style examples far beyond motions captured by MoCap systems.
- The paper evaluates the results using quantitative and qualitative measures, and compares them with state-of-the-art methods. The paper also shows some applications of the framework, such as video-driven animation and style interpolation.

## Method Summary

[1]: https://arxiv.org/abs/2005.05751v1 "Unpaired Motion Style Transfer from Video to Animation"
[2]: https://arxiv.org/pdf/2005.05751v1 "Unpaired Motion Style Transfer from Video to Animation - arXiv.org"
[3]: http://export.arxiv.org/abs/1605.05751v1 "[1605.05751v1] A Holographic Dual of the Quantum Inequalities"

Here is a summary of the method section of the paper[^1^][1]:

- The paper's style transfer network consists of two encoders, one for content and one for style, and a decoder that synthesizes the output motion. The content encoder takes as input 3D joint rotations and outputs a content code. The style encoder takes as input either 3D or 2D joint positions and outputs a style code. The decoder takes as input the content code and the style code, and outputs 3D joint rotations with the desired style.
- The paper uses temporal convolutional layers to encode and decode motions, which capture the temporal dependencies and preserve the motion continuity. The paper also uses adaptive instance normalization (AdaIN) to inject the style code into the decoder's features, which allows for flexible style manipulation and transfer. The paper uses a common embedding space for style, which enables learning from both 3D and 2D inputs, and extracting styles from videos.
- The paper trains the network using an unpaired collection of motions with style labels, such as happy, sad, angry, etc. The paper uses a cycle-consistency loss to ensure that the content is preserved after style transfer, and a classification loss to ensure that the style is transferred correctly. The paper also uses a reconstruction loss to ensure that the network can reconstruct the original motions, and a smoothness loss to ensure that the output motions are smooth and natural.
- The paper evaluates the network using quantitative and qualitative measures, such as mean squared error (MSE), Fr√©chet Inception Distance (FID), perceptual user study, and visual comparison. The paper compares the network with state-of-the-art methods, such as [Holden et al. 2016], [Huang et al. 2017], [Aberman et al. 2019], and [Wang et al. 2019]. The paper shows that the network can achieve comparable or better results than these methods, despite not requiring paired training data, and can outperform them when transferring previously unseen styles.
- The paper also shows some applications of the network, such as video-driven animation and style interpolation. The paper demonstrates that the network can extract motion styles from videos of human actors or animals, and apply them to 3D input motions of different characters or species. The paper also shows that the network can interpolate between different styles smoothly and realistically.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the style transfer network
class StyleTransferNetwork(nn.Module):
  def __init__(self):
    # Initialize the content encoder, style encoder and decoder
    self.content_encoder = ContentEncoder()
    self.style_encoder = StyleEncoder()
    self.decoder = Decoder()

  def forward(self, x_content, x_style):
    # Encode the content and style inputs
    c = self.content_encoder(x_content) # c is the content code
    s = self.style_encoder(x_style) # s is the style code
    # Decode the output motion with the desired style
    y = self.decoder(c, s) # y is the output motion
    return y

# Define the training procedure
def train(network, data_loader, optimizer, loss_functions):
  # Loop over the training data
  for x_content, x_style in data_loader:
    # Forward pass
    y = network(x_content, x_style)
    # Compute the losses
    loss_cycle = cycle_consistency_loss(y, x_content)
    loss_class = classification_loss(y, x_style)
    loss_recon = reconstruction_loss(y, x_content, x_style)
    loss_smooth = smoothness_loss(y)
    loss_total = loss_cycle + loss_class + loss_recon + loss_smooth
    # Backward pass and update
    optimizer.zero_grad()
    loss_total.backward()
    optimizer.step()

# Define the evaluation procedure
def evaluate(network, test_data):
  # Initialize the metrics
  mse = MeanSquaredError()
  fid = FrechetInceptionDistance()
  user_study = UserStudy()
  # Loop over the test data
  for x_content, x_style in test_data:
    # Forward pass
    y = network(x_content, x_style)
    # Compute the metrics
    mse.update(y, x_content)
    fid.update(y, x_style)
    user_study.update(y, x_content, x_style)
  # Report the results
  print("MSE:", mse.result())
  print("FID:", fid.result())
  print("User study:", user_study.result())

# Define some applications of the network
def video_driven_animation(network, video, motion):
  # Extract the joint positions from the video using OpenPose
  video_joints = openpose(video)
  # Forward pass
  y = network(motion, video_joints)
  # Visualize the output motion with the video style
  visualize(y)

def style_interpolation(network, motion, style1, style2):
  # Encode the styles
  s1 = network.style_encoder(style1)
  s2 = network.style_encoder(style2)
  # Interpolate between the styles using a linear combination
  alpha = np.linspace(0, 1, num_steps)
  s_interp = alpha * s1 + (1 - alpha) * s2
  # Decode the output motions with the interpolated styles
  y_interp = network.decoder(motion, s_interp)
  # Visualize the output motions with the interpolated styles
  visualize(y_interp)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import openpose # A library for human pose estimation from videos
import visualize # A library for visualizing 3D animations

# Define the hyperparameters
num_joints = 22 # The number of joints in the motion data
num_styles = 10 # The number of style labels in the motion data
latent_dim = 128 # The dimension of the latent codes for content and style
hidden_dim = 256 # The dimension of the hidden units in the network
kernel_size = 3 # The kernel size of the temporal convolutional layers
stride = 1 # The stride of the temporal convolutional layers
padding = 1 # The padding of the temporal convolutional layers
num_layers = 4 # The number of temporal convolutional layers in the network
dropout = 0.1 # The dropout rate in the network
learning_rate = 0.001 # The learning rate for the optimizer
num_epochs = 100 # The number of epochs for training
batch_size = 32 # The batch size for training and evaluation

# Define the content encoder
class ContentEncoder(nn.Module):
  def __init__(self):
    super(ContentEncoder, self).__init__()
    # Initialize the temporal convolutional layers
    self.conv_layers = nn.ModuleList()
    for i in range(num_layers):
      if i == 0:
        # The first layer takes as input the joint rotations
        self.conv_layers.append(nn.Conv1d(num_joints * 3, hidden_dim, kernel_size, stride, padding))
      else:
        # The other layers take as input the previous layer's output
        self.conv_layers.append(nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride, padding))
      # Add a dropout layer after each convolutional layer
      self.conv_layers.append(nn.Dropout(dropout))
    # Initialize the linear layer that outputs the content code
    self.linear_layer = nn.Linear(hidden_dim, latent_dim)

  def forward(self, x):
    # x is a tensor of shape (batch_size, num_joints * 3, sequence_length)
    # Apply the temporal convolutional layers
    for layer in self.conv_layers:
      x = F.relu(layer(x))
    # x is a tensor of shape (batch_size, hidden_dim, sequence_length)
    # Apply global average pooling to reduce the temporal dimension
    x = torch.mean(x, dim=2)
    # x is a tensor of shape (batch_size, hidden_dim)
    # Apply the linear layer to get the content code
    c = self.linear_layer(x)
    # c is a tensor of shape (batch_size, latent_dim)
    return c

# Define the style encoder
class StyleEncoder(nn.Module):
  def __init__(self):
    super(StyleEncoder, self).__init__()
    # Initialize the temporal convolutional layers
    self.conv_layers = nn.ModuleList()
    for i in range(num_layers):
      if i == 0:
        # The first layer takes as input either 3D or 2D joint positions
        self.conv_layers.append(nn.Conv1d(num_joints * 3, hidden_dim, kernel_size, stride, padding))
      else:
        # The other layers take as input the previous layer's output
        self.conv_layers.append(nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride, padding))
      # Add a dropout layer after each convolutional layer
      self.conv_layers.append(nn.Dropout(dropout))
    # Initialize the linear layer that outputs the style code
    self.linear_layer = nn.Linear(hidden_dim + num_styles, latent_dim)

  def forward(self, x):
    # x is a tensor of shape (batch_size, num_joints * 3 + num_styles, sequence_length)
    # Split the input into joint positions and style labels
    x_joints = x[:, :num_joints * 3] # A tensor of shape (batch_size, num_joints * 3, sequence_length)
    x_styles = x[:, num_joints * 3:] # A tensor of shape (batch_size, num_styles, sequence_length)
    # Apply the temporal convolutional layers to the joint positions
    for layer in self.conv_layers:
      x_joints = F.relu(layer(x_joints))
    # x_joints is a tensor of shape (batch_size, hidden_dim, sequence_length)
    # Apply global average pooling to reduce the temporal dimension
    x_joints = torch.mean(x_joints, dim=2)
    # x_joints is a tensor of shape (batch_size, hidden_dim)
    # Apply global average pooling to the style labels
    x_styles = torch.mean(x_styles, dim=2)
    # x_styles is a tensor of shape (batch_size, num_styles)
    # Concatenate the joint positions and style labels
    x = torch.cat([x_joints, x_styles], dim=1)
    # x is a tensor of shape (batch_size, hidden_dim + num_styles)
    # Apply the linear layer to get the style code
    s = self.linear_layer(x)
    # s is a tensor of shape (batch_size, latent_dim)
    return s

# Define the decoder
class Decoder(nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    # Initialize the temporal convolutional layers
    self.conv_layers = nn.ModuleList()
    for i in range(num_layers):
      if i == num_layers - 1:
        # The last layer outputs the joint rotations
        self.conv_layers.append(nn.Conv1d(hidden_dim, num_joints * 3, kernel_size, stride, padding))
      else:
        # The other layers output hidden features
        self.conv_layers.append(nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride, padding))
      # Add a dropout layer after each convolutional layer
      self.conv_layers.append(nn.Dropout(dropout))
    # Initialize the AdaIN layers
    self.adain_layers = nn.ModuleList()
    for i in range(num_layers):
      # Each AdaIN layer takes as input the style code and modifies the features
      self.adain_layers.append(AdaIN(latent_dim, hidden_dim))

  def forward(self, c, s):
    # c is a tensor of shape (batch_size, latent_dim)
    # s is a tensor of shape (batch_size, latent_dim)
    # Repeat the content code along the temporal dimension
    c = c.unsqueeze(2).repeat(1, 1, sequence_length)
    # c is a tensor of shape (batch_size, latent_dim, sequence_length)
    # Apply the first temporal convolutional layer to the content code
    x = F.relu(self.conv_layers[0](c))
    # x is a tensor of shape (batch_size, hidden_dim, sequence_length)
    # Apply the remaining temporal convolutional and AdaIN layers alternately
    for i in range(1, num_layers):
      # Apply the AdaIN layer to modify the features with the style code
      x = self.adain_layers[i-1](x, s)
      # Apply the temporal convolutional layer to get the next features or output
      x = F.relu(self.conv_layers[i](x))
    # x is a tensor of shape (batch_size, num_joints * 3, sequence_length)
    return x

# Define the AdaIN layer
class AdaIN(nn.Module):
  def __init__(self, latent_dim, feature_dim):
    super(AdaIN, self).__init__()
    # Initialize the linear layers that compute the scale and bias from the style code
    self.scale_layer = nn.Linear(latent_dim, feature_dim)
    self.bias_layer = nn.Linear(latent_dim, feature_dim)

  def forward(self, x, s):
    # x is a tensor of shape (batch_size, feature_dim, sequence_length)
    # s is a tensor of shape (batch_size, latent_dim)
    # Compute the mean and standard deviation of the features along the feature dimension
    mean = torch.mean(x, dim=1, keepdim=True) # A tensor of shape (batch_size, 1, sequence_length)
    std = torch.std(x, dim=1, keepdim=True) + 1e-8 # A tensor of shape (batch_size, 1, sequence_length)
    # Normalize the features by subtracting the mean and dividing by the standard deviation
    x_norm = (x - mean) / std
    # Compute the scale and bias from the style code using the linear layers
    scale = self.scale_layer(s).unsqueeze(2) # A tensor of shape (batch_size, feature_dim, 1)
    bias = self.bias_layer(s).unsqueeze(2) # A tensor of shape (batch_size, feature_dim, 1)
    # Apply the scale and bias to the normalized features
    x_adain = scale * x_norm + bias
    # x_adain is a tensor of shape (batch_size, feature_dim, sequence_length)
    return x_adain

# Define the cycle-consistency loss function
def cycle_consistency_loss(y, x_content):
  # y is a tensor of shape (batch_size, num_joints * 3 , sequence_length)
  # x_content is a tensor of shape (batch_size , num_joints * 3 , sequence_length)
  # Compute the mean squared