---
title: 2204.10965v5 CLIP-Dissect  Automatic Description of Neuron Representations in Deep Vision Networks
date: 2022-04-11
---

# [CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks](http://arxiv.org/abs/2204.10965v5)

authors: Tuomas Oikarinen, Tsui-Wei Weng


## What, Why and How

[1]: https://arxiv.org/abs/2204.10965 "[2204.10965] CLIP-Dissect: Automatic Description of Neuron ... - arXiv.org"
[2]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"
[3]: https://arxiv.org/pdf/2204.10965.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes **CLIP-Dissect**, a new technique to automatically describe the function of individual hidden neurons inside vision networks.
- **Why**: The paper aims to provide more accurate and open-ended descriptions of neuron representations than existing methods, without the need for any labeled data or human examples.
- **How**: The paper leverages recent advances in multimodal vision/language models, such as **CLIP**, to label internal neurons with concepts from a large vocabulary. The paper also evaluates the accuracy and quality of the descriptions on last layer neurons and hidden layer neurons of ResNet-50, and compares them with existing methods such as NetDissect and Concept Bottleneck Models.



## Main Contributions

[1]: https://arxiv.org/abs/2204.10965 "[2204.10965] CLIP-Dissect: Automatic Description of Neuron ... - arXiv.org"
[2]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"
[3]: https://arxiv.org/pdf/2204.10965.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- A new technique, **CLIP-Dissect**, to automatically describe the function of individual hidden neurons inside vision networks with open-ended concepts, without the need for any labeled data or human examples.
- A comprehensive evaluation of **CLIP-Dissect** on last layer neurons and hidden layer neurons of ResNet-50, showing that it provides more accurate descriptions than existing methods such as NetDissect and Concept Bottleneck Models.
- A demonstration of the flexibility, scalability and extensibility of **CLIP-Dissect**, which is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future.


## Method Summary

[1]: https://arxiv.org/abs/2204.10965 "[2204.10965] CLIP-Dissect: Automatic Description of Neuron ... - arXiv.org"
[2]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"
[3]: https://arxiv.org/pdf/2204.10965.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the **CLIP-Dissect** technique, which consists of three main steps: **activation extraction**, **concept labeling**, and **description generation**.
- **Activation extraction** involves applying a vision network to a set of unlabeled images and recording the activation values of each neuron for each image. The paper uses ResNet-50 as the vision network and a subset of ImageNet as the unlabeled images.
- **Concept labeling** involves using a multimodal vision/language model, such as **CLIP**, to label each neuron with a concept from a large vocabulary. The paper uses CLIP-ViT-B/32 as the multimodal model and a vocabulary of 20,000 concepts derived from WordNet and Open Images. The paper also introduces a novel metric, called **conceptual alignment score (CAS)**, to measure how well a concept matches a neuron's activation pattern.
- **Description generation** involves using natural language processing techniques to generate a natural language description for each neuron based on its concept label and activation pattern. The paper uses a template-based approach that fills in the blanks with relevant information such as concept name, layer name, activation statistics, and example images. The paper also provides some qualitative examples of the generated descriptions for different layers of ResNet-50.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the vision network, the multimodal model, the vocabulary and the image set
vision_network = ResNet-50
multimodal_model = CLIP-ViT-B/32
vocabulary = 20,000 concepts from WordNet and Open Images
image_set = subset of ImageNet

# Define a function to extract activations from a vision network
def extract_activations(vision_network, image_set):
  # Initialize an empty dictionary to store activations
  activations = {}
  # Loop over each layer in the vision network
  for layer in vision_network.layers:
    # Initialize an empty list to store activations for each neuron in the layer
    activations[layer] = []
    # Loop over each neuron in the layer
    for neuron in layer.neurons:
      # Initialize an empty list to store activations for each image in the image set
      activations[layer][neuron] = []
      # Loop over each image in the image set
      for image in image_set:
        # Apply the vision network to the image and record the activation value of the neuron
        activation = vision_network(image)[layer][neuron]
        # Append the activation value to the list
        activations[layer][neuron].append(activation)
  # Return the dictionary of activations
  return activations

# Define a function to label neurons with concepts from a vocabulary using a multimodal model
def label_neurons(activations, multimodal_model, vocabulary):
  # Initialize an empty dictionary to store labels
  labels = {}
  # Loop over each layer in the activations dictionary
  for layer in activations.keys():
    # Initialize an empty list to store labels for each neuron in the layer
    labels[layer] = []
    # Loop over each neuron in the layer
    for neuron in range(len(activations[layer])):
      # Initialize an empty list to store scores for each concept in the vocabulary
      scores = []
      # Loop over each concept in the vocabulary
      for concept in vocabulary:
        # Encode the concept as a text input for the multimodal model
        text_input = multimodal_model.encode_text(concept)
        # Encode the activation pattern of the neuron as a visual input for the multimodal model
        visual_input = multimodal_model.encode_visual(activations[layer][neuron])
        # Compute the similarity score between the text input and the visual input using cosine similarity
        score = cosine_similarity(text_input, visual_input)
        # Append the score to the list
        scores.append(score)
      # Find the index of the concept with the highest score
      best_index = argmax(scores)
      # Find the name of the concept with the highest score
      best_concept = vocabulary[best_index]
      # Append the best concept to the list of labels for the layer
      labels[layer].append(best_concept)
  # Return the dictionary of labels
  return labels

# Define a function to generate descriptions for neurons based on their labels and activations
def generate_descriptions(labels, activations):
  # Initialize an empty dictionary to store descriptions
  descriptions = {}
  # Loop over each layer in the labels dictionary
  for layer in labels.keys():
    # Initialize an empty list to store descriptions for each neuron in the layer
    descriptions[layer] = []
    # Loop over each neuron in the layer
    for neuron in range(len(labels[layer])):
      # Get the concept label for the neuron
      concept = labels[layer][neuron]
      # Get some statistics about the activation pattern of the neuron, such as mean, standard deviation, maximum and minimum values
      mean = mean(activations[layer][neuron])
      std = std(activations[layer][neuron])
      max = max(activations[layer][neuron])
      min = min(activations[layer][neuron])
      # Get some example images that activate the neuron strongly or weakly, such as top-k and bottom-k images based on activation values
      top_k_images = get_top_k_images(activations[layer][neuron], k=5)
      bottom_k_images = get_bottom_k_images(activations[layer][neuron], k=5)
      # Use a template-based approach to fill in a natural language description with relevant information, such as concept name, layer name, activation statistics and example images. For example:
      description = f"This neuron in {layer} detects {concept}. It has a mean activation of {mean} and a standard deviation of {std}. It is most activated by images like {top_k_images} and least activated by images like {bottom_k_images}."
      # Append the description to the list of descriptions for the layer
      descriptions[layer].append(description)
  # Return the dictionary of descriptions
  return descriptions

# Apply the functions to the vision network, the multimodal model, the vocabulary and the image set
activations = extract_activations(vision_network, image_set)
labels = label_neurons(activations, multimodal_model, vocabulary)
descriptions = generate_descriptions(labels, activations)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for vision models and datasets
import clip # for multimodal models
import numpy as np # for numerical operations
import scipy # for cosine similarity
import nltk # for natural language processing
from nltk.corpus import wordnet # for WordNet concepts
from nltk.corpus import stopwords # for removing stopwords
from PIL import Image # for image processing

# Define the vision network, the multimodal model, the vocabulary and the image set
vision_network = torchvision.models.resnet50(pretrained=True) # use a pretrained ResNet-50 model
multimodal_model, preprocess = clip.load("ViT-B/32", device="cuda") # use a pretrained CLIP-ViT-B/32 model on GPU
vocabulary = get_vocabulary() # get a vocabulary of 20,000 concepts from WordNet and Open Images
image_set = torchvision.datasets.ImageNet(root="data", split="val") # use a subset of ImageNet validation set

# Define a function to get a vocabulary of 20,000 concepts from WordNet and Open Images
def get_vocabulary():
  # Initialize an empty set to store concepts
  concepts = set()
  # Loop over each synset in WordNet
  for synset in wordnet.all_synsets():
    # Get the name of the synset
    name = synset.name()
    # Remove the part of speech and sense number from the name
    name = name.split(".")[0]
    # Replace underscores with spaces in the name
    name = name.replace("_", " ")
    # Add the name to the set of concepts
    concepts.add(name)
  # Loop over each line in the Open Images class descriptions file
  for line in open("class-descriptions-boxable.csv"):
    # Split the line by comma and get the second element as the name of the class
    name = line.split(",")[1]
    # Remove the newline character from the name
    name = name.strip()
    # Add the name to the set of concepts
    concepts.add(name)
  # Convert the set of concepts to a list and sort it alphabetically
  concepts = sorted(list(concepts))
  # Return the first 20,000 concepts as the vocabulary
  return concepts[:20000]

# Define a function to extract activations from a vision network
def extract_activations(vision_network, image_set):
  # Initialize an empty dictionary to store activations
  activations = {}
  # Define a hook function to record activations for each layer
  def hook_fn(layer):
    def hook(model, input, output):
      activations[layer] = output.detach()
    return hook
  # Register the hook function for each layer in the vision network that we want to analyze (conv1, layer1, layer2, layer3 and layer4)
  vision_network.conv1.register_forward_hook(hook_fn("conv1"))
  vision_network.layer1.register_forward_hook(hook_fn("layer1"))
  vision_network.layer2.register_forward_hook(hook_fn("layer2"))
  vision_network.layer3.register_forward_hook(hook_fn("layer3"))
  vision_network.layer4.register_forward_hook(hook_fn("layer4"))
  # Loop over each image in the image set
  for image, label in image_set:
    # Preprocess the image to match the input format of the vision network
    image = preprocess_image(image)
    # Apply the vision network to the image and record the activations for each layer using the hook functions
    vision_network(image)
  # Return the dictionary of activations
  return activations

# Define a function to preprocess an image to match the input format of the vision network
def preprocess_image(image):
  # Resize the image to 224 x 224 pixels using bicubic interpolation
  image = image.resize((224, 224), Image.BICUBIC)
  # Convert the image to a tensor and normalize it using mean and standard deviation values from ImageNet
  image = torchvision.transforms.ToTensor()(image)
  image = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)
  # Add a batch dimension to the image tensor and move it to GPU
  image = image.unsqueeze(0).to("cuda")
  # Return the preprocessed image tensor
  return image

# Define a function to label neurons with concepts from a vocabulary using a multimodal model
def label_neurons(activations, multimodal_model, vocabulary):
  # Initialize an empty dictionary to store labels
  labels = {}
  # Loop over each layer in the activations dictionary
  for layer in activations.keys():
    # Initialize an empty list to store labels for each neuron in the layer
    labels[layer] = []
    # Get the number of neurons in the layer
    num_neurons = activations[layer].shape[1]
    # Loop over each neuron in the layer
    for neuron in range(num_neurons):
      # Initialize an empty list to store scores for each concept in the vocabulary
      scores = []
      # Loop over each concept in the vocabulary
      for concept in vocabulary:
        # Encode the concept as a text input for the multimodal model
        text_input = clip.tokenize([concept]).to("cuda")
        # Encode the activation pattern of the neuron as a visual input for the multimodal model
        visual_input = activations[layer][:,neuron,:,:].unsqueeze(1)
        # Compute the similarity score between the text input and the visual input using the multimodal model
        score = multimodal_model(text_input, visual_input).item()
        # Append the score to the list
        scores.append(score)
      # Find the index of the concept with the highest score
      best_index = np.argmax(scores)
      # Find the name of the concept with the highest score
      best_concept = vocabulary[best_index]
      # Append the best concept to the list of labels for the layer
      labels[layer].append(best_concept)
  # Return the dictionary of labels
  return labels

# Define a function to generate descriptions for neurons based on their labels and activations
def generate_descriptions(labels, activations):
  # Initialize an empty dictionary to store descriptions
  descriptions = {}
  # Loop over each layer in the labels dictionary
  for layer in labels.keys():
    # Initialize an empty list to store descriptions for each neuron in the layer
    descriptions[layer] = []
    # Get the number of neurons in the layer
    num_neurons = len(labels[layer])
    # Loop over each neuron in the layer
    for neuron in range(num_neurons):
      # Get the concept label for the neuron
      concept = labels[layer][neuron]
      # Get some statistics about the activation pattern of the neuron, such as mean, standard deviation, maximum and minimum values
      mean = torch.mean(activations[layer][:,neuron,:,:]).item()
      std = torch.std(activations[layer][:,neuron,:,:]).item()
      max = torch.max(activations[layer][:,neuron,:,:]).item()
      min = torch.min(activations[layer][:,neuron,:,:]).item()
      # Get some example images that activate the neuron strongly or weakly, such as top-k and bottom-k images based on activation values
      top_k_images = get_top_k_images(activations[layer][:,neuron,:,:], k=5)
      bottom_k_images = get_bottom_k_images(activations[layer][:,neuron,:,:], k=5)
      # Use a template-based approach to fill in a natural language description with relevant information, such as concept name, layer name, activation statistics and example images. For example:
      description = f"This neuron in {layer} detects {concept}. It has a mean activation of {mean:.2f} and a standard deviation of {std:.2f}. It is most activated by images like {top_k_images} and least activated by images like {bottom_k_images}."
      # Append the description to the list of descriptions for the layer
      descriptions[layer].append(description)
  # Return the dictionary of descriptions
  return descriptions

# Define a function to get top-k images that activate a neuron strongly based on activation values
def get_top_k_images(activation, k):
  # Flatten the activation tensor to a one-dimensional vector
  activation = activation.flatten()
  # Get the indices of the top-k activation values using torch.topk function
  indices = torch.topk(activation, k).indices
  # Initialize an empty list to store top-k images
  top_k_images = []
  # Loop over each index in indices
  for index in indices:
    # Get the image from the image set corresponding to the index
    image, label = image_set[index]
    # Convert the image to a numpy array and normalize it to [0, 255] range
    image = np.array(image)
    image = (image - image.min()) / (image.max() - image.min()) * 255
    # Convert the image to an unsigned 8-bit integer type
    image = image.astype(np.uint8)
    # Append the image to the list of top-k images
    top_k_images.append(image)
  # Return the list of top-k images
  return top_k_images

# Define a function to get bottom-k images that activate a neuron weakly based on activation values
def get_bottom_k_images(activation, k):
  # Flatten the activation tensor to