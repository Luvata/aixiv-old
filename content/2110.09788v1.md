---
title: 2110.09788v1 CIPS-3D  A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis
date: 2021-10-10
---

# [CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis](http://arxiv.org/abs/2110.09788v1)

authors: Peng Zhou, Lingxi Xie, Bingbing Ni, Qi Tian


## What, Why and How

[1]: https://arxiv.org/pdf/2110.09788v1.pdf "arXiv:2110.09788v1 [cs.CV] 19 Oct 2021"
[2]: https://arxiv.org/abs/2110.09788 "[2110.09788] CIPS-3D: A 3D-Aware Generator of GANs Based on ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2211.09788v1 "[2211.09788v1] DiffusionDet: Diffusion Model for Object Detection"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents CIPS-3D, a style-based, 3D-aware generator of GANs based on conditionally-independent pixel synthesis.
- **Why**: The paper aims to address the limitations of existing 3D-aware GANs that either lack explicit and precise control over camera poses (such as StyleGAN) or generate low-quality images with artifacts (such as NeRF-based GANs).
- **How**: The paper proposes a novel generator that consists of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. The paper also introduces an auxiliary discriminator to solve the problem of mirror symmetry that implies a suboptimal solution. The paper evaluates the proposed method on FFHQ dataset and demonstrates its advantages over existing methods in terms of image quality, pose control, transfer learning and face stylization.

## Main Contributions

The paper claims the following contributions:

- It proposes a novel style-based, 3D-aware generator of GANs that synthesizes each pixel value independently without any spatial convolution or upsampling operation.
- It introduces an auxiliary discriminator to solve the problem of mirror symmetry that implies a suboptimal solution for 3D-aware image synthesis.
- It sets new records for 3D-aware image synthesis with an impressive FID of 6.97 for images at the 256  256 resolution on FFHQ.
- It demonstrates several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization.

## Method Summary

[1]: https://arxiv.org/pdf/2110.09788v1.pdf "arXiv:2110.09788v1 [cs.CV] 19 Oct 2021"
[2]: https://arxiv.org/abs/2110.09788 "[2110.09788] CIPS-3D: A 3D-Aware Generator of GANs Based on ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2211.09788v1 "[2211.09788v1] DiffusionDet: Diffusion Model for Object Detection"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a novel generator that consists of a **shallow NeRF network** and a **deep implicit neural representation (INR) network**. The NeRF network takes the camera pose and a latent code as inputs and outputs a 3D feature volume. The INR network takes the 3D feature volume and a style code as inputs and outputs the pixel value for each location on the image plane. The generator synthesizes each pixel value **independently** without any spatial convolution or upsampling operation.
- The paper introduces an **auxiliary discriminator** to solve the problem of mirror symmetry that implies a suboptimal solution for 3D-aware image synthesis. The auxiliary discriminator takes the camera pose and the generated image as inputs and outputs a score indicating how well the image matches the pose. The paper also modifies the original StyleGAN discriminator to take both the camera pose and the image as inputs.
- The paper trains the proposed model on raw, single-view images using an **adversarial loss** and an **auxiliary loss**. The adversarial loss consists of two terms: one for the original StyleGAN discriminator and one for the auxiliary discriminator. The auxiliary loss is a weighted sum of three terms: a **pose consistency loss**, a **style consistency loss**, and a **perceptual loss**. The pose consistency loss encourages the generated images to be consistent with different camera poses. The style consistency loss encourages the generated images to be consistent with different style codes. The perceptual loss encourages the generated images to be similar to real images in terms of high-level features.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator network
def generator(pose, latent_code, style_code):
  # Generate a 3D feature volume from the pose and the latent code using a shallow NeRF network
  feature_volume = NeRF(pose, latent_code)
  # Generate a pixel value for each location on the image plane from the feature volume and the style code using a deep INR network
  pixel_value = INR(feature_volume, style_code)
  # Return the generated pixel value
  return pixel_value

# Define the original StyleGAN discriminator network
def discriminator_1(pose, image):
  # Concatenate the pose and the image along the channel dimension
  input = concat(pose, image)
  # Compute a score indicating how realistic the image is using a convolutional network
  score = conv(input)
  # Return the score
  return score

# Define the auxiliary discriminator network
def discriminator_2(pose, image):
  # Compute a score indicating how well the image matches the pose using a convolutional network
  score = conv(pose, image)
  # Return the score
  return score

# Define the adversarial loss function
def adversarial_loss(generator, discriminator_1, discriminator_2, pose, latent_code, style_code, real_image):
  # Generate a fake image from the pose, the latent code and the style code using the generator
  fake_image = generator(pose, latent_code, style_code)
  # Compute the score for the fake image using both discriminators
  fake_score_1 = discriminator_1(pose, fake_image)
  fake_score_2 = discriminator_2(pose, fake_image)
  # Compute the score for the real image using both discriminators
  real_score_1 = discriminator_1(pose, real_image)
  real_score_2 = discriminator_2(pose, real_image)
  # Compute the adversarial loss as the sum of two hinge losses
  loss = hinge_loss(fake_score_1 - real_score_1) + hinge_loss(fake_score_2 - real_score_2)
  # Return the loss
  return loss

# Define the pose consistency loss function
def pose_consistency_loss(generator, pose_1, pose_2, latent_code, style_code):
  # Generate two images from different poses but same latent code and style code using the generator
  image_1 = generator(pose_1, latent_code, style_code)
  image_2 = generator(pose_2, latent_code, style_code)
  # Compute the L1 distance between the two images
  loss = L1(image_1 - image_2)
  # Return the loss
  return loss

# Define the style consistency loss function
def style_consistency_loss(generator, pose, latent_code, style_code_1, style_code_2):
  # Generate two images from same pose and latent code but different style codes using the generator
  image_1 = generator(pose, latent_code, style_code_1)
  image_2 = generator(pose, latent_code, style_code_2)
  # Compute the L1 distance between the two images
  loss = L1(image_1 - image_2)
  # Return the loss
  return loss

# Define the perceptual loss function
def perceptual_loss(generator, pose, latent_code, style_code, real_image):
  # Generate a fake image from the pose, the latent code and the style code using the generator
  fake_image = generator(pose, latent_code, style_code)
  # Extract high-level features from both fake and real images using a pre-trained VGG network
  fake_features = VGG(fake_image)
  real_features = VGG(real_image)
  # Compute the L2 distance between the features
  loss = L2(fake_features - real_features)
  # Return the loss
  return loss

# Define the auxiliary loss function as a weighted sum of three terms
def auxiliary_loss(generator, pose_1, pose_2, latent_code, style_code_1, style_code_2):
   return lambda_p * pose_consistency_loss(generator) + lambda_s * style_consistency_loss(generator) + lambda_per * perceptual_loss(generator)

# Train the model using gradient descent on both adversarial and auxiliary losses
for epoch in epochs:
   for batch in batches:
      # Sample random poses and codes from prior distributions
      pose = sample_pose()
      latent_code = sample_latent()
      style_code = sample_style()
      # Get corresponding real images from dataset
      real_image = get_real_image(pose)
      # Update the generator parameters by minimizing the adversarial loss and the auxiliary loss
      generator_loss = adversarial_loss(generator, discriminator_1, discriminator_2, pose, latent_code, style_code, real_image) + auxiliary_loss(generator, pose_1, pose_2, latent_code, style_code_1, style_code_2)
      generator_params = gradient_descent(generator_params, generator_loss)
      # Update the discriminator parameters by minimizing the adversarial loss
      discriminator_loss = adversarial_loss(generator, discriminator_1, discriminator_2, pose, latent_code, style_code, real_image)
      discriminator_params = gradient_descent(discriminator_params, discriminator_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np

# Define the hyperparameters
num_epochs = 100 # number of training epochs
batch_size = 16 # size of mini-batches
image_size = 256 # size of image resolution
latent_dim = 512 # dimension of latent code
style_dim = 512 # dimension of style code
feature_dim = 64 # dimension of 3D feature volume
num_layers = 12 # number of layers in INR network
num_channels = 3 # number of channels in images
lambda_p = 0.1 # weight for pose consistency loss
lambda_s = 0.1 # weight for style consistency loss
lambda_per = 0.1 # weight for perceptual loss

# Define the NeRF network as a fully-connected network with ReLU activations and skip connections
class NeRF(nn.Module):
   def __init__(self, latent_dim, feature_dim):
      super(NeRF, self).__init__()
      self.fc1 = nn.Linear(3 + latent_dim, feature_dim) # input: (x, y, z) coordinates and latent code
      self.fc2 = nn.Linear(feature_dim, feature_dim)
      self.fc3 = nn.Linear(feature_dim + 3 + latent_dim, feature_dim) # skip connection
      self.fc4 = nn.Linear(feature_dim, feature_dim)

   def forward(self, x):
      x1 = F.relu(self.fc1(x))
      x2 = F.relu(self.fc2(x1))
      x3 = F.relu(self.fc3(torch.cat([x2, x], dim=-1))) # concatenate along the last dimension
      x4 = F.relu(self.fc4(x3))
      return x4

# Define the INR network as a fully-connected network with ReLU activations and skip connections
class INR(nn.Module):
   def __init__(self, num_layers, feature_dim, style_dim, num_channels):
      super(INR, self).__init__()
      self.num_layers = num_layers
      self.feature_dim = feature_dim
      self.style_dim = style_dim
      self.num_channels = num_channels
      self.fcs = nn.ModuleList() # a list of fully-connected layers
      for i in range(num_layers):
         if i == 0:
            self.fcs.append(nn.Linear(feature_dim + style_dim, feature_dim)) # input: feature volume and style code
         elif i == num_layers - 1:
            self.fcs.append(nn.Linear(feature_dim, num_channels)) # output: pixel value
         else:
            self.fcs.append(nn.Linear(feature_dim, feature_dim))

   def forward(self, x):
      for i in range(self.num_layers):
         if i == 0:
            x = F.relu(self.fcs[i](x))
         elif i == self.num_layers - 1:
            x = torch.sigmoid(self.fcs[i](x)) # apply sigmoid activation to output pixel value in [0, 1]
         else:
            x = F.relu(self.fcs[i](x) + x) # add residual connection
      return x

# Define the generator network as a combination of NeRF and INR networks
class Generator(nn.Module):
   def __init__(self, latent_dim, style_dim, feature_dim, num_layers, num_channels):
      super(Generator, self).__init__()
      self.latent_dim = latent_dim
      self.style_dim = style_dim
      self.feature_dim = feature_dim
      self.num_layers = num_layers
      self.num_channels = num_channels
      self.nerf = NeRF(latent_dim, feature_dim) # a NeRF network to generate a 3D feature volume from pose and latent code
      self.inr = INR(num_layers, feature_dim, style_dim, num_channels) # an INR network to generate a pixel value from feature volume and style code

   def forward(self, pose, latent_code, style_code):
      # Generate a grid of (x, y, z) coordinates on the image plane according to the pose matrix
      grid = generate_grid(pose)
      # Concatenate the grid and the latent code along the last dimension
      input_1 = torch.cat([grid, latent_code], dim=-1)
      # Generate a 3D feature volume from the input using the NeRF network
      feature_volume = self.nerf(input_1)
      # Concatenate the feature volume and the style code along the last dimension
      input_2 = torch.cat([feature_volume, style_code], dim=-1)
      # Generate a pixel value for each location on the image plane from the input using the INR network
      pixel_value = self.inr(input_2)
      # Reshape the pixel value to a 2D image
      image = pixel_value.view(-1, self.num_channels, image_size, image_size)
      # Return the generated image
      return image

# Define the original StyleGAN discriminator network as a convolutional network with residual blocks
class Discriminator_1(nn.Module):
   def __init__(self, num_channels):
      super(Discriminator_1, self).__init__()
      self.num_channels = num_channels
      self.convs = nn.ModuleList() # a list of convolutional layers
      self.convs.append(nn.Conv2d(num_channels + 3, 32, 3, 1, 1)) # input: image and pose concatenated along the channel dimension
      self.convs.append(nn.Conv2d(32, 64, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(64, 128, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(128, 256, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(256, 512, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(512, 1024, 4, 2, 1)) # downsample by a factor of 2
      self.fc = nn.Linear(1024 * 4 * 4, 1) # output: a score indicating how realistic the image is

   def forward(self, pose, image):
      # Concatenate the pose and the image along the channel dimension
      input = torch.cat([pose, image], dim=1)
      # Apply convolutional layers with leaky ReLU activations and residual connections
      x = input
      for i in range(len(self.convs)):
         x = F.leaky_relu(self.convs[i](x), 0.2)
         if i > 0 and i % 2 == 0:
            x = x + F.interpolate(input, scale_factor=0.5 ** (i // 2), mode='bilinear') # add skip connection from input
      # Flatten the output and apply a fully-connected layer
      x = x.view(-1, 1024 * 4 * 4)
      x = self.fc(x)
      # Return the score
      return x

# Define the auxiliary discriminator network as a convolutional network with residual blocks
class Discriminator_2(nn.Module):
   def __init__(self, num_channels):
      super(Discriminator_2, self).__init__()
      self.num_channels = num_channels
      self.convs = nn.ModuleList() # a list of convolutional layers
      self.convs.append(nn.Conv2d(num_channels + 3, 32, 3, 1, 1)) # input: image and pose concatenated along the channel dimension
      self.convs.append(nn.Conv2d(32, 64, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(64, 128, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(128, 256, 4, 2, 1)) # downsample by a factor of 2
      self.convs.append(nn.Conv2d(256, 512, 4, 2, 1)) # downsample by a factor of 
```