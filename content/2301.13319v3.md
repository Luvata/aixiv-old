---
title: 2301.13319v3 [Work in progress
date: 2023-01-14
---

# [[Work in progress] Scalable, out-of-the box segmentation of individual particles from mineral samples acquired with micro CT](http://arxiv.org/abs/2301.13319v3)

authors: Karol Gotkowski, Shuvam Gupta, Jose R. A. Godinho, Camila G. S. Tochtrop, Klaus H. Maier-Hein, Fabian Isensee


## What, Why and How

[1]: https://arxiv.org/pdf/2301.13319v3.pdf "ParticleSeg3D: Scalable, out-of-the box segmentation of individual ..."
[2]: https://arxiv.org/abs/2301.13319 "[2301.13319] [Work in progress] Scalable, out-of-the box ... - arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2301.13319v3 "[2301.13319v3] [Work in progress] Scalable, out-of-the box segmentation ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes an instance segmentation method called ParticleSeg3D that can extract individual particles from large micro CT images taken from mineral samples embedded in an epoxy matrix.
- **Why**: The paper aims to address the need for optimizing the exploration and extraction of mineral resources by enabling an extensive characterization of the shapes, appearances and compositions of the processed particles. Current approaches rely on rudimentary postprocessing techniques to separate touching particles, which are unreliable and require retraining or reconfiguration for each new image.
- **How**: The paper builds on the powerful nnU-Net framework, introduces a particle size normalization, makes use of a border-core representation to enable instance segmentation and trains the method with a large dataset containing particles of numerous different materials and minerals. The paper demonstrates that ParticleSeg3D can be applied out-of-the box to a large variety of particle types, without further manual annotations and retraining. The paper also makes the code and dataset publicly available.

## Main Contributions

The paper claims to make the following contributions:

- A novel instance segmentation method for individual particle characterization from micro CT images of mineral samples
- A particle size normalization technique that allows the method to handle particles of varying sizes
- A border-core representation that enables the method to separate touching particles and assign them unique labels
- A large and diverse dataset of annotated micro CT images of mineral samples containing particles of different materials and minerals
- An extensive evaluation of the method on unseen mineral samples and a comparison with existing methods
- A public release of the code and dataset to facilitate further research and applications


## Method Summary

[1]: https://arxiv.org/pdf/2301.13319v3.pdf "ParticleSeg3D: Scalable, out-of-the box segmentation of individual ..."
[2]: https://arxiv.org/abs/2301.13319 "[2301.13319] [Work in progress] Scalable, out-of-the box ... - arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2301.13319v3 "[2301.13319v3] [Work in progress] Scalable, out-of-the box segmentation ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the main components of ParticleSeg3D, which are: data preprocessing, network architecture, training strategy and postprocessing.
- Data preprocessing involves cropping the input images into smaller patches, applying a particle size normalization technique that rescales the patches based on the median particle size, and augmenting the patches with random rotations, flips and elastic deformations.
- Network architecture is based on the nnU-Net framework, which adapts the U-Net architecture to different input modalities and tasks. The paper uses a 3D U-Net with residual blocks and deep supervision. The network outputs two channels: one for the border region of the particles and one for the core region. The border region is defined as a thin layer around the particle boundary, while the core region is defined as the remaining part of the particle excluding the border region. The network is trained to predict both regions simultaneously using a weighted sum of dice loss and cross entropy loss.
- Training strategy follows the nnU-Net pipeline, which includes automatic hyperparameter optimization, model selection and ensembling. The paper trains five models with different initialization seeds and ensembles them using majority voting. The paper also uses mixed precision training to reduce memory consumption and speed up training.
- Postprocessing involves thresholding the border and core predictions, applying connected component analysis to label each core region with a unique ID, and merging core regions that share a border region. The paper also applies a size filter to remove small particles that are likely to be noise or artifacts.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a micro CT image of mineral samples
# Output: an instance segmentation mask of individual particles

# Data preprocessing
patches = crop_image_into_patches(image)
patches = normalize_particle_size(patches)
patches = augment_patches(patches)

# Network architecture
network = nnUNet_3D()
border_output, core_output = network(patches)

# Training strategy
loss = weighted_sum_of_dice_loss_and_cross_entropy_loss(border_output, core_output, border_label, core_label)
optimize_network_parameters(loss)
ensemble_five_models_with_majority_voting()

# Postprocessing
border_mask = threshold(border_output)
core_mask = threshold(core_output)
core_labels = connected_component_analysis(core_mask)
instance_mask = merge_core_regions_that_share_border_region(core_labels, border_mask)
instance_mask = filter_out_small_particles(instance_mask)

# Return the instance segmentation mask
return instance_mask
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import SimpleITK as sitk
import scipy.ndimage as ndimage
import skimage.measure as measure

# Define some hyperparameters
patch_size = 128 # the size of the cropped patches
median_filter_size = 3 # the size of the median filter for particle size normalization
border_thickness = 3 # the thickness of the border region in pixels
size_threshold = 1000 # the minimum size of particles to keep in voxels

# Define the nnU-Net 3D architecture
# Refer to https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/network_architecture/neural_network.py for details
class nnUNet_3D(nn.Module):
    def __init__(self):
        super(nnUNet_3D, self).__init__()
        # Initialize the encoder and decoder blocks with residual connections and deep supervision
        self.encoder_blocks = nn.ModuleList([ResidualBlock(1, 30), ResidualBlock(30, 60), ResidualBlock(60, 120), ResidualBlock(120, 240), ResidualBlock(240, 320)])
        self.decoder_blocks = nn.ModuleList([ResidualBlock(320 + 240, 240), ResidualBlock(240 + 120, 120), ResidualBlock(120 + 60, 60), ResidualBlock(60 + 30, 30)])
        self.deep_supervision_blocks = nn.ModuleList([nn.Conv3d(240, 2, kernel_size=1), nn.Conv3d(120, 2, kernel_size=1), nn.Conv3d(60, 2, kernel_size=1)])
        # Initialize the final output layer with two channels for border and core predictions
        self.output_layer = nn.Conv3d(30, 2, kernel_size=1)

    def forward(self, x):
        # Encode the input patch with downsampling and skip connections
        skips = []
        for encoder_block in self.encoder_blocks:
            x = encoder_block(x)
            skips.append(x)
            x = F.max_pool3d(x, kernel_size=2)

        # Decode the encoded patch with upsampling and concatenation
        for i, decoder_block in enumerate(self.decoder_blocks):
            x = F.interpolate(x, scale_factor=2, mode='trilinear', align_corners=False)
            x = torch.cat([x, skips[-i-2]], dim=1)
            x = decoder_block(x)

        # Apply deep supervision to intermediate decoder outputs
        ds_outputs = []
        for i, ds_block in enumerate(self.deep_supervision_blocks):
            ds_output = ds_block(skips[-i-2])
            ds_output = F.interpolate(ds_output, size=patch_size, mode='trilinear', align_corners=False)
            ds_outputs.append(ds_output)

        # Apply the final output layer to get the border and core predictions
        output = self.output_layer(x)

        # Return the output and the deep supervision outputs
        return output, ds_outputs

# Define the residual block with convolutional layers and batch normalization
# Refer to https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/network_architecture/generic_UNet.py for details
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResidualBlock, self).__init__()
        # Initialize two convolutional layers with batch normalization and leaky ReLU activation
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm3d(out_channels)
        self.lrelu1 = nn.LeakyReLU(negative_slope=0.01)
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm3d(out_channels)
        self.lrelu2 = nn.LeakyReLU(negative_slope=0.01)
        # Initialize a skip connection if the input and output channels are different
        self.skip = None
        if in_channels != out_channels:
            self.skip = nn.Conv3d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        # Apply the first convolutional layer with batch normalization and leaky ReLU activation
        y = self.conv1(x)
        y = self.bn1(y)
        y = self.lrelu1(y)
        # Apply the second convolutional layer with batch normalization and leaky ReLU activation
        y = self.conv2(y)
        y = self.bn2(y)
        y = self.lrelu2(y)
        # Add the skip connection if it exists
        if self.skip is not None:
            x = self.skip(x)
        # Add the input and output and return
        y = x + y
        return y

# Define the weighted sum of dice loss and cross entropy loss
# Refer to https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/training/loss_functions/dice_loss.py for details
def weighted_sum_of_dice_loss_and_cross_entropy_loss(output, target, border_weight, core_weight, smooth=1e-5):
    # Flatten the output and target tensors
    output = output.view(-1)
    target = target.view(-1)
    # Compute the dice loss for border and core channels separately
    border_output = output[0::2]
    border_target = target[0::2]
    border_intersection = (border_output * border_target).sum()
    border_dice_loss = 1 - (2 * border_intersection + smooth) / (border_output.sum() + border_target.sum() + smooth)
    core_output = output[1::2]
    core_target = target[1::2]
    core_intersection = (core_output * core_target).sum()
    core_dice_loss = 1 - (2 * core_intersection + smooth) / (core_output.sum() + core_target.sum() + smooth)
    # Compute the cross entropy loss for border and core channels separately
    border_cross_entropy_loss = F.binary_cross_entropy(border_output, border_target)
    core_cross_entropy_loss = F.binary_cross_entropy(core_output, core_target)
    # Compute the weighted sum of dice loss and cross entropy loss for border and core channels separately
    border_loss = border_weight * (border_dice_loss + border_cross_entropy_loss)
    core_loss = core_weight * (core_dice_loss + core_cross_entropy_loss)
    # Compute the total loss as the sum of border and core losses
    total_loss = border_loss + core_loss
    # Return the total loss
    return total_loss

# Define the function to crop the image into patches
def crop_image_into_patches(image):
    # Initialize an empty list to store the patches
    patches = []
    # Loop over the image with a sliding window of patch size and a stride of half patch size
    for i in range(0, image.shape[0], patch_size // 2):
        for j in range(0, image.shape[1], patch_size // 2):
            for k in range(0, image.shape[2], patch_size // 2):
                # Extract a patch from the image
                patch = image[i:i+patch_size, j:j+patch_size, k:k+patch_size]
                # If the patch is not full size, pad it with zeros
                if patch.shape != (patch_size, patch_size, patch_size):
                    patch = np.pad(patch, [(0, patch_size - patch.shape[0]), (0, patch_size - patch.shape[1]), (0, patch_size - patch.shape[2])], mode='constant')
                # Append the patch to the list of patches
                patches.append(patch)
    # Convert the list of patches to a numpy array
    patches = np.array(patches)
    # Return the patches
    return patches

# Define the function to normalize the particle size based on the median particle size
def normalize_particle_size(patches):
    # Initialize an empty list to store the normalized patches
    normalized_patches = []
    # Loop over each patch in the patches array
    for patch in patches:
        # Convert the patch to a SimpleITK image object
        sitk_patch = sitk.GetImageFromArray(patch)
        # Apply a median filter to smooth the patch and reduce noise
        sitk_patch = sitk.Median(sitk_patch, [median_filter_size] * 3)
        # Apply a threshold to binarize the patch
        sitk_patch = sitk.BinaryThreshold(sitk_patch, lowerThreshold=0.5, upperThreshold=1.0, insideValue=1.0, outsideValue=0.0)
        # Apply a connected component analysis to label each particle with a unique ID
        sitk_patch = sitk.ConnectedComponent(sitk_patch)
        # Convert the SimpleITK image object back to a numpy array
        patch = sitk.GetArrayFromImage(sitk_patch)
        # Compute the median particle size in voxels
        median_particle_size = np.median(np.bincount(patch.flatten())[1:])
        # Compute the scaling factor based on the median particle size and a target particle size of 32 voxels
        scaling_factor = 32 / median_particle_size