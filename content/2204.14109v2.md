---
title: 2204.14109v2 TEMOS  Generating diverse human motions from textual descriptions
date: 2022-04-15
---

# [TEMOS: Generating diverse human motions from textual descriptions](http://arxiv.org/abs/2204.14109v2)

authors: Mathis Petrovich, Michael J. Black, GÃ¼l Varol


## What, Why and How

[1]: https://arxiv.org/abs/2204.14109v2 "[2204.14109v2] TEMOS: Generating diverse human motions from textual ..."
[2]: https://arxiv.org/pdf/2204.14109v2 "arXiv:2204.14109v2 [cs.CV] 22 Jul 2022"
[3]: http://export.arxiv.org/abs/2204.14109v2 "[2204.14109v2] TEMOS: Generating diverse human motions from textual ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes TEMOS, a text-conditioned generative model that can produce diverse 3D human motions from textual descriptions.
- **Why**: The paper aims to address the problem of generating realistic and varied human motions from natural language input, which has many applications in virtual and real worlds.
- **How**: The paper leverages a variational autoencoder (VAE) trained with human motion data, and a text encoder that produces distribution parameters compatible with the VAE latent space. The paper shows that TEMOS can generate both skeleton-based animations and more expressive SMPL body motions, and evaluates its performance on the KIT Motion-Language benchmark.

## Main Contributions

[1]: https://arxiv.org/abs/2204.14109 "TEMOS: Generating diverse human motions from textual descriptions"
[2]: https://link.springer.com/chapter/10.1007/978-3-031-20047-2_28 "TEMOS : Generating Diverse Human Motions from Textual ... - Springer"
[3]: https://paperswithcode.com/paper/temos-generating-diverse-human-motions-from "TEMOS: Generating diverse human motions from textual descriptions"

According to the paper[^1^][1], the contributions are the following:

- The paper presents **TEMOS**, a novel cross-modal variational model that can produce diverse 3D human movements given textual descriptions in natural language[^2^][2].
- The paper provides an extensive ablation study of the model components and outperforms the state of the art by a large margin on the KIT Motion-Language benchmark[^2^][2].
- The paper shows that TEMOS can generate both skeleton-based animations and more expressive SMPL body motions[^1^][1] [^2^][2].
- The paper releases code and models on their webpage[^1^][1].

## Method Summary

[1]: https://www.scribbr.com/apa-style/methods-section/ "How to Write an APA Methods Section | With Examples - Scribbr"
[2]: https://cancer.dartmouth.edu/sites/default/files/2019-05/methods-section.pdf "How to Write the Methods Section of a Research Paper"
[3]: https://www.verywellmind.com/how-to-write-a-method-section-2795726 "How to Write a Methods Section of an APA Paper - Verywell Mind"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses the subheadings of "Text Encoder", "Motion Decoder", and "Training" to structure the method section.
- The paper describes the text encoder as a transformer-based model that encodes a textual description into a latent vector that represents the distribution parameters of a Gaussian distribution compatible with the VAE latent space.
- The paper describes the motion decoder as a transformer-based model that decodes a latent vector sampled from the text encoder into a sequence of 3D human poses, either in skeleton or SMPL format.
- The paper describes the training procedure as a combination of VAE training with human motion data and text encoder training with textual descriptions. The paper also explains how they handle different motion durations and how they evaluate the diversity and quality of the generated motions.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the text encoder model
text_encoder = TransformerEncoder(vocab_size, hidden_size, num_layers)

# Define the motion decoder model
motion_decoder = TransformerDecoder(output_size, hidden_size, num_layers)

# Define the VAE loss function
vae_loss = reconstruction_loss + kl_divergence

# Define the diversity metric
diversity = average_pairwise_distance

# Define the quality metric
quality = mean_per_joint_position_error

# Load the human motion data and the textual descriptions
motion_data = load_motion_data()
text_data = load_text_data()

# Train the VAE with human motion data
for epoch in range(num_epochs):
  for batch in motion_data:
    # Encode the motion sequence into a latent vector
    mu, log_var = text_encoder(batch)
    # Sample a latent vector from the Gaussian distribution
    z = sample(mu, log_var)
    # Decode the latent vector into a reconstructed motion sequence
    output = motion_decoder(z)
    # Compute the VAE loss
    loss = vae_loss(output, batch, mu, log_var)
    # Update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Train the text encoder with textual descriptions
for epoch in range(num_epochs):
  for batch in text_data:
    # Encode the textual description into a latent vector
    mu, log_var = text_encoder(batch)
    # Sample a latent vector from the Gaussian distribution
    z = sample(mu, log_var)
    # Decode the latent vector into a motion sequence
    output = motion_decoder(z)
    # Compute the diversity and quality metrics
    div = diversity(output)
    qual = quality(output)
    # Update the model parameters
    optimizer.zero_grad()
    div.backward()
    qual.backward()
    optimizer.step()

# Generate diverse human motions from textual descriptions
def generate(text):
  # Encode the text into a latent vector
  mu, log_var = text_encoder(text)
  # Sample multiple latent vectors from the Gaussian distribution
  zs = sample_multiple(mu, log_var)
  # Decode each latent vector into a motion sequence
  outputs = [motion_decoder(z) for z in zs]
  # Return the generated motions
  return outputs

```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define some hyperparameters
vocab_size = 10000 # the size of the vocabulary
hidden_size = 512 # the hidden size of the transformer models
num_layers = 6 # the number of layers in the transformer models
output_size = 72 # the output size of the motion decoder (24 joints x 3 coordinates)
num_epochs = 100 # the number of epochs for training
batch_size = 32 # the batch size for training
learning_rate = 0.001 # the learning rate for training
num_samples = 10 # the number of samples to generate from each text

# Define the text encoder model as a transformer encoder
class TextEncoder(nn.Module):
  def __init__(self, vocab_size, hidden_size, num_layers):
    super(TextEncoder, self).__init__()
    # Embed the input tokens into hidden vectors
    self.embedding = nn.Embedding(vocab_size, hidden_size)
    # Apply a positional encoding to capture the order of the tokens
    self.positional_encoding = nn.PositionalEncoding(hidden_size)
    # Stack multiple transformer encoder layers
    self.transformer_encoder = nn.TransformerEncoder(hidden_size, num_layers)
    # Output two linear layers to produce the mean and log variance of the Gaussian distribution
    self.mean_layer = nn.Linear(hidden_size, hidden_size)
    self.log_var_layer = nn.Linear(hidden_size, hidden_size)

  def forward(self, x):
    # x is a batch of token ids of shape (batch_size, max_length)
    # Embed the tokens into hidden vectors
    x = self.embedding(x) # shape: (batch_size, max_length, hidden_size)
    # Apply positional encoding
    x = self.positional_encoding(x) # shape: (batch_size, max_length, hidden_size)
    # Transpose x to match the transformer encoder input shape
    x = x.transpose(0, 1) # shape: (max_length, batch_size, hidden_size)
    # Pass x through the transformer encoder
    x = self.transformer_encoder(x) # shape: (max_length, batch_size, hidden_size)
    # Take the last hidden vector as the representation of the whole sequence
    x = x[-1] # shape: (batch_size, hidden_size)
    # Compute the mean and log variance of the Gaussian distribution
    mu = self.mean_layer(x) # shape: (batch_size, hidden_size)
    log_var = self.log_var_layer(x) # shape: (batch_size, hidden_size)
    return mu, log_var

# Define the motion decoder model as a transformer decoder
class MotionDecoder(nn.Module):
  def __init__(self, output_size, hidden_size, num_layers):
    super(MotionDecoder, self).__init__()
    # Stack multiple transformer decoder layers
    self.transformer_decoder = nn.TransformerDecoder(hidden_size, num_layers)
    # Output a linear layer to produce the motion sequence
    self.output_layer = nn.Linear(hidden_size, output_size)

  def forward(self, z):
    # z is a batch of latent vectors of shape (batch_size, hidden_size)
    # Transpose z to match the transformer decoder input shape
    z = z.transpose(0, 1) # shape: (1, batch_size, hidden_size)
    # Pass z through the transformer decoder
    z = self.transformer_decoder(z) # shape: (1, batch_size, hidden_size)
    # Transpose z back to match the output shape
    z = z.transpose(0, 1) # shape: (batch_size, 1, hidden_size)
    # Compute the output motion sequence
    output = self.output_layer(z) # shape: (batch_size, 1, output_size)
    return output

# Define a function to sample a latent vector from a Gaussian distribution
def sample(mu, log_var):
  # mu and log_var are tensors of shape (batch_size, hidden_size)
  # Compute the standard deviation from the log variance
  std = torch.exp(0.5 * log_var) # shape: (batch_size, hidden_size)
  # Sample a random noise vector from a standard normal distribution
  eps = torch.randn_like(std) # shape: (batch_size, hidden_size)
  # Reparameterize the latent vector using mu and std
  z = mu + eps * std # shape: (batch_size, hidden_size)
  return z

# Define a function to sample multiple latent vectors from a Gaussian distribution
def sample_multiple(mu, log_var, num_samples):
  # mu and log_var are tensors of shape (batch_size, hidden_size)
  # num_samples is an integer
  # Repeat mu and log_var num_samples times along the first dimension
  mu = mu.repeat(num_samples, 1) # shape: (num_samples * batch_size, hidden_size)
  log_var = log_var.repeat(num_samples, 1) # shape: (num_samples * batch_size, hidden_size)
  # Sample a latent vector for each repeated pair of mu and log_var
  zs = sample(mu, log_var) # shape: (num_samples * batch_size, hidden_size)
  return zs

# Define the VAE loss function as a combination of reconstruction loss and KL divergence
def vae_loss(output, target, mu, log_var):
  # output and target are tensors of shape (batch_size, 1, output_size)
  # mu and log_var are tensors of shape (batch_size, hidden_size)
  # Compute the reconstruction loss as the mean squared error between output and target
  reconstruction_loss = nn.MSELoss()(output, target) # scalar
  # Compute the KL divergence between the Gaussian distribution and the standard normal distribution
  kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) # scalar
  # Combine the two losses with a weight factor of 0.001
  loss = reconstruction_loss + 0.001 * kl_divergence # scalar
  return loss

# Define the diversity metric as the average pairwise distance between generated motions
def diversity(outputs):
  # outputs is a tensor of shape (num_samples * batch_size, 1, output_size)
  # Compute the pairwise distance matrix between outputs
  distances = torch.cdist(outputs, outputs) # shape: (num_samples * batch_size, num_samples * batch_size)
  # Compute the average pairwise distance
  div = torch.mean(distances) # scalar
  return div

# Define the quality metric as the mean per joint position error between generated motions and ground truth motions
def quality(outputs, targets):
  # outputs is a tensor of shape (num_samples * batch_size, 1, output_size)
  # targets is a tensor of shape (batch_size, output_size)
  # Repeat targets num_samples times along the first dimension
  targets = targets.repeat(num_samples, 1) # shape: (num_samples * batch_size, output_size)
  # Reshape outputs and targets to have three dimensions for joints
  outputs = outputs.view(-1, output_size // 3, 3) # shape: (num_samples * batch_size, num_joints, num_coordinates)
  targets = targets.view(-1, output_size // 3, 3) # shape: (num_samples * batch_size, num_joints, num_coordinates)
  # Compute the per joint position error as the Euclidean distance between outputs and targets
  errors = torch.norm(outputs - targets, dim=2) # shape: (num_samples * batch_size, num_joints)
  # Compute the mean per joint position error
  qual = torch.mean(errors) # scalar
  return qual

# Load the human motion data and the textual descriptions from files
motion_data = np.load("motion_data.npy") # shape: (num_examples, output_size)
text_data = np.load("text_data.npy") # shape: (num_examples, max_length)

# Convert the numpy arrays to torch tensors
motion_data = torch.from_numpy(motion_data).float() # shape: (num_examples, output_size)
text_data = torch.from_numpy(text_data).long() # shape: (num_examples, max_length)

# Create data loaders for batching the data
motion_loader = torch.utils.data.DataLoader(motion_data, batch_size=batch_size, shuffle=True)
text_loader = torch.utils.data.DataLoader(text_data, batch_size=batch_size)

# Instantiate the text encoder and motion decoder models
text_encoder = TextEncoder(vocab_size, hidden_size, num_layers)
motion_decoder = MotionDecoder(output_size, hidden_size, num_layers)

# Create an optimizer for updating the model parameters
optimizer = optim.Adam(list(text_encoder.parameters()) + list(motion_decoder.parameters()), lr=learning_rate)

# Train the VAE with human motion data
for epoch in range(num_epochs):
  for batch in motion_loader:
    # Reshape the batch to have an extra dimension for time steps
    batch = batch.unsqueeze(1) # shape: (batch_size, 1, output_size)
    # Encode the motion sequence into a latent vector
    mu, log_var = text_encoder(batch)
    # Sample a latent vector from the Gaussian distribution
    z = sample(mu, log_var)
    # Decode the latent vector into a reconstructed motion sequence
    output = motion_decoder(z)
    # Compute the VAE