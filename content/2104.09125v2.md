---
title: 2104.09125v2 SAPE  Spatially-Adaptive Progressive Encoding for Neural Optimization
date: 2021-04-10
---

# [SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization](http://arxiv.org/abs/2104.09125v2)

authors: Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2104.09125v2 "SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization"
[2]: https://arxiv.org/abs/2104.09125 "[2104.09125] SAPE: Spatially-Adaptive Progressive Encoding ... - arXiv.org"
[3]: http://export.arxiv.org/pdf/1912.09125 "arXiv:1912.09125v2 [math.NT] 7 Jan 2020"

Here is a summary of the paper:

- **What**: The paper proposes a new method called **SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization**[^1^][1] [^2^][2], which aims to improve the performance of multilayer-perceptrons (MLP) on learning functions with high-frequency components.
- **Why**: The paper claims that MLPs are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands[^1^][1] [^2^][2]. This limits their applicability to various domains and applications that require high-fidelity signal processing, such as image and shape synthesis, representation learning, and geometric tasks[^1^][1] [^2^][2].
- **How**: The paper introduces a novel scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing[^1^][1] [^2^][2]. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space[^1^][1] [^2^][2]. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space[^1^][1] [^2^][2]. The paper demonstrates the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes[^1^][1] [^2^][2].

## Main Contributions

According to the paper, the main contributions are:

- A novel spatially-adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing.
- A feedback loop mechanism that monitors the progressive exposure of frequencies and adjusts the learning rate accordingly, allowing changes to propagate at different rates among local spatial portions of the signal space.
- Extensive experiments on various domains and applications that demonstrate the superiority of SAPE over existing methods in terms of accuracy, fidelity, and generalization.

## Method Summary

[1]: https://arxiv.org/abs/2104.09125v2 "SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization"
[2]: https://arxiv.org/abs/2209.00796 "Diffusion Models: A Comprehensive Survey of Methods and Applications"
[3]: http://arxiv-export3.library.cornell.edu/pdf/2001.09125 "arXiv:2001.09125v2 [nucl-th] 29 Apr 2020"

Here is a summary of the method section of the paper:

- The paper describes the **SAPE scheme** in detail, which consists of two main components: a **progressive encoding function** and a **feedback loop**[^1^][1] [^2^][2].
- The progressive encoding function takes an input signal x and transforms it into a masked signal x̃ that gradually reveals higher frequency components over time[^1^][1] [^2^][2]. The masking is done by applying a low-pass filter with a cutoff frequency that depends on the training iteration and the spatial location[^1^][1] [^2^][2]. The paper defines the cutoff frequency as a function of the local signal variance and the global training progress[^1^][1] [^2^][2].
- The feedback loop monitors the progressive exposure of frequencies and adjusts the learning rate accordingly, allowing changes to propagate at different rates among local spatial portions of the signal space[^1^][1] [^2^][2]. The paper defines the learning rate as a function of the local signal variance and the local gradient magnitude[^1^][1] [^2^][2].
- The paper also discusses how to implement SAPE for different types of input signals, such as 1D signals, 2D images, and 3D shapes[^1^][1] [^2^][2]. The paper provides details on how to design the low-pass filter, how to compute the local signal variance, and how to handle boundary conditions[^1^][1] [^2^][2].
- The paper also presents some theoretical analysis on the convergence and stability properties of SAPE, as well as some empirical results on synthetic data that illustrate its advantages over existing methods[^1^][1] [^2^][2].

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the MLP network f with parameters theta
f = MLP(theta)

# Define the progressive encoding function g that takes an input signal x and returns a masked signal x̃
g = ProgressiveEncoding()

# Define the feedback loop function h that takes a masked signal x̃ and returns a learning rate alpha
h = FeedbackLoop()

# Define the loss function L that measures the discrepancy between the output of f and the target signal y
L = LossFunction()

# Initialize the training iteration t to zero
t = 0

# Loop until convergence or maximum number of iterations
while not converged or t < max_iter:

  # Sample a batch of input signals x and target signals y from the data distribution
  x, y = sample_batch()

  # Apply the progressive encoding function g to x to get the masked signal x̃
  x̃ = g(x, t)

  # Feed the masked signal x̃ to the MLP network f to get the output signal z
  z = f(x̃)

  # Compute the loss L between z and y
  loss = L(z, y)

  # Compute the gradient of loss with respect to theta
  grad = gradient(loss, theta)

  # Apply the feedback loop function h to x̃ to get the learning rate alpha
  alpha = h(x̃, grad)

  # Update theta using gradient descent with learning rate alpha
  theta = theta - alpha * grad

  # Increment t by one
  t = t + 1

# Return the trained MLP network f
return f
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Define the MLP network f with parameters theta
f = MLP(theta)

# Define the progressive encoding function g that takes an input signal x and returns a masked signal x̃
def g(x, t):

  # Define the cutoff frequency function c that depends on the local signal variance sigma and the global training progress p
  def c(sigma, p):
    # Use a sigmoid function to map p to the range [0, 1]
    p = 1 / (1 + exp(-p))
    # Use a power function to map sigma to the range [0, 1]
    sigma = sigma ** gamma
    # Return the cutoff frequency as a linear combination of p and sigma
    return c_min + (c_max - c_min) * (p + beta * sigma)

  # Compute the local signal variance sigma for each spatial location using a sliding window
  sigma = local_variance(x, window_size)

  # Compute the global training progress p as a function of t and the maximum number of iterations max_iter
  p = (t - t_0) / (max_iter - t_0)

  # Compute the cutoff frequency c for each spatial location using the function c
  c = c(sigma, p)

  # Apply a low-pass filter with cutoff frequency c to x to get the masked signal x̃
  x̃ = low_pass_filter(x, c)

  # Return the masked signal x̃
  return x̃

# Define the feedback loop function h that takes a masked signal x̃ and returns a learning rate alpha
def h(x̃, grad):

  # Define the learning rate function alpha that depends on the local signal variance sigma and the local gradient magnitude g
  def alpha(sigma, g):
    # Use a power function to map sigma to the range [0, 1]
    sigma = sigma ** gamma
    # Use a power function to map g to the range [0, 1]
    g = g ** delta
    # Return the learning rate as a linear combination of sigma and g
    return alpha_min + (alpha_max - alpha_min) * (sigma + eta * g)

  # Compute the local signal variance sigma for each spatial location using a sliding window
  sigma = local_variance(x̃, window_size)

  # Compute the local gradient magnitude g for each spatial location using a sliding window
  g = local_magnitude(grad, window_size)

  # Compute the learning rate alpha for each spatial location using the function alpha
  alpha = alpha(sigma, g)

  # Return the learning rate alpha
  return alpha

# Define the loss function L that measures the discrepancy between the output of f and the target signal y
L = LossFunction()

# Initialize the training iteration t to zero
t = 0

# Loop until convergence or maximum number of iterations
while not converged or t < max_iter:

  # Sample a batch of input signals x and target signals y from the data distribution
  x, y = sample_batch()

  # Apply the progressive encoding function g to x to get the masked signal x̃
  x̃ = g(x, t)

  # Feed the masked signal x̃ to the MLP network f to get the output signal z
  z = f(x̃)

  # Compute the loss L between z and y
  loss = L(z, y)

  # Compute the gradient of loss with respect to theta
  grad = gradient(loss, theta)

  # Apply the feedback loop function h to x̃ to get the learning rate alpha
  alpha = h(x̃, grad)

  # Update theta using gradient descent with learning rate alpha
  theta = theta - alpha * grad

  # Increment t by one
  t = t + 1

# Return the trained MLP network f
return f
```