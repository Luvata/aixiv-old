---
title: 2304.04947v1 Conditional Adapters  Parameter-efficient Transfer Learning with Fast Inference
date: 2023-04-05
---

# [Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference](http://arxiv.org/abs/2304.04947v1)

authors: Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, Ming-Wei Chang


## What, Why and How

[1]: https://arxiv.org/abs/2304.04947 "[2304.04947] Conditional Adapters: Parameter-efficient Transfer ..."
[2]: https://arxiv.org/pdf/2304.04947.pdf "Abstract arXiv:2304.04947v1 [cs.CL] 11 Apr 2023"
[3]: http://export.arxiv.org/abs/2104.04947v1 "[2104.04947v1] Conversational Semantic Role Labeling"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation.
- **Why**: The paper addresses the problem of adapting and deploying large pretrained models for various downstream tasks, which can be costly and inefficient. The paper aims to achieve both parameter efficiency and inference efficiency, which are often at odds with each other in existing methods.
- **How**: The paper introduces sparse activation together with a small number of new parameters and a light-weight training phase to the existing dense pretrained model. The paper leverages conditional computation to dynamically select which parts of the model to activate based on the input. The paper evaluates CoDA on a variety of language, vision, and speech tasks, and shows that it achieves 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.


## Main Contributions

[1]: https://arxiv.org/abs/2304.04947v1 "[2304.04947v1] Conditional Adapters: Parameter-efficient Transfer ..."
[2]: https://arxiv.org/pdf/2304.04947.pdf "Abstract arXiv:2304.04947v1 [cs.CL] 11 Apr 2023"
[3]: http://export.arxiv.org/abs/2104.04947v1 "[2104.04947v1] Conversational Semantic Role Labeling"

According to the paper[^1^][1], the main contributions are:

- **A novel parameter-efficient transfer learning method** that also improves inference efficiency by using conditional computation and sparse activation.
- **A generalization of the adapter approach** that can dynamically select which parts of the model to activate based on the input, without changing the original model architecture or parameters.
- **A comprehensive evaluation of CoDA** on a variety of language, vision, and speech tasks, showing that it achieves 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.


## Method Summary

[1]: https://arxiv.org/abs/2304.04947v1 "[2304.04947v1] Conditional Adapters: Parameter-efficient Transfer ..."
[2]: https://arxiv.org/pdf/2304.04947.pdf "Abstract arXiv:2304.04947v1 [cs.CL] 11 Apr 2023"
[3]: http://export.arxiv.org/abs/2104.04947v1 "[2104.04947v1] Conversational Semantic Role Labeling"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces **Conditional Adapter (CoDA)**, a parameter-efficient transfer learning method that also improves inference efficiency by using conditional computation and sparse activation.
- The paper describes how CoDA **adds a sparse activation layer** to the existing dense pretrained model, which can dynamically select which parts of the model to activate based on the input. The paper also explains how CoDA **adds a small number of new parameters** for each downstream task, which are trained with a light-weight training phase.
- The paper presents the **mathematical formulation** of CoDA, including the conditional computation mechanism, the sparse activation function, and the training objective. The paper also discusses some **implementation details** and **hyperparameters** of CoDA.
- The paper provides some **theoretical analysis** of CoDA, including its parameter efficiency, inference efficiency, and generalization ability. The paper also compares CoDA with other parameter-efficient transfer learning methods, such as Adapter and Prompt tuning.


## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper:

```python
# Define a dense pretrained model
model = PretrainedModel()

# Define a sparse activation layer
sparse_activation = SparseActivation()

# Define a conditional computation mechanism
conditional_computation = ConditionalComputation()

# Define a small number of new parameters for each task
task_parameters = TaskParameters()

# Define a training objective
objective = Objective()

# For each input x and task t
for x, t in data:
  # Compute the conditional mask for the input and task
  mask = conditional_computation(x, t)
  # Apply the sparse activation layer to the model with the mask
  output = sparse_activation(model(x), mask)
  # Apply the task-specific parameters to the output
  output = task_parameters(output, t)
  # Compute the loss and update the parameters
  loss = objective(output, t)
  update_parameters(loss)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import transformers
import numpy as np

# Define a dense pretrained model
model = transformers.AutoModel.from_pretrained("bert-base-uncased")

# Define a sparse activation layer
class SparseActivation(torch.nn.Module):
  def __init__(self, num_layers, num_heads):
    super().__init__()
    # Initialize the sparse activation function as a sigmoid
    self.activation = torch.nn.Sigmoid()
    # Initialize the sparse activation parameters as learnable scalars
    self.parameters = torch.nn.Parameter(torch.ones(num_layers, num_heads))

  def forward(self, x, mask):
    # Apply the sparse activation function element-wise to the parameters
    alpha = self.activation(self.parameters)
    # Multiply the input by the mask and the parameters
    return x * mask * alpha

# Define a conditional computation mechanism
class ConditionalComputation(torch.nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super().__init__()
    # Initialize a feed-forward network to compute the conditional mask
    self.network = torch.nn.Sequential(
      torch.nn.Linear(input_dim, hidden_dim),
      torch.nn.ReLU(),
      torch.nn.Linear(hidden_dim, output_dim),
      torch.nn.Sigmoid()
    )

  def forward(self, x, t):
    # Concatenate the input and the task embedding
    x_t = torch.cat([x, t], dim=-1)
    # Compute the conditional mask using the network
    mask = self.network(x_t)
    # Reshape the mask to match the model dimensions
    mask = mask.view(-1, model.config.num_hidden_layers, model.config.num_attention_heads)
    return mask

# Define a small number of new parameters for each task
class TaskParameters(torch.nn.Module):
  def __init__(self, num_tasks, hidden_dim):
    super().__init__()
    # Initialize a dictionary to store the task-specific parameters
    self.parameters = {}
    # For each task, create a linear layer with a bias term
    for i in range(num_tasks):
      self.parameters[i] = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)

  def forward(self, x, t):
    # Apply the task-specific linear layer to the input
    return self.parameters[t](x)

# Define a training objective
objective = torch.nn.CrossEntropyLoss()

# Define a task embedding layer
task_embedding = torch.nn.Embedding(num_tasks, model.config.hidden_size)

# Define an optimizer
optimizer = torch.optim.Adam(model.parameters() + sparse_activation.parameters() + conditional_computation.parameters() + task_parameters.parameters())

# For each input x and task t
for x, t in data:
  # Compute the task embedding for t
  t_emb = task_embedding(t)
  # Compute the conditional mask for x and t_emb using conditional computation
  mask = conditional_computation(x, t_emb)
  # Apply the model to x and get the last hidden state
  output = model(x)[0]
  # Apply the sparse activation layer to output and mask
  output = sparse_activation(output, mask)
  # Apply the task-specific parameters to output
  output = task_parameters(output, t)
  # Compute the loss using the objective
  loss = objective(output, t)
  # Update the parameters using the optimizer
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```