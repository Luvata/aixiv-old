---
title: 2204.03458v2 Video Diffusion Models
date: 2022-04-04
---

# [Video Diffusion Models](http://arxiv.org/abs/2204.03458v2)

authors: Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J. Fleet


## What, Why and How

[1]: https://arxiv.org/abs/2204.03458 "[2204.03458] Video Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2204.03458v2.pdf "Abstract - arXiv.org"
[3]: http://export.arxiv.org/abs/2204.03458v2 "[2204.03458v2] Video Diffusion Models - export.arxiv.org"

Here is a summary of the paper:

- **What**: The paper proposes a diffusion model for video generation that shows promising results on various tasks, such as video prediction, unconditional video generation, and text-conditioned video generation.
- **Why**: The paper aims to achieve an important milestone in generative modeling research: generating temporally coherent high fidelity video. The paper also demonstrates the benefits of joint training on image and video data, and introduces a new conditional sampling technique for spatial and temporal video extension.
- **How**: The paper uses a 3D U-Net diffusion model architecture that is a natural extension of the standard image diffusion model. The paper trains the model on a fixed number of video frames, and then applies it autoregressively to generate longer videos. The paper evaluates the model on established benchmarks and a large text-conditioned video dataset, and compares it with previous methods.

## Main Contributions

The paper claims the following contributions:

- The first diffusion model for video generation that shows very promising initial results.
- A new conditional sampling technique for spatial and temporal video extension that performs better than previous methods.
- The first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation.
- A demonstration of the benefits of joint training on image and video data for diffusion models.

## Method Summary

The method section of the paper consists of four subsections:

- **Diffusion models**: The paper reviews the basics of diffusion models, which are generative models that learn to reverse a Markovian Gaussian process that starts from data and ends at noise. The paper also describes how to train and sample from diffusion models using denoising score matching and annealed Langevin dynamics, respectively.
- **Video diffusion model architecture**: The paper presents the 3D U-Net architecture that is used for video diffusion models. The paper explains how the architecture is adapted from the image diffusion model by using 3D convolutions and residual blocks, and how it handles different input and output resolutions for image and video data.
- **Conditional sampling for video extension**: The paper introduces a new technique for generating longer and higher resolution videos by conditioning on previously generated frames. The paper shows how to use a spatial transformer network to align the previous frames with the current frame, and how to use a temporal transformer network to select the most relevant previous frames for conditioning. The paper also discusses how to handle text conditioning for video generation.
- **Joint training on image and video data**: The paper demonstrates how to jointly train a video diffusion model on both image and video data, by using a shared encoder-decoder network and a separate head network for each modality. The paper argues that joint training reduces the variance of minibatch gradients and speeds up optimization.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the video diffusion model architecture
model = VideoDiffusionModel()

# Define the noise schedule and the denoising score matching loss
noise_schedule = get_noise_schedule()
loss = DSM_Loss(noise_schedule)

# Define the optimizer and the learning rate scheduler
optimizer = Adam(model.parameters())
lr_scheduler = CosineAnnealingLR(optimizer)

# Train the model on image and video data
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get image or video data and text condition (if any)
    x, text = batch

    # Add noise to data according to noise schedule
    z = add_noise(x, noise_schedule)

    # Predict denoised data using model
    x_hat = model(z, text)

    # Compute loss and update model parameters
    loss_value = loss(x, x_hat)
    optimizer.zero_grad()
    loss_value.backward()
    optimizer.step()

  # Update learning rate
  lr_scheduler.step()

# Sample from the model using annealed Langevin dynamics
def sample(num_frames, resolution, text=None):
  # Initialize noise sample
  z = torch.randn(num_frames, resolution, resolution, 3)

  # Loop over noise schedule in reverse order
  for t in reversed(noise_schedule):
    # Add noise to current sample
    z = add_noise(z, t)

    # Predict denoised sample using model
    x_hat = model(z, text)

    # Update sample using gradient ascent
    z = z + lr * (x_hat - z)

  # Return final sample
  return z

# Generate longer and higher resolution videos by conditioning on previous frames
def extend_video(video, num_frames, resolution):
  # Loop over the number of frames to generate
  for i in range(num_frames):
    # Select the most relevant previous frames using temporal transformer network
    prev_frames = temporal_transformer(video)

    # Align the previous frames with the current frame using spatial transformer network
    prev_frames = spatial_transformer(prev_frames)

    # Generate the current frame by conditioning on the previous frames
    curr_frame = sample(1, resolution, prev_frames)

    # Append the current frame to the video
    video = torch.cat([video, curr_frame], dim=0)

  # Return extended video
  return video

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the video diffusion model architecture
class VideoDiffusionModel(nn.Module):
  def __init__(self):
    super().__init__()
    # Define the encoder-decoder network with 3D convolutions and residual blocks
    self.encoder_decoder = EncoderDecoder3D()
    # Define the image head network with 2D convolutions and residual blocks
    self.image_head = ImageHead2D()
    # Define the video head network with 3D convolutions and residual blocks
    self.video_head = VideoHead3D()
    # Define the text encoder network with a transformer encoder
    self.text_encoder = TextEncoder()

  def forward(self, z, text=None):
    # Encode the input noise z using the encoder-decoder network
    h = self.encoder_decoder(z)
    # Decode the hidden representation h using the image or video head network depending on the input shape
    if z.shape[1] == 1: # image input
      x_hat = self.image_head(h)
    else: # video input
      x_hat = self.video_head(h)
    # Encode the text condition (if any) using the text encoder network and add it to the output
    if text is not None:
      text_emb = self.text_encoder(text)
      x_hat = x_hat + text_emb
    # Return the predicted denoised output x_hat
    return x_hat

# Define the noise schedule and the denoising score matching loss
def get_noise_schedule():
  # Define a list of noise levels t that decrease from 1 to 0 according to a cosine schedule
  noise_schedule = [0.5 * (1 + math.cos(math.pi * t / num_timesteps)) for t in range(num_timesteps)]
  # Return the noise schedule as a tensor
  return torch.tensor(noise_schedule)

class DSM_Loss(nn.Module):
  def __init__(self, noise_schedule):
    super().__init__()
    # Store the noise schedule as an attribute
    self.noise_schedule = noise_schedule

  def forward(self, x, x_hat):
    # Compute the log signal-to-noise ratio for each timestep
    log_snr = torch.log(2 * self.noise_schedule / (self.noise_schedule ** 2))
    # Compute the mean squared error between x and x_hat for each timestep
    mse = F.mse_loss(x, x_hat, reduction="none")
    # Compute the loss as the weighted sum of mse and log_snr over all timesteps and dimensions
    loss = torch.sum(mse * log_snr.view(-1, 1, 1, 1, 1), dim=[0, 2, 3, 4])
    # Return the mean loss over the batch dimension
    return torch.mean(loss)

# Define the optimizer and the learning rate scheduler
model = VideoDiffusionModel()
optimizer = optim.Adam(model.parameters(), lr=initial_lr)
lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Train the model on image and video data
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get image or video data and text condition (if any) from the batch
    x, text = batch

    # Add noise to data according to noise schedule using a random timestep for each sample in the batch
    timesteps = torch.randint(0, num_timesteps, size=(x.shape[0],))
    noise_levels = self.noise_schedule[timesteps]
    z = x + torch.randn_like(x) * noise_levels.view(-1, 1, 1, 1, 1)

    # Predict denoised data using model
    x_hat = model(z, text)

    # Compute loss and update model parameters
    loss_value = loss(x, x_hat)
    optimizer.zero_grad()
    loss_value.backward()
    optimizer.step()

  # Update learning rate
  lr_scheduler.step()

# Sample from the model using annealed Langevin dynamics
def sample(num_frames, resolution, text=None):
  # Initialize noise sample with shape (num_frames, resolution, resolution, 3)
  z = torch.randn(num_frames, resolution, resolution, 3)

  # Loop over noise schedule in reverse order
  for t in reversed(noise_schedule):
    # Add noise to current sample according to noise level t
    z = z + torch.randn_like(z) * t

    # Predict denoised sample using model
    x_hat = model(z, text)

    # Update sample using gradient ascent with a small learning rate
    z = z + lr * (x_hat - z)

  # Return final sample
  return z

# Define the spatial transformer network
class SpatialTransformer(nn.Module):
  def __init__(self):
    super().__init__()
    # Define a convolutional network that predicts the affine transformation parameters
    self.conv_net = ConvNet()
    # Initialize the last layer to predict the identity transformation
    self.conv_net[-1].weight.data.zero_()
    self.conv_net[-1].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))

  def forward(self, x):
    # Compute the affine transformation parameters using the convolutional network
    theta = self.conv_net(x)
    theta = theta.view(-1, 2, 3)
    # Compute the grid of coordinates to sample from x using the affine transformation
    grid = F.affine_grid(theta, x.size())
    # Sample from x using the grid and bilinear interpolation
    x = F.grid_sample(x, grid)
    # Return the transformed x
    return x

# Define the temporal transformer network
class TemporalTransformer(nn.Module):
  def __init__(self):
    super().__init__()
    # Define a convolutional network that predicts the attention weights for each previous frame
    self.conv_net = ConvNet()
    # Define a softmax layer to normalize the attention weights
    self.softmax = nn.Softmax(dim=1)

  def forward(self, x):
    # Compute the attention weights for each previous frame using the convolutional network
    alpha = self.conv_net(x)
    alpha = self.softmax(alpha)
    # Compute the weighted sum of the previous frames using the attention weights
    x = torch.sum(alpha * x, dim=1)
    # Return the transformed x
    return x

# Generate longer and higher resolution videos by conditioning on previous frames
def extend_video(video, num_frames, resolution):
  # Define the spatial transformer network
  spatial_transformer = SpatialTransformer()
  # Define the temporal transformer network
  temporal_transformer = TemporalTransformer()
  # Loop over the number of frames to generate
  for i in range(num_frames):
    # Select the most relevant previous frames using temporal transformer network
    prev_frames = temporal_transformer(video)
    # Align the previous frames with the current frame using spatial transformer network
    prev_frames = spatial_transformer(prev_frames)
    # Generate the current frame by conditioning on the previous frames
    curr_frame = sample(1, resolution, prev_frames)
    # Append the current frame to the video
    video = torch.cat([video, curr_frame], dim=0)
  # Return extended video
  return video

```