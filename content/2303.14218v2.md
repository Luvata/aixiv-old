---
title: 2303.14218v2 Curricular Contrastive Regularization for Physics-aware Single Image Dehazing
date: 2023-03-15
---

# [Curricular Contrastive Regularization for Physics-aware Single Image Dehazing](http://arxiv.org/abs/2303.14218v2)

authors: Yu Zheng, Jiahui Zhan, Shengfeng He, Junyu Dong, Yong Du


## What, Why and How

[1]: https://arxiv.org/abs/2303.14218 "[2303.14218] Curricular Contrastive Regularization for Physics-aware ..."
[2]: https://arxiv.org/pdf/2303.14218.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export2.library.cornell.edu/abs/2303.14218v2 "[2303.14218v2] Curricular Contrastive Regularization for Physics-aware ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method for single image dehazing, which is the task of restoring a clear image from a hazy one.
- **Why**: The paper aims to address the limitations of existing methods that use contrastive regularization, which is a technique that introduces information from negative images (i.e., images that are not clear) as a lower bound for the dehazing process. The paper argues that the contrastive samples are nonconsensual, meaning that they are too different from the clear image, and that they leave the solution space still under-constrained. The paper also aims to improve the interpretability of the dehazing model by incorporating the physics of the hazing process.
- **How**: The paper introduces a curricular contrastive regularization method that targets a consensual contrastive space, meaning that the negative images are more similar to the clear image and provide better lower-bound constraints. The paper generates the negative images by using 1) the hazy image itself, and 2) corresponding restorations by other existing methods. The paper also customizes a curriculum learning strategy to reweight the importance of different negatives according to their similarities to the clear image. Moreover, the paper builds a physics-aware dual-branch unit that follows the atmospheric scattering model, which is a mathematical model that describes how light is scattered by particles in the air. The paper combines the unit with the curricular contrastive regularization method to establish a dehazing network, named C2PNet.


## Main Contributions

According to the paper, the main contributions are:

- A novel curricular contrastive regularization method that targets a consensual contrastive space for single image dehazing, which provides better lower-bound constraints and improves the dehazing performance.
- A customized curriculum learning strategy that reweights the importance of different negatives according to their similarities to the clear image, which balances the learning difficulty and enhances the dehazing quality.
- A physics-aware dual-branch unit that incorporates the atmospheric scattering model into the dehazing network, which improves the interpretability and robustness of the model.

## Method Summary

The method section of the paper consists of three subsections: Curricular Contrastive Regularization, Curriculum Learning Strategy, and Physics-aware Dual-branch Unit. Here is a summary of each subsection:

- Curricular Contrastive Regularization: This subsection introduces the proposed method for generating and using negative images for single image dehazing. The paper defines a contrastive loss function that measures the distance between the embeddings of the clear image and the negative images in a latent space. The paper generates the negative images by using 1) the hazy image itself, and 2) corresponding restorations by other existing methods. The paper argues that these negatives are more consensual than random images, as they share some common features with the clear image. The paper also shows that using these negatives can improve the dehazing performance over existing methods that use random negatives.
- Curriculum Learning Strategy: This subsection introduces the proposed method for reweighting the importance of different negatives according to their similarities to the clear image. The paper defines a similarity score function that measures how close a negative image is to the clear image in terms of structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR). The paper then uses this score to assign a weight to each negative image in the contrastive loss function. The paper argues that this strategy can balance the learning difficulty and enhance the dehazing quality, as it gives more attention to the negatives that are more informative and challenging.
- Physics-aware Dual-branch Unit: This subsection introduces the proposed unit that incorporates the atmospheric scattering model into the dehazing network. The paper defines a dual-branch unit that consists of two parallel branches: a transmission branch and an atmospheric light branch. The transmission branch estimates the transmission map, which is a measure of how much light is transmitted through the haze. The atmospheric light branch estimates the atmospheric light, which is a measure of how much light is scattered by the haze. The paper then uses these two branches to reconstruct the clear image according to the atmospheric scattering model, which is a mathematical model that describes how light is scattered by particles in the air. The paper argues that this unit can improve the interpretability and robustness of the model, as it follows the physical principles of the hazing process.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a hazy image I
# Output: a clear image J

# Generate negative images N1 and N2 from I and other methods
N1 = I
N2 = restore(I, other_methods)

# Define a contrastive loss function Lc that measures the distance between the embeddings of J and N1, N2
Lc = distance(embed(J), embed(N1)) + distance(embed(J), embed(N2))

# Define a similarity score function S that measures how close N1, N2 are to J in terms of SSIM and PSNR
S = SSIM(N1, J) + PSNR(N1, J) + SSIM(N2, J) + PSNR(N2, J)

# Define a weight function W that assigns a weight to each negative image based on S
W = softmax(S)

# Define a dual-branch unit U that consists of a transmission branch T and an atmospheric light branch A
U = (T, A)

# Estimate the transmission map t and the atmospheric light A from I using U
t = T(I)
A = A(I)

# Reconstruct the clear image J from I, t, A using the atmospheric scattering model
J = (I - A) / t + A

# Define a reconstruction loss function Lr that measures the difference between J and the ground truth clear image G
Lr = difference(J, G)

# Define the total loss function L as a combination of Lc and Lr
L = Lc + Lr

# Train the dehazing network C2PNet using L as the objective function
C2PNet = train(L)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the hyperparameters
batch_size = 16
learning_rate = 0.001
num_epochs = 100
num_negatives = 2

# Define the dehazing network C2PNet as a class
class C2PNet(torch.nn.Module):
    # Initialize the network with a backbone encoder, a decoder, and a dual-branch unit
    def __init__(self):
        super(C2PNet, self).__init__()
        # Use a pretrained ResNet-50 as the backbone encoder
        self.encoder = torchvision.models.resnet50(pretrained=True)
        # Use a convolutional layer as the decoder
        self.decoder = torch.nn.Conv2d(2048, 3, kernel_size=1)
        # Use two convolutional layers as the transmission branch and the atmospheric light branch
        self.transmission_branch = torch.nn.Conv2d(2048, 1, kernel_size=1)
        self.atmospheric_light_branch = torch.nn.Conv2d(2048, 3, kernel_size=1)

    # Define the forward pass of the network
    def forward(self, x):
        # Encode the input image x using the encoder
        x = self.encoder(x)
        # Estimate the transmission map t using the transmission branch
        t = self.transmission_branch(x)
        # Estimate the atmospheric light A using the atmospheric light branch
        A = self.atmospheric_light_branch(x)
        # Reconstruct the clear image J using the decoder and the atmospheric scattering model
        J = self.decoder(x) * t + A * (1 - t)
        # Return J, t, A as the output
        return J, t, A

# Define a function to generate negative images from a hazy image and other methods
def generate_negatives(I, other_methods):
    # Initialize an empty list to store the negative images
    negatives = []
    # Add the hazy image itself as the first negative image
    negatives.append(I)
    # For each other method in other_methods
    for method in other_methods:
        # Restore the hazy image using the method and add it as a negative image
        N = restore(I, method)
        negatives.append(N)
    # Return negatives as a list of negative images
    return negatives

# Define a function to compute the contrastive loss between a clear image and negative images
def contrastive_loss(J, negatives):
    # Initialize an empty list to store the embeddings of J and negatives
    embeddings = []
    # Use a pretrained ResNet-50 as an embedding network
    embedding_network = torchvision.models.resnet50(pretrained=True)
    # Embed J using the embedding network and add it to embeddings
    J_embed = embedding_network(J)
    embeddings.append(J_embed)
    # For each negative image in negatives
    for N in negatives:
        # Embed N using the embedding network and add it to embeddings
        N_embed = embedding_network(N)
        embeddings.append(N_embed)
    # Compute the pairwise distances between J_embed and N_embeds using torch.cdist
    distances = torch.cdist(J_embed, torch.stack(embeddings[1:]))
    # Compute the contrastive loss as the sum of distances using torch.sum
    loss = torch.sum(distances)
    # Return loss as a scalar tensor
    return loss

# Define a function to compute the similarity score between a clear image and negative images
def similarity_score(J, negatives):
    # Initialize an empty list to store the scores of J and negatives
    scores = []
    # For each negative image in negatives
    for N in negatives:
        # Compute the SSIM between J and N using cv2.SSIM (a function from OpenCV library)
        ssim = cv2.SSIM(J.numpy(), N.numpy())
        # Compute the PSNR between J and N using cv2.PSNR (a function from OpenCV library)
        psnr = cv2.PSNR(J.numpy(), N.numpy())
        # Compute the score as the sum of SSIM and PSNR and add it to scores
        score = ssim + psnr
        scores.append(score)
    # Return scores as a list of scalar values
    return scores

# Define a function to compute the weight for each negative image based on its similarity score
def weight_function(scores):
    # Convert scores to a numpy array using np.array (a function from NumPy library)
    scores = np.array(scores)
    # Apply softmax to scores using np.exp and np.sum (functions from NumPy library)
    weights = np.exp(scores) / np.sum(np.exp(scores))
    # Return weights as a numpy array
    return weights

# Define a function to compute the reconstruction loss between a clear image and a reconstructed image
def reconstruction_loss(J, G):
    # Compute the mean squared error between J and G using torch.nn.MSELoss (a function from PyTorch library)
    mse = torch.nn.MSELoss()(J, G)
    # Return mse as a scalar tensor
    return mse

# Define a function to compute the total loss as a combination of contrastive loss and reconstruction loss
def total_loss(J, G, negatives):
    # Compute the contrastive loss between J and negatives using contrastive_loss function
    Lc = contrastive_loss(J, negatives)
    # Compute the reconstruction loss between J and G using reconstruction_loss function
    Lr = reconstruction_loss(J, G)
    # Compute the similarity score between J and negatives using similarity_score function
    S = similarity_score(J, negatives)
    # Compute the weight for each negative image using weight_function function
    W = weight_function(S)
    # Compute the total loss as a weighted sum of Lc and Lr using torch.tensor (a function from PyTorch library)
    L = torch.tensor(W[0]) * Lc + torch.tensor(W[1]) * Lr
    # Return L as a scalar tensor
    return L

# Load the hazy images and clear images as PyTorch tensors using torchvision.datasets.ImageFolder (a function from PyTorch library)
hazy_images = torchvision.datasets.ImageFolder('hazy_images')
clear_images = torchvision.datasets.ImageFolder('clear_images')

# Create data loaders for hazy images and clear images using torch.utils.data.DataLoader (a function from PyTorch library)
hazy_loader = torch.utils.data.DataLoader(hazy_images, batch_size=batch_size, shuffle=True)
clear_loader = torch.utils.data.DataLoader(clear_images, batch_size=batch_size, shuffle=True)

# Create an instance of C2PNet class
c2pnet = C2PNet()

# Create an optimizer for C2PNet using torch.optim.Adam (a function from PyTorch library)
optimizer = torch.optim.Adam(c2pnet.parameters(), lr=learning_rate)

# For each epoch in num_epochs
for epoch in range(num_epochs):
    # Initialize the epoch loss to zero
    epoch_loss = 0
    # For each batch of hazy images and clear images in hazy_loader and clear_loader
    for (I, _), (G, _) in zip(hazy_loader, clear_loader):
        # Generate num_negatives negative images from I and other methods using generate_negatives function
        negatives = generate_negatives(I, other_methods)
        # Forward pass I through C2PNet to get J, t, A
        J, t, A = c2pnet(I)
        # Compute the total loss between J, G, negatives using total_loss function
        loss = total_loss(J, G, negatives)
        # Backward pass the loss through C2PNet using loss.backward() (a method from PyTorch library)
        loss.backward()
        # Update the parameters of C2PNet using optimizer.step() (a method from PyTorch library)
        optimizer.step()
        # Reset the gradients of C2PNet using optimizer.zero_grad() (a method from PyTorch library)
        optimizer.zero_grad()
        # Add the loss to the epoch loss using loss.item() (a method from PyTorch library)
        epoch_loss += loss.item()
    # Print the average epoch loss by dividing the epoch loss by the number of batches
    print(f'Epoch {epoch+1}, Loss: {epoch_loss / len(hazy_loader)}')
```