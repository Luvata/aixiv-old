---
title: 1808.06601v2 Video-to-Video Synthesis
date: 2018-08-07
---

# [Video-to-Video Synthesis](http://arxiv.org/abs/1808.06601v2)

authors: Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, Bryan Catanzaro


## What, Why and How

[1]: https://arxiv.org/abs/1808.06601 "[1808.06601] Video-to-Video Synthesis - arXiv.org"
[2]: https://www.researchgate.net/publication/327134591_Video-to-Video_Synthesis "(PDF) Video-to-Video Synthesis - ResearchGate"
[3]: https://arxiv.org/pdf/1808.06601v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper proposes a novel video-to-video synthesis approach that can generate photorealistic and temporally coherent videos from various input formats such as segmentation masks, sketches, and poses[^1^][1].
- **Why**: The paper aims to address the challenge of video-to-video synthesis, which is less explored than image-to-image synthesis, and requires understanding temporal dynamics and preserving visual quality[^1^][1].
- **How**: The paper adopts a generative adversarial learning framework, and designs a generator and a discriminator that can handle spatio-temporal inputs and outputs. The generator consists of a flow prediction network, an appearance generation network, and a fusion network. The discriminator consists of a spatial discriminator and a temporal discriminator. The paper also introduces a spatio-temporal adversarial objective that encourages realistic and consistent video generation[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/1808.06601 "[1808.06601] Video-to-Video Synthesis - arXiv.org"
[2]: https://arxiv.org/abs/1910.12713 "[1910.12713] Few-shot Video-to-Video Synthesis - arXiv.org"
[3]: https://arxiv.org/pdf/1808.06601.pdf "Abstract - arXiv.org"

According to the paper[^1^][1], the main contributions are:

- **A novel video-to-video synthesis approach** that can handle diverse input formats and generate high-resolution, photorealistic, and temporally coherent videos.
- **A carefully-designed generator architecture** that consists of three components: a flow prediction network, an appearance generation network, and a fusion network. The flow prediction network predicts optical flow between consecutive frames to model temporal dynamics. The appearance generation network generates appearance features for each frame based on the input format. The fusion network combines the flow and appearance features to produce realistic frames.
- **A discriminator architecture** that consists of a spatial discriminator and a temporal discriminator. The spatial discriminator evaluates the realism of each frame independently, while the temporal discriminator evaluates the realism of consecutive frames jointly.
- **A spatio-temporal adversarial objective** that encourages the generator to produce videos that are indistinguishable from real videos by both the spatial and temporal discriminators.
- **Extensive experiments** on multiple benchmarks, such as Cityscapes, FaceForensics, and Dance, to demonstrate the effectiveness and superiority of the proposed approach compared to existing methods. The paper also shows the application of the approach to future video prediction.

## Method Summary

[1]: https://arxiv.org/abs/1808.06601 "[1808.06601] Video-to-Video Synthesis - arXiv.org"
[2]: https://arxiv.org/abs/1910.12713 "[1910.12713] Few-shot Video-to-Video Synthesis - arXiv.org"
[3]: https://papers.nips.cc/paper/7391-video-to-video-synthesis "Video-to-Video Synthesis - NIPS"
[4]: https://arxiv.org/pdf/1910.12713.pdf "arXiv:1910.12713v1 [cs.CV] 28 Oct 2019"
[5]: https://arxiv.org/pdf/1808.06601.pdf "Abstract - arXiv.org"

Here is a summary of the method section of the paper:

- The paper casts the video-to-video synthesis problem as a distribution matching problem, where the goal is to train a model such that the conditional distribution of the synthesized videos given input videos resembles that of real videos[^1^][1].
- The paper adopts a conditional generative adversarial network (cGAN) framework, where a generator G tries to fool a discriminator D by producing realistic videos, while D tries to distinguish between real and fake videos[^1^][1].
- The paper designs a generator G that consists of three components: a flow prediction network F, an appearance generation network A, and a fusion network U. F predicts optical flow between consecutive frames to model temporal dynamics. A generates appearance features for each frame based on the input format. U combines the flow and appearance features to produce realistic frames[^1^][1].
- The paper designs a discriminator D that consists of a spatial discriminator Ds and a temporal discriminator Dt. Ds evaluates the realism of each frame independently, while Dt evaluates the realism of consecutive frames jointly[^1^][1].
- The paper introduces a spatio-temporal adversarial objective that encourages G to produce videos that are indistinguishable from real videos by both Ds and Dt. The objective also includes several auxiliary losses, such as feature matching loss, perceptual loss, style loss, and flow consistency loss[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generator G
G = FlowPredictionNetwork(F) + AppearanceGenerationNetwork(A) + FusionNetwork(U)

# Define the spatial discriminator Ds
Ds = PatchGAN + MultiScale

# Define the temporal discriminator Dt
Dt = PatchGAN + MultiScale + 3DConv

# Define the spatio-temporal adversarial objective L
L = LGAN(G, Ds, Dt) + LFM(G, Ds, Dt) + LPER(G) + LSTY(G) + LFLW(G)

# Train G and D alternately
for epoch in epochs:
  for batch in batches:
    # Get input video x and output video y
    x, y = get_batch()
    # Generate fake video y_hat
    y_hat = G(x)
    # Update D by maximizing L_D
    L_D = -L(G, Ds, Dt)
    update(Ds, Dt, L_D)
    # Update G by minimizing L_G
    L_G = L(G, Ds, Dt)
    update(G, L_G)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Define the flow prediction network F
F = ResNet + ConvLSTM

# Define the appearance generation network A
A = UNet + SPADE

# Define the fusion network U
U = UNet + SPADE

# Define the generator G
G = F + A + U

# Define the spatial discriminator Ds
Ds = PatchGAN + MultiScale

# Define the temporal discriminator Dt
Dt = PatchGAN + MultiScale + 3DConv

# Define the adversarial loss LGAN
def LGAN(G, Ds, Dt):
  # Get input video x and output video y
  x, y = get_batch()
  # Generate fake video y_hat
  y_hat = G(x)
  # Compute real/fake logits for spatial discriminator
  logit_real_s = Ds(y)
  logit_fake_s = Ds(y_hat)
  # Compute real/fake logits for temporal discriminator
  logit_real_t = Dt(y)
  logit_fake_t = Dt(y_hat)
  # Compute hinge loss for discriminator
  L_Ds = mean(max(0, 1 - logit_real_s) + max(0, 1 + logit_fake_s))
  L_Dt = mean(max(0, 1 - logit_real_t) + max(0, 1 + logit_fake_t))
  L_D = L_Ds + L_Dt
  # Compute hinge loss for generator
  L_Gs = -mean(logit_fake_s)
  L_Gt = -mean(logit_fake_t)
  L_G = L_Gs + L_Gt
  return L_G, L_D

# Define the feature matching loss LFM
def LFM(G, Ds, Dt):
  # Get input video x and output video y
  x, y = get_batch()
  # Generate fake video y_hat
  y_hat = G(x)
  # Compute feature maps for spatial discriminator
  feat_real_s = Ds.get_features(y)
  feat_fake_s = Ds.get_features(y_hat)
  # Compute feature maps for temporal discriminator
  feat_real_t = Dt.get_features(y)
  feat_fake_t = Dt.get_features(y_hat)
  # Compute L1 loss for feature matching
  L_FMs = mean(L1(feat_real_s - feat_fake_s))
  L_FMt = mean(L1(feat_real_t - feat_fake_t))
  L_FM = L_FMs + L_FMt
  return L_FM

# Define the perceptual loss LPER
def LPER(G):
  # Get input video x and output video y
  x, y = get_batch()
  # Generate fake video y_hat
  y_hat = G(x)
  # Extract features from VGG network
  vgg_real = VGG(y)
  vgg_fake = VGG(y_hat)
  # Compute perceptual loss as weighted L1 loss of features
  weights = [1/32, 1/16, 1/8, 1/4, 1]
  L_PER = sum(weights[i] * mean(L1(vgg_real[i] - vgg_fake[i])) for i in range(5))
  return L_PER

# Define the style loss LSTY
def LSTY(G):
   # Get input video x and output video y
   x, y = get_batch()
   # Generate fake video y_hat
   y_hat = G(x)
   # Extract features from VGG network
   vgg_real = VGG(y)
   vgg_fake = VGG(y_hat)
   # Compute gram matrices of features
   gram_real = [gram_matrix(f) for f in vgg_real]
   gram_fake = [gram_matrix(f) for f in vgg_fake]
   # Compute style loss as weighted L1 loss of gram matrices
   weights = [1/32, 1/16, 1/8, 1/4, 1]
   L_STY = sum(weights[i] * mean(L1(gram_real[i] - gram_fake[i])) for i in range(5))
   return L_STY

# Define the flow consistency loss LFLW
def LFLW(G):
   # Get input video x and output video y
   x, y = get_batch()
   # Generate fake video y_hat and flow map f_hat from G
   y_hat, f_hat = G(x)
   # Warp the previous frame using f_hat to get the warped frame w_hat
   w_hat = warp(y_hat[:-1], f_hat[1:])
   # Compute flow consistency loss as L1 loss between w_hat and y_hat
   L_FLW = mean(L1(w_hat - y_hat[1:]))
   return L_FLW

# Define the spatio-temporal adversarial objective L
L = LGAN(G, Ds, Dt) + LFM(G, Ds, Dt) + LPER(G) + LSTY(G) + LFLW(G)

# Train G and D alternately
for epoch in epochs:
  for batch in batches:
    # Update D by maximizing L_D
    L_G, L_D = LGAN(G, Ds, Dt)
    update(Ds, Dt, L_D)
    # Update G by minimizing L_G
    update(G, L_G)
    # Update G by minimizing L_FM
    L_FM = LFM(G, Ds, Dt)
    update(G, L_FM)
    # Update G by minimizing L_PER
    L_PER = LPER(G)
    update(G, L_PER)
    # Update G by minimizing L_STY
    L_STY = LSTY(G)
    update(G, L_STY)
    # Update G by minimizing L_FLW
    L_FLW = LFLW(G)
    update(G, L_FLW)
```