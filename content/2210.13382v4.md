---
title: 2210.13382v4 Emergent World Representations  Exploring a Sequence Model Trained on a Synthetic Task
date: 2022-10-14
---

# [Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task](http://arxiv.org/abs/2210.13382v4)

authors: Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg


## What, Why and How

[1]: https://arxiv.org/abs/2210.13382 "[2210.13382] Emergent World Representations: Exploring a Sequence Model ..."
[2]: https://arxiv.org/pdf/2210.13382v4 "EMERGENT WORLD REPRESENTATIONS: EXPLORING A SEQUENCE MODEL TRAINED ON A ..."
[3]: https://arxiv.org/pdf/2210.13382v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper explores how a sequence model trained on a synthetic task of predicting legal moves in Othello develops an internal representation of the board state and the game rules.
- **Why**: The paper aims to understand the source of competence of language models and whether they rely on surface statistics or deeper representations of the underlying process that generates the sequences they see.
- **How**: The paper applies a variant of the GPT model to the Othello task and analyzes its hidden states using various methods, such as probing, intervention, and latent saliency maps. The paper finds evidence of an emergent nonlinear representation that can capture complex patterns and dependencies in the data.


## Main Contributions

[1]: https://arxiv.org/abs/2210.13382 "[2210.13382] Emergent World Representations: Exploring a Sequence Model ..."
[2]: https://arxiv.org/pdf/2210.13382v4 "EMERGENT WORLD REPRESENTATIONS: EXPLORING A SEQUENCE MODEL TRAINED ON A ..."
[3]: https://arxiv.org/pdf/2210.13382v2.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], some of the main contributions are:

- Introducing a synthetic task of predicting legal moves in Othello as a testbed for studying the internal representations of sequence models.
- Applying a variant of the GPT model to the Othello task and showing that it can achieve high accuracy and generalization without any prior knowledge of the game or its rules.
- Analyzing the hidden states of the model using various methods, such as probing, intervention, and latent saliency maps, and finding evidence of an emergent nonlinear representation of the board state that can capture complex patterns and dependencies in the data.
- Demonstrating that the emergent representation can be used to control the output of the model and create interpretable visualizations that can help explain predictions in human terms.


## Method Summary

[1]: https://arxiv.org/abs/2210.13382 "[2210.13382] Emergent World Representations: Exploring a Sequence Model ..."
[2]: https://arxiv.org/pdf/2210.13382v4 "EMERGENT WORLD REPRESENTATIONS: EXPLORING A SEQUENCE MODEL TRAINED ON A ..."
[3]: https://arxiv.org/pdf/2210.13382v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper describes the Othello task as a sequence prediction problem, where the input is a board state encoded as a string of 64 characters and the output is a list of legal moves encoded as a string of 8 characters.
- The paper uses a variant of the GPT model with 12 layers, 768 hidden units, and 12 attention heads, and trains it on a dataset of 10 million randomly generated board states and their corresponding legal moves.
- The paper evaluates the model on three metrics: accuracy, generalization, and diversity. Accuracy measures how often the model predicts at least one legal move correctly. Generalization measures how well the model performs on board states that are not seen during training. Diversity measures how many different moves the model can produce for a given board state.
- The paper analyzes the hidden states of the model using various methods, such as probing, intervention, and latent saliency maps. Probing tests whether the hidden states contain information about specific features of the board state, such as piece counts, mobility, or parity. Intervention modifies the hidden states to see how they affect the output of the model. Latent saliency maps visualize which hidden units are most relevant for predicting a given move.


## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the GPT model with 12 layers, 768 hidden units, and 12 attention heads
model = GPT(layers=12, hidden_units=768, attention_heads=12)

# Define the Othello task as a sequence prediction problem
# Input: a board state encoded as a string of 64 characters
# Output: a list of legal moves encoded as a string of 8 characters
task = Othello()

# Generate a dataset of 10 million board states and their corresponding legal moves
dataset = task.generate_dataset(size=10_000_000)

# Train the model on the dataset using cross-entropy loss
model.train(dataset, loss_function=cross_entropy)

# Evaluate the model on three metrics: accuracy, generalization, and diversity
accuracy = task.evaluate_accuracy(model)
generalization = task.evaluate_generalization(model)
diversity = task.evaluate_diversity(model)

# Analyze the hidden states of the model using various methods
probing_results = task.probe_hidden_states(model)
intervention_results = task.intervene_hidden_states(model)
latent_saliency_maps = task.visualize_hidden_states(model)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import matplotlib.pyplot as plt

# Define the GPT model with 12 layers, 768 hidden units, and 12 attention heads
# Assume the model is already implemented using PyTorch
model = GPT(layers=12, hidden_units=768, attention_heads=12)

# Define the Othello task as a sequence prediction problem
class Othello:

  # Initialize the task with a vocabulary of 4 characters: B (black), W (white), . (empty), and * (legal move)
  def __init__(self):
    self.vocab = ["B", "W", ".", "*"]
    self.vocab_size = len(self.vocab)
    self.board_size = 64
    self.move_size = 8

  # Generate a random board state as a string of 64 characters
  def generate_board_state(self):
    board_state = ""
    for i in range(self.board_size):
      board_state += np.random.choice(self.vocab[:3])
    return board_state

  # Generate a list of legal moves for a given board state and a player color as a string of 8 characters
  def generate_legal_moves(self, board_state, player_color):
    legal_moves = ""
    for i in range(self.move_size):
      # Check if the move is legal by applying some rules of Othello
      # For simplicity, assume the rules are already implemented as a function
      if is_legal_move(board_state, player_color, i):
        legal_moves += "*"
      else:
        legal_moves += "."
    return legal_moves

  # Generate a dataset of board states and legal moves for both black and white players
  def generate_dataset(self, size):
    dataset = []
    for i in range(size):
      # Generate a random board state
      board_state = self.generate_board_state()
      # Generate legal moves for black and white players
      black_moves = self.generate_legal_moves(board_state, "B")
      white_moves = self.generate_legal_moves(board_state, "W")
      # Concatenate the board state and the legal moves as input-output pairs
      black_pair = board_state + black_moves
      white_pair = board_state + white_moves
      # Add the pairs to the dataset
      dataset.append(black_pair)
      dataset.append(white_pair)
    return dataset

  # Encode a string of characters as a tensor of integers using the vocabulary
  def encode(self, string):
    tensor = torch.zeros(len(string), dtype=torch.long)
    for i, char in enumerate(string):
      tensor[i] = self.vocab.index(char)
    return tensor

  # Decode a tensor of integers as a string of characters using the vocabulary
  def decode(self, tensor):
    string = ""
    for i in tensor:
      string += self.vocab[i]
    return string

  # Evaluate the accuracy of the model on a given dataset
  def evaluate_accuracy(self, model, dataset):
    accuracy = 0
    for pair in dataset:
      # Split the pair into input and output
      input = pair[:self.board_size]
      output = pair[self.board_size:]
      # Encode the input as a tensor
      input_tensor = self.encode(input)
      # Predict the output using the model
      prediction_tensor = model.predict(input_tensor)
      # Decode the prediction as a string
      prediction = self.decode(prediction_tensor)
      # Compare the prediction with the output and count the number of correct matches
      matches = 0
      for i in range(self.move_size):
        if prediction[i] == output[i] == "*":
          matches += 1
      # Compute the accuracy as the ratio of correct matches to total legal moves
      accuracy += matches / output.count("*")
    # Return the average accuracy over the dataset
    return accuracy / len(dataset)

  # Evaluate the generalization of the model on a given dataset
  def evaluate_generalization(self, model, dataset):
    generalization = 0
    for pair in dataset:
      # Split the pair into input and output
      input = pair[:self.board_size]
      output = pair[self.board_size:]
      # Encode the input as a tensor
      input_tensor = self.encode(input)
      # Predict the output using the model
      prediction_tensor = model.predict(input_tensor)
      # Decode the prediction as a string
      prediction = self.decode(prediction_tensor)
      # Compare the prediction with the output and count the number of correct matches only for unseen board states
      if input not in training_dataset:
        matches = 0
        for i in range(self.move_size):
          if prediction[i] == output[i] == "*":
            matches += 1
        # Compute the generalization as the ratio of correct matches to total legal moves
        generalization += matches / output.count("*")
    # Return the average generalization over the dataset
    return generalization / len(dataset)

  # Evaluate the diversity of the model on a given dataset
  def evaluate_diversity(self, model, dataset):
    diversity = 0
    for pair in dataset:
      # Split the pair into input and output
      input = pair[:self.board_size]
      output = pair[self.board_size:]
      # Encode the input as a tensor
      input_tensor = self.encode(input)
      # Predict the output using the model with different random seeds
      predictions = []
      for seed in range(10):
        torch.manual_seed(seed)
        prediction_tensor = model.predict(input_tensor)
        prediction = self.decode(prediction_tensor)
        predictions.append(prediction)
      # Compute the diversity as the number of unique predictions divided by the number of legal moves
      diversity += len(set(predictions)) / output.count("*")
    # Return the average diversity over the dataset
    return diversity / len(dataset)

  # Probe the hidden states of the model for a given board state and a feature
  def probe_hidden_states(self, model, board_state, feature):
    # Encode the board state as a tensor
    input_tensor = self.encode(board_state)
    # Get the hidden states of the model for each layer and each position
    hidden_states = model.get_hidden_states(input_tensor)
    # Define a linear classifier to predict the feature from the hidden states
    classifier = torch.nn.Linear(768, 1)
    # Train the classifier on a subset of board states and their corresponding features
    # For simplicity, assume the features are already computed as a function of board state
    classifier.train(feature_dataset)
    # Test the classifier on the given board state and get the accuracy score
    accuracy = classifier.test(board_state, feature)
    # Return the accuracy score for each layer and each position
    return accuracy

  # Intervene on the hidden states of the model for a given board state and a target move
  def intervene_hidden_states(self, model, board_state, target_move):
    # Encode the board state as a tensor
    input_tensor = self.encode(board_state)
    # Get the hidden states of the model for each layer and each position
    hidden_states = model.get_hidden_states(input_tensor)
    # Define an optimizer to modify the hidden states to maximize the probability of the target move
    optimizer = torch.optim.Adam(hidden_states, lr=0.01)
    # Define a loss function to measure the distance between the prediction and the target move
    loss_function = torch.nn.CrossEntropyLoss()
    # Run the optimizer for a fixed number of steps
    for step in range(100):
      # Predict the output using the modified hidden states
      prediction_tensor = model.predict_from_hidden_states(hidden_states)
      # Compute the loss between the prediction and the target move
      target_tensor = self.encode(target_move)
      loss = loss_function(prediction_tensor, target_tensor)
      # Update the hidden states to minimize the loss
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    # Return the modified hidden states and the final prediction
    return hidden_states, prediction

  # Visualize the hidden states of the model for a given board state and a move using latent saliency maps
  def visualize_hidden_states(self, model, board_state, move):
    # Encode the board state as a tensor
    input_tensor = self.encode(board_state)
    # Get the hidden states of the model for each layer and each position
    hidden_states = model.get_hidden_states(input_tensor)
    # Define a function to compute the saliency score of each hidden unit for a given move
    def saliency_score(hidden_unit, move):
      # Intervene on the hidden unit by setting it to zero
      hidden_unit.zero_()
      # Predict the output using the modified hidden states
      prediction_tensor = model.predict_from_hidden_states(hidden_states)
      # Decode the prediction as a string
      prediction = self.decode(prediction_tensor)
      # Compute the saliency score as the difference between the original and modified predictions for the given move
      original_score = model.get_probability(board_state, move)
      modified_score = model.get_probability(prediction, move)
      saliency_score = original_score - modified_score
      return saliency_score
    
    # Compute the saliency score of each hidden unit for each layer and each position using the function above
    saliency_scores = []
    for layer in range(12):
      for position in range(64):
        for unit in range(768):
          saliency_scores[layer][position][unit] = saliency_score(hidden_states[layer][