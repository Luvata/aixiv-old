---
title: 2209.03160v2 AI Illustrator  Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation
date: 2022-09-04
---

# [AI Illustrator: Translating Raw Descriptions into Images by Prompt-based Cross-Modal Generation](http://arxiv.org/abs/2209.03160v2)

authors: Yiyang Ma, Huan Yang, Bei Liu, Jianlong Fu, Jiaying Liu


## What, Why and How

[1]: https://arxiv.org/abs/2209.03160v2 "[2209.03160v2] AI Illustrator: Translating Raw Descriptions into Images ..."
[2]: https://arxiv.org/pdf/2209.03160v2.pdf "AI Illustrator: Translating Raw Descriptions into Images by Prompt ..."
[3]: http://export.arxiv.org/abs/2209.03160v2 "[2209.03160v2] AI Illustrator: Translating Raw Descriptions into Images ..."

Here is a summary of the paper:

- **What**: The paper proposes a framework for translating raw descriptions with complex semantics into semantically corresponding images, which can be used for AI illustration.
- **Why**: The paper aims to automatically design visually appealing images for books to provoke rich thoughts and emotions. The paper addresses the challenge of handling complex descriptions that may be hard to be visualized (e.g., "gloomy" or "Asian").
- **How**: The paper leverages two powerful pre-trained models, including CLIP and StyleGAN. The paper consists of two components: a projection module from Text Embeddings to Image Embeddings based on prompts, and an adapted image generation module built on StyleGAN which takes Image Embeddings as inputs and is trained by combined semantic consistency losses. The paper also adopts a stylization model as post-processing for better visual effects. The paper does not require external paired data for training. The paper evaluates its method on a benchmark of 200 raw descriptions and conducts a user study to demonstrate its superiority over the competing methods with complicated texts[^1^][1] [^2^][2].

## Main Contributions

The paper claims the following contributions:

- It proposes a novel Prompt-based Cross-Modal Generation Framework (PCM-Frame) that can translate raw descriptions with complex semantics into semantically corresponding images by leveraging pre-trained models.
- It introduces a projection module that can map Text Embeddings to Image Embeddings based on prompts, which can handle various semantic expressions and improve the diversity of the generated images.
- It adapts the image generation module of StyleGAN to take Image Embeddings as inputs and trains it with combined semantic consistency losses to ensure the quality and relevance of the generated images.
- It adopts a stylization model as post-processing to bridge the gap between realistic images and illustration designs, which can enhance the visual effects and artistic styles of the generated images.
- It builds a benchmark of 200 raw descriptions for AI illustration and conducts a user study to show that its method outperforms the competing methods with complicated texts.

## Method Summary

The method section of the paper describes the details of the proposed framework, which consists of three modules: the projection module, the image generation module, and the stylization module.

- The projection module aims to project Text Embeddings to Image Embeddings based on prompts. It first uses CLIP to encode the raw description into a Text Embedding. Then it uses a prompt generator to generate a set of prompts that can capture the semantic aspects of the description. Next, it uses CLIP again to encode each prompt into a Prompt Embedding. Finally, it uses a weighted sum of the Prompt Embeddings to obtain an Image Embedding that can represent the description.
- The image generation module aims to generate an image from the Image Embedding. It adopts the generator of StyleGAN and modifies it to take Image Embeddings as inputs instead of random noise vectors. It also trains the generator with combined semantic consistency losses, which include a CLIP-based loss and an LPIPS-based loss. The CLIP-based loss measures the semantic similarity between the generated image and the description, while the LPIPS-based loss measures the perceptual diversity among different generated images.
- The stylization module aims to enhance the visual effects and artistic styles of the generated images. It adopts a pre-trained stylization model that can transfer the style of a reference image to a content image. It selects a reference image from a pool of illustration images based on its semantic relevance and style diversity with respect to the description and the content image. It then applies the stylization model to transfer the style of the reference image to the content image and produces a stylized image.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a raw description D
# Output: a stylized image I_s

# Projection module
T = CLIP.encode(D) # encode the description into a Text Embedding
P = prompt_generator.generate(T) # generate a set of prompts
E = [] # initialize an empty list of Prompt Embeddings
for p in P:
  e = CLIP.encode(p) # encode each prompt into a Prompt Embedding
  E.append(e)
W = softmax(E * T) # compute the weights for each Prompt Embedding
I_e = sum(W * E) # compute the Image Embedding as a weighted sum of Prompt Embeddings

# Image generation module
G = StyleGAN.generator # load the generator of StyleGAN
L = CLIP_loss + LPIPS_loss # define the combined semantic consistency losses
G.train(L) # train the generator with the losses
I_c = G(I_e) # generate a content image from the Image Embedding

# Stylization module
S = stylization_model # load a pre-trained stylization model
R = reference_selector.select(D, I_c) # select a reference image from a pool of illustration images
I_s = S(I_c, R) # transfer the style of the reference image to the content image

return I_s # return the stylized image
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a raw description D
# Output: a stylized image I_s

# Projection module
T = CLIP.encode(D) # encode the description into a Text Embedding of size 512
P = prompt_generator.generate(T) # generate a set of prompts using a GPT-3 model with T as the input
E = [] # initialize an empty list of Prompt Embeddings
for p in P:
  e = CLIP.encode(p) # encode each prompt into a Prompt Embedding of size 512
  E.append(e)
W = softmax(E * T) # compute the weights for each Prompt Embedding by taking the dot product with T and applying softmax
I_e = sum(W * E) # compute the Image Embedding as a weighted sum of Prompt Embeddings

# Image generation module
G = StyleGAN.generator # load the generator of StyleGAN with 18 layers and 512 latent dimensions
L = CLIP_loss + LPIPS_loss # define the combined semantic consistency losses
CLIP_loss = -log(cosine_similarity(CLIP.encode(G(I_e)), T)) # define the CLIP-based loss as the negative log of the cosine similarity between the generated image and the description
LPIPS_loss = LPIPS(G(I_e), G(I_e + epsilon)) # define the LPIPS-based loss as the LPIPS distance between two slightly different generated images
G.train(L) # train the generator with the losses using Adam optimizer and gradient clipping
I_c = G(I_e) # generate a content image from the Image Embedding

# Stylization module
S = stylization_model # load a pre-trained stylization model based on AdaIN
R = reference_selector.select(D, I_c) # select a reference image from a pool of illustration images using a scoring function that considers semantic relevance and style diversity
I_s = S(I_c, R) # transfer the style of the reference image to the content image using AdaIN

return I_s # return the stylized image
```