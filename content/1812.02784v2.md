---
title: 1812.02784v2 StoryGAN  A Sequential Conditional GAN for Story Visualization
date: 2018-12-03
---

# [StoryGAN: A Sequential Conditional GAN for Story Visualization](http://arxiv.org/abs/1812.02784v2)

authors: Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, Jianfeng Gao


## What, Why and How

[1]: https://arxiv.org/pdf/1812.02784v2.pdf "StoryGAN: A Sequential Conditional GAN for Story Visualization - arXiv.org"
[2]: https://arxiv.org/abs/1812.02784 "StoryGAN: A Sequential Conditional GAN for Story Visualization"
[3]: https://arxiv-export1.library.cornell.edu/abs/1812.02784v2 "[1812.02784v2] StoryGAN: A Sequential Conditional GAN for Story ..."

Here is a summary of the paper:

- **What**: The paper proposes a new task, called Story Visualization, and a new model, called StoryGAN, to generate a sequence of images from a multi-sentence paragraph.
- **Why**: The paper aims to address the challenge of generating coherent and consistent image sequences that depict dynamic scenes and characters based on natural language stories, which is not solved by existing text-to-image or video generation methods.
- **How**: The paper introduces StoryGAN, a sequential conditional GAN framework that consists of a deep Context Encoder that tracks the story flow, and two discriminators at the story and image levels that enhance the image quality and the consistency of the generated sequences. The paper also creates two new datasets, CLEVR-SV and Pororo-SV, to evaluate the model. The paper shows that StoryGAN outperforms state-of-the-art models in image quality, contextual consistency metrics, and human evaluation[^1^][1].

## Main Contributions

The paper claims the following contributions:

- It proposes a new task, Story Visualization, which requires generating a sequence of images from a multi-sentence paragraph.
- It proposes a new model, StoryGAN, which leverages a deep Context Encoder and two discriminators to generate high-quality and consistent image sequences.
- It creates two new datasets, CLEVR-SV and Pororo-SV, which contain stories and corresponding image sequences for evaluation.
- It demonstrates the effectiveness of StoryGAN on the new datasets and compares it with existing models.

## Method Summary

The method section of the paper describes the details of the StoryGAN model, which consists of three components: a Context Encoder, an Image Generator, and two Discriminators. The Context Encoder is a recurrent neural network that encodes each sentence of the story and updates a hidden state that represents the story context. The Image Generator is a conditional GAN that takes the hidden state and a random noise vector as inputs and generates an image for each sentence. The two Discriminators are a Story Discriminator and an Image Discriminator. The Story Discriminator evaluates the global consistency of the generated image sequence and the input story, while the Image Discriminator evaluates the local quality of each generated image and its corresponding sentence. The paper also introduces a novel loss function that combines the adversarial losses from the two discriminators and a KL-divergence loss that encourages the diversity of the generated images. The paper then explains how to train the model using an alternating optimization strategy.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Context Encoder, Image Generator, and two Discriminators
context_encoder = RNN()
image_generator = ConditionalGAN()
story_discriminator = CNN()
image_discriminator = CNN()

# Define the loss function
def loss_function(real_images, fake_images, real_stories, fake_stories):
  # Adversarial losses from the two discriminators
  story_loss = story_discriminator(real_images, real_stories) - story_discriminator(fake_images, fake_stories)
  image_loss = image_discriminator(real_images, real_stories) - image_discriminator(fake_images, fake_stories)
  # KL-divergence loss to encourage diversity
  kl_loss = KL_divergence(fake_images)
  # Total loss
  return story_loss + image_loss + kl_loss

# Define the optimization strategy
def optimize():
  # Initialize the parameters of the model
  initialize_parameters()
  # Loop until convergence
  while not converged:
    # Sample a batch of stories and corresponding image sequences
    stories, images = sample_batch()
    # Encode the stories using the Context Encoder
    hidden_states = context_encoder(stories)
    # Generate fake image sequences using the Image Generator
    fake_images = image_generator(hidden_states)
    # Compute the loss function
    loss = loss_function(images, fake_images, stories, hidden_states)
    # Update the parameters of the model using gradient descent
    update_parameters(loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import numpy as np

# Define the hyperparameters
batch_size = 64 # The size of the mini-batch
num_sentences = 5 # The number of sentences in each story
num_words = 10 # The maximum number of words in each sentence
vocab_size = 10000 # The size of the vocabulary
embed_size = 256 # The size of the word embedding
hidden_size = 512 # The size of the hidden state of the Context Encoder
noise_size = 100 # The size of the random noise vector for the Image Generator
image_size = 64 # The size of the generated image
num_channels = 3 # The number of channels in the image (RGB)
num_filters = 64 # The number of filters for the convolutional layers
lr = 0.0002 # The learning rate for the optimizer
beta1 = 0.5 # The beta1 parameter for the optimizer
beta2 = 0.999 # The beta2 parameter for the optimizer
num_epochs = 100 # The number of epochs to train the model

# Define the Context Encoder, which is a bidirectional GRU with attention mechanism
class ContextEncoder(nn.Module):
  def __init__(self):
    super(ContextEncoder, self).__init__()
    # Define the word embedding layer
    self.embed = nn.Embedding(vocab_size, embed_size)
    # Define the bidirectional GRU layer
    self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)
    # Define the attention layer
    self.attention = nn.Linear(hidden_size * 2, num_sentences)
    # Define the output layer
    self.output = nn.Linear(hidden_size * 2, hidden_size)

  def forward(self, stories):
    # stories: a tensor of shape (batch_size, num_sentences, num_words)
    # Embed the stories into a tensor of shape (batch_size * num_sentences, num_words, embed_size)
    stories = stories.view(-1, num_words) # reshape to (batch_size * num_sentences, num_words)
    stories = self.embed(stories) # embed to (batch_size * num_sentences, num_words, embed_size)
    # Encode the stories using the bidirectional GRU into a tensor of shape (batch_size * num_sentences, hidden_size * 2)
    stories, _ = self.gru(stories) # encode to (num_words, batch_size * num_sentences, hidden_size * 2)
    stories = stories[-1] # take the last hidden state to (batch_size * num_sentences, hidden_size * 2)
    # Compute the attention weights over the sentences into a tensor of shape (batch_size, num_sentences)
    attention_weights = self.attention(stories) # compute to (batch_size * num_sentences, num_sentences)
    attention_weights = attention_weights.view(batch_size, -1) # reshape to (batch_size, num_sentences)
    attention_weights = torch.softmax(attention_weights, dim=1) # normalize to (batch_size, num_sentences)
    # Compute the weighted sum of the encoded sentences into a tensor of shape (batch_size, hidden_size * 2)
    stories = stories.view(batch_size, num_sentences, -1) # reshape to (batch_size, num_sentences, hidden_size * 2)
    context = torch.bmm(attention_weights.unsqueeze(1), stories) # multiply to (batch_size, 1, hidden_size * 2)
    context = context.squeeze(1) # squeeze to (batch_size, hidden_size * 2)
    # Compute the output hidden state into a tensor of shape (batch_size, hidden_size)
    output = self.output(context) # compute to (batch_size, hidden_size)
    return output

# Define the Image Generator, which is a conditional GAN with residual blocks and upsampling layers
class ImageGenerator(nn.Module):
  def __init__(self):
    super(ImageGenerator, self).__init__()
    # Define the input layer that concatenates the hidden state and the noise vector into a tensor of shape (batch_size * num_sentences,
hidden_size + noise_size)
    self.input = nn.Linear(hidden_size + noise_size,
hidden_size + noise_size)
    # Define four residual blocks that process the input into a tensor of shape (batch_size * num_sentences,
hidden_size + noise_size)
    self.res_block1 = ResBlock(hidden_siz