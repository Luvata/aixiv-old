---
title: 2203.00759v2 HyperPrompt  Prompt-based Task-Conditioning of Transformers
date: 2022-03-01
---

# [HyperPrompt: Prompt-based Task-Conditioning of Transformers](http://arxiv.org/abs/2203.00759v2)

authors: Yun He, Huaixiu Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuang Li, Zhao Chen, Donald Metzler, Heng-Tze Cheng, Ed H. Chi


## What, Why and How

[1]: https://arxiv.org/pdf/2203.00759.pdf "arXiv:2203.00759v2 [cs.CL] 15 Jun 2022"
[2]: https://arxiv.org/abs/2203.00759 "HyperPrompt: Prompt-based Task-Conditioning of Transformers"
[3]: http://export.arxiv.org/abs/2203.00759 "[2203.00759] HyperPrompt: Prompt-based Task-Conditioning of Transformers"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes **HyperPrompt**, a novel architecture for prompt-based task-conditioning of self-attention in Transformers. HyperPrompt uses HyperNetworks to generate task-specific hyper-prompts that serve as global memories for the queries to attend to.
- **Why**: The paper aims to address the challenges of parameter-efficient fine-tuning of pre-trained language models for multiple tasks. HyperPrompt leverages the benefits of Prompt-Tuning, which is a new paradigm that conditions large language models with soft learnable memory tokens, while enabling flexible information sharing among tasks.
- **How**: The paper introduces a HyperNetwork that takes as input a task embedding and outputs a hyper-prompt for each layer of the Transformer. The hyper-prompt is then concatenated with the input embeddings and fed into the self-attention mechanism. The paper evaluates HyperPrompt on Natural Language Understanding benchmarks of GLUE and SuperGLUE across different model sizes and shows that it outperforms strong multi-task learning baselines and parameter-efficient adapter variants.

## Main Contributions

According to the paper, the main contributions are:

- They propose **HyperPrompt**, a novel architecture for prompt-based task-conditioning of self-attention in Transformers that uses HyperNetworks to generate task-specific hyper-prompts.
- They show that HyperPrompt can achieve **great parameter and computational efficiency** with as few as 0.14% of additional task-conditioning parameters compared to the original model.
- They demonstrate that HyperPrompt can achieve **superior performances** over strong T5 multi-task learning baselines and parameter-efficient adapter variants including Prompt-Tuning and HyperFormer++ on Natural Language Understanding benchmarks of GLUE and SuperGLUE across many model sizes.

## Method Summary

[1]: https://arxiv.org/pdf/2203.00759.pdf "arXiv:2203.00759v2 [cs.CL] 15 Jun 2022"
[2]: https://arxiv.org/abs/2203.00759 "HyperPrompt: Prompt-based Task-Conditioning of Transformers"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2203.00759 "[2203.00759] HyperPrompt: Prompt-based Task-Conditioning of Transformers"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces a **HyperNetwork** that takes as input a task embedding and outputs a **hyper-prompt** for each layer of the Transformer. The task embedding is learned from a task vocabulary that covers all the tasks in the dataset. The hyper-prompt is a vector of learnable memory tokens that are concatenated with the input embeddings and fed into the self-attention mechanism.
- The paper modifies the self-attention mechanism to incorporate the hyper-prompt as an additional key-value pair. The hyper-prompt serves as a **task global memory** that provides task-specific information for the queries to attend to. The paper also introduces a **task attention mask** that controls the attention between the hyper-prompt and the input tokens.
- The paper proposes two variants of HyperPrompt: **HyperPrompt-Global** and **HyperPrompt-Local**. HyperPrompt-Global uses a single HyperNetwork to generate hyper-prompts for all layers, while HyperPrompt-Local uses separate HyperNetworks for each layer. The paper compares the two variants in terms of parameter efficiency and performance.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a task vocabulary that covers all the tasks in the dataset
task_vocab = ["MNLI", "SST-2", "RTE", ...]

# Define a HyperNetwork that takes a task embedding and outputs a hyper-prompt
def HyperNetwork(task_embedding):
  # Initialize a linear layer with output size equal to the hyper-prompt length
  linear_layer = LinearLayer(output_size = hyper_prompt_length)
  # Apply the linear layer to the task embedding and return the result
  return linear_layer(task_embedding)

# Define a modified self-attention mechanism that incorporates the hyper-prompt
def SelfAttentionWithHyperPrompt(input_embeddings, hyper_prompt):
  # Concatenate the input embeddings and the hyper-prompt along the sequence dimension
  input_and_prompt = Concatenate(input_embeddings, hyper_prompt)
  # Compute the query, key and value matrices from the input and prompt
  query = QueryMatrix(input_and_prompt)
  key = KeyMatrix(input_and_prompt)
  value = ValueMatrix(input_and_prompt)
  # Compute the attention scores between the query and the key
  attention_scores = DotProduct(query, key)
  # Apply a task attention mask to control the attention between the hyper-prompt and the input tokens
  attention_scores = ApplyTaskAttentionMask(attention_scores)
  # Normalize the attention scores with softmax
  attention_weights = Softmax(attention_scores)
  # Compute the output by multiplying the attention weights and the value
  output = DotProduct(attention_weights, value)
  # Return the output
  return output

# Define a Transformer model with HyperPrompt
def TransformerWithHyperPrompt(input_embeddings, task_id):
  # Get the task embedding from the task vocabulary
  task_embedding = Lookup(task_vocab, task_id)
  # For each layer in the Transformer model:
  for layer in TransformerLayers:
    # Generate a hyper-prompt using the HyperNetwork (global or local)
    hyper_prompt = HyperNetwork(task_embedding)
    # Apply self-attention with hyper-prompt to the input embeddings
    output = SelfAttentionWithHyperPrompt(input_embeddings, hyper_prompt)
    # Apply feed-forward network and residual connection to the output
    output = FeedForwardNetwork(output) + output
    # Update the input embeddings for the next layer
    input_embeddings = output
  # Return the final output of the Transformer model
  return output

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the hyperparameters
num_tasks = 10 # The number of tasks in the dataset
task_vocab_size = 100 # The size of the task vocabulary
task_embedding_size = 32 # The size of the task embedding
hyper_prompt_length = 16 # The length of the hyper-prompt
model_size = 512 # The size of the model
num_heads = 8 # The number of attention heads
num_layers = 6 # The number of Transformer layers
dropout_rate = 0.1 # The dropout rate

# Define a task vocabulary that covers all the tasks in the dataset
task_vocab = nn.Embedding(task_vocab_size, task_embedding_size)

# Define a HyperNetwork that takes a task embedding and outputs a hyper-prompt
class HyperNetwork(nn.Module):
  def __init__(self):
    super(HyperNetwork, self).__init__()
    # Initialize a linear layer with output size equal to the hyper-prompt length times the model size
    self.linear_layer = nn.Linear(task_embedding_size, hyper_prompt_length * model_size)
  
  def forward(self, task_embedding):
    # Apply the linear layer to the task embedding and reshape the result to (batch_size, hyper_prompt_length, model_size)
    hyper_prompt = self.linear_layer(task_embedding).view(-1, hyper_prompt_length, model_size)
    # Return the hyper-prompt
    return hyper_prompt

# Define a modified self-attention mechanism that incorporates the hyper-prompt
class SelfAttentionWithHyperPrompt(nn.Module):
  def __init__(self):
    super(SelfAttentionWithHyperPrompt, self).__init__()
    # Initialize linear layers for computing query, key and value matrices
    self.query_layer = nn.Linear(model_size, model_size)
    self.key_layer = nn.Linear(model_size, model_size)
    self.value_layer = nn.Linear(model_size, model_size)
    # Initialize a dropout layer for applying dropout to the attention weights
    self.dropout_layer = nn.Dropout(dropout_rate)
    # Initialize a scaling factor for normalizing the dot product
    self.scaling_factor = torch.sqrt(torch.tensor(model_size / num_heads))
  
  def forward(self, input_embeddings, hyper_prompt, attention_mask):
    # Concatenate the input embeddings and the hyper-prompt along the sequence dimension
    input_and_prompt = torch.cat([input_embeddings, hyper_prompt], dim=1)
    # Compute the query, key and value matrices from the input and prompt by applying linear layers and splitting into multiple heads
    query = self.query_layer(input_and_prompt).view(-1, input_and_prompt.size(1), num_heads, model_size // num_heads).transpose(1, 2)
    key = self.key_layer(input_and_prompt).view(-1, input_and_prompt.size(1), num_heads, model_size // num_heads).transpose(1, 2)
    value = self.value_layer(input_and_prompt).view(-1, input_and_prompt.size(1), num_heads, model_size // num_heads).transpose(1, 2)
    # Compute the attention scores between the query and the key by applying dot product and scaling
    attention_scores = torch.matmul(query, key.transpose(-1, -2)) / self.scaling_factor
    # Apply a task attention mask to control the attention between the hyper-prompt and the input tokens by adding a large negative value to masked positions
    attention_scores = attention_scores + attention_mask.unsqueeze(1)
    # Normalize the attention scores with softmax
    attention_weights = F.softmax(attention_scores, dim=-1)
    # Apply dropout to the attention weights
    attention_weights = self.dropout_layer(attention_weights)
    # Compute the output by multiplying the attention weights and the value and concatenating the heads
    output = torch.matmul(attention_weights, value).transpose(1, 2).contiguous().view(-1, input_and_prompt.size(1), model_size)
    # Return the output and the attention weights
    return output, attention_weights

# Define a feed-forward network for each Transformer layer
class FeedForwardNetwork(nn.Module):
  def __init__(self):
    super(FeedForwardNetwork, self).__init__()
    # Initialize linear layers with intermediate and output sizes
    self.linear_layer_1 = nn.Linear(model_size, model_size * 4)
    self.linear_layer_2 = nn.Linear(model_size * 4, model_size)
    # Initialize a dropout layer for applying dropout to the output
    self.dropout_layer = nn.Dropout(dropout_rate)

  def forward(self, input):
    # Apply the first linear layer with a ReLU activation
    output = F.relu(self.linear_layer_1(input))
    # Apply dropout to the output
    output = self.dropout_layer(output)
    # Apply the second linear layer
    output = self.linear_layer_2(output)
    # Return the output
    return output

# Define a Transformer layer with HyperPrompt
class TransformerLayerWithHyperPrompt(nn.Module):
  def __init__(self, hyper_network):
    super(TransformerLayerWithHyperPrompt, self).__init__()
    # Initialize a HyperNetwork for generating hyper-prompt
    self.hyper_network = hyper_network
    # Initialize a self-attention mechanism with hyper-prompt
    self.self_attention_with_hyper_prompt = SelfAttentionWithHyperPrompt()
    # Initialize a feed-forward network
    self.feed_forward_network = FeedForwardNetwork()
    # Initialize layer normalization layers for applying normalization after each sub-layer
    self.layer_norm_1 = nn.LayerNorm(model_size)
    self.layer_norm_2 = nn.LayerNorm(model_size)

  def forward(self, input_embeddings, task_embedding, attention_mask):
    # Generate a hyper-prompt using the HyperNetwork
    hyper_prompt = self.hyper_network(task_embedding)
    # Apply self-attention with hyper-prompt to the input embeddings and add a residual connection
    attention_output, attention_weights = self.self_attention_with_hyper_prompt(input_embeddings, hyper_prompt, attention_mask)
    attention_output = attention_output + input_embeddings
    # Apply layer normalization to the attention output
    attention_output = self.layer_norm_1(attention_output)
    # Apply feed-forward network to the attention output and add a residual connection
    feed_forward_output = self.feed_forward_network(attention_output)
    feed_forward_output = feed_forward_output + attention_output
    # Apply layer normalization to the feed-forward output
    feed_forward_output = self.layer_norm_2(feed_forward_output)
    # Return the feed-forward output and the attention weights
    return feed_forward_output, attention_weights

# Define a Transformer model with HyperPrompt
class TransformerWithHyperPrompt(nn.Module):
  def __init__(self):
    super(TransformerWithHyperPrompt, self).__init__()
    # Initialize an embedding layer for input tokens
    self.input_embedding_layer = nn.Embedding(input_vocab_size, model_size)
    # Initialize a positional encoding layer for adding positional information to input embeddings
    self.positional_encoding_layer = PositionalEncodingLayer(max_position, model_size)
    # Initialize a list of Transformer layers with HyperPrompt
    self.transformer_layers_with_hyper_prompt = nn.ModuleList([TransformerLayerWithHyperPrompt(HyperNetwork()) for _ in range(num_layers)])
  
  def forward(self, input_tokens, task_id):
    # Get the task embedding from the task vocabulary
    task_embedding = task_vocab(task_id)
    # Get the input embeddings from the input tokens and add positional encoding
    input_embeddings = self.input_embedding_layer(input_tokens) + self.positional_encoding_layer(input_tokens)
    # For each layer in the Transformer model:
    for layer in self.transformer_layers_with_hyper_prompt:
      # Apply the layer to the input embeddings and get the output and the attention weights
      output, attention_weights = layer(input_embeddings, task_embedding, attention_mask)
      # Update the input embeddings for the next layer
      input_embeddings = output
    # Return the final output of the Transformer model
    return output

```