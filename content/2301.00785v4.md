---
title: 2301.00785v4 CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection
date: 2023-01-01
---

# [CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection](http://arxiv.org/abs/2301.00785v4)

authors: Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A. Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, Zongwei Zhou


## What, Why and How

[1]: https://arxiv.org/abs/2301.00785 "[2301.00785] CLIP-Driven Universal Model for Organ Segmentation and ..."
[2]: https://arxiv.org/pdf/2301.00785 "PDF for 2301.00785 - arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2301.00785v4 "[2301.00785v4] CLIP-Driven Universal Model for Organ Segmentation and ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a **CLIP-Driven Universal Model** for organ segmentation and tumor detection in abdominal CT scans. The model can segment 25 organs and 6 types of tumors by exploiting the semantic relationship between anatomical structures learned from CLIP, a contrastive language-image pre-training method.
- **Why**: The paper aims to tackle the limitations of existing models that are dataset-specific, small, partially labeled, and rarely investigate severe tumor subjects. The paper also aims to enable the model to be easily extended to new classes without forgetting the previous ones.
- **How**: The paper develops the model from an assembly of 14 datasets with 3,410 CT scans and evaluates it on 6,162 external CT scans from 3 datasets. The paper introduces CLIP embedding to the segmentation model, which allows the model to leverage natural language descriptions of anatomical structures. The paper also uses a novel loss function that combines cross-entropy loss and contrastive loss to train the model. The paper shows that the model achieves state-of-the-art results on Beyond The Cranial Vault (BTCV) dataset, outperforms dataset-specific models on generalization and transfer learning, and can segment new classes with few-shot learning.

## Main Contributions

According to the paper, the main contributions are:

- The paper introduces a **CLIP-Driven Universal Model** that can segment 25 organs and 6 types of tumors in abdominal CT scans by exploiting the semantic relationship between anatomical structures learned from CLIP.
- The paper presents a **novel loss function** that combines cross-entropy loss and contrastive loss to train the model effectively and efficiently.
- The paper demonstrates the **state-of-the-art performance** of the model on BTCV dataset, which is a large-scale and diverse dataset for organ segmentation and tumor detection.
- The paper shows the **superior generalization and transfer learning** ability of the model compared to dataset-specific models on external datasets from varying sites and tasks.
- The paper proves the **scalability and extensibility** of the model to new classes with few-shot learning without forgetting the previous ones.

## Method Summary

[1]: https://arxiv.org/abs/2301.00785 "[2301.00785] CLIP-Driven Universal Model for Organ Segmentation and ..."
[2]: https://arxiv.org/pdf/2301.00785 "PDF for 2301.00785 - arXiv.org"
[3]: https://arxiv-export2.library.cornell.edu/abs/2301.00785v4 "[2301.00785v4] CLIP-Driven Universal Model for Organ Segmentation and ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper adopts a **U-Net** architecture as the backbone of the segmentation model, which consists of an encoder and a decoder. The encoder extracts features from the input image, and the decoder reconstructs the segmentation mask from the features. The paper uses **skip connections** between the encoder and the decoder to preserve spatial information.
- The paper introduces **CLIP embedding** to the segmentation model, which is a vector representation of natural language descriptions of anatomical structures learned from CLIP. CLIP is a contrastive language-image pre-training method that learns to align text and images in a large-scale dataset. The paper uses CLIP embedding as an additional input to the decoder, which helps the model to exploit the semantic relationship between abdominal structures and improve segmentation accuracy.
- The paper proposes a **novel loss function** that combines cross-entropy loss and contrastive loss to train the model. Cross-entropy loss measures the pixel-wise similarity between the predicted mask and the ground truth mask. Contrastive loss measures the feature-wise similarity between the predicted mask and the CLIP embedding. The paper uses a temperature parameter to balance the two losses and optimize them jointly.
- The paper develops the model from an assembly of 14 datasets with 3,410 CT scans, which cover 25 organs and 6 types of tumors in abdominal region. The paper uses data augmentation techniques such as random cropping, scaling, rotation, flipping, and elastic deformation to increase the diversity of the training data. The paper also uses a class-balanced sampling strategy to deal with the class imbalance problem in organ segmentation and tumor detection.
- The paper evaluates the model on 6,162 external CT scans from 3 datasets: Beyond The Cranial Vault (BTCV), Medical Segmentation Decathlon (MSD), and Liver Tumor Segmentation Challenge (LiTS). The paper uses Dice score as the main metric to measure the segmentation performance. The paper also compares the model with dataset-specific models trained on each dataset separately. The paper shows that the model achieves state-of-the-art results on BTCV dataset, outperforms dataset-specific models on generalization and transfer learning, and can segment new classes with few-shot learning.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the U-Net architecture with an encoder and a decoder
model = UNet()

# Load the CLIP model and get the embedding of the text descriptions
clip_model = load_clip_model()
text_descriptions = ["liver", "spleen", "kidney", ...]
clip_embeddings = clip_model.encode_text(text_descriptions)

# Define the loss function that combines cross-entropy loss and contrastive loss
def loss_function(pred_mask, gt_mask, pred_feature, clip_embedding):
  # Compute the cross-entropy loss between the predicted mask and the ground truth mask
  ce_loss = cross_entropy_loss(pred_mask, gt_mask)
  # Compute the contrastive loss between the predicted feature and the CLIP embedding
  ct_loss = contrastive_loss(pred_feature, clip_embedding)
  # Balance the two losses with a temperature parameter
  temperature = 0.07
  total_loss = ce_loss + temperature * ct_loss
  return total_loss

# Load the training data from 14 datasets
train_data = load_data(datasets)

# Train the model with data augmentation and class-balanced sampling
for epoch in range(num_epochs):
  for batch in train_data:
    # Apply data augmentation techniques to the input image
    image = augment_image(batch.image)
    # Get the ground truth mask and the corresponding text description
    gt_mask = batch.mask
    text = batch.text
    # Get the CLIP embedding of the text description
    clip_embedding = clip_embeddings[text]
    # Forward pass the image and the CLIP embedding to the model
    pred_mask, pred_feature = model(image, clip_embedding)
    # Compute the loss function
    loss = loss_function(pred_mask, gt_mask, pred_feature, clip_embedding)
    # Backward pass and update the model parameters
    loss.backward()
    optimizer.step()

# Evaluate the model on external datasets
test_data = load_data([BTCV, MSD, LiTS])
for batch in test_data:
  # Get the input image and the ground truth mask
  image = batch.image
  gt_mask = batch.mask
  # Forward pass the image to the model without CLIP embedding
  pred_mask, _ = model(image)
  # Compute the Dice score between the predicted mask and the ground truth mask
  dice_score = dice_score(pred_mask, gt_mask)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define the U-Net architecture with an encoder and a decoder
class UNet(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super(UNet, self).__init__()
    # Define the encoder blocks with convolutional layers and max pooling layers
    self.encoder1 = self.conv_block(in_channels, 64)
    self.encoder2 = self.conv_block(64, 128)
    self.encoder3 = self.conv_block(128, 256)
    self.encoder4 = self.conv_block(256, 512)
    # Define the max pooling layer
    self.pool = torch.nn.MaxPool2d(2)
    # Define the bottleneck block with convolutional layers and dropout layer
    self.bottleneck = torch.nn.Sequential(
      torch.nn.Conv2d(512, 1024, 3, padding=1),
      torch.nn.BatchNorm2d(1024),
      torch.nn.ReLU(),
      torch.nn.Dropout(0.5),
      torch.nn.Conv2d(1024, 1024, 3, padding=1),
      torch.nn.BatchNorm2d(1024),
      torch.nn.ReLU(),
      torch.nn.Dropout(0.5)
    )
    # Define the decoder blocks with convolutional layers and up-sampling layers
    self.decoder1 = self.conv_block(1536, 512)
    self.decoder2 = self.conv_block(768, 256)
    self.decoder3 = self.conv_block(384, 128)
    self.decoder4 = self.conv_block(192, 64)
    # Define the up-sampling layer
    self.up = torch.nn.Upsample(scale_factor=2, mode="bilinear", align_corners=True)
    # Define the output layer with a convolutional layer
    self.output = torch.nn.Conv2d(64, out_channels, 1)

  # Define a helper function to create a convolutional block
  def conv_block(self, in_channels, out_channels):
    return torch.nn.Sequential(
      torch.nn.Conv2d(in_channels, out_channels, 3, padding=1),
      torch.nn.BatchNorm2d(out_channels),
      torch.nn.ReLU(),
      torch.nn.Conv2d(out_channels, out_channels, 3, padding=1),
      torch.nn.BatchNorm2d(out_channels),
      torch.nn.ReLU()
    )

  # Define the forward pass of the model
  def forward(self, x, clip_embedding):
    # Encode the input image
    e1 = self.encoder1(x) # e1.shape = (batch_size, 64, h/1, w/1)
    e2 = self.encoder2(self.pool(e1)) # e2.shape = (batch_size, 128, h/2, w/2)
    e3 = self.encoder3(self.pool(e2)) # e3.shape = (batch_size, 256, h/4, w/4)
    e4 = self.encoder4(self.pool(e3)) # e4.shape = (batch_size, 512, h/8, w/8)
    # Pass through the bottleneck
    b = self.bottleneck(self.pool(e4)) # b.shape = (batch_size, 1024, h/16, w/16)
    # Decode the features and concatenate with CLIP embedding
    d1 = torch.cat([e4, self.up(b)], dim=1) # d1.shape = (batch_size, 1536, h/8 , w/8 )
    d1 = d1 + clip_embedding.unsqueeze(-1).unsqueeze(-1) # add CLIP embedding to each pixel
    d1 = self.decoder1(d1) # d1.shape = (batch_size ,512 , h/8 , w/8 )
    d2 = torch.cat([e3,self.up(d1)], dim=1) # d2.shape = (batch_size ,768 , h/4 , w/4 )
    d2 = self.decoder2(d2) # d2.shape = (batch_size ,256 , h/4 , w/4 )
    d3 = torch.cat([e2,self.up(d2)], dim=1) # d3.shape = (batch_size ,384 , h/2 , w/2 )
    d3 = self.decoder3(d3) # d3.shape = (batch_size ,128 , h/2 , w/2 )
    d4 = torch.cat([e1,self.up(d3)], dim=1) # d4.shape = (batch_size ,192 , h/1 , w/1 )
    d4 = self.decoder4(d4) # d4.shape = (batch_size ,64 , h/1 , w/1 )
    # Output the predicted mask
    out = self.output(d4) # out.shape = (batch_size, out_channels, h/1, w/1)
    # Return the predicted mask and the bottleneck feature
    return out, b

# Load the CLIP model and get the embedding of the text descriptions
clip_model = clip.load("ViT-B/32", device="cuda")
text_descriptions = ["liver", "spleen", "kidney", ...]
clip_embeddings = clip_model.encode_text(clip.tokenize(text_descriptions).to("cuda"))

# Define the loss function that combines cross-entropy loss and contrastive loss
def loss_function(pred_mask, gt_mask, pred_feature, clip_embedding):
  # Compute the cross-entropy loss between the predicted mask and the ground truth mask
  ce_loss = torch.nn.CrossEntropyLoss()(pred_mask, gt_mask)
  # Compute the contrastive loss between the predicted feature and the CLIP embedding
  pred_feature = pred_feature.view(pred_feature.size(0), -1) # flatten the feature map
  clip_embedding = clip_embedding.unsqueeze(0).repeat(pred_feature.size(0), 1) # repeat the embedding for each sample
  similarity = torch.nn.CosineSimilarity(dim=1)(pred_feature, clip_embedding) # compute the cosine similarity
  ct_loss = -torch.mean(similarity) # compute the negative mean similarity
  # Balance the two losses with a temperature parameter
  temperature = 0.07
  total_loss = ce_loss + temperature * ct_loss
  return total_loss

# Load the training data from 14 datasets
train_data = load_data(datasets)

# Create an instance of the model and an optimizer
model = UNet(in_channels=1, out_channels=25).to("cuda")
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train the model with data augmentation and class-balanced sampling
for epoch in range(num_epochs):
  for batch in train_data:
    # Apply data augmentation techniques to the input image
    image = augment_image(batch.image).to("cuda")
    # Get the ground truth mask and the corresponding text description
    gt_mask = batch.mask.to("cuda")
    text = batch.text
    # Get the CLIP embedding of the text description
    clip_embedding = clip_embeddings[text].to("cuda")
    # Forward pass the image and the CLIP embedding to the model
    pred_mask, pred_feature = model(image, clip_embedding)
    # Compute the loss function
    loss = loss_function(pred_mask, gt_mask, pred_feature, clip_embedding)
    # Backward pass and update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Evaluate the model on external datasets
test_data = load_data([BTCV, MSD, LiTS])
for batch in test_data:
  # Get the input image and the ground truth mask
  image = batch.image.to("cuda")
  gt_mask = batch.mask.to("cuda")
  # Forward pass the image to the model without CLIP embedding
  pred_mask, _ = model(image)
  # Compute the Dice score between the predicted mask and the ground truth mask
  dice_score = dice_score(pred_mask, gt_mask)
```