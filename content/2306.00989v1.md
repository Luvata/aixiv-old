---
title: 2306.00989v1 Hiera  A Hierarchical Vision Transformer without the Bells-and-Whistles
date: 2023-06-01
---

# [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](http://arxiv.org/abs/2306.00989v1)

authors: Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00989v1.pdf "Abstract arXiv:2306.00989v1 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2306.00989 "[2306.00989] Hiera: A Hierarchical Vision Transformer without the Bells ..."
[3]: http://export.arxiv.org/abs/2306.00989 "[2306.00989] Hiera: A Hierarchical Vision Transformer without the Bells ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper proposes Hiera, a hierarchical vision transformer that removes the extra components from previous models and relies on a strong visual pretext task (MAE) for pretraining.
- **Why**: The paper argues that the extra components in previous models are unnecessary and make them slower than vanilla ViT. The paper aims to create a simple, efficient, and accurate model that is fast across many image and video tasks.
- **How**: The paper uses a multi-stage architecture with different patch sizes and channel dimensions for each stage. The paper uses MAE as the pretraining task, which learns to reconstruct masked patches using attention. The paper evaluates Hiera on ImageNet-1K, ImageNet-21K, ImageNet-V2, COCO detection and segmentation, Kinetics-400, and Something-Something V2. The paper shows that Hiera outperforms previous models in accuracy and speed.

## Main Contributions

[1]: https://arxiv.org/pdf/2306.00989v1.pdf "Abstract arXiv:2306.00989v1 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2306.00989 "[2306.00989] Hiera: A Hierarchical Vision Transformer without the Bells ..."
[3]: http://export.arxiv.org/abs/2306.00989 "[2306.00989] Hiera: A Hierarchical Vision Transformer without the Bells ..."

According to the paper[^1^][1], the main contributions are:

- **Hiera**: A simple and efficient hierarchical vision transformer that removes the extra components from previous models and relies on MAE for pretraining.
- **MAE**: A powerful visual pretext task that learns to reconstruct masked patches using attention and enables Hiera to achieve competitive accuracy with fewer parameters and FLOPs.
- **Experiments**: A comprehensive evaluation of Hiera on various image and video recognition tasks, showing that it outperforms previous models in accuracy and speed.

## Method Summary

[1]: https://arxiv.org/pdf/2306.00989v1.pdf "Abstract arXiv:2306.00989v1 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2306.00989 "[2306.00989] Hiera: A Hierarchical Vision Transformer without the Bells ..."
[3]: http://export.arxiv.org/abs/2306.00989 "[2306.00989] Hiera: A Hierarchical Vision Transformer without the Bells ..."

Here is a summary of the method section of the paper[^1^][1]:

- **Model Architecture**: The paper uses a multi-stage architecture with different patch sizes and channel dimensions for each stage. The paper follows the design of MViT (Fan et al.,2021), but removes the convolutional stem, the convolutional projection, and the relative position embeddings. The paper also simplifies the patch merging operation by using average pooling instead of linear projection. The paper uses four stages with patch sizes of 16, 8, 4, and 2 pixels and channel dimensions of 64, 128, 256, and 512 respectively. The paper uses 2, 2, 10, and 2 transformer blocks for each stage. Each transformer block consists of a multi-head self-attention layer followed by a feed-forward network with a hidden dimension of four times the input dimension. The paper uses layer normalization and dropout for regularization.
- **Pretraining Task**: The paper uses MAE (He et al.,2022) as the pretraining task, which learns to reconstruct masked patches using attention. The paper randomly masks out patches from the input image and feeds them to the model. The model then predicts the original patches using a decoder that shares weights with the encoder. The paper uses a patch-level contrastive loss to encourage the model to learn discriminative features for each patch. The paper also uses a pixel-level reconstruction loss to encourage the model to generate high-fidelity patches. The paper pretrains Hiera on ImageNet-21K (Sun et al.,2017) with a batch size of 4096 and a learning rate of 0.001 for 300 epochs.
- **Fine-tuning Task**: The paper fine-tunes Hiera on various image and video recognition tasks, such as ImageNet-1K (Deng et al.,2009), ImageNet-V2 (Recht et al.,2019), COCO detection and segmentation (Lin et al.,2014), Kinetics-400 (Carreira and Zisserman,2017), and Something-Something V2 (Goyal et al.,2017). The paper uses different learning rates, batch sizes, epochs, and data augmentations for each task. The paper also uses different heads for classification and detection tasks. For classification tasks, the paper uses a linear layer that takes the output of the last transformer block as input. For detection tasks, the paper uses a deformable DETR (Zhu et al.,2020) head that takes the outputs of all transformer blocks as input.


## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms

# Define hyperparameters
num_stages = 4 # number of stages in the model
patch_sizes = [16, 8, 4, 2] # patch sizes for each stage
channel_dims = [64, 128, 256, 512] # channel dimensions for each stage
num_blocks = [2, 2, 10, 2] # number of transformer blocks for each stage
num_heads = [1, 2, 4, 8] # number of attention heads for each stage
hidden_dim = 4 # hidden dimension multiplier for feed-forward network
dropout = 0.1 # dropout rate for regularization
mask_prob = 0.15 # probability of masking a patch during pretraining
temperature = 0.07 # temperature parameter for contrastive loss
pretrain_lr = 0.001 # learning rate for pretraining
pretrain_bs = 4096 # batch size for pretraining
pretrain_epochs = 300 # number of epochs for pretraining

# Define model class
class Hiera(nn.Module):
    def __init__(self):
        super(Hiera, self).__init__()
        # Initialize stages
        self.stages = nn.ModuleList()
        for i in range(num_stages):
            self.stages.append(Stage(patch_sizes[i], channel_dims[i], num_blocks[i], num_heads[i]))
        # Initialize decoder for pretraining
        self.decoder = Decoder(channel_dims[-1])
        # Initialize head for finetuning
        self.head = None
    
    def forward(self, x):
        # Input x is a batch of images of shape (B, C, H, W)
        B, C, H, W = x.shape
        # Apply stages sequentially
        for stage in self.stages:
            x = stage(x)
        # Output x is a batch of feature maps of shape (B, D, P, P)
        B, D, P, P = x.shape
        # Flatten patches and apply layer normalization
        x = x.flatten(2).transpose(1, 2) # shape: (B, P*P, D)
        x = F.layer_norm(x, [D]) # shape: (B, P*P, D)
        # If pretraining mode, apply decoder and return reconstructed patches and features
        if self.training and self.head is None:
            y = self.decoder(x) # shape: (B, P*P, C*patch_sizes[-1]**2)
            return y, x 
        # If finetuning mode, apply head and return predictions
        else:
            y = self.head(x) # shape depends on the task
            return y

# Define stage class
class Stage(nn.Module):
    def __init__(self, patch_size, channel_dim, num_blocks, num_heads):
        super(Stage, self).__init__()
        self.patch_size = patch_size
        self.channel_dim = channel_dim
        self.num_blocks = num_blocks
        self.num_heads = num_heads
        # Initialize transformer blocks
        self.blocks = nn.ModuleList()
        for i in range(num_blocks):
            self.blocks.append(Block(channel_dim, num_heads))
    
    def forward(self, x):
        # Input x is a batch of feature maps of shape (B, C_in, H_in, W_in)
        B, C_in, H_in, W_in = x.shape
        # If first stage, reshape input to patches of shape (B*C_in/patch_size**2 , patch_size**2 , H_in/patch_size , W_in/patch_size)
        if C_in == 3:
            x = x.reshape(B*C_in//self.patch_size**2 , self.patch_size**2 , H_in//self.patch_size , W_in//self.patch_size).permute(0 , 2 , 3 , 1)
            H_out = H_in // self.patch_size 
            W_out = W_in // self.patch_size 
            C_out = C_in * self.patch_size**2 
        # Else merge patches by average pooling and reshape to patches of shape (B*C_in/patch_size**2 , patch_size**2 , H_in/patch_size , W_in/patch_size)
        else:
            x = F.avg_pool2d(x , kernel_size=self.patch_size//2 , stride=self.patch_size//2).reshape(B*C_in//self.patch_size**2 , self.patch_size**2 , H_in//self.patch_size , W_in//self.patch_size).permute(0 , 2 , 3 , 1)
            H_out = H_in // self.patch_size 
            W_out = W_in // self.patch_size 
            C_out = C_in * self.patch_size**2 // 4
        # Project patches to channel dimension
        x = F.linear(x, weight=torch.randn(self.channel_dim, C_out)) # shape: (B*C_in/patch_size**2 , H_out , W_out , channel_dim)
        # Apply transformer blocks sequentially
        for block in self.blocks:
            x = block(x)
        # Output x is a batch of feature maps of shape (B, channel_dim, H_out, W_out)
        x = x.permute(0, 3, 1, 2).reshape(B, self.channel_dim, H_out, W_out)
        return x

# Define transformer block class
class Block(nn.Module):
    def __init__(self, channel_dim, num_heads):
        super(Block, self).__init__()
        self.channel_dim = channel_dim
        self.num_heads = num_heads
        # Initialize multi-head self-attention layer
        self.attn = nn.MultiheadAttention(channel_dim, num_heads, dropout=dropout)
        # Initialize feed-forward network
        self.ffn = nn.Sequential(
            nn.Linear(channel_dim, hidden_dim * channel_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * channel_dim, channel_dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x):
        # Input x is a batch of patches of shape (B*C_in/patch_size**2 , H_out , W_out , channel_dim)
        B, H, W, C = x.shape
        # Reshape and transpose x to match the input format of multi-head attention
        x = x.reshape(B*H*W, C).transpose(0, 1) # shape: (channel_dim, B*H*W)
        # Apply multi-head self-attention layer with residual connection and layer normalization
        x = F.layer_norm(x + self.attn(x, x, x)[0], [C]) # shape: (channel_dim, B*H*W)
        # Transpose and reshape x to match the input format of feed-forward network
        x = x.transpose(0, 1).reshape(B, H, W, C) # shape: (B*C_in/patch_size**2 , H_out , W_out , channel_dim)
        # Apply feed-forward network with residual connection and layer normalization
        x = F.layer_norm(x + self.ffn(x), [C]) # shape: (B*C_in/patch_size**2 , H_out , W_out , channel_dim)
        return x

# Define decoder class
class Decoder(nn.Module):
    def __init__(self, channel_dim):
        super(Decoder, self).__init__()
        self.channel_dim = channel_dim
        # Initialize decoder blocks
        self.blocks = nn.ModuleList()
        for i in range(num_stages):
            self.blocks.append(Block(channel_dim, num_heads[-1]))
    
    def forward(self, x):
        # Input x is a batch of features of shape (B, P*P, D)
        B, P2, D = x.shape
        P = int(math.sqrt(P2)) # patch size of the last stage
        # Apply decoder blocks sequentially
        for block in self.blocks:
            x = block(x)
        # Output y is a batch of reconstructed patches of shape (B, P*P, C*patch_sizes[-1]**2)
        y = F.linear(x.reshape(B*P2,D), weight=torch.randn(C*patch_sizes[-1]**2,D)).reshape(B,P2,C*patch_sizes[-1]**2)
        return y

# Define contrastive loss function
def contrastive_loss(y_true, y_pred):
    # Input y_true is a batch of original patches of shape (B, P*P, C*patch_sizes[-1]**2)
    # Input y_pred is a batch of reconstructed patches of shape (B, P*P, C*patch_sizes[-1]**2)
    B, P2, C2 = y_true.shape
    P = int(math.sqrt(P2)) # patch size of the last stage
    C = int(math.sqrt(C2)) # patch size of the input image
    # Normalize patches along the feature dimension
    y_true = F.normalize(y_true.reshape(B*P2,C2), dim=1) # shape: (B*P*P,C*C)
    y_pred = F.normalize(y_pred.reshape(B*P2,C2), dim=1) # shape: (B*P*P,C*C)
    # Compute cosine similarity matrix

```