---
title: 2203.15243v2 Fine-tuning Image Transformers using Learnable Memory
date: 2022-03-16
---

# [Fine-tuning Image Transformers using Learnable Memory](http://arxiv.org/abs/2203.15243v2)

authors: Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, Andrew Jackson


## What, Why and How

[1]: https://arxiv.org/abs/2203.15243 "Fine-tuning Image Transformers using Learnable Memory"
[2]: https://arxiv.org/pdf/2203.15243v2.pdf "arXiv.org e-Print archive"
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the paper:

- **What**: The paper proposes a method to fine-tune image transformers using learnable memory tokens that provide contextual information for specific tasks.
- **Why**: The paper aims to improve the accuracy and efficiency of image transformers when adapting to new tasks, while optionally preserving their capabilities on previously learned tasks.
- **How**: The paper introduces a set of learnable embedding vectors at each layer of the transformer, called memory tokens, that are updated during fine-tuning. The paper also proposes an attention-masking approach that enables extension to new downstream tasks with computation reuse. The paper evaluates the method on several image classification and segmentation datasets and shows that it outperforms conventional head-only fine-tuning and performs close to full fine-tuning with fewer parameters[^1^][1].

## Main Contributions

The paper claims the following contributions:

- A novel method to fine-tune image transformers using learnable memory tokens that provide task-specific contextual information.
- An attention-masking approach that enables extension to new downstream tasks with computation reuse and parameter efficiency.
- An empirical evaluation of the method on several image classification and segmentation datasets, showing its advantages over conventional and full fine-tuning.

## Method Summary

[1]: https://arxiv.org/abs/2203.15243 "Fine-tuning Image Transformers using Learnable Memory"
[2]: https://arxiv.org/pdf/2203.15243v2.pdf "arXiv.org e-Print archive"
[3]: https://pubmed.ncbi.nlm.nih.gov/15447808/ "How to write the methods section of a research paper - PubMed"

Here is a summary of the method section of the paper:

- The paper proposes to augment vision transformer models with learnable memory tokens that are added to the input sequence at each layer of the transformer. The memory tokens are initialized randomly and updated during fine-tuning using gradient descent. The paper argues that the memory tokens can capture contextual information that is useful for specific tasks and datasets, and can improve the accuracy and efficiency of fine-tuning.
- The paper also proposes an attention-masking approach that enables extension to new downstream tasks with computation reuse and parameter efficiency. The paper argues that by masking out the attention weights between the memory tokens and the original input tokens, the model can preserve its original capabilities while adapting to new tasks. The paper also shows how to combine multiple attention masks for different tasks into a single mask that can be applied at inference time, allowing the model to execute both old and new tasks as a part of single inference at a small incremental cost.
- The paper describes the details of the implementation and training of the proposed method, such as the number and dimension of memory tokens, the learning rate and optimizer, the data augmentation and regularization techniques, and the evaluation metrics and baselines. The paper also provides some ablation studies and qualitative analysis to demonstrate the effectiveness and interpretability of the proposed method.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a vision transformer model with L layers
model = VisionTransformer()

# Initialize M memory tokens per layer with dimension D
memory_tokens = torch.randn(L, M, D)

# Define an attention mask for each task
attention_masks = {}

# For each task T in the set of tasks
for T in tasks:

  # Fine-tune the model and the memory tokens on task T
  for batch in data_loader[T]:
    # Concatenate the memory tokens to the input tokens at each layer
    input_tokens = torch.cat([input_tokens, memory_tokens], dim=1)
    # Compute the output and the loss
    output = model(input_tokens)
    loss = criterion(output, labels)
    # Update the model and the memory tokens using gradient descent
    loss.backward()
    optimizer.step()

  # Mask out the attention weights between the memory tokens and the input tokens
  attention_mask = torch.zeros(N+M, N+M)
  attention_mask[:N, :N] = 1 # keep the original attention weights
  attention_mask[N:, N:] = 1 # keep the self-attention weights of the memory tokens
  # Store the attention mask for task T
  attention_masks[T] = attention_mask

# Combine the attention masks for different tasks into a single mask
combined_mask = torch.max(torch.stack(attention_masks.values()), dim=0)

# For inference, apply the combined mask to the model
model.apply_mask(combined_mask)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary modules
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms

# Define some hyperparameters
L = 12 # number of layers in the transformer
M = 4 # number of memory tokens per layer
D = 768 # dimension of the tokens
N = 196 # number of input tokens (14x14 patches)
H = 12 # number of attention heads
LR = 1e-4 # learning rate
EPOCHS = 10 # number of epochs for fine-tuning
BATCH_SIZE = 32 # batch size for fine-tuning
TASKS = ["CIFAR10", "CIFAR100", "ImageNet"] # list of tasks

# Define a vision transformer model with L layers
model = VisionTransformer(
  image_size=224,
  patch_size=16,
  num_classes=1000,
  dim=D,
  depth=L,
  heads=H,
)

# Initialize M memory tokens per layer with dimension D
memory_tokens = nn.Parameter(torch.randn(L, M, D))

# Define an attention mask for each task
attention_masks = {}

# Define a cross-entropy loss function
criterion = nn.CrossEntropyLoss()

# Define an Adam optimizer for the model and the memory tokens
optimizer = torch.optim.Adam([model.parameters(), memory_tokens], lr=LR)

# Define some data augmentation and normalization transforms
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

# Load the datasets for each task
datasets = {}
datasets["CIFAR10"] = {
  "train": torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train),
  "test": torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test),
}
datasets["CIFAR100"] = {
  "train": torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train),
  "test": torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test),
}
datasets["ImageNet"] = {
  "train": torchvision.datasets.ImageNet(root='./data', split='train', download=True, transform=transform_train),
  "test": torchvision.datasets.ImageNet(root='./data', split='val', download=True, transform=transform_test),
}

# Define the data loaders for each task
data_loaders = {}
for T in TASKS:
  data_loaders[T] = {
    "train": torch.utils.data.DataLoader(datasets[T]["train"], batch_size=BATCH_SIZE, shuffle=True),
    "test": torch.utils.data.DataLoader(datasets[T]["test"], batch_size=BATCH_SIZE, shuffle=False),
  }

# For each task T in the set of tasks
for T in TASKS:

  # Fine-tune the model and the memory tokens on task T
  for epoch in range(EPOCHS):
    # Set the model to training mode
    model.train()
    # Loop over the training batches
    for i, (inputs, labels) in enumerate(data_loaders[T]["train"]):
      # Move the inputs and labels to the device (GPU or CPU)
      inputs = inputs.to(device)
      labels = labels.to(device)
      # Zero the parameter gradients
      optimizer.zero_grad()
      # Concatenate the memory tokens to the input tokens at each layer
      input_tokens = model.tokenize(inputs) # shape: (BATCH_SIZE, N+1, D)
      input_tokens = torch.cat([input_tokens, memory_tokens.repeat(BATCH_SIZE,1,1)], dim=1) # shape: (BATCH_SIZE, N+M+1, D)
      # Compute the output and the loss
      output = model(input_tokens)[:,0] # shape: (BATCH_SIZE, num_classes)
      loss = criterion(output, labels)
      # Update the model and the memory tokens using gradient descent
      loss.backward()
      optimizer.step()
      # Print some statistics every 200 batches
      if (i+1) % 200 == 0:
        print(f"[{epoch+1}, {i+1}] loss: {loss.item():.3f}")

  # Evaluate the model on the test set of task T
  # Set the model to evaluation mode
  model.eval()
  # Initialize some variables to store the accuracy and the total number of samples
  correct = 0
  total = 0
  # Loop over the test batches
  with torch.no_grad():
    for inputs, labels in data_loaders[T]["test"]:
      # Move the inputs and labels to the device (GPU or CPU)
      inputs = inputs.to(device)
      labels = labels.to(device)
      # Concatenate the memory tokens to the input tokens at each layer
      input_tokens = model.tokenize(inputs) # shape: (BATCH_SIZE, N+1, D)
      input_tokens = torch.cat([input_tokens, memory_tokens.repeat(BATCH_SIZE,1,1)], dim=1) # shape: (BATCH_SIZE, N+M+1, D)
      # Compute the output and the predictions
      output = model(input_tokens)[:,0] # shape: (BATCH_SIZE, num_classes)
      _, predicted = torch.max(output, 1)
      # Update the accuracy and the total number of samples
      total += labels.size(0)
      correct += (predicted == labels).sum().item()
  # Print the accuracy for task T
  print(f"Accuracy of the model on task {T}: {100 * correct / total:.2f}%")

  # Mask out the attention weights between the memory tokens and the input tokens
  attention_mask = torch.zeros(N+M+1, N+M+1)
  attention_mask[:N+1, :N+1] = 1 # keep the original attention weights
  attention_mask[N+1:, N+1:] = 1 # keep the self-attention weights of the memory tokens
  # Store the attention mask for task T
  attention_masks[T] = attention_mask

# Combine the attention masks for different tasks into a single mask
combined_mask = torch.max(torch.stack(attention_masks.values()), dim=0)

# For inference, apply the combined mask to the model
model.apply_mask(combined_mask)
```