---
title: 2306.00407v2 Towards Interactive Image Inpainting via Sketch Refinement
date: 2023-06-01
---

# [Towards Interactive Image Inpainting via Sketch Refinement](http://arxiv.org/abs/2306.00407v2)

authors: Chang Liu, Shunxin Xu, Jialun Peng, Kaidong Zhang, Dong Liu


## What, Why and How

[1]: https://arxiv.org/abs/2306.00407 "Towards Interactive Image Inpainting via Sketch Refinement"
[2]: https://arxiv.org/pdf/2305.00407v2.pdf "arXiv:2305.00407v2 [math.NA] 16 May 2023"
[3]: https://arxiv.org/pdf/2306.00407.pdf "arXiv.org"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a two-stage image inpainting method called SketchRefiner that can restore complex structures in corrupted regions of images with the help of user-provided sketches.
- **Why**: The paper aims to address the problem of interactive image inpainting, which leverages additional hints such as sketches to assist the inpainting process. Sketches are simple and intuitive to end users, but they also have free forms with much randomness that may confuse the inpainting models and cause artifacts in completed images.
- **How**: The paper introduces a cross-correlation loss function to robustly calibrate and refine the user-provided sketches in a coarse-to-fine fashion in the first stage. In the second stage, the paper learns to extract informative features from the abstracted sketches in the feature space and modulate the inpainting process. The paper also proposes an algorithm to simulate real sketches automatically and builds a test protocol with different applications.


## Main Contributions

According to the paper at , the main contributions are:

- A novel cross-correlation loss function that can effectively refine the user-provided sketches and reduce the artifacts in the inpainted images.
- A feature modulation mechanism that can exploit the sketch information in the feature space and guide the inpainting process.
- An automatic sketch simulation algorithm that can generate realistic sketches from images and facilitate the evaluation of interactive image inpainting methods.
- A comprehensive test protocol that covers different scenarios and applications of interactive image inpainting, such as object removal, structure completion, and content creation.


## Method Summary

[1]: https://arxiv.org/abs/2306.00407 "Towards Interactive Image Inpainting via Sketch Refinement"
[2]: https://arxiv.org/pdf/2305.00407v2.pdf "arXiv:2305.00407v2 [math.NA] 16 May 2023"
[3]: https://arxiv.org/abs/2305.00407v2 "[2305.00407v2] An optimal error estimate for a mixed finite element ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper presents a two-stage image inpainting method called SketchRefiner that consists of a sketch refinement stage and an image completion stage.
- In the sketch refinement stage, the paper proposes a cross-correlation loss function that measures the similarity between the user-provided sketch and the ground truth edge map in the corrupted region. The paper uses a coarse-to-fine strategy to refine the sketch progressively and reduce the randomness and noise in the sketch.
- In the image completion stage, the paper adopts a generative adversarial network (GAN) framework that consists of a generator and a discriminator. The paper introduces a feature modulation mechanism that extracts features from the refined sketch and modulates the generator's features through adaptive instance normalization (AdaIN). The paper also uses a perceptual loss function that combines content loss, style loss, and adversarial loss to guide the image completion process.
- The paper also proposes an automatic sketch simulation algorithm that can generate realistic sketches from images by using edge detection, thinning, and random perturbation techniques. The paper uses this algorithm to create a large-scale dataset for interactive image inpainting evaluation.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# Input: a corrupted image I and a user-provided sketch S
# Output: an inpainted image O

# Sketch refinement stage
for each scale level from coarse to fine
  compute the edge map E of I using Canny edge detector
  compute the cross-correlation loss L between S and E in the corrupted region
  update S by minimizing L using gradient descent
end for

# Image completion stage
initialize a generator G and a discriminator D
for each training iteration
  extract features F from the refined sketch S using a feature extractor network
  modulate the features of G using AdaIN with F as the style code
  generate an inpainted image O from I and S using G
  compute the content loss, style loss, and adversarial loss for O
  update G and D by minimizing the perceptual loss function
end for

# Return the inpainted image O
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Input: a corrupted image I and a user-provided sketch S
# Output: an inpainted image O

# Define the hyperparameters
num_scales = 3 # the number of scale levels for sketch refinement
alpha = 0.1 # the weight for cross-correlation loss
beta = 0.01 # the weight for sketch smoothness loss
gamma = 0.001 # the weight for sketch sparsity loss
delta = 0.0001 # the weight for sketch consistency loss
epsilon = 0.00001 # the weight for sketch gradient loss
eta = 10 # the weight for content loss
theta = 250 # the weight for style loss
phi = 0.1 # the weight for adversarial loss
num_iter = 10000 # the number of training iterations
batch_size = 16 # the batch size for training
lr_g = 0.0002 # the learning rate for generator
lr_d = 0.0002 # the learning rate for discriminator

# Sketch refinement stage
# Define the sketch refinement network SRN as a U-Net with residual blocks
# Define the cross-correlation loss function L_cc(S, E) as the negative Pearson correlation coefficient between S and E
# Define the sketch smoothness loss function L_sm(S) as the L1 norm of the Laplacian of S
# Define the sketch sparsity loss function L_sp(S) as the L1 norm of S
# Define the sketch consistency loss function L_sc(S, I) as the L1 norm of S * (1 - I)
# Define the sketch gradient loss function L_sg(S) as the L1 norm of the gradient magnitude of S
# Define the total loss function L(S, E, I) as alpha * L_cc(S, E) + beta * L_sm(S) + gamma * L_sp(S) + delta * L_sc(S, I) + epsilon * L_sg(S)
# Define an optimizer OPT_S for SRN with Adam algorithm

for each scale level from coarse to fine
  downsample I and S to the current scale level
  compute the edge map E of I using Canny edge detector with a threshold of 0.5
  feed I and S to SRN and get the refined sketch S'
  compute L(S', E, I) and backpropagate the gradients to SRN
  update SRN by OPT_S
end for

# Image completion stage
# Define the generator network G as a U-Net with residual blocks and AdaIN layers
# Define the discriminator network D as a PatchGAN with spectral normalization
# Define the feature extractor network F as a VGG-19 network pretrained on ImageNet
# Define the content loss function L_c(O, T) as the L1 norm of the difference between O and T in the feature space of F
# Define the style loss function L_s(O, S) as the sum of squared Frobenius norm of the difference between O and S in Gram matrices of F
# Define the adversarial loss function L_a(O, T) as the hinge loss between O and T in D's output
# Define the perceptual loss function L_p(O, T, S) as eta * L_c(O, T) + theta * L_s(O, S) + phi * L_a(O, T)
# Define an optimizer OPT_G for G with Adam algorithm
# Define an optimizer OPT_D for D with Adam algorithm

for each training iteration
  sample a batch of corrupted images I, sketches S, and ground truth images T from the dataset
  extract features F from S using F and reshape them to match G's feature maps
  modulate G's feature maps using AdaIN with F as the style code
  generate inpainted images O from I and S using G
  compute L_p(O, T, S) and backpropagate the gradients to G and D separately
  update G by OPT_G and D by OPT_D with gradient clipping
end for

# Return the inpainted image O from G(I, S)
```