---
title: 2207.02812v3 Towards Counterfactual Image Manipulation via CLIP
date: 2022-07-03
---

# [Towards Counterfactual Image Manipulation via CLIP](http://arxiv.org/abs/2207.02812v3)

authors: Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jiahui Zhang, Shijian Lu, Miaomiao Cui, Xuansong Xie, Xian-Sheng Hua, Chunyan Miao


## What, Why and How

[1]: https://arxiv.org/abs/2207.02812v3 "[2207.02812v3] Towards Counterfactual Image Manipulation via CLIP"
[2]: https://arxiv.org/pdf/2207.02812v3 "Towards Counterfactual Image Manipulation via CLIP - arXiv.org"
[3]: http://export.arxiv.org/abs/2207.02812v3 "[2207.02812v3] Towards Counterfactual Image Manipulation via CLIP"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method for text-driven counterfactual image manipulation, which can edit images according to text descriptions that are against the learned priors of generative models, such as "green lipstick" or "blue dog".
- **Why**: The paper aims to address the challenge of counterfactual editing, which is difficult for existing methods that rely on StyleGAN's latent codes and CLIP's semantic knowledge, as they often produce unrealistic or undesired results due to local minimum or adversarial solutions.
- **How**: The paper designs a contrastive loss that exploits predefined CLIP-space directions to guide the editing toward desired directions from different perspectives, and a text embedding mapping scheme that fuses CLIP embeddings of target text with latent codes for effective optimization and accurate editing. The paper demonstrates that the proposed method can achieve more accurate and robust counterfactual editing than the state-of-the-art StyleCLIP method.

## Main Contributions

[1]: https://arxiv.org/abs/2207.02812v3 "[2207.02812v3] Towards Counterfactual Image Manipulation via CLIP"
[2]: https://arxiv.org/pdf/2207.02812v3 "Towards Counterfactual Image Manipulation via CLIP - arXiv.org"
[3]: http://export.arxiv.org/abs/2207.02812v3 "[2207.02812v3] Towards Counterfactual Image Manipulation via CLIP"

The paper[^1^][1] lists the following contributions:

- **A novel contrastive loss CLIP-NCE** that exploits predefined CLIP-space directions to guide the editing toward desired directions from different perspectives, such as similarity, diversity, and realism.
- **A simple yet effective text embedding mapping scheme** that explicitly maps CLIP embeddings of target text to the latent space and fuses them with latent codes for effective latent code optimization and accurate editing.
- **Extensive experiments** that show the proposed method can achieve more accurate and robust counterfactual editing than the state-of-the-art StyleCLIP method on various counterfactual concepts and datasets.

## Method Summary

[1]: https://arxiv.org/abs/2207.02812v3 "[2207.02812v3] Towards Counterfactual Image Manipulation via CLIP"
[2]: https://arxiv.org/pdf/2207.02812v3 "Towards Counterfactual Image Manipulation via CLIP - arXiv.org"
[3]: http://export.arxiv.org/abs/2207.02812v3 "[2207.02812v3] Towards Counterfactual Image Manipulation via CLIP"

Here is a summary of the method section of the paper[^1^][1]:

- The paper adopts a text-driven counterfactual image manipulation framework that consists of three components: a StyleGAN-based generator, a CLIP-based evaluator, and an optimizer that updates the latent codes of the generator according to the target text.
- The paper introduces a novel contrastive loss CLIP-NCE that exploits predefined CLIP-space directions to guide the editing toward desired directions from different perspectives, such as similarity, diversity, and realism. The loss consists of three terms: a similarity term that maximizes the CLIP score between the edited image and the target text, a diversity term that minimizes the CLIP score between the edited image and the original text, and a realism term that minimizes the CLIP score between the edited image and a negative text that describes unrealistic images.
- The paper also introduces a simple yet effective text embedding mapping scheme that explicitly maps CLIP embeddings of target text to the latent space and fuses them with latent codes for effective latent code optimization and accurate editing. The scheme consists of two steps: first, it learns a linear mapping function that projects CLIP embeddings to the latent space using paired data; second, it fuses the mapped text embedding with the latent code using element-wise addition or concatenation.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an original image x, a target text t, a StyleGAN-based generator G, a CLIP-based evaluator E, and a linear mapping function M
# Output: an edited image y that matches the target text t

# Initialize the latent code w of the original image x
w = G.invert(x)

# Map the target text t to the latent space using M
z = M(E.encode_text(t))

# Fuse the mapped text embedding z with the latent code w using element-wise addition or concatenation
w' = w + z # or w' = concatenate(w, z)

# Define the contrastive loss CLIP-NCE using predefined CLIP-space directions
def CLIP_NCE(w', t):
  # Generate an edited image y from the latent code w'
  y = G(w')
  
  # Compute the CLIP score between y and t
  s_yt = E.score(y, t)
  
  # Compute the CLIP score between y and the original text t0
  s_yt0 = E.score(y, t0)
  
  # Compute the CLIP score between y and a negative text tn that describes unrealistic images
  s_ytn = E.score(y, tn)
  
  # Define the similarity term as the logit of s_yt
  L_sim = logit(s_yt)
  
  # Define the diversity term as the negative logit of s_yt0
  L_div = -logit(s_yt0)
  
  # Define the realism term as the negative logit of s_ytn
  L_real = -logit(s_ytn)
  
  # Return the weighted sum of the three terms as the contrastive loss
  return alpha * L_sim + beta * L_div + gamma * L_real

# Optimize the latent code w' using gradient descent to minimize the contrastive loss CLIP-NCE
for i in range(max_iterations):
  w' = w' - lr * grad(CLIP_NCE(w', t), w')
  
# Return the final edited image y from the optimized latent code w'
y = G(w')
return y
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import stylegan2

# Load the pretrained StyleGAN2 generator and CLIP evaluator
G = stylegan2.load_pretrained_model('ffhq')
E = clip.load_model('ViT-B/32')

# Load the paired data of images and texts for learning the linear mapping function M
data = load_data('data.pkl')

# Define the linear mapping function M as a torch.nn.Linear layer with input size 512 and output size 512
M = torch.nn.Linear(512, 512)

# Define the optimizer for M as torch.optim.Adam with learning rate 0.001
optimizer_M = torch.optim.Adam(M.parameters(), lr=0.001)

# Define the loss function for M as torch.nn.MSELoss
loss_M = torch.nn.MSELoss()

# Train the linear mapping function M for 100 epochs
for epoch in range(100):
  # Shuffle the data
  data.shuffle()
  
  # Loop over the batches of data
  for batch in data:
    # Get the images and texts from the batch
    images, texts = batch
    
    # Encode the images using StyleGAN2's encoder and get the latent codes w
    w = G.encoder(images)
    
    # Encode the texts using CLIP's text encoder and get the text embeddings t
    t = E.encode_text(texts)
    
    # Forward pass the text embeddings t through M and get the mapped embeddings z
    z = M(t)
    
    # Compute the mean squared error loss between z and w
    loss = loss_M(z, w)
    
    # Backpropagate the loss and update M's parameters
    optimizer_M.zero_grad()
    loss.backward()
    optimizer_M.step()
    
  # Print the epoch and loss
  print(f'Epoch {epoch}, Loss {loss.item()}')

# Define a function for text-driven counterfactual image manipulation
def counterfactual_editing(image, text, alpha, beta, gamma, max_iterations, lr):
  # Input: an original image x, a target text t, and hyperparameters alpha, beta, gamma, max_iterations, and lr
  # Output: an edited image y that matches the target text t
  
  # Encode the original image x using StyleGAN2's encoder and get the latent code w
  w = G.encoder(image)
  
  # Encode the target text t using CLIP's text encoder and get the text embedding t
  t = E.encode_text(text)
  
  # Map the text embedding t to the latent space using M and get the mapped embedding z
  z = M(t)
  
  # Fuse the mapped embedding z with the latent code w using element-wise addition or concatenation
  w' = w + z # or w' = torch.cat(w, z)
  
  # Define a function for computing the contrastive loss CLIP-NCE using predefined CLIP-space directions
  def CLIP_NCE(w', t):
    # Generate an edited image y from the latent code w'
    y = G(w')
    
    # Compute the CLIP score between y and t using CLIP's image-text similarity score function
    s_yt = E.score(y, t)
    
    # Compute the CLIP score between y and the original text t0 using CLIP's image-text similarity score function
    s_yt0 = E.score(y, t0)
    
    # Compute the CLIP score between y and a negative text tn that describes unrealistic images using CLIP's image-text similarity score function
    s_ytn = E.score(y, tn)
    
    # Define the similarity term as the logit of s_yt using torch.logit function
    L_sim = torch.logit(s_yt)
    
    # Define the diversity term as the negative logit of s_yt0 using torch.logit function
    L_div = -torch.logit(s_yt0)
    
    # Define the realism term as the negative logit of s_ytn using torch.logit function
    L_real = -torch.logit(s_ytn)
    
    # Return the weighted sum of the three terms as the contrastive loss
    return alpha * L_sim + beta * L_div + gamma * L_real
  
  # Optimize the latent code w' using gradient descent to minimize the contrastive loss CLIP-NCE
  for i in range(max_iterations):
    w' = w' - lr * torch.autograd.grad(CLIP_NCE(w', t), w')[0]
    
  # Return the final edited image y from the optimized latent code w'
  y = G(w')
  return y
```