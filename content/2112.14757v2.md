---
title: 2112.14757v2 A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-language Model
date: 2021-12-15
---

# [A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre-trained Vision-language Model](http://arxiv.org/abs/2112.14757v2)

authors: Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, Xiang Bai


## What, Why and How

[1]: https://arxiv.org/pdf/2112.14757v2 "A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre ..."
[2]: https://arxiv.org/abs/2112.14757 "[2112.14757] A Simple Baseline for Open-Vocabulary Semantic ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.14757v1 "arXiv:2112.14757v1 [cs.CV] 29 Dec 2021"

Here is a summary of the paper:

```
What: The paper proposes a simple baseline for open-vocabulary semantic segmentation, which is the task of assigning category labels to every pixel of an image without knowing the categories in advance.

Why: The paper aims to overcome the limitations of existing semantic segmentation methods that rely on a fixed and limited set of categories, and to leverage the power of pre-trained vision-language models, such as CLIP, that can classify arbitrary categories based on natural language descriptions.

How: The paper adopts a two-stage framework that first extracts generalizable mask proposals from an image using an off-the-shelf detector, and then performs open-vocabulary classification on the masked image crops using an image-based CLIP model. The paper shows that this framework outperforms previous methods of zero-shot semantic segmentation on several datasets, and can handle diverse and unseen categories.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2112.14757v2 "A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre ..."
[2]: https://arxiv.org/abs/2112.14757 "[2112.14757] A Simple Baseline for Open-Vocabulary Semantic ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.14757v1 "arXiv:2112.14757v1 [cs.CV] 29 Dec 2021"

Here are some of the contributions of the paper:

- The paper proposes a simple and effective two-stage framework for open-vocabulary semantic segmentation that leverages an off-the-shelf pre-trained vision-language model, i.e., CLIP.
- The paper shows that the two-stage framework can handle diverse and unseen categories by performing open-vocabulary classification on masked image crops, which are generated by extracting generalizable mask proposals in the first stage.
- The paper demonstrates that the two-stage framework outperforms previous methods of zero-shot semantic segmentation on several datasets, such as Pascal VOC 2012 and COCO Stuff, by a large margin.
- The paper provides a publicly available codebase to facilitate future research on open-vocabulary semantic segmentation.

## Method Summary

[1]: https://arxiv.org/pdf/2112.14757v2 "A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre ..."
[2]: https://arxiv.org/abs/2112.14757 "[2112.14757] A Simple Baseline for Open-Vocabulary Semantic ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.14757v1 "arXiv:2112.14757v1 [cs.CV] 29 Dec 2021"

Here is a summary of the method section of the paper:

```
The method section of the paper describes the two-stage framework for open-vocabulary semantic segmentation, which consists of a mask proposal stage and a classification stage.

The mask proposal stage aims to generate generalizable mask proposals that can cover diverse and unseen categories. The authors use an off-the-shelf detector, Mask R-CNN, trained on COCO Stuff dataset, to extract mask proposals from an input image. The authors also apply some post-processing steps, such as non-maximum suppression and confidence thresholding, to filter out low-quality proposals.

The classification stage aims to perform open-vocabulary classification on the masked image crops, which are obtained by applying the mask proposals to the input image. The authors use an off-the-shelf pre-trained vision-language model, CLIP, to classify the masked image crops based on natural language descriptions. The authors use a set of 1,000 categories from ImageNet-22K as the open vocabulary, and use the cosine similarity between the image and text embeddings as the classification score. The authors also introduce a background category to handle the case where none of the categories match the masked image crop.
```

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```
# Input: an image I and a set of categories C
# Output: a semantic segmentation map M

# Mask proposal stage
P = Mask R-CNN(I) # get mask proposals from Mask R-CNN
P = NMS(P) # apply non-maximum suppression to P
P = filter(P, threshold) # filter out proposals with low confidence

# Classification stage
M = zeros(size(I)) # initialize segmentation map
for each proposal p in P:
  crop = I * p # get masked image crop
  scores = CLIP(crop, C) # get classification scores from CLIP
  scores.append(background_score(p)) # add background score
  label = argmax(scores) # get the label with highest score
  M[p] = label # assign label to proposal pixels

return M
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2112.14757v2 "A Simple Baseline for Open-Vocabulary Semantic Segmentation with Pre ..."
[2]: https://arxiv.org/abs/2112.14757 "[2112.14757] A Simple Baseline for Open-Vocabulary Semantic ... - arXiv.org"
[3]: https://arxiv.org/pdf/2112.14757v1 "arXiv:2112.14757v1 [cs.CV] 29 Dec 2021"

Here is the detailed pseudo code for the paper:

```
# Input: an image I and a set of categories C
# Output: a semantic segmentation map M

# Mask proposal stage
P = Mask R-CNN(I) # get mask proposals from Mask R-CNN
P = NMS(P, iou_threshold=0.5) # apply non-maximum suppression to P with IoU threshold of 0.5
P = filter(P, conf_threshold=0.5) # filter out proposals with confidence lower than 0.5

# Classification stage
M = zeros(size(I)) # initialize segmentation map
for each proposal p in P:
  crop = I * p # get masked image crop
  crop = resize(crop, 224, 224) # resize crop to 224x224
  scores = CLIP(crop, C) # get classification scores from CLIP
  scores.append(background_score(p, M)) # add background score based on overlap with existing labels in M
  label = argmax(scores) # get the label with highest score
  if label != background: # if not background
    M[p] = label # assign label to proposal pixels

return M

# Background score function
def background_score(p, M):
  overlap = count(M[p] != 0) / count(p != 0) # compute the ratio of overlapping pixels with existing labels in M
  return sigmoid(overlap - 0.5) # return a score between 0 and 1 based on overlap
```