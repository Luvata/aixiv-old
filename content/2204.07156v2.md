---
title: 2204.07156v2 Any-resolution Training for High-resolution Image Synthesis
date: 2022-04-08
---

# [Any-resolution Training for High-resolution Image Synthesis](http://arxiv.org/abs/2204.07156v2)

authors: Lucy Chai, Michael Gharbi, Eli Shechtman, Phillip Isola, Richard Zhang


## What, Why and How

[1]: https://arxiv.org/abs/2204.07156 "Any-resolution Training for High-resolution Image Synthesis"
[2]: http://export.arxiv.org/abs/2204.07156 "[2204.07156] Any-resolution Training for High-resolution Image Synthesis"
[3]: https://arxiv.org/abs/2204.07562 "[2204.07562] Evaluating Factuality in Text Simplification - arXiv.org"

Here is a summary of the paper:

- **What**: The paper proposes a new method for training generative models that can synthesize high-resolution images at any scale, using variable-size images collected at their native resolutions.
- **Why**: The paper argues that existing generative models operate at fixed resolution, which limits their ability to capture the diversity and details of natural images. The paper also claims that using variable-size images can provide more supervision and avoid downsampling or discarding high-resolution or low-resolution images.
- **How**: The paper introduces continuous-scale training, which samples patches at random scales from the variable-size images and trains a generator with variable output resolutions. The generator is conditioned on a target scale and continuous coordinates, which allow it to generate higher resolution images than previously possible and maintain a consistent global layout across different scales. The paper evaluates the method on several natural image domains and shows that it outperforms discrete multi-scale approaches in terms of FID scores and visual quality.

## Main Contributions

[1]: https://arxiv.org/abs/2204.07156 "Any-resolution Training for High-resolution Image Synthesis"
[2]: http://export.arxiv.org/abs/2204.07156 "[2204.07156] Any-resolution Training for High-resolution Image Synthesis"
[3]: https://arxiv.org/abs/2204.07562 "[2204.07562] Evaluating Factuality in Text Simplification - arXiv.org"

According to the paper[^1^][1], the main contributions are:

- **Creating datasets with variable-size images** collected at their native resolutions, which capture the diversity and details of natural images better than fixed-resolution images.
- **Introducing continuous-scale training**, a process that samples patches at random scales from the variable-size images and trains a generator with variable output resolutions, conditioned on a target scale and continuous coordinates.
- **Demonstrating arbitrary scale synthesis** with both coherent global layouts and realistic local details, going beyond 2K resolution in the experiments, and outperforming discrete multi-scale approaches in terms of FID scores and visual quality.

## Method Summary

[1]: https://arxiv.org/abs/2204.07156 "Any-resolution Training for High-resolution Image Synthesis"
[2]: http://export.arxiv.org/abs/2204.07156 "[2204.07156] Any-resolution Training for High-resolution Image Synthesis"
[3]: https://arxiv.org/abs/2204.07562 "[2204.07562] Evaluating Factuality in Text Simplification - arXiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the **data collection** process, which involves scraping images from Flickr and Google Images using keywords related to natural image domains, such as churches, mountains, and birds. The images are collected at their native resolutions and stored in a variable-size dataset.
- The paper introduces the **continuous-scale training** process, which consists of two steps: patch sampling and patch synthesis. Patch sampling randomly selects a patch size and a scale factor from a predefined range, and then crops a patch from a variable-size image according to the scale factor. Patch synthesis uses a generator network that takes as input a latent vector, a target scale, and continuous coordinates, and outputs a patch at the target scale. The generator is trained with an adversarial loss and a feature matching loss against a discriminator network that classifies patches as real or fake.
- The paper also describes the **arbitrary scale synthesis** process, which allows generating images at any desired scale by stitching together patches from the generator. The paper proposes two methods for stitching: grid-based stitching and overlap-based stitching. Grid-based stitching divides the output image into a grid of patches and generates each patch independently using the generator. Overlap-based stitching generates patches sequentially with some overlap between them and blends them using alpha compositing. The paper compares the two methods and shows that overlap-based stitching produces smoother transitions and fewer artifacts.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Data collection
images = scrape_images_from_Flickr_and_Google_Images(keywords)
variable_size_dataset = store_images_at_native_resolutions(images)

# Continuous-scale training
generator = Generator()
discriminator = Discriminator()
for epoch in epochs:
  for image in variable_size_dataset:
    # Patch sampling
    patch_size = random_choice(patch_size_range)
    scale_factor = random_choice(scale_factor_range)
    patch = crop_patch_from_image(image, patch_size, scale_factor)
    # Patch synthesis
    latent_vector = sample_latent_vector()
    target_scale = random_choice(target_scale_range)
    continuous_coordinates = compute_continuous_coordinates(patch_size, target_scale)
    generated_patch = generator(latent_vector, target_scale, continuous_coordinates)
    # Adversarial and feature matching losses
    real_score = discriminator(patch)
    fake_score = discriminator(generated_patch)
    adversarial_loss = compute_adversarial_loss(real_score, fake_score)
    feature_matching_loss = compute_feature_matching_loss(patch, generated_patch)
    total_loss = adversarial_loss + feature_matching_loss
    # Update generator and discriminator parameters
    update_parameters(generator, discriminator, total_loss)

# Arbitrary scale synthesis
desired_scale = user_input()
output_image_size = compute_output_image_size(desired_scale)
# Grid-based stitching
output_image_grid = divide_output_image_into_grid(output_image_size)
for patch in output_image_grid:
  latent_vector = sample_latent_vector()
  continuous_coordinates = compute_continuous_coordinates(patch_size, desired_scale)
  generated_patch = generator(latent_vector, desired_scale, continuous_coordinates)
  paste_generated_patch_to_output_image_grid(generated_patch, patch)
output_image_grid_based = stitch_output_image_grid_together(output_image_grid)
# Overlap-based stitching
output_image_overlap_based = initialize_empty_output_image(output_image_size)
current_patch_position = (0, 0)
while current_patch_position is not out of bounds:
  latent_vector = sample_latent_vector()
  continuous_coordinates = compute_continuous_coordinates(patch_size, desired_scale)
  generated_patch = generator(latent_vector, desired_scale, continuous_coordinates)
  paste_generated_patch_to_output_image_overlap_based(generated_patch, current_patch_position, alpha_compositing)
  current_patch_position = move_current_patch_position_by_overlap_amount(current_patch_position) 
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Data collection
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO

keywords = ["churches", "mountains", "birds"] # example keywords
images = [] # list of images
for keyword in keywords:
  # Scrape images from Flickr
  flickr_url = f"https://www.flickr.com/search/?text={keyword}"
  flickr_response = requests.get(flickr_url)
  flickr_soup = BeautifulSoup(flickr_response.text, "html.parser")
  flickr_image_tags = flickr_soup.find_all("img", class_="view photo-list-photo-view requiredToShowOnServer awake")
  for flickr_image_tag in flickr_image_tags:
    flickr_image_url = flickr_image_tag["src"]
    flickr_image_response = requests.get(flickr_image_url)
    flickr_image = Image.open(BytesIO(flickr_image_response.content))
    images.append(flickr_image)
  # Scrape images from Google Images
  google_url = f"https://www.google.com/search?q={keyword}&tbm=isch"
  google_response = requests.get(google_url)
  google_soup = BeautifulSoup(google_response.text, "html.parser")
  google_image_tags = google_soup.find_all("img", class_="rg_i Q4LuWd")
  for google_image_tag in google_image_tags:
    google_image_url = google_image_tag["src"]
    google_image_response = requests.get(google_image_url)
    google_image = Image.open(BytesIO(google_image_response.content))
    images.append(google_image)

variable_size_dataset = [] # list of images at native resolutions
for image in images:
  native_resolution = image.size # get the width and height of the image
  variable_size_dataset.append((image, native_resolution))

# Continuous-scale training
import torch
import torch.nn as nn
import torchvision.models as models

# Define the generator network
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    self.latent_dim = 512 # dimension of the latent vector
    self.scale_dim = 1 # dimension of the target scale
    self.coord_dim = 2 # dimension of the continuous coordinates
    self.input_dim = self.latent_dim + self.scale_dim + self.coord_dim # dimension of the input to the generator
    self.hidden_dim = 256 # dimension of the hidden layer
    self.output_dim = 3 # dimension of the output patch (RGB channels)
    self.linear1 = nn.Linear(self.input_dim, self.hidden_dim) # first linear layer
    self.relu1 = nn.ReLU() # first activation layer
    self.linear2 = nn.Linear(self.hidden_dim, self.output_dim) # second linear layer

  def forward(self, latent_vector, target_scale, continuous_coordinates):
    input_vector = torch.cat([latent_vector, target_scale, continuous_coordinates], dim=-1) # concatenate the input components
    hidden_vector = self.linear1(input_vector) # apply the first linear layer
    hidden_vector = self.relu1(hidden_vector) # apply the first activation layer
    output_vector = self.linear2(hidden_vector) # apply the second linear layer
    output_patch = output_vector.reshape(-1, self.output_dim, patch_size, patch_size) # reshape the output vector to a patch
    return output_patch

# Define the discriminator network
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    self.patch_size = patch_size # size of the input patch
    self.input_dim = 3 # dimension of the input patch (RGB channels)
    self.hidden_dim = 64 # dimension of the hidden layer
    self.output_dim = 1 # dimension of the output score (real or fake)
    self.conv1 = nn.Conv2d(self.input_dim, self.hidden_dim, kernel_size=3, stride=2, padding=1) # first convolutional layer
    self.relu1 = nn.ReLU() # first activation layer
    self.conv2 = nn.Conv2d(self.hidden_dim, self.output_dim, kernel_size=3, stride=2, padding=1) # second convolutional layer

  def forward(self, patch):
    hidden_vector = self.conv1(patch) # apply the first convolutional layer
    hidden_vector = self.relu1(hidden_vector) # apply the first activation layer
    output_vector = self.conv2(hidden_vector) # apply the second convolutional layer
    output_score = output_vector.mean() # average the output vector to get a scalar score
    return output_score

# Initialize the generator and discriminator networks
generator = Generator()
discriminator = Discriminator()

# Define the adversarial and feature matching losses
adversarial_loss = nn.BCEWithLogitsLoss() # binary cross entropy loss with logits
feature_extractor = models.vgg16(pretrained=True).features # pretrained VGG16 model for feature extraction
feature_matching_loss = nn.L1Loss() # L1 loss for feature matching

# Define the optimizer for the generator and discriminator networks
optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999)) # Adam optimizer for the generator
optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)) # Adam optimizer for the discriminator

# Define the training parameters
epochs = 100 # number of epochs
patch_size_range = [32, 64, 128] # range of patch sizes
scale_factor_range = [0.25, 0.5, 1.0] # range of scale factors
target_scale_range = [1.0, 2.0, 4.0] # range of target scales

# Train the generator and discriminator networks
for epoch in range(epochs):
  for image, native_resolution in variable_size_dataset:
    # Patch sampling
    patch_size = random.choice(patch_size_range) # randomly choose a patch size
    scale_factor = random.choice(scale_factor_range) # randomly choose a scale factor
    scaled_image = image.resize((int(native_resolution[0] * scale_factor), int(native_resolution[1] * scale_factor))) # resize the image according to the scale factor
    patch_x = random.randint(0, scaled_image.width - patch_size) # randomly choose the x coordinate of the patch
    patch_y = random.randint(0, scaled_image.height - patch_size) # randomly choose the y coordinate of the patch
    patch = scaled_image.crop((patch_x, patch_y, patch_x + patch_size, patch_y + patch_size)) # crop the patch from the scaled image
    patch = torch.from_numpy(np.array(patch)).permute(2, 0, 1).float() / 255.0 # convert the patch to a torch tensor and normalize it
    # Patch synthesis
    latent_vector = torch.randn(1, generator.latent_dim) # sample a latent vector from a standard normal distribution
    target_scale = random.choice(target_scale_range) # randomly choose a target scale
    continuous_coordinates = torch.linspace(-1, 1, int(patch_size * target_scale)) # compute the continuous coordinates for the target scale
    continuous_coordinates_x, continuous_coordinates_y = torch.meshgrid(continuous_coordinates, continuous_coordinates) # create a mesh grid of continuous coordinates
    continuous_coordinates_x = continuous_coordinates_x.flatten().unsqueeze(0) # flatten and unsqueeze the x coordinates
    continuous_coordinates_y = continuous_coordinates_y.flatten().unsqueeze(0) # flatten and unsqueeze the y coordinates
    generated_patch = generator(latent_vector, target_scale, continuous_coordinates_x, continuous_coordinates_y) # generate a patch using the generator network
    # Adversarial and feature matching losses
    real_score = discriminator(patch) # get the score for the real patch from the discriminator network
    fake_score = discriminator(generated_patch) # get the score for the generated patch from the discriminator network
    real_label = torch.ones(1) # create a label of 1 for the real patch
    fake_label = torch.zeros(1) # create a label of 0 for the generated patch
    adversarial_loss_real = adversarial_loss(real_score, real_label) # compute the adversarial loss for the real patch
    adversarial_loss_fake = adversarial_loss(fake_score, fake_label) # compute the adversarial loss for the generated patch
    adversarial_loss_total = adversarial_loss_real + adversarial_loss_fake # compute the total adversarial loss
    real_features = feature_extractor(patch) # extract features from the real patch using the feature extractor network
    generated_features = feature_extractor(generated_patch) # extract features from the generated patch using the feature extractor network
    feature_matching_loss_total = feature_matching_loss(real_features, generated_features) # compute the feature matching loss between the real and generated features
    total_loss_generator = adversarial_loss_total + feature_matching_loss_total # compute the total loss for the generator network
    total_loss_discriminator = adversarial_loss_total # compute the total loss for the discriminator network 
    # Update generator and discriminator parameters
    optimizer_generator.zero_grad() # zero out the gradients for the generator optimizer
    total_loss_generator.backward() # backpropagate the total loss for the generator network 
    optimizer_generator.step() # update the parameters for the generator network 
    optimizer_discriminator.zero_grad() # zero out the gradients for the discriminator optimizer