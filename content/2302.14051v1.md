---
title: 2302.14051v1 Internet Explorer  Targeted Representation Learning on the Open Web
date: 2023-02-15
---

# [Internet Explorer: Targeted Representation Learning on the Open Web](http://arxiv.org/abs/2302.14051v1)

authors: Alexander C. Li, Ellis Brown, Alexei A. Efros, Deepak Pathak


## What, Why and How

[1]: https://arxiv.org/abs/2302.14051v1 "Internet Explorer: Targeted Representation Learning on the Open Web"
[2]: https://arxiv.org/pdf/2302.14051.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export3.library.cornell.edu/abs/2302.14051?context=cs.RO "[2302.14051] Internet Explorer: Targeted Representation Learning on the ..."

Here is a summary of the paper:

- **What**: The paper proposes a method called **Internet Explorer** that explores the web in a self-supervised manner to find relevant images for a target dataset and train a vision model on them[^1^][1].
- **Why**: The paper aims to overcome the limitations of existing vision models that rely on pre-training on large, static datasets that are out-of-date and do not capture the diversity of the Internet[^1^][1].
- **How**: The paper describes a cycle of four steps: (1) searching for images on the Internet with text queries, (2) self-supervised training on downloaded images, (3) determining which images were useful, and (4) prioritizing what to search for next[^1^][1]. The paper evaluates the method on several datasets and shows that it outperforms or matches CLIP oracle performance by using just a single GPU desktop to actively query the Internet for 30--40 hours[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2302.14051v1 "Internet Explorer: Targeted Representation Learning on the Open Web"
[2]: https://arxiv.org/pdf/2302.14051.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export3.library.cornell.edu/abs/2302.14051?context=cs.RO "[2302.14051] Internet Explorer: Targeted Representation Learning on the ..."

The paper claims the following contributions[^1^][1]:

- **A novel method** for targeted representation learning on the open web that leverages self-supervised learning and active learning to find and train on relevant images for a given task.
- **A simple and efficient implementation** of the method that uses a single GPU desktop and a web browser to query the Internet and download images.
- **An extensive evaluation** of the method on several datasets and tasks, such as classification, segmentation, and few-shot learning, showing that it outperforms or matches CLIP oracle performance and other baselines.

## Method Summary

[1]: https://arxiv.org/abs/2302.14051v1 "Internet Explorer: Targeted Representation Learning on the Open Web"
[2]: https://arxiv.org/pdf/2302.14051.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export3.library.cornell.edu/abs/2302.14051?context=cs.RO "[2302.14051] Internet Explorer: Targeted Representation Learning on the ..."

Here is a summary of the method section of the paper:

- The paper describes **Internet Explorer** as a cycle of four steps that iteratively explore the web and train a vision model on relevant images for a target dataset[^1^][1].
- The first step is **searching** for images on the Internet with text queries that are either randomly sampled from a vocabulary or generated by a query generator network that learns from previous queries and images[^1^][1].
- The second step is **training** on the downloaded images using a self-supervised contrastive learning objective that encourages the model to learn representations that are similar for images with the same query and different for images with different queries[^1^][1].
- The third step is **evaluating** the usefulness of the downloaded images by measuring their impact on the performance of the model on the target dataset using a validation set[^1^][1].
- The fourth step is **prioritizing** what to search for next by updating the query generator network with a reward signal that reflects the usefulness of the previous queries and images[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Initialize a vision model M and a query generator network Q
M = VisionModel()
Q = QueryGenerator()

# Initialize a vocabulary V of possible text queries
V = ["cat", "dog", "car", ...]

# Initialize a target dataset D of images and labels
D = [(x1, y1), (x2, y2), ...]

# Repeat until convergence or budget limit
while True:
  # Step 1: Search for images on the Internet with text queries
  # Sample some queries from V or generate some queries from Q
  queries = sample(V) + generate(Q)
  # Download some images for each query using a web browser
  images = download(queries)
  
  # Step 2: Train on the downloaded images using self-supervised contrastive learning
  # Encode the images and queries using M and Q
  image_features = M.encode(images)
  query_features = Q.encode(queries)
  # Compute the contrastive loss between image and query features
  loss = contrastive_loss(image_features, query_features)
  # Update M and Q using gradient descent
  M.update(loss)
  Q.update(loss)
  
  # Step 3: Evaluate the usefulness of the downloaded images
  # Measure the performance of M on the target dataset D using a validation set
  performance = M.evaluate(D)
  # Compute the usefulness score for each query and image pair
  usefulness = compute_usefulness(queries, images, performance)
  
  # Step 4: Prioritize what to search for next
  # Update Q with a reward signal based on the usefulness score
  reward = compute_reward(usefulness)
  Q.update(reward)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for deep learning
import requests # for web requests
import bs4 # for web scraping
import PIL # for image processing

# Define some hyperparameters
num_queries = 100 # number of queries per iteration
num_images = 10 # number of images per query
image_size = 224 # size of the images
embedding_size = 512 # size of the image and query features
temperature = 0.1 # temperature for contrastive loss
learning_rate = 0.01 # learning rate for gradient descent
discount_factor = 0.9 # discount factor for reward computation

# Initialize a vision model M and a query generator network Q
# Use ResNet-50 as the backbone for M and a transformer encoder for Q
M = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)
M.fc = torch.nn.Linear(M.fc.in_features, embedding_size) # replace the final layer with an embedding layer
Q = torch.nn.TransformerEncoderLayer(embedding_size, 8) # use a single transformer encoder layer

# Initialize a vocabulary V of possible text queries
V = ["cat", "dog", "car", ...]

# Initialize a target dataset D of images and labels
D = torch.utils.data.ImageFolder("path/to/dataset") # use ImageFolder to load images and labels from a folder

# Define a function to sample some queries from V or generate some queries from Q
def sample_or_generate_queries(V, Q, num_queries):
  queries = [] # list to store the queries
  for i in range(num_queries):
    if torch.rand(1) < 0.5: # with 50% probability, sample a query from V
      query = torch.choice(V)
    else: # otherwise, generate a query from Q
      query = Q.generate() # use beam search or greedy decoding to generate a query from Q
    queries.append(query) # append the query to the list
  return queries

# Define a function to download some images for each query using a web browser
def download_images(queries, num_images):
  images = [] # list to store the images
  for query in queries: # for each query
    response = requests.get("https://www.google.com/search?q=" + query + "&tbm=isch") # send a web request to Google Images with the query
    soup = bs4.BeautifulSoup(response.text, "html.parser") # parse the response text using BeautifulSoup
    image_urls = soup.find_all("img")[:num_images] # find the image urls in the parsed text and take the first num_images urls
    for image_url in image_urls: # for each image url
      image_response = requests.get(image_url) # send a web request to the image url
      image = PIL.Image.open(image_response.content) # open the image content using PIL
      image = image.resize((image_size, image_size)) # resize the image to the desired size
      images.append(image) # append the image to the list
  return images

# Define a function to encode the images and queries using M and Q
def encode_images_and_queries(images, queries, M, Q):
  image_features = [] # list to store the image features
  query_features = [] # list to store the query features
  for image in images: # for each image
    image_tensor = torch.from_numpy(image) # convert the image to a tensor
    image_tensor = image_tensor.permute(2, 0, 1) # permute the tensor to have channels first
    image_feature = M(image_tensor) # encode the image tensor using M
    image_features.append(image_feature) # append the image feature to the list
  
  for query in queries: # for each query
    query_tensor = torch.nn.functional.one_hot(query) # convert the query to a one-hot tensor
    query_feature = Q(query_tensor) # encode the query tensor using Q 
    query_features.append(query_feature) # append the query feature to the list
  
  return torch.stack(image_features), torch.stack(query_features) # stack the lists into tensors and return them

# Define a function to compute the contrastive loss between image and query features 
def contrastive_loss(image_features, query_features, temperature):
  logits = torch.matmul(image_features, query_features.t()) / temperature # compute the logits by matrix multiplication and scaling by temperature 
  labels = torch.arange(len(image_features)) # use the indices as labels (each image matches with its corresponding query)
  loss = torch.nn.functional.cross_entropy(logits, labels) # use cross entropy loss with the logits and labels
  return loss

# Define a function to measure the performance of M on the target dataset D using a validation set
def evaluate_model(M, D):
  # Split D into a training set and a validation set
  train_set, val_set = torch.utils.data.random_split(D, [int(0.8 * len(D)), int(0.2 * len(D))])
  # Use a data loader to load batches of images and labels from the validation set
  val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True)
  # Initialize some variables to store the accuracy and the number of samples
  accuracy = 0
  num_samples = 0
  # Loop over the batches of images and labels
  for images, labels in val_loader:
    # Encode the images using M
    image_features = M(images)
    # Predict the labels using a linear classifier on top of the image features
    predictions = torch.nn.Linear(embedding_size, len(D.classes))(image_features)
    # Compute the number of correct predictions
    num_correct = torch.sum(predictions.argmax(dim=1) == labels)
    # Update the accuracy and the number of samples
    accuracy += num_correct.item()
    num_samples += len(labels)
  # Compute the average accuracy
  accuracy /= num_samples
  return accuracy

# Define a function to compute the usefulness score for each query and image pair
def compute_usefulness(queries, images, performance):
  usefulness = [] # list to store the usefulness scores
  for query, image in zip(queries, images): # for each query and image pair
    # Remove the image from the downloaded images
    images.remove(image)
    # Re-train M on the remaining images using contrastive loss
    image_features, query_features = encode_images_and_queries(images, queries, M, Q)
    loss = contrastive_loss(image_features, query_features, temperature)
    M.update(loss)
    # Re-evaluate M on the target dataset D using a validation set
    new_performance = evaluate_model(M, D)
    # Compute the usefulness score as the difference between the old and new performance
    usefulness_score = performance - new_performance
    # Append the usefulness score to the list
    usefulness.append(usefulness_score)
    # Restore the image to the downloaded images
    images.append(image)
  return usefulness

# Define a function to compute a reward signal based on the usefulness score
def compute_reward(usefulness):
  reward = [] # list to store the reward values
  for usefulness_score in usefulness: # for each usefulness score
    # Compute the reward value as a discounted sum of future usefulness scores
    reward_value = sum([usefulness_score * (discount_factor ** i) for i in range(len(usefulness))])
    # Append the reward value to the list
    reward.append(reward_value)
  return reward

# Repeat until convergence or budget limit
while True:
  # Step 1: Search for images on the Internet with text queries
  queries = sample_or_generate_queries(V, Q, num_queries) # sample or generate some queries 
  images = download_images(queries, num_images) # download some images for each query
  
  # Step 2: Train on the downloaded images using self-supervised contrastive learning
  image_features, query_features = encode_images_and_queries(images, queries, M, Q) # encode the images and queries 
  loss = contrastive_loss(image_features, query_features, temperature) # compute the contrastive loss 
  M.update(loss) # update M using gradient descent 
  Q.update(loss) # update Q using gradient descent
  
  # Step 3: Evaluate the usefulness of the downloaded images 
  performance = evaluate_model(M, D) # measure the performance of M on D 
  usefulness = compute_usefulness(queries, images, performance) # compute the usefulness score for each query and image pair
  
  # Step 4: Prioritize what to search for next 
  reward = compute_reward(usefulness) # compute a reward signal based on the usefulness score 
  Q.update(reward) # update Q with a reward signal 
```