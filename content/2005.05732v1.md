---
title: 2005.05732v1 Skeleton-Aware Networks for Deep Motion Retargeting
date: 2020-05-06
---

# [Skeleton-Aware Networks for Deep Motion Retargeting](http://arxiv.org/abs/2005.05732v1)

authors: Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or, Baoquan Chen


## What, Why and How

[1]: https://arxiv.org/abs/2005.05732 "[2005.05732] Skeleton-Aware Networks for Deep Motion Retargeting"
[2]: https://arxiv.org/pdf/2005.05732.pdf "Skeleton-Aware Networks for Deep Motion Retargeting - arXiv.org"
[3]: http://export.arxiv.org/abs/2005.05732 "[2005.05732] Skeleton-Aware Networks for Deep Motion Retargeting"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a novel deep learning framework for data-driven motion retargeting between skeletons, which may have different structure, yet corresponding to homeomorphic graphs.
- **Why**: The paper aims to address the challenges of motion retargeting, such as the lack of paired examples, the different sampling and structure of kinematic chains, and the preservation of motion style and quality.
- **How**: The paper leverages the fact that different homeomorphic skeletons may be reduced to a common primal skeleton by a sequence of edge merging operations, which are referred to as skeletal pooling. The paper introduces novel differentiable convolution, pooling, and unpooling operators that are skeleton-aware, meaning that they explicitly account for the skeleton's hierarchical structure and joint adjacency. These operators transform the original motion into a collection of deep temporal features associated with the joints of the primal skeleton, forming a common latent space shared by a collection of homeomorphic skeletons. Retargeting can be achieved simply by encoding to, and decoding from this latent space. The paper evaluates the effectiveness of the framework for motion retargeting, as well as motion processing in general, compared to existing approaches.

## Main Contributions

According to the paper, the main contributions are:

- A novel deep learning framework for data-driven motion retargeting between skeletons with different structure and sampling, without requiring any explicit pairing between the motions in the training set.
- Novel differentiable convolution, pooling, and unpooling operators that are skeleton-aware, meaning that they explicitly account for the skeleton's hierarchical structure and joint adjacency, and serve to transform the original motion into a collection of deep temporal features associated with the joints of the primal skeleton.
- A comprehensive evaluation of the proposed framework on various motion retargeting and processing tasks, demonstrating its effectiveness and advantages over existing approaches.

## Method Summary

The method section of the paper describes the proposed framework for motion retargeting in detail. It consists of four subsections:

- **Skeletal pooling and unpooling**: This subsection introduces the concept of skeletal pooling, which is a sequence of edge merging operations that reduce a skeleton to a primal skeleton. It also defines the inverse operation, skeletal unpooling, which restores the original skeleton from the primal skeleton. The subsection also presents the differentiable pooling and unpooling operators that are used to transform the motion features between different skeletons.
- **Skeleton-aware convolution**: This subsection introduces the skeleton-aware convolution operator, which is a generalization of the standard convolution operator that accounts for the skeleton's hierarchical structure and joint adjacency. It also describes how to implement this operator efficiently using sparse tensors and graph convolution.
- **Motion retargeting network**: This subsection describes the architecture of the motion retargeting network, which consists of an encoder-decoder structure with skip connections. The encoder transforms the input motion features into a latent representation associated with the primal skeleton, while the decoder reconstructs the output motion features from the latent representation. The network also employs residual blocks and temporal convolutions to capture motion dynamics and style.
- **Training and inference**: This subsection describes how to train and infer the motion retargeting network using unpaired motion data. It also introduces a novel loss function that combines reconstruction loss, cycle-consistency loss, and style loss to ensure motion quality and consistency. The subsection also discusses some implementation details and hyperparameters.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the primal skeleton and the pooling and unpooling operations
primal_skeleton = define_primal_skeleton()
pooling_ops = define_pooling_ops()
unpooling_ops = define_unpooling_ops()

# Define the skeleton-aware convolution operator
def skeleton_aware_convolution(features, skeleton):
  # Convert features and skeleton to sparse tensors
  features = to_sparse_tensor(features)
  skeleton = to_sparse_tensor(skeleton)
  # Perform graph convolution on features and skeleton
  output = graph_convolution(features, skeleton)
  # Convert output to dense tensor
  output = to_dense_tensor(output)
  return output

# Define the motion retargeting network
def motion_retargeting_network(input_motion, input_skeleton, output_skeleton):
  # Encode the input motion features to the primal skeleton
  latent_features = input_motion
  for pooling_op in pooling_ops:
    latent_features = pooling_op(latent_features)
    latent_features = skeleton_aware_convolution(latent_features, primal_skeleton)
  
  # Decode the latent features to the output motion features
  output_motion = latent_features
  for unpooling_op in unpooling_ops:
    output_motion = unpooling_op(output_motion)
    output_motion = skeleton_aware_convolution(output_motion, output_skeleton)
  
  return output_motion

# Define the loss function
def loss_function(input_motion, input_skeleton, output_skeleton):
  # Compute the reconstruction loss
  reconstructed_motion = motion_retargeting_network(input_motion, input_skeleton, input_skeleton)
  reconstruction_loss = mean_squared_error(input_motion, reconstructed_motion)

  # Compute the cycle-consistency loss
  retargeted_motion = motion_retargeting_network(input_motion, input_skeleton, output_skeleton)
  cycle_motion = motion_retargeting_network(retargeted_motion, output_skeleton, input_skeleton)
  cycle_loss = mean_squared_error(input_motion, cycle_motion)

  # Compute the style loss
  style_loss = style_distance(input_motion, retargeted_motion)

  # Combine the losses with weights
  total_loss = reconstruction_loss + lambda1 * cycle_loss + lambda2 * style_loss

  return total_loss

# Train the network using unpaired motion data
for epoch in epochs:
  for batch in batches:
    # Sample a pair of motions from different skeletons
    input_motion, input_skeleton = sample_motion()
    output_motion, output_skeleton = sample_motion()
    # Compute the loss and update the network parameters
    loss = loss_function(input_motion, input_skeleton, output_skeleton)
    optimizer.step(loss)

# Infer the network on new motions
for new_input in new_inputs:
  # Get the input motion and skeleton
  input_motion, input_skeleton = new_input
  # Get the desired output skeleton
  output_skeleton = get_output_skeleton()
  # Retarget the input motion to the output skeleton
  output_motion = motion_retargeting_network(input_motion, input_skeleton, output_skeleton)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_geometric as tg
import torch_sparse as ts

# Define the primal skeleton and the pooling and unpooling operations
primal_skeleton = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]) # A simple chain of 7 joints
pooling_ops = [ts.coalesce(np.array([[0, 1], [1, 2], [2, 3]]), np.array([0.5, 0.5]), 7, 4), # Merge joints 0 and 1, and joints 2 and 3
               ts.coalesce(np.array([[0, 1]]), np.array([0.5]), 4, 3)] # Merge joints 0 and 1
unpooling_ops = [ts.coalesce(np.array([[0, 1]]), np.array([1]), 3, 4), # Split joint 0 into joints 0 and 1
                 ts.coalesce(np.array([[0, 1], [2, 3]]), np.array([1, 1]), 4, 7)] # Split joint 0 into joints 0 and 1, and joint 2 into joints 2 and 3

# Define the skeleton-aware convolution operator
class SkeletonAwareConvolution(nn.Module):
  
  def __init__(self, in_channels, out_channels):
    super(SkeletonAwareConvolution, self).__init__()
    # Initialize the weight matrix
    self.weight = nn.Parameter(torch.Tensor(in_channels * out_channels))
    # Initialize the bias vector
    self.bias = nn.Parameter(torch.Tensor(out_channels))
    # Reset the parameters
    self.reset_parameters()

  def reset_parameters(self):
    # Initialize the weight matrix with Xavier initialization
    nn.init.xavier_uniform_(self.weight)
    # Initialize the bias vector with zeros
    nn.init.zeros_(self.bias)

  def forward(self, features, skeleton):
    # Convert features and skeleton to sparse tensors
    features = ts.SparseTensor.from_dense(features)
    skeleton = ts.SparseTensor.from_dense(skeleton)
    # Perform graph convolution on features and skeleton using message passing
    output = tg.nn.MessagePassing(aggr="add")(features, skeleton)
    # Reshape the output to match the weight matrix dimensions
    output = output.view(-1, self.weight.size(0))
    # Multiply the output by the weight matrix
    output = torch.matmul(output, self.weight)
    # Reshape the output to match the original features dimensions
    output = output.view(features.size(0), -1)
    # Add the bias vector to the output
    output = output + self.bias
    return output

# Define the motion retargeting network
class MotionRetargetingNetwork(nn.Module):

  def __init__(self):
    super(MotionRetargetingNetwork, self).__init__()
    # Define the encoder layers
    self.encoder_conv1 = SkeletonAwareConvolution(3, 64) # Convolve each joint position (3D) to a feature vector (64D)
    self.encoder_conv2 = SkeletonAwareConvolution(64 * pooling_ops[0].size(1), 128) # Convolve each pooled joint feature (64D * number of children) to a feature vector (128D)
    self.encoder_conv3 = SkeletonAwareConvolution(128 * pooling_ops[1].size(1), 256) # Convolve each pooled joint feature (128D * number of children) to a feature vector (256D)
    
    # Define the decoder layers
    self.decoder_conv1 = SkeletonAwareConvolution(256 * unpooling_ops[0].size(1), 128) # Convolve each unpooled joint feature (256D * number of children) to a feature vector (128D)
    self.decoder_conv2 = SkeletonAwareConvolution(128 * unpooling_ops[1].size(1), 64) # Convolve each unpooled joint feature (128D * number of children) to a feature vector (64D)
    self.decoder_conv3 = SkeletonAwareConvolution(64 * unpooling_ops[2].size(1), 3) # Convolve each unpooled joint feature (64D * number of children) to a position vector (3D)

    # Define the residual blocks for motion dynamics and style
    self.res_block1 = nn.Sequential(nn.Conv1d(64, 64, kernel_size=3, padding=1), nn.ReLU(), nn.Conv1d(64, 64, kernel_size=3, padding=1))
    self.res_block2 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.Conv1d(128, 128, kernel_size=3, padding=1))
    self.res_block3 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=3, padding=1), nn.ReLU(), nn.Conv1d(256, 256, kernel_size=3, padding=1))
    self.res_block4 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=3, padding=1), nn.ReLU(), nn.Conv1d(128, 128, kernel_size=3, padding=1))
    self.res_block5 = nn.Sequential(nn.Conv1d(64, 64, kernel_size=3, padding=1), nn.ReLU(), nn.Conv1d(64, 64, kernel_size=3, padding=1))

  def forward(self, input_motion, input_skeleton, output_skeleton):
    # Encode the input motion features to the primal skeleton
    latent_features = input_motion
    for i in range(len(pooling_ops)):
      latent_features = pooling_ops[i](latent_features) # Apply the pooling operator
      latent_features = self.encoder_conv[i](latent_features) # Apply the convolution operator
      latent_features = F.relu(latent_features) # Apply the ReLU activation function
      latent_features = self.res_block[i](latent_features) # Apply the residual block
    
    # Decode the latent features to the output motion features
    output_motion = latent_features
    for i in range(len(unpooling_ops)):
      output_motion = unpooling_ops[i](output_motion) # Apply the unpooling operator
      output_motion = self.decoder_conv[i](output_motion) # Apply the convolution operator
      output_motion = F.relu(output_motion) # Apply the ReLU activation function
      output_motion = self.res_block[i + len(pooling_ops)](output_motion) # Apply the residual block
    
    return output_motion

# Define the loss function
def loss_function(input_motion, input_skeleton, output_skeleton):
  # Compute the reconstruction loss
  reconstructed_motion = motion_retargeting_network(input_motion, input_skeleton, input_skeleton)
  reconstruction_loss = F.mse_loss(input_motion, reconstructed_motion)

  # Compute the cycle-consistency loss
  retargeted_motion = motion_retargeting_network(input_motion, input_skeleton, output_skeleton)
  cycle_motion = motion_retargeting_network(retargeted_motion, output_skeleton, input_skeleton)
  cycle_loss = F.mse_loss(input_motion, cycle_motion)

  # Compute the style loss
  style_loss = style_distance(input_motion, retargeted_motion)

  # Combine the losses with weights
  total_loss = reconstruction_loss + lambda1 * cycle_loss + lambda2 * style_loss

  return total_loss

# Define the style distance function
def style_distance(motion1, motion2):
  # Compute the mean and standard deviation of each motion along the temporal dimension
  mean1 = torch.mean(motion1, dim=0)
  std1 = torch.std(motion1, dim=0)
  mean2 = torch.mean(motion2, dim=0)
  std2 = torch.std(motion2, dim=0)
  
  # Compute the distance between the means and standard deviations using L2 norm
  mean_distance = torch.norm(mean1 - mean2)
  std_distance = torch.norm(std1 - std2)

  # Return the sum of the distances
  return mean_distance + std_distance

# Train the network using unpaired motion data
motion_retargeting_network = MotionRetargetingNetwork()
optimizer = torch.optim.Adam(motion_retargeting_network.parameters(), lr=0.001)
for epoch in epochs:
  for batch in batches:
    # Sample a pair of motions from different skeletons
    input_motion, input_skeleton = sample_motion()
    output_motion, output_skeleton = sample_motion()
    # Compute the loss and update the network parameters
    loss = loss_function(input_motion, input_skeleton, output_skeleton)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Infer the network on new motions
for new_input in new_inputs:
  # Get the input motion and skeleton
  input_motion, input_skeleton = new_input
  # Get the desired output skeleton
  output_skeleton = get_output_skeleton()
  # Retarget the input motion to the output skeleton
  output_motion = motion_retargeting_network(input_motion, input_skeleton, output_skeleton)
```