---
title: 2206.02104v1 ContraCLIP  Interpretable GAN generation driven by pairs of contrasting sentences
date: 2022-06-03
---

# [ContraCLIP: Interpretable GAN generation driven by pairs of contrasting sentences](http://arxiv.org/abs/2206.02104v1)

authors: Christos Tzelepis, James Oldfield, Georgios Tzimiropoulos, Ioannis Patras


## What, Why and How

[1]: https://arxiv.org/abs/2206.02104 "[2206.02104] ContraCLIP: Interpretable GAN generation driven by pairs ..."
[2]: https://arxiv.org/pdf/2209.02104v1.pdf "arXiv:2209.02104v1 [math.DS] 5 Sep 2022"
[3]: http://export.arxiv.org/abs/2206.02104 "[2206.02104] ContraCLIP: Interpretable GAN generation driven by pairs ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a method for discovering non-linear interpretable paths in the latent space of pre-trained GANs that can generate images with controlled semantic changes based on pairs of contrasting sentences.
- **Why**: The paper aims to address some of the limitations of existing methods for GAN interpretation and manipulation, such as being tailored to specific GAN architectures, disregarding the relative position of the images and the texts in the embedding space, and leading to abrupt image manipulations and low image quality.
- **How**: The paper uses a set of pairs of natural language sentences with contrasting semantics, named semantic dipoles, that serve as the limits of the interpretation that the latent paths should encode. By using the pre-trained CLIP encoder, the sentences are projected into the vision-language space, where they serve as dipoles, and where RBF-based warping functions define a set of non-linear directional paths, one for each semantic dipole. By defining an objective that discovers paths in the latent space of GANs that generate changes along the desired paths in the vision-language embedding space, the paper provides an intuitive way of controlling the underlying generative factors. The paper evaluates the proposed method on two pre-trained GANs and shows qualitative and quantitative results that demonstrate its advantages over existing methods.

## Main Contributions

[1]: https://arxiv.org/abs/2206.02104 "[2206.02104] ContraCLIP: Interpretable GAN generation driven by pairs ..."
[2]: https://arxiv.org/pdf/2209.02104v1.pdf "arXiv:2209.02104v1 [math.DS] 5 Sep 2022"
[3]: http://export.arxiv.org/abs/2206.02104 "[2206.02104] ContraCLIP: Interpretable GAN generation driven by pairs ..."

According to the paper at [^1^][1], the main contributions are:

- A novel method for discovering **non-linear interpretable paths** in the latent space of pre-trained GANs that can generate images with controlled semantic changes based on pairs of contrasting sentences.
- A model-agnostic approach that can work with **any pre-trained GAN** without requiring any retraining or fine-tuning of the generator or the encoder.
- A use of the pre-trained CLIP encoder to project the sentences and the images into a **common vision-language space**, where they serve as dipoles and where RBF-based warping functions define a set of non-linear directional paths.
- An objective function that discovers paths in the latent space of GANs that generate changes along the desired paths in the vision-language embedding space, while **preserving the relative position** of the manipulated and the original image in the image embedding and the relative position of the image and the text embeddings.
- A demonstration of the advantages of the proposed method over existing methods in terms of **interpretability, controllability, and image quality**, using two pre-trained GANs and various semantic dipoles.

## Method Summary

[1]: https://arxiv.org/abs/2206.02104 "[2206.02104] ContraCLIP: Interpretable GAN generation driven by pairs ..."
[2]: https://arxiv.org/pdf/2209.02104v1.pdf "arXiv:2209.02104v1 [math.DS] 5 Sep 2022"
[3]: http://export.arxiv.org/abs/2206.02104 "[2206.02104] ContraCLIP: Interpretable GAN generation driven by pairs ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces the concept of **semantic dipoles**, which are pairs of natural language sentences with contrasting semantics, such as "a young woman with long hair" and "an old man with short hair".
- The paper uses the pre-trained CLIP encoder [^2^][2] to project the sentences and the images into a **common vision-language space**, where they serve as dipoles, and where RBF-based warping functions define a set of non-linear directional paths, one for each semantic dipole.
- The paper defines an objective function that discovers paths in the latent space of GANs that generate changes along the desired paths in the vision-language embedding space, while **preserving the relative position** of the manipulated and the original image in the image embedding and the relative position of the image and the text embeddings.
- The paper optimizes the objective function using gradient descent and obtains a set of **interpretable latent paths** that can generate images with controlled semantic changes based on semantic dipoles.
- The paper evaluates the proposed method on two pre-trained GANs: StyleGAN2 [^3^][3] trained on FFHQ  and BigGAN  trained on ImageNet . The paper shows qualitative and quantitative results that demonstrate the advantages of the proposed method over existing methods in terms of interpretability, controllability, and image quality.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a pre-trained GAN G, a pre-trained CLIP encoder E, a set of semantic dipoles S
# Output: a set of interpretable latent paths P

# Initialize the latent paths P as random vectors
P = random_vectors(len(S))

# For each semantic dipole s in S
for s in S:
  # Get the text embeddings of the two sentences using E
  t1, t2 = E(s[0]), E(s[1])
  # Get the latent path p corresponding to s
  p = P[s]
  # Define a RBF-based warping function f that maps [0, 1] to [0, 1] with f(0) = 0 and f(1) = 1
  f = RBF_warping_function()
  # Define a loss function L that measures the distance between the image and text embeddings in the vision-language space
  L = vision_language_distance(E, G)
  # Optimize p using gradient descent to minimize L
  p = gradient_descent(p, L)
  # Update P with the optimized p
  P[s] = p

# Return P as the set of interpretable latent paths
return P
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch # for tensor operations
import clip # for CLIP encoder
import torch_optimizer as optim # for RAdam optimizer
from pytorch_pretrained_biggan import BigGAN # for BigGAN model
from stylegan2_pytorch import StyleGAN2 # for StyleGAN2 model

# Define some hyperparameters
num_paths = 10 # number of latent paths to discover
path_length = 10 # length of each latent path
num_steps = 1000 # number of optimization steps
learning_rate = 0.01 # learning rate for gradient descent
warping_sigma = 0.1 # sigma parameter for RBF warping function
warping_lambda = 0.001 # lambda parameter for RBF warping function

# Load the pre-trained GAN model (choose one of them)
G = BigGAN.from_pretrained('biggan-deep-256') # for BigGAN
G = StyleGAN2('stylegan2-ffhq-config-f.pt') # for StyleGAN2

# Load the pre-trained CLIP encoder
E = clip.load('ViT-B/32', jit=False)[0]

# Define a set of semantic dipoles as pairs of sentences
S = [
  ("a young woman with long hair", "an old man with short hair"),
  ("a smiling face", "a frowning face"),
  ("a blue sky with clouds", "a dark night with stars"),
  ("a dog with floppy ears", "a cat with pointy ears"),
  ("a red apple", "a green pear"),
  ("a forest with trees", "a desert with cacti"),
  ("a car with wheels", "a boat with sails"),
  ("a bird with feathers", "a fish with scales"),
  ("a cake with candles", "a pizza with cheese"),
  ("a guitar with strings", "a drum with sticks")
]

# Initialize the latent paths as random vectors
P = torch.randn(num_paths, path_length, G.z_dim) # for BigGAN
P = torch.randn(num_paths, path_length, G.style_dim) # for StyleGAN2

# Define a RBF-based warping function that maps [0, 1] to [0, 1] with f(0) = 0 and f(1) = 1
def RBF_warping_function(x):
  # Define the RBF kernel function
  def RBF_kernel(x, y):
    return torch.exp(-warping_sigma * (x - y) ** 2)
  
  # Define the basis functions as RBF kernels centered at equally spaced points in [0, 1]
  num_basis = 11 # number of basis functions
  basis_centers = torch.linspace(0, 1, num_basis) # basis centers
  basis_functions = [lambda x: RBF_kernel(x, c) for c in basis_centers] # basis functions
  
  # Define the warping coefficients as learnable parameters
  warping_coefficients = torch.nn.Parameter(torch.randn(num_basis))
  
  # Define the warping function as a linear combination of basis functions
  def warping_function(x):
    return torch.sum(warping_coefficients * torch.stack([f(x) for f in basis_functions]))
  
  # Normalize the warping function to satisfy the boundary conditions f(0) = 0 and f(1) = 1
  def normalized_warping_function(x):
    return (warping_function(x) - warping_function(0)) / (warping_function(1) - warping_function(0))
  
  # Return the normalized warping function
  return normalized_warping_function(x)

# Define a loss function that measures the distance between the image and text embeddings in the vision-language space
def vision_language_distance(E, G):
  # Define a cosine similarity function
  def cosine_similarity(x, y):
    return torch.sum(x * y) / (torch.norm(x) * torch.norm(y))
  
  # Define a loss function that computes the negative cosine similarity between two embeddings
  def loss_function(e1, e2):
    return -cosine_similarity(e1, e2)
  
  # For each semantic dipole s in S
  for s in S:
    # Get the text embeddings of the two sentences using E
    t1, t2 = E(s[0]), E(s[1])
    # Get the latent path p corresponding to s
    p = P[s]
    # For each point z in p
    for z in p:
      # Generate an image x using G
      x = G(z) # for BigGAN
      x = G.synthesis(z) # for StyleGAN2
      # Get the image embedding of x using E
      i = E(x)
      # Compute the warping function value f at the normalized index of z in p
      f = RBF_warping_function(torch.tensor(p.index(z) / (path_length - 1)))
      # Compute the target text embedding t as a linear interpolation of t1 and t2 using f
      t = (1 - f) * t1 + f * t2
      # Compute the loss as the sum of the loss function and a regularization term for the warping coefficients
      loss = loss_function(i, t) + warping_lambda * torch.norm(warping_coefficients)
  
  # Return the total loss
  return loss

# Define an optimizer for gradient descent using RAdam
optimizer = optim.RAdam([P, warping_coefficients], lr=learning_rate)

# For each optimization step
for step in range(num_steps):
  # Zero the gradients
  optimizer.zero_grad()
  # Compute the loss
  loss = vision_language_distance(E, G)
  # Backpropagate the gradients
  loss.backward()
  # Update the parameters
  optimizer.step()
  # Print the loss
  print(f"Step {step}, Loss {loss}")

# Return P as the set of interpretable latent paths
return P
```