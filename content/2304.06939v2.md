---
title: 2304.06939v2 Multimodal C4  An Open, Billion-scale Corpus of Images Interleaved with Text
date: 2023-04-07
---

# [Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text](http://arxiv.org/abs/2304.06939v2)

authors: Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi


## What, Why and How

[1]: https://arxiv.org/pdf/2304.06939v2.pdf "Multimodal C4: An Open, Billion-scale Corpus of Images ... - arXiv.org"
[2]: https://arxiv.org/abs/2304.06939 "Title: Multimodal C4: An Open, Billion-scale Corpus of Images ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2304.06939v2 "[2304.06939v2] Multimodal C4: An Open, Billion-scale Corpus of Images ..."

Here is a summary of the paper:

- **What**: The paper introduces Multimodal C4 (mmc4), a public, billion-scale image-text dataset consisting of interleaved image/text sequences. It is an augmentation of the popular text-only c4 corpus with images interleaved using a linear assignment algorithm based on CLIP features[^1^][1].
- **Why**: The paper aims to support in-context vision and language models like Flamingo that can handle arbitrarily interleaved sequences of images and text as input. This format enables few-shot learning and more complex prompts involving interaction between images and text. However, such a large-scale corpus of interleaved image/text sequences has not been publicly available before.
- **How**: The paper describes the process of constructing mmc4 from public webpages contained in the cleaned English c4 corpus. It also evaluates the quality of mmc4 in terms of topical relevance, alignment, diversity, and coverage. It shows that mmc4 outperforms alternative methods of placing images into text and covers a wide range of everyday topics like cooking, travel, technology, etc.



## Main Contributions

[1]: https://arxiv.org/pdf/2304.06939v2.pdf "Multimodal C4: An Open, Billion-scale Corpus of Images ... - arXiv.org"
[2]: https://arxiv.org/abs/2304.06939 "Title: Multimodal C4: An Open, Billion-scale Corpus of Images ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2304.06939v2 "[2304.06939v2] Multimodal C4: An Open, Billion-scale Corpus of Images ..."

The paper claims the following contributions:

- It introduces **Multimodal C4 (mmc4)**, a public, billion-scale image-text dataset consisting of interleaved image/text sequences. It is an augmentation of the popular text-only c4 corpus with images interleaved using a linear assignment algorithm based on CLIP features[^1^][1].
- It describes the process of constructing mmc4 from public webpages contained in the cleaned English c4 corpus. It also provides a datasheet[^2^][3] for mmc4 that documents its provenance, characteristics, and limitations.
- It evaluates the quality of mmc4 in terms of topical relevance, alignment, diversity, and coverage. It shows that mmc4 outperforms alternative methods of placing images into text and covers a wide range of everyday topics like cooking, travel, technology, etc.
- It demonstrates the usefulness of mmc4 for pretraining multimodal models like Flamingo that can handle arbitrarily interleaved sequences of images and text as input. It shows that Flamingo pretrained on mmc4 achieves state-of-the-art results on several multimodal benchmarks and tasks.


## Method Summary

[1]: https://arxiv.org/pdf/2304.06939v2.pdf "Multimodal C4: An Open, Billion-scale Corpus of Images ... - arXiv.org"
[2]: https://arxiv.org/abs/2304.06939 "Title: Multimodal C4: An Open, Billion-scale Corpus of Images ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2304.06939v2 "[2304.06939v2] Multimodal C4: An Open, Billion-scale Corpus of Images ..."

Here is a summary of the method section of the paper:

- The paper describes the process of constructing mmc4 from public webpages contained in the cleaned English c4 corpus. It consists of four main steps: crawling, deduplication, image placement, and filtering.
- Crawling: The paper uses the Common Crawl dataset as the source of webpages. It extracts the text and images from each webpage using Beautiful Soup and Pillow libraries. It also extracts the metadata of each image such as size, format, and URL.
- Deduplication: The paper removes duplicate documents and images using SHA-256 hashes. It also removes documents that are too short (less than 100 tokens) or too long (more than 10,000 tokens).
- Image placement: The paper places images into longer bodies of text using a linear assignment algorithm based on CLIP features[^1^][1]. CLIP is a vision-language model that can encode images and text into a common semantic space. The paper computes the CLIP features for each image and each sentence in a document, and then assigns each image to the sentence that has the highest cosine similarity with it. This process aims to maximize the alignment between images and text.
- Filtering: The paper filters out NSFW images, ads, logos, icons, etc. using a combination of heuristics and classifiers. It also filters out documents that have too few (less than 2) or too many (more than 50) images. It also removes documents that contain non-English text or offensive language.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the source of webpages
source = CommonCrawl()

# Define the output corpus
mmc4 = []

# For each webpage in the source
for webpage in source:

  # Extract the text and images from the webpage
  text, images = extract(webpage)

  # Remove duplicate documents and images
  text, images = deduplicate(text, images)

  # Skip if the document is too short or too long
  if len(text) < 100 or len(text) > 10000:
    continue

  # Compute the CLIP features for each image and each sentence
  image_features = CLIP.encode(images)
  sentence_features = CLIP.encode(text)

  # Assign each image to the sentence with the highest cosine similarity
  assignments = linear_assignment(image_features, sentence_features)

  # Interleave the images and text according to the assignments
  interleaved = interleave(text, images, assignments)

  # Filter out NSFW images, ads, logos, icons, etc.
  interleaved = filter(interleaved)

  # Skip if the document has too few or too many images
  if len(interleaved.images) < 2 or len(interleaved.images) > 50:
    continue

  # Skip if the document contains non-English text or offensive language
  if not is_english(interleaved.text) or is_offensive(interleaved.text):
    continue

  # Add the document to the output corpus
  mmc4.append(interleaved)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the required libraries
import requests
import hashlib
import bs4
import PIL
import torch
import clip
import scipy
import nsfw_detector
import langdetect
import profanity_check

# Define the source of webpages
source = CommonCrawl()

# Define the output corpus
mmc4 = []

# Define the minimum and maximum length of documents
min_length = 100
max_length = 10000

# Define the minimum and maximum number of images per document
min_images = 2
max_images = 50

# Load the CLIP model and tokenizer
model, tokenizer = clip.load("ViT-B/32")

# For each webpage in the source
for webpage in source:

  # Get the HTML content of the webpage
  html = requests.get(webpage.url).text

  # Parse the HTML using Beautiful Soup
  soup = bs4.BeautifulSoup(html, "html.parser")

  # Extract the text from the webpage
  text = soup.get_text()

  # Remove duplicate documents using SHA-256 hashes
  text_hash = hashlib.sha256(text.encode()).hexdigest()
  if text_hash in seen_hashes:
    continue
  else:
    seen_hashes.add(text_hash)

  # Skip if the document is too short or too long
  if len(text) < min_length or len(text) > max_length:
    continue

  # Extract the images from the webpage
  images = []
  for img in soup.find_all("img"):
    # Get the image URL and format
    img_url = img["src"]
    img_format = img_url.split(".")[-1]

    # Remove duplicate images using SHA-256 hashes
    img_hash = hashlib.sha256(img_url.encode()).hexdigest()
    if img_hash in seen_hashes:
      continue
    else:
      seen_hashes.add(img_hash)

    # Download the image and convert it to PIL format
    img_data = requests.get(img_url).content
    img_pil = PIL.Image.open(img_data)

    # Get the image size and metadata
    img_size = img_pil.size
    img_meta = {"url": img_url, "format": img_format, "size": img_size}

    # Append the image and its metadata to the list of images
    images.append((img_pil, img_meta))

  # Compute the CLIP features for each image and each sentence in the document
  image_features = []
  for img_pil, img_meta in images:
    # Preprocess the image using CLIP's transform function
    img_tensor = clip.transforms(img_pil).unsqueeze(0)

    # Encode the image using CLIP's model and get the feature vector
    with torch.no_grad():
      img_feature = model.encode_image(img_tensor)

    # Append the image feature and its metadata to the list of image features
    image_features.append((img_feature, img_meta))

  sentence_features = []
  for sentence in text.split("."):
    # Tokenize the sentence using CLIP's tokenizer and get the input ids and attention mask
    tokens = tokenizer(sentence, return_tensors="pt")
    input_ids = tokens["input_ids"]
    attention_mask = tokens["attention_mask"]

    # Encode the sentence using CLIP's model and get the feature vector
    with torch.no_grad():
      sentence_feature = model.encode_text(input_ids, attention_mask)

    # Append the sentence feature and its text to the list of sentence features
    sentence_features.append((sentence_feature, sentence))

  # Assign each image to the sentence with the highest cosine similarity using a linear assignment algorithm based on CLIP features

  # Create a matrix of cosine similarities between each image feature and each sentence feature
  similarity_matrix = torch.zeros(len(image_features), len(sentence_features))
  for i, (img_feature, _) in enumerate(image_features):
    for j, (sentence_feature, _) in enumerate(sentence_features):
      similarity_matrix[i][j] = torch.cosine_similarity(img_feature, sentence_feature)

  # Use scipy's linear_sum_assignment function to find the optimal assignment that maximizes the total similarity score
  row_ind, col_ind = scipy.optimize.linear_sum_assignment(-similarity_matrix)

  # Interleave the images and text according to the assignments

  # Create a list of interleaved items (image or sentence) sorted by their position in the document
  interleaved_items = []
  
  # For each pair of assigned image and sentence indices 
  for i, j in zip(row_ind, col_ind):
    
    # Get the image feature and its metadata from the image features list 
    img_feature, img_meta = image_features[i]

    # Get the sentence feature and its text from the sentence features list
    sentence_feature, sentence = sentence_features[j]

    # Get the position of the image and the sentence in the document
    img_position = webpage.images.index(img_meta["url"])
    sentence_position = text.index(sentence)

    # Create an interleaved item object with the type, content, and position of the image or the sentence
    img_item = {"type": "image", "content": img_meta, "position": img_position}
    sentence_item = {"type": "sentence", "content": sentence, "position": sentence_position}

    # Append the interleaved item object to the list of interleaved items
    interleaved_items.append(img_item)
    interleaved_items.append(sentence_item)

  # Sort the list of interleaved items by their position in the document
  interleaved_items.sort(key=lambda x: x["position"])

  # Filter out NSFW images, ads, logos, icons, etc. using a combination