---
title: 2111.12159v1 Rhythm is a Dancer  Music-Driven Motion Synthesis with Global Structure
date: 2021-11-13
---

# [Rhythm is a Dancer: Music-Driven Motion Synthesis with Global Structure](http://arxiv.org/abs/2111.12159v1)

authors: Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman, Daniel Cohen-Or, Ariel Shamir, Yiorgos Chrysanthou


## What, Why and How

[1]: https://arxiv.org/abs/2111.12159 "[2111.12159] Rhythm is a Dancer: Music-Driven Motion Synthesis with ..."
[2]: https://arxiv.org/pdf/2111.12159v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2304.12159v1 "[2304.12159v1] First Experimental Study of the Purely Leptonic Decay $D ..."

Here is a summary of the paper at http://arxiv.org/pdf/2111.12159v1:

- **What**: The paper presents a music-driven motion synthesis framework that generates long-term sequences of human motions that are synchronized with the input beats and follow the global structure of a specific dance genre.
- **Why**: The paper aims to address the challenge of synthesizing human motion with a global structure, such as a choreography, which is neglected by existing methods that focus on local smooth pose transitions.
- **How**: The paper proposes a hierarchical system that consists of three levels: pose, motif, and choreography. The pose level generates temporally coherent sequences of poses using an LSTM component. The motif level guides sets of consecutive poses to form a movement that belongs to a specific distribution using a novel motion perceptual-loss. And the choreography level selects the order of the performed movements and drives the system to follow the global structure of a dance genre. The paper also enables generation of diverse motions that are controlled by the content of the music, and not only by the beat[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2111.12159 "[2111.12159] Rhythm is a Dancer: Music-Driven Motion Synthesis with ..."
[2]: https://arxiv.org/pdf/2111.12159v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2304.12159v1 "[2304.12159v1] First Experimental Study of the Purely Leptonic Decay $D ..."

According to the paper at http://arxiv.org/pdf/2111.12159v1, the main contributions are:

- **A music-driven motion synthesis framework** that generates long-term sequences of human motions that are synchronized with the input beats and follow the global structure of a specific dance genre[^1^][1].
- **A novel motion perceptual-loss** that guides sets of consecutive poses to form a movement that belongs to a specific distribution[^1^][1].
- **A method to control the content of the synthesized motions** based on the music features, such as tempo, pitch, and timbre[^1^][1].
- **A comprehensive evaluation** of the proposed framework on various dance types, such as ballet, hip-hop, and salsa, and a comparison with state-of-the-art methods[^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/2111.12159 "[2111.12159] Rhythm is a Dancer: Music-Driven Motion Synthesis with ..."
[2]: https://arxiv.org/pdf/2111.12159v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2304.12159v1 "[2304.12159v1] First Experimental Study of the Purely Leptonic Decay $D ..."

Here is a summary of the method section of the paper at http://arxiv.org/pdf/2111.12159v1:

- The method section describes the proposed music-driven motion synthesis framework, which is a hierarchical system that consists of three levels: pose, motif, and choreography[^1^][1].
- The pose level consists of an LSTM component that generates temporally coherent sequences of poses based on the input beats and a latent vector that encodes the motion content[^1^][1].
- The motif level guides sets of consecutive poses to form a movement that belongs to a specific distribution using a novel motion perceptual-loss, which measures the similarity between two motions based on their perceptual features, such as speed, acceleration, and curvature[^1^][1].
- The choreography level selects the order of the performed movements and drives the system to follow the global structure of a dance genre using a Markov chain model that learns the transition probabilities between different motifs from a large-scale motion dataset[^1^][1].
- The method section also explains how the system can generate diverse motions that are controlled by the content of the music, such as tempo, pitch, and timbre, by using a conditional variational autoencoder (CVAE) that learns to map music features to latent vectors that encode the motion content[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at http://arxiv.org/pdf/2111.12159v1:

```python
# Input: a music clip M
# Output: a sequence of human poses P that are synchronized with M and follow a specific dance genre

# Preprocessing: extract the beats and the features (tempo, pitch, timbre) from M

# Pose level: generate temporally coherent sequences of poses based on the beats and a latent vector z
z = CVAE.encode(M.features) # encode the music features to a latent vector using a conditional variational autoencoder
P = LSTM.generate(M.beats, z) # generate a sequence of poses based on the beats and the latent vector using a long short-term memory network

# Motif level: guide sets of consecutive poses to form a movement that belongs to a specific distribution using a motion perceptual-loss
for each set of poses in P:
  optimize the set of poses to minimize the motion perceptual-loss with respect to a target distribution
  # the motion perceptual-loss measures the similarity between two motions based on their perceptual features, such as speed, acceleration, and curvature

# Choreography level: select the order of the performed movements and drive the system to follow the global structure of a dance genre using a Markov chain model
initialize an empty sequence of movements Q
while Q is not complete:
  sample a movement from the Markov chain model based on the previous movement and the dance genre
  append the movement to Q
  # the Markov chain model learns the transition probabilities between different motifs from a large-scale motion dataset

# Postprocessing: align the sequence of movements Q with the sequence of poses P and smooth the transitions between movements

return P
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at http://arxiv.org/pdf/2111.12159v1:

```python
# Input: a music clip M
# Output: a sequence of human poses P that are synchronized with M and follow a specific dance genre

# Preprocessing: extract the beats and the features (tempo, pitch, timbre) from M using librosa library
M.beats = librosa.beat.beat_track(M) # get the beat frames and tempo from M
M.features = librosa.feature.melspectrogram(M) # get the mel-scaled spectrogram from M

# Pose level: generate temporally coherent sequences of poses based on the beats and a latent vector z
z = CVAE.encode(M.features) # encode the music features to a latent vector using a conditional variational autoencoder
# the CVAE consists of an encoder network that maps the music features to a latent vector, and a decoder network that reconstructs the music features from the latent vector
# the CVAE is trained on a large-scale music dataset to learn the conditional distribution of the latent vector given the music features
P = LSTM.generate(M.beats, z) # generate a sequence of poses based on the beats and the latent vector using a long short-term memory network
# the LSTM consists of an input layer that takes the beat frames and the latent vector as inputs, a hidden layer that updates its hidden state based on the inputs and the previous hidden state, and an output layer that produces a pose vector as output
# the LSTM is trained on a large-scale motion dataset to learn the conditional distribution of the pose vector given the beat frames and the latent vector

# Motif level: guide sets of consecutive poses to form a movement that belongs to a specific distribution using a motion perceptual-loss
for each set of poses in P:
  optimize the set of poses to minimize the motion perceptual-loss with respect to a target distribution
  # the motion perceptual-loss measures the similarity between two motions based on their perceptual features, such as speed, acceleration, and curvature
  # the motion perceptual-loss is defined as L(P,Q) = ||F(P) - F(Q)||_2^2, where F is a function that extracts the perceptual features from a motion, P is a set of poses, and Q is a target motion from a specific distribution
  # the optimization is done using gradient descent with Adam optimizer

# Choreography level: select the order of the performed movements and drive the system to follow the global structure of a dance genre using a Markov chain model
initialize an empty sequence of movements Q
while Q is not complete:
  sample a movement from the Markov chain model based on the previous movement and the dance genre
  append the movement to Q
  # the Markov chain model learns the transition probabilities between different motifs from a large-scale motion dataset
  # each motif is a cluster of movements that share similar perceptual features
  # each movement is represented by its centroid pose and its duration

# Postprocessing: align the sequence of movements Q with the sequence of poses P and smooth the transitions between movements
for each movement in Q:
  find the closest matching pose in P using Euclidean distance
  align the movement with the pose by applying rigid transformation
  interpolate between movements using linear interpolation or spline interpolation
  smooth the transitions using Gaussian smoothing or low-pass filtering

return P
```