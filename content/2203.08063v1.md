---
title: 2203.08063v1 MotionCLIP  Exposing Human Motion Generation to CLIP Space
date: 2022-03-09
---

# [MotionCLIP: Exposing Human Motion Generation to CLIP Space](http://arxiv.org/abs/2203.08063v1)

authors: Guy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2203.08063 "MotionCLIP: Exposing Human Motion Generation to CLIP Space"
[2]: https://arxiv.org/pdf/2203.08063.pdf "MotionCLIP: Exposing Human Motion Generation to CLIP Space - arXiv.org"
[3]: http://export.arxiv.org/abs/2301.08063v1 "[2301.08063v1] Chaos in the Mass-Deformed ABJM Model"

Here is a summary of the paper:

- **What**: The paper introduces **MotionCLIP**, a 3D human motion auto-encoder that aligns its latent space with that of the **Contrastive Language-Image Pre-training (CLIP)** model[^1^][1].
- **Why**: The paper aims to exploit the rich semantic knowledge of CLIP to enable human motion generation and editing based on natural language descriptions[^1^][1].
- **How**: The paper trains a transformer-based motion auto-encoder to reconstruct motion while being aligned to its text label's position in CLIP-space. It also aligns motion to rendered frames in a self-supervised manner. The paper demonstrates various applications of MotionCLIP, such as text-to-motion, motion interpolation, motion editing and motion recognition[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2203.08063 "MotionCLIP: Exposing Human Motion Generation to CLIP Space"
[2]: https://arxiv.org/pdf/2203.08063.pdf "MotionCLIP: Exposing Human Motion Generation to CLIP Space - arXiv.org"
[3]: http://export.arxiv.org/abs/2301.08063v1 "[2301.08063v1] Chaos in the Mass-Deformed ABJM Model"

According to the paper[^1^][1], the main contributions are:

- **A novel motion auto-encoder that aligns its latent space with CLIP space**, enabling text-to-motion and motion-to-text capabilities.
- **A self-supervised alignment of motion to rendered frames**, enhancing the visual semantics of the latent space and improving the reconstruction quality.
- **A comprehensive evaluation of MotionCLIP on various tasks**, such as motion interpolation, motion editing, motion recognition and out-of-domain generation.

## Method Summary

[1]: https://arxiv.org/abs/2203.08063 "MotionCLIP: Exposing Human Motion Generation to CLIP Space"
[2]: https://arxiv.org/pdf/2203.08063.pdf "MotionCLIP: Exposing Human Motion Generation to CLIP Space - arXiv.org"
[3]: http://export.arxiv.org/abs/2301.08063v1 "[2301.08063v1] Chaos in the Mass-Deformed ABJM Model"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses a **transformer-based motion auto-encoder** that takes as input a sequence of 3D human poses and outputs a latent vector that represents the motion.
- The paper uses a **text encoder** that takes as input a text label describing the motion and outputs a latent vector that represents the text.
- The paper uses a **CLIP encoder** that takes as input an image or a text and outputs a latent vector that represents the image or text in CLIP space.
- The paper trains the motion auto-encoder to minimize three losses: a **reconstruction loss** that measures the difference between the input and output poses, a **text alignment loss** that measures the distance between the motion and text vectors in CLIP space, and a **frame alignment loss** that measures the distance between the motion vector and the CLIP vector of a rendered frame from the motion sequence.
- The paper uses a **motion decoder** that takes as input a latent vector in CLIP space and outputs a sequence of 3D human poses that corresponds to the vector. The motion decoder can be used to generate motions from text prompts or from interpolated or edited vectors.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the motion auto-encoder
motion_encoder = TransformerEncoder(input_size=pose_size, hidden_size=latent_size)
motion_decoder = TransformerDecoder(input_size=latent_size, output_size=pose_size)

# Define the text encoder
text_encoder = TransformerEncoder(input_size=text_size, hidden_size=latent_size)

# Define the CLIP encoder
clip_encoder = CLIPModel()

# Define the losses
reconstruction_loss = L2Loss()
text_alignment_loss = CosineDistanceLoss()
frame_alignment_loss = CosineDistanceLoss()

# Train the motion auto-encoder
for epoch in epochs:
  for batch in batches:
    # Get the input poses and text labels
    poses = batch.poses # shape: (batch_size, seq_len, pose_size)
    texts = batch.texts # shape: (batch_size, text_size)

    # Encode the poses and texts
    motion_vectors = motion_encoder(poses) # shape: (batch_size, latent_size)
    text_vectors = text_encoder(texts) # shape: (batch_size, latent_size)

    # Project the motion and text vectors to CLIP space
    motion_clip_vectors = clip_encoder.project(motion_vectors) # shape: (batch_size, clip_size)
    text_clip_vectors = clip_encoder.project(text_vectors) # shape: (batch_size, clip_size)

    # Decode the motion vectors
    reconstructed_poses = motion_decoder(motion_vectors) # shape: (batch_size, seq_len, pose_size)

    # Render the poses to frames
    frames = render(poses) # shape: (batch_size, seq_len, image_size)

    # Encode the frames with CLIP
    frame_clip_vectors = clip_encoder.encode_image(frames) # shape: (batch_size, seq_len, clip_size)

    # Compute the losses
    rec_loss = reconstruction_loss(poses, reconstructed_poses)
    txt_loss = text_alignment_loss(motion_clip_vectors, text_clip_vectors)
    frm_loss = frame_alignment_loss(motion_clip_vectors, frame_clip_vectors.mean(dim=1))

    # Update the parameters
    loss = rec_loss + txt_loss + frm_loss
    loss.backward()
    optimizer.step()

# Generate motions from text prompts
for prompt in prompts:
  # Encode the prompt with CLIP
  prompt_vector = clip_encoder.encode_text(prompt) # shape: (clip_size)

  # Project the prompt vector to latent space
  latent_vector = clip_encoder.unproject(prompt_vector) # shape: (latent_size)

  # Decode the latent vector to poses
  generated_poses = motion_decoder(latent_vector) # shape: (seq_len, pose_size)

  # Display the generated motion
  display(generated_poses)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import clip
import numpy as np

# Define the constants
pose_size = 72 # number of joints * 3
text_size = 256 # length of text tokens
latent_size = 512 # size of latent vectors
clip_size = 512 # size of CLIP vectors
image_size = 224 # size of rendered images
batch_size = 32 # size of mini-batches
seq_len = 64 # length of motion sequences
num_heads = 8 # number of attention heads
num_layers = 6 # number of transformer layers
lr = 0.0001 # learning rate

# Define the transformer encoder
class TransformerEncoder(nn.Module):
  def __init__(self, input_size, hidden_size):
    super().__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size

    # Define the embedding layer
    self.embedding = nn.Linear(input_size, hidden_size)

    # Define the positional encoding
    self.positional_encoding = self.get_positional_encoding(seq_len, hidden_size)

    # Define the transformer encoder layers
    self.encoder_layers = nn.ModuleList([nn.TransformerEncoderLayer(hidden_size, num_heads) for _ in range(num_layers)])

  def get_positional_encoding(self, seq_len, hidden_size):
    # Compute the positional encoding matrix
    pe = torch.zeros(seq_len, hidden_size)
    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, hidden_size, 2).float() * (-np.log(10000.0) / hidden_size))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0)
    return pe

  def forward(self, x):
    # x: (batch_size, seq_len, input_size)

    # Embed the input
    x = self.embedding(x) # (batch_size, seq_len, hidden_size)

    # Add the positional encoding
    x = x + self.positional_encoding # (batch_size, seq_len, hidden_size)

    # Transpose the input for transformer encoder
    x = x.transpose(0, 1) # (seq_len, batch_size, hidden_size)

    # Apply the transformer encoder layers
    for layer in self.encoder_layers:
      x = layer(x) # (seq_len, batch_size, hidden_size)

    # Transpose the output back
    x = x.transpose(0, 1) # (batch_size, seq_len, hidden_size)

    # Return the last hidden state as the latent vector
    return x[:, -1, :] # (batch_size, hidden_size)


# Define the transformer decoder
class TransformerDecoder(nn.Module):
  def __init__(self, input_size, output_size):
    super().__init__()
    self.input_size = input_size
    self.output_size = output_size

    # Define the embedding layer
    self.embedding = nn.Linear(input_size, output_size)

    # Define the positional encoding
    self.positional_encoding = self.get_positional_encoding(seq_len, output_size)

    # Define the transformer decoder layers
    self.decoder_layers = nn.ModuleList([nn.TransformerDecoderLayer(output_size, num_heads) for _ in range(num_layers)])

  def get_positional_encoding(self, seq_len, output_size):
    # Compute the positional encoding matrix
    pe = torch.zeros(seq_len, output_size)
    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, output_size, 2).float() * (-np.log(10000.0) / output_size))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0)
    return pe

  def forward(self, x):
    # x: (batch_size, input_size)

    # Repeat the input for seq_len times
    x = x.unsqueeze(1).repeat(1, seq_len, 1) # (batch_size, seq_len, input_size)

    # Embed the input
    x = self.embedding(x) # (batch_size, seq_len, output_size)

    # Add the positional encoding
    x = x + self.positional_encoding # (batch_size, seq_len, output_size)

    # Transpose the input for transformer decoder
    x = x.transpose(0, 1) # (seq_len, batch_size, output_size)

    # Apply the transformer decoder layers
    for layer in self.decoder_layers:
      x = layer(x, x) # (seq_len, batch_size, output_size)

    # Transpose the output back
    x = x.transpose(0, 1) # (batch_size, seq_len, output_size)

    # Return the output poses
    return x # (batch_size, seq_len, output_size)


# Define the motion auto-encoder
motion_encoder = TransformerEncoder(input_size=pose_size, hidden_size=latent_size)
motion_decoder = TransformerDecoder(input_size=latent_size, output_size=pose_size)

# Define the text encoder
text_encoder = TransformerEncoder(input_size=text_size, hidden_size=latent_size)

# Define the CLIP encoder
clip_encoder = clip.load("ViT-B/32", jit=False)[0]

# Define the losses
reconstruction_loss = nn.MSELoss()
text_alignment_loss = nn.CosineEmbeddingLoss()
frame_alignment_loss = nn.CosineEmbeddingLoss()

# Define the optimizer
optimizer = torch.optim.Adam(list(motion_encoder.parameters()) + list(motion_decoder.parameters()) + list(text_encoder.parameters()), lr=lr)

# Load the data
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Train the motion auto-encoder
for epoch in epochs:
  for batch in data_loader:
    # Get the input poses and text labels
    poses = batch.poses # shape: (batch_size, seq_len, pose_size)
    texts = batch.texts # shape: (batch_size, text_size)

    # Encode the poses and texts
    motion_vectors = motion_encoder(poses) # shape: (batch_size, latent_size)
    text_vectors = text_encoder(texts) # shape: (batch_size, latent_size)

    # Project the motion and text vectors to CLIP space
    motion_clip_vectors = clip_encoder.visual.proj(motion_vectors) # shape: (batch_size, clip_size)
    text_clip_vectors = clip_encoder.encode_text(texts).float() # shape: (batch_size, clip_size)

    # Decode the motion vectors
    reconstructed_poses = motion_decoder(motion_vectors) # shape: (batch_size, seq_len, pose_size)

    # Render the poses to frames
    frames = render(poses) # shape: (batch_size, seq_len, image_size)

    # Encode the frames with CLIP
    frame_clip_vectors = clip_encoder.encode_image(frames).float() # shape: (batch_size, seq_len, clip_size)

    # Compute the losses
    rec_loss = reconstruction_loss(poses, reconstructed_poses)
    txt_loss = text_alignment_loss(motion_clip_vectors, text_clip_vectors, torch.ones(batch_size))
    frm_loss = frame_alignment_loss(motion_clip_vectors.unsqueeze(1), frame_clip_vectors.mean(dim=1).unsqueeze(1), torch.ones(batch_size))

    # Update the parameters
    loss = rec_loss + txt_loss + frm_loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Generate motions from text prompts
for prompt in prompts:
  # Encode the prompt with CLIP
  prompt_vector = clip_encoder.encode_text(prompt).float() # shape: (clip_size)

  # Project the prompt vector to latent space
  latent_vector = clip_encoder.visual.inv_proj(prompt_vector) # shape: (latent_size)

  # Decode the latent vector to poses
  generated_poses = motion_decoder(latent_vector) # shape: (seq_len, pose_size)

  # Display the generated motion
  display(generated_poses)
```