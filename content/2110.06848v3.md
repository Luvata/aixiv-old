---
title: 2110.06848v3 Decoupled Contrastive Learning
date: 2021-10-07
---

# [Decoupled Contrastive Learning](http://arxiv.org/abs/2110.06848v3)

authors: Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, Yann LeCun


## What, Why and How

[1]: https://arxiv.org/abs/2110.06848 "[2110.06848] Decoupled Contrastive Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2110.06848v3.pdf "Decoupled Contrastive Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2110.06848v2.pdf "arXiv:2110.06848v2 [cs.LG] 23 Oct 2021"

Here is a summary of the paper based on the abstract[^1^][1]:

- **What**: The paper proposes a new loss function for contrastive learning (CL), called decoupled contrastive learning (DCL), which removes the positive term from the denominator of the InfoNCE loss and improves the learning efficiency and performance of self-supervised learning (SSL).
- **Why**: The paper identifies a negative-positive-coupling (NPC) effect in the InfoNCE loss, which leads to unsuitable learning efficiency concerning the batch size. The paper aims to tackle this issue and establish a simple, efficient, yet competitive baseline of contrastive learning that does not require large batches, momentum encoding, or large epochs.
- **How**: The paper removes the NPC effect by decoupling the positive term from the denominator of the InfoNCE loss, resulting in the DCL loss. The paper demonstrates the effectiveness and robustness of DCL on various benchmarks and shows that it can achieve competitive or state-of-the-art performance with less sensitivity to sub-optimal hyperparameters. The paper also combines DCL with NNCLR, a recent contrastive learning method, to achieve a new state-of-the-art in contrastive learning.


## Main Contributions

[1]: https://arxiv.org/abs/2110.06848 "[2110.06848] Decoupled Contrastive Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2110.06848v3.pdf "Decoupled Contrastive Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2110.06848v2.pdf "arXiv:2110.06848v2 [cs.LG] 23 Oct 2021"

According to the paper[^1^][1], the main contributions are:

- The paper identifies a negative-positive-coupling (NPC) effect in the InfoNCE loss, which leads to unsuitable learning efficiency concerning the batch size and sub-optimal hyperparameters.
- The paper proposes a decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator of the InfoNCE loss and significantly improves the learning efficiency and performance of contrastive learning.
- The paper demonstrates the effectiveness and robustness of DCL on various benchmarks and shows that it can achieve competitive or state-of-the-art performance with less sensitivity to sub-optimal hyperparameters.
- The paper combines DCL with NNCLR, a recent contrastive learning method, to achieve a new state-of-the-art in contrastive learning on ImageNet-1K.


## Method Summary

[1]: https://arxiv.org/abs/2110.06848 "[2110.06848] Decoupled Contrastive Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2110.06848v3.pdf "Decoupled Contrastive Learning - arXiv.org"
[3]: https://arxiv.org/pdf/2110.06848v2.pdf "arXiv:2110.06848v2 [cs.LG] 23 Oct 2021"

Here is a summary of the method section based on the paper[^1^][1]:

- The paper introduces the InfoNCE loss, which is a widely used loss function for contrastive learning that maximizes the mutual information between two views of the same image. The paper shows that the InfoNCE loss can be decomposed into two terms: a positive term that measures the similarity between the positive pair, and a negative term that measures the similarity between the negative pairs. The paper also shows that the gradient of the InfoNCE loss can be decomposed into two factors: a gradient multiplier that depends on both the positive and negative terms, and a gradient direction that depends only on the negative term.
- The paper identifies a negative-positive-coupling (NPC) effect in the InfoNCE loss, which means that the gradient multiplier is affected by both the positive and negative terms. The paper argues that this effect leads to unsuitable learning efficiency concerning the batch size, as larger batches tend to increase the positive term and decrease the gradient multiplier. The paper also shows that this effect makes the InfoNCE loss more sensitive to sub-optimal hyperparameters, such as temperature and learning rate.
- The paper proposes a decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator of the InfoNCE loss and eliminates the NPC effect. The paper shows that the DCL loss has a constant gradient multiplier of 1, which means that it is independent of the batch size and the positive term. The paper also shows that the DCL loss has a simpler and more intuitive form than the InfoNCE loss, as it only depends on the negative term.
- The paper evaluates the DCL loss on various benchmarks and compares it with other contrastive learning methods, such as SimCLR, MoCo, NNCLR, etc. The paper shows that DCL achieves competitive or state-of-the-art performance with less sensitivity to sub-optimal hyperparameters. The paper also shows that DCL can be combined with NNCLR, a recent contrastive learning method that uses nearest neighbors for positive pairs, to achieve a new state-of-the-art in contrastive learning on ImageNet-1K.


## Pseudo Code - High level

Here is a possible high-level pseudo code for the paper:

```
# Define the DCL loss function
def DCL_loss(z_i, z_j):
  # z_i and z_j are the representations of two views of the same image
  # Compute the cosine similarity between z_i and z_j
  sim_pos = cos_sim(z_i, z_j)
  # Compute the cosine similarity between z_i and all other representations in the batch
  sim_neg = cos_sim(z_i, batch_z)
  # Remove the positive pair from the negative pairs
  sim_neg = remove(sim_neg, sim_pos)
  # Apply a temperature scaling to the similarities
  sim_pos = sim_pos / tau
  sim_neg = sim_neg / tau
  # Compute the log-sum-exp of the negative similarities
  lse_neg = log_sum_exp(sim_neg)
  # Compute the DCL loss as the negative log-likelihood of the positive pair
  loss = -sim_pos + lse_neg
  return loss

# Define the contrastive learning model
def CL_model():
  # Initialize an encoder network f and a projection head g
  f = Encoder()
  g = Projection()
  # Initialize an optimizer for f and g
  optimizer = Optimizer(f, g)
  # Loop over the training data
  for x in data:
    # Generate two views of x using data augmentation
    x_i, x_j = augment(x)
    # Encode and project the views using f and g
    z_i = g(f(x_i))
    z_j = g(f(x_j))
    # Compute the DCL loss for the views
    loss = DCL_loss(z_i, z_j)
    # Update f and g using gradient descent
    optimizer.step(loss)
  return f

# Train the contrastive learning model on unlabeled data
f = CL_model()
# Evaluate the learned representations on downstream tasks using a linear classifier or a fine-tuning strategy
evaluate(f)
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
batch_size = 256 # The batch size for training
tau = 0.07 # The temperature scaling factor
lr = 0.03 # The learning rate for the optimizer
epochs = 200 # The number of epochs for training
num_classes = 1000 # The number of classes for ImageNet-1K

# Define the data augmentation pipeline
augment = torchvision.transforms.Compose([
  torchvision.transforms.RandomResizedCrop(224), # Randomly crop and resize the image to 224x224
  torchvision.transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally
  torchvision.transforms.ColorJitter(0.4, 0.4, 0.4, 0.1), # Randomly change the brightness, contrast, saturation and hue of the image
  torchvision.transforms.ToTensor(), # Convert the image to a tensor
  torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # Normalize the image using the mean and standard deviation of ImageNet
])

# Define the encoder network f as a ResNet-50 model pretrained on ImageNet
f = torchvision.models.resnet50(pretrained=True)
# Remove the last fully connected layer of f
f.fc = torch.nn.Identity()

# Define the projection head g as a two-layer MLP with ReLU activation
g = torch.nn.Sequential(
  torch.nn.Linear(2048, 2048), # The first linear layer with input dimension 2048 and output dimension 2048
  torch.nn.ReLU(), # The ReLU activation function
  torch.nn.Linear(2048, 128) # The second linear layer with input dimension 2048 and output dimension 128
)

# Define the cosine similarity function
def cos_sim(x, y):
  # x and y are two tensors of shape (batch_size, dim)
  # Compute the dot product between x and y along the dim dimension
  dot = torch.sum(x * y, dim=1)
  # Compute the L2 norm of x and y along the dim dimension
  norm_x = torch.sqrt(torch.sum(x * x, dim=1))
  norm_y = torch.sqrt(torch.sum(y * y, dim=1))
  # Compute the cosine similarity as the dot product divided by the product of norms
  sim = dot / (norm_x * norm_y)
  return sim

# Define the DCL loss function
def DCL_loss(z_i, z_j):
  # z_i and z_j are two tensors of shape (batch_size, dim) representing two views of the same image
  # Compute the cosine similarity between z_i and z_j
  sim_pos = cos_sim(z_i, z_j)
  # Compute the cosine similarity between z_i and all other representations in the batch using matrix multiplication
  sim_neg = torch.matmul(z_i, z_j.T)
  # Remove the positive pair from the negative pairs by setting its similarity to a large negative value
  sim_neg[torch.arange(batch_size), torch.arange(batch_size)] = -1e9
  # Apply a temperature scaling to the similarities
  sim_pos = sim_pos / tau
  sim_neg = sim_neg / tau
  # Compute the log-sum-exp of the negative similarities using the logsumexp function
  lse_neg = torch.logsumexp(sim_neg, dim=1)
  # Compute the DCL loss as the negative log-likelihood of the positive pair using the mean function
  loss = torch.mean(-sim_pos + lse_neg)
  return loss

# Define the contrastive learning model as a combination of f and g
model = torch.nn.Sequential(f, g)

# Define the optimizer as a stochastic gradient descent (SGD) optimizer with momentum and weight decay
optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)

# Load the unlabeled ImageNet-1K dataset using torchvision.datasets.ImageFolder
data = torchvision.datasets.ImageFolder(root='path/to/ImageNet-1K', transform=None)

# Define a custom collate function that generates two views of each image using data augmentation
def collate_fn(batch):
  # batch is a list of (image, label) pairs
  images = [] # A list to store the images
  labels = [] # A list to store the labels
  for image, label in batch:
    # Generate two views of the image using data augmentation
    image_i = augment(image)
    image_j = augment(image)
    # Append the views and the label to the lists
    images.append(image_i)
    images.append(image_j)
    labels.append(label)
  # Stack the images and labels into tensors
  images = torch.stack(images)
  labels = torch.tensor(labels)
  return images, labels

# Define a data loader that shuffles and batches the data using the custom collate function
loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

# Loop over the number of epochs
for epoch in range(epochs):
  # Loop over the batches of data
  for images, labels in loader:
    # Split the images into two views
    images_i = images[0::2] # The first view of each image
    images_j = images[1::2] # The second view of each image
    # Encode and project the views using the model
    z_i = model(images_i)
    z_j = model(images_j)
    # Compute the DCL loss for the views
    loss = DCL_loss(z_i, z_j)
    # Update the model using gradient descent
    optimizer.zero_grad() # Zero the gradients
    loss.backward() # Compute the gradients
    optimizer.step() # Update the parameters
  # Print the epoch and the loss
  print(f'Epoch {epoch}, Loss {loss.item()}')

# Save the model
torch.save(model, 'model.pth')

# Evaluate the learned representations on downstream tasks using a linear classifier or a fine-tuning strategy
evaluate(model)
```