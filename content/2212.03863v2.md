---
title: 2212.03863v2 X-Paste  Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion
date: 2022-12-04
---

# [X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion](http://arxiv.org/abs/2212.03863v2)

authors: Hanqing Zhao, Dianmo Sheng, Jianmin Bao, Dongdong Chen, Dong Chen, Fang Wen, Lu Yuan, Ce Liu, Wenbo Zhou, Qi Chu, Weiming Zhang, Nenghai Yu


## What, Why and How

[1]: https://arxiv.org/pdf/2212.03863v2.pdf "Abstract arXiv:2212.03863v2 [cs.CV] 31 May 2023"
[2]: https://arxiv.org/abs/2212.03863 "[2212.03863] X-Paste: Revisiting Scalable Copy-Paste for Instance ..."
[3]: https://arxiv-export1.library.cornell.edu/abs/2212.03863v2 "[2212.03863v2] X-Paste: Revisiting Scalable Copy-Paste for Instance ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a data augmentation strategy for instance segmentation called X-Paste, which uses zero-shot recognition models (e.g., CLIP) and text2image models (e.g., StableDiffusion) to obtain diverse and high-quality object instances for different categories.
- **Why**: The paper aims to address the challenge of data scarcity and long-tail distribution in instance segmentation, which limits the performance of existing methods that rely on human-annotated or 3D-rendered object instances.
- **How**: The paper designs a data acquisition and processing framework that consists of four steps: (1) generating or crawling images for each category using text2image models or web search engines; (2) filtering out noisy or irrelevant images using zero-shot recognition models; (3) extracting object instances from the filtered images using existing instance segmentation models; and (4) pasting the extracted instances onto new background images to create augmented training data. The paper also conducts a systematic study on the LVIS dataset to evaluate the effectiveness of X-Paste.

## Main Contributions

According to the paper, the main contributions are:

- The paper revisits Copy-Paste at scale with the power of newly emerged zero-shot recognition models and text2image models, and demonstrates that they can provide diverse and high-quality object instances for different categories in a scalable way.
- The paper designs a data acquisition and processing framework, dubbed X-Paste, which consists of four steps: image generation or crawling, image filtering, instance extraction, and instance pasting. The paper also provides detailed analysis and ablation studies on each step.
- The paper shows that X-Paste can significantly improve the performance of instance segmentation models on the LVIS dataset, especially for long-tail classes. The paper also reports competitive results on the COCO dataset.

## Method Summary

The method section of the paper describes the four steps of X-Paste in detail:

- **Image generation or crawling**: For each category in the LVIS dataset, the paper either uses a text2image model (StableDiffusion) to generate images from category names, or uses a web search engine (Bing) to crawl images from the Internet. The paper also compares the quality and diversity of the generated and crawled images using human evaluation and CLIP scores.
- **Image filtering**: To remove noisy or irrelevant images from the crawled data, the paper uses a zero-shot recognition model (CLIP) to compute the similarity between each image and the category name, and filters out images with low similarity scores. The paper also studies the impact of different similarity thresholds on the performance of instance segmentation models.
- **Instance extraction**: To obtain object instances from the filtered images, the paper uses an existing instance segmentation model (CenterNet2) to predict bounding boxes and masks for each image, and crops out the instances according to the predictions. The paper also applies some post-processing techniques to refine the masks and remove duplicates or overlaps.
- **Instance pasting**: To create augmented training data, the paper randomly pastes the extracted instances onto new background images from the LVIS dataset, and adjusts the brightness and contrast of the pasted instances to match the background. The paper also considers some factors such as instance size, aspect ratio, and category frequency when sampling instances for pasting.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion

# Inputs: LVIS dataset, category names, text2image model, web search engine, zero-shot recognition model, instance segmentation model
# Outputs: Augmented training data for instance segmentation

# Step 1: Image generation or crawling
for each category in LVIS:
  if category is common:
    # use web search engine to crawl images
    images = web_search(category)
  else:
    # use text2image model to generate images
    images = text2image(category)
  # store images in a category-specific folder

# Step 2: Image filtering
for each category in LVIS:
  # load images from the folder
  images = load_images(category)
  # compute similarity scores between images and category name using zero-shot recognition model
  scores = zeroshot_recognition(images, category)
  # filter out images with low scores
  images = filter_images(images, scores, threshold)
  # store filtered images in a new folder

# Step 3: Instance extraction
for each category in LVIS:
  # load filtered images from the folder
  images = load_images(category)
  # predict bounding boxes and masks for each image using instance segmentation model
  boxes, masks = instance_segmentation(images)
  # crop out instances according to the predictions
  instances = crop_instances(images, boxes, masks)
  # apply post-processing techniques to refine masks and remove duplicates or overlaps
  instances = post_process(instances)
  # store instances in a new folder

# Step 4: Instance pasting
# initialize an empty list for augmented data
augmented_data = []
# loop over the number of augmented samples to create
for i in range(num_samples):
  # randomly sample a background image from the LVIS dataset
  background = sample_background(LVIS)
  # randomly sample a number of instances to paste
  num_instances = sample_num_instances()
  # initialize an empty list for pasted instances and their labels
  pasted_instances = []
  pasted_labels = []
  # loop over the number of instances to paste
  for j in range(num_instances):
    # randomly sample an instance from the folder according to some factors (size, aspect ratio, frequency, etc.)
    instance = sample_instance()
    # randomly sample a position to paste the instance on the background
    position = sample_position(background, instance)
    # adjust the brightness and contrast of the instance to match the background
    instance = adjust_brightness_contrast(instance, background)
    # paste the instance on the background at the position
    background = paste_instance(background, instance, position)
    # append the instance and its label to the lists
    pasted_instances.append(instance)
    pasted_labels.append(instance.label)
  # append the background image and the lists of pasted instances and labels to the augmented data list
  augmented_data.append((background, pasted_instances, pasted_labels))

# return the augmented data list as the output
return augmented_data

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion

# Import the necessary libraries and modules
import os
import random
import numpy as np
import cv2
import torch
from PIL import Image
from stable_diffusion import StableDiffusion
from clip import load_model, tokenize
from centernet2 import CenterNet2

# Define some constants and hyperparameters
NUM_SAMPLES = 100000 # the number of augmented samples to create
THRESHOLD = 0.5 # the similarity threshold for image filtering
MAX_INSTANCES = 10 # the maximum number of instances to paste per sample
MIN_SIZE = 32 # the minimum size of instances to paste
MAX_SIZE = 256 # the maximum size of instances to paste
MIN_ASPECT_RATIO = 0.5 # the minimum aspect ratio of instances to paste
MAX_ASPECT_RATIO = 2.0 # the maximum aspect ratio of instances to paste
BRIGHTNESS_FACTOR = 0.2 # the brightness adjustment factor for instance pasting
CONTRAST_FACTOR = 0.2 # the contrast adjustment factor for instance pasting

# Load the LVIS dataset and category names
LVIS = load_lvis_dataset()
categories = load_lvis_categories()

# Load the text2image model (StableDiffusion)
text2image_model = StableDiffusion()

# Load the web search engine (Bing)
web_search_engine = load_bing()

# Load the zero-shot recognition model (CLIP)
zeroshot_recognition_model, preprocess = load_model("ViT-B/32", device="cuda")

# Load the instance segmentation model (CenterNet2)
instance_segmentation_model = CenterNet2()

# Step 1: Image generation or crawling
for category in categories:
  # create a folder for each category
  os.mkdir(category)
  # check if the category is common or rare
  if category.is_common():
    # use web search engine to crawl images
    images = web_search_engine.search(category.name, num_images=1000)
  else:
    # use text2image model to generate images
    images = text2image_model.generate(category.name, num_images=1000)
  # save images in the category folder
  for i, image in enumerate(images):
    image.save(os.path.join(category, f"{i}.jpg"))

# Step 2: Image filtering
for category in categories:
  # load images from the category folder
  images = []
  for filename in os.listdir(category):
    image = Image.open(os.path.join(category, filename))
    images.append(image)
  # convert images to tensors and preprocess them for CLIP model
  images_tensor = torch.stack([preprocess(image) for image in images]).to("cuda")
  # encode category name as text tokens for CLIP model
  text_tokens = tokenize([category.name]).to("cuda")
  # compute similarity scores between images and category name using CLIP model
  with torch.no_grad():
    image_features = zeroshot_recognition_model.encode_image(images_tensor)
    text_features = zeroshot_recognition_model.encode_text(text_tokens)
    scores = torch.matmul(image_features, text_features.T).squeeze()
  # filter out images with low scores using a threshold
  filtered_images = []
  for i, image in enumerate(images):
    if scores[i] > THRESHOLD:
      filtered_images.append(image)
  # create a new folder for filtered images
  os.mkdir(category + "_filtered")
  # save filtered images in the new folder
  for i, image in enumerate(filtered_images):
    image.save(os.path.join(category + "_filtered", f"{i}.jpg"))

# Step 3: Instance extraction
for category in categories:
  # load filtered images from the folder
  images = []
  for filename in os.listdir(category + "_filtered"):
    image = Image.open(os.path.join(category + "_filtered", filename))
    images.append(image)
  # convert images to tensors and preprocess them for CenterNet2 model
  images_tensor = torch.stack([instance_segmentation_model.preprocess(image) for image in images]).to("cuda")
  # predict bounding boxes and masks for each image using CenterNet2 model
  with torch.no_grad():
    boxes, masks = instance_segmentation_model.predict(images_tensor)
  
```