---
title: 2206.02903v1 Polymorphic-GAN  Generating Aligned Samples across Multiple Domains with Learned Morph Maps
date: 2022-06-03
---

# [Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps](http://arxiv.org/abs/2206.02903v1)

authors: Seung Wook Kim, Karsten Kreis, Daiqing Li, Antonio Torralba, Sanja Fidler


## What, Why and How

[1]: https://arxiv.org/abs/2206.02903 "[2206.02903] Polymorphic-GAN: Generating Aligned Samples across ..."
[2]: https://arxiv.org/pdf/2206.02903.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2203.02903v1 "[2203.02903v1] Geometric Hermite Interpolation in $\\mathbb{R}^n$ by ..."

Here is a summary of the paper based on the first search result[^1^][1]:

- **What**: The paper introduces a generative adversarial network (GAN) that can generate aligned image samples from multiple related domains with learned morph maps. The paper calls this model Polymorphic-GAN.
- **Why**: The paper aims to address the challenge of modelling images with highly varying geometries, such as human faces, animal faces, and artistic faces. The paper also demonstrates how the model can be used for applications such as segmentation transfer, cross-domain image editing, and image-to-image translation.
- **How**: The paper proposes to learn shared features across all domains and a per-domain morph layer to morph shared features according to each domain. The paper also introduces a novel alignment loss that encourages the generated samples to be aligned across domains. The paper evaluates the model on several datasets and compares it with previous approaches.

## Main Contributions

[1]: https://arxiv.org/abs/2206.02903 "[2206.02903] Polymorphic-GAN: Generating Aligned Samples across ..."
[2]: https://arxiv.org/pdf/2206.02903.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2203.02903v1 "[2203.02903v1] Geometric Hermite Interpolation in $\\mathbb{R}^n$ by ..."

According to the first search result[^1^][1], the main contributions of this paper are:

- **Polymorphic-GAN**: A novel GAN that can generate aligned samples across multiple domains with learned morph maps. The model leverages shared features across domains and a per-domain morph layer to adapt the features to each domain.
- **Alignment loss**: A new loss function that encourages the generated samples to be aligned across domains by minimizing the distance between the morphed features and the real features of each domain.
- **Applications**: The paper shows how the model can be used for segmentation transfer, cross-domain image editing, and image-to-image translation, especially when the geometric differences between domains are large.
- **Experiments**: The paper evaluates the model on several datasets, such as CelebA-HQ, Animal Faces-HQ, and Artistic Faces-HQ, and compares it with previous approaches, such as StarGAN v2 and StyleGAN2. The paper demonstrates that the model produces high-quality and diverse samples that are well-aligned across domains.

## Method Summary

[1]: https://arxiv.org/abs/2206.02903 "[2206.02903] Polymorphic-GAN: Generating Aligned Samples across ..."
[2]: https://arxiv.org/pdf/2206.02903.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2203.02903v1 "[2203.02903v1] Geometric Hermite Interpolation in $\\mathbb{R}^n$ by ..."

Here is a summary of the method section of the paper based on the first search result[^1^][1]:

- **Overview**: The paper proposes a GAN that consists of a generator G, a discriminator D, and a classifier C. The generator G takes a latent code z and a domain label y as inputs and outputs an image x. The discriminator D tries to distinguish between real and fake images, while the classifier C tries to predict the domain label of the images. The paper also introduces a morph layer M that can morph the features of one domain to another domain by applying affine transformations.
- **Shared features**: The paper proposes to learn shared features across all domains by using a shared encoder E and a shared decoder D. The encoder E maps an image x to a feature map f, while the decoder D maps a feature map f and a domain label y to an image x. The paper argues that this design allows the model to capture common attributes across domains, such as color, texture, and style.
- **Morph layer**: The paper proposes to learn a per-domain morph layer M that can morph the shared features f according to each domain y. The morph layer M consists of two sub-layers: a spatial transformer network (STN) that applies spatial transformations to f, and a channel-wise affine transformation (CAT) that applies channel-wise scaling and shifting to f. The paper argues that this design allows the model to adapt the shared features f to different geometries and shapes of each domain.
- **Alignment loss**: The paper proposes a new loss function that encourages the generated samples to be aligned across domains by minimizing the distance between the morphed features and the real features of each domain. The alignment loss is defined as follows:

$$
\mathcal{L}_{align} = \mathbb{E}_{x,y}[\|M_y(E(x)) - E(x)\|_1] + \mathbb{E}_{z,y}[\|M_y(G(z,y)) - G(z,y)\|_1]
$$

The paper argues that this loss function helps the model to learn meaningful morph maps that preserve the identity and structure of the images across domains.
- **Other losses**: The paper also uses other standard losses for GANs, such as adversarial loss, classification loss, reconstruction loss, and diversity loss. The paper combines these losses with the alignment loss to form the final objective function for the model.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the generator G, the discriminator D, the classifier C, the encoder E, the decoder D, and the morph layer M
G = Generator()
D = Discriminator()
C = Classifier()
E = Encoder()
D = Decoder()
M = MorphLayer()

# Define the losses
adv_loss = AdversarialLoss()
cls_loss = ClassificationLoss()
rec_loss = ReconstructionLoss()
div_loss = DiversityLoss()
align_loss = AlignmentLoss()

# Define the optimizers
opt_G = Optimizer(G.parameters())
opt_D = Optimizer(D.parameters())
opt_C = Optimizer(C.parameters())

# Define the training loop
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get real images and labels
    x_real, y_real = batch
    
    # Sample latent codes and labels
    z_fake, y_fake = sample_latent_codes_and_labels()
    
    # Generate fake images
    x_fake = G(z_fake, y_fake)
    
    # Encode real and fake images
    f_real = E(x_real)
    f_fake = E(x_fake)
    
    # Morph real and fake features
    f_morph_real = M(f_real, y_real)
    f_morph_fake = M(f_fake, y_fake)
    
    # Decode real and fake features
    x_rec_real = D(f_morph_real, y_real)
    x_rec_fake = D(f_morph_fake, y_fake)
    
    # Discriminate real and fake images
    d_real = D(x_real)
    d_fake = D(x_fake)
    
    # Classify real and fake images
    c_real = C(x_real)
    c_fake = C(x_fake)
    
    # Compute losses for G
    loss_G_adv = adv_loss(d_fake, 1) # fool D
    loss_G_cls = cls_loss(c_fake, y_fake) # match labels
    loss_G_rec = rec_loss(x_rec_real, x_real) + rec_loss(x_rec_fake, x_fake) # reconstruct images
    loss_G_div = div_loss(x_fake) # diversify samples
    loss_G_align = align_loss(f_morph_real, f_real) + align_loss(f_morph_fake, f_fake) # align features
    
    # Combine losses for G
    loss_G = loss_G_adv + loss_G_cls + loss_G_rec + loss_G_div + loss_G_align
    
    # Update G
    opt_G.zero_grad()
    loss_G.backward()
    opt_G.step()
    
    # Compute losses for D
    loss_D_adv = adv_loss(d_real, 1) + adv_loss(d_fake, 0) # discriminate real and fake
    
    # Combine losses for D
    loss_D = loss_D_adv
    
    # Update D
    opt_D.zero_grad()
    loss_D.backward()
    opt_D.step()
    
    # Compute losses for C
    loss_C_cls = cls_loss(c_real, y_real) + cls_loss(c_fake, y_fake) # match labels
    
    # Combine losses for C
    loss_C = loss_C_cls
    
    # Update C
    opt_C.zero_grad()
    loss_C.backward()
    opt_C.step()
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
num_epochs = 100 # number of training epochs
batch_size = 32 # batch size
num_domains = 4 # number of domains
latent_dim = 256 # dimension of latent code
feature_dim = 512 # dimension of feature map
image_size = 256 # size of image
num_channels = 3 # number of channels in image
lr = 0.0002 # learning rate
beta1 = 0.5 # beta1 for Adam optimizer
beta2 = 0.999 # beta2 for Adam optimizer
lambda_adv = 1.0 # weight for adversarial loss
lambda_cls = 1.0 # weight for classification loss
lambda_rec = 10.0 # weight for reconstruction loss
lambda_div = 1.0 # weight for diversity loss
lambda_align = 10.0 # weight for alignment loss

# Define the data loader
transform = transforms.Compose([
  transforms.Resize(image_size),
  transforms.CenterCrop(image_size),
  transforms.ToTensor(),
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
dataset = torchvision.datasets.ImageFolder(root='data', transform=transform)
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define the generator G
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the shared encoder E
    self.encoder = nn.Sequential(
      nn.Conv2d(num_channels, feature_dim // 16, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 16, image_size // 2, image_size // 2]
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 16, feature_dim // 8, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 8, image_size // 4, image_size // 4]
      nn.BatchNorm2d(feature_dim // 8),
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 8, feature_dim // 4, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 4, image_size // 8, image_size // 8]
      nn.BatchNorm2d(feature_dim // 4),
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 4, feature_dim // 2, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 2, image_size // 16, image_size // 16]
      nn.BatchNorm2d(feature_dim // 2),
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 2, feature_dim , kernel_size=4, stride=2, padding=1), # [B, feature_dim , image_size //32 , image_size //32 ]
    )
    # Define the shared decoder D
    self.decoder = nn.Sequential(
      nn.ConvTranspose2d(feature_dim , feature_dim // 2 , kernel_size=4 , stride=2 , padding=1), # [B , feature_dim//2 , image_size//16 , image_size//16 ]
      nn.BatchNorm2d(feature_dim//2 ),
      nn.ReLU(),
      nn.ConvTranspose2d(feature_dim//2 , feature_dim//4 , kernel_size=4 , stride=2 , padding=1), # [B , feature_dim//4 , image_size//8 , image_size//8 ]
      nn.BatchNorm2d(feature_dim//4 ),
      nn.ReLU(),
      nn.ConvTranspose2d(feature_dim//4 , feature_dim//8 , kernel_size=4 , stride=2 , padding=1), # [B , feature_dim//8 , image_size//4 , image_size//4 ]
      nn.BatchNorm2d(feature_dim//8 ),
      nn.ReLU(),
      nn.ConvTranspose2d(feature_dim//8 , feature_dim//16 , kernel_size=4 , stride=2 , padding=1), # [B , feature_dim//16 , image_size//2 , image_size//2 ]
      nn.BatchNorm2d(feature_dim//16 ),
      nn.ReLU(),
      nn.ConvTranspose2d(feature_dim//16 , num_channels , kernel_size=4 , stride=2 , padding=1), # [B , num_channels , image_size , image_size ]
      nn.Tanh()
    )
    # Define the morph layer M
    self.morph = nn.ModuleList([
      MorphLayer(feature_dim) for _ in range(num_domains)
    ])
  
  def forward(self, z, y):
    # Generate a feature map from the latent code
    f = self.encoder(z)
    # Morph the feature map according to the domain label
    f = self.morph[y](f)
    # Decode the feature map to an image
    x = self.decoder(f)
    return x

# Define the morph layer M
class MorphLayer(nn.Module):
  def __init__(self, feature_dim):
    super(MorphLayer, self).__init__()
    # Define the spatial transformer network (STN)
    self.stn = nn.Sequential(
      nn.Linear(feature_dim, feature_dim // 2),
      nn.ReLU(),
      nn.Linear(feature_dim // 2, feature_dim // 4),
      nn.ReLU(),
      nn.Linear(feature_dim // 4, 6)
    )
    # Initialize the STN parameters to identity matrix
    self.stn[-1].weight.data.zero_()
    self.stn[-1].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))
    # Define the channel-wise affine transformation (CAT)
    self.cat = nn.Sequential(
      nn.Linear(feature_dim, feature_dim // 2),
      nn.ReLU(),
      nn.Linear(feature_dim // 2, feature_dim // 4),
      nn.ReLU(),
      nn.Linear(feature_dim // 4, feature_dim * 2)
    )
  
  def forward(self, f):
    # Get the batch size and the spatial size
    B = f.size(0)
    H = f.size(2)
    W = f.size(3)
    # Compute the STN parameters
    theta = self.stn(f.mean(dim=[2, 3])) # [B, 6]
    theta = theta.view(-1, 2, 3) # [B, 2, 3]
    # Compute the grid for STN
    grid = F.affine_grid(theta, f.size()) # [B, H, W, 2]
    # Apply STN to f
    f = F.grid_sample(f, grid) # [B, feature_dim , H , W ]
    # Compute the CAT parameters
    gamma_beta = self.cat(f.mean(dim=[2, 3])) # [B , feature_dim *2 ]
    gamma = gamma_beta[:, :feature_dim] # [B , feature_dim ]
    beta = gamma_beta[:, feature_dim:] # [B , feature_dim ]
    # Apply CAT to f
    f = gamma.view(B , feature_dim ,1 ,1 ) * f + beta.view(B , feature_dim ,1 ,1 ) # [B , feature_dim , H , W ]
    return f

# Define the discriminator D
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Define the discriminator network
    self.network = nn.Sequential(
      nn.Conv2d(num_channels, feature_dim // 16, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 16, image_size // 2, image_size // 2]
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 16, feature_dim // 8, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 8, image_size // 4, image_size // 4]
      nn.BatchNorm2d(feature_dim // 8),
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 8, feature_dim // 4, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 4, image_size // 8, image_size // 8]
      nn.BatchNorm2d(feature_dim // 4),
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim // 4, feature_dim // 2, kernel_size=4, stride=2, padding=1), # [B, feature_dim // 2,image_size//16,image_size//16]
      nn.BatchNorm2d(feature_dim//2 ),
      nn.LeakyReLU(0.2),
      nn.Conv2d(feature_dim//2 ,feature_dim,kernel_size=4,stride=1,padding=0),# [B ,feature_dim,image_size//32-3,image_size//32-3]
      nn.BatchNorm2d(feature_dim ),
      nn.LeakyReLU(0.2),
      nn.Conv2