---
title: 2201.13433v1 Third Time's the Charm? Image and Video Editing with StyleGAN3
date: 2022-01-14
---

# [Third Time's the Charm? Image and Video Editing with StyleGAN3](http://arxiv.org/abs/2201.13433v1)

authors: Yuval Alaluf, Or Patashnik, Zongze Wu, Asif Zamir, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2201.13433 "Third Time's the Charm? Image and Video Editing with StyleGAN3"
[2]: https://arxiv.org/pdf/2201.13433v1 "arXiv.org"
[3]: http://export.arxiv.org/abs/2305.13433v1 "[2305.13433v1] Efficient numerical description of the dynamics of ..."

Here is a summary of the paper:

- **What**: The paper explores the recent StyleGAN3 architecture for image and video editing, and compares it to its predecessor StyleGAN2.
- **Why**: The paper aims to investigate the unique advantages and drawbacks of StyleGAN3, such as its ability to handle unaligned data, its disentanglement properties, and its video editing capabilities.
- **How**: The paper conducts several experiments on different datasets and tasks, such as image generation, inversion, manipulation, and video editing. The paper also proposes a novel encoding scheme for unaligned images and a video inversion and editing workflow that leverages a fine-tuned StyleGAN3 generator.

## Main Contributions

[1]: https://pythonrepo.com/repo/official-implementation-of-third-times-the-charm-image-and-video-editing-with-stylegan-httpsarxivorgabs "Official Implementation of \"Third Time's the Charm? Image and Video ..."
[2]: https://arxiv.org/abs/2201.13433 "Third Time's the Charm? Image and Video Editing with StyleGAN3"
[3]: https://link.springer.com/chapter/10.1007/978-3-031-25063-7_13 "Third Time’s the Charm? Image and Video Editing with StyleGAN3 - Springer"
[4]: https://github.com/yuval-alaluf/stylegan3-editing "Third Time's the Charm? Image and Video Editing with StyleGAN3 (AIM ..."

According to the paper[^1^][2] and its official implementation[^2^][1], some of the contributions of this paper are:

- Demonstrating that StyleGAN3 can be trained on unaligned data without losing the ability to generate aligned imagery.
- Analyzing the disentanglement of the different latent spaces of StyleGAN3 and showing that the StyleSpace is more suitable for fine-grained editing than the W/W+ spaces.
- Proposing an encoding scheme that can invert unaligned images using a StyleGAN3 encoder trained on aligned data.
- Introducing a video inversion and editing workflow that leverages a fine-tuned StyleGAN3 generator to reduce texture sticking and expand the field of view of the edited video.

## Method Summary

[1]: https://arxiv.org/abs/2201.13433 "Third Time's the Charm? Image and Video Editing with StyleGAN3"
[2]: https://github.com/yuval-alaluf/stylegan3-editing "Third Time's the Charm? Image and Video Editing with StyleGAN3 (AIM ..."
[3]: https://www.scribd.com/document/556979563/2201-13433v1 "Third Time'S The Charm? Image and Video Editing With Stylegan3"
[4]: https://link.springer.com/chapter/10.1007/978-3-031-25063-7_13 "Third Time’s the Charm? Image and Video Editing with StyleGAN3 - Springer"

Here is a summary of the method section of the paper[^1^][2]:

- The paper uses the official implementation of StyleGAN3[^2^][4] and trains it on various datasets, such as FFHQ, CelebA-HQ, LSUN Car, and LSUN Cat, using both aligned and unaligned data.
- The paper evaluates the disentanglement of the different latent spaces of StyleGAN3 (Z, W, W+, S) using metrics such as FID, PPL, LPIPS, and MIG. The paper also compares the editability of the W/W+ spaces and the StyleSpace using techniques such as InterFaceGAN and StyleCLIP.
- The paper proposes an encoding scheme that consists of a StyleGAN3 encoder and an optimization-based refinement step. The paper trains the encoder on aligned data and tests it on unaligned images. The paper also compares the encoder with existing methods such as e4e and ReStyle.
- The paper introduces a video inversion and editing workflow that uses a fine-tuned StyleGAN3 generator to invert and manipulate videos. The paper also shows how to use StyleFlow to edit videos in a controllable manner. The paper evaluates the video editing quality using metrics such as FID-Vid and LPIPS-Vid.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Train StyleGAN3 on aligned and unaligned data
stylegan3 = StyleGAN3()
stylegan3.train(aligned_data, unaligned_data)

# Evaluate the disentanglement of the latent spaces
metrics = [FID, PPL, LPIPS, MIG]
for space in [Z, W, W+, S]:
  scores = evaluate_disentanglement(stylegan3, space, metrics)

# Compare the editability of the latent spaces
editing_techniques = [InterFaceGAN, StyleCLIP]
for space in [W, W+, S]:
  edits = apply_editing(stylegan3, space, editing_techniques)

# Propose an encoding scheme for unaligned images
encoder = Encoder()
encoder.train(aligned_data)
for image in unaligned_data:
  code = encoder.encode(image)
  code = refine_code(code, image, stylegan3)
  reconstruction = stylegan3.generate(code)

# Compare the encoder with existing methods
methods = [e4e, ReStyle]
for method in methods:
  reconstructions = method.invert(unaligned_data, stylegan3)
  scores = evaluate_inversion(reconstructions, unaligned_data)

# Introduce a video inversion and editing workflow
finetuner = Finetuner()
stylegan3_finetuned = finetuner.finetune(stylegan3)
for video in videos:
  codes = invert_video(video, stylegan3_finetuned)
  codes = edit_video(codes, stylegan3_finetuned, StyleFlow)
  edited_video = generate_video(codes, stylegan3_finetuned)

# Evaluate the video editing quality
metrics = [FID-Vid, LPIPS-Vid]
scores = evaluate_video_editing(edited_videos, videos, metrics)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import torchvision
import lpips
import stylegan3
import interfacegan
import styleclip
import styleflow

# Define some constants and hyperparameters
BATCH_SIZE = 16
LR = 0.002
NUM_EPOCHS = 1000
NUM_STEPS = 1000
LAMBDA_L2 = 0.01
LAMBDA_ID = 0.5
LAMBDA_LP = 10

# Load the datasets
aligned_data = torchvision.datasets.ImageFolder("aligned_data")
unaligned_data = torchvision.datasets.ImageFolder("unaligned_data")
videos = torchvision.datasets.VideoFolder("videos")

# Train StyleGAN3 on aligned and unaligned data
stylegan3 = StyleGAN3()
stylegan3.train(aligned_data, unaligned_data)

# Evaluate the disentanglement of the latent spaces
metrics = [FID, PPL, LPIPS, MIG]
for space in [Z, W, W+, S]:
  # Sample random codes from the latent space
  codes = stylegan3.sample_codes(space, BATCH_SIZE)
  # Generate images from the codes
  images = stylegan3.generate(codes)
  # Compute the scores for each metric
  scores = {}
  for metric in metrics:
    scores[metric] = metric.compute(images, codes)
  # Print the scores for the latent space
  print(f"Scores for {space}: {scores}")

# Compare the editability of the latent spaces
editing_techniques = [InterFaceGAN, StyleCLIP]
for space in [W, W+, S]:
  # Sample random codes from the latent space
  codes = stylegan3.sample_codes(space, BATCH_SIZE)
  # Generate images from the codes
  images = stylegan3.generate(codes)
  # Apply each editing technique to the codes and images
  for technique in editing_techniques:
    # Define the editing direction and strength
    direction = technique.get_direction(space)
    strength = np.random.uniform(-5, 5)
    # Apply the editing to the codes
    edited_codes = codes + strength * direction
    # Generate edited images from the edited codes
    edited_images = stylegan3.generate(edited_codes)
    # Show the original and edited images side by side
    show_images(images, edited_images)

# Propose an encoding scheme for unaligned images
encoder = Encoder()
# Train the encoder on aligned data using a reconstruction loss and an identity loss
encoder.train(aligned_data, stylegan3, LR, NUM_EPOCHS, LAMBDA_L2, LAMBDA_ID)
for image in unaligned_data:
  # Encode the image using the encoder to get an initial code
  code = encoder.encode(image)
  # Refine the code using an optimization-based approach with a perceptual loss and a regularization loss
  code = refine_code(code, image, stylegan3, LR, NUM_STEPS, LAMBDA_LP, LAMBDA_L2)
  # Generate a reconstruction from the refined code using StyleGAN3
  reconstruction = stylegan3.generate(code)
  # Show the original and reconstructed images side by side
  show_images(image, reconstruction)

# Compare the encoder with existing methods
methods = [e4e, ReStyle]
for method in methods:
  # Invert unaligned images using each method and StyleGAN3 as the generator model
  reconstructions = method.invert(unaligned_data, stylegan3)
  # Evaluate the inversion quality using FID and LPIPS metrics
  scores = {}
  scores["FID"] = FID.compute(unaligned_data, reconstructions)
  scores["LPIPS"] = LPIPS.compute(unaligned_data, reconstructions)
  # Print the scores for each method
  print(f"Scores for {method}: {scores}")

# Introduce a video inversion and editing workflow
finetuner = Finetuner()
# Finetune StyleGAN3 on a specific domain using a reconstruction loss and a perceptual loss
stylegan3_finetuned = finetuner.finetune(stylegan3, domain_data, LR, NUM_EPOCHS, LAMBDA_LP, LAMBDA_L2)
for video in videos:
  # Invert each frame of the video using the encoding scheme proposed above and store the codes in a list
  codes = []
  for frame in video:
    code = encoder.encode(frame)
    code = refine_code(code, frame, stylegan3_finetuned, LR, NUM_STEPS, LAMBDA_LP, LAMBDA_L2)
    codes.append(code)
  # Edit the codes using StyleFlow to control the attributes of the video
  edited_codes = styleflow.edit(codes, attributes, stylegan3_finetuned)
  # Generate an edited video from the edited codes using StyleGAN3
  edited_video = generate_video(edited_codes, stylegan3_finetuned)
  # Show the original and edited videos side by side
  show_videos(video, edited_video)

# Evaluate the video editing quality
metrics = [FID-Vid, LPIPS-Vid]
# Compute the scores for each metric
scores = {}
for metric in metrics:
  scores[metric] = metric.compute(videos, edited_videos)
# Print the scores for the video editing quality
print(f"Scores for video editing: {scores}")
```