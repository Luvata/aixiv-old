---
title: 2212.09597v5 Reasoning with Language Model Prompting  A Survey
date: 2022-12-10
---

# [Reasoning with Language Model Prompting: A Survey](http://arxiv.org/abs/2212.09597v5)

authors: Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen


## What, Why and How

[1]: https://arxiv.org/pdf/2212.09597v5.pdf "Reasoning with Language Model Prompting: A Survey - arXiv.org"
[2]: https://arxiv.org/abs/2212.09597 "[2212.09597] Reasoning with Language Model Prompting: A Survey - arXiv.org"
[3]: http://export.arxiv.org/abs/2212.11565 "[2212.11565] Tune-A-Video: One-Shot Tuning of Image Diffusion Models ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper is a survey of recent progress in reasoning with language model prompting, which is a technique to enhance the reasoning abilities of pre-trained language models by providing them with suitable prompts (e.g., examples, knowledge, questions) as input.
- **Why**: The paper aims to provide a comprehensive overview of the state-of-the-art research on this topic, as well as to discuss the potential reasons for the emergence of such reasoning abilities and to highlight future research directions.
- **How**: The paper organizes the relevant works by taxonomy, based on the types of reasoning tasks (e.g., arithmetic, commonsense, symbolic) and the types of prompting strategies (e.g., chain-of-thought, generated knowledge). The paper also provides in-depth comparisons and insights for each category, as well as some open resources and challenges for beginners.

## Main Contributions

[1]: https://arxiv.org/pdf/2212.09597v5.pdf "Reasoning with Language Model Prompting: A Survey - arXiv.org"
[2]: https://arxiv.org/abs/2212.09597 "[2212.09597] Reasoning with Language Model Prompting: A Survey - arXiv.org"
[3]: http://export.arxiv.org/abs/2212.11565 "[2212.11565] Tune-A-Video: One-Shot Tuning of Image Diffusion Models ..."

The paper claims to make the following contributions[^1^][1]:

- It is the first survey of recent progress in reasoning with language model prompting, covering various types of reasoning tasks and prompting strategies.
- It provides a comprehensive overview of the existing works with comparisons and summaries, as well as insights and discussions for each category.
- It offers systematic resources, such as datasets, code, and benchmarks, to help beginners get started with this field.
- It discusses the potential reasons for the emergence of such reasoning abilities in language models and highlights some future research directions and challenges.

## Method Summary

[1]: https://arxiv.org/pdf/2212.09597v5.pdf "Reasoning with Language Model Prompting: A Survey - arXiv.org"
[2]: https://arxiv.org/abs/2212.09597 "[2212.09597] Reasoning with Language Model Prompting: A Survey - arXiv.org"
[3]: http://export.arxiv.org/abs/2212.11565 "[2212.11565] Tune-A-Video: One-Shot Tuning of Image Diffusion Models ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a taxonomy of reasoning tasks and prompting strategies, based on the types of reasoning (e.g., arithmetic, commonsense, symbolic) and the types of prompts (e.g., in-context exemplars, knowledge, questions).
- The paper reviews the existing works on reasoning with language model prompting according to the taxonomy, and provides comparisons and summaries for each category.
- The paper also discusses some insights and challenges for each category, such as the effectiveness, efficiency, and generalization of different prompting strategies.

## Pseudo Code - High level

This paper does not propose a specific algorithm or model, but rather a survey of existing works on reasoning with language model prompting. Therefore, there is no high-level pseudo code for this paper. However, a possible pseudo code for the general framework of reasoning with language model prompting is:

```python
# Define the reasoning task and the input
task = "arithmetic" # or "commonsense", "symbolic", etc.
input = "2 + 3 = ?" # or any other natural language input

# Choose a pre-trained language model
model = GPT-3 # or any other large-scale language model

# Choose a prompting strategy and generate a prompt
strategy = "chain-of-thought" # or "generated knowledge", "question answering", etc.
prompt = generate_prompt(input, strategy) # a function that generates a suitable prompt based on the input and the strategy

# Feed the prompt to the language model and get the output
output = model(prompt)

# Evaluate the output and optionally refine the prompt
score = evaluate(output, task) # a function that evaluates the output based on the task
if score < threshold: # a predefined threshold for acceptable performance
  prompt = refine_prompt(prompt, output, strategy) # a function that refines the prompt based on the output and the strategy
  output = model(prompt)
  score = evaluate(output, task)

# Return the output and the score
return output, score
```

## Pseudo Code - Detail

As I mentioned before, this paper does not propose a specific algorithm or model, but rather a survey of existing works on reasoning with language model prompting. Therefore, there is no detailed pseudo code to implement this paper. However, if you are interested in implementing some of the works that are reviewed in this paper, you can refer to the resources section (Â§5) of the paper, where the authors provide links to datasets, code, and benchmarks for various reasoning tasks and prompting strategies. You can also check the references section (Â§7) of the paper, where the authors cite the original papers that propose different methods and models for reasoning with language model prompting. I hope this helps.ðŸ˜Š