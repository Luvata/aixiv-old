---
title: 2306.07596v1 Paste, Inpaint and Harmonize via Denoising  Subject-Driven Image Editing with Pre-Trained Diffusion Model
date: 2023-06-08
---

# [Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model](http://arxiv.org/abs/2306.07596v1)

authors: Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa


## What, Why and How

[1]: https://arxiv.org/abs/2306.07596 "[2306.07596] Paste, Inpaint and Harmonize via Denoising ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.04596v1.pdf "arXiv:2306.04596v1 [math.NA] 7 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.07596 "[2306.07596] Paste, Inpaint and Harmonize via Denoising: Subject-Driven ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a new framework called **Paste, Inpaint and Harmonize via Denoising (PhD)**, which leverages an exemplar image in addition to text descriptions to specify user intentions for image editing and generation.
- **Why**: The paper aims to address the limitations of text-to-image generative models, which often compromise the subjects' identity or require additional per-subject fine-tuning when using text descriptions alone.
- **How**: The paper proposes a three-step process: pasting, inpainting and harmonizing. In the pasting step, an off-the-shelf segmentation model is employed to identify a user-specified subject within an exemplar image which is subsequently inserted into a background image to serve as an initialization. In the inpainting step, a pre-trained diffusion model is guided by text descriptions to fill in the missing regions around the inserted subject. In the harmonizing step, the same diffusion model is used to blend the inserted subject into the scene naturally. The paper demonstrates that this approach achieves state-of-the-art performance in both subject-driven image editing and text-driven scene generation tasks.


## Main Contributions

[1]: https://arxiv.org/abs/2306.07596 "[2306.07596] Paste, Inpaint and Harmonize via Denoising ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.04596v1.pdf "arXiv:2306.04596v1 [math.NA] 7 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.07596 "[2306.07596] Paste, Inpaint and Harmonize via Denoising: Subject-Driven ..."

According to the paper[^1^][1], the main contributions are:

- **A novel framework for subject-driven image editing and generation**: The paper proposes PhD, which leverages an exemplar image in addition to text descriptions to specify user intentions for image editing and generation. PhD can handle diverse texts and preserve the identity of the subjects.
- **A three-step process for image editing and generation**: The paper introduces a pasting, inpainting and harmonizing module that uses an off-the-shelf segmentation model and a pre-trained diffusion model to create realistic and coherent images from text and exemplar images.
- **State-of-the-art performance in both tasks**: The paper demonstrates that PhD outperforms existing methods in both subject-driven image editing and text-driven scene generation tasks, in terms of quantitative metrics and qualitative evaluations. The paper also provides extensive ablation studies and comparisons with baseline methods.

## Method Summary

[1]: https://arxiv.org/abs/2306.07596 "[2306.07596] Paste, Inpaint and Harmonize via Denoising ... - arXiv.org"
[2]: https://arxiv.org/pdf/2306.04596v1.pdf "arXiv:2306.04596v1 [math.NA] 7 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.07596 "[2306.07596] Paste, Inpaint and Harmonize via Denoising: Subject-Driven ..."

Here is a summary of the method section of the paper[^1^][1]:

- **Pasting**: The paper uses an off-the-shelf segmentation model called DeepLabV3+ [^2^][2] to extract a user-specified subject from an exemplar image. The subject is then pasted into a background image at a random location and scale. The paper also applies a Gaussian blur to the boundary of the subject to reduce the sharpness. The paper argues that this step provides an initialization that captures both the scene context and the subject identity in one image.
- **Inpainting**: The paper uses a pre-trained diffusion model called Text2Diffusion [^3^][3] to inpaint the missing regions around the pasted subject. The paper feeds the text descriptions and the pasted image as inputs to the diffusion model and samples from its reverse process to generate realistic images. The paper also uses a mask to indicate which regions need to be inpainted and which regions should be preserved. The paper claims that this step allows flexible editing with diverse texts and preserves the identity of the subjects.
- **Harmonizing**: The paper uses the same diffusion model as in the inpainting step to harmonize the pasted subject with the background image. The paper feeds the text descriptions and the inpainted image as inputs to the diffusion model and samples from its reverse process to generate coherent images. The paper also uses a mask to indicate which regions need to be harmonized and which regions should be preserved. The paper asserts that this step ensures the visual coherence of the generated or edited image and seamlessly blends the inserted subject into the scene naturally.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: text descriptions T, exemplar image I_e, background image I_b
# Output: edited image I_o

# Pasting
S = segment(I_e) # use DeepLabV3+ to segment the subject from the exemplar image
I_p = paste(S, I_b) # paste the subject into the background image at a random location and scale
I_p = blur(I_p) # apply Gaussian blur to the boundary of the subject

# Inpainting
M_i = create_mask(I_p) # create a mask to indicate which regions need to be inpainted
I_i = inpaint(I_p, T, M_i) # use Text2Diffusion to inpaint the missing regions with text guidance

# Harmonizing
M_h = create_mask(I_i) # create a mask to indicate which regions need to be harmonized
I_o = harmonize(I_i, T, M_h) # use Text2Diffusion to harmonize the pasted subject with the background with text guidance

# Return the edited image
return I_o
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: text descriptions T, exemplar image I_e, background image I_b
# Output: edited image I_o

# Pasting
S = segment(I_e) # use DeepLabV3+ to segment the subject from the exemplar image
# S is a binary mask with 1 for the subject and 0 for the background
x, y = random_location(I_b) # choose a random location in the background image
s = random_scale(S) # choose a random scale factor for the subject
S = resize(S, s) # resize the subject mask according to the scale factor
I_p = copy_and_paste(S, I_e, I_b, x, y) # copy and paste the subject pixels from the exemplar image to the background image according to the location and mask
I_p = blur(I_p) # apply Gaussian blur to the boundary of the subject

# Inpainting
M_i = create_mask(I_p) # create a mask to indicate which regions need to be inpainted
# M_i is a binary mask with 1 for the missing regions and 0 for the preserved regions
I_i = inpaint(I_p, T, M_i) # use Text2Diffusion to inpaint the missing regions with text guidance
# Text2Diffusion is a pre-trained diffusion model that takes text and image as inputs and outputs a realistic image
# The inpainting process is as follows:
# 1. Encode the text T into a latent vector z using a pre-trained CLIP model
# 2. Apply noise to the image I_p according to a noise schedule and the mask M_i
# 3. Use a U-Net model to predict the denoising distribution of each pixel given the noisy image and z
# 4. Sample from the denoising distribution to obtain a denoised image
# 5. Repeat steps 2-4 in reverse order until reaching the original noise level
# 6. Return the final denoised image as I_i

# Harmonizing
M_h = create_mask(I_i) # create a mask to indicate which regions need to be harmonized
# M_h is a binary mask with 1 for the pasted subject and 0 for the background
I_o = harmonize(I_i, T, M_h) # use Text2Diffusion to harmonize the pasted subject with the background with text guidance
# The harmonizing process is similar to the inpainting process, except that it uses a different noise schedule and mask M_h

# Return the edited image
return I_o
```