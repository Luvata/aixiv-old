---
title: 2001.08740v2 Audiovisual SlowFast Networks for Video Recognition
date: 2020-01-09
---

# [Audiovisual SlowFast Networks for Video Recognition](http://arxiv.org/abs/2001.08740v2)

authors: Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, Christoph Feichtenhofer


## What, Why and How

[1]: https://arxiv.org/pdf/2001.08740v2.pdf "Abstract arXiv:2001.08740v2 [cs.CV] 9 Mar 2020"
[2]: https://arxiv.org/abs/2001.08740 "[2001.08740] Audiovisual SlowFast Networks for Video Recognition"
[3]: http://export.arxiv.org/abs/2201.08740v2 "[2201.08740v2] H\\\"ormander-Mikhlin criteria on Lie group von Neumann ..."

Here is a summary of the paper:

- **What**: The paper presents Audiovisual SlowFast Networks, an architecture for integrated audiovisual perception. AVSlowFast has Slow and Fast visual pathways that are deeply integrated with a Faster Audio pathway to model vision and sound in a unified representation[^1^][1].
- **Why**: The paper aims to go beyond previous work that performs "late-fusion" of independent audio and visual pathways, to instead learn hierarchical audiovisual concepts that are influenced by both modalities. The paper also aims to overcome training difficulties that arise from different learning dynamics for audio and visual modalities, and to show the generalization of AVSlowFast to learn self-supervised audiovisual features[^1^][1].
- **How**: The paper introduces DropPathway, which randomly drops the Audio pathway during training as an effective regularization technique. The paper also performs hierarchical audiovisual synchronization to learn joint audiovisual features. The paper reports state-of-the-art results on six video action classification and detection datasets, performs detailed ablation studies, and shows the generalization of AVSlowFast to learn self-supervised audiovisual features[^1^][1].

## Main Contributions

The paper claims the following contributions:

- It proposes Audiovisual SlowFast Networks, an architecture for integrated audiovisual perception that fuses audio and visual features at multiple layers.
- It introduces DropPathway, a regularization technique that randomly drops the Audio pathway during training to balance the learning dynamics of audio and visual modalities.
- It performs hierarchical audiovisual synchronization to learn joint audiovisual features that are aligned across different temporal resolutions.
- It reports state-of-the-art results on six video action classification and detection datasets, outperforming previous methods that use only visual or late-fusion of audio and visual modalities.
- It shows the generalization of AVSlowFast to learn self-supervised audiovisual features that can be transferred to downstream tasks.

## Method Summary

[1]: https://arxiv.org/pdf/2001.08740v2.pdf "Abstract arXiv:2001.08740v2 [cs.CV] 9 Mar 2020"
[2]: https://arxiv.org/abs/2001.08740 "[2001.08740] Audiovisual SlowFast Networks for Video Recognition"
[3]: http://export.arxiv.org/abs/2201.08740v2 "[2201.08740v2] H\\\"ormander-Mikhlin criteria on Lie group von Neumann ..."

Here is a summary of the method section of the paper:

- The paper builds on the SlowFast Networks [16], which have two visual pathways: a Slow pathway that operates at a low frame rate and captures high-level semantics, and a Fast pathway that operates at a high frame rate and captures motion details. The paper adds a Faster Audio pathway that operates at an even higher sampling rate and captures sound information.
- The paper fuses audio and visual features at multiple layers using lateral connections, enabling audio to contribute to the formation of hierarchical audiovisual concepts. The paper also uses channel-wise attention to modulate the feature fusion based on the input signals.
- The paper introduces DropPathway, which randomly drops the Audio pathway during training with a certain probability. This technique helps to balance the learning dynamics of audio and visual modalities, and prevents overfitting to audio or visual signals alone.
- The paper performs hierarchical audiovisual synchronization to learn joint audiovisual features that are aligned across different temporal resolutions. The paper uses contrastive learning to maximize the agreement between audiovisual features from the same video clip, and minimize the agreement between features from different clips.
- The paper evaluates the proposed method on six video action classification and detection datasets: Kinetics-400 [30], Kinetics-600 [30], Kinetics-700 [31], AVA [21], Charades [56], and Epic-Kitchens [14]. The paper also shows the generalization of AVSlowFast to learn self-supervised audiovisual features that can be transferred to downstream tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Slow, Fast and Audio pathways
slow = ResNet50(input=low_frame_rate_video)
fast = ResNet50(input=high_frame_rate_video)
audio = ResNet18(input=high_sampling_rate_audio)

# Fuse audio and visual features at multiple layers using lateral connections and channel-wise attention
for i in [1, 2, 3, 4]:
  slow[i] = slow[i] + lateral_connection(fast[i], audio[i])
  fast[i] = fast[i] + lateral_connection(slow[i], audio[i])
  audio[i] = audio[i] + lateral_connection(slow[i], fast[i])
  slow[i], fast[i], audio[i] = channel_wise_attention(slow[i], fast[i], audio[i])

# Randomly drop the Audio pathway during training with a certain probability
if random() < drop_probability:
  audio = None

# Perform hierarchical audiovisual synchronization to learn joint audiovisual features
slow_features, fast_features, audio_features = global_pooling(slow, fast, audio)
slow_features, fast_features, audio_features = projection_head(slow_features, fast_features, audio_features)
av_features = concatenate(slow_features, fast_features, audio_features)
loss = contrastive_loss(av_features)

# Evaluate the method on video action classification and detection datasets
predictions = classifier(av_features)
metrics = evaluate(predictions, ground_truth)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import torchaudio
import numpy as np

# Define the hyperparameters
slow_frame_rate = 4 # frames per second for the Slow pathway
fast_frame_rate = 32 # frames per second for the Fast pathway
audio_sampling_rate = 44100 # samples per second for the Audio pathway
beta = 8 # the ratio between the Fast and Slow pathways
alpha = 4 # the ratio between the Audio and Slow pathways
drop_probability = 0.5 # the probability of dropping the Audio pathway during training
temperature = 0.07 # the temperature parameter for contrastive loss
batch_size = 32 # the batch size for training and evaluation
num_epochs = 100 # the number of epochs for training
learning_rate = 0.01 # the learning rate for training

# Define the lateral connection function that reduces the temporal resolution and increases the channel dimension of a feature map
def lateral_connection(x, y):
  x = torch.nn.AvgPool3d(kernel_size=(x.shape[2] // y.shape[2], 1, 1), stride=(x.shape[2] // y.shape[2], 1, 1))(x) # temporal pooling
  x = torch.nn.Conv3d(x.shape[1], y.shape[1], kernel_size=1, stride=1)(x) # channel projection
  x = torch.nn.BatchNorm3d(y.shape[1])(x) # batch normalization
  x = torch.nn.ReLU()(x) # activation function
  return x

# Define the channel-wise attention function that modulates the feature fusion based on the input signals
def channel_wise_attention(x, y, z):
  x_weight = torch.mean(x * y * z, dim=(2, 3, 4), keepdim=True) # compute the channel-wise weight for x
  y_weight = torch.mean(x * y * z, dim=(2, 3, 4), keepdim=True) # compute the channel-wise weight for y
  z_weight = torch.mean(x * y * z, dim=(2, 3, 4), keepdim=True) # compute the channel-wise weight for z
  x = x * x_weight # modulate x by its weight
  y = y * y_weight # modulate y by its weight
  z = z * z_weight # modulate z by its weight
  return x, y, z

# Define the global pooling function that reduces the spatial and temporal dimensions of a feature map to a single vector
def global_pooling(x):
  if x is not None:
    x = torch.mean(x, dim=(2, 3, 4)) # average pooling over space and time
    x = torch.nn.ReLU()(x) # activation function
  return x

# Define the projection head function that maps a feature vector to a lower-dimensional embedding space
def projection_head(x):
  if x is not None:
    x = torch.nn.Linear(x.shape[1], x.shape[1] // 4)(x) # linear projection
    x = torch.nn.BatchNorm1d(x.shape[1] // 4)(x) # batch normalization
    x = torch.nn.ReLU()(x) # activation function
    x = torch.nn.Linear(x.shape[1], x.shape[1] // 4)(x) # linear projection
    x = torch.nn.BatchNorm1d(x.shape[1] // 4)(x) # batch normalization
    x = torch.nn.ReLU()(x) # activation function
    x = torch.nn.Linear(x.shape[1], x.shape[1] // 4)(x) # linear projection
    x = torch.nn.BatchNorm1d(x.shape[1] // 4)(x) # batch normalization
    x = torch.nn.ReLU()(x) # activation function
    return x

# Define the contrastive loss function that maximizes the agreement between audiovisual features from the same video clip and minimizes the agreement between features from different clips 
def contrastive_loss(av_features):
  av_features = torch.nn.functional.normalize(av_features, dim=1) # normalize the features to unit length
  similarity_matrix = torch.matmul(av_features, av_features.t()) / temperature # compute the cosine similarity matrix scaled by temperature 
  mask_matrix = torch.eye(batch_size).to(device) # create a mask matrix that has ones on the diagonal and zeros elsewhere 
  positive_matrix = similarity_matrix * mask_matrix # extract the positive similarities (i.e., from same clip)
  negative_matrix = similarity_matrix * (1 - mask_matrix) # extract the negative similarities (i.e., from different clips)
  positive_logsumexp = torch.logsumexp(positive_matrix, dim=1) # compute the log-sum-exp of the positive similarities
  negative_logsumexp = torch.logsumexp(negative_matrix, dim=1) # compute the log-sum-exp of the negative similarities
  loss = torch.mean(- positive_logsumexp + negative_logsumexp) # compute the average negative log-likelihood
  return loss

# Define the classifier function that predicts the action class from the audiovisual features
def classifier(av_features):
  av_features = torch.nn.Linear(av_features.shape[1], num_classes)(av_features) # linear projection to class logits
  av_features = torch.nn.Softmax(dim=1)(av_features) # softmax activation to class probabilities
  return av_features

# Define the evaluation function that computes the accuracy and other metrics from the predictions and ground truth labels
def evaluate(predictions, ground_truth):
  accuracy = torch.sum(predictions.argmax(dim=1) == ground_truth) / batch_size # compute the accuracy
  # compute other metrics as needed
  return accuracy

# Load the video and audio data
video_data = load_video_data() # a tensor of shape (num_samples, num_frames, height, width, channels)
audio_data = load_audio_data() # a tensor of shape (num_samples, num_samples)
labels = load_labels() # a tensor of shape (num_samples)

# Split the data into training and validation sets
train_video_data, val_video_data = split(video_data)
train_audio_data, val_audio_data = split(audio_data)
train_labels, val_labels = split(labels)

# Create data loaders for training and validation sets
train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_video_data, train_audio_data, train_labels), batch_size=batch_size, shuffle=True)
val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(val_video_data, val_audio_data, val_labels), batch_size=batch_size, shuffle=False)

# Define the Slow, Fast and Audio pathways using ResNet models
slow = torchvision.models.resnet50(pretrained=True)
fast = torchvision.models.resnet50(pretrained=True)
audio = torchvision.models.resnet18(pretrained=True)

# Modify the first convolution layer of each pathway to adapt to the input shape
slow.conv1 = torch.nn.Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)
fast.conv1 = torch.nn.Conv3d(3, 8 * beta, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)
audio.conv1 = torch.nn.Conv2d(1, 64 // alpha, kernel_size=7, stride=2, padding=3, bias=False)

# Move the models to the device (CPU or GPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
slow.to(device)
fast.to(device)
audio.to(device)

# Define the optimizer and scheduler for training
optimizer = torch.optim.SGD(list(slow.parameters()) + list(fast.parameters()) + list(audio.parameters()), lr=learning_rate, momentum=0.9, weight_decay=0.0001)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# Train the models for a fixed number of epochs
for epoch in range(num_epochs):
  
  # Set the models to training mode
  slow.train()
  fast.train()
  audio.train()

  # Loop over the training batches
  for i, (video_batch, audio_batch, label_batch) in enumerate(train_loader):

    # Move the batches to the device
    video_batch = video_batch.to(device)
    audio_batch = audio_batch.to(device)
    label_batch = label_batch.to(device)

    # Extract the low frame rate video for the Slow pathway
    slow_video_batch = video_batch[:, ::slow_frame_rate // fast_frame_rate]

    # Extract the log-mel spectrogram for the Audio pathway
    audio_batch = torchaudio.transforms.MelSpectrogram(sample_rate=audio_sampling_rate)(audio_batch.unsqueeze(1))
    audio_batch = torchaudio.transforms.AmplitudeToDB()(audio_batch)

    # Forward pass the batches through the pathways
    slow_output = slow(slow_video_batch.permute(0, 4, 1, 2 ,3))
    fast_output = fast(video_batch.permute(0 ,4 ,1 ,2 ,3))
    audio_output = audio(audio_batch)

    # Fuse audio and visual features at multiple layers using lateral connections and channel-wise attention
    for i in [1 ,2 ,3 ,4]:
      slow