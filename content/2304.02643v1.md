---
title: 2304.02643v1 Segment Anything
date: 2023-04-03
---

# [Segment Anything](http://arxiv.org/abs/2304.02643v1)

authors: Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll√°r, Ross Girshick


## What, Why and How

[1]: https://arxiv.org/pdf/2304.02643v1.pdf "arXiv:2304.02643v1 [cs.CV] 5 Apr 2023"
[2]: https://arxiv.org/abs/2304.02643 "[2304.02643] Segment Anything - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/abs/2304.02643 "[2304.02643] Segment Anything - Cornell University"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces the Segment Anything (SA) project, which consists of a new task, model, and dataset for image segmentation.
- **Why**: The paper aims to build a foundation model for computer vision that can generalize to new image distributions and tasks via prompt engineering, similar to large language models in NLP.
- **How**: The paper proposes a promptable segmentation task, where a text prompt is used to specify the desired segmentation output. The paper also presents a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering. The paper further describes a data engine for collecting SA-1B, a dataset of over 1 billion masks on 11 million licensed and privacy respecting images. The paper evaluates the model's zero-shot performance on numerous tasks and compares it with prior fully supervised results.

## Main Contributions

According to the paper at , the main contributions are:

- The introduction of a new task, promptable segmentation, that enables zero-shot generalization to novel visual concepts and data distributions via text prompts.
- The design and training of a segmentation model (SAM) that is efficient, scalable, and promptable, and that can be used in a data collection loop to annotate large-scale segmentation datasets.
- The creation and release of the largest segmentation dataset to date (by far), SA-1B, with over 1 billion masks on 11 million licensed and privacy respecting images.
- The evaluation of SAM's zero-shot performance on numerous tasks and datasets, showing that it is often competitive with or even superior to prior fully supervised results.

## Method Summary

The method section of the paper at  consists of three subsections: promptable segmentation task, segmentation model (SAM), and data engine.

- Promptable segmentation task: The paper defines a new task where a text prompt is used to specify the desired segmentation output. The text prompt can be a natural language description, a keyword, or a combination of both. The paper also introduces a validity mask that indicates which pixels in the image are relevant for the prompt. The paper argues that this task is more flexible and generalizable than conventional segmentation tasks that rely on predefined categories or labels.
- Segmentation model (SAM): The paper presents a model architecture that consists of an image encoder, a prompt encoder, and a lightweight mask decoder. The image encoder extracts features from the input image, the prompt encoder embeds the text prompt into a latent space, and the mask decoder predicts a segmentation mask based on the image features and the prompt embedding. The paper also describes the training procedure and the loss function of the model, which uses contrastive learning and cross-entropy loss.
- Data engine: The paper describes a data engine that leverages SAM to annotate large-scale segmentation datasets. The data engine consists of three components: a data source, a prompt generator, and a mask validator. The data source provides images from various domains and sources, the prompt generator produces text prompts for each image based on metadata or heuristics, and the mask validator filters out invalid or low-quality masks produced by SAM. The paper also details the data collection process and the resulting dataset, SA-1B.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the promptable segmentation task
def promptable_segmentation(image, prompt, valid_mask):
  # Encode the image and the prompt
  image_features = image_encoder(image)
  prompt_embedding = prompt_encoder(prompt)
  # Decode the segmentation mask
  mask = mask_decoder(image_features, prompt_embedding)
  # Apply the validity mask
  mask = mask * valid_mask
  # Return the segmentation mask
  return mask

# Define the segmentation model (SAM)
class SAM(nn.Module):
  def __init__(self):
    # Initialize the image encoder, the prompt encoder, and the mask decoder
    self.image_encoder = ResNet50()
    self.prompt_encoder = BERT()
    self.mask_decoder = ConvTranspose2d()
  
  def forward(self, image, prompt):
    # Encode the image and the prompt
    image_features = self.image_encoder(image)
    prompt_embedding = self.prompt_encoder(prompt)
    # Decode the segmentation mask
    mask = self.mask_decoder(image_features, prompt_embedding)
    # Return the segmentation mask
    return mask

# Define the data engine
class DataEngine():
  def __init__(self):
    # Initialize the data source, the prompt generator, and the mask validator
    self.data_source = ImageNet()
    self.prompt_generator = PromptGenerator()
    self.mask_validator = MaskValidator()
  
  def collect_data(self):
    # Initialize an empty dataset
    dataset = []
    # Loop over the images from the data source
    for image in self.data_source:
      # Generate a text prompt for the image
      prompt = self.prompt_generator(image)
      # Predict a segmentation mask using SAM
      mask = SAM(image, prompt)
      # Validate the segmentation mask
      if self.mask_validator(mask):
        # Add the image, prompt, and mask to the dataset
        dataset.append((image, prompt, mask))
    # Return the dataset
    return dataset
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the promptable segmentation task
def promptable_segmentation(image, prompt, valid_mask):
  # Encode the image and the prompt
  image_features = image_encoder(image) # shape: (batch_size, 2048, h/32, w/32)
  prompt_embedding = prompt_encoder(prompt) # shape: (batch_size, 768)
  # Decode the segmentation mask
  mask = mask_decoder(image_features, prompt_embedding) # shape: (batch_size, 1, h, w)
  # Apply the validity mask
  mask = mask * valid_mask # shape: (batch_size, 1, h, w)
  # Return the segmentation mask
  return mask

# Define the segmentation model (SAM)
class SAM(nn.Module):
  def __init__(self):
    # Initialize the image encoder, the prompt encoder, and the mask decoder
    self.image_encoder = torchvision.models.resnet50(pretrained=True)
    self.prompt_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
    self.mask_decoder = nn.Sequential(
      nn.ConvTranspose2d(2816, 1024, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(1024),
      nn.ReLU(),
      nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(512),
      nn.ReLU(),
      nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(256),
      nn.ReLU(),
      nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(128),
      nn.ReLU(),
      nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
      nn.BatchNorm2d(64),
      nn.ReLU(),
      nn.ConvTranspose2d(64, 1, kernel_size