---
title: 2006.09011v2 Improved Techniques for Training Score-Based Generative Models
date: 2020-06-10
---

# [Improved Techniques for Training Score-Based Generative Models](http://arxiv.org/abs/2006.09011v2)

authors: Yang Song, Stefano Ermon


## What, Why and How

[1]: https://arxiv.org/pdf/2006.09011v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2006.09011 "Improved Techniques for Training Score-Based Generative Models"
[3]: http://export.arxiv.org/abs/2006.09011v2 "[2006.09011v2] Improved Techniques for Training Score-Based Generative ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes improved techniques for training score-based generative models, which are models that learn a score function that points to the direction of higher likelihood for data.
- **Why**: Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution and can be unstable under some settings. The paper aims to overcome these limitations and scale score-based models to high resolution images.
- **How**: The paper provides a new theoretical analysis of learning and sampling from score-based models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. The paper also proposes to maintain an exponential moving average of model weights to enhance stability. The paper demonstrates that with these improvements, score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets.

## Main Contributions

The paper claims to make the following contributions:

- A method to analytically compute an effective set of Gaussian noise scales from training data, based on a simplified mixture model.
- An efficient architecture to amortize the score estimation task across a large number of noise scales with a single neural network.
- A technique to approximately optimize the performance of the Langevin dynamics sampling procedure as a function of the noise scales, based on a simplified analysis of its convergence properties.
- A demonstration that score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, with resolutions ranging from 64x64 to 256x256.

## Method Summary

[1]: https://arxiv.org/pdf/2006.09011v2 "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2006.09011 "Improved Techniques for Training Score-Based Generative Models"
[3]: http://export.arxiv.org/abs/2006.09011 "[2006.09011] Improved Techniques for Training Score-Based Generative Models"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first reviews the background of score-based generative models, which learn a score function that estimates the gradient of the log-density of data. The paper also reviews the denoising score matching objective, which trains the score function by minimizing the squared difference between its output and the true score at noisy data points. The paper then introduces the annealed Langevin dynamics algorithm, which generates samples by starting from white noise and applying a sequence of gradient ascent steps on the log-density using the score function at different noise levels.
- The paper then presents its main contributions, which are three techniques to improve the training and sampling of score-based models. The first technique is to analytically compute an effective set of noise scales from training data, based on a simplified mixture model assumption. The paper shows that this technique can avoid degenerate solutions and achieve better performance than existing heuristics. The second technique is to design an efficient architecture that can amortize the score estimation task across a large number of noise scales with a single neural network. The paper shows that this technique can reduce the memory and computation cost of training and sampling, and also improve the quality of samples. The third technique is to approximately optimize the performance of the annealed Langevin dynamics algorithm as a function of the noise scales, based on a simplified analysis of its convergence properties. The paper shows that this technique can speed up the sampling process and reduce the variance of samples.
- The paper then provides some theoretical analysis to support its proposed techniques. The paper derives an analytical formula for computing the optimal noise scales for a mixture model, and shows that it can be applied to real data by estimating some statistics from training data. The paper also analyzes the convergence rate and variance of annealed Langevin dynamics, and shows that it can be improved by choosing appropriate noise scales and step sizes. The paper also discusses some practical issues and extensions of its techniques, such as how to handle discrete data, how to use exponential moving average of model weights, and how to incorporate data augmentation.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a score network that takes an image and a noise scale as input and outputs a score vector
score_network = ScoreNetwork()

# Define a set of noise scales based on the data statistics
noise_scales = compute_noise_scales(data)

# Train the score network using denoising score matching
for epoch in epochs:
  for batch in data:
    # Sample a noise scale from the set
    sigma = sample(noise_scales)
    # Add Gaussian noise to the data
    noisy_batch = batch + sigma * normal(0, 1, size=batch.shape)
    # Compute the true score using finite difference
    true_score = (batch - noisy_batch) / sigma**2
    # Compute the predicted score using the score network
    pred_score = score_network(noisy_batch, sigma)
    # Compute the loss as the mean squared error
    loss = mse(true_score, pred_score)
    # Update the score network parameters using gradient descent
    score_network.update(loss)

# Use exponential moving average of model weights for stability
score_network = ema(score_network)

# Generate samples using annealed Langevin dynamics
for i in range(num_samples):
  # Start from white noise
  sample = normal(0, 1, size=image_size)
  # Loop over the noise scales in reverse order
  for sigma in reversed(noise_scales):
    # Compute the step size based on the noise scale
    step_size = compute_step_size(sigma)
    # Run Langevin dynamics for some iterations
    for t in range(num_iterations):
      # Compute the score using the score network
      score = score_network(sample, sigma)
      # Update the sample using gradient ascent and Gaussian noise
      sample = sample + step_size * score + normal(0, sigma, size=image_size)
  # Save the final sample
  save(sample)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import some libraries
import numpy as np
import torch
import torchvision

# Define some hyperparameters
batch_size = 64 # The size of each mini-batch
num_epochs = 100 # The number of epochs to train
num_samples = 100 # The number of samples to generate
num_iterations = 100 # The number of Langevin dynamics iterations per noise scale
num_scales = 10 # The number of noise scales to use
alpha = 0.999 # The decay rate for exponential moving average
beta1 = 0.9 # The beta1 parameter for Adam optimizer
beta2 = 0.999 # The beta2 parameter for Adam optimizer
lr = 1e-4 # The learning rate for Adam optimizer
eps = 1e-8 # The epsilon parameter for Adam optimizer

# Define a function to compute the noise scales from data statistics
def compute_noise_scales(data):
  # Compute the mean and standard deviation of the data
  mean = data.mean()
  std = data.std()
  # Compute the maximum and minimum noise scales
  max_scale = std / 10
  min_scale = std / 1000
  # Compute the noise scales as a geometric sequence
  noise_scales = np.geomspace(max_scale, min_scale, num_scales)
  # Return the noise scales as a torch tensor
  return torch.tensor(noise_scales)

# Define a function to compute the step size for Langevin dynamics based on the noise scale
def compute_step_size(sigma):
  # Use a heuristic formula from the paper
  step_size = sigma**2 / (sigma**2 + lr)
  # Return the step size as a torch scalar
  return torch.scalar(step_size)

# Define a score network that takes an image and a noise scale as input and outputs a score vector
class ScoreNetwork(torch.nn.Module):
  def __init__(self):
    super(ScoreNetwork, self).__init__()
    # Use a U-Net architecture with residual blocks and skip connections
    self.encoder = Encoder() # A function that encodes an image into a latent representation
    self.decoder = Decoder() # A function that decodes a latent representation into a score vector

  def forward(self, x, sigma):
    # Concatenate the image and the noise scale along the channel dimension
    x = torch.cat([x, sigma * torch.ones_like(x[:, :1])], dim=1)
    # Encode the image into a latent representation
    z, skips = self.encoder(x)
    # Decode the latent representation into a score vector using skip connections
    score = self.decoder(z, skips)
    # Return the score vector
    return score

# Define an encoder that encodes an image into a latent representation using residual blocks and skip connections
class Encoder(torch.nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Define some convolutional layers with residual blocks and downsampling
    self.conv1 = ConvBlock(4, 64) # A function that applies a convolutional layer with residual block and activation function
    self.conv2 = ConvBlock(64, 128) 
    self.conv3 = ConvBlock(128, 256) 
    self.conv4 = ConvBlock(256, 512) 
    self.conv5 = ConvBlock(512, 512) 
    self.conv6 = ConvBlock(512, 512) 
    self.conv7 = ConvBlock(512, 512) 
    self.conv8 = ConvBlock(512, 512) 

  def forward(self, x):
    # Apply the convolutional layers and store the skip connections
    x1 = self.conv1(x)
    x2 = self.conv2(x1)
    x3 = self.conv3(x2)
    x4 = self.conv4(x3)
    x5 = self.conv5(x4)
    x6 = self.conv6(x5)
    x7 = self.conv7(x6)
    z = self.conv8(x7)
    skips = [x1, x2, x3, x4, x5, x6, x7]
    # Return the latent representation and the skip connections
    return z, skips

# Define a decoder that decodes a latent representation into a score vector using residual blocks and skip connections
class Decoder(torch.nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    # Define some convolutional layers with residual blocks and upsampling
    self.deconv1 = DeconvBlock(512, 512) # A function that applies a convolutional layer with residual block and activation function
    self.deconv2 = DeconvBlock(1024, 512) # A function that applies a convolutional layer with residual block, activation function and skip connection
    self.deconv3 = DeconvBlock(1024, 512) 
    self.deconv4 = DeconvBlock(1024, 512) 
    self.deconv5 = DeconvBlock(1024, 256) 
    self.deconv6 = DeconvBlock(512, 128) 
    self.deconv7 = DeconvBlock(256, 64) 
    self.deconv8 = DeconvBlock(128, 3) 

  def forward(self, z, skips):
    # Apply the convolutional layers and use the skip connections
    z = self.deconv1(z)
    z = self.deconv2(torch.cat([z, skips[6]], dim=1))
    z = self.deconv3(torch.cat([z, skips[5]], dim=1))
    z = self.deconv4(torch.cat([z, skips[4]], dim=1))
    z = self.deconv5(torch.cat([z, skips[3]], dim=1))
    z = self.deconv6(torch.cat([z, skips[2]], dim=1))
    z = self.deconv7(torch.cat([z, skips[1]], dim=1))
    score = self.deconv8(torch.cat([z, skips[0]], dim=1))
    # Return the score vector
    return score

# Define a convolutional layer with residual block and activation function
class ConvBlock(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super(ConvBlock, self).__init__()
    # Define a convolutional layer with kernel size 3 and padding 1
    self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
    # Define a batch normalization layer
    self.bn = torch.nn.BatchNorm2d(out_channels)
    # Define a leaky ReLU activation function with negative slope 0.2
    self.relu = torch.nn.LeakyReLU(negative_slope=0.2)
    # Define a residual block that consists of two convolutional layers and a skip connection
    self.res = ResBlock(out_channels)

  def forward(self, x):
    # Apply the convolutional layer
    x = self.conv(x)
    # Apply the batch normalization layer
    x = self.bn(x)
    # Apply the leaky ReLU activation function
    x = self.relu(x)
    # Apply the residual block
    x = self.res(x)
    # Return the output
    return x

# Define a convolutional layer with residual block, activation function and skip connection
class DeconvBlock(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super(DeconvBlock, self).__init__()
    # Define a transposed convolutional layer with kernel size 4 and stride 2 for upsampling
    self.deconv = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)
    # Define a batch normalization layer
    self.bn = torch.nn.BatchNorm2d(out_channels)
    # Define a leaky ReLU activation function with negative slope 0.2
    self.relu = torch.nn.LeakyReLU(negative_slope=0.2)
    # Define a residual block that consists of two convolutional layers and a skip connection
    self.res = ResBlock(out_channels)

  def forward(self, x):
    # Apply the transposed convolutional layer
    x = self.deconv(x)
    # Apply the batch normalization layer
    x = self.bn(x)
    # Apply the leaky ReLU activation function
    x = self.relu(x)
    # Apply the residual block
    x = self.res(x)
    # Return the output
    return x

# Define a residual block that consists of two convolutional layers and a skip connection
class ResBlock(torch.nn.Module):
  def __init__(self, channels):
    super(ResBlock, self).__init__()
    # Define two convolutional layers with kernel size 3 and padding 1
    self.conv1 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)
    self.conv2 = torch.nn.Conv2d(channels, channels, kernel_size=3, padding=1)
  
  def forward(self, x):
     # Save the input as the skip connection
     skip = x 
     # Apply the first convolutional layer
     x = self.conv1(x) 
     # Apply the second convolutional layer 
     x = self.conv2(x) 
     # Add the skip connection to the output 
     x += skip