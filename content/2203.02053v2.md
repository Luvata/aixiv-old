---
title: 2203.02053v2 Mind the Gap  Understanding the Modality Gap in Multi-modal Contrastive Representation Learning
date: 2022-03-03
---

# [Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](http://arxiv.org/abs/2203.02053v2)

authors: Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, James Zou


## What, Why and How

[1]: https://arxiv.org/abs/2203.02053 "[2203.02053] Mind the Gap: Understanding the Modality Gap in Multi ..."
[2]: https://arxiv.org/pdf/2203.02053v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.02053v1 "[2203.02053v1] Mind the Gap: Understanding the Modality Gap in Multi ..."

Here is a summary of the paper based on the abstract[^1^][1]:

- **What**: The paper presents **modality gap**, an intriguing geometric phenomenon of the representation space of multi-modal models. 
- **Why**: The paper aims to understand how different data modalities (e.g. images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP, and how this affects the model's performance and fairness.
- **How**: The paper analyzes the causes and effects of the modality gap, and shows that it is influenced by model initialization and contrastive learning optimization. The paper also demonstrates that varying the modality gap distance can improve the model's downstream zero-shot classification performance and fairness.

## Main Contributions

[1]: https://arxiv.org/abs/2203.02053 "[2203.02053] Mind the Gap: Understanding the Modality Gap in Multi ..."
[2]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2203.02053?context=cs.MM "[2203.02053] Mind the Gap: Understanding the Modality Gap in Multi ..."
[3]: https://arxiv.org/abs/2203.05369 "[2203.05369] A Contribution-based Device Selection Scheme in Federated ..."

According to the paper[^1^][1], the main contributions are:

- **Introducing modality gap**, a geometric phenomenon of the representation space of multi-modal models, and providing a systematic analysis of its causes and effects.
- **Proposing a novel metric**, modality gap distance, to measure the distance between different modalities in the representation space, and showing its correlation with downstream performance and fairness.
- **Conducting extensive experiments** on various multi-modal models and datasets, and demonstrating that varying the modality gap distance can improve the model's zero-shot classification performance and fairness.

## Method Summary

[1]: https://arxiv.org/abs/2203.02053 "[2203.02053] Mind the Gap: Understanding the Modality Gap in Multi ..."
[2]: https://www.scribbr.com/apa-style/methods-section/ "How to Write an APA Methods Section | With Examples - Scribbr"
[3]: https://plos.org/resource/how-to-write-your-methods/ "How to Write Your Methods - PLOS"

Here is a summary of the method section based on the paper[^1^][1]:

- The paper uses **three types of datasets** to evaluate the modality gap phenomenon: natural language and vision (NLV) datasets, natural language and speech (NLS) datasets, and natural language and knowledge graph (NLKG) datasets.
- The paper adopts **four types of multi-modal models** to study the modality gap: CLIP, SpeechCLIP, KGCLIP, and ViT-BERT. These models are trained using contrastive learning with different temperature parameters.
- The paper proposes a **novel metric** called modality gap distance (MGD) to measure the distance between different modalities in the representation space. MGD is defined as the cosine distance between the mean vectors of each modality.
- The paper conducts **various experiments** to analyze the causes and effects of the modality gap. The paper investigates how model initialization and contrastive learning optimization affect the modality gap, and how varying the modality gap distance influences the model's zero-shot classification performance and fairness.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the multi-modal model with two encoders
model = MultiModalModel(image_encoder, text_encoder)

# Define the contrastive loss function with a temperature parameter
loss_fn = ContrastiveLoss(temperature)

# Define the modality gap distance metric
def MGD(model, data):
  # Compute the mean vectors of each modality
  image_mean = mean(model.image_encoder(data.images))
  text_mean = mean(model.text_encoder(data.texts))
  # Compute the cosine distance between the mean vectors
  return cosine_distance(image_mean, text_mean)

# Train the model using contrastive learning
for epoch in epochs:
  for batch in data_loader:
    # Forward pass the batch through the model
    image_output, text_output = model(batch.images, batch.texts)
    # Compute the contrastive loss
    loss = loss_fn(image_output, text_output)
    # Backpropagate and update the model parameters
    loss.backward()
    optimizer.step()
    # Compute and log the modality gap distance
    mgd = MGD(model, batch)
    logger.log(mgd)

# Evaluate the model on zero-shot classification tasks
for task in tasks:
  # Load the task data and labels
  data, labels = load_task_data(task)
  # Forward pass the data through the model
  image_output, text_output = model(data.images, data.texts)
  # Compute the logits by taking the dot product of image and text outputs
  logits = dot_product(image_output, text_output)
  # Compute and report the accuracy and fairness metrics
  accuracy = compute_accuracy(logits, labels)
  fairness = compute_fairness(logits, labels)
  report(accuracy, fairness)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
batch_size = 256
num_epochs = 100
learning_rate = 1e-4
temperature = 0.07

# Define the multi-modal model with two encoders
class MultiModalModel(torch.nn.Module):
  def __init__(self, image_encoder, text_encoder):
    super().__init__()
    # Initialize the image encoder (e.g., ViT)
    self.image_encoder = image_encoder
    # Initialize the text encoder (e.g., BERT)
    self.text_encoder = text_encoder
  
  def forward(self, images, texts):
    # Forward pass the images through the image encoder
    image_output = self.image_encoder(images)
    # Forward pass the texts through the text encoder
    text_output = self.text_encoder(texts)
    # Normalize the outputs to unit length
    image_output = torch.nn.functional.normalize(image_output, dim=-1)
    text_output = torch.nn.functional.normalize(text_output, dim=-1)
    # Return the outputs
    return image_output, text_output

# Define the contrastive loss function with a temperature parameter
class ContrastiveLoss(torch.nn.Module):
  def __init__(self, temperature):
    super().__init__()
    # Initialize the temperature parameter
    self.temperature = temperature
  
  def forward(self, image_output, text_output):
    # Compute the dot product of image and text outputs
    logits = torch.matmul(image_output, text_output.t())
    # Divide the logits by the temperature
    logits = logits / self.temperature
    # Compute the labels by using the identity matrix
    labels = torch.eye(batch_size).to(device)
    # Compute the cross entropy loss
    loss = torch.nn.functional.cross_entropy(logits, labels)
    # Return the loss
    return loss

# Define the modality gap distance metric
def MGD(model, data):
  # Compute the mean vectors of each modality
  image_mean = torch.mean(model.image_encoder(data.images), dim=0)
  text_mean = torch.mean(model.text_encoder(data.texts), dim=0)
  # Compute the cosine distance between the mean vectors
  return 1 - torch.dot(image_mean, text_mean)

# Load and preprocess the data (e.g., ImageNet and Conceptual Captions)
image_transform = torchvision.transforms.Compose([
  torchvision.transforms.Resize(224),
  torchvision.transforms.CenterCrop(224),
  torchvision.transforms.ToTensor(),
  torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])
text_transform = transformers.AutoTokenizer.from_pretrained("bert-base-uncased")
dataset = CustomDataset(image_transform, text_transform)
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Instantiate the model and the optimizer
model = MultiModalModel(
  image_encoder=torchvision.models.vit_base_patch16_224(pretrained=True).head,
  text_encoder=transformers.BertModel.from_pretrained("bert-base-uncased").pooler
).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model using contrastive learning
for epoch in range(num_epochs):
  for batch in data_loader:
    # Move the batch to device (e.g., GPU)
    batch = {k: v.to(device) for k, v in batch.items()}
    # Forward pass the batch through the model
    image_output, text_output = model(batch["images"], batch["texts"])
    # Compute the contrastive loss
    loss = loss_fn(image_output, text_output)
    # Backpropagate and update the model parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    # Compute and log the modality gap distance
    mgd = MGD(model, batch)
    print(f"Epoch {epoch}, Loss {loss.item()}, MGD {mgd.item()}")

# Evaluate the model on zero-shot classification tasks (e.g., ImageNet-R and ImageNet-Sketch)
for task in tasks:
  # Load the task data and labels
  data, labels = load_task_data(task)
  # Forward pass the data through the model
  image_output, text_output = model(data["images"], data["texts"])
  # Compute the logits by taking the dot product of image and text outputs
  logits = torch.matmul(image_output, text_output.t())
  # Compute and report the accuracy and fairness metrics (e.g., top-1 accuracy and demographic parity)
  accuracy = compute_accuracy(logits, labels)
  fairness = compute_fairness(logits, labels)
  print(f"Task {task}, Accuracy {accuracy}, Fairness {fairness}")
```