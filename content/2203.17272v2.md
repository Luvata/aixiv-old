---
title: 2203.17272v2 MyStyle  A Personalized Generative Prior
date: 2022-03-18
---

# [MyStyle: A Personalized Generative Prior](http://arxiv.org/abs/2203.17272v2)

authors: Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen-or


## What, Why and How

[1]: https://arxiv.org/abs/2203.17272 "[2203.17272] MyStyle: A Personalized Generative Prior - arXiv.org"
[2]: http://arxiv-export3.library.cornell.edu/abs/2203.17272 "[2203.17272] MyStyle: A Personalized Generative Prior"
[3]: https://arxiv.org/abs/2303.17272 "[2303.17272] Angular distribution of photoelectrons generated in atomic ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics.
- **Why**: The paper aims to address the challenge of ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing, for which existing methods often fail to preserve the identity and style of the individual in the input image.
- **How**: The paper proposes to tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space, given a small reference set of portrait images of a person (~100). The paper shows that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. The paper also demonstrates how to use the personalized generative prior as a regularizer for various image enhancement and editing tasks. The paper evaluates the method against few-shots baselines and shows that it outperforms state-of-the-art alternatives in terms of fidelity and faithfulness.

## Main Contributions

According to the paper, the main contributions are:

- The introduction of MyStyle, a personalized deep generative prior trained with a few shots of an individual, that allows to reconstruct, enhance and edit images of a specific person with high fidelity and faithfulness.
- The demonstration of how to tune the weights of a pretrained StyleGAN face generator to form a personalized manifold in the latent space, given a small reference set of portrait images of a person.
- The proposal of a unified approach to apply the personalized generative prior to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing.
- The evaluation of the method against few-shots baselines and state-of-the-art alternatives on fair-use images of numerous widely recognizable individuals.

## Method Summary

[1]: https://arxiv.org/abs/2203.17272 "[2203.17272] MyStyle: A Personalized Generative Prior - arXiv.org"
[2]: http://arxiv-export3.library.cornell.edu/abs/2203.17272 "[2203.17272] MyStyle: A Personalized Generative Prior"
[3]: https://arxiv.org/pdf/2203.15722 "Transformer Network based Reinforcement Learning Method for Power ..."

Here is a summary of the method section of the paper[^1^][1]:

- The method consists of three main steps: **personalized manifold learning**, **personalized generative prior learning**, and **personalized image enhancement and editing**.
- In the **personalized manifold learning** step, the method tunes the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space, given a small reference set of portrait images of a person. The method uses a gradient-based optimization scheme that minimizes the reconstruction loss between the generated images and the reference images, while regularizing the latent codes to be close to the mean latent code of the reference set. The method also introduces a novel adaptive learning rate scheme that balances between exploration and exploitation of the latent space.
- In the **personalized generative prior learning** step, the method learns a personalized generative prior that maps an input image to a latent code on the personalized manifold. The method uses a convolutional neural network (CNN) that is trained with a self-supervised loss function that encourages the network to produce latent codes that can reconstruct the input image with high fidelity and faithfulness. The method also leverages a perceptual loss function that measures the similarity between the input image and the generated image in a feature space of a pretrained face recognition network.
- In the **personalized image enhancement and editing** step, the method applies the personalized generative prior to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. The method uses a unified optimization framework that minimizes an objective function that consists of three terms: a data term that measures the consistency between the input image and the generated image in the observed regions, a prior term that measures the distance between the latent code and the personalized manifold, and an edit term that measures the deviation from a desired edit direction in the latent space. The method also introduces a novel adaptive weighting scheme that balances between these terms according to the quality of the input image.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a small reference set of portrait images of a person R
# Output: a personalized generative prior P and a personalized manifold M

# Step 1: Personalized manifold learning
# Initialize the latent codes Z and the learning rates L for each image in R
# Initialize the StyleGAN face generator G with pretrained weights
# Repeat until convergence:
  # For each image in R:
    # Generate an image I from G using the corresponding latent code Z
    # Compute the reconstruction loss L_rec between I and the reference image
    # Compute the regularization loss L_reg between Z and the mean latent code Z_mean
    # Compute the total loss L_tot as a weighted sum of L_rec and L_reg
    # Update Z by gradient descent using L_tot and L as the learning rate
    # Update L by an adaptive scheme based on L_tot
# Obtain the personalized manifold M as the set of tuned latent codes Z

# Step 2: Personalized generative prior learning
# Initialize a CNN encoder E with random weights
# Initialize a face recognition network F with pretrained weights
# Repeat until convergence:
  # For each image in R:
    # Encode the image into a latent code Z using E
    # Generate an image I from G using Z
    # Compute the reconstruction loss L_rec between I and the reference image
    # Compute the perceptual loss L_per between I and the reference image in the feature space of F
    # Compute the total loss L_tot as a weighted sum of L_rec and L_per
    # Update E by gradient descent using L_tot
# Obtain the personalized generative prior P as the trained encoder E

# Step 3: Personalized image enhancement and editing
# Input: an input image X, an edit direction D (optional)
# Output: an enhanced or edited image Y
# Encode X into a latent code Z using P
# Initialize the weights W for the data term, prior term and edit term in the objective function
# Repeat until convergence:
  # Generate an image Y from G using Z
  # Compute the data term L_data between X and Y in the observed regions
  # Compute the prior term L_prior between Z and M
  # Compute the edit term L_edit between Z and D (if D is given)
  # Compute the total loss L_tot as a weighted sum of L_data, L_prior and L_edit using W
  # Update Z by gradient descent using L_tot
  # Update W by an adaptive scheme based on X and Y
# Return Y as the enhanced or edited image

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2
import stylegan2_pytorch # a PyTorch implementation of StyleGAN2
import facenet_pytorch # a PyTorch implementation of FaceNet

# Define some hyperparameters
num_images = 100 # the number of reference images
latent_dim = 512 # the dimension of the latent space
image_size = 256 # the size of the input and output images
lambda_rec = 1.0 # the weight for the reconstruction loss
lambda_reg = 0.01 # the weight for the regularization loss
lambda_per = 0.1 # the weight for the perceptual loss
lambda_data = 1.0 # the initial weight for the data term
lambda_prior = 1.0 # the initial weight for the prior term
lambda_edit = 1.0 # the initial weight for the edit term
alpha = 0.5 # the parameter for the adaptive learning rate scheme
beta = 0.5 # the parameter for the adaptive weighting scheme
max_iter = 1000 # the maximum number of iterations

# Load the reference set of portrait images of a person R
R = [] # an empty list to store the images
for i in range(num_images):
  # Read an image from a file and resize it to image_size x image_size
  image = cv2.imread(f"image_{i}.jpg")
  image = cv2.resize(image, (image_size, image_size))
  # Convert the image from BGR to RGB and normalize it to [0, 1]
  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
  image = image / 255.0
  # Convert the image to a PyTorch tensor and add a batch dimension
  image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)
  # Append the image to the list R
  R.append(image)

# Step 1: Personalized manifold learning
# Initialize the latent codes Z and the learning rates L for each image in R
Z = torch.randn(num_images, latent_dim) # random normal vectors in latent space
L = torch.ones(num_images) # initial learning rates of 1.0 for each image

# Initialize the StyleGAN face generator G with pretrained weights
G = stylegan2_pytorch.StyleGAN2Generator(resolution=image_size)
G.load_state_dict(torch.load("stylegan2-ffhq-config-f.pt")) # load pretrained weights on FFHQ dataset

# Repeat until convergence:
for iter in range(max_iter):
  # For each image in R:
  for i in range(num_images):
    # Generate an image I from G using the corresponding latent code Z
    I = G(Z[i].unsqueeze(0))
    # Compute the reconstruction loss L_rec between I and the reference image using L1 norm
    L_rec = torch.nn.functional.l1_loss(I, R[i])
    # Compute the regularization loss L_reg between Z and the mean latent code Z_mean using L2 norm
    Z_mean = torch.mean(Z, dim=0) # compute the mean latent code over all images in R
    L_reg = torch.nn.functional.mse_loss(Z[i], Z_mean)
    # Compute the total loss L_tot as a weighted sum of L_rec and L_reg using lambda_rec and lambda_reg as weights
    L_tot = lambda_rec * L_rec + lambda_reg * L_reg
    # Update Z by gradient descent using L_tot and L as the learning rate
    Z[i] -= L[i] * torch.autograd.grad(L_tot, Z[i])[0]
    # Update L by an adaptive scheme based on L_tot using alpha as a parameter
    if iter > 0 and L_tot > prev_L_tot: # if the loss increased from previous iteration
      L[i] *= alpha # decrease the learning rate by alpha factor
    prev_L_tot = L_tot # store the current loss for next iteration

# Obtain the personalized manifold M as the set of tuned latent codes Z
M = Z.clone()

# Step 2: Personalized generative prior learning
# Initialize a CNN encoder E with random weights
E = torchvision.models.resnet18(pretrained=False) # use ResNet-18 as a backbone model
E.fc = torch.nn.Linear(E.fc.in_features, latent_dim) # replace the final layer with a linear layer that outputs latent_dim features

# Initialize a face recognition network F with pretrained weights
F = facenet_pytorch.InceptionResnetV1(pretrained="vggface2").eval() # use Inception-ResNet-v1 pretrained on VGGFace2 dataset

# Repeat until convergence:
for iter in range(max_iter):
  # For each image in R:
  for i in range(num_images):
    # Encode the image into a latent code Z using E
    Z = E(R[i])
    # Generate an image I from G using Z
    I = G(Z.unsqueeze(0))
    # Compute the reconstruction loss L_rec between I and the reference image using L1 norm
    L_rec = torch.nn.functional.l1_loss(I, R[i])
    # Compute the perceptual loss L_per between I and the reference image in the feature space of F using L2 norm
    F_I = F(I) # extract features from the generated image using F
    F_R = F(R[i]) # extract features from the reference image using F
    L_per = torch.nn.functional.mse_loss(F_I, F_R)
    # Compute the total loss L_tot as a weighted sum of L_rec and L_per using lambda_rec and lambda_per as weights
    L_tot = lambda_rec * L_rec + lambda_per * L_per
    # Update E by gradient descent using L_tot
    E.zero_grad() # reset the gradients of E
    L_tot.backward() # compute the gradients of L_tot with respect to E
    E.step() # update the weights of E

# Obtain the personalized generative prior P as the trained encoder E
P = E

# Step 3: Personalized image enhancement and editing
# Input: an input image X, an edit direction D (optional)
# Output: an enhanced or edited image Y

# Load an input image X from a file and resize it to image_size x image_size
X = cv2.imread("input.jpg")
X = cv2.resize(X, (image_size, image_size))
# Convert the image from BGR to RGB and normalize it to [0, 1]
X = cv2.cvtColor(X, cv2.COLOR_BGR2RGB)
X = X / 255.0
# Convert the image to a PyTorch tensor and add a batch dimension
X = torch.from_numpy(X).permute(2, 0, 1).unsqueeze(0)

# Encode X into a latent code Z using P
Z = P(X)

# Initialize the weights W for the data term, prior term and edit term in the objective function
W_data = lambda_data # initial weight for the data term
W_prior = lambda_prior # initial weight for the prior term
W_edit = lambda_edit # initial weight for the edit term

# Repeat until convergence:
for iter in range(max_iter):
  # Generate an image Y from G using Z
  Y = G(Z)
  # Compute the data term L_data between X and Y in the observed regions using L1 norm
  mask = X > 0 # create a binary mask that indicates the observed regions in X (non-zero pixels)
  L_data = torch.nn.functional.l1_loss(X * mask, Y * mask) # compute the loss only in the observed regions
  # Compute the prior term L_prior between Z and M using L2 norm
  dists = torch.sum((Z - M) ** 2, dim=1) # compute the squared distances between Z and each latent code in M
  nearest = torch.argmin(dists) # find the index of the nearest latent code in M to Z
  L_prior = dists[nearest] # use the distance to the nearest latent code as the prior loss
  # Compute the edit term L_edit between Z and D (if D is given) using cosine similarity
  if D is not None: # if an edit direction is given
    cos_sim = torch.nn.functional.cosine_similarity(Z, D) # compute the cosine similarity between Z and D
    L_edit = 1 - cos_sim # use 1 minus cosine similarity as the edit loss
  else: # if no edit direction is given
    L_edit = 0 # set the edit loss to zero
  # Compute the total loss L_tot as a weighted sum of L_data, L_prior and L_edit using W_data, W_prior and W_edit as weights
  L_tot = W_data * L_data + W_prior * L_prior + W_edit * L_edit
  # Update Z by gradient descent using L_tot
  Z -= torch.autograd.grad(L_tot, Z)[0]
  # Update W by an adaptive scheme based on X and Y using beta as a parameter
  psnr = -10 * torch.log10(torch.mean((X - Y) ** 2)) # compute the peak signal-to-noise ratio (PSNR) between X and Y
  if iter > 0 and psnr < prev_psnr: # if the PSNR decreased from previous iteration
    W_data *=