---
title: 2106.14851v2 Data Poisoning Won't Save You From Facial Recognition
date: 2021-06-15
---

# [Data Poisoning Won't Save You From Facial Recognition](http://arxiv.org/abs/2106.14851v2)

authors: Evani Radiya-Dixit, Sanghyun Hong, Nicholas Carlini, Florian Tram√®r


## What, Why and How

[1]: https://arxiv.org/pdf/2106.14851v2.pdf "A arXiv:2106.14851v2 [cs.LG] 14 Mar 2022"
[2]: https://arxiv.org/abs/2106.14851 "Title: Data Poisoning Won't Save You From Facial Recognition - arXiv.org"
[3]: http://export.arxiv.org/abs/2006.14851v2 "[2006.14851v2] Secure Beamforming for Multiple Intelligent Reflecting ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper evaluates the effectiveness of data poisoning as a defense against facial recognition models trained on web-scraped pictures. Data poisoning is a technique that lets users perturb their pictures before posting them online, so that models trained on these pictures will misclassify future (unperturbed) pictures of the same user.
- **Why**: The paper argues that data poisoning provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures are perturbed once and for all before being published (at which point they are scraped) and must thereafter fool all future models -- including models trained adaptively against the users' past attacks, or models that use technologies discovered after the attack.
- **How**: The paper demonstrates how an "oblivious" model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. The paper also shows that an adversary with black-box access to the attack can (i) train a robust model that resists the perturbations of collected pictures and (ii) detect poisoned pictures uploaded online. The paper evaluates two systems for poisoning attacks against large-scale facial recognition, Fawkes and LowKey.

## Main Contributions

The paper claims to make the following contributions:

- It provides the first comprehensive evaluation of data poisoning as a defense against facial recognition models trained on web-scraped pictures.
- It exposes the limitations and risks of data poisoning, and cautions that facial recognition poisoning will not admit an "arms race" between attackers and defenders.
- It proposes novel techniques to train robust models that resist data poisoning, and to detect poisoned pictures uploaded online.

## Method Summary

[1]: https://arxiv.org/pdf/2106.14851v2.pdf "A arXiv:2106.14851v2 [cs.LG] 14 Mar 2022"
[2]: https://arxiv.org/abs/2106.14851 "Title: Data Poisoning Won't Save You From Facial Recognition - arXiv.org"
[3]: http://export.arxiv.org/abs/2006.14851v2 "[2006.14851v2] Secure Beamforming for Multiple Intelligent Reflecting ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper uses two systems for poisoning attacks against large-scale facial recognition, Fawkes and LowKey, which are based on different principles: Fawkes uses a feature-level perturbation that aims to align the user's features with those of a randomly chosen target identity, while LowKey uses a pixel-level perturbation that aims to minimize the similarity between the user's features and those of any other identity in a pre-defined set.
- The paper evaluates the effectiveness of these systems against three types of adversaries: an oblivious adversary who trains a standard model on the poisoned dataset without any countermeasure, an adaptive adversary who trains a robust model on the poisoned dataset using techniques such as adversarial training or feature squeezing, and a detection adversary who tries to identify and filter out poisoned pictures from the dataset using techniques such as anomaly detection or reverse engineering.
- The paper uses two datasets for evaluation: a public dataset called Labeled Faces in the Wild (LFW), which contains 13,233 face images of 5,749 celebrities, and a private dataset called MegaFace, which contains over 4.7 million face images of 672,057 identities scraped from Flickr. The paper also uses two models for evaluation: a state-of-the-art model called ArcFace, which is trained on over 5 million face images of 85,742 identities, and a smaller model called FaceNet, which is trained on over 200,000 face images of 8,000 identities.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the poisoning systems Fawkes and LowKey
def Fawkes(user_images, target_images):
  # For each user image, find the closest target image in feature space
  # Compute the feature-level perturbation that aligns the user image with the target image
  # Apply the perturbation to the user image and clip the pixel values
  # Return the perturbed user images

def LowKey(user_images, protected_set):
  # For each user image, find the most similar identity in the protected set in feature space
  # Compute the pixel-level perturbation that minimizes the similarity between the user image and the protected identity
  # Apply the perturbation to the user image and clip the pixel values
  # Return the perturbed user images

# Define the adversaries oblivious, adaptive, and detection
def oblivious(poisoned_dataset):
  # Train a standard model on the poisoned dataset using cross-entropy loss
  # Return the trained model

def adaptive(poisoned_dataset):
  # Train a robust model on the poisoned dataset using one of the following techniques:
  # - Adversarial training: augment the dataset with adversarial examples generated by gradient-based attacks
  # - Feature squeezing: reduce the input dimensionality by applying transformations such as smoothing or quantization
  # Return the trained model

def detection(poisoned_dataset):
  # Identify and filter out poisoned pictures from the dataset using one of the following techniques:
  # - Anomaly detection: use an autoencoder or a generative model to reconstruct the input images and measure the reconstruction error
  # - Reverse engineering: use gradient-based or optimization-based methods to estimate the perturbation applied to each image and measure its magnitude
  # Return the filtered dataset

# Define the evaluation metrics accuracy and privacy
def accuracy(model, test_images, test_labels):
  # Compute the predictions of the model on the test images
  # Compare the predictions with the test labels and calculate the accuracy
  # Return the accuracy

def privacy(model, user_images, user_labels):
  # Compute the predictions of the model on the user images
  # Compare the predictions with the user labels and calculate the privacy
  # Privacy is defined as the percentage of user images that are misclassified by the model
  # Return the privacy

# Define the datasets LFW and MegaFace
LFW = load_LFW()
MegaFace = load_MegaFace()

# Define the models ArcFace and FaceNet
ArcFace = load_ArcFace()
FaceNet = load_FaceNet()

# Define a set of users who want to protect their privacy
users = select_users_from_MegaFace()

# Define a set of identities that are protected by LowKey
protected_set = select_identities_from_MegaFace()

# For each poisoning system and each model, evaluate the accuracy and privacy against each adversary
for system in [Fawkes, LowKey]:
  for model in [ArcFace, FaceNet]:
    # Perturb the user images using the poisoning system
    poisoned_user_images = system(user_images, target_images or protected_set)

    # Add the poisoned user images to LFW or MegaFace to create a poisoned dataset
    poisoned_dataset = add_poisoned_user_images(LFW or MegaFace, poisoned_user_images)

    for adversary in [oblivious, adaptive, detection]:
      if adversary == detection:
        # Filter out poisoned pictures from the poisoned dataset using detection techniques
        filtered_dataset = detection(poisoned_dataset)

        # Evaluate accuracy and privacy on filtered dataset using original model
        accuracy = accuracy(model, filtered_dataset.images, filtered_dataset.labels)
        privacy = privacy(model, user_images, user_labels)
      else:
        # Train a new model on poisoned dataset using oblivious or adaptive techniques
        new_model = adversary(poisoned_dataset)

        # Evaluate accuracy and privacy on LFW or MegaFace using new model
        accuracy = accuracy(new_model, LFW or MegaFace.images, LFW or MegaFace.labels)
        privacy = privacy(new_model, user_images, user_labels)

      # Report accuracy and privacy results for each combination of system, model, and adversary
      report_results(system, model, adversary, accuracy, privacy)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import numpy as np
import torch
import torchvision
import cv2
import sklearn
import scipy

# Define the poisoning systems Fawkes and LowKey
def Fawkes(user_images, target_images):
  # Initialize the perturbed user images as copies of the original user images
  perturbed_user_images = user_images.copy()

  # For each user image, find the closest target image in feature space
  for i in range(len(user_images)):
    # Extract the features of the user image using ArcFace
    user_features = ArcFace(user_images[i])

    # Compute the cosine similarity between the user features and the target features
    similarities = sklearn.metrics.pairwise.cosine_similarity(user_features, target_features)

    # Find the index of the target image with the highest similarity
    j = np.argmax(similarities)

    # Compute the feature-level perturbation that aligns the user image with the target image
    # The perturbation is computed as the difference between the target features and the user features scaled by a factor alpha
    alpha = 0.05 # This is a hyperparameter that can be tuned
    perturbation = alpha * (target_features[j] - user_features)

    # Apply the perturbation to the user image and clip the pixel values to [0, 255]
    perturbed_user_images[i] = np.clip(user_images[i] + perturbation, 0, 255)

  # Return the perturbed user images
  return perturbed_user_images

def LowKey(user_images, protected_set):
  # Initialize the perturbed user images as copies of the original user images
  perturbed_user_images = user_images.copy()

  # For each user image, find the most similar identity in the protected set in feature space
  for i in range(len(user_images)):
    # Extract the features of the user image using ArcFace
    user_features = ArcFace(user_images[i])

    # Compute the cosine similarity between the user features and the protected features
    similarities = sklearn.metrics.pairwise.cosine_similarity(user_features, protected_features)

    # Find the index of the protected identity with the highest similarity
    j = np.argmax(similarities)

    # Compute the pixel-level perturbation that minimizes the similarity between the user image and the protected identity
    # The perturbation is computed as a solution to a constrained optimization problem using scipy.optimize.minimize
    def objective(p):
      # The objective function is defined as the negative cosine similarity between the perturbed user features and the protected features
      return -scipy.spatial.distance.cosine(ArcFace(user_images[i] + p), protected_features[j])

    def constraint(p):
      # The constraint function is defined as the L2 norm of the perturbation being less than or equal to epsilon
      epsilon = 10 # This is a hyperparameter that can be tuned
      return epsilon - np.linalg.norm(p)

    # Define an initial guess for the perturbation as a zero vector
    p0 = np.zeros_like(user_images[i])

    # Define an optimization problem with bounds on each pixel value to be in [0, 255]
    bounds = [(0, 255)] * len(p0)

    # Define an optimization problem with a nonlinear constraint on the L2 norm of the perturbation
    cons = {'type': 'ineq', 'fun': constraint}

    # Solve the optimization problem using scipy.optimize.minimize with SLSQP method
    solution = scipy.optimize.minimize(objective, p0, method='SLSQP', bounds=bounds, constraints=cons)

    # Extract the optimal perturbation from the solution
    perturbation = solution.x

    # Apply the perturbation to the user image and clip the pixel values to [0, 255]
    perturbed_user_images[i] = np.clip(user_images[i] + perturbation, 0, 255)

  # Return the perturbed user images
  return perturbed_user_images

# Define the adversaries oblivious, adaptive, and detection
def oblivious(poisoned_dataset):
  # Train a standard model on the poisoned dataset using cross-entropy loss
  model = torchvision.models.resnet18(pretrained=True) # This is an example model that can be changed
  model.fc = torch.nn.Linear(model.fc.in_features, num_classes) # Replace the last layer with a new one for classification
  optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # This is an example optimizer that can be changed
  criterion = torch.nn.CrossEntropyLoss() # This is an example loss function that can be changed

  # Define the number of epochs for training
  num_epochs = 10 # This is a hyperparameter that can be tuned

  # Loop over the epochs
  for epoch in range(num_epochs):
    # Loop over the batches of the poisoned dataset
    for inputs, labels in poisoned_dataset:
      # Move the inputs and labels to the device (CPU or GPU)
      inputs = inputs.to(device)
      labels = labels.to(device)

      # Zero the parameter gradients
      optimizer.zero_grad()

      # Forward pass
      outputs = model(inputs)
      loss = criterion(outputs, labels)

      # Backward pass and optimize
      loss.backward()
      optimizer.step()

  # Return the trained model
  return model

def adaptive(poisoned_dataset):
  # Train a robust model on the poisoned dataset using one of the following techniques:
  # - Adversarial training: augment the dataset with adversarial examples generated by gradient-based attacks
  # - Feature squeezing: reduce the input dimensionality by applying transformations such as smoothing or quantization

  # Here we use adversarial training as an example technique, but feature squeezing can also be implemented similarly
  model = torchvision.models.resnet18(pretrained=True) # This is an example model that can be changed
  model.fc = torch.nn.Linear(model.fc.in_features, num_classes) # Replace the last layer with a new one for classification
  optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # This is an example optimizer that can be changed
  criterion = torch.nn.CrossEntropyLoss() # This is an example loss function that can be changed

  # Define the number of epochs for training
  num_epochs = 10 # This is a hyperparameter that can be tuned

  # Define the epsilon value for generating adversarial examples
  epsilon = 0.01 # This is a hyperparameter that can be tuned

  # Loop over the epochs
  for epoch in range(num_epochs):
    # Loop over the batches of the poisoned dataset
    for inputs, labels in poisoned_dataset:
      # Move the inputs and labels to the device (CPU or GPU)
      inputs = inputs.to(device)
      labels = labels.to(device)

      # Generate adversarial examples using a gradient-based attack such as FGSM
      inputs.requires_grad = True # Set the requires_grad attribute to True to compute gradients
      outputs = model(inputs) # Forward pass
      loss = criterion(outputs, labels) # Compute the loss
      loss.backward() # Backward pass
      perturbation = epsilon * inputs.grad.sign() # Compute the perturbation as epsilon times the sign of the gradient
      adversarial_inputs = inputs + perturbation # Add the perturbation to the inputs to get adversarial inputs
      adversarial_inputs = torch.clamp(adversarial_inputs, 0, 1) # Clip the pixel values to [0, 1]

      # Zero the parameter gradients
      optimizer.zero_grad()

      # Forward pass with adversarial inputs
      outputs = model(adversarial_inputs)
      loss = criterion(outputs, labels)

      # Backward pass and optimize
      loss.backward()
      optimizer.step()

  # Return the trained model
  return model

def detection(poisoned_dataset):
  # Identify and filter out poisoned pictures from the dataset using one of the following techniques:
  # - Anomaly detection: use an autoencoder or a generative model to reconstruct the input images and measure the reconstruction error
  # - Reverse engineering: use gradient-based or optimization-based methods to estimate the perturbation applied to each image and measure its magnitude

  # Here we use anomaly detection as an example technique, but reverse engineering can also be implemented similarly
  autoencoder = torchvision.models.resnet18(pretrained=True) # This is an example autoencoder that can be changed
  autoencoder.fc = torch.nn.Linear(autoencoder.fc.in_features, num_features) # Replace the last layer with a new one for feature extraction
  decoder = torch.nn.Linear(num_features, image_size) # Define a decoder layer to reconstruct the images from features
  optimizer = torch.optim.Adam(autoencoder.parameters() + decoder.parameters(), lr=0.001) # This is an example optimizer that can be changed
  criterion = torch.nn.MSELoss() # This is an example loss function that can be changed

  # Define the number of epochs for training
  num_epochs = 10 # This is a hyperparameter that can be tuned

  # Loop over the epochs
  for epoch in range(num_epochs):
    # Loop over the batches of the poisoned dataset
    for inputs, _ in poisoned_dataset:
      # Move the inputs to the device (CPU or GPU)
      inputs = inputs.to(device)

      # Zero the parameter gradients
      optimizer.zero_grad()

      # Forward pass
      features = autoencoder(inputs) 
      outputs = decoder(features)
      
      loss = criterion(outputs, inputs) 

      loss.backward()