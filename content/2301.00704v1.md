---
title: 2301.00704v1 Muse  Text-To-Image Generation via Masked Generative Transformers
date: 2023-01-01
---

# [Muse: Text-To-Image Generation via Masked Generative Transformers](http://arxiv.org/abs/2301.00704v1)

authors: Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan


## What, Why and How

[1]: https://arxiv.org/abs/2301.00704 "[2301.00704] Muse: Text-To-Image Generation via Masked Generative ..."
[2]: https://arxiv.org/pdf/2301.00704v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2301.00704 "[2301.00704] Muse: Text-To-Image Generation via Masked Generative ..."

Here is a summary of the paper:

- **What**: The paper presents Muse, a text-to-image Transformer model that generates high-quality images from text prompts using masked modeling in discrete token space.
- **Why**: The paper aims to improve the efficiency and fidelity of text-to-image generation models by leveraging pre-trained large language models and parallel decoding.
- **How**: The paper trains Muse to predict randomly masked image tokens given the text embedding extracted from a pre-trained T5-XXL model. The paper also demonstrates various image editing applications enabled by Muse without the need to fine-tune or invert the model.

## Main Contributions

The paper claims the following contributions:

- A new text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models.
- A masked modeling task in discrete token space that enables parallel decoding and reduces sampling iterations.
- A fine-grained language understanding capability that translates to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc.
- A number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing.

## Method Summary

[1]: https://arxiv.org/abs/2301.00704 "[2301.00704] Muse: Text-To-Image Generation via Masked Generative ..."
[2]: https://arxiv.org/pdf/2301.00704v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2301.00704 "[2301.00704] Muse: Text-To-Image Generation via Masked Generative ..."

Here is a summary of the method section of the paper:

- The paper proposes a text-to-image Transformer model that consists of a pre-trained T5-XXL encoder and a Muse decoder.
- The Muse decoder is a Transformer decoder with masked self-attention and cross-attention to the text embedding from the encoder.
- The Muse decoder takes as input a sequence of discrete image tokens obtained by applying a VQGAN quantizer to the image pixels.
- The Muse decoder is trained on a masked modeling task, where it predicts randomly masked image tokens given the text embedding and the unmasked image tokens.
- The paper uses a two-stage sampling procedure to generate images from text prompts: first, it samples an initial image token sequence using top-k sampling; second, it refines the image token sequence using masked sampling with beam search.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Muse model
class Muse(nn.Module):
  def __init__(self):
    # Initialize the T5-XXL encoder
    self.encoder = T5Encoder()
    # Freeze the encoder parameters
    for param in self.encoder.parameters():
      param.requires_grad = False
    # Initialize the Muse decoder
    self.decoder = MuseDecoder()
    # Initialize the VQGAN quantizer
    self.quantizer = VQGAN()

  def forward(self, text, image=None):
    # Encode the text into a text embedding
    text_embedding = self.encoder(text)
    # Quantize the image into a sequence of image tokens
    image_tokens = self.quantizer(image)
    # Decode the image tokens given the text embedding
    output_tokens = self.decoder(text_embedding, image_tokens)
    # Reconstruct the image from the output tokens
    output_image = self.quantizer.inverse(output_tokens)
    return output_image

# Define the Muse decoder
class MuseDecoder(nn.Module):
  def __init__(self):
    # Initialize the Transformer decoder layers
    self.layers = nn.ModuleList([TransformerDecoderLayer() for _ in range(num_layers)])

  def forward(self, text_embedding, image_tokens):
    # Mask some of the image tokens randomly
    masked_tokens, mask = mask_image_tokens(image_tokens)
    # Initialize the decoder hidden state with zeros
    hidden_state = torch.zeros_like(masked_tokens)
    # Loop over the decoder layers
    for layer in self.layers:
      # Apply masked self-attention to the hidden state
      hidden_state = layer.self_attention(hidden_state, hidden_state, hidden_state, mask)
      # Apply cross-attention to the hidden state and the text embedding
      hidden_state = layer.cross_attention(hidden_state, text_embedding, text_embedding)
      # Apply feed-forward network to the hidden state
      hidden_state = layer.feed_forward(hidden_state)
    # Return the final hidden state as the output tokens
    return hidden_state

# Define the sampling procedure
def sample(text):
  # Initialize the Muse model
  model = Muse()
  # Sample an initial image token sequence using top-k sampling
  initial_tokens = top_k_sampling(model, text)
  # Refine the image token sequence using masked sampling with beam search
  refined_tokens = masked_sampling(model, text, initial_tokens)
  # Reconstruct the image from the refined tokens
  image = model.quantizer.inverse(refined_tokens)
  return image

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np

# Define some hyperparameters
num_layers = 24 # Number of decoder layers
num_heads = 16 # Number of attention heads
hidden_size = 1024 # Size of hidden state
vocab_size = 8192 # Size of image token vocabulary
embed_size = 256 # Size of image token embedding
text_size = 1024 # Size of text embedding
temperature = 0.9 # Temperature for sampling
top_k = 50 # Top-k value for sampling
beam_size = 4 # Beam size for sampling
max_length = 256 # Maximum length of image token sequence

# Define the Muse model
class Muse(nn.Module):
  def __init__(self):
    super(Muse, self).__init__()
    # Initialize the T5-XXL encoder
    self.encoder = T5Encoder()
    # Freeze the encoder parameters
    for param in self.encoder.parameters():
      param.requires_grad = False
    # Initialize the Muse decoder
    self.decoder = MuseDecoder()
    # Initialize the VQGAN quantizer
    self.quantizer = VQGAN()

  def forward(self, text, image=None):
    # Encode the text into a text embedding
    text_embedding = self.encoder(text)
    # Quantize the image into a sequence of image tokens
    image_tokens = self.quantizer(image)
    # Decode the image tokens given the text embedding
    output_tokens = self.decoder(text_embedding, image_tokens)
    # Reconstruct the image from the output tokens
    output_image = self.quantizer.inverse(output_tokens)
    return output_image

# Define the Muse decoder
class MuseDecoder(nn.Module):
  def __init__(self):
    super(MuseDecoder, self).__init__()
    # Initialize the Transformer decoder layers
    self.layers = nn.ModuleList([TransformerDecoderLayer() for _ in range(num_layers)])
    # Initialize the output projection layer
    self.proj = nn.Linear(hidden_size, vocab_size)

  def forward(self, text_embedding, image_tokens):
    # Mask some of the image tokens randomly
    masked_tokens, mask = mask_image_tokens(image_tokens)
    # Embed the masked tokens into a hidden state
    hidden_state = self.embed(masked_tokens)
    # Loop over the decoder layers
    for layer in self.layers:
      # Apply masked self-attention to the hidden state
      hidden_state = layer.self_attention(hidden_state, hidden_state, hidden_state, mask)
      # Apply cross-attention to the hidden state and the text embedding
      hidden_state = layer.cross_attention(hidden_state, text_embedding, text_embedding)
      # Apply feed-forward network to the hidden state
      hidden_state = layer.feed_forward(hidden_state)
    # Project the final hidden state to logits over the vocabulary
    logits = self.proj(hidden_state)
    return logits

# Define a Transformer decoder layer
class TransformerDecoderLayer(nn.Module):
  def __init__(self):
    super(TransformerDecoderLayer, self).__init__()
    # Initialize the masked self-attention layer and layer norm
    self.self_attention = MultiHeadAttention(hidden_size, num_heads)
    self.self_norm = nn.LayerNorm(hidden_size)
    # Initialize the cross-attention layer and layer norm
    self.cross_attention = MultiHeadAttention(hidden_size, num_heads)
    self.cross_norm = nn.LayerNorm(hidden_size)
    # Initialize the feed-forward network and layer norm
    self.feed_forward = nn.Sequential(
      nn.Linear(hidden_size, hidden_size * 4),
      nn.GELU(),
      nn.Linear(hidden_size * 4, hidden_size)
    )
    self.ff_norm = nn.LayerNorm(hidden_size)

  def forward(self, x, k, v, mask=None):
    # Apply masked self-attention and residual connection
    x1 = x + self.self_attention(x, x, x, mask)
    x1 = self.self_norm(x1)
    # Apply cross-attention and residual connection
    x2 = x1 + self.cross_attention(x1, k, v)
    x2 = self.cross_norm(x2)
    # Apply feed-forward network and residual connection
    x3 = x2 + self.feed_forward(x2)
    x3 = self.ff_norm(x3)
    return x3

# Define a multi-head attention layer
class MultiHeadAttention(nn.Module):
  def __init__(self, hidden_size, num_heads):
    super(MultiHeadAttention, self).__init__()
    assert hidden_size % num_heads == 0 
     # Check if hidden size is divisible by number of heads
    self.hidden_size = hidden_size
    self.num_heads = num_heads
    self.head_size = hidden_size // num_heads
    # Initialize the query, key and value projection layers
    self.q_proj = nn.Linear(hidden_size, hidden_size)
    self.k_proj = nn.Linear(hidden_size, hidden_size)
    self.v_proj = nn.Linear(hidden_size, hidden_size)
    # Initialize the output projection layer
    self.out_proj = nn.Linear(hidden_size, hidden_size)

  def forward(self, q, k, v, mask=None):
    # Get the batch size and sequence length
    batch_size, seq_len, _ = q.size()
    # Project the query, key and value tensors
    q = self.q_proj(q)
    k = self.k_proj(k)
    v = self.v_proj(v)
    # Reshape the tensors into [batch_size, num_heads, seq_len, head_size]
    q = q.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)
    k = k.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)
    v = v.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)
    # Compute the scaled dot-product attention scores
    scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.head_size)
    # Apply the mask if given
    if mask is not None:
      scores = scores.masked_fill(mask == 0, -1e9)
    # Apply the softmax function to get the attention weights
    weights = F.softmax(scores, dim=-1)
    # Compute the weighted sum of values
    output = torch.matmul(weights, v)
    # Reshape the output tensor into [batch_size, seq_len, hidden_size]
    output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
    # Project the output tensor to the original hidden size
    output = self.out_proj(output)
    return output

# Define a VQGAN quantizer
class VQGAN(nn.Module):
  def __init__(self):
    super(VQGAN, self).__init__()
    # Load a pre-trained VQGAN model from torchvision
    self.model = torchvision.models.vqgan(pretrained=True)
    # Freeze the model parameters
    for param in self.model.parameters():
      param.requires_grad = False

  def forward(self, image):
    # Resize the image to 256x256 pixels
    image = F.interpolate(image, size=(256, 256))
    # Encode the image into a latent code using the VQGAN encoder
    code = self.model.encode(image)
    # Quantize the latent code into a sequence of discrete tokens using the VQGAN codebook
    tokens = self.model.quantize(code)[0]
    return tokens

  def inverse(self, tokens):
     # Dequantize the tokens into a latent code using the VQGAN codebook
     code = self.model.dequantize(tokens)
     # Decode the latent code into an image using the VQGAN decoder
     image = self.model.decode(code)
     return image

# Define a function to mask some of the image tokens randomly
def mask_image_tokens(image_tokens):
  # Get the batch size and sequence length
  batch_size, seq_len = image_tokens.size()
  # Compute the probability of masking each token
  prob_mask = 0.15 * (torch.rand(batch_size * seq_len) < 0.15).float()
  # Compute the mask tensor by sampling masked tokens from a uniform distribution over the vocabulary
  mask = (torch.randint(0, vocab_size - 1 , (batch_size * seq_len)) * prob_mask).long()
  # Reshape the mask tensor into [batch_size, seq_len]
  mask = mask.view(batch_size ,seq_len)
  # Mask some of the image tokens randomly by replacing them with masked tokens
  masked_tokens = image_tokens * (1 - prob_mask.long()) + mask * prob_mask.long()
  return masked_tokens , mask

# Define a function to embed the image tokens into a hidden state
def embed(image_tokens):
  # Initialize an embedding layer with learnable weights
  embedding_layer = nn.Embedding(vocab_size , embed_size)
  # Embed the image tokens into a hidden state of size [batch_size ,seq_len ,embed _size]
  hidden_state = embedding_layer(image_tokens)
  return hidden_state

# Define a function to sample an initial image token sequence using top-k sampling
def top_k_sampling(model ,text):
   # Encode the text into a text embedding
   text_embedding = model.encoder(text)
   # Initialize an empty list to store the sampled tokens
   sampled_tokens = [