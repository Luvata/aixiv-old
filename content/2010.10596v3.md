---
title: 2010.10596v3 Counterfactual Explanations and Algorithmic Recourses for Machine Learning  A Review
date: 2020-10-11
---

# [Counterfactual Explanations and Algorithmic Recourses for Machine Learning: A Review](http://arxiv.org/abs/2010.10596v3)

authors: Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, Chirag Shah


## What, Why and How

[1]: https://arxiv.org/pdf/2010.10596v3.pdf "Counterfactual Explanations and Algorithmic Recourses for Machine ..."
[2]: https://arxiv.org/abs/2010.10596 "[2010.10596] Counterfactual Explanations and Algorithmic Recourses for ..."
[3]: http://export.arxiv.org/abs/2102.10596v3 "[2102.10596v3] Connecting multi-lepton anomalies at the LHC and in ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper reviews and categorizes research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a machine learning model been changed in a particular way.
- **Why**: The paper argues that counterfactual explanations are essential to the development of trustworthy machine learning based systems, especially in high-impact areas such as finance and healthcare, where they can help human stakeholders understand the relationship between the input and output of machine learning models and provide algorithmic recourses for undesired outcomes.
- **How**: The paper designs a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluates all currently proposed algorithms against that rubric. The paper also identifies gaps and discusses promising research directions in the space of counterfactual explainability.

## Main Contributions

According to the paper, the main contributions are:

- A comprehensive review and categorization of existing counterfactual explanation algorithms for machine learning models, covering both supervised and unsupervised learning settings.
- A rubric with desirable properties of counterfactual explanation algorithms, such as feasibility, sparsity, diversity, stability, and causality, and a systematic evaluation of the existing algorithms based on the rubric.
- A discussion of the challenges and opportunities in the field of counterfactual explainability, such as incorporating domain knowledge, handling complex data types, ensuring fairness and accountability, and integrating with human-in-the-loop systems.

## Method Summary

[1]: https://arxiv.org/pdf/2010.10596v3.pdf "Counterfactual Explanations and Algorithmic Recourses for Machine ..."
[2]: https://arxiv.org/abs/2010.10596 "[2010.10596] Counterfactual Explanations and Algorithmic Recourses for ..."
[3]: http://export.arxiv.org/abs/2102.10596v3 "[2102.10596v3] Connecting multi-lepton anomalies at the LHC and in ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper defines counterfactual explanations as statements of the form "if feature X had been x instead of x', then the outcome would have been y instead of y'", where x and x' are values of feature X, and y and y' are values of the outcome variable.
- The paper introduces a notation for counterfactual explanations, where CF(x) denotes the set of all possible counterfactuals for a given input x, and CF(x,y) denotes the subset of counterfactuals that result in a desired outcome y.
- The paper describes four main types of counterfactual explanation algorithms: optimization-based, distance-based, prototype-based, and model-based. Each type has its own advantages and disadvantages in terms of computational complexity, interpretability, diversity, and feasibility.
- The paper proposes a rubric with five desirable properties of counterfactual explanation algorithms: feasibility, sparsity, diversity, stability, and causality. Feasibility means that the counterfactuals are realistic and actionable; sparsity means that the counterfactuals involve minimal changes to the input; diversity means that the counterfactuals cover a range of possible scenarios; stability means that the counterfactuals are robust to small perturbations of the input; and causality means that the counterfactuals reflect causal relationships between features and outcomes.
- The paper evaluates 24 existing counterfactual explanation algorithms based on the rubric, using both synthetic and real-world datasets. The paper also provides a summary table that compares the algorithms along various dimensions, such as data type, model type, problem type, and rubric properties.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define counterfactual explanations as statements of the form
# "if feature X had been x instead of x', then the outcome would have been y instead of y'"
# where x and x' are values of feature X, and y and y' are values of the outcome variable

# Define CF(x) as the set of all possible counterfactuals for a given input x
# Define CF(x,y) as the subset of counterfactuals that result in a desired outcome y

# Define a rubric with five desirable properties of counterfactual explanation algorithms:
# feasibility, sparsity, diversity, stability, and causality

# Define four main types of counterfactual explanation algorithms:
# optimization-based, distance-based, prototype-based, and model-based

# For each type of algorithm, do the following:

  # Define the objective function and the constraints for generating counterfactuals
  # Define the algorithm steps and the parameters
  # Evaluate the algorithm on synthetic and real-world datasets using the rubric
  # Compare the algorithm with other algorithms in the same type and across types

# Summarize the results and discuss the strengths and weaknesses of each type of algorithm
# Identify gaps and suggest future research directions in the field of counterfactual explainability
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Define counterfactual explanations as statements of the form
# "if feature X had been x instead of x', then the outcome would have been y instead of y'"
# where x and x' are values of feature X, and y and y' are values of the outcome variable

# Define CF(x) as the set of all possible counterfactuals for a given input x
# Define CF(x,y) as the subset of counterfactuals that result in a desired outcome y

# Define a rubric with five desirable properties of counterfactual explanation algorithms:
# feasibility, sparsity, diversity, stability, and causality

# Define four main types of counterfactual explanation algorithms:
# optimization-based, distance-based, prototype-based, and model-based

# For each type of algorithm, do the following:

  # Optimization-based algorithms:
    # Define the objective function as minimizing the distance between the input and the counterfactual,
    # subject to the constraints that the counterfactual belongs to CF(x,y) and satisfies some predefined rules
    # Define the distance metric as either L1-norm, L2-norm, or weighted norm
    # Define the rules as either hard constraints or soft constraints with penalty terms
    # Define the algorithm steps as follows:
      # Initialize the counterfactual as the input
      # Repeat until convergence or maximum iterations:
        # Solve the optimization problem using gradient descent or other methods
        # Update the counterfactual with the optimal solution
      # Return the counterfactual
    # Define the parameters as the distance metric, the rules, the optimization method, and the convergence criteria
    # Evaluate the algorithm on synthetic and real-world datasets using the rubric
    # Compare the algorithm with other algorithms in the same type and across types

  # Distance-based algorithms:
    # Define the objective function as finding the nearest neighbor of the input in CF(x,y)
    # Define the distance metric as either L1-norm, L2-norm, or weighted norm
    # Define the algorithm steps as follows:
      # Initialize a candidate set of counterfactuals as empty
      # For each feature in the input:
        # Generate a set of values for that feature that satisfy CF(x,y) and some predefined rules
        # For each value in that set:
          # Replace the feature value in the input with that value
          # Add the modified input to the candidate set
      # Compute the distance between each candidate and the input using the distance metric
      # Return the candidate with the minimum distance as the counterfactual
    # Define the parameters as the distance metric, the rules, and the value generation method
    # Evaluate the algorithm on synthetic and real-world datasets using the rubric
    # Compare the algorithm with other algorithms in the same type and across types

  # Prototype-based algorithms:
    # Define the objective function as finding a prototype of CF(x,y) that is closest to the input
    # Define a prototype as a representative instance of a class or cluster of data points
    # Define the distance metric as either L1-norm, L2-norm, or weighted norm
    # Define the algorithm steps as follows:
      # Preprocess the data by applying feature selection, dimensionality reduction, normalization, etc.
      # Cluster or classify the data into different groups based on their outcomes and features
      # Compute or select a prototype for each group using methods such as k-means, medoids, centroids, etc.
      # Find the group that has a different outcome from the input and has a prototype closest to it using the distance metric
      # Return that prototype as the counterfactual
    # Define the parameters as the distance metric, the preprocessing method, and 
the clustering or classification method
    # Evaluate 
the algorithm on synthetic and real-world datasets using 
the rubric
    # Compare 
the algorithm with other algorithms in 
the same type and across types

  # Model-based algorithms:
    # Define 
the objective function as finding a counterfactual that maximizes 
the probability of 
the desired outcome given 
the input and 
the model parameters
    # Define 
the model parameters as either learned from data using methods such as logistic regression, neural networks, etc., or specified by domain experts using methods such as Bayesian networks, decision trees, etc.
    # Define 
the algorithm steps as follows:
      # Given an input and a desired outcome, compute 
the posterior probability of 
the outcome given 
the input and 
the model parameters using methods such as Bayes' rule, inference algorithms, etc.
      # If 
the posterior probability is above a threshold, return 
the input as 
the counterfactual
      # Otherwise, modify 
the input by changing one or more feature values that have 
the highest influence on 
the outcome using methods such as sensitivity analysis, feature importance, etc.
      # Repeat until 
the posterior probability is above 
the threshold or maximum iterations
      # Return 
the modified input as 
the counterfactual
    # Define 
the parameters as 
the model parameters, 
the posterior probability computation method, 
the threshold, and 
the feature modification method
    # Evaluate 
the algorithm on synthetic and real-world datasets using 
the rubric
    # Compare 
the algorithm with other algorithms in 
the same type and across types

# Summarize the results and discuss the strengths and weaknesses of each type of algorithm
# Identify gaps and suggest future research directions in the field of counterfactual explainability
```