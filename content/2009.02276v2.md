---
title: 2009.02276v2 Witches' Brew  Industrial Scale Data Poisoning via Gradient Matching
date: 2020-09-03
---

# [Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching](http://arxiv.org/abs/2009.02276v2)

authors: Jonas Geiping, Liam Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, Tom Goldstein


## What, Why and How

[1]: https://arxiv.org/abs/2009.02276v2 "[2009.02276v2] Witches' Brew: Industrial Scale Data Poisoning via ..."
[2]: https://arxiv.org/pdf/2009.02276.pdf "arXiv:2009.02276v2 [cs.CV] 10 May 2021"
[3]: https://arxiv.org/pdf/2003.02276v2.pdf "Observable Gravitational Waves in Minimal Scotogenic Model - arXiv.org"
[4]: http://arxiv-export3.library.cornell.edu/abs/2009.02276 "[2009.02276] Witches' Brew: Industrial Scale Data Poisoning via ..."

Here is a summary of the paper:

**What**: The paper presents a new data poisoning attack that can cause targeted misclassification in deep neural networks trained from scratch on large datasets.

**Why**: The paper aims to demonstrate the threat of data poisoning to real-world applications that rely on large training datasets of unknown origin and quality.

**How**: The paper proposes a method that matches the gradient direction of malicious examples with the target image, and inserts them into the training dataset with clean labels. The paper analyzes the effectiveness and limitations of this method, and compares it with existing poisoning attacks and defenses.

## Main Contributions

The paper claims to make the following contributions:

- It introduces a new data poisoning attack that is both from scratch and clean label, meaning it works against new, randomly initialized models and is nearly imperceptible to humans.
- It shows that the attack is the first to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.
- It analyzes the mechanism and practical considerations of the attack, and provides theoretical and empirical evidence for its success.
- It demonstrates the limitations of existing defensive strategies against the attack, and suggests possible directions for future research.

## Method Summary

The method section of the paper describes the proposed data poisoning attack in detail. The attack consists of three steps:

1. Select a target image that the attacker wants to misclassify, and a target class that the attacker wants to assign to the image.
2. Generate poisoned images by adding small perturbations to some images from the dataset that belong to the target class. The perturbations are chosen to match the gradient direction of the target image with respect to a surrogate model.
3. Insert the poisoned images into the dataset with clean labels, and train a victim model on the poisoned dataset.

The paper also discusses some practical aspects of the attack, such as how to choose the surrogate model, how to optimize the perturbations, and how to evaluate the attack success.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: target image x_t, target class c_t, dataset D, fraction of poisoned images f
# Output: poisoned dataset D_p

# Step 1: Select a target image and a target class
x_t = choose_target_image()
c_t = choose_target_class()

# Step 2: Generate poisoned images
D_p = [] # initialize empty list for poisoned images
n = int(f * len(D)) # number of images to poison
S = train_surrogate_model(D) # train a surrogate model on the original dataset
for i in range(n):
  x_i = sample_image_from_class(D, c_t) # sample an image from the target class
  p_i = optimize_perturbation(S, x_i, x_t) # optimize a perturbation to match the gradient direction of the target image
  x_p = x_i + p_i # add the perturbation to the image
  D_p.append((x_p, c_t)) # append the poisoned image and the clean label to the list

# Step 3: Insert the poisoned images into the dataset
D_p = D_p + D # concatenate the poisoned and original images
shuffle(D_p) # shuffle the dataset
return D_p # return the poisoned dataset
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: target image x_t, target class c_t, dataset D, fraction of poisoned images f
# Output: poisoned dataset D_p

# Step 1: Select a target image and a target class
x_t = choose_target_image() # choose any image from the dataset that is not in the target class
c_t = choose_target_class() # choose any class that is different from the true label of the target image

# Step 2: Generate poisoned images
D_p = [] # initialize empty list for poisoned images
n = int(f * len(D)) # number of images to poison
S = train_surrogate_model(D) # train a surrogate model on the original dataset using any standard architecture and optimizer
for i in range(n):
  x_i = sample_image_from_class(D, c_t) # sample an image from the target class uniformly at random
  p_i = optimize_perturbation(S, x_i, x_t) # optimize a perturbation to match the gradient direction of the target image using Algorithm 1
  x_p = clip(x_i + p_i, 0, 1) # add the perturbation to the image and clip it to the valid range
  D_p.append((x_p, c_t)) # append the poisoned image and the clean label to the list

# Step 3: Insert the poisoned images into the dataset
D_p = D_p + D # concatenate the poisoned and original images
shuffle(D_p) # shuffle the dataset using a random seed
return D_p # return the poisoned dataset

# Algorithm 1: Perturbation optimization
# Input: surrogate model S, base image x_i, target image x_t
# Output: perturbation p_i
p_i = initialize_perturbation() # initialize a small random perturbation
alpha = set_step_size() # set a fixed step size for gradient ascent
beta = set_momentum() # set a momentum parameter for gradient ascent
v = initialize_velocity() # initialize a zero velocity vector for gradient ascent
for t in range(T): # iterate for T steps or until convergence
  g_i = compute_gradient(S, x_i + p_i, c_t) # compute the gradient of the surrogate model loss with respect to the poisoned image and the target class label
  g_t = compute_gradient(S, x_t, c_t) # compute the gradient of the surrogate model loss with respect to the target image and the target class label
  d = normalize(g_t - g_i) # compute and normalize the gradient direction difference
  v = beta * v + alpha * d # update the velocity vector using momentum
  p_i = p_i + v # update the perturbation using gradient ascent
return p_i # return the perturbation
```