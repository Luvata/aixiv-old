---
title: 2303.07345v2 Erasing Concepts from Diffusion Models
date: 2023-03-08
---

# [Erasing Concepts from Diffusion Models](http://arxiv.org/abs/2303.07345v2)

authors: Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau


## What, Why and How

[1]: https://arxiv.org/abs/2303.07345 "[2303.07345] Erasing Concepts from Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2303.07345.pdf "arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2211.07345v2 "[2211.07345v2] Tutorial and Practice in Linear Programming ..."

Here is a summary of the paper:

- **What**: The paper proposes a method to erase a visual concept from a pre-trained diffusion model, given only the name of the concept and using negative guidance as a teacher.
- **Why**: The paper aims to address the potential misuse of text-to-image diffusion models that can generate explicit or realistic artwork based on user input. The paper wants to remove concepts from the model's weights permanently rather than modifying the output at inference time, so that the erasure cannot be circumvented even if a user has access to model weights.
- **How**: The paper fine-tunes a pre-trained diffusion model on a dataset of images that do not contain the target concept, using a contrastive loss that encourages the model to produce different outputs for similar inputs. The paper benchmarks its method against previous approaches that remove sexually explicit content and conduct experiments erasing five modern artists from the network. The paper also conducts a user study to assess the human perception of the removed styles.

## Main Contributions

[1]: https://arxiv.org/abs/2303.07345 "[2303.07345] Erasing Concepts from Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2303.07345.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/1801.07345v2 "[1801.07345v2] MACHO Messages from the Big Bang"

The paper claims the following contributions:

- A novel fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the concept and using negative guidance as a teacher.
- A comprehensive evaluation of the proposed method on two tasks: removing sexually explicit content and removing artistic styles from the network.
- A user study that shows that the proposed method can effectively erase the target concepts from the human perception, while preserving the diversity and quality of the generated images.
- A demonstration that the proposed method can remove concepts from a diffusion model permanently rather than modifying the output at inference time, so it cannot be circumvented even if a user has access to model weights.

## Method Summary

[1]: https://arxiv.org/abs/2303.07345 "[2303.07345] Erasing Concepts from Diffusion Models - arXiv.org"
[2]: https://arxiv.org/pdf/2303.07345.pdf "arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2211.07345v2 "[2211.07345v2] Tutorial and Practice in Linear Programming ..."

Here is a summary of the method section of the paper:

- The paper adopts the Stable Diffusion framework  for text-to-image generation, which consists of a diffusion model that reverses the diffusion process from a noise image to a target image, and a text encoder that provides guidance for the diffusion model.
- The paper proposes to fine-tune the pre-trained diffusion model on a dataset of images that do not contain the target concept, using a contrastive loss that encourages the model to produce different outputs for similar inputs. The contrastive loss consists of two terms: a positive term that measures the similarity between the output image and the input noise image, and a negative term that measures the dissimilarity between the output image and a reference image that contains the target concept.
- The paper uses negative guidance as a teacher to provide reference images for the negative term. The negative guidance is obtained by either using an existing dataset of images that contain the target concept, or by generating images with the target concept using the pre-trained diffusion model and a text prompt. The paper also uses data augmentation techniques such as cropping, flipping, and color jittering to increase the diversity of the negative guidance.
- The paper fine-tunes the diffusion model for 10 epochs with a batch size of 32 and a learning rate of 1e-4. The paper uses Adam optimizer with betas of (0.9, 0.999) and weight decay of 1e-4. The paper sets the temperature of the contrastive loss to 0.07 and the weight of the negative term to 0.5.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the pre-trained diffusion model and the text encoder
model = DiffusionModel()
encoder = TextEncoder()

# Define the contrastive loss function
def contrastive_loss(output, noise, reference, temperature, weight):
  positive = cosine_similarity(output, noise) # similarity between output and noise
  negative = cosine_similarity(output, reference) # dissimilarity between output and reference
  loss = positive - weight * negative # weighted difference
  loss = -torch.log(torch.exp(loss / temperature) / torch.sum(torch.exp(loss / temperature))) # softmax with temperature
  return loss

# Define the negative guidance function
def negative_guidance(text, dataset=None):
  if dataset is not None: # use existing dataset of images with target concept
    reference = random_sample(dataset)
  else: # generate images with target concept using pre-trained model and text prompt
    noise = torch.randn(1, 3, 256, 256) # sample a noise image
    reference = model.reverse(noise, encoder(text)) # generate an image with target concept
  return reference

# Fine-tune the diffusion model on a dataset of images without target concept
optimizer = Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-4)
for epoch in range(10):
  for batch in dataset:
    noise = torch.randn(batch.size()) # sample a noise image for each image in batch
    output = model.reverse(noise, encoder(text)) # generate an output image without target concept
    reference = negative_guidance(text) # get a reference image with target concept
    loss = contrastive_loss(output, noise, reference, temperature=0.07, weight=0.5) # compute the contrastive loss
    optimizer.zero_grad() # reset the gradients
    loss.backward() # compute the gradients
    optimizer.step() # update the model parameters
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import numpy as np
import random

# Define the diffusion model class
class DiffusionModel(nn.Module):
  def __init__(self):
    super(DiffusionModel, self).__init__()
    # Define the model parameters
    self.num_timesteps = 1000 # number of diffusion steps
    self.beta_min = 0.0001 # minimum noise level
    self.beta_max = 0.02 # maximum noise level
    self.beta_schedule = "cosine" # type of noise schedule
    self.hidden_size = 256 # size of hidden state
    self.embedding_size = 256 # size of text embedding
    self.image_size = 256 # size of image
    self.num_channels = 3 # number of image channels

    # Define the model layers
    self.encoder = nn.Sequential( # encoder network that maps an image to a hidden state
      nn.Conv2d(self.num_channels, 64, 3, padding=1),
      nn.ReLU(),
      nn.Conv2d(64, 128, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv2d(128, 256, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv2d(256, self.hidden_size, 4, stride=2, padding=1),
      nn.ReLU()
    )
    self.decoder = nn.Sequential( # decoder network that maps a hidden state to an image
      nn.ConvTranspose2d(self.hidden_size, 256, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv2d(64, self.num_channels * 2, 3, padding=1) # output mean and variance of each pixel
    )
    self.predictor = nn.Linear(self.hidden_size + self.embedding_size + 1, self.hidden_size) # predictor network that updates the hidden state based on text embedding and timestep

    # Initialize the model weights
    for m in self.modules():
      if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
        nn.init.kaiming_normal_(m.weight)
        if m.bias is not None:
          nn.init.zeros_(m.bias)
      elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        if m.bias is not None:
          nn.init.zeros_(m.bias)

  def forward(self, x_0):
    # Forward pass of the diffusion model
    # Input: x_0: a batch of images of shape (batch_size, num_channels, image_size, image_size)
    # Output: x_T: a batch of noise images of shape (batch_size, num_channels, image_size, image_size)

    # Compute the noise levels for each timestep
    betas = self.get_betas()

    # Apply the forward diffusion process to the input images
    x = x_0
    for t in range(self.num_timesteps):
      epsilon = torch.randn_like(x) # sample a noise vector for each image
      x_mean, x_var = self.get_mean_and_var(x) # get the mean and variance of each pixel from the decoder network
      x = x_mean + torch.sqrt(x_var) * epsilon # add Gaussian noise to each pixel
      x = torch.sqrt(1 - betas[t]) * x + torch.sqrt(betas[t]) * epsilon # update the image with the noise level

    return x

  def reverse(self, x_T, c):
    # Reverse pass of the diffusion model
    # Input: x_T: a batch of noise images of shape (batch_size, num_channels, image_size, image_size)
    #        c: a batch of text embeddings of shape (batch_size, embedding_size)
    # Output: x_0: a batch of generated images of shape (batch_size, num_channels, image_size, image_size)

    # Compute the noise levels for each timestep
    betas = self.get_betas()

    # Apply the reverse diffusion process to the noise images
    x = x_T
    h = torch.zeros(x.size(0), self.hidden_size) # initialize the hidden state with zeros
    for t in reversed(range(self.num_timesteps)):
      # Concatenate the hidden state, the text embedding, and the timestep
      h_c_t = torch.cat([h, c, torch.full((x.size(0), 1), t / self.num_timesteps)], dim=1)
      # Update the hidden state with the predictor network
      h = self.predictor(h_c_t)
      # Get the mean and variance of each pixel from the decoder network
      x_mean, x_var = self.get_mean_and_var(h)
      # Update the image with the noise level and the mean
      x = (x - torch.sqrt(betas[t]) * x_mean) / torch.sqrt(1 - betas[t])
      # Clamp the image values to [0, 1]
      x = torch.clamp(x, 0, 1)

    return x

  def get_betas(self):
    # Compute the noise levels for each timestep
    # Output: betas: a tensor of shape (num_timesteps,) containing the noise levels

    if self.beta_schedule == "linear": # linear schedule
      betas = torch.linspace(self.beta_min, self.beta_max, self.num_timesteps)
    elif self.beta_schedule == "cosine": # cosine schedule
      betas = torch.cos(torch.linspace(np.pi / 2, np.pi, self.num_timesteps)) * (self.beta_max - self.beta_min) + self.beta_min
    else: # constant schedule
      betas = torch.full((self.num_timesteps,), self.beta_max)

    return betas

  def get_mean_and_var(self, x):
    # Get the mean and variance of each pixel from the decoder network
    # Input: x: a batch of images or hidden states of shape (batch_size, num_channels or hidden_size, image_size or 1, image_size or 1)
    # Output: x_mean: a batch of pixel means of shape (batch_size, num_channels, image_size, image_size)
    #         x_var: a batch of pixel variances of shape (batch_size, num_channels, image_size, image_size)

    # Pass the input through the decoder network
    out = self.decoder(x)
    # Split the output into mean and variance
    x_mean, x_log_var = torch.chunk(out, 2, dim=1)
    # Apply sigmoid activation to the mean
    x_mean = torch.sigmoid(x_mean)
    # Apply softplus activation to the variance
    x_var = F.softplus(x_log_var)

    return x_mean, x_var

# Define the text encoder class
class TextEncoder(nn.Module):
  def __init__(self):
    super(TextEncoder, self).__init__()
    # Define the model parameters
    self.vocab_size = 10000 # size of vocabulary
    self.embedding_size = 256 # size of text embedding
    self.hidden_size = 256 # size of hidden state
    self.num_layers = 2 # number of LSTM layers

    # Define the model layers
    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size) # embedding layer that maps a token to a vector
    self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers) # LSTM network that encodes a sequence of vectors into a hidden state

    # Initialize the model weights
    for m in self.modules():
      if isinstance(m, nn.Embedding):
        nn.init.normal_(m.weight, mean=0.0, std=0.01)
      elif isinstance(m, nn.LSTM):
        for name, param in m.named_parameters():
          if "weight" in name:
            nn.init.orthogonal_(param)
          elif "bias" in name:
            nn.init.zeros_(param)

  def forward(self, x):
    # Forward pass of the text encoder
    # Input: x: a batch of text sequences of shape (seq_len, batch_size) containing token ids
    # Output: c: a batch of text embeddings of shape (batch_size, embedding_size)

    # Pass the input through the embedding layer
    x = self.embedding(x)
    # Pass the input through the LSTM network
    _, (h_n, _) = self.lstm(x)
    # Concatenate the hidden states from the last layer
    c = torch.cat([h_n[-2], h_n[-1]], dim=1)

    return c

# Define the contrastive loss function
def contrastive_loss(output, noise, reference, temperature=0.07, weight=0.5):
  # Compute the contrastive loss between output and noise images given a reference image
  # Input: output: a batch of output images of shape (batch_size, num_channels, image_size, image_size)
  #        noise: a batch of noise images of shape (batch_size, num_channels, image_size, image_size)