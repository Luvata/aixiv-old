---
title: 2206.00477v1 Anti-Forgery  Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations
date: 2022-06-01
---

# [Anti-Forgery: Towards a Stealthy and Robust DeepFake Disruption Attack via Adversarial Perceptual-aware Perturbations](http://arxiv.org/abs/2206.00477v1)

authors: Run Wang, Ziheng Huang, Zhikai Chen, Li Liu, Jing Chen, Lina Wang


## What, Why and How

[1]: https://arxiv.org/pdf/2206.00477v1.pdf "Anti-Forgery: Towards a Stealthy and Robust DeepFake ... - arXiv.org"
[2]: https://arxiv.org/abs/2206.00477 "[2206.00477] Anti-Forgery: Towards a Stealthy and Robust DeepFake ..."
[3]: https://arxiv-export3.library.cornell.edu/abs/2206.00477 "[2206.00477] Anti-Forgery: Towards a Stealthy and Robust DeepFake ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel anti-forgery technique that helps users protect their facial images from being manipulated by DeepFake techniques. The technique generates perceptual-aware perturbations that are stealthy to human eyes but disruptive to DeepFake models.
- **Why**: The paper aims to address the limitation of the existing proactive defense techniques that add adversarial noises to the source data, which are not robust and can be easily bypassed by image reconstruction methods. The paper also aims to provide a proactive and robust countermeasure against DeepFake threats that can harm individual privacy and political security.
- **How**: The paper investigates the vulnerability of the existing forgery techniques and designs a perceptual-aware adversarial attack that exploits the perceptual difference between human vision and DeepFake models. The paper evaluates the proposed technique on various DeepFake datasets and scenarios, and compares it with the state-of-the-art methods. The paper demonstrates that the proposed technique can effectively disrupt the DeepFake manipulation while preserving the visual quality of the original images. The paper also shows that the proposed technique is robust to diverse image transformations and evasion techniques, especially MagDR, a recent image reconstruction method.

## Main Contributions

[1]: https://arxiv.org/pdf/2206.00477v1.pdf "Anti-Forgery: Towards a Stealthy and Robust DeepFake ... - arXiv.org"
[2]: https://arxiv.org/abs/2206.00477 "[2206.00477] Anti-Forgery: Towards a Stealthy and Robust DeepFake ..."
[3]: https://arxiv-export3.library.cornell.edu/abs/2206.00477 "[2206.00477] Anti-Forgery: Towards a Stealthy and Robust DeepFake ..."

According to the paper[^1^][1], the main contributions are:

- The paper proposes a novel anti-forgery technique that generates perceptual-aware perturbations to disrupt the DeepFake manipulation in a stealthy and robust manner.
- The paper investigates the vulnerability of the existing forgery techniques and reveals that they are susceptible to perceptual-aware adversarial attack.
- The paper evaluates the proposed technique on various DeepFake datasets and scenarios, and demonstrates its effectiveness and robustness against diverse image transformations and evasion techniques, especially MagDR, a recent image reconstruction method.
- The paper opens up a new research direction for protecting facial images against DeepFake threats in a proactive and robust manner.

## Method Summary

[1]: https://arxiv.org/pdf/2206.00477v1.pdf "Anti-Forgery: Towards a Stealthy and Robust DeepFake ... - arXiv.org"
[2]: https://arxiv.org/abs/2206.00477 "[2206.00477] Anti-Forgery: Towards a Stealthy and Robust DeepFake ..."
[3]: https://arxiv-export3.library.cornell.edu/abs/2206.00477 "[2206.00477] Anti-Forgery: Towards a Stealthy and Robust DeepFake ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces the problem formulation and the threat model of the anti-forgery technique. The paper assumes that the attacker has access to the source facial images and can apply any DeepFake technique to manipulate them. The paper also assumes that the defender can add perceptual-aware perturbations to the source images before sharing them online, and that the perturbations are imperceptible to human eyes but disruptive to DeepFake models.
- The paper proposes a perceptual-aware adversarial attack framework that consists of three modules: a perceptual-aware perturbation generator, a perceptual-aware perturbation evaluator, and a perceptual-aware perturbation updater. The paper describes the objective functions and the optimization algorithms of each module.
- The paper implements the proposed framework using PyTorch and experiments with various DeepFake techniques, such as Face2Face, FaceSwap, DeepFakes, and NeuralTextures. The paper also experiments with various image transformations and evasion techniques, such as cropping, resizing, compression, blurring, and MagDR. The paper uses several metrics to evaluate the performance of the proposed technique, such as visual quality, disruption rate, robustness score, and perceptual similarity.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a source facial image x
# Output: a perturbed facial image x'
# Parameters: a perceptual-aware perturbation generator G, a perceptual-aware perturbation evaluator E, a perceptual-aware perturbation updater U, a DeepFake model F, a perceptual similarity metric P

# Initialize the perturbation delta to zero
delta = 0

# Repeat until convergence or maximum iterations
while not converged or not max_iter:

  # Generate the perturbed image x' by adding delta to x
  x' = x + delta

  # Evaluate the disruption rate and the perceptual similarity of x'
  d = E(x', F) # disruption rate is the probability that F fails to manipulate x'
  p = P(x', x) # perceptual similarity is the distance between x' and x in a perceptual space

  # Update the perturbation delta by minimizing the objective function
  delta = U(delta, G, d, p) # objective function is a weighted combination of d and p

# Return the perturbed image x'
return x'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a source facial image x
# Output: a perturbed facial image x'
# Parameters: a perceptual-aware perturbation generator G, a perceptual-aware perturbation evaluator E, a perceptual-aware perturbation updater U, a DeepFake model F, a perceptual similarity metric P, a learning rate alpha, a weight lambda

# Initialize the perturbation delta to zero
delta = 0

# Repeat until convergence or maximum iterations
while not converged or not max_iter:

  # Generate the perturbed image x' by adding delta to x
  x' = x + delta

  # Evaluate the disruption rate and the perceptual similarity of x'
  d = E(x', F) # disruption rate is the probability that F fails to manipulate x'
  p = P(x', x) # perceptual similarity is the distance between x' and x in a perceptual space

  # Define the objective function L as a weighted combination of d and p
  L = -d + lambda * p

  # Compute the gradient of L with respect to delta
  grad = dL / ddelta

  # Update the perturbation delta by applying gradient descent
  delta = delta - alpha * grad

  # Clip the perturbation delta to a small range to ensure imperceptibility
  delta = clip(delta, -epsilon, epsilon)

# Return the perturbed image x'
return x'
```