---
title: 1907.01108v2 Language2Pose  Natural Language Grounded Pose Forecasting
date: 2019-07-02
---

# [Language2Pose: Natural Language Grounded Pose Forecasting](http://arxiv.org/abs/1907.01108v2)

authors: Chaitanya Ahuja, Louis-Philippe Morency


## What, Why and How

[1]: https://arxiv.org/pdf/1907.01108 "Language2Pose: Natural Language Grounded Pose Forecasting - arXiv.org"
[2]: http://export.arxiv.org/abs/1907.01108v2 "[1907.01108v2] Language2Pose: Natural Language Grounded Pose Forecasting"
[3]: https://arxiv.org/pdf/1907.01108v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper introduces a neural architecture called **Joint Language-to-Pose (or JL2P)**, which learns a joint embedding of language and pose, and can generate animations from natural language sentences[^1^][1].
- **Why**: The paper aims to address the multimodal problem of mapping linguistic concepts to motion animations, which can have applications in movie script visualization, virtual human animation, robot motion planning and more[^1^][1].
- **How**: The paper proposes a model that uses a **curriculum learning approach** to learn a joint embedding space of language and pose, where both modalities can be mapped and decoded. The model consists of two encoders (one for language and one for pose) and one decoder (for pose). The model is trained on a publicly available corpus of 3D pose data and human-annotated sentences[^1^][1].

## Main Contributions

[1]: https://arxiv.org/pdf/1907.01108 "Language2Pose: Natural Language Grounded Pose Forecasting - arXiv.org"
[2]: http://export.arxiv.org/abs/1907.01108v2 "[1907.01108v2] Language2Pose: Natural Language Grounded Pose Forecasting"
[3]: https://arxiv.org/pdf/1907.01108v2.pdf "arXiv.org e-Print archive"

According to the paper, the main contributions are:

- **A novel neural architecture** called JL2P that learns a joint embedding of language and pose, and can generate animations from natural language sentences[^1^][1].
- **A curriculum learning approach** that trains the model end-to-end using shorter and easier sequences first before moving to longer and harder ones[^1^][1].
- **A publicly available corpus** of 3D pose data and human-annotated sentences, which can be used for future research on language-to-pose generation[^1^][1].
- **An extensive evaluation** of the proposed model using both objective metrics and human judgment, which shows that the model outperforms other data-driven approaches in terms of accuracy and visual quality[^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/1907.01108 "Language2Pose: Natural Language Grounded Pose Forecasting"
[2]: https://arxiv.org/pdf/1907.01203v2.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2102.01108v2 "[2102.01108v2] Polarization singularities and M\\\"{o}bius strips in ..."

Here is a summary of the method section of the paper:

- The paper proposes a model called **JL2P** that consists of two encoders (one for language and one for pose) and one decoder (for pose)[^1^][1].
- The language encoder is a **bidirectional LSTM** that takes a sentence as input and outputs a hidden state vector[^1^][1].
- The pose encoder is a **convolutional LSTM** that takes a sequence of poses as input and outputs a hidden state vector[^1^][1].
- The pose decoder is also a **convolutional LSTM** that takes a hidden state vector as input and outputs a sequence of poses[^1^][1].
- The model learns a joint embedding space of language and pose by minimizing the **mean squared error** between the hidden state vectors of the language encoder and the pose encoder[^1^][1].
- The model uses a **curriculum learning approach** that trains the model on shorter and easier sequences first before moving to longer and harder ones[^1^][1].
- The model is trained on a corpus of 3D pose data and human-annotated sentences, which are aligned using **dynamic time warping**[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the model parameters
num_joints = 21 # number of joints in a pose
num_frames = 50 # number of frames in a sequence
hidden_size = 256 # size of the hidden state vector
kernel_size = 3 # size of the convolutional kernel
learning_rate = 0.001 # learning rate for optimization
batch_size = 32 # batch size for training
max_length = 20 # maximum length of a sentence

# Define the model components
language_encoder = BidirectionalLSTM(input_size=num_words, hidden_size=hidden_size)
pose_encoder = ConvolutionalLSTM(input_size=num_joints*3, hidden_size=hidden_size, kernel_size=kernel_size)
pose_decoder = ConvolutionalLSTM(input_size=hidden_size, output_size=num_joints*3, kernel_size=kernel_size)

# Define the loss function
loss_function = MeanSquaredError()

# Define the optimizer
optimizer = Adam(learning_rate=learning_rate)

# Define the curriculum
curriculum = [10, 20, 30, 40, 50] # sequence lengths to train on

# Load the data
data = load_data("corpus.txt")

# Train the model
for length in curriculum:
  for batch in data:
    # Get the input and output sequences
    input_sentence = batch["sentence"]
    input_pose = batch["pose"][:length]
    output_pose = batch["pose"][length:]

    # Encode the input sentence
    language_hidden_state = language_encoder(input_sentence)

    # Encode the input pose
    pose_hidden_state = pose_encoder(input_pose)

    # Compute the joint embedding loss
    joint_embedding_loss = loss_function(language_hidden_state, pose_hidden_state)

    # Decode the output pose
    predicted_pose = pose_decoder(pose_hidden_state)

    # Compute the pose generation loss
    pose_generation_loss = loss_function(output_pose, predicted_pose)

    # Compute the total loss
    total_loss = joint_embedding_loss + pose_generation_loss

    # Update the model parameters
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import nltk
import fastdtw

# Define the model parameters
num_joints = 21 # number of joints in a pose
num_frames = 50 # number of frames in a sequence
hidden_size = 256 # size of the hidden state vector
kernel_size = 3 # size of the convolutional kernel
learning_rate = 0.001 # learning rate for optimization
batch_size = 32 # batch size for training
max_length = 20 # maximum length of a sentence
num_epochs = 100 # number of epochs for training

# Define the model components
class BidirectionalLSTM(nn.Module):
  def __init__(self, input_size, hidden_size):
    super(BidirectionalLSTM, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True)

  def forward(self, x):
    # x is a tensor of shape (batch_size, max_length, input_size)
    # output is a tensor of shape (batch_size, max_length, hidden_size*2)
    # hidden_state is a tuple of tensors of shape (2, batch_size, hidden_size)
    output, hidden_state = self.lstm(x)
    # concatenate the last hidden states from both directions
    # final_hidden_state is a tensor of shape (batch_size, hidden_size*2)
    final_hidden_state = torch.cat((hidden_state[0][0], hidden_state[0][1]), dim=1)
    return final_hidden_state

class ConvolutionalLSTM(nn.Module):
  def __init__(self, input_size, hidden_size, output_size=None, kernel_size=3):
    super(ConvolutionalLSTM, self).__init__()
    self.input_size = input_size
    self.hidden_size = hidden_size
    self.output_size = output_size or input_size
    self.kernel_size = kernel_size
    self.padding = kernel_size // 2 # to preserve the sequence length
    self.conv_i = nn.Conv1d(in_channels=input_size+hidden_size, out_channels=hidden_size, kernel_size=kernel_size, padding=self.padding) # input gate
    self.conv_f = nn.Conv1d(in_channels=input_size+hidden_size, out_channels=hidden_size, kernel_size=kernel_size, padding=self.padding) # forget gate
    self.conv_c = nn.Conv1d(in_channels=input_size+hidden_size, out_channels=hidden_size, kernel_size=kernel_size, padding=self.padding) # cell gate
    self.conv_o = nn.Conv1d(in_channels=input_size+hidden_size, out_channels=hidden_size, kernel_size=kernel_size, padding=self.padding) # output gate
    self.conv_out = nn.Conv1d(in_channels=hidden_size, out_channels=output_size, kernel_size=kernel_size, padding=self.padding) # output layer

  def forward(self, x):
    # x is a tensor of shape (batch_size, sequence_length, input_size)
    batch_size, sequence_length, _ = x.size()
    # transpose x to have shape (batch size, input size, sequence length)
    x = x.transpose(1, 2)
    # initialize the hidden state and cell state to zeros
    h = torch.zeros(batch_size, self.hidden_size, sequence_length).to(x.device)
    c = torch.zeros(batch_size, self.hidden_size, sequence_length).to(x.device)
    # initialize the output tensor to zeros
    y = torch.zeros(batch.size(), sequence_length. self.output.size()).to(x.device)
    # loop over the sequence length
    for t in range(sequence_length):
      # concatenate the input and hidden state along the channel dimension
      # x_t is a tensor of shape (batch size. input size + hidden size. sequence length)
      x_t = torch.cat((x[:, :, t], h), dim=1)
      # compute the gates using convolutional layers and sigmoid activation
      # i_t. f_t. o_t are tensors of shape (batch size. hidden size. sequence length)
      i_t = torch.sigmoid(self.conv_i(x_t))
      f_t = torch.sigmoid(self.conv_f(x_t))
      o_t = torch.sigmoid(self.conv_o(x_t))
      # compute the cell state using convolutional layer and tanh activation
      # c_t is a tensor of shape (batch size. hidden size. sequence length)
      c_t = f_t * c + i_t * torch.tanh(self.conv_c(x_t))
      # compute the hidden state using the output gate and tanh activation
      # h_t is a tensor of shape (batch size. hidden size. sequence length)
      h_t = o_t * torch.tanh(c_t)
      # compute the output using convolutional layer
      # y_t is a tensor of shape (batch size. output size. sequence length)
      y_t = self.conv_out(h_t)
      # store the output for the current time step
      y[:, t, :] = y_t[:, :, t]
    # return the output tensor of shape (batch size. sequence length. output size)
    return y

language_encoder = BidirectionalLSTM(input_size=num_words, hidden_size=hidden_size)
pose_encoder = ConvolutionalLSTM(input_size=num_joints*3, hidden_size=hidden_size, kernel_size=kernel_size)
pose_decoder = ConvolutionalLSTM(input_size=hidden_size, output_size=num_joints*3, kernel_size=kernel_size)

# Define the loss function
loss_function = nn.MSELoss()

# Define the optimizer
optimizer = optim.Adam(params=list(language_encoder.parameters()) + list(pose_encoder.parameters()) + list(pose_decoder.parameters()), lr=learning_rate)

# Define the curriculum
curriculum = [10, 20, 30, 40, 50] # sequence lengths to train on

# Load the data
data = load_data("corpus.txt")

# Preprocess the data
def preprocess_data(data):
  # data is a list of dictionaries with keys "sentence" and "pose"
  # pose is a numpy array of shape (num_frames, num_joints*3)
  # return a list of dictionaries with keys "sentence", "pose", "sentence_ids" and "pose_tensor"
  processed_data = []
  for sample in data:
    sentence = sample["sentence"]
    pose = sample["pose"]
    # tokenize the sentence and convert to lower case
    tokens = nltk.word_tokenize(sentence.lower())
    # pad or truncate the tokens to max_length
    if len(tokens) < max_length:
      tokens = tokens + ["<pad>"] * (max_length - len(tokens))
    else:
      tokens = tokens[:max_length]
    # convert the tokens to ids using a vocabulary
    sentence_ids = [vocab[token] for token in tokens]
    # convert the pose array to a tensor
    pose_tensor = torch.from_numpy(pose).float()
    # store the processed sample in the list
    processed_data.append({"sentence": sentence, "pose": pose, "sentence_ids": sentence_ids, "pose_tensor": pose_tensor})
  return processed_data

processed_data = preprocess_data(data)

# Define a function to align the pose sequences using dynamic time warping
def align_sequences(x, y):
  # x and y are numpy arrays of shape (num_frames, num_joints*3)
  # return a pair of numpy arrays of shape (num_frames, num_joints*3) that are aligned using dynamic time warping
  # use fastdtw library to compute the optimal alignment path
  distance, path = fastdtw.fastdtw(x, y, dist=lambda a, b: np.linalg.norm(a - b))
  # initialize the aligned arrays to zeros
  x_aligned = np.zeros_like(x)
  y_aligned = np.zeros_like(y)
  # loop over the alignment path and copy the corresponding frames
  for i, j in path:
    x_aligned[i] = x[i]
    y_aligned[j] = y[j]
  return x_aligned, y_aligned

# Train the model
for epoch in range(num_epochs):
  for length in curriculum:
    for i in range(0, len(processed_data), batch_size):
      # Get the input and output sequences
      batch = processed_data[i:i+batch_size]
      input_sentence_ids = torch.tensor([sample["sentence_ids"] for sample in batch]) # shape: (batch_size, max_length)
      input_pose_tensor = torch.stack([sample["pose_tensor"][:length] for sample in batch]) # shape: (batch_size, length, num_joints*3)
      output_pose_tensor = torch.stack([sample["pose_tensor"][length:] for sample in batch]) # shape: (batch_size, num_frames-length, num_joints*3)

      # Encode the input sentence
      language_hidden_state = language_encoder(input_sentence_ids) # shape: (batch_size, hidden_size*2)

      # Encode the input pose
      pose_hidden_state = pose_encoder(input_pose_tensor) # shape: (batch_size, length, hidden_size)

      # Compute the joint embedding loss
      joint_embedding_loss = loss_function(language_hidden_state, pose_hidden_state[:, -1]) # scalar

      # Decode the output pose
      predicted_pose_tensor = pose_decoder(p