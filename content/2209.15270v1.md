---
title: 2209.15270v1 ERNIE-ViL 2.0  Multi-view Contrastive Learning for Image-Text Pre-training
date: 2022-09-16
---

# [ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training](http://arxiv.org/abs/2209.15270v1)

authors: Bin Shan, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang


## What, Why and How

[1]: https://arxiv.org/pdf/2209.15270v1 "ERNIE-V L 2.0: M VIEW CONTRASTIVE LEARNING FOR I -TEXT PRE ... - arXiv.org"
[2]: https://arxiv.org/abs/2209.15270 "[2209.15270] ERNIE-ViL 2.0: Multi-view Contrastive Learning ... - arXiv.org"
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2209.15270v1 "[2209.15270v1] ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image ..."

Here is a summary of the paper:

- **What**: The paper proposes ERNIE-ViL 2.0, a multi-view contrastive learning framework for image-text pre-training based on dual encoder architecture.
- **Why**: The paper aims to learn a more robust cross-modal representation by building intra-modal and inter-modal correlations between diverse views within and across image and text modalities.
- **How**: The paper constructs multiple views for each modality, such as inherent visual/textual views and sequences of object tags as a special textual view. It then uses contrastive learning to align and enhance the views within and across modalities. It pre-trains ERNIE-ViL 2.0 on large-scale image-text pairs in English and Chinese, and evaluates it on cross-modal retrieval tasks.

## Main Contributions

According to the paper, the main contributions are:

- It proposes a novel multi-view contrastive learning framework for image-text pre-training, which leverages diverse views within and across modalities to learn a more robust cross-modal representation.
- It introduces sequences of object tags as a special textual view to bridge the semantic gap between image and text modalities, especially on noisy image-text pairs.
- It pre-trains ERNIE-ViL 2.0 on large-scale image-text pairs in both English and Chinese languages, and achieves competitive or state-of-the-art results on cross-modal retrieval tasks.

## Method Summary

The method section of the paper describes the details of the proposed multi-view contrastive learning framework for image-text pre-training. It consists of four subsections:

- **Model Architecture**: It introduces the dual encoder architecture of ERNIE-ViL 2.0, which consists of a visual encoder and a textual encoder. The visual encoder is based on ResNet-50 and Faster R-CNN, and the textual encoder is based on BERT. The encoders output fixed-length embeddings for image and text inputs, respectively.
- **Multi-View Contrastive Learning**: It presents the multi-view contrastive learning objective, which aims to maximize the agreement between diverse views within and across modalities. It defines four types of views: inherent visual view, inherent textual view, object tag view, and cross-modal view. It then formulates the contrastive loss function for each type of view and the overall loss function as a weighted sum of them.
- **View Construction**: It explains how to construct multiple views for each modality. For the inherent visual view, it uses random cropping and resizing to generate different regions of interest from the same image. For the inherent textual view, it uses random masking and shuffling to generate different word sequences from the same text. For the object tag view, it uses an object detection model to extract object tags from the image and construct a sequence of tags as a special textual view. For the cross-modal view, it uses image-text pairs as input and output embeddings as views.
- **Implementation Details**: It provides the details of the pre-training data, model configuration, optimization strategy, and evaluation metrics used in the experiments. It also compares ERNIE-ViL 2.0 with existing VLP models in terms of model size and computational efficiency.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the dual encoder model
visual_encoder = ResNet50 + FasterRCNN
textual_encoder = BERT
# Define the contrastive loss function
def contrastive_loss(view1, view2, temperature):
  # Compute the cosine similarity between view1 and view2
  similarity = cosine_similarity(view1, view2)
  # Normalize the similarity by temperature
  similarity = similarity / temperature
  # Compute the softmax cross entropy loss
  loss = softmax_cross_entropy(similarity)
  return loss
# Define the multi-view contrastive learning objective
def multi_view_contrastive_learning(image, text):
  # Construct multiple views for image and text
  inherent_visual_view = random_crop_and_resize(image)
  inherent_textual_view = random_mask_and_shuffle(text)
  object_tag_view = object_detection(image)
  cross_modal_view = image_text_pair(image, text)
  # Encode each view using the corresponding encoder
  inherent_visual_embedding = visual_encoder(inherent_visual_view)
  inherent_textual_embedding = textual_encoder(inherent_textual_view)
  object_tag_embedding = textual_encoder(object_tag_view)
  cross_modal_embedding = visual_encoder(image) + textual_encoder(text)
  # Compute the contrastive loss for each type of view
  intra_visual_loss = contrastive_loss(inherent_visual_embedding, inherent_visual_embedding, temperature1)
  intra_textual_loss = contrastive_loss(inherent_textual_embedding, inherent_textual_embedding, temperature2)
  object_tag_loss = contrastive_loss(object_tag_embedding, object_tag_embedding, temperature3)
  cross_modal_loss = contrastive_loss(cross_modal_embedding, cross_modal_embedding, temperature4)
  # Compute the overall loss as a weighted sum of the contrastive losses
  overall_loss = alpha * intra_visual_loss + beta * intra_textual_loss + gamma * object_tag_loss + delta * cross_modal_loss
  return overall_loss
# Pre-train the model on large-scale image-text pairs
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the image and text inputs from the batch
    image = batch["image"]
    text = batch["text"]
    # Compute the multi-view contrastive learning objective
    loss = multi_view_contrastive_learning(image, text)
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
# Evaluate the model on cross-modal retrieval tasks
for task in tasks:
  # Get the query and candidate inputs from the task
  query = task["query"]
  candidates = task["candidates"]
  # Encode the query and candidates using the corresponding encoder
  query_embedding = visual_encoder(query) if query is image else textual_encoder(query)
  candidate_embeddings = [visual_encoder(candidate) if candidate is image else textual_encoder(candidate) for candidate in candidates]
  # Compute the cosine similarity between query and candidates
  similarity_scores = [cosine_similarity(query_embedding, candidate_embedding) for candidate_embedding in candidate_embeddings]
  # Rank the candidates by similarity scores
  ranked_candidates = sort_by(candidates, similarity_scores)
  # Compute the evaluation metrics such as recall and mean reciprocal rank
  metrics = evaluate(ranked_candidates, task["ground_truth"])
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np
# Define the hyperparameters
num_epochs = 10 # Number of pre-training epochs
batch_size = 256 # Batch size for pre-training
learning_rate = 1e-4 # Learning rate for optimizer
temperature1 = 0.07 # Temperature for intra-visual contrastive loss
temperature2 = 0.07 # Temperature for intra-textual contrastive loss
temperature3 = 0.07 # Temperature for object tag contrastive loss
temperature4 = 0.07 # Temperature for cross-modal contrastive loss
alpha = 0.25 # Weight for intra-visual contrastive loss
beta = 0.25 # Weight for intra-textual contrastive loss
gamma = 0.25 # Weight for object tag contrastive loss
delta = 0.25 # Weight for cross-modal contrastive loss
# Define the dual encoder model
visual_encoder = torchvision.models.resnet50(pretrained=True) # Pre-trained ResNet-50 model
visual_encoder.fc = torch.nn.Linear(visual_encoder.fc.in_features, 768) # Replace the final layer with a linear layer of output size 768
faster_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) # Pre-trained Faster R-CNN model with ResNet-50 backbone and FPN feature extractor
textual_encoder = transformers.BertModel.from_pretrained("bert-base-uncased") # Pre-trained BERT model
# Define the contrastive loss function
def contrastive_loss(view1, view2, temperature):
  # Compute the cosine similarity between view1 and view2
  similarity = torch.nn.functional.cosine_similarity(view1.unsqueeze(1), view2.unsqueeze(0), dim=-1)
  # Normalize the similarity by temperature
  similarity = similarity / temperature
  # Compute the softmax cross entropy loss with positive pairs on the diagonal
  labels = torch.arange(similarity.size(0)).to(similarity.device)
  loss = torch.nn.functional.cross_entropy(similarity, labels)
  return loss
# Define the multi-view contrastive learning objective
def multi_view_contrastive_learning(image, text):
  # Construct multiple views for image and text using data augmentation techniques
  inherent_visual_view = torchvision.transforms.RandomResizedCrop(size=(224,224))(image) # Randomly crop and resize the image to a fixed size of 224x224 pixels
  inherent_textual_view = transformers.BertTokenizer.from_pretrained("bert-base-uncased").mask_tokens(text) # Randomly mask some tokens in the text using the BERT tokenizer and mask token id
  object_tag_view = faster_rcnn(image)["labels"] # Extract the object labels from the image using the Faster R-CNN model
  cross_modal_view = (image, text) # Use the original image-text pair as the cross-modal view
  # Encode each view using the corresponding encoder and get the fixed-length embeddings
  inherent_visual_embedding = visual_encoder(inherent_visual_view) # Encode the inherent visual view using the visual encoder and get a 768-dimensional embedding vector
  inherent_textual_embedding = textual_encoder(inherent_textual_view)["pooler_output"] # Encode the inherent textual view using the textual encoder and get a 768-dimensional embedding vector from the pooler output
  object_tag_embedding = textual_encoder(object_tag_view)["pooler_output"] # Encode the object tag view using the textual encoder and get a 768-dimensional embedding vector from the pooler output
  cross_modal_embedding = visual_encoder(image) + textual_encoder(text)["pooler_output"] # Encode the image and text separately using the visual and textual encoders, and get a 768-dimensional cross-modal embedding vector by element-wise addition of the two embeddings 
  # Compute the contrastive loss for each type of view using a fixed temperature parameter
  intra_visual_loss = contrastive_loss(inherent_visual_embedding, inherent_visual_embedding, temperature1) # Compute the intra-visual contrastive loss by comparing the inherent visual embeddings with themselves 
  intra_textual_loss = contrastive_loss(inherent_textual_embedding, inherent_textual_embedding, temperature2) # Compute the intra-textual contrastive loss by comparing the inherent textual embeddings with themselves 
  object_tag_loss = contrastive_loss(object_tag_embedding, object_tag_embedding, temperature3) # Compute the object tag contrastive loss by comparing the object tag embeddings with themselves 
  cross_modal_loss = contrastive_loss(cross_modal_embedding, cross_modal_embedding, temperature4) # Compute the cross-modal contrastive loss by comparing the cross-modal embeddings with themselves 
  # Compute the overall loss as a weighted sum of the contrastive losses using fixed weights
  overall_loss = alpha * intra_visual_loss + beta * intra_textual_loss + gamma * object_tag_loss + delta * cross_modal_loss
  return overall_loss
# Pre-train the model on large-scale image-text pairs using gradient descent
optimizer = torch.optim.Adam(params=list(visual_encoder.parameters()) + list(textual_encoder.parameters()), lr=learning_rate) # Define the optimizer as Adam with the learning rate and the parameters of both encoders
for epoch in range(num_epochs):
  for batch in data_loader: # Iterate over the batches of image-text pairs from the data loader
    # Get the image and text inputs from the batch
    image = batch["image"]
    text = batch["text"]
    # Compute the multi-view contrastive learning objective
    loss = multi_view_contrastive_learning(image, text)
    # Update the model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
# Evaluate the model on cross-modal retrieval tasks using cosine similarity and ranking metrics
for task in tasks: # Iterate over the cross-modal retrieval tasks such as image-to-text or text-to-image retrieval
  # Get the query and candidate inputs from the task
  query = task["query"]
  candidates = task["candidates"]
  # Encode the query and candidates using the corresponding encoder and get the fixed-length embeddings
  query_embedding = visual_encoder(query) if query is image else textual_encoder(query)["pooler_output"] # Encode the query using the visual encoder if it is an image or the textual encoder if it is a text, and get a 768-dimensional embedding vector
  candidate_embeddings = [visual_encoder(candidate) if candidate is image else textual_encoder(candidate)["pooler_output"] for candidate in candidates] # Encode each candidate using the visual encoder if it is an image or the textual encoder if it is a text, and get a list of 768-dimensional embedding vectors
  # Compute the cosine similarity between query and candidates
  similarity_scores = [torch.nn.functional.cosine_similarity(query_embedding, candidate_embedding, dim=-1) for candidate_embedding in candidate_embeddings] # Compute the cosine similarity between the query embedding and each candidate embedding, and get a list of scalar similarity scores
  # Rank the candidates by similarity scores in descending order
  ranked_candidates = [candidate for _, candidate in sorted(zip(similarity_scores, candidates), reverse=True)] # Sort the candidates by their similarity scores in descending order, and get a list of ranked candidates
  # Compute the evaluation metrics such as recall and mean reciprocal rank using the ranked candidates and the ground truth
  metrics = evaluate(ranked_candidates, task["ground_truth"]) # Evaluate the ranked candidates against the ground truth using metrics such as recall@k and mean reciprocal rank (MRR), and get a dictionary of metric values
```