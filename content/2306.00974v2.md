---
title: 2306.00974v2 Intriguing Properties of Text-guided Diffusion Models
date: 2023-06-01
---

# [Intriguing Properties of Text-guided Diffusion Models](http://arxiv.org/abs/2306.00974v2)

authors: Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille


## What, Why and How

[1]: https://arxiv.org/abs/2306.00974 "[2306.00974] Intriguing Properties of Text-guided Diffusion Models"
[2]: https://arxiv.org/pdf/2306.00974v2.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2306.00974v2 "[2306.00974v2] Intriguing Properties of Text-guided Diffusion Models"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper studies the failure modes and intriguing properties of text-guided diffusion models (TDMs), which are generative models that can produce images conditioned on natural language prompts.
- **Why**: The paper aims to understand why TDMs can fail unexpectedly, such as generating images with the wrong content or unrelated outputs for the same prompt, and to reveal some of their hidden behaviors and potential safety concerns.
- **How**: The paper proposes SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. The paper also verifies the validity of SAGE in a human study and categorizes the failures into ten distinct types based on the underlying causes.



## Main Contributions

[1]: https://arxiv.org/abs/2306.00974 "[2306.00974] Intriguing Properties of Text-guided Diffusion Models"
[2]: https://arxiv.org/pdf/2306.00974v2.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2306.00974v2 "[2306.00974v2] Intriguing Properties of Text-guided Diffusion Models"

According to the paper[^1^][1], the main contributions are:

- **SAGE**: A novel adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation.
- **Technical contributions**: Several technical contributions to ensure that SAGE finds failure cases of the diffusion model, rather than the classifier, such as using a gradient-free optimization algorithm, a diversity-promoting regularization term, and a CLIP-based filtering mechanism.
- **Human study**: A human study to verify the validity of SAGE and to compare it with existing methods for finding failure cases of TDMs, such as random sampling and gradient-based attacks.
- **Failure categorization**: A categorization of the failure cases into ten distinct types based on the underlying causes, such as semantic ambiguity, lexical ambiguity, syntactic ambiguity, semantic inconsistency, lexical inconsistency, syntactic inconsistency, semantic incompleteness, lexical incompleteness, syntactic incompleteness, and adversarial token embedding.
- **Intriguing properties**: A revelation of four intriguing properties of TDMs that have not been systematically studied before, such as natural text prompts producing images that fail to capture the semantics of input texts, samples in the latent space leading to distorted or unrelated images independent of the text prompt, and a single adversarial token embedding generating a variety of specified target objects while only minimally affecting the CLIP score.


## Method Summary

[1]: https://arxiv.org/abs/2306.00974 "[2306.00974] Intriguing Properties of Text-guided Diffusion Models"
[2]: https://arxiv.org/pdf/2306.00974v2.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2306.00974v2 "[2306.00974v2] Intriguing Properties of Text-guided Diffusion Models"

Here is a summary of the method section of the paper[^1^][1]:

- **Overview**: The paper proposes SAGE, an adversarial attack on TDMs that uses image classifiers as surrogate loss functions, to search over the discrete prompt space and the high-dimensional latent space of TDMs to automatically discover unexpected behaviors and failure cases in the image generation. SAGE consists of two main components: a prompt attack and a latent attack.
- **Prompt attack**: The prompt attack aims to find natural text prompts that produce images with the wrong content. It uses a gradient-free optimization algorithm called CMA-ES to search over the discrete prompt space, and a diversity-promoting regularization term to encourage exploration. It also uses a CLIP-based filtering mechanism to remove prompts that are too different from the original prompt or have low CLIP scores.
- **Latent attack**: The latent attack aims to find latent samples that produce distorted or unrelated images independent of the text prompt. It also uses CMA-ES to search over the high-dimensional latent space, and a diversity-promoting regularization term to encourage exploration. It also uses a CLIP-based filtering mechanism to remove latent samples that have low CLIP scores or are too similar to the original sample.
- **Surrogate loss functions**: The surrogate loss functions are image classifiers that are used to guide the search process of SAGE. The paper uses two types of classifiers: (1) object classifiers that can detect specific objects in an image, such as cars, dogs, or flowers, and (2) scene classifiers that can recognize general scenes in an image, such as beach, forest, or city. The paper uses pre-trained models from PyTorch Hub for both types of classifiers.
- **Human study**: The human study is conducted to verify the validity of SAGE and to compare it with existing methods for finding failure cases of TDMs, such as random sampling and gradient-based attacks. The paper recruits 50 participants from Amazon Mechanical Turk and asks them to rate the quality and relevance of the generated images for different prompts and methods on a scale of 1 to 5. The paper analyzes the results using statistical tests and reports the mean and standard deviation of the ratings for each method.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the text-guided diffusion model (TDM)
TDM = TextGuidedDiffusionModel()

# Define the image classifiers
object_classifier = ObjectClassifier()
scene_classifier = SceneClassifier()

# Define the prompt attack
def prompt_attack(original_prompt, target_class):
  # Initialize a population of candidate prompts
  population = initialize_population(original_prompt)
  # Repeat until convergence or maximum iterations
  while not converged and not max_iterations:
    # Evaluate the fitness of each candidate prompt
    fitness = []
    for prompt in population:
      # Generate an image conditioned on the prompt
      image = TDM.generate(prompt)
      # Compute the classifier score for the target class
      score = classifier.score(image, target_class)
      # Append the score to the fitness list
      fitness.append(score)
    # Update the population using CMA-ES
    population = CMA_ES.update(population, fitness)
    # Apply diversity-promoting regularization
    population = diversity_regularization(population)
    # Apply CLIP-based filtering
    population = CLIP_filtering(population, original_prompt)
  # Return the best candidate prompt
  return best_prompt(population)

# Define the latent attack
def latent_attack(original_prompt, original_latent):
  # Initialize a population of candidate latent samples
  population = initialize_population(original_latent)
  # Repeat until convergence or maximum iterations
  while not converged and not max_iterations:
    # Evaluate the fitness of each candidate latent sample
    fitness = []
    for latent in population:
      # Generate an image conditioned on the original prompt and the latent sample
      image = TDM.generate(original_prompt, latent)
      # Compute the CLIP score for the original prompt and the image
      score = CLIP.score(original_prompt, image)
      # Append the score to the fitness list
      fitness.append(score)
    # Update the population using CMA-ES
    population = CMA_ES.update(population, fitness)
    # Apply diversity-promoting regularization
    population = diversity_regularization(population)
    # Apply CLIP-based filtering
    population = CLIP_filtering(population, original_latent)
  # Return the best candidate latent sample
  return best_latent(population)

# Define the human study
def human_study():
  # Define the prompts and methods to compare
  prompts = ["a dog wearing a hat", "a car flying in the sky", "a flower with six petals"]
  methods = ["random sampling", "gradient-based attack", "SAGE"]
  # Recruit participants from Amazon Mechanical Turk
  participants = recruit_participants()
  # For each participant, prompt, and method
  for participant in participants:
    for prompt in prompts:
      for method in methods:
        # Generate an image using the method
        image = generate_image(prompt, method)
        # Ask the participant to rate the quality and relevance of the image on a scale of 1 to 5
        quality_rating = get_quality_rating(participant, image)
        relevance_rating = get_relevance_rating(participant, image)
        # Record the ratings
        record_ratings(quality_rating, relevance_rating)
  # Analyze the ratings using statistical tests and report the mean and standard deviation for each method
  analyze_ratings()
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np
import scipy.optimize

# Define the text-guided diffusion model (TDM)
TDM = TextGuidedDiffusionModel()

# Define the image classifiers
object_classifier = torchvision.models.resnet50(pretrained=True)
scene_classifier = torchvision.models.resnet101(pretrained=True)

# Define the prompt attack
def prompt_attack(original_prompt, target_class):
  # Initialize a population of candidate prompts
  population = initialize_population(original_prompt)
  # Initialize the mean and covariance matrix for CMA-ES
  mean = np.zeros(len(original_prompt))
  cov = np.eye(len(original_prompt))
  # Set the maximum number of iterations for CMA-ES
  max_iterations = 100
  # Repeat until convergence or maximum iterations
  for iteration in range(max_iterations):
    # Evaluate the fitness of each candidate prompt
    fitness = []
    for prompt in population:
      # Generate an image conditioned on the prompt
      image = TDM.generate(prompt)
      # Compute the classifier score for the target class
      score = classifier.score(image, target_class)
      # Append the score to the fitness list
      fitness.append(score)
    # Update the mean and covariance matrix using CMA-ES
    mean, cov = CMA_ES.update(mean, cov, population, fitness)
    # Sample a new population from the multivariate normal distribution
    population = np.random.multivariate_normal(mean, cov, size=len(population))
    # Apply diversity-promoting regularization by adding random noise to each prompt
    population = diversity_regularization(population)
    # Apply CLIP-based filtering by removing prompts that are too different from the original prompt or have low CLIP scores
    population = CLIP_filtering(population, original_prompt)
  # Return the best candidate prompt with the highest fitness score
  return best_prompt(population)

# Define the latent attack
def latent_attack(original_prompt, original_latent):
  # Initialize a population of candidate latent samples
  population = initialize_population(original_latent)
  # Initialize the mean and covariance matrix for CMA-ES
  mean = np.zeros(len(original_latent))
  cov = np.eye(len(original_latent))
  # Set the maximum number of iterations for CMA-ES
  max_iterations = 100
  # Repeat until convergence or maximum iterations
  for iteration in range(max_iterations):
    # Evaluate the fitness of each candidate latent sample
    fitness = []
    for latent in population:
      # Generate an image conditioned on the original prompt and the latent sample
      image = TDM.generate(original_prompt, latent)
      # Compute the CLIP score for the original prompt and the image
      score = CLIP.score(original_prompt, image)
      # Append the score to the fitness list
      fitness.append(score)
    # Update the mean and covariance matrix using CMA-ES
    mean, cov = CMA_ES.update(mean, cov, population, fitness)
    # Sample a new population from the multivariate normal distribution
    population = np.random.multivariate_normal(mean, cov, size=len(population))
    # Apply diversity-promoting regularization by adding random noise to each latent sample
    population = diversity_regularization(population)
    # Apply CLIP-based filtering by removing latent samples that have low CLIP scores or are too similar to the original sample
    population = CLIP_filtering(population, original_latent)
  # Return the best candidate latent sample with the lowest fitness score
  return best_latent(population)

# Define the surrogate loss functions as classifier scores for a given image and target class
def classifier_score(image, target_class):
  # Preprocess the image and convert it to a tensor
  image_tensor = preprocess(image)
  # Feed the image tensor to the classifier and get the output logits
  logits = classifier(image_tensor)
  # Get the probability of the target class using softmax function
  prob = softmax(logits)[target_class]
  # Return the probability as the score
  return prob

# Define CMA-ES update function that takes in mean, covariance matrix, population and fitness scores and returns updated mean and covariance matrix 
def CMA_ES.update(mean, cov, population, fitness):
  # Sort the population and fitness scores in descending order of fitness scores 
  population, fitness = sort(population, fitness)
  # Compute the weighted average of the top k candidates as the new mean 
  k = len(population) // 2 
  weights = np.log(k + 0.5) - np.log(np.arange(1, k + 1)) 
  weights = weights / np.sum(weights) 
  new_mean = np.sum(weights * population[:k], axis=0) 
  # Compute the covariance matrix update term using the top k candidates and the old mean 
  cov_update = np.zeros_like(cov) 
  for i in range(k): 
    diff = population[i] - mean 
    cov_update += weights[i] * np.outer(diff, diff) 
  # Compute the learning rate for the covariance matrix update 
  alpha = 0.5 / len(population) 
  # Update the covariance matrix using the update term and the learning rate 
  new_cov = (1 - alpha) * cov + alpha * cov_update 
  # Return the new mean and covariance matrix 
  return new_mean, new_cov

# Define diversity-promoting regularization function that takes in a population and returns a perturbed population
def diversity_regularization(population):
  # Define the noise scale
  noise_scale = 0.1
  # For each candidate in the population
  for i in range(len(population)):
    # Add random noise scaled by the noise scale to the candidate
    population[i] += noise_scale * np.random.randn(len(population[i]))
  # Return the perturbed population
  return population

# Define CLIP-based filtering function that takes in a population and an original input (prompt or latent) and returns a filtered population
def CLIP_filtering(population, original_input):
  # Define the similarity threshold
  similarity_threshold = 0.8
  # Define the score threshold
  score_threshold = 0.5
  # Initialize an empty list for the filtered population
  filtered_population = []
  # For each candidate in the population
  for candidate in population:
    # Compute the cosine similarity between the candidate and the original input using CLIP embeddings
    similarity = cosine_similarity(CLIP.embed(candidate), CLIP.embed(original_input))
    # Compute the CLIP score for the candidate and the original prompt using CLIP model
    score = CLIP.score(original_prompt, candidate)
    # If the similarity is above the threshold and the score is below the threshold, append the candidate to the filtered list
    if similarity > similarity_threshold and score < score_threshold:
      filtered_population.append(candidate)
  # Return the filtered population
  return filtered_population

# Define human study function that recruits participants, generates images, asks ratings, and analyzes results
def human_study():
  # Define the prompts and methods to compare
  prompts = ["a dog wearing a hat", "a car flying in the sky", "a flower with six petals"]
  methods = ["random sampling", "gradient-based attack", "SAGE"]
  # Recruit participants from Amazon Mechanical Turk
  participants = recruit_participants()
  # Initialize an empty list for ratings
  ratings = []
  # For each participant, prompt, and method
  for participant in participants:
    for prompt in prompts:
      for method in methods:
        # Generate an image using the method
        image = generate_image(prompt, method)
        # Ask the participant to rate the quality and relevance of the image on a scale of 1 to 5
        quality_rating = get_quality_rating(participant, image)
        relevance_rating = get_relevance_rating(participant, image)
        # Record the ratings as a tuple of (participant, prompt, method, quality_rating, relevance_rating)
        ratings.append((participant, prompt, method, quality_rating, relevance_rating))
  # Analyze the ratings using statistical tests and report the mean and standard deviation for each method
  analyze_ratings(ratings)
```