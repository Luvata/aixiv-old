---
title: 2207.06555v1 Supervised Attribute Information Removal and Reconstruction for Image Manipulation
date: 2022-07-07
---

# [Supervised Attribute Information Removal and Reconstruction for Image Manipulation](http://arxiv.org/abs/2207.06555v1)

authors: Nannan Li, Bryan A. Plummer


## What, Why and How

[1]: https://arxiv.org/abs/2207.06555v1 "[2207.06555v1] Supervised Attribute Information Removal and ... - arXiv.org"
[2]: https://arxiv.org/abs/2207.06555 "[2207.06555] Supervised Attribute Information Removal and ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2205.06555v1 "[2205.06555v1] Quantum control of tunable-coupling transmons using ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new method for image attribute manipulation, which is the task of controlling specified attributes (such as hair color, clothing style, etc.) in given images.
- **Why**: The paper aims to address the limitations of previous methods that use disentangled representations for each attribute, which can lead to unwanted image editing effects due to information hiding and correlation between attributes and image content.
- **How**: The paper introduces an Attribute Information Removal and Reconstruction (AIRR) network that learns how to remove the attribute information entirely from the input image, creating attribute excluded features, and then learns to directly inject the desired attributes in a reconstructed image. The paper evaluates the AIRR network on four diverse datasets with a variety of attributes and shows that it improves attribute manipulation accuracy and top-k retrieval rate by 10% on average over prior work. The paper also reports that AIRR manipulated images are preferred over prior work in up to 76% of cases in a user study.

## Main Contributions

The paper claims the following contributions:

- A novel method for image attribute manipulation that prevents information hiding and correlation issues by removing and reconstructing attribute information.
- A comprehensive evaluation of the proposed method on four datasets with different attributes and comparison with state-of-the-art methods.
- A user study that demonstrates the preference and quality of the manipulated images generated by the proposed method.

## Method Summary

[1]: https://arxiv.org/abs/2207.06555v1 "[2207.06555v1] Supervised Attribute Information Removal and ... - arXiv.org"
[2]: https://arxiv.org/pdf/2203.06555v1.pdf "arXiv:2203.06555v1 [cs.CR] 13 Mar 2022"
[3]: https://www.researchgate.net/publication/362013030_Supervised_Attribute_Information_Removal_and_Reconstruction_for_Image_Manipulation "(PDF) Supervised Attribute Information Removal and Reconstruction for ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper presents the **Attribute Information Removal and Reconstruction (AIRR) network**, which consists of three modules: an **attribute information removal module**, an **attribute information reconstruction module**, and an **attribute classifier**.
- The attribute information removal module takes an input image and a source attribute label as inputs and outputs an **attribute excluded feature** that contains no information about the source attribute. This is achieved by using a **conditional variational autoencoder (CVAE)** that learns to encode the input image into a latent space that is independent of the source attribute label. The CVAE consists of an encoder, a decoder, and a discriminator that are trained with a combination of reconstruction loss, KL divergence loss, and adversarial loss.
- The attribute information reconstruction module takes the attribute excluded feature and a target attribute label as inputs and outputs a **reconstructed image** that has the target attribute. This is achieved by using a **conditional generative adversarial network (cGAN)** that learns to generate realistic images with the desired attributes. The cGAN consists of a generator and a discriminator that are trained with a combination of reconstruction loss, perceptual loss, style loss, and adversarial loss.
- The attribute classifier takes the reconstructed image as input and outputs a predicted attribute label. This is used to measure the **attribute manipulation accuracy** of the AIRR network, which is one of the evaluation metrics. The attribute classifier is a pre-trained ResNet-18 model that is fine-tuned on each dataset.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the AIRR network
airr = AIRR()

# Define the attribute classifier
classifier = ResNet18()

# Load the training data
data = load_data()

# Train the AIRR network and the attribute classifier
for epoch in range(num_epochs):
  for batch in data:
    # Get the input image, source attribute label, and target attribute label
    image, source_label, target_label = batch
    
    # Remove the source attribute information from the input image
    excluded_feature = airr.remove_attribute(image, source_label)
    
    # Reconstruct the image with the target attribute information
    reconstructed_image = airr.reconstruct_attribute(excluded_feature, target_label)
    
    # Predict the attribute label of the reconstructed image
    predicted_label = classifier(reconstructed_image)
    
    # Compute the losses for the AIRR network and the attribute classifier
    airr_loss = compute_airr_loss(image, excluded_feature, reconstructed_image, source_label, target_label)
    classifier_loss = compute_classifier_loss(predicted_label, target_label)
    
    # Update the parameters of the AIRR network and the attribute classifier
    update_parameters(airr_loss, classifier_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Define the AIRR network
class AIRR(nn.Module):
  def __init__(self):
    super(AIRR, self).__init__()
    # Define the attribute information removal module
    self.encoder = Encoder() # A convolutional neural network that encodes the input image into a latent vector
    self.decoder = Decoder() # A deconvolutional neural network that decodes the latent vector into an attribute excluded feature
    self.discriminator = Discriminator() # A convolutional neural network that discriminates between real and fake attribute labels
    
    # Define the attribute information reconstruction module
    self.generator = Generator() # A deconvolutional neural network that generates a reconstructed image from an attribute excluded feature and a target attribute label
    self.discriminator = Discriminator() # A convolutional neural network that discriminates between real and fake images
  
  def remove_attribute(self, image, source_label):
    # Encode the input image into a latent vector
    mu, log_var = self.encoder(image)
    
    # Sample a latent vector from the normal distribution parameterized by mu and log_var
    z = reparameterize(mu, log_var)
    
    # Decode the latent vector into an attribute excluded feature
    excluded_feature = self.decoder(z)
    
    # Predict the attribute label of the excluded feature
    predicted_label = self.discriminator(excluded_feature)
    
    return excluded_feature, predicted_label
  
  def reconstruct_attribute(self, excluded_feature, target_label):
    # Generate a reconstructed image from the excluded feature and the target label
    reconstructed_image = self.generator(excluded_feature, target_label)
    
    # Predict the realism of the reconstructed image
    predicted_realism = self.discriminator(reconstructed_image)
    
    return reconstructed_image, predicted_realism

# Define the attribute classifier
class ResNet18(nn.Module):
  def __init__(self):
    super(ResNet18, self).__init__()
    # Load the pre-trained ResNet-18 model
    self.model = torchvision.models.resnet18(pretrained=True)
    
    # Replace the last fully connected layer with a new one that matches the number of attributes
    num_features = self.model.fc.in_features
    num_attributes = get_num_attributes()
    self.model.fc = nn.Linear(num_features, num_attributes)
  
  def forward(self, image):
    # Predict the attribute label of the input image
    predicted_label = self.model(image)
    
    return predicted_label

# Load the training data
data = load_data()

# Train the AIRR network and the attribute classifier
for epoch in range(num_epochs):
  for batch in data:
    # Get the input image, source attribute label, and target attribute label
    image, source_label, target_label = batch
    
    # Remove the source attribute information from the input image
    excluded_feature, predicted_source_label = airr.remove_attribute(image, source_label)
    
    # Reconstruct the image with the target attribute information
    reconstructed_image, predicted_realism = airr.reconstruct_attribute(excluded_feature, target_label)
    
    # Predict the attribute label of the reconstructed image
    predicted_target_label = classifier(reconstructed_image)
    
    # Compute the losses for the AIRR network and the attribute classifier
    
    # Reconstruction loss: measures how well the reconstructed image matches the input image in pixel-wise and perceptual-wise
    pixel_loss = nn.L1Loss()(reconstructed_image, image)
    perceptual_loss = nn.L1Loss()(vgg(reconstructed_image), vgg(image))
    
    # KL divergence loss: measures how close the latent vector distribution is to a standard normal distribution
    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    
    # Adversarial loss: measures how well the discriminator and generator fool each other in terms of attribute labels and realism
    real_label = torch.ones(batch_size)
    fake_label = torch.zeros(batch_size)
    
    d_loss_source = nn.BCELoss()(predicted_source_label, real_label) + nn.BCELoss()(discriminator(excluded_feature.detach()), fake_label)
    
    g_loss_source = nn.BCELoss()(predicted_source_label, fake_label)
    
    d_loss_realism = nn.BCELoss()(predicted_realism, real_label) + nn.BCELoss()(discriminator(reconstructed_image.detach()), fake_label)
    
    g_loss_realism = nn.BCELoss()(predicted_realism, fake_label)
    
    # Style loss: measures how well the style of the reconstructed image matches the style of the input image
    style_loss = compute_style_loss(reconstructed_image, image)
    
    # Attribute classifier loss: measures how well the classifier predicts the target attribute label
    classifier_loss = nn.CrossEntropyLoss()(predicted_target_label, target_label)
    
    # Total loss: a weighted sum of the above losses
    airr_loss = pixel_loss + perceptual_loss + kl_loss + d_loss_source + g_loss_source + d_loss_realism + g_loss_realism + style_loss
    classifier_loss = classifier_loss
    
    # Update the parameters of the AIRR network and the attribute classifier
    update_parameters(airr_loss, classifier_loss)
```