---
title: 2010.02315v1 SMILE  Semantically-guided Multi-attribute Image and Layout Editing
date: 2020-10-03
---

# [SMILE: Semantically-guided Multi-attribute Image and Layout Editing](http://arxiv.org/abs/2010.02315v1)

authors: Andr√©s Romero, Luc Van Gool, Radu Timofte


## What, Why and How

[1]: https://arxiv.org/abs/2010.02315v1 "[2010.02315v1] SMILE: Semantically-guided Multi-attribute Image and ..."
[2]: https://arxiv.org/pdf/2010.02315v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2010.02315 "[2010.02315] SMILE: Semantically-guided Multi-attribute Image and ..."

Here is a summary of the paper:

- **What**: The paper presents a method for semantically-guided multi-attribute image and layout editing, called SMILE, that can handle both random and reference transformations for multiple attributes of facial images.
- **Why**: The paper aims to address the challenge of exploring the disentangled attribute space within a transformation, which is difficult due to the multiple and mutually-inclusive nature of the facial images, where different labels (eyeglasses, hats, hair, identity, etc.) can co-exist at the same time. Existing methods cannot handle both random and reference transformations for multiple attributes, which limits the generality of the solutions.
- **How**: The paper exploits a multimodal representation that handles all attributes, be it guided by random noise or exemplar images, while only using the underlying domain information of the target domain. The paper uses a Generative Adversarial Network (GAN) framework with two generators and two discriminators, and introduces a novel semantic loss function that guides the attribute transformation. The paper shows extensive qualitative and quantitative results for facial datasets and several different attributes that demonstrate the superiority of the method. The paper also shows that the method can be easily extended to head-swapping and face-reenactment applications without being trained on videos.[^1^][1] [^2^][2] [^3^][3]


## Main Contributions

The paper claims the following contributions:

- A novel semantic loss function that guides the attribute transformation and preserves the identity of the source image.
- A multimodal representation that handles all attributes, be it guided by random noise or exemplar images, while only using the underlying domain information of the target domain.
- A method that can handle both random and reference transformations for multiple attributes of facial images, which is more general than existing methods.
- A method that can add, remove or change either fine-grained or coarse attributes by using an image as a reference or by exploring the style distribution space.
- A method that can be easily extended to head-swapping and face-reenactment applications without being trained on videos.

## Method Summary

[1]: https://arxiv.org/abs/2010.02315v1 "[2010.02315v1] SMILE: Semantically-guided Multi-attribute Image and ..."
[2]: https://arxiv.org/pdf/2010.02315v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2010.02315 "[2010.02315] SMILE: Semantically-guided Multi-attribute Image and ..."

Here is a summary of the method section of the paper:

- The paper proposes a Generative Adversarial Network (GAN) framework with two generators and two discriminators, where each generator is responsible for transforming an image from one domain to another, and each discriminator is responsible for distinguishing between real and fake images in each domain.
- The paper introduces a novel semantic loss function that guides the attribute transformation and preserves the identity of the source image. The semantic loss function consists of two terms: a semantic consistency term that measures the similarity between the source and target images in terms of their semantic labels, and a semantic diversity term that measures the diversity of the generated images in terms of their style codes.
- The paper uses a multimodal representation that handles all attributes, be it guided by random noise or exemplar images, while only using the underlying domain information of the target domain. The multimodal representation consists of two components: a domain code that encodes the domain information of the target image, and a style code that encodes the modality information of the target image. The domain code is obtained by applying a domain classifier to the target image, and the style code is either sampled from a Gaussian distribution or extracted from an exemplar image using an encoder network.
- The paper shows how to use the proposed method for both random and reference transformations for multiple attributes of facial images, such as eyeglasses, hats, hair, identity, etc. The paper also shows how to use the method for adding, removing or changing either fine-grained or coarse attributes by using an image as a reference or by exploring the style distribution space.
- The paper shows how to extend the method to head-swapping and face-reenactment applications without being trained on videos. The paper uses a face alignment network to align the source and target faces, and then applies the proposed method to generate realistic and diverse face images.[^1^][1] [^2^][2] [^3^][3]


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the generators and discriminators
G_AB = Generator() # Generator that transforms images from domain A to domain B
G_BA = Generator() # Generator that transforms images from domain B to domain A
D_A = Discriminator() # Discriminator that distinguishes between real and fake images in domain A
D_B = Discriminator() # Discriminator that distinguishes between real and fake images in domain B

# Define the domain classifier and the encoder network
C = DomainClassifier() # Classifier that predicts the domain code of an image
E = Encoder() # Encoder that extracts the style code of an image

# Define the semantic loss function
def semantic_loss(source_image, target_image, source_label, target_label, style_code):
  # Compute the semantic consistency term
  semantic_consistency = L1_loss(source_label, target_label)
  # Compute the semantic diversity term
  semantic_diversity = KL_divergence(style_code, N(0,1))
  # Return the weighted sum of the two terms
  return lambda_1 * semantic_consistency + lambda_2 * semantic_diversity

# Define the training loop
for epoch in range(num_epochs):
  for batch in data_loader:
    # Get the source and target images and labels
    source_image_A, source_label_A = batch["A"]
    source_image_B, source_label_B = batch["B"]
    target_image_A = random.choice(data_loader["A"])
    target_image_B = random.choice(data_loader["B"])
    target_label_A = C(target_image_A)
    target_label_B = C(target_image_B)

    # Get the style codes for reference transformations
    style_code_A = E(target_image_A)
    style_code_B = E(target_image_B)

    # Get the style codes for random transformations
    random_style_code_A = N(0,1)
    random_style_code_B = N(0,1)

    # Generate fake images using the generators
    fake_image_AB_ref = G_AB(source_image_A, target_label_B, style_code_B) # Reference transformation from A to B
    fake_image_AB_rand = G_AB(source_image_A, target_label_B, random_style_code_B) # Random transformation from A to B
    fake_image_BA_ref = G_BA(source_image_B, target_label_A, style_code_A) # Reference transformation from B to A
    fake_image_BA_rand = G_BA(source_image_B, target_label_A, random_style_code_A) # Random transformation from B to A

    # Compute the adversarial losses for the generators and discriminators
    adv_loss_G_AB_ref = D_B(fake_image_AB_ref)
    adv_loss_G_AB_rand = D_B(fake_image_AB_rand)
    adv_loss_G_BA_ref = D_A(fake_image_BA_ref)
    adv_loss_G_BA_rand = D_A(fake_image_BA_rand)
    adv_loss_D_A_real = D_A(source_image_A)
    adv_loss_D_A_fake_ref = D_A(fake_image_BA_ref)
    adv_loss_D_A_fake_rand = D_A(fake_image_BA_rand)
    adv_loss_D_B_real = D_B(source_image_B)
    adv_loss_D_B_fake_ref = D_B(fake_image_AB_ref)
    adv_loss_D_B_fake_rand = D_B(fake_image_AB_rand)

    # Compute the cycle-consistency losses for the generators
    cycle_loss_G_AB_ref = L1_loss(source_image_A, G_BA(fake_image_AB_ref, source_label_A, style_code_A))
    cycle_loss_G_AB_rand = L1_loss(source_image_A, G_BA(fake_image_AB_rand, source_label_A, random_style_code_A))
    cycle_loss_G_BA_ref = L1_loss(source_image_B, G_AB(fake_image_BA_ref, source_label_B, style_code_B))
    cycle_loss_G_BA_rand = L1_loss(source_image_B, G_AB(fake_image_BA_rand, source_label_B, random_style_code_B))

    # Compute the identity losses for the generators
    identity_loss_G_AB_ref = L1_loss(target_image_B, G_AB(target_image_B, target_label_B, style_code_B))
    identity_loss_G_AB_rand = L1_loss(target_image_B, G_AB(target_image_B, target_label_B, random_style_code_B))
    identity_loss_G_BA_ref = L1_loss(target_image_A, G_BA(target_image_A, target_label_A, style_code_A))
    identity_loss_G_BA_rand = L1_loss(target_image_A, G_BA(target_image_A, target_label_A, random_style_code_A))

    # Compute the semantic losses for the generators
    semantic_loss_G_AB_ref = semantic_loss(source_image_A, fake_image_AB_ref, source_label_A, target_label_B, style_code_B)
    semantic_loss_G_AB_rand = semantic_loss(source_image_A, fake_image_AB_rand, source_label_A, target_label_B, random_style_code_B)
    semantic_loss_G_BA_ref = semantic_loss(source_image_B, fake_image_BA_ref, source_label_B, target_label_A, style_code_A)
    semantic_loss_G_BA_rand = semantic_loss(source_image_B, fake_image_BA_rand, source_label_B, target_label_A, random_style_code_A)

    # Compute the total losses for the generators and discriminators
    total_loss_G_AB_ref = adv_loss_G_AB_ref + cycle_loss_G_AB_ref + identity_loss_G_AB_ref + semantic_loss_G_AB_ref
    total_loss_G_AB_rand = adv_loss_G_AB_rand + cycle_loss_G_AB_rand + identity_loss_G_AB_rand + semantic_loss_G_AB_rand
    total_loss_G_BA_ref = adv_loss_G_BA_ref + cycle_loss_G_BA_ref + identity_loss_G_BA_ref + semantic_loss_G_BA_ref
    total_loss_G_BA_rand = adv_loss_G_BA_rand + cycle_loss_G_BA_rand + identity_loss_G_BA_rand + semantic_loss_G_BA_rand
    total_loss_D_A = adv_loss_D_A_real - (adv_loss_D_A_fake_ref + adv_loss_D_A_fake_rand) / 2
    total_loss_D_B = adv_loss_D_B_real - (adv_loss_D_B_fake_ref + adv_loss_D_B_fake_rand) / 2

    # Update the parameters of the generators and discriminators using gradient descent
    update_parameters(G_AB, total_loss_G_AB_ref + total_loss_G_AB_rand)
    update_parameters(G_BA, total_loss_G_BA_ref + total_loss_G_BA_rand)
    update_parameters(D_A, total_loss_D_A)
    update_parameters(D_B, total_loss_D_B)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from PIL import Image

# Define the hyperparameters
num_epochs = 100 # Number of training epochs
batch_size = 16 # Batch size for training
lr = 0.0002 # Learning rate for optimizers
beta1 = 0.5 # Beta1 parameter for Adam optimizers
beta2 = 0.999 # Beta2 parameter for Adam optimizers
lambda_1 = 10 # Weight for semantic consistency term in semantic loss function
lambda_2 = 0.01 # Weight for semantic diversity term in semantic loss function
lambda_3 = 10 # Weight for cycle-consistency loss term in total generator loss
lambda_4 = 5 # Weight for identity loss term in total generator loss

# Define the image transformations
transform = transforms.Compose([
  transforms.Resize(256), # Resize the images to 256x256 pixels
  transforms.CenterCrop(256), # Crop the images to 256x256 pixels from the center
  transforms.ToTensor(), # Convert the images to PyTorch tensors
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize the images to [-1, 1] range
])

# Define the datasets and data loaders
dataset_A = torchvision.datasets.ImageFolder(root="data/A", transform=transform) # Dataset for domain A images and labels
dataset_B = torchvision.datasets.ImageFolder(root="data/B", transform=transform) # Dataset for domain B images and labels
data_loader_A = DataLoader(dataset_A, batch_size=batch_size, shuffle=True) # Data loader for domain A images and labels
data_loader_B = DataLoader(dataset_B, batch_size=batch_size, shuffle=True) # Data loader for domain B images and labels

# Define the device to use for computation (GPU or CPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Define the generator network architecture
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    # Define the encoder block that consists of convolutional layers with instance normalization and leaky ReLU activation
    self.encoder_block = nn.Sequential(
      nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, padding_mode="reflect"), # Convolutional layer with 64 filters of size 7x7 and stride 1
      nn.InstanceNorm2d(64), # Instance normalization layer for 64 channels
      nn.LeakyReLU(0.2), # Leaky ReLU activation layer with negative slope 0.2
      nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # Convolutional layer with 128 filters of size 3x3 and stride 2
      nn.InstanceNorm2d(128), # Instance normalization layer for 128 channels
      nn.LeakyReLU(0.2), # Leaky ReLU activation layer with negative slope 0.2
      nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1), # Convolutional layer with 256 filters of size 3x3 and stride 2
      nn.InstanceNorm2d(256), # Instance normalization layer for 256 channels
      nn.LeakyReLU(0.2) # Leaky ReLU activation layer with negative slope 0.2
    )
    # Define the residual block that consists of two convolutional layers with instance normalization and skip connection
    self.residual_block = nn.Sequential(
      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, padding_mode="reflect"), # Convolutional layer with 256 filters of size 3x3 and stride 1
      nn.InstanceNorm2d(256), # Instance normalization layer for 256 channels
      nn.LeakyReLU(0.2), # Leaky ReLU activation layer with negative slope 0.2
      nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, padding_mode="reflect"), # Convolutional layer with 256 filters of size 3x3 and stride 
      nn.InstanceNorm2d(256) # Instance normalization layer for 256 channels      
    )
    # Define the decoder block that consists of transposed convolutional layers with instance normalization and ReLU activation
    self.decoder_block = nn.Sequential(
      nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1), # Transposed convolutional layer with 128 filters of size 3x3 and stride 2
      nn.InstanceNorm2d(128), # Instance normalization layer for 128 channels
      nn.ReLU(), # ReLU activation layer
      nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1), # Transposed convolutional layer with 64 filters of size 3x3 and stride 2
      nn.InstanceNorm2d(64), # Instance normalization layer for 64 channels
      nn.ReLU(), # ReLU activation layer
      nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3, padding_mode="reflect"), # Convolutional layer with 3 filters of size 7x7 and stride 1
      nn.Tanh() # Tanh activation layer
    )

  def forward(self, x, y, z):
    # x: input image tensor of shape (batch_size, 3, 256, 256)
    # y: domain code tensor of shape (batch_size, num_domains)
    # z: style code tensor of shape (batch_size, num_styles)
    # Concatenate the domain code and the style code along the channel dimension
    yz = torch.cat([y, z], dim=1) # yz: concatenated code tensor of shape (batch_size, num_domains + num_styles)
    # Reshape the concatenated code tensor to match the spatial dimensions of the input image tensor
    yz = yz.view(yz.size(0), yz.size(1), 1, 1) # yz: reshaped code tensor of shape (batch_size, num_domains + num_styles, 1, 1)
    yz = yz.repeat(1, 1, x.size(2), x.size(3)) # yz: repeated code tensor of shape (batch_size, num_domains + num_styles, 256, 256)
    # Concatenate the input image tensor and the repeated code tensor along the channel dimension
    x_yz = torch.cat([x, yz], dim=1) # x_yz: concatenated image-code tensor of shape (batch_size, 3 + num_domains + num_styles, 256, 256)
    # Pass the concatenated image-code tensor through the encoder block
    e = self.encoder_block(x_yz) # e: encoded tensor of shape (batch_size, 256, 64, 64)
    # Pass the encoded tensor through six residual blocks
    r = e # r: residual tensor of shape (batch_size, 256, 64, 64)
    for i in range(6):
      r = r + self.residual_block(r) # Add the output of the residual block to the residual tensor
    # Pass the residual tensor through the decoder block
    d = self.decoder_block(r) # d: decoded tensor of shape (batch_size, 3, 256, 256)
    # Return the decoded tensor as the output image
    return d

# Define the discriminator network architecture
class Discriminator(nn.Module):
  def __init__(self):
    super(Discriminator, self).__init__()
    # Define the discriminator block that consists of convolutional layers with instance normalization and leaky ReLU activation
    self.discriminator_block = nn.Sequential(
      nn.Conv2d(3 + num_domains + num_styles ,64 ,kernel_size=4 ,stride=2 ,padding=1), # Convolutional layer with 64 filters of size 
      nn.LeakyReLU(0.2), # Leaky ReLU activation layer with negative slope 
      nn.Conv2d(64 ,128 ,kernel_size=4 ,stride=2 ,padding=1), # Convolutional layer with 
      nn.InstanceNorm2d(128), # Instance normalization layer for 
      nn.LeakyReLU(0.2), # Leaky ReLU activation layer with negative slope 
      nn.Conv2d(128 ,256 ,kernel_size=4 ,stride=2 ,padding=1), # Convolutional layer with 
      nn.InstanceNorm2d(256), # Instance normalization layer for 
      nn.LeakyReLU(0.2), # Leaky ReLU activation layer with negative slope 
      nn.Conv2d(256 ,512 ,kernel_size=4 ,stride=1 ,padding=1), # Convolutional layer with 
      nn.InstanceNorm2d(512), # Instance normalization layer for 
      nn.LeakyReLU(0.2),