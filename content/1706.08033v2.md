---
title: 1706.08033v2 Decomposing Motion and Content for Natural Video Sequence Prediction
date: 2017-06-09
---

# [Decomposing Motion and Content for Natural Video Sequence Prediction](http://arxiv.org/abs/1706.08033v2)

authors: Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee


## What, Why and How

[1]: https://arxiv.org/abs/1706.08033 "[1706.08033] Decomposing Motion and Content for Natural Video Sequence ..."
[2]: https://arxiv.org/pdf/1706.08033 "A arXiv:1706.08033v2 [cs.CV] 8 Jan 2018"
[3]: http://export.arxiv.org/abs/2011.08033v2 "[2011.08033v2] Convergence in law for Complex Gaussian Multiplicative ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a deep neural network for the prediction of future frames in natural video sequences. The network decomposes the motion and content, two key components generating dynamics in videos, and predicts the next frame by converting the content features with the motion features[^1^][1].
- **Why**: The paper aims to address the problem of pixel-level prediction in natural videos, which provides dense and direct description of the visual world and can be used for various tasks such as action recognition, event detection, and motion estimation. The paper also argues that decomposing motion and content simplifies the task of prediction and avoids the need for extra information such as foreground-background segmentation masks and static background[^1^][1].
- **How**: The paper builds upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. The network consists of four modules: a content encoder, a motion encoder, a motion decoder, and a content decoder. The content encoder extracts content features from the current frame, while the motion encoder extracts motion features from a sequence of previous frames. The motion decoder converts the content features into the next frame content by applying the motion features, while the content decoder reconstructs the current frame from the content features. The network is trained end-to-end over multiple time steps using a combination of reconstruction loss and adversarial loss[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/1706.08033 "[1706.08033] Decomposing Motion and Content for Natural Video Sequence ..."
[2]: http://export.arxiv.org/abs/2011.08033v2 "[2011.08033v2] Convergence in law for Complex Gaussian Multiplicative ..."
[3]: https://arxiv.org/pdf/1706.08033v2.pdf "arXiv.org e-Print archive"

The paper claims the following contributions[^1^][1]:

- It proposes a novel network architecture that decomposes motion and content for natural video sequence prediction, which simplifies the task and avoids the need for extra information.
- It shows that the network can naturally learn to separate motion and content without separate training, and that the motion and content features are interpretable and meaningful.
- It demonstrates state-of-the-art performance on three human activity video datasets (KTH, Weizmann action, and UCF-101) in comparison to recent approaches.

## Method Summary

[1]: https://arxiv.org/abs/1706.08033 "[1706.08033] Decomposing Motion and Content for Natural Video Sequence ..."
[2]: https://arxiv.org/pdf/1706.08033 "A arXiv:1706.08033v2 [cs.CV] 8 Jan 2018"
[3]: http://export.arxiv.org/abs/2011.08033v2 "[2011.08033v2] Convergence in law for Complex Gaussian Multiplicative ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a network architecture that consists of four modules: a content encoder, a motion encoder, a motion decoder, and a content decoder. The content encoder extracts content features from the current frame, while the motion encoder extracts motion features from a sequence of previous frames. The motion decoder converts the content features into the next frame content by applying the motion features, while the content decoder reconstructs the current frame from the content features.
- The paper describes the details of each module and how they are connected. The content encoder and decoder are based on the Encoder-Decoder Convolutional Neural Network (EDCNN), which uses convolutional and deconvolutional layers to encode and decode images. The motion encoder and decoder are based on the Convolutional LSTM (ConvLSTM), which uses convolutional operations to update the hidden states and cell states of LSTM units. The motion decoder also uses a skip connection to combine the motion features with the content features.
- The paper explains how the network is trained end-to-end over multiple time steps using a combination of reconstruction loss and adversarial loss. The reconstruction loss measures the pixel-wise difference between the predicted frames and the ground truth frames, while the adversarial loss measures the discrepancy between the predicted frames and natural frames using a discriminator network. The paper also discusses how to handle occlusions and disocclusions in videos using an occlusion mask.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the network modules
content_encoder = EDCNN()
content_decoder = EDCNN()
motion_encoder = ConvLSTM()
motion_decoder = ConvLSTM()
discriminator = CNN()

# Define the loss functions
reconstruction_loss = L2_loss()
adversarial_loss = binary_cross_entropy()

# Define the training parameters
learning_rate = 0.0002
batch_size = 4
num_steps = 10

# Define the input and output tensors
current_frame = tf.placeholder(tf.float32, [batch_size, height, width, channels])
previous_frames = tf.placeholder(tf.float32, [batch_size, num_steps, height, width, channels])
next_frame = tf.placeholder(tf.float32, [batch_size, height, width, channels])

# Extract content features from the current frame
content_features = content_encoder(current_frame)

# Extract motion features from the previous frames
motion_features = motion_encoder(previous_frames)

# Predict the next frame content by applying motion features to content features
next_frame_content = motion_decoder(motion_features, content_features)

# Reconstruct the current frame from the content features
current_frame_reconstruction = content_decoder(content_features)

# Compute the occlusion mask by comparing the next frame content and the current frame reconstruction
occlusion_mask = tf.abs(next_frame_content - current_frame_reconstruction)

# Apply the occlusion mask to the next frame content to get the final prediction
next_frame_prediction = next_frame_content * occlusion_mask

# Compute the reconstruction loss between the predicted frames and the ground truth frames
rec_loss = reconstruction_loss(next_frame_prediction, next_frame)

# Compute the adversarial loss between the predicted frames and natural frames using the discriminator network
adv_loss = adversarial_loss(discriminator(next_frame_prediction), discriminator(next_frame))

# Compute the total loss as a weighted sum of reconstruction loss and adversarial loss
total_loss = rec_loss + lambda * adv_loss

# Update the network parameters using Adam optimizer
optimizer = tf.train.AdamOptimizer(learning_rate)
train_op = optimizer.minimize(total_loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import tensorflow as tf
import numpy as np
import cv2

# Define the network modules
def EDCNN():
  # Define the encoder-decoder convolutional neural network
  # The network consists of four convolutional layers and four deconvolutional layers
  # Each convolutional layer has a kernel size of 3x3, a stride of 2, and a ReLU activation function
  # Each deconvolutional layer has a kernel size of 3x3, a stride of 2, and a ReLU activation function
  # The number of filters for each layer is [64, 128, 256, 512] for the encoder and [512, 256, 128, channels] for the decoder
  # The input and output tensors have the shape [batch_size, height, width, channels]
  
  # Define the weights and biases for each layer
  weights = {
    'enc1': tf.Variable(tf.random_normal([3, 3, channels, 64])),
    'enc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),
    'enc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),
    'enc4': tf.Variable(tf.random_normal([3, 3, 256, 512])),
    'dec1': tf.Variable(tf.random_normal([3, 3, 512, 512])),
    'dec2': tf.Variable(tf.random_normal([3, 3, 512, 256])),
    'dec3': tf.Variable(tf.random_normal([3, 3, 256, 128])),
    'dec4': tf.Variable(tf.random_normal([3, 3, 128, channels]))
  }
  
  biases = {
    'enc1': tf.Variable(tf.random_normal([64])),
    'enc2': tf.Variable(tf.random_normal([128])),
    'enc3': tf.Variable(tf.random_normal([256])),
    'enc4': tf.Variable(tf.random_normal([512])),
    'dec1': tf.Variable(tf.random_normal([512])),
    'dec2': tf.Variable(tf.random_normal([256])),
    'dec3': tf.Variable(tf.random_normal([128])),
    'dec4': tf.Variable(tf.random_normal([channels]))
  }
  
  # Define the forward pass function
  def forward_pass(x):
    # Encode the input tensor x using convolutional layers
    enc1 = tf.nn.relu(tf.nn.conv2d(x, weights['enc1'], strides=[1,2,2,1], padding='SAME') + biases['enc1'])
    enc2 = tf.nn.relu(tf.nn.conv2d(enc1, weights['enc2'], strides=[1,2,2,1], padding='SAME') + biases['enc2'])
    enc3 = tf.nn.relu(tf.nn.conv2d(enc2, weights['enc3'], strides=[1,2,2,1], padding='SAME') + biases['enc3'])
    enc4 = tf.nn.relu(tf.nn.conv2d(enc3, weights['enc4'], strides=[1,2,2,1], padding='SAME') + biases['enc4'])
    
    # Decode the encoded tensor enc4 using deconvolutional layers
    dec1 = tf.nn.relu(tf.nn.conv2d_transpose(enc4, weights['dec1'], output_shape=[batch_size,height//8,width//8,512], strides=[1,2,2,1], padding='SAME') + biases['dec1'])
    dec2 = tf.nn.relu(tf.nn.conv2d_transpose(dec1 + enc4 , weights['dec2'], output_shape=[batch_size,height//4,width//4,
256], strides=[1,
2,
2,
1], padding='SAME') + biases['dec2'])
    dec3 = tf.nn.relu(tf.nn.conv2d_transpose(dec2 + enc3 , weights['dec3'], output_shape=[batch_size,height//2,width//2,
128], strides=[1,
2,
2,
1], padding='SAME') + biases['dec3'])
    dec4 = tf.nn.relu(tf.nn.conv2d_transpose(dec3 + enc2 , weights['dec4'], output_shape=[batch_size,height,width,
channels], strides=[1,
2,
2,
1], padding='SAME') + biases['dec4'])
    
    # Return the decoded tensor dec4 as the output
    return dec4
  
  
def ConvLSTM():
  
# Define the convolutional LSTM network
# The network consists of a single ConvLSTM layer with a kernel size of 3x3 and a hidden state size of 512
# The input and output tensors have the shape [batch_size, num_steps, height, width, channels]

# Define the weights and biases for the ConvLSTM layer
weights = {
  'i': tf.Variable(tf.random_normal([3, 3, channels + 512, 512])),
  'f': tf.Variable(tf.random_normal([3, 3, channels + 512, 512])),
  'o': tf.Variable(tf.random_normal([3, 3, channels + 512, 512])),
  'c': tf.Variable(tf.random_normal([3, 3, channels + 512, 512]))
}

biases = {
  'i': tf.Variable(tf.random_normal([512])),
  'f': tf.Variable(tf.random_normal([512])),
  'o': tf.Variable(tf.random_normal([512])),
  'c': tf.Variable(tf.random_normal([512]))
}

# Define the forward pass function
def forward_pass(x):
  
# Initialize the hidden state and cell state as zero tensors
h = tf.zeros([batch_size, height, width, 512])
c = tf.zeros([batch_size, height, width, 512])

# Loop over the time steps
for t in range(num_steps):
  
# Concatenate the input tensor x at time t with the hidden state h
xh = tf.concat([x[:,t,:,:,:], h], axis=-1)

# Compute the input gate, forget gate, output gate and cell gate using convolutional operations
i = tf.sigmoid(tf.nn.conv2d(xh, weights['i'], strides=[1,1,1,1], padding='SAME') + biases['i'])
f = tf.sigmoid(tf.nn.conv2d(xh, weights['f'], strides=[1,1,1,1], padding='SAME') + biases['f'])
o = tf.sigmoid(tf.nn.conv2d(xh, weights['o'], strides=[1,1,1,1], padding='SAME') + biases['o'])
g = tf.tanh(tf.nn.conv2d(xh, weights['c'], strides=[1,1,1,1], padding='SAME') + biases['c'])

# Update the cell state and hidden state using the gates
c = f * c + i * g
h = o * tf.tanh(c)

# Return the hidden state h as the output
return h

def CNN():
  
# Define the discriminator network
# The network consists of four convolutional layers and a fully connected layer
# Each convolutional layer has a kernel size of 4x4, a stride of 2 and a LeakyReLU activation function with alpha=0.2
# The number of filters for each layer is [64,128,
256,
512]
# The fully connected layer has an output size of 1 and a sigmoid activation function
# The input and output tensors have the shape [batch_size,
height,
width,
channels] and [batch_size,
1]

# Define the weights and biases for each layer
weights = {
'conv1': tf.Variable(tf.random_normal([4,
4,
channels,
64])),
'conv2': tf.Variable(tf.random_normal([4,
4,
64,
128])),
'conv3': tf.Variable(tf.random_normal([4,
4,
128,
256])),
'conv4': tf.Variable(tf.random_normal([4,
4,
256,
512])),
'fc': tf.Variable(tf.random_normal([height//16*width//16*512,
1]))
}

biases = {
'conv1': tf.Variable(tf.random_normal([64])),
'conv2': tf.Variable(tf.random_normal([128])),
'conv3': tf.Variable(tf.random_normal([256])),
'conv4': tf.Variable(tf.random_normal([512])),
'fc': tf.Variable(tf.random_normal([1]))
}

# Define the forward pass function
def forward_pass(x):

# Apply convolutional layers to the input tensor x
conv1 = tf.nn.leaky_relu(tf.nn.conv2d(x,
weights['conv1'],
strides=[1,
2,
2,
1],
padding='SAME') + biases['conv1'],
alpha=0.2)
conv2 = tf.nn.leaky_relu(tf.nn.conv2d(conv1,
weights['conv2'],
strides=[1,
2,
2,
1],
padding='SAME') + biases['conv2'],
alpha=0.2)
conv3 = tf.nn.leaky_relu(tf.nn.conv2d(conv2,
weights['conv3'],
strides=[1,
2,
2,
1],
padding='SAME') + biases['conv3'],
alpha=0.2)
conv4 = tf.nn.leaky_relu(tf.nn.conv2d(conv3,
weights['conv4'],
strides=[1,
2,
2,
1],
padding='SAME') + biases['conv4'],
alpha=0.2)

# Flatten the