---
title: 2306.00188v1 Multi-environment lifelong deep reinforcement learning for medical imaging
date: 2023-06-01
---

# [Multi-environment lifelong deep reinforcement learning for medical imaging](http://arxiv.org/abs/2306.00188v1)

authors: Guangyao Zheng, Shuhao Lai, Vladimir Braverman, Michael A. Jacobs, Vishwa S. Parekh


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00188.pdf "Multi-environment lifelong deep reinforcement learning for medical imaging"
[2]: https://arxiv.org/pdf/2305.00188v1.pdf "Local Search for Integer Linear Programming ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2305.00188v1 "[2305.00188v1] Local Search for Integer Linear Programming"

Here is a summary of the paper:

- **What**: The paper presents a **Lifelong DRL framework, SERIL** to continually learn new tasks in changing imaging environments without catastrophic forgetting. SERIL was developed using selective experience replay based lifelong learning technique for the localization of five anatomical landmarks in brain MRI on a sequence of twenty-four different imaging environments[^1^][1].
- **Why**: The paper aims to address the challenge of applying DRL to medical imaging tasks, which are constantly evolving in terms of imaging orientations, imaging sequences, and pathologies. The paper claims that existing DRL methods are not suitable for such dynamic and diverse environments, as they suffer from catastrophic forgetting when learning new tasks[^1^][1].
- **How**: The paper proposes SERIL, which consists of three modes: **Search**, **Improve**, and **Restore**. In the Search mode, SERIL explores the environment and collects experiences. In the Improve mode, SERIL updates its policy using a subset of experiences that are relevant to the current task. In the Restore mode, SERIL recovers its performance on previous tasks by replaying old experiences. The paper evaluates SERIL on a dataset of brain MRI images with different imaging environments and compares it with two baseline setups: MERT (multi-environment-best-case) and SERT (single-environment-worst-case). The paper reports that SERIL outperforms both baselines in terms of localization accuracy and demonstrates its ability to learn multiple tasks across dynamically changing imaging environments without forgetting[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- It proposes **SERIL**, the first Lifelong DRL framework for medical imaging that can continually learn new tasks in changing imaging environments without catastrophic forgetting.
- It introduces **selective experience replay**, a novel technique that enables SERIL to select relevant experiences for policy update and performance recovery.
- It demonstrates the effectiveness of SERIL on a large-scale and diverse dataset of brain MRI images with different imaging environments and tasks. It shows that SERIL achieves superior localization accuracy and robustness compared to existing methods.

## Method Summary

Here is a summary of the method section of the paper:

- The paper formulates the problem of anatomical landmark localization in brain MRI as a DRL problem, where the agent is a 2D cursor that moves on the image plane and the goal is to reach the target landmark. The agent receives a reward based on its distance to the landmark and a penalty for exceeding the maximum number of steps.
- The paper proposes SERIL, a Lifelong DRL framework that consists of three modes: Search, Improve, and Restore. In each mode, SERIL uses a different operator to modify its policy and experience buffer.
- In the Search mode, SERIL explores the environment and collects experiences using an epsilon-greedy strategy. It stores the experiences in a buffer and assigns a priority score to each experience based on its reward and novelty. The priority score reflects the relevance of the experience to the current task and environment.
- In the Improve mode, SERIL updates its policy using a subset of experiences that have high priority scores. It applies a lift move operator that increases the values of actions that lead to high rewards and decreases the values of actions that lead to low rewards. The lift move operator aims to improve the quality of the objective function while maintaining feasibility.
- In the Restore mode, SERIL recovers its performance on previous tasks by replaying old experiences that have low priority scores. It applies a tight move operator that adaptively modifies the values of actions that make some constraint tight. The tight move operator aims to restore the feasibility of the policy while preserving the quality of the objective function.

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Initialize the agent's policy and experience buffer
policy = initialize_policy()
buffer = initialize_buffer()

# Loop over a sequence of tasks and environments
for task, environment in sequence:

  # Switch to the Search mode
  mode = Search

  # Loop until the task is completed or the maximum number of steps is reached
  while not done:

    # Observe the current state
    state = observe_state(environment)

    # Choose an action based on the policy and the exploration rate
    action = choose_action(policy, epsilon)

    # Execute the action and observe the next state and reward
    next_state, reward = execute_action(action, environment)

    # Store the experience in the buffer and assign a priority score
    experience = (state, action, reward, next_state)
    priority = compute_priority(experience)
    buffer.store(experience, priority)

    # Update the state and check if the task is completed
    state = next_state
    done = check_completion(state, task)

  # Switch to the Improve mode
  mode = Improve

  # Loop until the policy converges or the maximum number of iterations is reached
  while not converged:

    # Sample a batch of experiences with high priority scores from the buffer
    batch = buffer.sample(high_priority)

    # Update the policy using the lift move operator
    policy = lift_move(policy, batch)

    # Check if the policy converges
    converged = check_convergence(policy)

  # Switch to the Restore mode
  mode = Restore

  # Loop until the performance on previous tasks is recovered or the maximum number of iterations is reached
  while not recovered:

    # Sample a batch of experiences with low priority scores from the buffer
    batch = buffer.sample(low_priority)

    # Update the policy using the tight move operator
    policy = tight_move(policy, batch)

    # Check if the performance on previous tasks is recovered
    recovered = check_recovery(policy)
```