---
title: 2206.00048v2 PandA  Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs
date: 2022-06-01
---

# [PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs](http://arxiv.org/abs/2206.00048v2)

authors: James Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis A. Nicolaou, Ioannis Patras


## What, Why and How

[1]: https://arxiv.org/abs/2206.00048 "[2206.00048] PandA: Unsupervised Learning of Parts and Appearances in ..."
[2]: http://arxiv-export2.library.cornell.edu/pdf/2206.00048 "A arXiv:2206.00048v2 [cs.CV] 6 Feb 2023"
[3]: https://arxiv.org/pdf/2206.00048 "arXiv.org"

Here is a summary of the paper:

- **What**: The paper presents PandA, an unsupervised method for discovering spatial parts and their appearances in the feature maps of pre-trained GANs.
- **Why**: The paper aims to enable context-aware local image editing with pixel-level control, without requiring any supervision or being limited to specific GAN architectures or global semantic directions.
- **How**: The paper applies a semi-nonnegative tensor factorization on the feature maps of GANs, which decomposes them into factors representing parts and appearances. The paper then shows how these factors can be manipulated to edit images in a localized manner, as well as how they can be used to generate saliency maps that highlight concepts of interest. The paper evaluates the method on various GAN architectures and datasets, and compares it with state-of-the-art methods in terms of efficiency and accuracy.

## Main Contributions

[1]: https://arxiv.org/abs/2206.00048 "[2206.00048] PandA: Unsupervised Learning of Parts and Appearances in ..."
[2]: http://arxiv-export2.library.cornell.edu/pdf/2206.00048 "A arXiv:2206.00048v2 [cs.CV] 6 Feb 2023"
[3]: https://arxiv.org/pdf/2206.00048 "arXiv.org"

According to the paper[^1^][1] [^2^][2], the main contributions are:

- **A novel unsupervised method for discovering spatial parts and their appearances in the feature maps of pre-trained GANs**, which does not require any manual annotations or segmentation masks, and is applicable to any GAN architecture.
- **A framework for context-aware local image editing with pixel-level control**, which allows users to manipulate the parts and appearances of images generated by GANs in a fine-grained and intuitive way, without affecting other regions or losing coherence.
- **A technique for generating saliency maps that localize concepts of interest**, which leverages the appearance factors discovered by the method to highlight regions that correspond to specific attributes or categories, without using any labels or classifiers.
- **Extensive experiments on various GAN architectures and datasets**, which demonstrate that the proposed method outperforms state-of-the-art methods in terms of efficiency and accuracy, and enables a wide range of image editing and synthesis applications.

## Method Summary

[1]: https://arxiv.org/abs/2206.00048 "[2206.00048] PandA: Unsupervised Learning of Parts and Appearances in ..."
[2]: http://arxiv-export2.library.cornell.edu/pdf/2206.00048 "A arXiv:2206.00048v2 [cs.CV] 6 Feb 2023"
[3]: https://arxiv.org/pdf/2206.00048 "arXiv.org"

Here is a summary of the method section of the paper:

- **Overview**: The method takes as input a feature map of a pre-trained GAN and applies a semi-nonnegative tensor factorization to decompose it into two factors: a part factor and an appearance factor. The part factor represents the spatial locations of different parts in the image, while the appearance factor represents the visual attributes of each part. The method then allows users to edit the image by manipulating these factors in various ways, such as changing the appearance of a part, moving or resizing a part, adding or removing a part, or swapping parts between images. The method also generates saliency maps that highlight regions that correspond to specific concepts of interest, such as attributes or categories, by using the appearance factor as a proxy for saliency.
- **Semi-nonnegative tensor factorization**: The method uses a semi-nonnegative tensor factorization (SNTF) to decompose the feature map into two factors: a part factor P and an appearance factor A. The SNTF imposes nonnegativity constraints on both factors, which ensures that they are sparse and interpretable. The SNTF also imposes orthogonality constraints on the part factor, which ensures that each part is independent and localized. The SNTF is formulated as an optimization problem that minimizes the reconstruction error between the feature map and the product of the two factors, subject to the constraints. The method uses an alternating optimization algorithm to solve the SNTF problem efficiently.
- **Image editing**: The method enables various image editing operations by manipulating the part and appearance factors. For example, to change the appearance of a part, the method modifies the corresponding entry in the appearance factor and reconstructs the image using the modified factor. To move or resize a part, the method shifts or scales the corresponding column in the part factor and reconstructs the image using the modified factor. To add or remove a part, the method adds or deletes a column in both factors and reconstructs the image using the modified factors. To swap parts between images, the method exchanges columns between different feature maps and reconstructs the images using the modified feature maps.
- **Saliency map generation**: The method generates saliency maps that localize concepts of interest by using the appearance factor as a proxy for saliency. The method assumes that each entry in the appearance factor corresponds to a concept of interest, such as an attribute or a category, and computes its saliency score by measuring its variance across different images. The method then uses these scores to weight the columns of the part factor and sums them up to obtain a saliency map for each concept. The method can also generate saliency maps for combinations of concepts by linearly combining their scores.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a feature map F of a pre-trained GAN
# Output: a part factor P and an appearance factor A

# Step 1: Apply semi-nonnegative tensor factorization (SNTF) to F
P, A = SNTF(F)

# Step 2: Perform image editing by manipulating P and A
# Example: change the appearance of the i-th part
A[i] = new_appearance # modify the appearance factor
F_new = P * A # reconstruct the feature map
image_new = GAN(F_new) # generate the edited image

# Step 3: Generate saliency maps by using A as a proxy for saliency
# Example: generate a saliency map for the i-th concept
saliency_score[i] = variance(A[i]) # compute the saliency score
saliency_map[i] = sum(P[:,j] * saliency_score[i][j] for j in range(P.shape[1])) # compute the saliency map
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a feature map F of a pre-trained GAN
# Output: a part factor P and an appearance factor A

# Step 1: Apply semi-nonnegative tensor factorization (SNTF) to F
# Initialize P and A randomly with nonnegative values
P = random_nonnegative_matrix(F.shape[0], k) # k is the number of parts
A = random_nonnegative_matrix(k, F.shape[1])
# Repeat until convergence or maximum iterations
while not converged or not max_iter:
  # Update P by solving a least squares problem with nonnegativity and orthogonality constraints
  P = argmin_P ||F - P * A||_F^2 s.t. P >= 0 and P.T * P = I
  # Update A by solving a least squares problem with nonnegativity constraint
  A = argmin_A ||F - P * A||_F^2 s.t. A >= 0

# Step 2: Perform image editing by manipulating P and A
# Example: change the appearance of the i-th part
A[i] = new_appearance # modify the appearance factor
F_new = P * A # reconstruct the feature map
image_new = GAN(F_new) # generate the edited image

# Step 3: Generate saliency maps by using A as a proxy for saliency
# Example: generate a saliency map for the i-th concept
saliency_score[i] = variance(A[i]) # compute the saliency score
saliency_map[i] = sum(P[:,j] * saliency_score[i][j] for j in range(P.shape[1])) # compute the saliency map
```