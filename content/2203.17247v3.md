---
title: 2203.17247v3 VL-InterpreT  An Interactive Visualization Tool for Interpreting Vision-Language Transformers
date: 2022-03-18
---

# [VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers](http://arxiv.org/abs/2203.17247v3)

authors: Estelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei Liu, Chenfei Wu, Nan Duan, Vasudev Lal


## What, Why and How

[1]: https://arxiv.org/abs/2203.17247v3 "[2203.17247v3] VL-InterpreT: An Interactive Visualization Tool for ..."
[2]: https://arxiv.org/pdf/2203.17247v3.pdf "arXiv:2203.17247v3 [cs.CV] 22 Aug 2022"

Here is a summary of the paper at [^1^][2] in terms of What, Why, and How:

- What: The paper introduces **VL-InterpreT**, an interactive visualization tool for interpreting the attentions and hidden representations of vision-language transformers.
- Why: The paper aims to provide novel insights into the inner workings of vision-language transformers, which are increasingly successful but largely opaque models for multimodal tasks. The paper argues that understanding these models will lead to more capable and trustworthy systems.
- How: The paper proposes a task agnostic and integrated tool that (1) tracks a variety of statistics in attention heads throughout all layers for both vision and language components, (2) visualizes cross-modal and intra-modal attentions through easily readable heatmaps, and (3) plots the hidden representations of vision and language tokens as they pass through the transformer layers. The paper demonstrates the functionalities of VL-InterpreT through the analysis of KD-VLP, an end-to-end pretraining vision-language multimodal transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and WebQA, two visual question answering benchmarks. The paper also presents some interesting findings about multimodal transformer behaviors that were learned through the tool.

## Main Contributions

[1]: https://arxiv.org/abs/2203.17247v3 "[2203.17247v3] VL-InterpreT: An Interactive Visualization Tool for ..."
[2]: https://arxiv.org/pdf/2203.17247v3.pdf "arXiv:2203.17247v3 [cs.CV] 22 Aug 2022"

According to the paper at [^1^][2], the contributions are:

- The paper proposes **VL-InterpreT**, which is the first interactive visualization tool for interpreting the attentions and hidden representations of vision-language transformers.
- The paper demonstrates the functionalities of VL-InterpreT through the analysis of **KD-VLP**, an end-to-end pretraining vision-language multimodal transformer-based model, in the tasks of **Visual Commonsense Reasoning (VCR)** and **WebQA**, two visual question answering benchmarks.
- The paper presents some interesting findings about multimodal transformer behaviors that were learned through the tool, such as the role of special tokens, the distribution of attention weights, and the evolution of hidden representations.

## Method Summary

[1]: https://arxiv.org/abs/2203.17247v3 "[2203.17247v3] VL-InterpreT: An Interactive Visualization Tool for ..."
[2]: https://arxiv.org/pdf/2203.17247v3.pdf "arXiv:2203.17247v3 [cs.CV] 22 Aug 2022"

Here is a summary of the method section of the paper at [^1^][2]:

- The paper describes the design and implementation of VL-InterpreT, which consists of three main components: **Attention Statistics**, **Attention Heatmaps**, and **Hidden Representation Plots**.
- The paper explains how VL-InterpreT tracks and visualizes various statistics of attention heads, such as entropy, sparsity, and similarity, for both vision and language components across all layers of the transformer. The paper also shows how VL-InterpreT allows users to filter and compare attention heads based on these statistics.
- The paper illustrates how VL-InterpreT generates and displays cross-modal and intra-modal attention heatmaps for each attention head, which show the strength of the attention between different tokens. The paper also demonstrates how VL-InterpreT enables users to interact with the heatmaps by selecting different tokens or attention heads, and how VL-InterpreT provides additional information such as token embeddings and word clouds for each token.
- The paper describes how VL-InterpreT plots and analyzes the hidden representations of vision and language tokens as they pass through the transformer layers. The paper also details how VL-InterpreT applies dimensionality reduction techniques such as PCA and t-SNE to project the high-dimensional hidden vectors into a 2D space, and how VL-InterpreT allows users to explore the hidden representations by zooming, panning, or hovering over the plots.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a vision-language transformer model
model = VLTransformer()

# Load a multimodal dataset
dataset = load_dataset()

# For each sample in the dataset
for sample in dataset:
  # Get the image, question, and answer from the sample
  image = sample["image"]
  question = sample["question"]
  answer = sample["answer"]

  # Encode the image and question into tokens
  image_tokens = encode_image(image)
  question_tokens = encode_question(question)

  # Concatenate the image and question tokens
  input_tokens = concatenate(image_tokens, question_tokens)

  # Pass the input tokens through the model
  output_tokens = model(input_tokens)

  # Decode the output tokens into an answer
  predicted_answer = decode_answer(output_tokens)

  # Compare the predicted answer with the ground truth answer
  compare(predicted_answer, answer)

# Visualize the attentions and hidden representations of the model using VL-InterpreT
VLInterpreT(model)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np
import matplotlib.pyplot as plt
import sklearn

# Define a vision-language transformer model
class VLTransformer(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pretrained ResNet-50 model to extract image features
    self.image_encoder = torchvision.models.resnet50(pretrained=True)
    # Use a pretrained BERT tokenizer and model to encode and decode text
    self.text_tokenizer = transformers.BertTokenizer.from_pretrained("bert-base-uncased")
    self.text_encoder = transformers.BertModel.from_pretrained("bert-base-uncased")
    self.text_decoder = transformers.BertForMaskedLM.from_pretrained("bert-base-uncased")
    # Use a linear layer to project the image features to the same dimension as the text embeddings
    self.image_projector = torch.nn.Linear(2048, 768)
    # Use a special token to separate the image and question tokens
    self.sep_token = "[SEP]"
    # Use a special token to mask the answer tokens
    self.mask_token = "[MASK]"

  def forward(self, input_tokens):
    # Get the image and question tokens from the input tokens
    image_tokens = input_tokens[:49]
    question_tokens = input_tokens[49:]

    # Pass the image tokens through the image encoder and get the image features
    image_features = self.image_encoder(image_tokens)

    # Pass the image features through the image projector and get the image embeddings
    image_embeddings = self.image_projector(image_features)

    # Pass the question tokens through the text encoder and get the question embeddings
    question_embeddings = self.text_encoder(question_tokens)

    # Concatenate the image and question embeddings along the sequence dimension
    input_embeddings = torch.cat([image_embeddings, question_embeddings], dim=1)

    # Pass the input embeddings through the text decoder and get the output logits
    output_logits = self.text_decoder(input_embeddings)

    # Return the output logits
    return output_logits

# Load a multimodal dataset
dataset = load_dataset()

# For each sample in the dataset
for sample in dataset:
  # Get the image, question, and answer from the sample
  image = sample["image"]
  question = sample["question"]
  answer = sample["answer"]

  # Encode the image into a tensor of shape (3, 224, 224)
  image_tensor = torchvision.transforms.ToTensor()(image)
  image_tensor = torchvision.transforms.Resize((224, 224))(image_tensor)

  # Encode the question into a list of token ids using the text tokenizer
  question_token_ids = self.text_tokenizer.encode(question)

  # Encode the answer into a list of token ids using the text tokenizer
  answer_token_ids = self.text_tokenizer.encode(answer)

  # Mask the answer token ids with the mask token id
  mask_token_id = self.text_tokenizer.convert_tokens_to_ids(self.mask_token)
  masked_answer_token_ids = [mask_token_id] * len(answer_token_ids)

  # Concatenate the question token ids and the masked answer token ids with a separator token id
  sep_token_id = self.text_tokenizer.convert_tokens_to_ids(self.sep_token)
  question_and_masked_answer_token_ids = question_token_ids + [sep_token_id] + masked_answer_token_ids

  # Concatenate the image tensor and the question and masked answer token ids into a list of input tokens
  input_tokens = [image_tensor] + question_and_masked_answer_token_ids

  # Pass the input tokens through the model
  output_logits = model(input_tokens)

  # Get the output token ids by taking the argmax of the output logits along the vocabulary dimension
  output_token_ids = torch.argmax(output_logits, dim=-1)

  # Decode the output token ids into an answer using the text tokenizer
  predicted_answer = self.text_tokenizer.decode(output_token_ids)

  # Compare the predicted answer with the ground truth answer
  compare(predicted_answer, answer)

# Visualize the attentions and hidden representations of the model using VL-InterpreT

# Define a function to get the attention weights and hidden states from a given layer of the model
def get_attention_and_hidden(layer):
  # Get the attention weights from the layer's multi-head attention module
  attention_weights = layer.attention.self.attention_weights

  # Get the hidden states from the layer's output module
  hidden_states = layer.output.hidden_states

  # Return the attention weights and hidden states
  return attention_weights, hidden_states

# Define a function to plot the attention heatmap for a given pair of tokens
def plot_attention_heatmap(token1, token2, attention_weights):
  # Get the index of the token1 and token2 in the input tokens
  token1_index = input_tokens.index(token1)
  token2_index = input_tokens.index(token2)

  # Get the attention weight between the token1 and token2 from the attention weights
  attention_weight = attention_weights[token1_index, token2_index]

  # Create a 2D array of zeros with the shape of (len(input_tokens), len(input_tokens))
  attention_array = np.zeros((len(input_tokens), len(input_tokens)))

  # Set the value of the attention array at the position of (token1_index, token2_index) to the attention weight
  attention_array[token1_index, token2_index] = attention_weight

  # Plot the attention array as a heatmap using matplotlib
  plt.imshow(attention_array, cmap="Blues")
  plt.colorbar()
  plt.xlabel("Input tokens")
  plt.ylabel("Input tokens")
  plt.title(f"Attention weight between {token1} and {token2}")
  plt.show()

# Define a function to plot the hidden representation for a given token
def plot_hidden_representation(token, hidden_states):
  # Get the index of the token in the input tokens
  token_index = input_tokens.index(token)

  # Get the hidden state of the token from the hidden states
  hidden_state = hidden_states[token_index]

  # Apply PCA to reduce the dimensionality of the hidden state to 2
  pca = sklearn.decomposition.PCA(n_components=2)
  pca.fit(hidden_state)
  hidden_state_pca = pca.transform(hidden_state)

  # Plot the hidden state PCA as a scatter plot using matplotlib
  plt.scatter(hidden_state_pca[:,0], hidden_state_pca[:,1], c="red")
  plt.xlabel("PCA component 1")
  plt.ylabel("PCA component 2")
  plt.title(f"Hidden representation of {token}")
  plt.show()

# For each layer in the model
for layer in model.text_encoder.layers:
  # Get the attention weights and hidden states from the layer
  attention_weights, hidden_states = get_attention_and_hidden(layer)

  # For each pair of tokens in the input tokens
  for token1, token2 in itertools.combinations(input_tokens, 2):
    # Plot the attention heatmap for the pair of tokens
    plot_attention_heatmap(token1, token2, attention_weights)

    # For each token in the pair of tokens
    for token in [token1, token2]:
      # Plot the hidden representation for the token
      plot_hidden_representation(token, hidden_states)
```