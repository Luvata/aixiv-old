---
title: 2305.18079v3 Towards a Robust Framework for NeRF Evaluation
date: 2023-05-19
---

# [Towards a Robust Framework for NeRF Evaluation](http://arxiv.org/abs/2305.18079v3)

authors: Adrian Azzarelli, Nantheera Anantrasirichai, David R Bull


## What, Why and How

[1]: https://arxiv.org/abs/2305.17916 "[2305.17916] Volume Feature Rendering for Fast Neural ... - arXiv.org"
[2]: https://arxiv.org/pdf/2305.18479v1 "FMM-X3D: FPGA-based modeling and mapping of X3D for Human ... - arXiv.org"
[3]: https://arxiv-export3.library.cornell.edu/abs/2305.18079v3 "[2305.18079v3] Towards a Robust Framework for NeRF Evaluation"

Here is a summary of the paper:

- **What**: The paper proposes a framework for evaluating NeRF models based on four criteria: accuracy, efficiency, generalization, and robustness. The framework includes a set of metrics, datasets, and baselines to compare different NeRF models and identify their strengths and weaknesses.
- **Why**: The paper argues that current NeRF evaluation methods are insufficient, inconsistent, and biased. They do not capture the full spectrum of NeRF performance and quality, and they do not account for the diversity of NeRF applications and scenarios. The paper aims to provide a comprehensive and fair evaluation framework that can facilitate NeRF research and development.
- **How**: The paper defines four criteria for NeRF evaluation and proposes corresponding metrics for each criterion. The criteria are:

  - **Accuracy**: The ability to reconstruct the appearance of a scene from novel viewpoints. The metrics are: PSNR, SSIM, LPIPS, FID, and NDCG.
  - **Efficiency**: The trade-off between accuracy and computational cost. The metrics are: FPS, GFLOPs, memory usage, and model size.
  - **Generalization**: The ability to handle diverse scenes with varying complexity, lighting, geometry, and motion. The metrics are: scene coverage, scene diversity, scene difficulty, and scene motion.
  - **Robustness**: The ability to cope with challenging conditions such as occlusions, reflections, transparency, and low-resolution inputs. The metrics are: occlusion ratio, reflection ratio, transparency ratio, and input resolution.

The paper also introduces two new datasets for NeRF evaluation: NeRF-Synth and NeRF-Real. NeRF-Synth contains synthetic scenes with ground-truth geometry and appearance. NeRF-Real contains real-world scenes captured with a handheld camera. Both datasets cover a wide range of scenes with different characteristics and challenges.

The paper evaluates six representative NeRF models using the proposed framework and compares their performance on the four criteria. The paper also discusses the limitations and future directions of the framework.

## Main Contributions

The paper claims to make the following contributions:

- It proposes a robust framework for NeRF evaluation based on four criteria: accuracy, efficiency, generalization, and robustness.
- It introduces two new datasets for NeRF evaluation: NeRF-Synth and NeRF-Real, which cover a wide range of scenes with different characteristics and challenges.
- It evaluates six representative NeRF models using the proposed framework and compares their performance on the four criteria.
- It identifies the strengths and weaknesses of different NeRF models and provides insights for future NeRF research and development.

## Method Summary

The method section of the paper consists of three subsections:

- **Evaluation Criteria and Metrics**: This subsection defines the four criteria for NeRF evaluation and proposes corresponding metrics for each criterion. It also explains how to compute and interpret the metrics using the NeRF-Synth and NeRF-Real datasets.
- **Datasets**: This subsection introduces the two new datasets for NeRF evaluation: NeRF-Synth and NeRF-Real. It describes how the scenes were generated or captured, how they were split into training, validation, and test sets, and how they were annotated with ground-truth information. It also provides statistics and examples of the scenes in each dataset.
- **Baselines**: This subsection describes the six representative NeRF models that were evaluated using the proposed framework. It summarizes their main features, architectures, and training settings. It also lists the hardware and software configurations used for the experiments.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the four criteria and metrics for NeRF evaluation
criteria = ["accuracy", "efficiency", "generalization", "robustness"]
metrics = {
  "accuracy": ["PSNR", "SSIM", "LPIPS", "FID", "NDCG"],
  "efficiency": ["FPS", "GFLOPs", "memory usage", "model size"],
  "generalization": ["scene coverage", "scene diversity", "scene difficulty", "scene motion"],
  "robustness": ["occlusion ratio", "reflection ratio", "transparency ratio", "input resolution"]
}

# Load the two datasets for NeRF evaluation: NeRF-Synth and NeRF-Real
datasets = ["NeRF-Synth", "NeRF-Real"]
for dataset in datasets:
  load_dataset(dataset)
  split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)
  annotate_dataset(dataset, ground_truth_geometry, ground_truth_appearance)

# Load the six representative NeRF models: NeRF, NeRF-W, NSVF, IDR, IBRNet, and PixelNeRF
models = ["NeRF", "NeRF-W", "NSVF", "IDR", "IBRNet", "PixelNeRF"]
for model in models:
  load_model(model)
  set_model_architecture(model)
  set_model_training_settings(model)

# Evaluate each model on each dataset using each criterion and metric
results = {}
for model in models:
  results[model] = {}
  for dataset in datasets:
    results[model][dataset] = {}
    for criterion in criteria:
      results[model][dataset][criterion] = {}
      for metric in metrics[criterion]:
        results[model][dataset][criterion][metric] = compute_metric(model, dataset, metric)

# Compare the performance of different models on different criteria and metrics
for criterion in criteria:
  for metric in metrics[criterion]:
    compare_results(results, criterion, metric)
    plot_results(results, criterion, metric)
    analyze_results(results, criterion, metric)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Define the four criteria and metrics for NeRF evaluation
criteria = ["accuracy", "efficiency", "generalization", "robustness"]
metrics = {
  "accuracy": ["PSNR", "SSIM", "LPIPS", "FID", "NDCG"],
  "efficiency": ["FPS", "GFLOPs", "memory usage", "model size"],
  "generalization": ["scene coverage", "scene diversity", "scene difficulty", "scene motion"],
  "robustness": ["occlusion ratio", "reflection ratio", "transparency ratio", "input resolution"]
}

# Define the functions to compute each metric
def compute_PSNR(model, dataset):
  # Compute the peak signal-to-noise ratio between the model output and the ground truth image for each test image
  psnr = []
  for image in dataset.test_images:
    output = model.render(image.viewpoint)
    psnr.append(skimage.metrics.peak_signal_noise_ratio(image, output))
  # Return the average PSNR over all test images
  return np.mean(psnr)

def compute_SSIM(model, dataset):
  # Compute the structural similarity index measure between the model output and the ground truth image for each test image
  ssim = []
  for image in dataset.test_images:
    output = model.render(image.viewpoint)
    ssim.append(skimage.metrics.structural_similarity(image, output, multichannel=True))
  # Return the average SSIM over all test images
  return np.mean(ssim)

def compute_LPIPS(model, dataset):
  # Compute the learned perceptual image patch similarity between the model output and the ground truth image for each test image using a pretrained VGG network
  lpips = []
  vgg = torchvision.models.vgg16(pretrained=True).eval()
  for image in dataset.test_images:
    output = model.render(image.viewpoint)
    lpips.append(torch.nn.functional.l1_loss(vgg(image), vgg(output)))
  # Return the average LPIPS over all test images
  return np.mean(lpips)

def compute_FID(model, dataset):
  # Compute the Fr√©chet inception distance between the model output and the ground truth image for each test image using a pretrained Inception network
  fid = []
  inception = torchvision.models.inception_v3(pretrained=True).eval()
  for image in dataset.test_images:
    output = model.render(image.viewpoint)
    fid.append(scipy.stats.wasserstein_distance(inception(image), inception(output)))
  # Return the average FID over all test images
  return np.mean(fid)

def compute_NDCG(model, dataset):
  # Compute the normalized discounted cumulative gain between the model output and the ground truth image for each test image using a pretrained ResNet network
  ndcg = []
  resnet = torchvision.models.resnet50(pretrained=True).eval()
  for image in dataset.test_images:
    output = model.render(image.viewpoint)
    ndcg.append(sklearn.metrics.ndcg_score(resnet(image), resnet(output)))
  # Return the average NDCG over all test images
  return np.mean(ndcg)

def compute_FPS(model, dataset):
  # Compute the frames per second of the model rendering on a given device for each test image
  fps = []
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)
  for image in dataset.test_images:
    start_time = time.time()
    output = model.render(image.viewpoint)
    end_time = time.time()
    fps.append(1 / (end_time - start_time))
  # Return the average FPS over all test images
  return np.mean(fps)

def compute_GFLOPs(model, dataset):
  # Compute the giga floating point operations per second of the model rendering on a given device for each test image
  gflops = []
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)
  flops_counter = thop.profile(model, inputs=(dataset.test_images[0].viewpoint,))
  for image in dataset.test_images:
    start_time = time.time()
    output = model.render(image.viewpoint)
    end_time = time.time()
    gflops.append(flops_counter / (end_time - start_time) / (10 ** 9))
  # Return the average GFLOPs over all test images
  return np.mean(gflops)

def compute_memory_usage(model, dataset):
  # Compute the memory usage of the model rendering on a given device for each test image
  memory_usage = []
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  model.to(device)
  for image in dataset.test_images:
    start_memory = torch.cuda.memory_allocated(device) if device == "cuda" else psutil.Process(os.getpid()).memory_info().rss
    output = model.render(image.viewpoint)
    end_memory = torch.cuda.memory_allocated(device) if device == "cuda" else psutil.Process(os.getpid()).memory_info().rss
    memory_usage.append(end_memory - start_memory)
  # Return the average memory usage over all test images
  return np.mean(memory_usage)

def compute_model_size(model, dataset):
  # Compute the model size in terms of number of parameters
  model_size = sum(p.numel() for p in model.parameters())
  # Return the model size
  return model_size

def compute_scene_coverage(model, dataset):
  # Compute the scene coverage of the model as the percentage of test viewpoints that can be rendered without errors
  scene_coverage = []
  for scene in dataset.test_scenes:
    error_count = 0
    for viewpoint in scene.test_viewpoints:
      try:
        output = model.render(viewpoint)
      except Exception as e:
        error_count += 1
    scene_coverage.append(1 - error_count / len(scene.test_viewpoints))
  # Return the average scene coverage over all test scenes
  return np.mean(scene_coverage)

def compute_scene_diversity(model, dataset):
  # Compute the scene diversity of the model as the variance of the accuracy metrics over different scenes
  scene_diversity = {}
  for metric in metrics["accuracy"]:
    scene_diversity[metric] = []
    for scene in dataset.test_scenes:
      scene_diversity[metric].append(compute_metric(model, scene, metric))
    scene_diversity[metric] = np.var(scene_diversity[metric])
  # Return the scene diversity for each accuracy metric
  return scene_diversity

def compute_scene_difficulty(model, dataset):
  # Compute the scene difficulty of the model as the correlation between the accuracy metrics and the scene complexity
  scene_difficulty = {}
  for metric in metrics["accuracy"]:
    scene_difficulty[metric] = []
    for scene in dataset.test_scenes:
      scene_difficulty[metric].append((compute_metric(model, scene, metric), compute_scene_complexity(scene)))
    scene_difficulty[metric] = scipy.stats.pearsonr(scene_difficulty[metric])[0]
  # Return the scene difficulty for each accuracy metric
  return scene_difficulty

def compute_scene_motion(model, dataset):
  # Compute the scene motion of the model as the accuracy metrics on dynamic scenes versus static scenes
  scene_motion = {}
  for metric in metrics["accuracy"]:
    scene_motion[metric] = []
    for scene in dataset.test_scenes:
      if scene.is_dynamic():
        dynamic_score = compute_metric(model, scene, metric)
      else:
        static_score = compute_metric(model, scene, metric)
    scene_motion[metric] = (dynamic_score, static_score)
  # Return the scene motion for each accuracy metric
  return scene_motion

def compute_occlusion_ratio(model, dataset):
  # Compute the occlusion ratio of the model as the percentage of test viewpoints that have occluded objects in the ground truth image
  occlusion_ratio = []
  for image in dataset.test_images:
    if image.has_occlusion():
      occlusion_ratio.append(1)
    else:
      occlusion_ratio.append(0)
  # Return the average occlusion ratio over all test images
  return np.mean(occlusion_ratio)

def compute_reflection_ratio(model, dataset):
  # Compute the reflection ratio of the model as the percentage of test viewpoints that have reflective objects in the ground truth image
  reflection_ratio = []
  for image in dataset.test_images:
    if image.has_reflection():
      reflection_ratio.append(1)
    else:
      reflection_ratio.append(0)
  # Return the average reflection ratio over all test images
  return np.mean(reflection_ratio)

def compute_transparency_ratio(model, dataset):
  # Compute the transparency ratio of the model as the percentage of test viewpoints that have transparent objects in the ground truth image
  transparency_ratio = []
  for image in dataset.test_images:
    if image.has_transparency():
      transparency_ratio.append(1)
    else:
      transparency_ratio.append(0)
  # Return the average transparency ratio over all test images
  return np.mean(transparency_ratio)

def compute_input_resolution(model, dataset):
  # Compute the input resolution of the model as the spatial dimensions of the input images
  input_resolution = []
  for image in dataset.test_images:
    input_resolution.append(image.shape[:2])
  # Return the average input resolution over all test images
  return np.mean(input_resolution, axis=0)

# Load the two datasets for NeRF evaluation: NeRF-Synth and