---
title: 2007.11301v3 DeepSVG  A Hierarchical Generative Network for Vector Graphics Animation
date: 2020-07-12
---

# [DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation](http://arxiv.org/abs/2007.11301v3)

authors: Alexandre Carlier, Martin Danelljan, Alexandre Alahi, Radu Timofte


## What, Why and How

[1]: https://arxiv.org/pdf/2007.11301v3.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2007.11301 "[2007.11301] DeepSVG: A Hierarchical Generative Network for Vector ..."
[3]: http://export.arxiv.org/abs/2207.11301v3 "[2207.11301v3] A Path Integral Ground State Monte Carlo Algorithm for ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel hierarchical generative network, called **DeepSVG**, for complex SVG icons generation and interpolation.
- **Why**: The paper aims to address the problem of vector graphics representation learning and generation, which is largely unexplored despite the ubiquity and scalability of SVG in modern 2D interfaces.
- **How**: The paper introduces a new large-scale dataset of SVG icons and an open-source library for SVG manipulation. The paper also designs a network architecture that effectively disentangles high-level shapes from the low-level commands that encode the shape itself. The network directly predicts a set of shapes in a non-autoregressive fashion, and can perform interpolations and other latent space operations to create animations.

## Main Contributions

[1]: https://arxiv.org/pdf/2007.11301v3.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2007.11301 "[2007.11301] DeepSVG: A Hierarchical Generative Network for Vector ..."
[3]: http://export.arxiv.org/abs/2207.11301v3 "[2207.11301v3] A Path Integral Ground State Monte Carlo Algorithm for ..."

The paper claims the following contributions:

- A novel hierarchical generative network, called **DeepSVG**, for complex SVG icons generation and interpolation, which effectively disentangles high-level shapes from the low-level commands that encode the shape itself[^1^][1] [^2^][2].
- A new large-scale dataset of SVG icons, containing over **100k** icons from **20** categories[^1^][1].
- An open-source library for SVG manipulation, which provides a unified interface for loading, editing, and rendering SVG files[^1^][1].
- A comprehensive evaluation of the proposed network on the tasks of reconstruction, generation, and interpolation of SVG icons, showing that it outperforms existing methods and can create realistic and diverse animations[^1^][1].

## Method Summary

[1]: https://arxiv.org/pdf/2007.11301v3.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2007.11301 "[2007.11301] DeepSVG: A Hierarchical Generative Network for Vector ..."
[3]: http://export.arxiv.org/abs/2207.11301v3 "[2207.11301v3] A Path Integral Ground State Monte Carlo Algorithm for ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces a new data representation for SVG icons, which consists of a list of shapes, each shape being a list of draw commands, such as lines and BÃ©zier curves. The paper also defines a set of operations to manipulate SVG icons, such as translation, rotation, scaling, and color change[^1^][1].
- The paper proposes a hierarchical generative network, called **DeepSVG**, which consists of two modules: a **Shape Encoder** and a **Shape Decoder**. The Shape Encoder takes as input a list of shapes and encodes each shape into a latent vector. The Shape Decoder takes as input a list of latent vectors and decodes each vector into a list of draw commands[^1^][1].
- The paper trains the network using a reconstruction loss, which measures the difference between the input and output SVG icons in terms of pixel-wise mean squared error (MSE) and structural similarity index (SSIM). The paper also uses a KL-divergence loss to regularize the latent space and encourage disentanglement[^1^][1].
- The paper evaluates the network on the tasks of reconstruction, generation, and interpolation of SVG icons. For reconstruction, the paper measures the MSE and SSIM between the input and output icons. For generation, the paper samples random latent vectors from a standard normal distribution and decodes them into SVG icons. For interpolation, the paper performs linear interpolation between two latent vectors and decodes the intermediate vectors into SVG icons[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the Shape Encoder module
class ShapeEncoder(nn.Module):
  def __init__(self):
    # Initialize the network parameters
    self.embedding = nn.Embedding(num_draw_commands, embedding_dim)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)
    self.fc = nn.Linear(2 * hidden_dim, latent_dim)

  def forward(self, shape):
    # Encode a shape into a latent vector
    # shape: a list of draw commands
    # output: a latent vector of size latent_dim
    shape = self.embedding(shape) # embed the draw commands into vectors
    shape, _ = self.lstm(shape) # pass the embedded vectors through a bidirectional LSTM
    shape = torch.mean(shape, dim=0) # average the LSTM outputs over time
    output = self.fc(shape) # project the averaged output into the latent space
    return output

# Define the Shape Decoder module
class ShapeDecoder(nn.Module):
  def __init__(self):
    # Initialize the network parameters
    self.fc = nn.Linear(latent_dim, hidden_dim)
    self.lstm_cell = nn.LSTMCell(hidden_dim, hidden_dim)
    self.out = nn.Linear(hidden_dim, num_draw_commands)

  def forward(self, z):
    # Decode a latent vector into a list of draw commands
    # z: a latent vector of size latent_dim
    # output: a list of draw commands
    z = self.fc(z) # project the latent vector into the hidden space
    h, c = z, z # initialize the LSTM cell state with the projected vector
    output = [] # initialize the output list
    for i in range(max_shape_length):
      h, c = self.lstm_cell(h, c) # update the LSTM cell state
      out = self.out(h) # predict the next draw command
      output.append(out) # append the predicted command to the output list
    return output

# Define the DeepSVG network
class DeepSVG(nn.Module):
  def __init__(self):
    # Initialize the network parameters
    self.shape_encoder = ShapeEncoder()
    self.shape_decoder = ShapeDecoder()

  def forward(self, icon):
    # Generate an SVG icon from an input icon or a random latent vector
    # icon: a list of shapes or None
    # output: a list of shapes
    if icon is not None:
      # Encode each shape in the input icon into a latent vector
      z_list = [self.shape_encoder(shape) for shape in icon]
      # Optionally apply some latent space operations, such as interpolation or arithmetic
      z_list = manipulate_latent_space(z_list)
    else:
      # Sample random latent vectors from a standard normal distribution
      z_list = [torch.randn(latent_dim) for i in range(num_shapes)]
    
    # Decode each latent vector in the list into a list of draw commands
    output = [self.shape_decoder(z) for z in z_list]
    
    return output

# Train the network using reconstruction and KL-divergence losses
def train(network, data_loader, optimizer):
  for epoch in range(num_epochs):
    for batch in data_loader:
      # Get a batch of SVG icons from the data loader
      icons = batch["icons"]
      # Pass the icons through the network and get the output icons
      output_icons = network(icons)
      # Compute the reconstruction loss between the input and output icons
      recon_loss = compute_reconstruction_loss(icons, output_icons)
      # Compute the KL-divergence loss between the latent vectors and a standard normal distribution
      kl_loss = compute_kl_loss(icons, output_icons)
      # Compute the total loss as a weighted sum of the reconstruction and KL-divergence losses
      loss = recon_loss + beta * kl_loss
      # Backpropagate the loss and update the network parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from svglib import SVGDataset, SVGParser, SVGRenderer # the open-source library for SVG manipulation

# Define some hyperparameters
num_draw_commands = 10 # the number of possible draw commands
embedding_dim = 64 # the dimension of the draw command embeddings
hidden_dim = 256 # the dimension of the LSTM hidden states
latent_dim = 128 # the dimension of the shape latent vectors
max_shape_length = 20 # the maximum number of draw commands per shape
num_shapes = 10 # the number of shapes per icon
beta = 0.1 # the weight for the KL-divergence loss
num_epochs = 100 # the number of training epochs
batch_size = 32 # the size of the training batch
learning_rate = 0.001 # the learning rate for the optimizer

# Define the Shape Encoder module
class ShapeEncoder(nn.Module):
  def __init__(self):
    # Initialize the network parameters
    super(ShapeEncoder, self).__init__()
    self.embedding = nn.Embedding(num_draw_commands, embedding_dim) # an embedding layer for the draw commands
    self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True) # a bidirectional LSTM layer for encoding the shape sequence
    self.fc = nn.Linear(2 * hidden_dim, latent_dim) # a fully connected layer for projecting the LSTM output into the latent space

  def forward(self, shape):
    # Encode a shape into a latent vector
    # shape: a tensor of size (max_shape_length) containing the draw command indices
    # output: a tensor of size (latent_dim) containing the latent vector
    shape = self.embedding(shape) # embed the draw commands into vectors of size (max_shape_length, embedding_dim)
    shape = shape.unsqueeze(1) # add a dummy batch dimension of size (max_shape_length, 1, embedding_dim)
    shape, _ = self.lstm(shape) # pass the embedded vectors through a bidirectional LSTM and get an output of size (max_shape_length, 2 * hidden_dim)
    shape = torch.mean(shape, dim=0) # average the LSTM outputs over time and get a tensor of size (1, 2 * hidden_dim)
    shape = shape.squeeze(0) # remove the dummy batch dimension and get a tensor of size (2 * hidden_dim)
    output = self.fc(shape) # project the averaged output into the latent space and get a tensor of size (latent_dim)
    return output

# Define the Shape Decoder module
class ShapeDecoder(nn.Module):
  def __init__(self):
    # Initialize the network parameters
    super(ShapeDecoder, self).__init__()
    self.fc = nn.Linear(latent_dim, hidden_dim) # a fully connected layer for projecting the latent vector into the hidden space
    self.lstm_cell = nn.LSTMCell(hidden_dim, hidden_dim) # an LSTM cell for decoding the latent vector into a shape sequence
    self.out = nn.Linear(hidden_dim, num_draw_commands) # a fully connected layer for predicting the next draw command

  def forward(self, z):
    # Decode a latent vector into a list of draw commands
    # z: a tensor of size (latent_dim) containing the latent vector
    # output: a tensor of size (max_shape_length, num_draw_commands) containing the draw command probabilities
    z = self.fc(z) # project the latent vector into the hidden space and get a tensor of size (hidden_dim)
    h, c = z, z # initialize the LSTM cell state with the projected vector, both tensors of size (hidden_dim)
    output = [] # initialize an empty list for storing the output tensors
    for i in range(max_shape_length):
      h, c = self.lstm_cell(h, c) # update the LSTM cell state and get new hidden and cell states, both tensors of size (hidden_dim)
      out = self.out(h) # predict the next draw command and get a tensor of size (num_draw_commands)
      output.append(out) # append the predicted tensor to the output list
    
    output = torch.stack(output) # stack the output tensors along a new dimension and get a tensor of size (max_shape_length, num_draw_commands)
    
    return output

# Define the DeepSVG network
class DeepSVG(nn.Module):
  def __init__(self):
    # Initialize the network parameters
    super(DeepSVG, self).__init__()
    self.shape_encoder = ShapeEncoder() # the shape encoder module
    self.shape_decoder = ShapeDecoder() # the shape decoder module

  def forward(self, icon):
    # Generate an SVG icon from an input icon or a random latent vector
    # icon: a tensor of size (num_shapes, max_shape_length) containing the draw command indices or None
    # output: a tensor of size (num_shapes, max_shape_length, num_draw_commands) containing the draw command probabilities
    if icon is not None:
      # Encode each shape in the input icon into a latent vector
      z_list = [self.shape_encoder(shape) for shape in icon] # get a list of tensors of size (latent_dim) for each shape
      z_list = torch.stack(z_list) # stack the tensors along a new dimension and get a tensor of size (num_shapes, latent_dim)
      # Optionally apply some latent space operations, such as interpolation or arithmetic
      z_list = manipulate_latent_space(z_list) # get a modified tensor of size (num_shapes, latent_dim)
    else:
      # Sample random latent vectors from a standard normal distribution
      z_list = torch.randn(num_shapes, latent_dim) # get a tensor of size (num_shapes, latent_dim)
    
    # Decode each latent vector in the list into a list of draw commands
    output = [self.shape_decoder(z) for z in z_list] # get a list of tensors of size (max_shape_length, num_draw_commands) for each latent vector
    
    output = torch.stack(output) # stack the tensors along a new dimension and get a tensor of size (num_shapes, max_shape_length, num_draw_commands)
    
    return output

# Define the reconstruction loss function
def compute_reconstruction_loss(icons, output_icons):
  # Compute the pixel-wise mean squared error and structural similarity index between the input and output icons
  # icons: a tensor of size (batch_size, num_shapes, max_shape_length) containing the draw command indices
  # output_icons: a tensor of size (batch_size, num_shapes, max_shape_length, num_draw_commands) containing the draw command probabilities
  # loss: a scalar tensor containing the reconstruction loss
  icons = icons.argmax(dim=-1) # convert the draw command indices to one-hot vectors of size (batch_size, num_shapes, max_shape_length)
  icons = icons.unsqueeze(-1) # add a dummy dimension of size (batch_size, num_shapes, max_shape_length, 1)
  icons = icons.expand(-1, -1, -1, num_draw_commands) # expand the dummy dimension to match the output size (batch_size, num_shapes, max_shape_length, num_draw_commands)
  output_icons = output_icons.softmax(dim=-1) # apply softmax to the output logits to get probabilities of size (batch_size, num_shapes, max_shape_length, num_draw_commands)
  icons = SVGRenderer.render(icons) # render the input icons as raster images of size (batch_size, image_height, image_width)
  output_icons = SVGRenderer.render(output_icons) # render the output icons as raster images of size (batch_size, image_height, image_width)
  mse_loss = nn.MSELoss() # define the mean squared error loss function
  ssim_loss = nn.SSIMLoss() # define the structural similarity index loss function
  loss = mse_loss(icons, output_icons) + ssim_loss(icons, output_icons) # compute the weighted sum of the two losses
  return loss

# Define the KL-divergence loss function
def compute_kl_loss(icons, output_icons):
  # Compute the KL-divergence between the latent vectors and a standard normal distribution
  # icons: a tensor of size (batch_size, num_shapes, max_shape_length) containing the draw command indices
  # output_icons: a tensor of size (batch_size, num_shapes, max_shape_length, num_draw_commands) containing the draw command probabilities
  # loss: a scalar tensor containing the KL-divergence loss
  z_list = [network.shape_encoder(shape) for shape in icons] # encode each shape in the input icon into a latent vector and get a list of tensors of size (latent_dim)
  z_list = torch.stack(z_list) # stack the tensors along a new dimension and get a tensor of size (batch_size * num_shapes, latent_dim)
  mu = torch.mean(z_list, dim=0) # compute the mean of the latent vectors and get a tensor of size (latent_dim)
  sigma = torch.std(z_list, dim=0) # compute the standard deviation of the latent vectors and get a tensor of size (latent_dim)
  normal_dist = torch.distributions.Normal(mu, sigma) # define a normal distribution with the computed mean and standard deviation
  standard_normal_dist = torch.distributions.Normal(torch.zeros(latent_dim), torch.ones(latent_dim)) # define a standard normal distribution with