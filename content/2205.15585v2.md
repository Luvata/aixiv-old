---
title: 2205.15585v2 Decomposing NeRF for Editing via Feature Field Distillation
date: 2022-05-16
---

# [Decomposing NeRF for Editing via Feature Field Distillation](http://arxiv.org/abs/2205.15585v2)

authors: Sosuke Kobayashi, Eiichi Matsumoto, Vincent Sitzmann


## What, Why and How

[1]: https://arxiv.org/abs/2205.15585 "Decomposing NeRF for Editing via Feature Field Distillation"
[2]: https://arxiv.org/pdf/2205.15585v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2211.15585v2 "[2211.15585v2] A black hole solution in conformal supergravity"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to decompose a neural radiance field (NeRF) into a 3D feature field and a radiance field, and use the feature field to enable query-based local editing of the 3D scene.
- **Why**: The paper aims to address the challenge of editing a scene represented by a NeRF, which is a powerful but opaque scene representation that does not support object-centric or compositional operations.
- **How**: The paper leverages off-the-shelf, self-supervised 2D image feature extractors such as CLIP-LSeg or DINO to distill their knowledge into a 3D feature field that is optimized in parallel to the radiance field. The feature field can then be used to semantically select and edit regions in the radiance field based on user-specified queries of various modalities such as text, an image patch, or a point-and-click selection.

## Main Contributions

[1]: https://arxiv.org/abs/2205.15585 "Decomposing NeRF for Editing via Feature Field Distillation"
[2]: https://arxiv.org/pdf/2205.15585v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2211.15585v2 "[2211.15585v2] A black hole solution in conformal supergravity"

According to the paper at [^1^][1], the main contributions are:

- **A novel method to decompose a NeRF into a 3D feature field and a radiance field**, which can be jointly optimized using off-the-shelf 2D image feature extractors.
- **A query-based editing framework that leverages the 3D feature field to semantically select and edit regions in the radiance field**, without requiring re-training or manual annotations.
- **A comprehensive evaluation of the proposed method on various 3D scenes and editing tasks**, demonstrating its effectiveness, robustness, and generality compared to existing methods.

## Method Summary

[1]: https://arxiv.org/abs/2205.15585 "Decomposing NeRF for Editing via Feature Field Distillation"
[2]: https://arxiv.org/pdf/2205.15585v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2211.15585v2 "[2211.15585v2] A black hole solution in conformal supergravity"

Here is a summary of the method section of the paper at [^1^][1]:

- The method consists of three main components: **3D feature field distillation**, **query-based 3D segmentation**, and **query-based 3D editing**.
- **3D feature field distillation** is the process of learning a 3D feature field that captures the semantic information of the scene from 2D image feature extractors. The paper uses two types of 2D image feature extractors: CLIP-LSeg and DINO, which are both self-supervised and can handle natural language and image patch queries. The paper defines a distillation loss that measures the similarity between the 2D image features and the 3D feature field at corresponding locations, and optimizes the 3D feature field along with the radiance field using gradient descent.
- **Query-based 3D segmentation** is the process of selecting a region of interest in the 3D scene based on a user-specified query. The paper supports three types of queries: text, image patch, and point-and-click. The paper computes a similarity score between the query and the 3D feature field at each location, and applies a threshold to obtain a binary mask that indicates the selected region. The paper also introduces a refinement step that uses graph-cut optimization to improve the boundary of the mask.
- **Query-based 3D editing** is the process of modifying the radiance field of the selected region based on a user-specified editing operation. The paper supports three types of editing operations: color adjustment, texture replacement, and object removal. The paper applies the editing operation to the radiance field only at the locations where the mask is active, and renders the edited scene using volume rendering. The paper also introduces a blending step that smooths the transition between the edited and unedited regions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a set of images of a 3D scene, a 2D image feature extractor, a query, and an editing operation
# Output: an edited image of the 3D scene from a novel viewpoint

# Define a 3D feature field and a radiance field as MLPs with positional encoding
feature_field = MLP()
radiance_field = MLP()

# Distill the 2D image features into the 3D feature field
for image in images:
  # Extract the 2D image features using the feature extractor
  image_features = feature_extractor(image)
  # Sample the 3D feature field and the radiance field along the camera rays
  feature_samples, radiance_samples = sample_fields(image, feature_field, radiance_field)
  # Compute the distillation loss between the 2D image features and the 3D feature field
  distillation_loss = compute_loss(image_features, feature_samples)
  # Update the feature field and the radiance field using gradient descent
  update_fields(distillation_loss, feature_field, radiance_field)

# Segment the region of interest in the 3D scene based on the query
# Compute the similarity score between the query and the 3D feature field at each location
similarity_score = compute_similarity(query, feature_field)
# Apply a threshold to obtain a binary mask
mask = threshold(similarity_score)
# Refine the mask using graph-cut optimization
mask = refine_mask(mask)

# Edit the radiance field of the selected region based on the editing operation
# Apply the editing operation to the radiance field only at the locations where the mask is active
radiance_field = edit_radiance(radiance_field, mask, editing_operation)
# Blend the edited and unedited regions using a smooth function
radiance_field = blend_radiance(radiance_field, mask)

# Render the edited scene from a novel viewpoint using volume rendering
edited_image = render_image(radiance_field, novel_viewpoint)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a set of images of a 3D scene, a 2D image feature extractor, a query, and an editing operation
# Output: an edited image of the 3D scene from a novel viewpoint

# Define the hyperparameters
num_layers = 8 # number of layers in the MLPs
num_freqs = 10 # number of frequencies in the positional encoding
num_samples = 64 # number of samples along each camera ray
distill_weight = 1.0 # weight for the distillation loss
similarity_threshold = 0.5 # threshold for the similarity score
graph_cut_weight = 0.1 # weight for the graph-cut optimization
blend_sigma = 0.1 # sigma for the blending function

# Define a 3D feature field and a radiance field as MLPs with positional encoding
feature_field = MLP(num_layers, num_freqs)
radiance_field = MLP(num_layers, num_freqs)

# Distill the 2D image features into the 3D feature field
for image in images:
  # Extract the 2D image features using the feature extractor
  image_features = feature_extractor(image) # shape: [H, W, C]
  # Sample the 3D feature field and the radiance field along the camera rays
  feature_samples, radiance_samples = sample_fields(image, feature_field, radiance_field) # shape: [H, W, num_samples, C]
  # Compute the distillation loss between the 2D image features and the 3D feature field
  distillation_loss = distill_weight * cosine_similarity_loss(image_features, feature_samples) # scalar
  # Update the feature field and the radiance field using gradient descent
  update_fields(distillation_loss, feature_field, radiance_field)

# Segment the region of interest in the 3D scene based on the query
# Compute the similarity score between the query and the 3D feature field at each location
similarity_score = compute_similarity(query, feature_field) # shape: [H, W]
# Apply a threshold to obtain a binary mask
mask = similarity_score > similarity_threshold # shape: [H, W]
# Refine the mask using graph-cut optimization
mask = refine_mask(mask, graph_cut_weight) # shape: [H, W]

# Edit the radiance field of the selected region based on the editing operation
# Apply the editing operation to the radiance field only at the locations where the mask is active
radiance_field = edit_radiance(radiance_field, mask, editing_operation)
# Blend the edited and unedited regions using a smooth function
radiance_field = blend_radiance(radiance_field, mask, blend_sigma)

# Render the edited scene from a novel viewpoint using volume rendering
edited_image = render_image(radiance_field, novel_viewpoint) # shape: [H', W', C']
```