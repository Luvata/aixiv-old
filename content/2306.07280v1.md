---
title: 2306.07280v1 Controlling Text-to-Image Diffusion by Orthogonal Finetuning
date: 2023-06-08
---

# [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](http://arxiv.org/abs/2306.07280v1)

authors: Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, Bernhard Sch√∂lkopf


## What, Why and How

[1]: https://arxiv.org/abs/2306.07280 "Controlling Text-to-Image Diffusion by Orthogonal Finetuning"
[2]: https://arxiv.org/pdf/2306.07280v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07280 "[2306.07280] Controlling Text-to-Image Diffusion by Orthogonal Finetuning"

Here is a summary of the paper:

```
What: The paper introduces a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks.

Why: The paper aims to address the challenge of how to effectively guide or control text-to-image diffusion models to perform different downstream tasks, such as subject-driven generation and controllable generation.

How: The paper proposes OFT, which can provably preserve hyperspherical energy, a property that is crucial for preserving the semantic generation ability of text-to-image diffusion models. The paper also proposes Constrained Orthogonal Finetuning (COFT), which imposes an additional radius constraint to the hypersphere. The paper empirically shows that OFT outperforms existing methods in generation quality and convergence speed.
```


## Main Contributions

[1]: https://arxiv.org/abs/2306.07280 "Controlling Text-to-Image Diffusion by Orthogonal Finetuning"
[2]: https://arxiv.org/pdf/2306.07280v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07280 "[2306.07280] Controlling Text-to-Image Diffusion by Orthogonal Finetuning"

Here are some of the contributions of the paper:

- The paper introduces a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks.
- The paper proves that OFT can preserve hyperspherical energy, a property that is crucial for preserving the semantic generation ability of text-to-image diffusion models.
- The paper proposes Constrained Orthogonal Finetuning (COFT), which imposes an additional radius constraint to the hypersphere to improve finetuning stability.
- The paper considers two important finetuning text-to-image tasks: subject-driven generation and controllable generation, and shows that OFT outperforms existing methods in generation quality and convergence speed.

## Method Summary

[1]: https://arxiv.org/abs/2306.07280 "Controlling Text-to-Image Diffusion by Orthogonal Finetuning"
[2]: https://arxiv.org/pdf/2306.07280v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07280 "[2306.07280] Controlling Text-to-Image Diffusion by Orthogonal Finetuning"

Here is a summary of the method section of the paper:

```
The paper proposes two methods for finetuning text-to-image diffusion models: Orthogonal Finetuning (OFT) and Constrained Orthogonal Finetuning (COFT).

OFT is based on the idea of orthogonal projection, which ensures that the finetuned parameters are orthogonal to the original parameters. This way, OFT can preserve hyperspherical energy, which characterizes the pairwise neuron relationship on the unit hypersphere. The paper proves that OFT can achieve this property under certain conditions.

COFT is an extension of OFT that adds a radius constraint to the hypersphere. This constraint prevents the finetuned parameters from deviating too much from the original parameters, which can improve finetuning stability and avoid overfitting.

The paper applies OFT and COFT to two finetuning text-to-image tasks: subject-driven generation and controllable generation. Subject-driven generation aims to generate subject-specific images given a few images of a subject and a text prompt. Controllable generation aims to enable the model to take in additional control signals, such as style codes or segmentation masks. The paper shows how to incorporate these signals into the diffusion process using OFT and COFT.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.07280 "Controlling Text-to-Image Diffusion by Orthogonal Finetuning"
[2]: https://arxiv.org/pdf/2306.07280v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07280 "[2306.07280] Controlling Text-to-Image Diffusion by Orthogonal Finetuning"

Here is the high-level pseudo code for the paper:

```
# Define the text-to-image diffusion model
model = TextToImageDiffusionModel()

# Define the finetuning task
task = SubjectDrivenGeneration() or ControllableGeneration()

# Define the finetuning method
method = OrthogonalFinetuning() or ConstrainedOrthogonalFinetuning()

# Finetune the model on the task using the method
model.finetune(task, method)

# Generate images from text prompts and control signals using the finetuned model
images = model.generate(text, control)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2306.07280 "Controlling Text-to-Image Diffusion by Orthogonal Finetuning"
[2]: https://arxiv.org/pdf/2306.07280v1.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2306.07280 "[2306.07280] Controlling Text-to-Image Diffusion by Orthogonal Finetuning"

Here is the detailed pseudo code to implement the paper:

```
# Define the text-to-image diffusion model
model = TextToImageDiffusionModel()

# Define the finetuning task
task = SubjectDrivenGeneration() or ControllableGeneration()

# Define the finetuning method
method = OrthogonalFinetuning() or ConstrainedOrthogonalFinetuning()

# Define the finetuning hyperparameters
learning_rate = 0.0001
batch_size = 32
num_steps = 10000

# Define the orthogonal projection function
def orthogonal_projection(original, update):
  # Compute the dot product between original and update
  dot_product = torch.sum(original * update, dim=-1, keepdim=True)
  # Compute the orthogonal component of update
  orthogonal_component = update - dot_product * original
  # Return the orthogonal component
  return orthogonal_component

# Define the radius constraint function
def radius_constraint(original, update, radius):
  # Compute the norm of original and update
  original_norm = torch.norm(original, dim=-1, keepdim=True)
  update_norm = torch.norm(update, dim=-1, keepdim=True)
  # Compute the ratio between original norm and radius
  ratio = original_norm / radius
  # Scale the update by the ratio
  scaled_update = ratio * update
  # Clip the scaled update norm by radius
  clipped_update = torch.clamp(scaled_update, max=radius)
  # Return the clipped update
  return clipped_update

# Finetune the model on the task using the method
for step in range(num_steps):
  # Sample a batch of data from the task
  data = task.sample(batch_size)
  # Extract text prompts and control signals from data
  text, control = data["text"], data["control"]
  # Forward pass the model on text and control signals
  output = model(text, control)
  # Compute the loss function based on output and data
  loss = task.loss(output, data)
  # Backward propagate the loss to compute gradients
  loss.backward()
  # Apply the finetuning method to modify gradients
  for name, param in model.named_parameters():
    # Get the original parameter value before update
    original = param.data.clone()
    # Get the gradient value after update
    update = -learning_rate * param.grad.data.clone()
    # Apply orthogonal projection to update if using OFT
    if method == OrthogonalFinetuning():
      update = orthogonal_projection(original, update)
    # Apply radius constraint to update if using COFT
    if method == ConstrainedOrthogonalFinetuning():
      radius = task.radius(name) # Get task-specific radius for each parameter name
      update = radius_constraint(original, update, radius)
    # Update the parameter value with modified gradient
    param.data.add_(update)
  
# Generate images from text prompts and control signals using the finetuned model
images = model.generate(text, control)
```