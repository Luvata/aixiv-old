---
title: 2106.14843v1 CLIPDraw  Exploring Text-to-Drawing Synthesis through Language-Image Encoders
date: 2021-06-15
---

# [CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders](http://arxiv.org/abs/2106.14843v1)

authors: Kevin Frans, L. B. Soros, Olaf Witkowski


## What, Why and How

[1]: https://arxiv.org/pdf/2106.14843v1.pdf "CLIPDraw: Exploring Text-to-Drawing Synthesisthrough Language-Image ..."
[2]: https://arxiv.org/abs/2106.14843 "[2106.14843] CLIPDraw: Exploring Text-to-Drawing Synthesis through ..."
[3]: http://export.arxiv.org/abs/2103.14843v1 "[2103.14843v1] From Synthetic to Real: Unsupervised Domain Adaptation ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper presents CLIPDraw, an algorithm that synthesizes novel drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language-image encoder is used as a metric for maximizing similarity between the given description and a generated drawing. CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes.
- **Why**: The paper aims to explore text-to-drawing synthesis through language-image encoders, and to demonstrate the capabilities and limitations of CLIPDraw as a novel method for this task. The paper also aims to highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased.
- **How**: The paper describes the implementation details of CLIPDraw, which consists of three main components: a Bézier curve generator, a renderer, and a CLIP model. The Bézier curve generator produces a set of RGBA Bézier curves that define the shape and color of the drawing. The renderer converts the curves into a pixel image that can be fed into the CLIP model. The CLIP model encodes both the image and the text prompt into a common representation space, and computes the cosine distance between them as a measure of similarity. The paper uses gradient descent to optimize the Bézier curves with respect to the similarity metric, and generates drawings that match the text prompt. The paper also compares CLIPDraw with other synthesis-through-optimization methods, such as BigGAN and VQGAN+CLIP, and evaluates the quality and diversity of the generated drawings using human ratings and Fréchet Inception Distance (FID).

## Main Contributions

According to the paper, the main contributions are:

- Introducing CLIPDraw, a novel algorithm for text-to-drawing synthesis that does not require any training and operates over vector strokes rather than pixel images.
- Demonstrating that CLIPDraw can generate diverse and interpretable drawings that match natural language prompts in various domains and styles.
- Analyzing the behavior and limitations of CLIPDraw, such as its sensitivity to text ambiguity, stroke count, and initialization.
- Comparing CLIPDraw with other synthesis-through-optimization methods and showing that CLIPDraw can produce higher quality and more diverse drawings according to human ratings and FID scores.

## Method Summary

[1]: https://arxiv.org/pdf/2106.14843v1.pdf "CLIPDraw: Exploring Text-to-Drawing Synthesisthrough Language-Image ..."
[2]: https://arxiv.org/abs/2106.14843 "[2106.14843] CLIPDraw: Exploring Text-to-Drawing Synthesis through ..."
[3]: http://export.arxiv.org/abs/2103.14843v1 "[2103.14843v1] From Synthetic to Real: Unsupervised Domain Adaptation ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the implementation details of CLIPDraw, which consists of three main components: a **Bézier curve generator**, a **renderer**, and a **CLIP model**.
- The Bézier curve generator produces a set of RGBA Bézier curves that define the shape and color of the drawing. Each curve is parameterized by four control points and an alpha value. The paper uses a fixed number of curves for each drawing, and initializes them randomly or with a prior distribution learned from human drawings.
- The renderer converts the curves into a pixel image that can be fed into the CLIP model. The paper uses an anti-aliased rasterization algorithm that renders each curve as a smooth line with variable thickness and opacity. The paper also applies a background color to the image based on the text prompt.
- The CLIP model encodes both the image and the text prompt into a common representation space, and computes the cosine distance between them as a measure of similarity. The paper uses a pre-trained CLIP model that is not fine-tuned or modified for this task. The paper also uses a temperature parameter to control the sharpness of the similarity metric.
- The paper uses gradient descent to optimize the Bézier curves with respect to the similarity metric, and generates drawings that match the text prompt. The paper uses Adam optimizer with a learning rate scheduler and gradient clipping. The paper also applies various regularization techniques, such as L2 penalty, curve smoothing, and diversity loss, to improve the quality and diversity of the drawings.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the number of curves and the temperature parameter
num_curves = 10
temperature = 0.07

# Initialize the Bézier curves randomly or with a prior distribution
curves = init_curves(num_curves)

# Load the pre-trained CLIP model
clip_model = load_clip_model()

# Define the text prompt
text_prompt = "a happy cat"

# Encode the text prompt with CLIP
text_encoding = clip_model.encode_text(text_prompt)

# Define the optimizer and the learning rate scheduler
optimizer = Adam(curves.parameters())
scheduler = CosineAnnealingLR(optimizer)

# Define the regularization parameters
l2_weight = 0.01
smooth_weight = 0.1
diversity_weight = 0.01

# Define the number of optimization steps
num_steps = 1000

# Loop over the optimization steps
for step in range(num_steps):

  # Zero the gradients
  optimizer.zero_grad()

  # Render the curves into an image
  image = render(curves)

  # Encode the image with CLIP
  image_encoding = clip_model.encode_image(image)

  # Compute the cosine similarity between the image and the text encodings
  similarity = cosine_similarity(image_encoding, text_encoding)

  # Compute the loss as the negative similarity divided by the temperature
  loss = -similarity / temperature

  # Add regularization terms to the loss
  loss += l2_weight * l2_norm(curves)
  loss += smooth_weight * smoothness(curves)
  loss += diversity_weight * diversity(curves)

  # Compute the gradients
  loss.backward()

  # Clip the gradients
  clip_grad_norm_(curves.parameters(), max_norm=1.0)

  # Update the curves with the optimizer
  optimizer.step()

  # Update the learning rate with the scheduler
  scheduler.step()

# Return the final image and curves
return image, curves

```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import clip
from PIL import Image

# Define the number of curves and the temperature parameter
num_curves = 10
temperature = 0.07

# Define the image size and the background color
image_size = 256
background_color = (255, 255, 255)

# Define the Bézier curve class
class BezierCurve(torch.nn.Module):

  def __init__(self, num_points=4, alpha=1.0):
    # Initialize the module
    super().__init__()

    # Initialize the control points randomly in the range [0, 1]
    self.points = torch.nn.Parameter(torch.rand(num_points, 2))

    # Initialize the color randomly in the range [0, 1]
    self.color = torch.nn.Parameter(torch.rand(3))

    # Initialize the alpha value as a constant
    self.alpha = torch.nn.Parameter(torch.tensor(alpha), requires_grad=False)

  def forward(self):
    # Return the points, color, and alpha as a tuple
    return self.points, self.color, self.alpha

# Define the function to initialize the curves randomly or with a prior distribution
def init_curves(num_curves):
  # Create an empty list to store the curves
  curves = []

  # Loop over the number of curves
  for i in range(num_curves):

    # Create a new Bézier curve with four control points and an alpha value of 0.5
    curve = BezierCurve(num_points=4, alpha=0.5)

    # Append the curve to the list
    curves.append(curve)

  # Return the list of curves as a module list
  return torch.nn.ModuleList(curves)

# Define the function to render the curves into an image
def render(curves):
  # Create an empty image with the background color
  image = Image.new("RGBA", (image_size, image_size), background_color)

  # Loop over the curves
  for curve in curves:

    # Get the points, color, and alpha from the curve
    points, color, alpha = curve()

    # Rescale the points and color to the image size and range [0, 255]
    points = (points * image_size).int().numpy()
    color = (color * 255).int().numpy()

    # Convert the points into a list of tuples
    points = [(x, y) for x, y in points]

    # Create a new image to draw the curve on
    curve_image = Image.new("RGBA", (image_size, image_size))

    # Create a drawing object to draw on the image
    draw = ImageDraw.Draw(curve_image)

    # Draw the curve as a smooth line with a thickness of 3 pixels and the given color and alpha
    draw.line(points, fill=tuple(color) + (int(alpha * 255),), width=3, joint="curve")

    # Composite the curve image onto the main image using alpha blending
    image = Image.alpha_composite(image, curve_image)

  # Return the image as a tensor normalized to the range [-1, 1]
  return clip.transforms.ToTensor()(image) * 2 - 1

# Load the pre-trained CLIP model
clip_model = clip.load("ViT-B/32", jit=False)[0].eval()

# Define the text prompt
text_prompt = "a happy cat"

# Encode the text prompt with CLIP and normalize it
text_encoding = clip_model.encode_text(clip.tokenize(text_prompt)).squeeze(0)
text_encoding /= text_encoding.norm(dim=-1, keepdim=True)

# Define the optimizer and the learning rate scheduler
optimizer = torch.optim.Adam(curves.parameters(), lr=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000)

# Define the regularization parameters
l2_weight = 0.01
smooth_weight = 0.1
diversity_weight = 0.01

# Define the function to compute the L2 norm of the curves
def l2_norm(curves):
  # Initialize the norm as zero
  norm = 0

  # Loop over the curves
  for curve in curves:

    # Get the points and color from the curve
    points, color, _ = curve()

    # Add the squared L2 norm of the points and color to the norm
    norm += torch.sum(points ** 2) + torch.sum(color ** 2)

  # Return the norm divided by the number of curves
  return norm / len(curves)

# Define the function to compute the smoothness of the curves
def smoothness(curves):
  # Initialize the smoothness as zero
  smoothness = 0

  # Loop over the curves
  for curve in curves:

    # Get the points from the curve
    points, _, _ = curve()

    # Compute the pairwise differences between the points
    diffs = points[1:] - points[:-1]

    # Compute the pairwise distances between the points
    dists = torch.sqrt(torch.sum(diffs ** 2, dim=-1))

    # Compute the pairwise angles between the points
    angles = torch.atan2(diffs[:, 1], diffs[:, 0])

    # Compute the pairwise differences between the angles
    angle_diffs = angles[1:] - angles[:-1]

    # Normalize the angle differences to the range [-pi, pi]
    angle_diffs = (angle_diffs + np.pi) % (2 * np.pi) - np.pi

    # Add the sum of squared distances and squared angle differences to the smoothness
    smoothness += torch.sum(dists ** 2) + torch.sum(angle_diffs ** 2)

  # Return the smoothness divided by the number of curves
  return smoothness / len(curves)

# Define the function to compute the diversity of the curves
def diversity(curves):
  # Initialize the diversity as zero
  diversity = 0

  # Loop over the pairs of curves
  for i in range(len(curves)):
    for j in range(i + 1, len(curves)):

      # Get the points and colors from the pair of curves
      points_i, color_i, _ = curves[i]()
      points_j, color_j, _ = curves[j]()

      # Compute the L2 distance between the colors
      color_dist = torch.sqrt(torch.sum((color_i - color_j) ** 2))

      # Compute the Chamfer distance between the points
      point_dist = chamfer_distance(points_i.unsqueeze(0), points_j.unsqueeze(0))

      # Add the product of color and point distances to the diversity
      diversity += color_dist * point_dist

  # Return the diversity divided by the number of pairs
  return diversity / (len(curves) * (len(curves) - 1) / 2)

# Define the function to compute the Chamfer distance between two sets of points
def chamfer_distance(x, y):
  # Compute the pairwise squared L2 distances between x and y
  x = x.unsqueeze(-2)
  y = y.unsqueeze(-3)
  dists = torch.sum((x - y) ** 2, dim=-1)

  # Compute the minimum distances from x to y and vice versa
  min_dists_x = torch.min(dists, dim=-1)[0]
  min_dists_y = torch.min(dists, dim=-2)[0]

  # Return the sum of minimum distances divided by the number of points
  return (torch.sum(min_dists_x) + torch.sum(min_dists_y)) / (x.shape[-2] + y.shape[-2])

# Define the number of optimization steps
num_steps = 1000

# Loop over the optimization steps
for step in range(num_steps):

  # Zero the gradients
  optimizer.zero_grad()

  # Render the curves into an image
  image = render(curves)

  # Encode the image with CLIP and normalize it
  image_encoding = clip_model.encode_image(image.unsqueeze(0)).squeeze(0)
  image_encoding /= image_encoding.norm(dim=-1, keepdim=True)

  # Compute the cosine similarity between the image and the text encodings
  similarity = torch.dot(image_encoding, text_encoding)

  # Compute the loss as the negative similarity divided by the temperature
  loss = -similarity / temperature

  # Add regularization terms to the loss
  loss += l2_weight * l2_norm(curves)
  loss += smooth_weight * smoothness(curves)
  loss += diversity_weight * diversity(curves)

  # Compute the gradients
  loss.backward()

  # Clip the gradients
  torch.nn.utils.clip_grad_norm_(curves.parameters(), max_norm=1.0)

  # Update the curves with the optimizer
  optimizer.step()

  # Update the learning rate with the scheduler
  scheduler.step()

# Return or save or display or use in some way final image and curves.
```