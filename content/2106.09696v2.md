---
title: 2106.09696v2 BABEL  Bodies, Action and Behavior with English Labels
date: 2021-06-10
---

# [BABEL: Bodies, Action and Behavior with English Labels](http://arxiv.org/abs/2106.09696v2)

authors: Abhinanda R. Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, Michael J. Black


## What, Why and How

[1]: https://arxiv.org/pdf/2106.09696v2 "BABEL: Bodies, Action and Behavior with English Labels - arXiv.org"
[2]: https://arxiv.org/abs/2106.09696 "BABEL: Bodies, Action and Behavior with English Labels"
[3]: http://export.arxiv.org/abs/1802.09696v2 "[1802.09696v2] Stacked lensing estimators and their covariance matrices ..."

Here is a summary of the paper:

- **What**: The paper presents BABEL, a large dataset with language labels describing the actions being performed in mocap sequences. BABEL contains sequence labels and frame labels for about 43 hours of mocap sequences from AMASS, which belong to over 250 unique action categories. BABEL can be used for tasks like action recognition, temporal action localization, motion synthesis, etc.
- **Why**: The paper aims to address the gap between large-scale video datasets that have many action labels but no ground-truth 3D human motion, and mocap datasets that have precise body motions but limited action diversity. The paper argues that understanding the semantics of human movement requires datasets that capture variations in human movement and language descriptions that express the what, how and why of the movement.
- **How**: The paper describes the data collection process, which involves annotating mocap sequences from AMASS with natural language labels using Amazon Mechanical Turk. The paper also defines the evaluation metrics and baselines for 3D action recognition using BABEL. The paper reports the results of the experiments and discusses the challenges and limitations of BABEL. The paper also provides the dataset, baseline method, and evaluation code at https://babel.is.tue.mpg.de/.

## Main Contributions

The paper lists the following contributions:

- It introduces BABEL, a large dataset with language labels describing the actions being performed in mocap sequences. BABEL contains sequence labels and frame labels for about 43 hours of mocap sequences from AMASS, which belong to over 250 unique action categories.
- It provides a benchmark for 3D action recognition using BABEL, and evaluates the performance of several models on this task. It demonstrates that BABEL poses interesting learning challenges that are applicable to real-world scenarios, and can serve as a useful benchmark of progress in 3D action recognition.
- It makes the dataset, baseline method, and evaluation code publicly available and supported for academic research purposes at https://babel.is.tue.mpg.de/.

## Method Summary

[1]: https://arxiv.org/pdf/2106.09696v2 "BABEL: Bodies, Action and Behavior with English Labels - arXiv.org"
[2]: https://arxiv.org/abs/2106.09696 "BABEL: Bodies, Action and Behavior with English Labels"
[3]: http://export.arxiv.org/abs/1802.09696v2 "[1802.09696v2] Stacked lensing estimators and their covariance matrices ..."

Here is a summary of the method section of the paper:

- The paper describes the data collection process, which involves annotating mocap sequences from AMASS with natural language labels using Amazon Mechanical Turk. The paper defines two types of labels: sequence labels and frame labels. Sequence labels describe the overall action in the sequence, and frame labels describe all actions in every frame of the sequence. Each frame label is precisely aligned with the duration of the corresponding action in the mocap sequence, and multiple actions can overlap. The paper also provides statistics and examples of the labels in BABEL.
- The paper defines the evaluation metrics and baselines for 3D action recognition using BABEL. The paper uses two metrics: mean average precision (mAP) and mean per-class accuracy (mAcc). The paper also introduces two baselines: a simple baseline that uses a linear classifier on top of hand-crafted features, and a strong baseline that uses a transformer-based model on top of learned features. The paper compares the performance of the baselines on different subsets of BABEL, such as single-action sequences, multi-action sequences, and simultaneous actions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Data collection
for each mocap sequence in AMASS:
  get sequence label from AMT worker
  for each frame in mocap sequence:
    get frame label from AMT worker
    align frame label with action duration
    handle overlapping actions
  end for
end for

# Evaluation metrics
define mAP and mAcc as functions of true and predicted labels

# Baselines
define simple baseline as linear classifier on hand-crafted features
define strong baseline as transformer model on learned features

# Experiments
for each subset of BABEL:
  train and test simple baseline and strong baseline
  compute mAP and mAcc for each baseline
  compare and analyze the results
end for
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Data collection
import numpy as np
import pandas as pd
import os
from tqdm import tqdm
from nltk.tokenize import word_tokenize

# Load AMASS dataset
AMASS_DIR = "path/to/amass/dataset"
amass_files = os.listdir(AMASS_DIR)

# Initialize BABEL dataset
BABEL_DIR = "path/to/babel/dataset"
babel_data = pd.DataFrame(columns=["sequence_id", "sequence_label", "frame_id", "frame_label"])

# Define action categories and vocabulary
action_categories = ["walk", "run", "jump", "sit", "stand", ...] # list of 250+ action categories
vocab = ["<pad>", "<unk>", "<sos>", "<eos>"] + action_categories # list of tokens for labels

# Define label tokenizer and encoder
def tokenize_label(label):
  # Split label into tokens and add <sos> and <eos> tokens
  tokens = ["<sos>"] + word_tokenize(label.lower()) + ["<eos>"]
  return tokens

def encode_label(tokens):
  # Convert tokens into indices using vocab
  indices = [vocab.index(token) if token in vocab else vocab.index("<unk>") for token in tokens]
  return indices

# Define label alignment function
def align_label(label, start_frame, end_frame, num_frames):
  # Align label with action duration by padding zeros before and after the label indices
  label_len = len(label)
  action_len = end_frame - start_frame + 1
  assert label_len <= action_len, "Label length cannot exceed action length"
  pad_before = start_frame
  pad_after = num_frames - end_frame - 1
  aligned_label = [0] * pad_before + label + [0] * pad_after
  return aligned_label

# Define label overlap function
def handle_overlap(labels):
  # Handle overlapping actions by taking the maximum of the label indices at each frame
  max_label = np.max(labels, axis=0)
  return max_label

# Loop over AMASS files
for amass_file in tqdm(amass_files):
  # Load mocap sequence from AMASS file
  mocap_seq = np.load(os.path.join(AMASS_DIR, amass_file))
  
  # Get sequence id from file name
  seq_id = amass_file.split(".")[0]

  # Get sequence label from AMT worker using a web interface (not shown here)
  seq_label = get_sequence_label_from_AMT_worker(seq_id, mocap_seq)

  # Tokenize and encode sequence label
  seq_tokens = tokenize_label(seq_label)
  seq_indices = encode_label(seq_tokens)

  # Get number of frames in mocap sequence
  num_frames = mocap_seq.shape[0]

  # Initialize frame labels as empty list
  frame_labels = []

  # Loop over frames in mocap sequence
  for frame_id in range(num_frames):
    # Get frame label from AMT worker using a web interface (not shown here)
    frame_label = get_frame_label_from_AMT_worker(seq_id, frame_id, mocap_seq)

    # Tokenize and encode frame label
    frame_tokens = tokenize_label(frame_label)
    frame_indices = encode_label(frame_tokens)

    # Append frame label to frame labels list
    frame_labels.append(frame_indices)
  
  # Convert frame labels list to numpy array
  frame_labels = np.array(frame_labels)

  # Align frame labels with action durations using start and end frames of each action (not shown here)
  aligned_frame_labels = []
  for frame_label in frame_labels:
    start_frame, end_frame = get_action_duration(frame_label) # get start and end frames of each action (not shown here)
    aligned_frame_label = align_label(frame_label, start_frame, end_frame, num_frames)
    aligned_frame_labels.append(aligned_frame_label)
  
  # Convert aligned frame labels list to numpy array
  aligned_frame_labels = np.array(aligned_frame_labels)

  # Handle overlapping actions by taking the maximum of the label indices at each frame
  overlapped_frame_labels = handle_overlap(aligned_frame_labels)

  # Save sequence id, sequence label, and frame labels to BABEL dataset
  babel_data.loc[len(babel_data)] = [seq_id, seq_indices, overlapped_frame_labels]

# Save BABEL dataset to disk
babel_data.to_csv(os.path.join(BABEL_DIR, "babel_data.csv"))

# Evaluation metrics

# Import libraries for evaluation metrics
from sklearn.metrics import average_precision_score, accuracy_score

# Define mean average precision (mAP) metric as the mean of the average precision scores for each action category
def mAP(true_labels, pred_labels):
  # true_labels and pred_labels are numpy arrays of shape (num_sequences, num_frames, num_categories)
  # where each element is either 0 or 1 indicating the presence or absence of an action category
  num_categories = true_labels.shape[-1]
  ap_scores = []
  for i in range(num_categories):
    # Compute the average precision score for each action category
    ap_score = average_precision_score(true_labels[:,:,i], pred_labels[:,:,i])
    ap_scores.append(ap_score)
  # Compute the mean of the average precision scores
  map_score = np.mean(ap_scores)
  return map_score

# Define mean per-class accuracy (mAcc) metric as the mean of the accuracy scores for each action category
def mAcc(true_labels, pred_labels):
  # true_labels and pred_labels are numpy arrays of shape (num_sequences, num_frames, num_categories)
  # where each element is either 0 or 1 indicating the presence or absence of an action category
  num_categories = true_labels.shape[-1]
  acc_scores = []
  for i in range(num_categories):
    # Compute the accuracy score for each action category
    acc_score = accuracy_score(true_labels[:,:,i], pred_labels[:,:,i])
    acc_scores.append(acc_score)
  # Compute the mean of the accuracy scores
  macc_score = np.mean(acc_scores)
  return macc_score

# Baselines

# Import libraries for baselines
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, BertTokenizer

# Define simple baseline as linear classifier on hand-crafted features

# Define hand-crafted features as a function of mocap sequence
def hand_crafted_features(mocap_seq):
  # mocap_seq is a numpy array of shape (num_frames, num_joints, num_coords)
  # where each element is the x, y, or z coordinate of a joint in a frame
  # Compute the mean and standard deviation of the coordinates across frames and joints
  mean_coords = np.mean(mocap_seq, axis=(0,1))
  std_coords = np.std(mocap_seq, axis=(0,1))
  # Concatenate the mean and standard deviation vectors to form a feature vector
  feature_vec = np.concatenate([mean_coords, std_coords])
  return feature_vec

# Define simple baseline model as a linear classifier with a sigmoid output layer
class SimpleBaseline(nn.Module):
  def __init__(self, input_size, output_size):
    super(SimpleBaseline, self).__init__()
    # Define a linear layer with input size and output size
    self.linear = nn.Linear(input_size, output_size)
    # Define a sigmoid layer for binary classification
    self.sigmoid = nn.Sigmoid()
  
  def forward(self, x):
    # x is a tensor of shape (batch_size, input_size)
    # Apply linear layer to x
    x = self.linear(x)
    # Apply sigmoid layer to x
    x = self.sigmoid(x)
    return x

# Define strong baseline as transformer model on learned features

# Define learned features as embeddings from a pre-trained BERT model

# Load pre-trained BERT model and tokenizer
bert_model = BertModel.from_pretrained("bert-base-uncased")
bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Define learned features as a function of sequence label
def learned_features(seq_label):
  # seq_label is a list of tokens for the sequence label
  # Convert tokens to ids using bert tokenizer
  seq_ids = bert_tokenizer.convert_tokens_to_ids(seq_label)
  # Convert ids to tensor and add batch dimension
  seq_ids = torch.tensor(seq_ids).unsqueeze(0)
  # Get embeddings from bert model
  embeddings = bert_model(seq_ids).last_hidden_state
  # Average embeddings across tokens to get a feature vector
  feature_vec = torch.mean(embeddings, dim=1)
  return feature_vec

# Define strong baseline model as a transformer model with a sigmoid output layer

# Define transformer model parameters
d_model = bert_model.config.hidden_size # input and output size of transformer model
nhead = bert_model.config.num_attention_heads # number of attention heads in transformer model
num_encoder_layers = bert_model.config.num_hidden_layers # number of encoder layers in transformer model
dim_feedforward = bert_model.config.intermediate_size # hidden size of feedforward network in transformer model

# Define transformer model class
class TransformerModel(nn.Module):
  def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward):
    super(TransformerModel, self).__init__()
    # Define an encoder layer with given parameters
    self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,
                                                    nhead=nhead,
                                                    dim