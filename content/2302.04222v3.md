---
title: 2302.04222v3 GLAZE  Protecting Artists from Style Mimicry by Text-to-Image Models
date: 2023-02-05
---

# [GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models](http://arxiv.org/abs/2302.04222v3)

authors: Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben Y. Zhao


## What, Why and How

[1]: https://arxiv.org/pdf/2302.04222v3 "GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models"
[2]: https://arxiv.org/abs/2302.04222 "[2302.04222] GLAZE: Protecting Artists from Style Mimicry by Text-to ..."
[3]: http://export.arxiv.org/abs/2302.04222 "[2302.04222] GLAZE: Protecting Artists from Style Mimicry by Text-to ..."

Here is a summary of the paper:

- **What**: The paper presents Glaze, a tool that enables artists to apply "style cloaks" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist[^1^][1].
- **Why**: The paper aims to protect artists from style mimicry by text-to-image models such as MidJourney and Stable Diffusion, which can learn to generate AI art that resembles the artistic style of specific artists after "fine-tuning" on samples of their art[^1^][1]. The paper argues that these models have significant negative impacts on independent artists, such as loss of income, identity, and creativity[^1^][1].
- **How**: The paper describes the design, implementation and evaluation of Glaze, which uses adversarial examples and CLIP-based metrics to generate style cloaks that disrupt mimicry under normal conditions and against adaptive countermeasures[^1^][1]. The paper also deploys user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of Glaze, its usability and tolerability of perturbations, and robustness across different scenarios[^1^][1]. The paper reports that Glaze is highly successful at disrupting mimicry (>92%) and has minimal impact on the artistic quality of the images[^1^][1].

## Main Contributions

The paper claims the following contributions:

- It is the first to propose and evaluate a practical tool that enables artists to protect their style from mimicry by text-to-image models.
- It is the first to leverage CLIP-based metrics to measure and disrupt style mimicry by text-to-image models.
- It is the first to conduct large-scale user studies with professional artists to assess their views of AI art and the usability and efficacy of Glaze.
- It is the first to evaluate the robustness of Glaze against adaptive countermeasures by text-to-image models.

## Method Summary

The method section of the paper consists of four subsections:

- Style Cloaking: This subsection describes how Glaze generates style cloaks for a given image by adding small perturbations that maximize the difference between the CLIP-based style scores of the original image and the cloaked image. The style score is defined as the cosine similarity between the CLIP text and image embeddings of a given prompt and image. The perturbations are constrained by a parameter p that controls the perceptibility of the cloaks.
- Mimicry Detection: This subsection describes how Glaze detects mimicry by text-to-image models by comparing the style scores of the original image and the generated image. If the style score of the generated image is higher than a threshold t, Glaze flags it as mimicry. The threshold t is determined by a calibration process that uses a set of reference images and prompts.
- User Studies: This subsection describes how Glaze conducts user studies with professional artists to evaluate its usability and efficacy. The user studies consist of three parts: a survey on the views of AI art, a task on applying and removing style cloaks using Glaze, and a task on rating the artistic quality and mimicry level of generated images with and without style cloaks.
- Robustness Evaluation: This subsection describes how Glaze evaluates its robustness against adaptive countermeasures by text-to-image models. The adaptive countermeasures include fine-tuning on cloaked images, applying inverse cloaks to generated images, and using different text prompts. Glaze measures the robustness by calculating the mimicry detection accuracy and the style score difference between the original image and the generated image.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the style score function
def style_score(prompt, image):
  # Compute the CLIP text and image embeddings
  text_embedding = CLIP.encode_text(prompt)
  image_embedding = CLIP.encode_image(image)
  # Return the cosine similarity between the embeddings
  return cosine_similarity(text_embedding, image_embedding)

# Define the style cloaking function
def style_cloak(image, prompt, p):
  # Initialize the cloaked image as the original image
  cloaked_image = image
  # Initialize the perturbation as a zero matrix
  perturbation = zeros(image.shape)
  # Repeat until the perturbation norm exceeds p
  while norm(perturbation) < p:
    # Compute the gradient of the style score with respect to the cloaked image
    gradient = gradient(style_score(prompt, cloaked_image), cloaked_image)
    # Update the perturbation by adding a small step along the negative gradient direction
    perturbation = perturbation - epsilon * gradient
    # Clip the perturbation values to [-p, p]
    perturbation = clip(perturbation, -p, p)
    # Update the cloaked image by adding the perturbation to the original image
    cloaked_image = image + perturbation
    # Clip the cloaked image values to [0, 1]
    cloaked_image = clip(cloaked_image, 0, 1)
  # Return the cloaked image and the perturbation
  return cloaked_image, perturbation

# Define the mimicry detection function
def mimicry_detect(image, prompt, t):
  # Generate an image from the prompt using a text-to-image model
  generated_image = text_to_image(prompt)
  # Compute the style scores of the original image and the generated image
  original_score = style_score(prompt, image)
  generated_score = style_score(prompt, generated_image)
  # Return True if the generated score is higher than the threshold t, False otherwise
  return generated_score > t

# Define the user study function
def user_study():
  # Create a set of images and prompts for evaluation
  images = load_images()
  prompts = load_prompts()
  # Create a set of reference images and prompts for calibration
  references = load_references()
  reference_prompts = load_reference_prompts()
  # Initialize an empty list of results
  results = []
  # For each artist in the user study
  for artist in user_study:
    # Conduct a survey on their views of AI art
    survey_result = survey(artist)
    # Conduct a task on applying and removing style cloaks using Glaze
    cloak_result = cloak_task(artist, images, prompts)
    # Conduct a task on rating the artistic quality and mimicry level of generated images with and without style cloaks
    rating_result = rating_task(artist, images, prompts, references, reference_prompts)
    # Append the results to the list of results
    results.append([survey_result, cloak_result, rating_result])
  # Return the list of results
  return results

# Define the robustness evaluation function
def robustness_eval():
  # Create a set of images and prompts for evaluation
  images = load_images()
  prompts = load_prompts()
  # Create a set of reference images and prompts for calibration
  references = load_references()
  reference_prompts = load_reference_prompts()
  # Initialize an empty list of scores
  scores = []
  # For each scenario in [normal, fine-tuned, inverse-cloaked, different-prompt]
  for scenario in scenarios:
    # Initialize an empty list of scores for this scenario
    scenario_scores = []
    # For each image and prompt pair in the evaluation set
    for image, prompt in zip(images, prompts):
      # Apply a style cloak to the image using Glaze with p=0.05
      cloaked_image, _ = style_cloak(image, prompt, p=0.05)
      # Generate an image from the prompt using a text-to-image model under this scenario
      generated_image = text_to_image(scenario, prompt)
      # Compute the mimicry detection accuracy and the style score difference between the original image and the generated image using Glaze with t=0.8
      mimicry_accuracy = mimicry_detect(image, prompt, t=0.8) == mimicry_detect(cloaked_image, prompt, t=0.8)
      score_difference = style_score(prompt, image) - style_score(prompt, generated_image)
      # Append the scores to the list of scores for this scenario
      scenario_scores.append([mimicry_accuracy, score_difference])
    # Append the list of scores for this scenario to the list of scores
    scores.append(scenario_scores)
  # Return the list of scores
  return scores
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing
import clip # for CLIP model
import numpy as np # for numerical operations
import requests # for downloading images
import PIL # for image manipulation
import matplotlib.pyplot as plt # for plotting

# Load the CLIP model and the text-to-image model
clip_model = clip.load("ViT-B/32", device="cuda")
text_to_image_model = load_text_to_image_model("StableDiffusion")

# Define the style score function
def style_score(prompt, image):
  # Convert the prompt to a tensor of tokens
  prompt_tokens = clip.tokenize(prompt).to("cuda")
  # Convert the image to a tensor of pixels
  image_pixels = torchvision.transforms.ToTensor()(image).unsqueeze(0).to("cuda")
  # Compute the CLIP text and image embeddings
  with torch.no_grad():
    text_embedding = clip_model.encode_text(prompt_tokens)
    image_embedding = clip_model.encode_image(image_pixels)
  # Return the cosine similarity between the embeddings
  return torch.nn.functional.cosine_similarity(text_embedding, image_embedding).item()

# Define the style cloaking function
def style_cloak(image, prompt, p):
  # Convert the prompt to a tensor of tokens
  prompt_tokens = clip.tokenize(prompt).to("cuda")
  # Convert the image to a tensor of pixels
  image_pixels = torchvision.transforms.ToTensor()(image).unsqueeze(0).to("cuda")
  # Initialize the cloaked image as the original image
  cloaked_image = image_pixels.clone()
  # Initialize the perturbation as a zero tensor
  perturbation = torch.zeros_like(image_pixels)
  # Repeat until the perturbation norm exceeds p
  while torch.norm(perturbation) < p:
    # Compute the gradient of the style score with respect to the cloaked image
    cloaked_image.requires_grad_(True)
    gradient = torch.autograd.grad(style_score(prompt_tokens, cloaked_image), cloaked_image)[0]
    # Update the perturbation by adding a small step along the negative gradient direction
    perturbation = perturbation - epsilon * gradient.sign()
    # Clip the perturbation values to [-p, p]
    perturbation = torch.clamp(perturbation, -p, p)
    # Update the cloaked image by adding the perturbation to the original image
    cloaked_image = image_pixels + perturbation
    # Clip the cloaked image values to [0, 1]
    cloaked_image = torch.clamp(cloaked_image, 0, 1)
    # Detach the cloaked image from the computation graph
    cloaked_image = cloaked_image.detach()
  # Convert the cloaked image and the perturbation to PIL images
  cloaked_image = torchvision.transforms.ToPILImage()(cloaked_image.squeeze(0))
  perturbation = torchvision.transforms.ToPILImage()(perturbation.squeeze(0))
  # Return the cloaked image and the perturbation
  return cloaked_image, perturbation

# Define the mimicry detection function
def mimicry_detect(image, prompt, t):
  # Generate an image from the prompt using a text-to-image model
  generated_image = text_to_image_model.generate(prompt)
  # Compute the style scores of the original image and the generated image
  original_score = style_score(prompt, image)
  generated_score = style_score(prompt, generated_image)
  # Return True if the generated score is higher than the threshold t, False otherwise
  return generated_score > t

# Define the user study function
def user_study():
  # Create a set of images and prompts for evaluation
  images = [PIL.Image.open(requests.get(url, stream=True).raw) for url in load_images()]
  prompts = load_prompts()
  # Create a set of reference images and prompts for calibration
  references = [PIL.Image.open(requests.get(url, stream=True).raw) for url in load_references()]
  reference_prompts = load_reference_prompts()
  # Initialize an empty list of results
  results = []
  # For each artist in the user study
  for artist in user_study:
    # Conduct a survey on their views of AI art using a web interface or a chatbot interface
    survey_result = survey(artist)
    # Conduct a task on applying and removing style cloaks using Glaze using a web interface or a chatbot interface
    cloak_result = cloak_task(artist, images, prompts)
    # Conduct a task on rating the artistic quality and mimicry level of generated images with and without style cloaks using a web interface or a chatbot interface
    rating_result = rating_task(artist, images, prompts, references, reference_prompts)
    # Append the results to the list of results
    results.append([survey_result, cloak_result, rating_result])
  # Return the list of results
  return results

# Define the robustness evaluation function
def robustness_eval():
  # Create a set of images and prompts for evaluation
  images = [PIL.Image.open(requests.get(url, stream=True).raw) for url in load_images()]
  prompts = load_prompts()
  # Create a set of reference images and prompts for calibration
  references = [PIL.Image.open(requests.get(url, stream=True).raw) for url in load_references()]
  reference_prompts = load_reference_prompts()
  # Initialize an empty list of scores
  scores = []
  # For each scenario in [normal, fine-tuned, inverse-cloaked, different-prompt]
  for scenario in scenarios:
    # Initialize an empty list of scores for this scenario
    scenario_scores = []
    # For each image and prompt pair in the evaluation set
    for image, prompt in zip(images, prompts):
      # Apply a style cloak to the image using Glaze with p=0.05
      cloaked_image, _ = style_cloak(image, prompt, p=0.05)
      # Generate an image from the prompt using a text-to-image model under this scenario
      generated_image = text_to_image_model.generate(scenario, prompt)
      # Compute the mimicry detection accuracy and the style score difference between the original image and the generated image using Glaze with t=0.8
      mimicry_accuracy = mimicry_detect(image, prompt, t=0.8) == mimicry_detect(cloaked_image, prompt, t=0.8)
      score_difference = style_score(prompt, image) - style_score(prompt, generated_image)
      # Append the scores to the list of scores for this scenario
      scenario_scores.append([mimicry_accuracy, score_difference])
    # Append the list of scores for this scenario to the list of scores
    scores.append(scenario_scores)
  # Return the list of scores
  return scores
```