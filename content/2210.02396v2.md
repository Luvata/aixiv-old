---
title: 2210.02396v2 Temporally Consistent Transformers for Video Generation
date: 2022-10-03
---

# [Temporally Consistent Transformers for Video Generation](http://arxiv.org/abs/2210.02396v2)

authors: Wilson Yan, Danijar Hafner, Stephen James, Pieter Abbeel


## What, Why and How

[1]: https://arxiv.org/pdf/2102.02396v2.pdf "arXiv:2102.02396v2 [math.OC] 6 Feb 2021"
[2]: https://lib-arxiv-013.serverfarm.cornell.edu/abs/2210.02396v2 "[2210.02396v2] Temporally Consistent Transformers for Video Generation"
[3]: http://export.arxiv.org/abs/2211.02396v2 "[2211.02396v2] Rethinking the positive role of cluster structure in ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper is about analyzing the exact linear convergence rate of gradient descent for factorization-based matrix completion for symmetric matrices.
- **Why**: The paper aims to provide a more accurate estimate of the rate of convergence of gradient descent for matrix completion, which is a scalable and efficient algorithm for recovering low-rank matrices from partial observations. The paper also wants to bridge the gap between theory and practice, as previous results only offer global convergence guarantees under certain statistical assumptions.
- **How**: The paper performs a local analysis of the convergence rate of gradient descent for matrix completion, without any additional assumptions on the underlying model. The paper identifies the deterministic condition for local convergence, which only depends on the solution matrix and the sampling set. The paper also provides a closed-form expression of the asymptotic rate of convergence that matches exactly with the linear convergence observed in practice. The paper validates its theoretical results with numerical experiments on synthetic and real-world data sets.

## Main Contributions

The paper claims to make the following contributions:

- It offers the first result that provides the exact rate of convergence of gradient descent for matrix factorization in Euclidean space for matrix completion.
- It performs a local analysis of the convergence rate of gradient descent for matrix completion for symmetric matrices, without any additional assumptions on the underlying model.
- It identifies the deterministic condition for local convergence, which only depends on the solution matrix and the sampling set.
- It provides a closed-form expression of the asymptotic rate of convergence that matches exactly with the linear convergence observed in practice.
- It validates its theoretical results with numerical experiments on synthetic and real-world data sets.

## Method Summary

[1]: https://arxiv.org/pdf/2102.02396v2.pdf "arXiv:2102.02396v2 [math.OC] 6 Feb 2021"
[2]: https://arxiv.org/pdf/2210.03243v2.pdf "Approximate Methods for Bayesian Computation - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/pdf/1812.02396v2 "arXiv:1812.02396v2 [math.DG] 28 Apr 2020"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper considers the factorization-based matrix completion problem for symmetric matrices, where the goal is to find a rank-r matrix X that minimizes the squared error over the observed entries of M: min X∈Rn×n 1 2 ∑ (i,j)∈Ω (Mij − Xij) 2 s.t. rank(X) = r. (3)
- The paper uses gradient descent to solve (3), by initializing X(0) randomly and updating it as X(t+1) = X(t) − ηt∇f(X(t)), where ηt is the step size and f(X) is the objective function in (3).
- The paper analyzes the convergence rate of gradient descent for (3), by deriving a deterministic condition for local convergence, which only depends on the solution matrix X∗ and the sampling set Ω. The condition is given by λmin(ΠΩ(X ∗)) > 0, where ΠΩ(X ∗) is the projection of X∗ onto the subspace spanned by the columns of MΩ, and λmin(·) is the smallest eigenvalue.
- The paper also derives a closed-form expression for the asymptotic rate of convergence of gradient descent for (3), which is given by ρ = 1 − λmin(ΠΩ(X ∗)) λmax(ΠΩ(X ∗)) , where λmax(·) is the largest eigenvalue. The paper shows that ρ matches exactly with the linear convergence observed in practice.
- The paper validates its theoretical results with numerical experiments on synthetic and real-world data sets, and compares them with existing results in the literature. The paper shows that its condition for local convergence is less restrictive than previous ones, and that its expression for the rate of convergence is more accurate than previous ones.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Input: a symmetric matrix M with observed entries in Ω, a rank r, a step size η
# Output: a rank-r matrix X that approximates M on Ω
# Initialize X randomly
X = randn(n, n)
# Project X onto the rank-r subspace
X = project(X, r)
# Repeat until convergence
while not converged do
  # Compute the gradient of the objective function
  G = zeros(n, n)
  for (i, j) in Ω do
    G[i, j] = X[i, j] - M[i, j]
  end for
  # Update X using gradient descent
  X = X - η * G
  # Project X onto the rank-r subspace
  X = project(X, r)
end while
# Return X
return X
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Input: a symmetric matrix M with observed entries in Ω, a rank r, a step size η, a tolerance ε
# Output: a rank-r matrix X that approximates M on Ω
# Initialize X randomly
X = randn(n, n)
# Project X onto the rank-r subspace using singular value decomposition (SVD)
[U, S, V] = svd(X)
X = U[:, :r] * S[:r, :r] * V[:, :r].T
# Initialize the objective value and the convergence flag
f = inf
converged = false
# Repeat until convergence
while not converged do
  # Compute the gradient of the objective function
  G = zeros(n, n)
  for (i, j) in Ω do
    G[i, j] = X[i, j] - M[i, j]
  end for
  # Update X using gradient descent
  X = X - η * G
  # Project X onto the rank-r subspace using SVD
  [U, S, V] = svd(X)
  X = U[:, :r] * S[:r, :r] * V[:, :r].T
  # Compute the new objective value
  f_new = 0.5 * norm(G[Ω])^2 / |Ω|
  # Check the convergence criterion
  if abs(f - f_new) / f < ε then
    converged = true
  else
    f = f_new
  end if
end while
# Return X
return X
```