---
title: 2303.09522v2 $P+$  Extended Textual Conditioning in Text-to-Image Generation
date: 2023-03-10
---

# [$P+$: Extended Textual Conditioning in Text-to-Image Generation](http://arxiv.org/abs/2303.09522v2)

authors: Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, Kfir Aberman


## What, Why and How

[1]: https://arxiv.org/abs/2303.09522 "$P+$: Extended Textual Conditioning in Text-to-Image Generation"
[2]: https://arxiv.org/pdf/2303.09522.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export3.library.cornell.edu/abs/2303.09522v2 "[2303.09522v2] $P+$: Extended Textual Conditioning in Text-to-Image ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces an Extended Textual Conditioning space in text-to-image models, referred to as $P+$. This space consists of multiple textual conditions, derived from per-layer prompts, each corresponding to a layer of the denoising U-net of the diffusion model. The paper also introduces Extended Textual Inversion (XTI), where the images are inverted into $P+$, and represented by per-layer tokens.
- **Why**: The paper aims to provide greater disentangling and control over image synthesis, as well as more expressive and precise textual inversion. The paper also demonstrates the potential of $P+$ for object-style mixing using text-to-image models.
- **How**: The paper uses a diffusion model with a denoising U-net as the generator, and extends the textual conditioning space by adding per-layer prompts that are concatenated with the latent codes at each layer. The paper also proposes a method to invert images into $P+$ by optimizing the per-layer tokens to minimize the reconstruction loss. The paper conducts a series of experiments to evaluate the properties and effectiveness of $P+$ and XTI, and compares them with existing methods. The paper also shows some applications of $P+$ for object-style mixing and image editing.

## Main Contributions

[1]: https://arxiv.org/abs/2303.09522 "$P+$: Extended Textual Conditioning in Text-to-Image Generation"
[2]: https://arxiv.org/pdf/2303.09522.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export3.library.cornell.edu/abs/2303.09522v2 "[2303.09522v2] $P+$: Extended Textual Conditioning in Text-to-Image ..."

According to the paper[^1^][1], the main contributions are:

- The introduction of an Extended Textual Conditioning space ($P+$) in text-to-image models, which allows for more fine-grained and flexible control over image synthesis by using per-layer prompts.
- The introduction of Extended Textual Inversion (XTI), which enables more accurate and efficient inversion of images into $P+$ by optimizing per-layer tokens.
- The demonstration of the advantages of $P+$ and XTI over existing methods in terms of disentangling, editability, expressiveness, precision, and convergence.
- The application of $P+$ for object-style mixing, which achieves novel and realistic results that are not possible with existing methods.

## Method Summary

[1]: https://arxiv.org/abs/2303.09522 "$P+$: Extended Textual Conditioning in Text-to-Image Generation"
[2]: https://arxiv.org/pdf/2303.09522.pdf "arXiv.org e-Print archive"
[3]: http://arxiv-export3.library.cornell.edu/abs/2303.09522v2 "[2303.09522v2] $P+$: Extended Textual Conditioning in Text-to-Image ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses a diffusion model with a denoising U-net as the generator, which takes a latent code and a textual condition as inputs and outputs an image.
- The paper extends the textual condition space by adding per-layer prompts that are concatenated with the latent code at each layer of the U-net. The per-layer prompts are derived from a global prompt using a tokenizer and an encoder. The paper defines $P+$ as the set of all per-layer prompts for a given global prompt.
- The paper proposes a method to invert images into $P+$ by optimizing the per-layer tokens to minimize the reconstruction loss. The paper uses a gradient-based optimizer with a momentum term and a learning rate scheduler. The paper also introduces a regularization term to encourage smoothness and diversity in the inversion space.
- The paper evaluates the properties and effectiveness of $P+$ and XTI on various datasets, such as COCO, FFHQ, and LSUN. The paper compares $P+$ and XTI with existing methods, such as CLIP-guided diffusion and Textual Inversion (TI). The paper also shows some applications of $P+$ for object-style mixing and image editing.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion model with a denoising U-net as the generator
def diffusion_model(latent_code, textual_condition):
  # Initialize the image from a Gaussian distribution
  image = sample_from_gaussian(latent_code.shape)
  # Loop over the diffusion steps
  for t in range(T):
    # Compute the noise level and the denoising coefficient
    noise_level = get_noise_level(t)
    denoising_coeff = get_denoising_coeff(t)
    # Add noise to the image
    noisy_image = image + noise_level * random_noise(image.shape)
    # Concatenate the latent code and the textual condition at each layer of the U-net
    latent_and_condition = concatenate(latent_code, textual_condition)
    # Pass the noisy image and the latent and condition through the U-net
    denoised_image = u_net(noisy_image, latent_and_condition)
    # Update the image by a weighted average of the noisy and denoised images
    image = denoising_coeff * denoised_image + (1 - denoising_coeff) * noisy_image
  # Return the final image
  return image

# Define the extended textual conditioning space ($P+$) using a tokenizer and an encoder
def extended_textual_conditioning(global_prompt):
  # Tokenize the global prompt into a sequence of tokens
  tokens = tokenizer(global_prompt)
  # Encode the tokens into a sequence of embeddings
  embeddings = encoder(tokens)
  # Split the embeddings into per-layer prompts according to a predefined scheme
  per_layer_prompts = split_embeddings(embeddings)
  # Return $P+$ as the set of all per-layer prompts
  return per_layer_prompts

# Define the extended textual inversion (XTI) method using an optimizer and a regularizer
def extended_textual_inversion(target_image):
  # Initialize the per-layer tokens randomly
  per_layer_tokens = random_init()
  # Loop over the optimization steps
  for i in range(N):
    # Convert the per-layer tokens into per-layer prompts using the encoder
    per_layer_prompts = encoder(per_layer_tokens)
    # Generate an image from the per-layer prompts using the diffusion model
    generated_image = diffusion_model(None, per_layer_prompts)
    # Compute the reconstruction loss between the target and generated images
    reconstruction_loss = mse_loss(target_image, generated_image)
    # Compute the regularization loss to encourage smoothness and diversity in $P+$
    regularization_loss = smoothness_loss(per_layer_tokens) + diversity_loss(per_layer_tokens)
    # Compute the total loss as a weighted sum of the reconstruction and regularization losses
    total_loss = reconstruction_loss + lambda * regularization_loss
    # Update the per-layer tokens by gradient descent with momentum and learning rate scheduler
    per_layer_tokens = update_by_gradient_descent(per_layer_tokens, total_loss, momentum, learning_rate)
  # Return $P+$ as the set of all per-layer prompts
  return per_layer_prompts

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define the hyperparameters
T = 1000 # number of diffusion steps
N = 1000 # number of optimization steps
lambda = 0.1 # weight of the regularization loss
momentum = 0.9 # momentum coefficient for the optimizer
learning_rate = 0.01 # initial learning rate for the optimizer
learning_rate_decay = 0.99 # learning rate decay factor for the scheduler

# Define the diffusion model with a denoising U-net as the generator
class DiffusionModel(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the U-net architecture with skip connections and residual blocks
    self.u_net = UNet()
    # Define the noise level and the denoising coefficient as learnable parameters
    self.noise_level = torch.nn.Parameter(torch.ones(T))
    self.denoising_coeff = torch.nn.Parameter(torch.ones(T))

  def forward(self, latent_code, textual_condition):
    # Initialize the image from a Gaussian distribution
    image = torch.randn_like(latent_code)
    # Loop over the diffusion steps
    for t in range(T):
      # Add noise to the image
      noisy_image = image + self.noise_level[t] * torch.randn_like(image)
      # Concatenate the latent code and the textual condition at each layer of the U-net
      latent_and_condition = torch.cat([latent_code, textual_condition], dim=1)
      # Pass the noisy image and the latent and condition through the U-net
      denoised_image = self.u_net(noisy_image, latent_and_condition)
      # Update the image by a weighted average of the noisy and denoised images
      image = self.denoising_coeff[t] * denoised_image + (1 - self.denoising_coeff[t]) * noisy_image
    # Return the final image
    return image

# Define the extended textual conditioning space ($P+$) using a tokenizer and an encoder
class ExtendedTextualConditioning(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the tokenizer and the encoder from CLIP
    self.tokenizer = clip.tokenize
    self.encoder = clip.load("ViT-B/32", jit=False)[0].visual
    # Define the scheme to split the embeddings into per-layer prompts
    self.split_scheme = [1, 2, 4, 8, 16]

  def forward(self, global_prompt):
    # Tokenize the global prompt into a sequence of tokens
    tokens = self.tokenizer(global_prompt)
    # Encode the tokens into a sequence of embeddings
    embeddings = self.encoder(tokens)
    # Split the embeddings into per-layer prompts according to the predefined scheme
    per_layer_prompts = []
    start_index = 0
    for split_size in self.split_scheme:
      end_index = start_index + split_size
      per_layer_prompt = embeddings[:, start_index:end_index].mean(dim=1)
      per_layer_prompts.append(per_layer_prompt)
      start_index = end_index
    # Return $P+$ as a list of all per-layer prompts
    return per_layer_prompts

# Define the extended textual inversion (XTI) method using an optimizer and a regularizer
class ExtendedTextualInversion(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Define the diffusion model and the extended textual conditioning space as submodules
    self.diffusion_model = DiffusionModel()
    self.extended_textual_conditioning = ExtendedTextualConditioning()
    # Define the optimizer and the scheduler for updating the per-layer tokens
    self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)
    self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=learning_rate_decay)

  def forward(self, target_image):
    # Initialize the per-layer tokens randomly from a uniform distribution
    per_layer_tokens = []
    for split_size in self.extended_textual_conditioning.split_scheme:
      token_shape = (1, split_size, 77) # 77 is the vocabulary size of CLIP tokenizer
      per_layer_token = torch.rand(token_shape, requires_grad=True)
      per_layer_tokens.append(per_layer_token)
      self.register_parameter(f"token_{split_size}", per_layer_token) # register as parameters for optimization
    # Loop over the optimization steps
    for i in range(N):
      # Convert the per-layer tokens into per-layer prompts using the encoder
      per_layer_prompts = []
      for per_layer_token in per_layer_tokens:
        per_layer_prompt = self.extended_textual_conditioning.encoder(per_layer_token)
        per_layer_prompts.append(per_layer_prompt)
      # Generate an image from the per-layer prompts using the diffusion model
      generated_image = self.diffusion_model(None, per_layer_prompts)
      # Compute the reconstruction loss between the target and generated images
      reconstruction_loss = torch.nn.functional.mse_loss(target_image, generated_image)
      # Compute the regularization loss to encourage smoothness and diversity in $P+$
      smoothness_loss = 0
      diversity_loss = 0
      for per_layer_token in per_layer_tokens:
        # Smoothness loss is the sum of squared differences between adjacent tokens
        smoothness_loss += torch.sum(torch.pow(per_layer_token[:, 1:] - per_layer_token[:, :-1], 2))
        # Diversity loss is the negative entropy of the token distribution
        diversity_loss -= torch.sum(per_layer_token * torch.log(per_layer_token + 1e-9))
      regularization_loss = smoothness_loss + diversity_loss
      # Compute the total loss as a weighted sum of the reconstruction and regularization losses
      total_loss = reconstruction_loss + lambda * regularization_loss
      # Update the per-layer tokens by gradient descent with momentum and learning rate scheduler
      self.optimizer.zero_grad()
      total_loss.backward()
      self.optimizer.step()
      self.scheduler.step()
    # Return $P+$ as a list of all per-layer prompts
    return per_layer_prompts

```