---
title: 2305.19406v2 PaintSeg  Training-free Segmentation via Painting
date: 2023-05-20
---

# [PaintSeg: Training-free Segmentation via Painting](http://arxiv.org/abs/2305.19406v2)

authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, Bhiksha Raj


## What, Why and How

[1]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."
[2]: https://arxiv.org/abs/2305.16291 "Voyager: An Open-Ended Embodied Agent with Large Language Models"
[3]: https://pypi.org/project/arxiv-dl/ "arxiv-dl Â· PyPI"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a new framework for language model inference, called **Tree of Thoughts (ToT)**, which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.
- **Why**: The paper aims to overcome the limitations of language models that are still confined to token-level, left-to-right decision-making processes during inference, which means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role.
- **How**: The paper proposes ToT as a way to allow language models to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. The paper also introduces three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords, and shows that ToT significantly enhances language models' problem-solving abilities on these tasks. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, ToT achieved a success rate of 74%.

## Main Contributions

The paper claims the following contributions:

- A new framework for language model inference, Tree of Thoughts (ToT), which enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.
- A new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement.
- Three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords, and empirical results showing that ToT outperforms prior methods on these tasks.

## Method Summary

[1]: https://arxiv.org/abs/2305.20050 "[2305.20050] Let's Verify Step by Step - arXiv.org"
[2]: https://arxiv.org/abs/2305.18290 "[2305.18290] Direct Preference Optimization: Your Language Model is ..."
[3]: https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K/abstract "Adam: A Method for Stochastic Optimization - NASA/ADS"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the **Tree of Thoughts (ToT)** framework, which consists of three main components: **thoughts**, **thought selection**, and **thought execution**.
- **Thoughts** are coherent units of text that represent intermediate steps toward problem solving, such as subgoals, actions, or hypotheses. Thoughts are generated by querying a language model (LM) with a prompt that encodes the problem statement and the current state of reasoning. Thoughts can be either terminal (i.e., a final answer) or non-terminal (i.e., requiring further exploration).
- **Thought selection** is the process of choosing the best thought among a set of candidates generated by the LM. The paper proposes two methods for thought selection: **greedy** and **exploratory**. Greedy selection chooses the thought with the highest probability assigned by the LM, while exploratory selection chooses a thought randomly according to a softmax distribution over the probabilities. The paper also introduces a parameter called **exploration rate**, which controls the trade-off between exploration and exploitation in thought selection.
- **Thought execution** is the process of applying a selected thought to the current state of reasoning and updating it accordingly. The paper defines two types of thought execution: **forward** and **backward**. Forward execution appends a selected thought to the current state of reasoning and queries the LM with the updated state. Backward execution removes the last thought from the current state of reasoning and queries the LM with the previous state. The paper also introduces a parameter called **backtracking rate**, which controls the frequency of backward execution in thought execution.
- The paper also describes how to use ToT for three novel tasks requiring non-trivial planning or search: **Game of 24**, **Creative Writing**, and **Mini Crosswords**. The paper provides detailed descriptions of how to design prompts, thoughts, and evaluation metrics for each task, as well as how to tune the exploration rate and backtracking rate parameters for optimal performance. The paper also reports empirical results showing that ToT outperforms prior methods on these tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a language model (LM) and a problem statement (P)
LM = GPT-4
P = "Solve the equation x^2 + 5x - 24 = 0"

# Initialize the current state of reasoning (S) and the final answer (A)
S = P
A = None

# Define the exploration rate (e) and the backtracking rate (b)
e = 0.1
b = 0.05

# Loop until A is found or a maximum number of iterations is reached
while A is None and not timeout:

  # Generate a set of candidate thoughts (T) by querying LM with S
  T = LM(S)

  # Select a thought (t) from T according to e
  if random() < e:
    t = sample(T, softmax)
  else:
    t = argmax(T, probability)

  # Check if t is terminal or non-terminal
  if t is terminal:
    # Set A to t and break the loop
    A = t
    break
  else:
    # Execute t according to b
    if random() < b:
      # Backward execution: remove the last thought from S
      S = S[:-1]
    else:
      # Forward execution: append t to S
      S = S + t

# Return A or report failure
if A is not None:
  return A
else:
  return "Failed to find an answer"
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import transformers
import random
import math

# Define a language model (LM) and a tokenizer (TK)
LM = transformers.GPT4LMHeadModel.from_pretrained("gpt4")
TK = transformers.GPT4Tokenizer.from_pretrained("gpt4")

# Define a problem statement (P) and a maximum number of iterations (N)
P = "Solve the equation x^2 + 5x - 24 = 0"
N = 100

# Initialize the current state of reasoning (S) and the final answer (A)
S = P
A = None

# Define the exploration rate (e) and the backtracking rate (b)
e = 0.1
b = 0.05

# Define a function to generate a set of candidate thoughts (T) by querying LM with S
def generate_thoughts(S):
  # Encode S as input ids and attention mask
  input_ids = TK.encode(S, return_tensors="pt")
  attention_mask = torch.ones_like(input_ids)

  # Generate a set of candidate thoughts with beam search
  output_ids = LM.generate(input_ids, attention_mask=attention_mask, num_beams=5, num_return_sequences=5)

  # Decode the output ids as text and return them as a list
  output_text = TK.batch_decode(output_ids, skip_special_tokens=True)
  return output_text

# Define a function to select a thought (t) from T according to e
def select_thought(T, e):
  # Compute the probabilities of each thought using the LM's logits
  logits = LM(TK(T, return_tensors="pt", padding=True).input_ids, return_dict=True).logits[:, -1]
  probabilities = torch.softmax(logits, dim=-1).max(dim=-1).values.tolist()

  # Select a thought according to e
  if random.random() < e:
    # Exploratory selection: sample a thought according to a softmax distribution over the probabilities
    indices = list(range(len(T)))
    weights = [math.exp(p) for p in probabilities]
    index = random.choices(indices, weights)[0]
    t = T[index]
  else:
    # Greedy selection: choose the thought with the highest probability
    index = probabilities.index(max(probabilities))
    t = T[index]

  # Return the selected thought and its probability
  return t, probabilities[index]

# Define a function to check if a thought is terminal or non-terminal
def is_terminal(t):
  # A thought is terminal if it contains an equal sign (=) or a question mark (?)
  return "=" in t or "?" in t

# Loop until A is found or N iterations are reached
for i in range(N):

  # Generate a set of candidate thoughts by querying LM with S
  T = generate_thoughts(S)

  # Select a thought from T according to e
  t, p = select_thought(T, e)

  # Check if t is terminal or non-terminal
  if is_terminal(t):
    # Set A to t and break the loop
    A = t
    break
  else:
    # Execute t according to b
    if random.random() < b:
      # Backward execution: remove the last thought from S
      S = S.rsplit("\n", 1)[0]
    else:
      # Forward execution: append t to S with a newline separator
      S = S + "\n" + t

# Return A or report failure
if A is not None:
  print(f"Found an answer: {A}")
else:
  print("Failed to find an answer")
```