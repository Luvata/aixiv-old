---
title: 1807.11152v1 Pose Guided Human Video Generation
date: 2018-07-12
---

# [Pose Guided Human Video Generation](http://arxiv.org/abs/1807.11152v1)

authors: Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, Dahua Lin


## What, Why and How

[1]: https://arxiv.org/abs/1807.11152v1 "[1807.11152v1] Pose Guided Human Video Generation - arXiv.org"
[2]: https://arxiv.org/pdf/1807.11152v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1807.11152 "[1807.11152] Pose Guided Human Video Generation"

Here is a summary of the paper:

- **What**: The paper proposes a method to synthesize human videos from a single image and a class label, using human pose as a guidance.
- **Why**: The paper aims to address the challenges of video synthesis, such as controlling the dynamics, preserving the appearance, and handling noisy or abnormal poses.
- **How**: The paper introduces two stages: (1) a Pose Sequence Generative Adversarial Network (PSGAN) that generates realistic pose sequences from the class label; and (2) a Semantic Consistent Generative Adversarial Network (SCGAN) that produces video frames from the poses and the input image, while enforcing semantic consistency between the generated and ground-truth poses at a high feature level. The paper evaluates the method on human action and human face datasets and compares it with other state-of-the-art methods. [^1^][1]

## Main Contributions

[1]: https://arxiv.org/abs/1807.11152v1 "[1807.11152v1] Pose Guided Human Video Generation - arXiv.org"
[2]: https://arxiv.org/pdf/1807.11152 "Pose Guided Human Video Generation - arXiv.org"
[3]: https://www.fec.gov/introduction-campaign-finance/how-to-research-public-records/individual-contributions/ "FEC | Individual Contribution Research"

According to the paper[^1^][1], the main contributions are:

- **A pose guided method** to synthesize human videos in a disentangled way: plausible motion prediction and coherent appearance generation.
- **A Pose Sequence Generative Adversarial Network (PSGAN)** that learns to generate realistic pose sequences from the class label in an adversarial manner.
- **A Semantic Consistent Generative Adversarial Network (SCGAN)** that produces video frames from the poses and the input image, while enforcing semantic consistency between the generated and ground-truth poses at a high feature level.
- **Extensive experiments** on both human action and human face datasets that demonstrate the superiority of the proposed method over other state-of-the-art methods.

## Method Summary

[1]: https://arxiv.org/abs/1807.11152v1 "[1807.11152v1] Pose Guided Human Video Generation - arXiv.org"
[2]: https://www.scribbr.com/apa-style/methods-section/ "How to Write an APA Methods Section | With Examples - Scribbr"
[3]: https://arxiv.org/pdf/1807.11152v1.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper proposes a **two-stage method** to synthesize human videos from a single image and a class label, using human pose as a guidance.
- In the **first stage**, the paper introduces a **Pose Sequence Generative Adversarial Network (PSGAN)** that consists of a generator and a discriminator. The generator takes as input a class label and a random noise vector, and outputs a sequence of poses. The discriminator tries to distinguish between real and fake pose sequences. The PSGAN is trained with an adversarial loss and a classification loss to encourage realistic and diverse pose sequences.
- In the **second stage**, the paper presents a **Semantic Consistent Generative Adversarial Network (SCGAN)** that also consists of a generator and a discriminator. The generator takes as input a pose sequence and an image, and outputs a video sequence. The discriminator tries to distinguish between real and fake video sequences. The SCGAN is trained with an adversarial loss, a reconstruction loss, and a semantic consistency loss. The semantic consistency loss measures the similarity between the generated and ground-truth poses at a high feature level, which helps to handle noisy or abnormal poses.
- The paper evaluates the proposed method on two datasets: **Human3.6M** for human action synthesis and **FaceForensics** for human face synthesis. The paper compares the method with several baselines and state-of-the-art methods using quantitative and qualitative metrics. The paper also conducts ablation studies to analyze the effects of different components of the method. [^1^][2]

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a single image I and a class label c
# Output: a video sequence V

# Stage 1: Pose Sequence Generation
# Initialize a pose sequence generator G_p and a pose sequence discriminator D_p
# Train G_p and D_p with an adversarial loss and a classification loss
# Generate a pose sequence P from c and a random noise vector z using G_p

# Stage 2: Video Frame Generation
# Initialize a video frame generator G_v and a video frame discriminator D_v
# Train G_v and D_v with an adversarial loss, a reconstruction loss, and a semantic consistency loss
# Generate a video sequence V from P and I using G_v
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a single image I and a class label c
# Output: a video sequence V

# Hyperparameters
T = 16 # number of frames in a video sequence
N = 32 # number of keypoints in a pose
H = 256 # height of an image or a video frame
W = 256 # width of an image or a video frame
C = 3 # number of channels in an image or a video frame
K = 10 # number of classes in the dataset
Z = 128 # dimension of the noise vector
L_p = 0.01 # weight for the classification loss in PSGAN
L_r = 10 # weight for the reconstruction loss in SCGAN
L_s = 0.1 # weight for the semantic consistency loss in SCGAN

# Stage 1: Pose Sequence Generation
# Initialize a pose sequence generator G_p and a pose sequence discriminator D_p
G_p = PoseSequenceGenerator(Z, K, N, T) # input: noise vector z and class label c; output: pose sequence P
D_p = PoseSequenceDiscriminator(N, T, K) # input: pose sequence P; output: probability of being real and class prediction

# Train G_p and D_p with an adversarial loss and a classification loss
for each batch of real pose sequences P_real and class labels c_real:
    # Sample a batch of random noise vectors z
    z = sample_noise(Z)

    # Generate a batch of fake pose sequences P_fake using G_p
    P_fake = G_p(z, c_real)

    # Compute the adversarial loss for D_p
    D_real = D_p(P_real) # probability and class prediction for real pose sequences
    D_fake = D_p(P_fake) # probability and class prediction for fake pose sequences
    L_Dp_adv = -log(D_real[0]) - log(1 - D_fake[0]) # binary cross-entropy loss

    # Compute the classification loss for D_p
    L_Dp_cls = cross_entropy(D_real[1], c_real) + cross_entropy(D_fake[1], c_real) # cross-entropy loss

    # Compute the total loss for D_p
    L_Dp = L_Dp_adv + L_p * L_Dp_cls

    # Update the parameters of D_p using gradient descent
    update(D_p, L_Dp)

    # Compute the adversarial loss for G_p
    D_fake = D_p(P_fake) # probability and class prediction for fake pose sequences
    L_Gp_adv = -log(D_fake[0]) # binary cross-entropy loss

    # Compute the classification loss for G_p
    L_Gp_cls = cross_entropy(D_fake[1], c_real) # cross-entropy loss

    # Compute the total loss for G_p
    L_Gp = L_Gp_adv + L_p * L_Gp_cls

    # Update the parameters of G_p using gradient descent
    update(G_p, L_Gp)

# Generate a pose sequence P from c and a random noise vector z using G_p
z = sample_noise(Z)
P = G_p(z, c)

# Stage 2: Video Frame Generation
# Initialize a video frame generator G_v and a video frame discriminator D_v
G_v = VideoFrameGenerator(N, C, H, W, T) # input: pose sequence P and image I; output: video sequence V
D_v = VideoFrameDiscriminator(C, H, W, T) # input: video sequence V; output: probability of being real

# Train G_v and D_v with an adversarial loss, a reconstruction loss, and a semantic consistency loss
for each batch of real video sequences V_real and images I_real:
    # Generate a batch of fake video sequences V_fake using G_v and P_fake from stage 1
    V_fake = G_v(P_fake, I_real)

    # Compute the adversarial loss for D_v
    D_real = D_v(V_real) # probability for real video sequences
    D_fake = D_v(V_fake) # probability for fake video sequences
    L_Dv_adv = -log(D_real) - log(1 - D_fake) # binary cross-entropy loss

    # Compute the total loss for D_v
    L_Dv = L_Dv_adv

    # Update the parameters of D_v using gradient descent
    update(D_v, L_Dv)

    # Compute the adversarial loss for G_v
    D_fake = D_v(V_fake) # probability for fake video sequences
    L_Gv_adv = -log(D_fake) # binary cross-entropy loss

    # Compute the reconstruction loss for G_v
    L_Gv_rec = l1_loss(V_fake, V_real) # L1 loss

    # Compute the semantic consistency loss for G_v
    P_real = extract_poses(V_real) # extract poses from real video sequences using a pose estimator
    P_fake = extract_poses(V_fake) # extract poses from fake video sequences using a pose estimator
    F_real = extract_features(P_real) # extract high-level features from real poses using a feature extractor
    F_fake = extract_features(P_fake) # extract high-level features from fake poses using a feature extractor
    L_Gv_sem = l2_loss(F_fake, F_real) # L2 loss

    # Compute the total loss for G_v
    L_Gv = L_Gv_adv + L_r * L_Gv_rec + L_s * L_Gv_sem

    # Update the parameters of G_v using gradient descent
    update(G_v, L_Gv)

# Generate a video sequence V from P and I using G_v
V = G_v(P, I)
```