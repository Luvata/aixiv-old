---
title: 2302.12066v1 Teaching CLIP to Count to Ten
date: 2023-02-13
---

# [Teaching CLIP to Count to Ten](http://arxiv.org/abs/2302.12066v1)

authors: Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, Tali Dekel


## What, Why and How

[1]: https://arxiv.org/abs/2302.12066 "[2302.12066] Teaching CLIP to Count to Ten - arXiv.org"
[2]: http://arxiv-export3.library.cornell.edu/abs/2302.12066 "[2302.12066] Teaching CLIP to Count to Ten"
[3]: https://arxiv.org/pdf/2302.12066.pdf "arXiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to improve the quantitative understanding of large vision-language models (VLMs), such as CLIP, by teaching them to count objects in images and captions.
- **Why**: The paper addresses a well-documented limitation of existing VLMs - they fail to encapsulate compositional concepts such as counting, which are essential for many downstream tasks and applications.
- **How**: The paper introduces a new counting-contrastive loss that is used to finetune a pre-trained VLM in tandem with its original objective. The counting loss is applied over automatically-created counterfactual examples, each consisting of an image and a caption containing an incorrect object count. The paper also introduces a new image-text counting benchmark called CountBench to evaluate the model's performance on object counting. The paper demonstrates that the proposed method significantly improves the counting ability of CLIP, while maintaining its overall performance on common benchmarks. The paper also shows that the count-aware CLIP model can produce more reliable results for image retrieval and text-conditioned image generation tasks that require specific counts of objects.

## Main Contributions

The paper claims to make the following contributions:

- A simple yet effective method to improve the quantitative understanding of VLMs, such as CLIP, by teaching them to count objects in images and captions using a counting-contrastive loss.
- A new image-text counting benchmark called CountBench to evaluate the model's understanding of object counting.
- The first work to extend CLIP's capabilities to object counting and demonstrate its benefits for downstream tasks that require specific counts of objects, such as image retrieval and text-conditioned image generation.

## Method Summary

The method section of the paper describes the following steps:

- The paper uses a pre-trained CLIP model as the base model and finetunes it on a new objective that combines the original contrastive loss and the proposed counting-contrastive loss.
- The paper generates counterfactual examples by randomly selecting images from the ImageNet dataset and modifying their captions to contain incorrect object counts. For example, an image depicting three dogs is paired with the caption "Six dogs playing in the yard".
- The paper defines the counting-contrastive loss as the negative log-likelihood of choosing the correct caption over the incorrect one for a given image. The paper also introduces a temperature parameter to control the hardness of the negative examples.
- The paper finetunes the CLIP model on a balanced mix of original and counterfactual examples using the Adam optimizer with a learning rate of 1e-5 and a batch size of 256 for 10 epochs.
- The paper evaluates the finetuned model on the CountBench benchmark, which consists of 10K image-caption pairs with varying object counts and categories. The paper also compares the model with several baseline models, such as CLIP without finetuning, CLIP with random finetuning, and CLIP with masked language modeling finetuning.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load a pre-trained CLIP model
clip = load_clip_model()

# Define the original contrastive loss
def contrastive_loss(image, caption):
  # Compute the similarity between the image and the caption embeddings
  similarity = clip.similarity(image, caption)
  # Normalize the similarity by a temperature parameter
  similarity = similarity / temperature
  # Compute the negative log-likelihood of choosing the correct caption over other captions in the batch
  loss = -log_softmax(similarity)
  return loss

# Define the counting-contrastive loss
def counting_loss(image, caption):
  # Generate a counterfactual caption by changing the object count in the caption
  counter_caption = generate_counter_caption(caption)
  # Compute the similarity between the image and the caption embeddings
  similarity = clip.similarity(image, caption)
  # Compute the similarity between the image and the counter-caption embeddings
  counter_similarity = clip.similarity(image, counter_caption)
  # Normalize the similarities by a temperature parameter
  similarity = similarity / temperature
  counter_similarity = counter_similarity / temperature
  # Compute the negative log-likelihood of choosing the correct caption over the counter-caption
  loss = -log_softmax([similarity, counter_similarity])
  return loss

# Define the total loss as a weighted sum of the original contrastive loss and the counting-contrastive loss
def total_loss(image, caption):
  # Compute the original contrastive loss
  original_loss = contrastive_loss(image, caption)
  # Compute the counting-contrastive loss
  counting_loss = counting_loss(image, caption)
  # Compute the weighted sum of the losses with a hyperparameter alpha
  total_loss = alpha * original_loss + (1 - alpha) * counting_loss
  return total_loss

# Finetune the CLIP model on a balanced mix of original and counterfactual examples
for epoch in range(10):
  for batch in data_loader:
    # Get a batch of images and captions from ImageNet dataset
    images, captions = batch
    # Compute the total loss for the batch
    loss = total_loss(images, captions)
    # Update the model parameters using Adam optimizer with a learning rate of 1e-5
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Evaluate the finetuned model on CountBench benchmark and compare with baseline models
evaluate(clip, countbench)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import nltk
import random

# Load a pre-trained CLIP model and tokenizer
model, tokenizer = clip.load("ViT-B/32")

# Define the original contrastive loss
def contrastive_loss(image, caption):
  # Tokenize the caption and encode it as a tensor
  caption = tokenizer(caption, return_tensors="pt")
  # Compute the image and text embeddings using the CLIP model
  image_embed = model.encode_image(image)
  text_embed = model.encode_text(caption)
  # Compute the dot product between the image and text embeddings
  similarity = torch.matmul(image_embed, text_embed.T)
  # Normalize the similarity by a temperature parameter
  temperature = 0.07 # a hyperparameter
  similarity = similarity / temperature
  # Compute the negative log-likelihood of choosing the correct caption over other captions in the batch
  loss = torch.nn.functional.cross_entropy(similarity, torch.arange(len(image)))
  return loss

# Define a function to generate a counterfactual caption by changing the object count in the caption
def generate_counter_caption(caption):
  # Tokenize the caption using NLTK library
  tokens = nltk.word_tokenize(caption)
  # Find the index of the first number in the caption
  number_index = None
  for i, token in enumerate(tokens):
    if token.isdigit():
      number_index = i
      break
  # If no number is found, return the original caption
  if number_index is None:
    return caption
  # Otherwise, generate a random number that is different from the original one
  original_number = int(tokens[number_index])
  counter_number = random.randint(1,10)
  while counter_number == original_number:
    counter_number = random.randint(1,10)
  # Replace the original number with the counter number in the caption
  tokens[number_index] = str(counter_number)
  # Join the tokens back into a string and return it
  counter_caption = " ".join(tokens)
  return counter_caption

# Define the counting-contrastive loss
def counting_loss(image, caption):
  # Generate a counterfactual caption by changing the object count in the caption
  counter_caption = generate_counter_caption(caption)
  # Tokenize and encode both captions as tensors
  caption = tokenizer(caption, return_tensors="pt")
  counter_caption = tokenizer(counter_caption, return_tensors="pt")
  # Compute the image and text embeddings using the CLIP model
  image_embed = model.encode_image(image)
  text_embed = model.encode_text(caption)
  counter_text_embed = model.encode_text(counter_caption)
  # Compute the dot product between the image and text embeddings
  similarity = torch.matmul(image_embed, text_embed.T)
  counter_similarity = torch.matmul(image_embed, counter_text_embed.T)
  # Normalize the similarities by a temperature parameter
  temperature = 0.07 # a hyperparameter
  similarity = similarity / temperature
  counter_similarity = counter_similarity / temperature
  # Concatenate the similarities into a matrix of shape (batch_size,2)
  similarities = torch.cat([similarity.unsqueeze(1),counter_similarity.unsqueeze(1)],dim=1)
  # Compute the negative log-likelihood of choosing the correct caption over the counter-caption
  loss = torch.nn.functional.cross_entropy(similarities, torch.zeros(len(image)).long())
  return loss

# Define the total loss as a weighted sum of the original contrastive loss and the counting-contrastive loss
def total_loss(image, caption):
  # Compute the original contrastive loss
  original_loss = contrastive_loss(image, caption)
  # Compute the counting-contrastive loss
  counting_loss = counting_loss(image, caption)
  # Compute the weighted sum of the losses with a hyperparameter alpha
  alpha = 0.5 # a hyperparameter
  total_loss = alpha * original_loss + (1 - alpha) * counting_loss
  return total_loss

# Finetune the CLIP model on a balanced mix of original and counterfactual examples using ImageNet dataset as an example data source 
# Create an ImageNet data loader with torchvision library 
data_loader = torchvision.datasets.ImageNet(root="data", split="train", transform=torchvision.transforms.ToTensor())
data_loader = torch.utils.data.DataLoader(data_loader, batch_size=256)

# Create an Adam optimizer with a learning rate of 1e-5 
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

# Finetune for 10 epochs 
for epoch in range(10):
  for batch in data_loader:
    # Get a batch of images and captions from ImageNet dataset
    images, captions = batch
    # Compute the total loss for the batch
    loss = total_loss(images, captions)
    # Update the model parameters using Adam optimizer
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Evaluate the finetuned model on CountBench benchmark and compare with baseline models
# Load the CountBench dataset from https://github.com/ronipaiss/CountBench
countbench = load_countbench_dataset()
# Define a function to compute the accuracy of the model on CountBench
def evaluate(model, countbench):
  # Initialize a variable to store the number of correct predictions
  correct = 0
  # Loop over the CountBench dataset
  for image, caption in countbench:
    # Tokenize and encode the caption as a tensor
    caption = tokenizer(caption, return_tensors="pt")
    # Compute the image and text embeddings using the CLIP model
    image_embed = model.encode_image(image)
    text_embed = model.encode_text(caption)
    # Compute the dot product between the image and text embeddings
    similarity = torch.matmul(image_embed, text_embed.T)
    # Normalize the similarity by a temperature parameter
    temperature = 0.07 # a hyperparameter
    similarity = similarity / temperature
    # Compute the softmax probability of choosing the correct caption over other captions in the dataset
    probability = torch.nn.functional.softmax(similarity, dim=1)
    # Find the index of the caption with the highest probability
    prediction = torch.argmax(probability, dim=1)
    # Check if the prediction matches the ground truth index
    if prediction == torch.arange(len(image)):
      # Increment the number of correct predictions by one
      correct += 1
  # Compute the accuracy as the ratio of correct predictions to total predictions
  accuracy = correct / len(countbench)
  return accuracy

# Print the accuracy of the finetuned model on CountBench 
print(evaluate(model, countbench))
```