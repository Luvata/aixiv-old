---
title: 2305.19725v2 Direct Learning-Based Deep Spiking Neural Networks  A Review
date: 2023-05-20
---

# [Direct Learning-Based Deep Spiking Neural Networks: A Review](http://arxiv.org/abs/2305.19725v2)

authors: Yufei Guo, Xuhui Huang, Zhe Ma


## What, Why and How

[1]: https://arxiv.org/pdf/2305.19725v2.pdf "Direct Learning-Based Deep Spiking Neural Networks: A Review - arXiv.org"
[2]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."
[3]: https://avdata.ford.com/ "Ford AV Dataset - Home"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper is a review of direct learning-based deep spiking neural networks (SNNs), which are brain-inspired computational models that use binary spikes to transmit information and process it in a spatial-temporal manner.
- **Why**: The paper aims to provide a comprehensive survey of the recent progress and challenges in directly training deep SNNs using the surrogate gradient method, which can overcome the optimization difficulty caused by the undifferentiable spiking mechanism of SNNs.
- **How**: The paper categorizes the direct learning-based deep SNN works into three main groups: accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods. It further divides these groups into finer granularities and introduces them with examples and comparisons. It also prospects the future research directions and trends in this field.

## Main Contributions

[1]: https://arxiv.org/pdf/2305.19725v2.pdf "Direct Learning-Based Deep Spiking Neural Networks: A Review - arXiv.org"
[2]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."
[3]: https://avdata.ford.com/ "Ford AV Dataset - Home"

According to the paper at [^1^][1], some of its contributions are:

- It provides a **comprehensive survey** of the direct learning-based deep SNN works, covering various aspects such as accuracy, efficiency, and temporal dynamics.
- It **categorizes** the direct learning-based deep SNN works into three main groups and further divides them into finer granularities, which can help organize and compare them more systematically and clearly.
- It **prospects** the future research directions and trends in this field, such as improving the surrogate gradient method, exploring the temporal coding scheme, and applying deep SNNs to more tasks and domains.

## Method Summary

[1]: https://arxiv.org/pdf/2305.19725v2.pdf "Direct Learning-Based Deep Spiking Neural Networks: A Review - arXiv.org"
[2]: https://arxiv.org/abs/2305.10601 "[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large ..."
[3]: https://avdata.ford.com/ "Ford AV Dataset - Home"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper introduces the **surrogate gradient method**, which is a general technique to train deep SNNs by approximating the gradient of the spiking activation function with a smooth function.
- The paper **categorizes** the direct learning-based deep SNN works into three main groups based on their objectives and methods: accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods.
- The paper further **divides** each group into finer granularities and **introduces** them with examples and comparisons. For example, the accuracy improvement methods are divided into four subgroups: loss function design, spike encoding scheme, network architecture design, and regularization technique. The paper gives a brief overview of each subgroup and compares their advantages and disadvantages.
- The paper also **discusses** some common issues and challenges in directly training deep SNNs, such as the choice of surrogate gradient function, the trade-off between accuracy and efficiency, and the evaluation metrics.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the spiking activation function and its surrogate gradient function
def spiking_activation(x):
  if x > threshold:
    return 1 # fire a spike
  else:
    return 0 # no spike

def surrogate_gradient(x):
  return smooth_function(x) # approximate the gradient of spiking activation

# Define the loss function for direct learning
def loss_function(y_true, y_pred):
  return some_function(y_true, y_pred) # e.g. cross-entropy, hinge loss, etc.

# Define the spike encoding scheme
def spike_encoding(x):
  return some_function(x) # e.g. rate coding, temporal coding, etc.

# Define the network architecture
def network_architecture():
  return some_function() # e.g. convolutional layers, recurrent layers, etc.

# Define the regularization technique
def regularization():
  return some_function() # e.g. dropout, weight decay, etc.

# Train the deep SNN using the surrogate gradient method
def train_deep_SNN(data, labels):
  # Initialize the network parameters
  parameters = initialize_parameters()
  # Loop over the training data
  for x, y in zip(data, labels):
    # Encode the input data into spikes
    spikes = spike_encoding(x)
    # Forward propagate the spikes through the network
    outputs = forward_propagate(spikes, parameters)
    # Compute the loss between the outputs and the labels
    loss = loss_function(y, outputs)
    # Backpropagate the loss using the surrogate gradient function
    gradients = backpropagate(loss, parameters, surrogate_gradient)
    # Update the network parameters using the gradients
    parameters = update_parameters(parameters, gradients)
    # Apply regularization to prevent overfitting
    parameters = regularization(parameters)
  # Return the trained network parameters
  return parameters
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# Define the spiking activation function and its surrogate gradient function
def spiking_activation(x):
  # Use a Heaviside step function to fire spikes
  return torch.heaviside(x, torch.tensor([0.0]))

def surrogate_gradient(x):
  # Use a piecewise linear function to approximate the gradient of spiking activation
  return torch.where(x < -1, torch.zeros_like(x), torch.where(x <= 1, x + 1, torch.ones_like(x)))

# Define the loss function for direct learning
def loss_function(y_true, y_pred):
  # Use the cross-entropy loss function
  return nn.CrossEntropyLoss()(y_pred, y_true)

# Define the spike encoding scheme
def spike_encoding(x):
  # Use the rate coding scheme to encode the input data into spikes
  # The input data is normalized to [0, 1] and multiplied by a scaling factor
  scaling_factor = 100
  x = x / x.max() * scaling_factor
  # The input data is compared with a random tensor to generate spikes
  random_tensor = torch.rand_like(x)
  spikes = spiking_activation(x - random_tensor)
  return spikes

# Define the network architecture
def network_architecture():
  # Use a convolutional network with two convolutional layers and one fully connected layer
  # Each layer is followed by a spiking activation function and a pooling layer
  model = nn.Sequential(
    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),
    spiking_activation,
    nn.AvgPool2d(kernel_size=2),
    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),
    spiking_activation,
    nn.AvgPool2d(kernel_size=2),
    nn.Flatten(),
    nn.Linear(in_features=32*7*7, out_features=10),
    spiking_activation,
    nn.AvgPool1d(kernel_size=10)
  )
  return model

# Define the regularization technique
def regularization():
  # Use dropout as a regularization technique to prevent overfitting
  dropout = nn.Dropout(p=0.5)
  return dropout

# Train the deep SNN using the surrogate gradient method
def train_deep_SNN(data, labels):
  # Initialize the network parameters
  model = network_architecture()
  optimizer = optim.Adam(model.parameters(), lr=0.001)
  # Loop over the training data for a number of epochs
  epochs = 10
  for epoch in range(epochs):
    # Loop over the batches of data and labels
    batch_size = 32
    for i in range(0, len(data), batch_size):
      x = data[i:i+batch_size]
      y = labels[i:i+batch_size]
      # Encode the input data into spikes
      spikes = spike_encoding(x)
      # Forward propagate the spikes through the network
      outputs = model(spikes)
      # Compute the loss between the outputs and the labels
      loss = loss_function(y, outputs)
      # Backpropagate the loss using the surrogate gradient function
      optimizer.zero_grad()
      loss.backward(surrogate_gradient)
      optimizer.step()
      # Apply regularization to prevent overfitting
      model = regularization(model)
    # Print the epoch and the loss
    print(f"Epoch {epoch}, Loss {loss.item()}")
  # Return the trained network parameters
  return model.parameters()
```