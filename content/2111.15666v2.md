---
title: 2111.15666v2 HyperStyle  StyleGAN Inversion with HyperNetworks for Real Image Editing
date: 2021-11-16
---

# [HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing](http://arxiv.org/abs/2111.15666v2)

authors: Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit H. Bermano


## What, Why and How

[1]: https://arxiv.org/abs/2111.15666 "[2111.15666] HyperStyle: StyleGAN Inversion with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2111.15666.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export1.library.cornell.edu/abs/2103.15666v2 "[2103.15666v2] Spatial Characterization of Electromagnetic Random Channels"

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces HyperStyle, a novel method for inverting real images into StyleGAN's latent space for image editing.
- **Why**: The paper aims to overcome the trade-off between reconstruction and editability that existing methods face when inverting real images into StyleGAN's latent space. The paper claims that HyperStyle can achieve high-fidelity reconstructions and semantic control over the inverted images.
- **How**: The paper proposes to use a hypernetwork that learns to modulate StyleGAN's weights to adapt them to a given image. The paper shows that this approach can reduce the number of parameters needed for the hypernetwork and improve the quality and editability of the reconstructions. The paper also demonstrates the applications of HyperStyle on various image editing tasks, such as out-of-domain editing, style transfer, and attribute manipulation.

## Main Contributions

[1]: https://arxiv.org/abs/2111.15666 "[2111.15666] HyperStyle: StyleGAN Inversion with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2111.15666.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export1.library.cornell.edu/abs/2103.15666v2 "[2103.15666v2] Spatial Characterization of Electromagnetic Random Channels"

According to the paper[^1^][1], the main contributions are:

- **HyperStyle**, a novel method for inverting real images into StyleGAN's latent space using a hypernetwork that modulates StyleGAN's weights.
- A **parameter-efficient** design for the hypernetwork that reduces the number of parameters from over three billion to be comparable to existing encoders.
- A **comprehensive evaluation** of HyperStyle on various image editing tasks, such as out-of-domain editing, style transfer, and attribute manipulation, showing its advantages over existing methods in terms of reconstruction quality and editability.

## Method Summary

[1]: https://arxiv.org/abs/2111.15666 "[2111.15666] HyperStyle: StyleGAN Inversion with ... - arXiv.org"
[2]: https://arxiv.org/pdf/2111.15666.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export1.library.cornell.edu/abs/2103.15666v2 "[2103.15666v2] Spatial Characterization of Electromagnetic Random Channels"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first reviews the background of StyleGAN and its latent space structure, which consists of two subspaces: W and W+.
- The paper then introduces the concept of hypernetworks, which are networks that generate the weights of another network (the target network) based on some input (the hyperinput).
- The paper proposes to use a hypernetwork to modulate StyleGAN's weights based on a given image, such that the image can be reconstructed in W+ space with high fidelity and editability.
- The paper describes the architecture and training procedure of the hypernetwork, which consists of three components: an encoder, a modulator, and a discriminator.
- The encoder maps the input image to a latent code in W space, which serves as the hyperinput for the modulator.
- The modulator generates modulation parameters for each layer of StyleGAN based on the latent code, which are used to adjust StyleGAN's weights via adaptive instance normalization (AdaIN).
- The discriminator distinguishes between real images and reconstructions from the modulated StyleGAN, and provides a perceptual loss for training the encoder and the modulator.
- The paper also introduces a regularization term to encourage the latent code to be close to the mean of W space, which improves the editability of the reconstructions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define StyleGAN and its latent space W and W+
StyleGAN = load_pretrained_model()
W = StyleGAN.get_W_space()
W_plus = StyleGAN.get_W_plus_space()

# Define the hypernetwork components: encoder, modulator, and discriminator
Encoder = Convolutional_Network(input_shape = image_shape, output_shape = W.shape)
Modulator = Fully_Connected_Network(input_shape = W.shape, output_shape = num_layers * modulation_params.shape)
Discriminator = Convolutional_Network(input_shape = image_shape, output_shape = 1)

# Define the loss functions: perceptual loss and regularization loss
Perceptual_Loss = LPIPS_Loss()
Regularization_Loss = L2_Loss()

# Define the hyperparameters: learning rate, batch size, number of iterations
lr = 0.001
batch_size = 16
num_iter = 10000

# Define the optimizer: Adam
Optimizer = Adam(lr)

# Train the hypernetwork
for i in range(num_iter):
  # Sample a batch of real images
  real_images = sample_batch(data_loader, batch_size)
  
  # Encode the real images to latent codes in W space
  latent_codes = Encoder(real_images)
  
  # Modulate StyleGAN's weights based on the latent codes
  modulation_params = Modulator(latent_codes)
  StyleGAN.apply_modulation(modulation_params)
  
  # Reconstruct the real images from the modulated StyleGAN in W+ space
  reconstructions = StyleGAN.generate_images(latent_codes)
  
  # Compute the perceptual loss between the real images and the reconstructions
  perceptual_loss = Perceptual_Loss(real_images, reconstructions)
  
  # Compute the regularization loss to keep the latent codes close to the mean of W space
  regularization_loss = Regularization_Loss(latent_codes, W.mean())
  
  # Compute the total loss as a weighted sum of the perceptual loss and the regularization loss
  total_loss = perceptual_loss + lambda * regularization_loss
  
  # Update the encoder and the modulator parameters using the optimizer and the total loss
  Optimizer.zero_grad()
  total_loss.backward()
  Optimizer.step()
  
  # Update the discriminator parameters using the optimizer and the adversarial loss
  Optimizer.zero_grad()
  adversarial_loss = Discriminator.get_adversarial_loss(real_images, reconstructions)
  adversarial_loss.backward()
  Optimizer.step()
  
# Save the trained hypernetwork
save_model(Encoder, Modulator, Discriminator)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import lpips
import numpy as np

# Define the image size and the number of StyleGAN layers
image_size = 256
num_layers = 18

# Load the pretrained StyleGAN model and its latent space W and W+
StyleGAN = torch.hub.load('NVIDIA/StyleGAN2-ADA-PyTorch', 'generator', pretrained=True)
W = StyleGAN.get_latent_space('w')
W_plus = StyleGAN.get_latent_space('w+')

# Define the encoder network as a convolutional network with residual blocks and a global average pooling layer
class Encoder(torch.nn.Module):
  def __init__(self, input_shape, output_shape):
    super(Encoder, self).__init__()
    self.input_shape = input_shape
    self.output_shape = output_shape
    
    # Define the convolutional layers with ReLU activation and batch normalization
    self.conv1 = torch.nn.Conv2d(input_shape[0], 64, kernel_size=7, stride=2, padding=3)
    self.relu1 = torch.nn.ReLU()
    self.bn1 = torch.nn.BatchNorm2d(64)
    self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
    self.relu2 = torch.nn.ReLU()
    self.bn2 = torch.nn.BatchNorm2d(128)
    self.conv3 = torch.nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
    self.relu3 = torch.nn.ReLU()
    self.bn3 = torch.nn.BatchNorm2d(256)
    
    # Define the residual blocks with skip connections and ReLU activation
    self.resblock1 = ResBlock(256, 256)
    self.resblock2 = ResBlock(256, 256)
    self.resblock3 = ResBlock(256, 256)
    
    # Define the global average pooling layer to reduce the spatial dimensions
    self.gap = torch.nn.AdaptiveAvgPool2d((1, 1))
    
    # Define the fully connected layer to map the pooled features to the output shape
    self.fc = torch.nn.Linear(256, output_shape[0])
  
  def forward(self, x):
    # Apply the convolutional layers
    x = self.conv1(x)
    x = self.relu1(x)
    x = self.bn1(x)
    x = self.conv2(x)
    x = self.relu2(x)
    x = self.bn2(x)
    x = self.conv3(x)
    x = self.relu3(x)
    x = self.bn3(x)
    
    # Apply the residual blocks
    x = self.resblock1(x)
    x = self.resblock2(x)
    x = self.resblock3(x)
    
    # Apply the global average pooling layer
    x = self.gap(x)
    
    # Flatten the features and apply the fully connected layer
    x = x.view(x.size(0), -1)
    x = self.fc(x)
    
    # Return the output
    return x

# Define the residual block as a convolutional network with two convolutional layers and a skip connection
class ResBlock(torch.nn.Module):
  def __init__(self, in_channels, out_channels):
    super(ResBlock, self).__init__()
    
    # Define the first convolutional layer with ReLU activation and batch normalization
    self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
    self.relu1 = torch.nn.ReLU()
    self.bn1 = torch.nn.BatchNorm2d(out_channels)
    
    # Define the second convolutional layer with batch normalization
    self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
    self.bn2 = torch.nn.BatchNorm2d(out_channels)
    
  def forward(self, x):
  
     # Save the input for the skip connection
     identity = x
     
     # Apply the first convolutional layer
     x = self.conv1(x)
     x = self.relu1(x)
     x = self.bn1(x)
     
     # Apply the second convolutional layer
     x = self.conv2(x)
     x = self.bn2(x)

     # Add the input to the output via the skip connection
     x += identity
     
     # Return the output
     return x

# Define the modulator network as a fully connected network with two hidden layers and a sigmoid activation
class Modulator(torch.nn.Module):
  def __init__(self, input_shape, output_shape):
    super(Modulator, self).__init__()
    self.input_shape = input_shape
    self.output_shape = output_shape
    
    # Define the first hidden layer with ReLU activation and batch normalization
    self.fc1 = torch.nn.Linear(input_shape[0], 512)
    self.relu1 = torch.nn.ReLU()
    self.bn1 = torch.nn.BatchNorm1d(512)
    
    # Define the second hidden layer with ReLU activation and batch normalization
    self.fc2 = torch.nn.Linear(512, 1024)
    self.relu2 = torch.nn.ReLU()
    self.bn2 = torch.nn.BatchNorm1d(1024)
    
    # Define the output layer with sigmoid activation
    self.fc3 = torch.nn.Linear(1024, output_shape[0])
    self.sigmoid = torch.nn.Sigmoid()
  
  def forward(self, x):
    # Apply the first hidden layer
    x = self.fc1(x)
    x = self.relu1(x)
    x = self.bn1(x)
    
    # Apply the second hidden layer
    x = self.fc2(x)
    x = self.relu2(x)
    x = self.bn2(x)
    
    # Apply the output layer
    x = self.fc3(x)
    x = self.sigmoid(x)
    
    # Return the output
    return x

# Define the discriminator network as a convolutional network with four convolutional layers and a fully connected layer
class Discriminator(torch.nn.Module):
  def __init__(self, input_shape, output_shape):
    super(Discriminator, self).__init__()
    self.input_shape = input_shape
    self.output_shape = output_shape
    
    # Define the first convolutional layer with LeakyReLU activation and batch normalization
    self.conv1 = torch.nn.Conv2d(input_shape[0], 64, kernel_size=4, stride=2, padding=1)
    self.lrelu1 = torch.nn.LeakyReLU(0.2)
    self.bn1 = torch.nn.BatchNorm2d(64)
    
    # Define the second convolutional layer with LeakyReLU activation and batch normalization
    self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
    self.lrelu2 = torch.nn.LeakyReLU(0.2)
    self.bn2 = torch.nn.BatchNorm2d(128)
    
     # Define the third convolutional layer with LeakyReLU activation and batch normalization
     self.conv3 = torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)
     self.lrelu3 = torch.nn.LeakyReLU(0.2)
     self.bn3 = torch.nn.BatchNorm2d(256)

     # Define the fourth convolutional layer with LeakyReLU activation and batch normalization
     self.conv4 = torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)
     self.lrelu4 = torch.nn.LeakyReLU(0.2)
     self.bn4 = torch.nn.BatchNorm2d(512)

     # Define the fully connected layer to map the features to the output shape
     self.fc = torch.nn.Linear(512 * 4 * 4, output_shape[0])
  
  def forward(self, x):
  
     # Apply the convolutional layers
     x = self.conv1(x)
     x = self.lrelu1(x)
     x = self.bn1(x)
     x = self.conv2(x)
     x = self.lrelu2(x)
     x = self.bn2(x)
     x = self.conv3(x)
     x = self.lrelu3(x)
     x = self.bn3(x)
     x = self.conv4(x)
     x = self.lrelu4(x)
     x = self.bn4(x)

     # Flatten the features and apply the fully connected layer
     x = x.view(x.size(0), -1)
     x = self.fc(x)

     # Return the output
     return x
  
  def get_adversarial_loss(self, real_images, fake_images):
  
      # Compute the logits for the real images and the fake images
      real_logits = self.forward(real_images)
      fake_logits = self.forward(fake_images)

      # Compute the binary cross entropy loss for the real images and the fake images
      real_loss = torch.nn.BCEWithLogitsLoss()(real_logits, torch.ones(real_logits.size()))
      fake_loss = torch.nn.BCEWithLogitsLoss()(fake_logits, torch.zeros(fake_logits.size()))

      # Compute the total loss as the sum of the real loss and the fake loss
      total_loss = real_loss + fake_loss

      # Return the total loss
      return total_loss

# Define the perceptual loss function