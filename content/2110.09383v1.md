---
title: 2110.09383v1 Neuro-Symbolic Forward Reasoning
date: 2021-10-10
---

# [Neuro-Symbolic Forward Reasoning](http://arxiv.org/abs/2110.09383v1)

authors: Hikaru Shindo, Devendra Singh Dhami, Kristian Kersting


## What, Why and How

[1]: https://arxiv.org/pdf/2110.09383v1 "arXiv:2110.09383v1 [cs.AI] 18 Oct 2021"
[2]: https://arxiv.org/abs/2110.09383 "[2110.09383] Neuro-Symbolic Forward Reasoning - arXiv.org"
[3]: http://export.arxiv.org/abs/1804.09383v1 "[1804.09383v1] Hidden-Strange $N\\phi$ molecular state in a ..."

Here is a summary of the paper:

- **What**: The paper proposes a new approach for reasoning tasks called **Neuro-Symbolic Forward Reasoner (NSFR)**[^1^][1] [^2^][2], which combines differentiable forward-chaining reasoning with object-centric learning.
- **Why**: The paper aims to address the challenge of performing low-level visual perception and high-level reasoning on objects and their attributes, which is a major goal of artificial intelligence research[^1^][1] [^2^][2].
- **How**: The paper introduces a framework that factorizes raw inputs into object-centric representations, converts them into probabilistic ground atoms, and performs differentiable forward-chaining inference using weighted rules for inference[^1^][1] [^2^][2]. The paper evaluates the approach on object-centric reasoning data sets, 2D Kandinsky patterns and 3D CLEVR-Hans, and shows its effectiveness and advantage over existing methods[^1^][1] [^2^][2].


## Main Contributions

According to the paper, the main contributions are:

- A novel approach for reasoning tasks, **Neuro-Symbolic Forward Reasoner (NSFR)**, that leverages differentiable forward-chaining reasoning with object-centric learning.
- A consistent framework to perform the forward-chaining inference from raw inputs, which can handle both symbolic and sub-symbolic information.
- A comprehensive experimental evaluation on object-centric reasoning data sets, 2D Kandinsky patterns and 3D CLEVR-Hans, and a variety of tasks, such as classification, counting, and relational reasoning.
- A comparison with existing methods and an analysis of the strengths and limitations of the proposed approach.

## Method Summary

[1]: https://arxiv.org/pdf/2110.09383.pdf "arXiv:2110.09383v1 [cs.AI] 18 Oct 2021"
[2]: https://www.sagepub.com/sites/default/files/upm-binaries/14649_Chapter5.pdf "The Method Chapter - SAGE Publications Inc"
[3]: https://arxiv.org/pdf/2110.09260 "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The method section describes the proposed approach, **Neuro-Symbolic Forward Reasoner (NSFR)**, which consists of three main components: **object-centric learning**, **probabilistic grounding**, and **differentiable forward-chaining reasoning**[^1^][1].
- The object-centric learning component uses a neural network to extract object-centric representations from raw inputs, such as images or videos. The representations consist of object features, such as shape, color, size, and position[^1^][1].
- The probabilistic grounding component converts the object-centric representations into probabilistic ground atoms, which are the basic units of first-order logic. The probabilistic ground atoms represent the existence and attributes of each object in a probabilistic way[^1^][1].
- The differentiable forward-chaining reasoning component performs inference using weighted rules for inference, which are derived from first-order logic rules. The inference is done in a differentiable manner, using a soft-max function to compute logical entailments smoothly. The inference can deduce new facts from given facts and rules[^1^][1].
- The method section also explains how to train the NSFR model using a loss function that combines classification loss and consistency loss. The classification loss measures the accuracy of answering questions based on the inferred facts. The consistency loss measures the coherence of the inferred facts with the given facts and rules[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the object-centric learning network
network = ObjectCentricNetwork()

# Define the probabilistic grounding function
def probabilistic_grounding(object_features):
  # Initialize an empty list of probabilistic ground atoms
  atoms = []
  # For each object feature vector
  for f in object_features:
    # Compute the existence probability of the object
    p_exist = sigmoid(f[0])
    # Compute the attribute probabilities of the object
    p_shape = softmax(f[1:6])
    p_color = softmax(f[6:11])
    p_size = softmax(f[11:16])
    p_position = softmax(f[16:21])
    # Create a probabilistic ground atom for the object
    atom = ProbabilisticAtom(p_exist, p_shape, p_color, p_size, p_position)
    # Append the atom to the list
    atoms.append(atom)
  # Return the list of probabilistic ground atoms
  return atoms

# Define the differentiable forward-chaining reasoning function
def differentiable_forward_chaining(atoms, rules):
  # Initialize an empty list of inferred atoms
  inferred_atoms = []
  # For each rule in the rules
  for rule in rules:
    # Extract the premise and conclusion of the rule
    premise = rule.premise
    conclusion = rule.conclusion
    # Compute the entailment probability of the rule
    p_entail = softmax(rule.weight)
    # For each pair of atoms in the atoms
    for a1, a2 in itertools.combinations(atoms, 2):
      # Check if the pair matches the premise of the rule
      if match(premise, a1, a2):
        # Apply the rule to infer a new atom
        inferred_atom = apply(conclusion, a1, a2)
        # Multiply the entailment probability with the existence probabilities of the pair
        inferred_atom.p_exist *= p_entail * a1.p_exist * a2.p_exist
        # Append the inferred atom to the list
        inferred_atoms.append(inferred_atom)
  # Return the list of inferred atoms
  return inferred_atoms

# Define the NSFR model
class NSFR(nn.Module):
  def __init__(self, network, rules):
    # Initialize the network and rules as attributes
    self.network = network
    self.rules = rules
  
  def forward(self, inputs):
    # Extract object-centric representations from inputs using network
    object_features = self.network(inputs)
    # Convert object-centric representations into probabilistic ground atoms using probabilistic grounding function
    atoms = probabilistic_grounding(object_features)
    # Perform inference using weighted rules for inference using differentiable forward-chaining reasoning function
    inferred_atoms = differentiable_forward_chaining(atoms, self.rules)
    # Return both given and inferred atoms as output facts
    return atoms + inferred_atoms

# Define the loss function for NSFR model
def loss_function(output_facts, questions, answers):
  # Initialize the total loss as zero
  total_loss = 0.0
  # For each question and answer pair in the batch
  for q, a in zip(questions, answers):
    # Extract the query and target from the question and answer pair
    query = q.query
    target = a.target
    # Compute the classification loss by comparing the output facts with the target using query 
    classification_loss = cross_entropy(query(output_facts), target)
    # Add the classification loss to the total loss
    total_loss += classification_loss
  
  # For each output fact in the batch
  for f in output_facts:
    # Compute the consistency loss by comparing the existence probability with a threshold value 
    consistency_loss = binary_cross_entropy(f.p_exist, threshold)
    # Add the consistency loss to the total loss with a weight factor 
    total_loss += weight * consistency_loss
  
  # Return the total loss 
  return total_loss

# Train and evaluate NSFR model on data sets and tasks 
train_and_evaluate(NSFR(network, rules), data_sets, tasks)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import itertools

# Define the object-centric learning network
class ObjectCentricNetwork(nn.Module):
  def __init__(self, num_objects, num_features):
    # Initialize the base class
    super(ObjectCentricNetwork, self).__init__()
    # Define the number of objects and features as attributes
    self.num_objects = num_objects
    self.num_features = num_features
    # Define the convolutional layers
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
    self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
    # Define the pooling layer
    self.pool = nn.MaxPool2d(2, 2)
    # Define the linear layers
    self.fc1 = nn.Linear(64 * 8 * 8, 256)
    self.fc2 = nn.Linear(256, num_objects * num_features)
  
  def forward(self, inputs):
    # Apply the convolutional and pooling layers to the inputs
    x = self.pool(F.relu(self.conv1(inputs)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))
    # Flatten the output of the last convolutional layer
    x = x.view(-1, 64 * 8 * 8)
    # Apply the linear layers to the flattened output
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    # Reshape the output into a batch of object feature vectors
    x = x.view(-1, self.num_objects, self.num_features)
    # Return the output as object-centric representations
    return x

# Define the probabilistic grounding function
def probabilistic_grounding(object_features):
  # Initialize an empty list of probabilistic ground atoms
  atoms = []
  # For each object feature vector in the batch
  for f in object_features:
    # Compute the existence probability of the object using a sigmoid function
    p_exist = torch.sigmoid(f[0])
    # Compute the attribute probabilities of the object using a softmax function over each attribute category
    p_shape = F.softmax(f[1:6], dim=0)
    p_color = F.softmax(f[6:11], dim=0)
    p_size = F.softmax(f[11:16], dim=0)
    p_position = F.softmax(f[16:21], dim=0)
    # Create a probabilistic ground atom for the object using a named tuple
    atom = ProbabilisticAtom(p_exist, p_shape, p_color, p_size, p_position)
    # Append the atom to the list
    atoms.append(atom)
  # Return the list of probabilistic ground atoms
  return atoms

# Define the differentiable forward-chaining reasoning function
def differentiable_forward_chaining(atoms, rules):
  # Initialize an empty list of inferred atoms
  inferred_atoms = []
  # For each rule in the rules
  for rule in rules:
    # Extract the premise and conclusion of the rule using named tuples
    premise = Premise(rule.premise[0], rule.premise[1], rule.premise[2])
    conclusion = Conclusion(rule.conclusion[0], rule.conclusion[1], rule.conclusion[2])
    # Compute the entailment probability of the rule using a softmax function over the rule weight 
    p_entail = F.softmax(rule.weight, dim=0)
    # For each pair of atoms in the atoms
    for a1, a2 in itertools.combinations(atoms, 2):
      # Check if the pair matches the premise of the rule using a matching function that compares attribute probabilities 
      if match(premise, a1, a2):
        # Apply the rule to infer a new atom using an apply function that copies attribute probabilities from premise to conclusion 
        inferred_atom = apply(conclusion, a1, a2)
        # Multiply the entailment probability with the existence probabilities of the pair 
        inferred_atom.p_exist *= p_entail * a1.p_exist * a2.p_exist
        # Append the inferred atom to the list 
        inferred_atoms.append(inferred_atom)
  # Return the list of inferred atoms 
  return inferred_atoms

# Define a matching function that compares attribute probabilities 
def match(premise, a1, a2):
  # Extract the attribute categories and values from the premise 
  category1, value1 = premise[0]
  category2, value2 = premise[1]
  relation = premise[2]
  # Extract the attribute probabilities from the atoms 
  p_attr1 = getattr(a1, category1)
  p_attr2 = getattr(a2, category2)
  # Compute the similarity scores between the attribute values and probabilities using a dot product 
  score1 = torch.dot(value1, p_attr1)
  score2 = torch.dot(value2, p_attr2)
  # Check if the similarity scores are above a threshold value 
  if score1 > threshold and score2 > threshold:
    # Check if the relation between the attribute values is satisfied using a relation function that compares attribute probabilities 
    if relation(a1, a2):
      # Return True if the pair matches the premise 
      return True
  # Return False otherwise 
  return False

# Define an apply function that copies attribute probabilities from premise to conclusion 
def apply(conclusion, a1, a2):
  # Extract the attribute categories and values from the conclusion 
  category1, value1 = conclusion[0]
  category2, value2 = conclusion[1]
  relation = conclusion[2]
  # Extract the attribute probabilities from the atoms 
  p_attr1 = getattr(a1, category1)
  p_attr2 = getattr(a2, category2)
  # Create a new atom with the same attribute probabilities as the atoms 
  new_atom = ProbabilisticAtom(a1.p_exist, a1.p_shape, a1.p_color, a1.p_size, a1.p_position)
  # Copy the attribute values to the new atom using a copy function that assigns attribute probabilities 
  copy(value1, new_atom, category1)
  copy(value2, new_atom, category2)
  # Return the new atom as the inferred atom 
  return new_atom

# Define a copy function that assigns attribute probabilities 
def copy(value, atom, category):
  # Create a one-hot vector for the attribute value 
  one_hot = torch.zeros(len(value))
  one_hot[value] = 1.0
  # Assign the one-hot vector to the attribute probability of the atom 
  setattr(atom, category, one_hot)

# Define some relation functions that compare attribute probabilities 
def same_color(a1, a2):
  # Return True if the color probabilities of the atoms are equal 
  return torch.equal(a1.p_color, a2.p_color)

def same_shape(a1, a2):
  # Return True if the shape probabilities of the atoms are equal 
  return torch.equal(a1.p_shape, a2.p_shape)

def same_size(a1, a2):
  # Return True if the size probabilities of the atoms are equal 
  return torch.equal(a1.p_size, a2.p_size)

def left_of(a1, a2):
  # Return True if the position probability of the first atom is higher for left than for right and vice versa for the second atom 
  return a1.p_position[0] > a1.p_position[4] and a2.p_position[4] > a2.p_position[0]

# Define some query functions that compare output facts with targets 
def count_color(output_facts, target):
  # Initialize an empty dictionary for counting color frequencies 
  color_count = {}
  # For each output fact in the batch
  for f in output_facts:
    # Extract the color probability of the fact
    p_color = f.p_color
    # Find the index of the maximum value in the color probability
    color_index = torch.argmax(p_color)
    # Increment the count of that color in the dictionary
    color_count[color_index] = color_count.get(color_index, 0) + f.p_exist
  # Return the count of the target color as output
  return color_count[target]

def count_shape(output_facts, target):
  # Initialize an empty dictionary for counting shape frequencies
  shape_count = {}
  # For each output fact in the batch
  for f in output_facts:
    # Extract the shape probability of the fact
    p_shape = f.p_shape
    # Find the index of the maximum value in the shape probability
    shape_index = torch.argmax(p_shape)
    # Increment the count of that shape in the dictionary
    shape_count[shape_index] = shape_count.get(shape_index, 0) + f.p_exist
  # Return the count of the target shape as output
  return shape_count[target]

def has_attribute(output_facts, target):
   # Initialize an empty list for storing output facts that have target attribute
   facts_with_attribute = []
   # For each output fact in batch
   for f in output_facts:
     # Extract attribute category and value from target using named tuple
     category, value = target.attribute