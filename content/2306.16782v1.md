---
title: 2306.16782v1 Low-Light Enhancement in the Frequency Domain
date: 2023-06-17
---

# [Low-Light Enhancement in the Frequency Domain](http://arxiv.org/abs/2306.16782v1)

authors: Hao Chen, Zhi Jin


## What, Why and How

[1]: https://arxiv.org/abs/2306.16782 "[2306.16782] Low-Light Enhancement in the Frequency Domain - arXiv.org"
[2]: https://arxiv.org/pdf/2306.09782v1.pdf "A arXiv:2306.09782v1 [cs.CL] 16 Jun 2023"
[3]: http://export.arxiv.org/abs/2306.16782 "[2306.16782] Low-Light Enhancement in the Frequency Domain"

Here is a summary of the paper:

```
What: The paper proposes a novel residual recurrent multi-wavelet convolutional neural network (R2-MWCNN) for low-light image enhancement in the frequency domain.

Why: Low-light images suffer from decreased visibility, intensive noise, and biased color, which affect the performance of high-level vision tasks. Existing methods in the spatial domain are prone to noise amplification and color distortion.

How: The paper uses a multi-level discrete wavelet transform to divide input feature maps into distinct frequencies, resulting in better denoising effects. The paper also introduces a channel-wise loss function to correct the color distortion for more realistic results. The paper evaluates the proposed method on several benchmark datasets and compares it with state-of-the-art methods quantitatively and qualitatively.
```

## Main Contributions

The paper claims the following contributions:

- It proposes a novel residual recurrent multi-wavelet convolutional neural network (R2-MWCNN) for low-light image enhancement in the frequency domain, which can simultaneously increase the image contrast and reduce noise signals well.
- It utilizes a multi-level discrete wavelet transform to divide input feature maps into distinct frequencies, resulting in a better denoise impact.
- It proposes a channel-wise loss function to correct the color distortion for more realistic results.
- It demonstrates that the proposed method outperforms the state-of-the-art methods quantitively and qualitatively on several benchmark datasets.


## Method Summary

Here is a summary of the method section:

```
The proposed method consists of three main components: a multi-level discrete wavelet transform (MDWT), a residual recurrent multi-wavelet convolutional neural network (R2-MWCNN), and a channel-wise loss function.

- The MDWT is used to decompose the input low-light image into four sub-bands: low-low (LL), low-high (LH), high-low (HL), and high-high (HH). The LL sub-band contains the low-frequency information, while the other three sub-bands contain the high-frequency information. The MDWT can separate the noise signals from the image content and make the network focus on enhancing the LL sub-band.

- The R2-MWCNN is designed to enhance the LL sub-band in the frequency domain. It consists of two modules: a residual recurrent module (RRM) and a multi-wavelet convolutional module (MWC). The RRM is composed of several residual blocks with recurrent connections, which can capture long-range dependencies and learn residual mappings. The MWC is a novel convolutional operation that uses multiple wavelet filters to convolve with the input feature maps, which can increase the receptive field and extract multi-scale features.

- The channel-wise loss function is defined as the weighted sum of the mean squared error (MSE) and the structural similarity index measure (SSIM) for each color channel. The MSE term measures the pixel-wise difference between the enhanced image and the ground truth image, while the SSIM term measures the perceptual quality of the enhanced image. The channel-wise loss function can balance the trade-off between brightness enhancement and color preservation.
```

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Input: a low-light image I
# Output: an enhanced image O

# Step 1: Perform multi-level discrete wavelet transform (MDWT) on I
LL, LH, HL, HH = MDWT(I)

# Step 2: Enhance the LL sub-band using R2-MWCNN
LL_enhanced = R2-MWCNN(LL)

# Step 3: Perform inverse MDWT on the enhanced LL sub-band and the other three sub-bands
O = IMDWT(LL_enhanced, LH, HL, HH)

# Step 4: Compute the channel-wise loss function between O and the ground truth image G
L = channel-wise-loss(O, G)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Input: a low-light image I of size H x W x 3
# Output: an enhanced image O of size H x W x 3

# Step 1: Perform multi-level discrete wavelet transform (MDWT) on I
# Use the Haar wavelet as the basis function
# LL, LH, HL, HH are sub-bands of size H/2 x W/2 x 3
LL, LH, HL, HH = MDWT(I)

# Step 2: Enhance the LL sub-band using R2-MWCNN
# R2-MWCNN consists of two modules: RRM and MWC
# RRM is composed of N residual blocks with recurrent connections
# Each residual block has two convolutional layers with ReLU activation and batch normalization
# MWC is a convolutional operation that uses K wavelet filters to convolve with the input feature maps
# The wavelet filters are initialized with the Daubechies wavelet as the basis function
# The output feature maps are concatenated along the channel dimension
# LL_enhanced is a sub-band of size H/2 x W/2 x 3

# Define the RRM module
def RRM(x):
  # x is a feature map of size H/2 x W/2 x C
  # Initialize a hidden state h with zeros of size H/2 x W/2 x C
  h = zeros(H/2, W/2, C)
  # Loop over N residual blocks
  for i in range(N):
    # Save the input feature map as a skip connection
    skip = x
    # Apply the first convolutional layer with C filters of size 3 x 3 and a stride of 1
    x = conv(x, C, 3, 1)
    # Apply ReLU activation and batch normalization
    x = relu(x)
    x = batch_norm(x)
    # Apply the second convolutional layer with C filters of size 3 x 3 and a stride of 1
    x = conv(x, C, 3, 1)
    # Apply ReLU activation and batch normalization
    x = relu(x)
    x = batch_norm(x)
    # Add the skip connection and the hidden state to the output feature map
    x = x + skip + h
    # Update the hidden state with the output feature map
    h = x
  # Return the output feature map
  return x

# Define the MWC module
def MWC(x):
  # x is a feature map of size H/2 x W/2 x C
  # Initialize an empty list to store the output feature maps
  outputs = []
  # Loop over K wavelet filters
  for k in range(K):
    # Apply the k-th wavelet filter to convolve with the input feature map
    # The wavelet filter has C filters of size 3 x 3 and a stride of 1
    y = wavelet_conv(x, C, k, 3, 1)
    # Append the output feature map to the list
    outputs.append(y)
  # Concatenate the output feature maps along the channel dimension
  # The output feature map has size H/2 x W/2 x KC
  z = concat(outputs, axis=3)
  # Return the output feature map
  return z

# Apply the RRM module to the LL sub-band
LL_RRM = RRM(LL)

# Apply the MWC module to the output of the RRM module
LL_MWC = MWC(LL_RRM)

# Apply a final convolutional layer with 3 filters of size 1 x 1 and a stride of 1 to obtain the enhanced LL sub-band
LL_enhanced = conv(LL_MWC, 3, 1, 1)

# Step 3: Perform inverse MDWT on the enhanced LL sub-band and the other three sub-bands
# Use the Haar wavelet as the basis function
# O is an image of size H x W x 3
O = IMDWT(LL_enhanced, LH, HL, HH)

# Step 4: Compute the channel-wise loss function between O and the ground truth image G
# The channel-wise loss function is defined as L = alpha * MSE + beta * (1 - SSIM) for each color channel
# alpha and beta are hyperparameters that control the trade-off between brightness enhancement and color preservation

# Define the MSE function for a single channel
def MSE(x, y):
  # x and y are images of size H x W x 1
  # Return the mean squared error between x and y
  return mean((x - y) ** 2)

# Define the SSIM function for a single channel
def SSIM(x, y):
  # x and y are images of size H x W x 1
  # Use a Gaussian filter of size 11 x 11 and a standard deviation of 1.5 as the window function
  # Use the default values of C1 = (0.01 * 255) ** 2 and C2 = (0.03 * 255) ** 2 as the constants
  # Return the structural similarity index measure between x and y
  return ssim(x, y, window=gaussian(11, 1.5), C1=(0.01 * 255) ** 2, C2=(0.03 * 255) ** 2)

# Initialize the channel-wise loss function as zero
L = 0

# Loop over the three color channels
for c in range(3):
  # Extract the c-th channel from O and G
  O_c = O[:, :, c]
  G_c = G[:, :, c]
  # Compute the MSE and SSIM for the c-th channel
  MSE_c = MSE(O_c, G_c)
  SSIM_c = SSIM(O_c, G_c)
  # Compute the channel-wise loss function for the c-th channel
  L_c = alpha * MSE_c + beta * (1 - SSIM_c)
  # Add the channel-wise loss function to the total loss function
  L = L + L_c

# Return the enhanced image O and the channel-wise loss function L
return O, L
```