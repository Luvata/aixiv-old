---
title: 2004.09965v4 Single Pair Cross-Modality Super Resolution
date: 2020-04-10
---

# [Single Pair Cross-Modality Super Resolution](http://arxiv.org/abs/2004.09965v4)

authors: Guy Shacht, Sharon Fogel, Dov Danon, Daniel Cohen-Or, Ilya Leizerson


## What, Why and How

[1]: https://arxiv.org/abs/2004.09965v4 "[2004.09965v4] Single Pair Cross-Modality Super Resolution - arXiv.org"
[2]: https://arxiv.org/pdf/2004.09965v4.pdf "Single Pair Joint Cross-Modality Super Resolution - arXiv.org"
[3]: http://export.arxiv.org/abs/2108.09965v4 "[2108.09965v4] Simulation study of Open Charm production in proton ..."

Here is a summary of the paper at [^1^][2] in terms of What, Why, and How:

- **What**: The paper presents CMSR, a deep network for Single Pair Cross-Modality Super Resolution, which can increase the resolution of a low-resolution non-visual image (such as IR or depth) using a high-resolution RGB image as a guide.
- **Why**: The paper aims to address the challenges of fusing images from different modalities, such as alignment, artifacts, and modality fidelity. The paper also claims that existing methods are either trained on external datasets that may not match the input pair, or rely on hand-crafted features that may not capture the complex correlations between modalities.
- **How**: The paper proposes a single-pair approach that learns the internal statistics and correlations of the input pair only, without requiring any external data or supervision. The paper also introduces an internal transformer module that is trained on-the-fly to deal with weakly aligned pairs. The paper evaluates the proposed method on several datasets and compares it with state-of-the-art methods, showing that CMSR produces high-quality super-resolved images that are faithful to the target modality.

## Main Contributions

According to the paper at , the main contributions are:

- A novel single-pair approach for Cross-Modality Super-Resolution that does not require any external data or supervision.
- An internal transformer module that is trained on-the-fly to handle weakly aligned pairs and avoid artifacts.
- A comprehensive evaluation on several datasets and modalities, demonstrating the effectiveness and robustness of the proposed method.

## Method Summary

The method section of the paper at  describes the proposed CMSR network in detail. The network consists of three main components: a feature extraction module, an internal transformer module, and a reconstruction module. The feature extraction module extracts low-level and high-level features from both the low-resolution target modality image and the high-resolution RGB image using convolutional layers. The internal transformer module learns the cross-modality correlations between the features using self-attention mechanisms and produces a fused feature map. The reconstruction module upsamples the fused feature map using sub-pixel convolution layers and generates the super-resolved target modality image. The network is trained on the input pair only, using a combination of pixel-wise loss and perceptual loss. The paper also explains how the internal transformer module is designed to handle weakly aligned pairs by using local patches and positional encodings. The paper provides the network architecture and the training details in the supplementary material.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a low-resolution target modality image X and a high-resolution RGB image Y
# Output: a super-resolved target modality image Z

# Define the network parameters and the loss functions
initialize network parameters theta
define pixel-wise loss L_p
define perceptual loss L_per

# Train the network on the input pair only
for epoch in range(num_epochs):
  # Extract features from X and Y using convolutional layers
  F_X = feature_extraction(X, theta)
  F_Y = feature_extraction(Y, theta)

  # Fuse features using self-attention and positional encodings
  F_Z = internal_transformer(F_X, F_Y, theta)

  # Upsample and reconstruct Z using sub-pixel convolution layers
  Z = reconstruction(F_Z, theta)

  # Compute the total loss and update the network parameters
  loss = L_p(Z, X) + L_per(Z, X)
  theta = theta - learning_rate * gradient(loss, theta)

# Return the super-resolved image Z
return Z
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: a low-resolution target modality image X and a high-resolution RGB image Y
# Output: a super-resolved target modality image Z

# Define the network parameters and the loss functions
# The network consists of three modules: feature extraction, internal transformer, and reconstruction
# The feature extraction module has two branches: one for X and one for Y
# Each branch has four convolutional layers with ReLU activation and batch normalization
# The internal transformer module has two sub-modules: self-attention and fusion
# The self-attention sub-module computes the attention weights between patches of F_X and F_Y
# The fusion sub-module combines the features using the attention weights and adds positional encodings
# The reconstruction module has three sub-pixel convolution layers with ReLU activation and batch normalization
# The pixel-wise loss is the mean squared error between Z and X
# The perceptual loss is the mean squared error between the VGG19 features of Z and X
initialize network parameters theta
define pixel-wise loss L_p
define perceptual loss L_per

# Train the network on the input pair only
for epoch in range(num_epochs):
  # Extract features from X and Y using convolutional layers
  # F_X and F_Y are feature maps of size C x H x W, where C is the number of channels, H is the height, and W is the width
  F_X = feature_extraction(X, theta)
  F_Y = feature_extraction(Y, theta)

  # Fuse features using self-attention and positional encodings
  # F_Z is a feature map of size C x H x W
  F_Z = internal_transformer(F_X, F_Y, theta)

  # Upsample and reconstruct Z using sub-pixel convolution layers
  # Z is an image of size C x rH x rW, where r is the upscaling factor
  Z = reconstruction(F_Z, theta)

  # Compute the total loss and update the network parameters
  loss = L_p(Z, X) + L_per(Z, X)
  theta = theta - learning_rate * gradient(loss, theta)

# Return the super-resolved image Z
return Z

# Define the feature extraction module
def feature_extraction(I, theta):
  # I is an input image of size C x H x W
  # theta is a set of network parameters for this module

  # Apply four convolutional layers with ReLU activation and batch normalization
  # Each layer has a kernel size of 3 x 3, a stride of 1, and a padding of 1
  # The number of output channels for each layer are 64, 64, 128, and 128 respectively
  F = conv(I, theta[0]) # F is a feature map of size 64 x H x W
  F = relu(F)
  F = batch_norm(F)
  F = conv(F, theta[1]) # F is a feature map of size 64 x H x W
  F = relu(F)
  F = batch_norm(F)
  F = conv(F, theta[2]) # F is a feature map of size 128 x H x W
  F = relu(F)
  F = batch_norm(F)
  F = conv(F, theta[3]) # F is a feature map of size 128 x H x W

  # Return the extracted features F
  return F

# Define the internal transformer module
def internal_transformer(F_X, F_Y, theta):
  # F_X and F_Y are feature maps of size C x H x W from different modalities
  # theta is a set of network parameters for this module

  # Compute the self-attention weights between patches of F_X and F_Y using dot product similarity
  # P_X and P_Y are matrices of size N x (C * k * k), where N is the number of patches, k is the patch size, and C is the number of channels
  # A is a matrix of size N x N, where A[i][j] is the similarity between patch i of P_X and patch j of P_Y 
  P_X = patchify(F_X) 
  P_Y = patchify(F_Y)
  A = softmax(P_X @ transpose(P_Y)) 

  # Fuse the features using the attention weights and add positional encodings to preserve spatial information
  # P_Z is a matrix of size N x (C * k * k), where P_Z[i] is a weighted sum of patches from P_Y that are similar to patch i from P_X
  # PE is a matrix of size N x (C * k * k), where PE[i] is a positional encoding vector for patch i
  # F_Z is a feature map of size C x H x W, obtained by reshaping and concatenating P_Z
  P_Z = A @ P_Y
  PE = positional_encoding(N, C * k * k)
  P_Z = P_Z + PE
  F_Z = unpatchify(P_Z)

  # Return the fused features F_Z
  return F_Z

# Define the reconstruction module
def reconstruction(F_Z, theta):
  # F_Z is a feature map of size C x H x W
  # theta is a set of network parameters for this module

  # Apply three sub-pixel convolution layers with ReLU activation and batch normalization
  # Each layer has a kernel size of 3 x 3, a stride of 1, and a padding of 1
  # The number of output channels for each layer are r^2 * C, r^2 * C, and r^2 * C respectively, where r is the upscaling factor
  # The sub-pixel convolution layer rearranges the output channels into r x r spatial blocks and upsamples the feature map by a factor of r
  Z = sub_pixel_conv(F_Z, theta[0]) # Z is an image of size C x rH x rW
  Z = relu(Z)
  Z = batch_norm(Z)
  Z = sub_pixel_conv(Z, theta[1]) # Z is an image of size C x r^2H x r^2W
  Z = relu(Z)
  Z = batch_norm(Z)
  Z = sub_pixel_conv(Z, theta[2]) # Z is an image of size C x r^3H x r^3W

  # Return the super-resolved image Z
  return Z
```