---
title: 2305.01644v1 Key-Locked Rank One Editing for Text-to-Image Personalization
date: 2023-05-02
---

# [Key-Locked Rank One Editing for Text-to-Image Personalization](http://arxiv.org/abs/2305.01644v1)

authors: Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon


## What, Why and How

[1]: https://arxiv.org/abs/2305.01644 "Key-Locked Rank One Editing for Text-to-Image Personalization"
[2]: https://arxiv.org/pdf/2305.01644.pdf "arXiv.org"
[3]: https://arxiv-export3.library.cornell.edu/abs/2305.01644 "[2305.01644] Key-Locked Rank One Editing for Text-to-Image Personalization"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a new method for text-to-image personalization, which allows users to modify the output of a text-to-image model with their own visual concepts.
- **Why**: The paper aims to address the challenges of text-to-image personalization, such as maintaining high visual fidelity, combining multiple concepts, and keeping a small model size.
- **How**: The paper proposes Perfusion, a method that uses dynamic rank-1 updates to the underlying text-to-image model. Perfusion introduces a key-locking mechanism that aligns new concepts with their superordinate category, and a gated rank-1 approach that controls the influence of a learned concept during inference time. Perfusion achieves better results than baselines in terms of visual quality, textual alignment, and model efficiency.


## Main Contributions

[1]: https://arxiv.org/abs/2305.01644 "Key-Locked Rank One Editing for Text-to-Image Personalization"
[2]: https://arxiv.org/pdf/2305.01644.pdf "arXiv.org"
[3]: https://arxiv-export3.library.cornell.edu/abs/2305.01644 "[2305.01644] Key-Locked Rank One Editing for Text-to-Image Personalization"

According to the paper[^1^][1], the main contributions are:

- A novel method for text-to-image personalization that uses dynamic rank-1 updates to the underlying text-to-image model.
- A key-locking mechanism that prevents overfitting and aligns new concepts with their superordinate category.
- A gated rank-1 approach that allows runtime control of the influence of a learned concept and enables combining multiple concepts.
- A comprehensive evaluation that shows the superiority of Perfusion over baselines in terms of visual quality, textual alignment, and model efficiency.


## Method Summary

[1]: https://arxiv.org/abs/2305.01644 "Key-Locked Rank One Editing for Text-to-Image Personalization"
[2]: https://arxiv-export3.library.cornell.edu/abs/2305.01644 "[2305.01644] Key-Locked Rank One Editing for Text-to-Image Personalization"
[3]: https://arxiv.org/pdf/2305.01644v1.pdf "arXiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper builds on a text-to-image model called DALL-E, which uses a transformer architecture to generate images from text descriptions.
- The paper introduces Perfusion, a method that allows users to personalize the output of DALL-E with their own visual concepts, such as colors, shapes, textures, etc.
- Perfusion consists of two main components: a rank-1 update module and a key-locking module.
- The rank-1 update module learns a low-rank matrix that modifies the cross-attention weights of DALL-E based on a given concept and its corresponding image examples. The rank-1 matrix is composed of a query vector and a key vector, which are learned by minimizing the reconstruction loss between the generated image and the image examples.
- The key-locking module prevents overfitting and improves generalization by constraining the key vector to be close to the key vector of its superordinate category in DALL-E. For example, if the concept is "red apple", the key vector is locked to the key vector of "apple" in DALL-E. This ensures that the concept inherits the semantic properties of its category and can be applied to other objects as well.
- The paper also proposes a gated rank-1 approach that allows users to control the influence of a learned concept during inference time by adjusting a scalar gate value. This enables users to combine multiple concepts in a single image and balance between visual fidelity and textual alignment.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a text description t and a set of image examples I for a concept c
# Output: a personalized image x that matches t and c

# Initialize DALL-E as a text-to-image model with cross-attention weights W
# Initialize a rank-1 update module U with query vector q and key vector k
# Initialize a key-locking module L with category key vector k_c

# Train U and L with I and t
for each image i in I:
  # Generate an image x from t and W
  x = DALL-E(t, W)
  # Compute the reconstruction loss between x and i
  loss = MSE(x, i)
  # Update q and k by backpropagating the loss
  q, k = update(q, k, loss)
  # Lock k to k_c by projecting it onto the subspace spanned by k_c
  k = L(k, k_c)

# Generate a personalized image x from t and W + U
x = DALL-E(t, W + U)

# Optionally, adjust the gate value g to control the influence of U
x = DALL-E(t, W + g * U)

# Optionally, combine multiple concepts by adding multiple rank-1 updates
x = DALL-E(t, W + g_1 * U_1 + g_2 * U_2 + ...)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
batch_size = 32 # the number of images per batch
num_epochs = 10 # the number of training epochs
lr = 0.001 # the learning rate for the optimizer
g = 1.0 # the initial gate value for the rank-1 update
alpha = 0.1 # the key-locking coefficient

# Load the DALL-E model and its tokenizer
dalle = torch.hub.load('openai/DALL-E', 'dalle')
tokenizer = torch.hub.load('openai/DALL-E', 'tokenizer')

# Define the rank-1 update module as a subclass of nn.Module
class RankOneUpdate(nn.Module):
  def __init__(self, dim):
    super(RankOneUpdate, self).__init__()
    # Initialize the query vector and the key vector randomly
    self.q = nn.Parameter(torch.randn(dim))
    self.k = nn.Parameter(torch.randn(dim))

  def forward(self, W):
    # Compute the rank-1 update matrix as the outer product of q and k
    U = torch.ger(self.q, self.k)
    # Add the rank-1 update matrix to the original cross-attention weights
    W_new = W + g * U
    return W_new

# Define the key-locking module as a function
def key_locking(k, k_c):
  # Project the key vector onto the subspace spanned by the category key vector
  k_new = k - alpha * (k - k_c) / torch.norm(k - k_c)
  return k_new

# Load the text description and the image examples for a concept
t = "a red apple" # example text description
I = torchvision.datasets.ImageFolder("images/red_apple") # example image folder

# Preprocess the image examples with resizing and normalization
transform = transforms.Compose([
  transforms.Resize((256, 256)), # resize to 256 x 256 pixels
  transforms.ToTensor(), # convert to tensor
  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalize to [-1, 1]
])
I = I.transform(transform)

# Create a data loader for the image examples
data_loader = torch.utils.data.DataLoader(I, batch_size=batch_size, shuffle=True)

# Tokenize the text description and get its embedding from DALL-E
tokens = tokenizer.tokenize(t).unsqueeze(0) # add a batch dimension
text_emb = dalle.text_encoder(tokens) # get the text embedding

# Get the category key vector from DALL-E by using a generic text description
t_c = "an apple" # generic text description for the category
tokens_c = tokenizer.tokenize(t_c).unsqueeze(0) # add a batch dimension
text_emb_c = dalle.text_encoder(tokens_c) # get the text embedding for the category
k_c = text_emb_c[:, -1] # get the category key vector as the last token

# Initialize the rank-1 update module with the same dimension as DALL-E's cross-attention weights
U = RankOneUpdate(dalle.dim)

# Define the optimizer and the loss function
optimizer = optim.Adam(U.parameters(), lr=lr) # use Adam optimizer for U's parameters
criterion = nn.MSELoss() # use mean squared error loss for reconstruction

# Train U and L with I and t for num_epochs epochs
for epoch in range(num_epochs):
  for i, (images, _) in enumerate(data_loader): # loop over batches of images
    # Get the image embeddings from DALL-E's image encoder
    image_emb = dalle.image_encoder(images)
    # Get the original cross-attention weights from DALL-E's transformer encoder-decoder 
    W = dalle.transformer_enc_dec.cross_attn.weight.data 
    # Get the updated cross-attention weights from U's forward pass 
    W_new = U(W)
    # Set DALL-E's transformer encoder-decoder cross-attention weights to W_new 
    dalle.transformer_enc_dec.cross_attn.weight.data.copy_(W_new)
    # Generate images from t and W_new using DALL-E's decoder 
    x_logits = dalle.decoder(text_emb, image_emb) 
    x = torch.sigmoid(x_logits) # apply sigmoid to get values between [0, 1]
    # Compute the reconstruction loss between x and images 
    loss = criterion(x, images)
    # Update U's parameters by backpropagating the loss 
    optimizer.zero_grad() # zero the gradients
    loss.backward() # compute the gradients
    optimizer.step() # update the parameters
    # Lock U's key vector to k_c by using L's function 
    U.k.data.copy_(key_locking(U.k, k_c))
    # Print the loss every 10 batches 
    if (i + 1) % 10 == 0:
      print(f"Epoch {epoch + 1}, Batch {i + 1}, Loss {loss.item():.4f}")

# Generate a personalized image from t and W + U
x = dalle.generate_images(tokens, dalle.transformer_enc_dec.cross_attn.weight.data + U(W))

# Optionally, adjust the gate value g to control the influence of U
g = 0.5 # example gate value
x = dalle.generate_images(tokens, dalle.transformer_enc_dec.cross_attn.weight.data + g * U(W))

# Optionally, combine multiple concepts by adding multiple rank-1 updates
U_1 = RankOneUpdate(dalle.dim) # example rank-1 update for concept 1
U_2 = RankOneUpdate(dalle.dim) # example rank-1 update for concept 2
g_1 = 0.8 # example gate value for concept 1
g_2 = 0.6 # example gate value for concept 2
x = dalle.generate_images(tokens, dalle.transformer_enc_dec.cross_attn.weight.data + g_1 * U_1(W) + g_2 * U_2(W))
```