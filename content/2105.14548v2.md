---
title: 2105.14548v2 Z2P  Instant Visualization of Point Clouds
date: 2021-05-15
---

# [Z2P: Instant Visualization of Point Clouds](http://arxiv.org/abs/2105.14548v2)

authors: Gal Metzer, Rana Hanocka, Raja Giryes, Niloy J. Mitra, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/2105.14548v2.pdf "Z2P: Instant Visualization of Point Clouds - arXiv.org"
[2]: https://arxiv.org/abs/2105.14548 "[2105.14548] Z2P: Instant Visualization of Point Clouds - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/abs/2010.14548v2 "[2010.14548v2] Relatively Complete Verification of Probabilistic Programs"

Here is a summary of the paper:

- **What**: The paper presents a technique for visualizing point clouds using a neural network. Point clouds are a popular and flexible representation of 3D shapes, but they are difficult to render directly. The technique bypasses the need for surface reconstruction or normal estimation, and instead translates point depth-maps into images that look like surfaces.
- **Why**: The paper aims to provide a fast and effective way to preview point clouds, especially those generated by neural networks that do not have consistent normal orientation. The technique can also handle noise, non-uniform sampling, and thin surface sheets, which are common challenges in point cloud processing.
- **How**: The paper casts the preview problem as a conditional image-to-image translation task, and designs a neural network that takes a point depth-map as input and outputs an image. The network consists of an encoder-decoder architecture with skip connections and residual blocks. The output image can be optionally conditioned on simple control variables, such as color and light. The paper evaluates the technique on various point cloud datasets and compares it with existing methods.

## Main Contributions

The paper claims the following contributions:

- A novel technique for instant visualization of point clouds using a neural network that translates point depth-maps into images.
- A conditional image-to-image translation framework that allows for controlling the appearance of the output image using simple variables.
- A comprehensive evaluation of the technique on various point cloud datasets and a comparison with existing methods.

## Method Summary

The method section of the paper describes the following steps:

- The input point cloud is projected onto a 2D plane using a perspective camera model, and a point depth-map is generated by taking the minimum depth value for each pixel.
- The point depth-map is fed into a neural network that consists of an encoder-decoder architecture with skip connections and residual blocks. The encoder extracts features from the input, and the decoder generates an output image. The network is trained using a combination of L1 loss, perceptual loss, and adversarial loss.
- The output image can be conditioned on simple control variables, such as color and light. The color variable is a one-hot vector that indicates the desired color scheme for the output image. The light variable is a 2D vector that specifies the direction of the light source. These variables are concatenated with the encoder features and passed to the decoder.
- The network can handle various challenges in point cloud visualization, such as noise, non-uniform sampling, and thin surface sheets. The network learns to fill in the gaps and smooth out the artifacts in the input point depth-map. The network can also generate realistic shadows and highlights based on the light variable.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: point cloud P, color variable C, light variable L
# Output: image I

# Project point cloud onto 2D plane
D = project(P)

# Encode point depth-map into features
F = encode(D)

# Concatenate features with control variables
F = concat(F, C, L)

# Decode features into image
I = decode(F)

# Return image
return I
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: point cloud P, color variable C, light variable L
# Output: image I

# Define network parameters
num_channels = 3 # RGB channels
num_colors = 4 # number of color schemes
num_lights = 2 # x and y coordinates of light source
image_size = 256 # output image size
num_filters = 64 # number of filters in the first convolution layer
num_blocks = 9 # number of residual blocks in the encoder and decoder
kernel_size = 3 # kernel size for convolution layers
stride = 2 # stride for convolution layers
padding = 1 # padding for convolution layers

# Define network layers
encoder = Encoder(num_channels, num_filters, num_blocks, kernel_size, stride, padding)
decoder = Decoder(num_filters * (2 ** num_blocks), num_channels, num_blocks, kernel_size, stride, padding)
discriminator = Discriminator(num_channels, num_filters, kernel_size, stride, padding)
vgg = VGG19(pretrained=True) # for perceptual loss

# Define loss functions
l1_loss = torch.nn.L1Loss()
perceptual_loss = PerceptualLoss(vgg)
adversarial_loss = AdversarialLoss(discriminator)

# Define optimizer
optimizer = torch.optim.Adam(encoder.parameters() + decoder.parameters(), lr=0.0002)

# Project point cloud onto 2D plane
D = project(P)

# Encode point depth-map into features
F = encoder(D)

# Concatenate features with control variables
C_onehot = one_hot(C, num_colors) # convert color variable to one-hot vector
F = concat(F, C_onehot, L)

# Decode features into image
I = decoder(F)

# Compute losses
L_l1 = l1_loss(I, D) # L1 loss between output image and input depth-map
L_perceptual = perceptual_loss(I, D) # perceptual loss between output image and input depth-map
L_adversarial = adversarial_loss(I) # adversarial loss for output image
L_total = L_l1 + L_perceptual + L_adversarial # total loss

# Update network parameters
optimizer.zero_grad()
L_total.backward()
optimizer.step()

# Return image
return I
```