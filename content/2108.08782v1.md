---
title: 2108.08782v1 Causal Attention for Unbiased Visual Recognition
date: 2021-08-09
---

# [Causal Attention for Unbiased Visual Recognition](http://arxiv.org/abs/2108.08782v1)

authors: Tan Wang, Chang Zhou, Qianru Sun, Hanwang Zhang


## What, Why and How

[1]: https://arxiv.org/abs/2108.08782 "[2108.08782] Causal Attention for Unbiased Visual Recognition - arXiv.org"
[2]: https://arxiv.org/pdf/2108.08782.pdf "Causal Attention for Unbiased Visual Recognition - arXiv.org"
[3]: http://export.arxiv.org/abs/2208.08782v1 "[2208.08782v1] Fair Division of Indivisible Goods: A Survey"

Here is a summary of the paper based on the search results:

- **What**: The paper proposes a causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion. Confounders are factors that trick the attention to capture spurious correlations that harm the prediction when the data are out-of-distribution (OOD). CaaM can be integrated in conventional attention CNN and self-attention Vision Transformer[^1^][1] [^2^][2].
- **Why**: The paper aims to improve the robustness and saliency of attention models in OOD settings, where the training and testing data are not identically and independently distributed (IID). Existing methods rely on causal intervention, which requires additional annotations of the confounders, which are expensive and elusive[^1^][1] [^2^][2].
- **How**: The paper introduces a novel attention mechanism that learns to mask out the confounders by minimizing the mutual information between the attended features and the confounding context. The paper also proposes a self-supervised learning scheme that generates pseudo confounders by randomly cropping and pasting image patches. The paper evaluates CaaM on several OOD benchmarks and shows that it outperforms baseline methods significantly[^1^][1] [^2^][2].

## Main Contributions

According to the paper, the contributions are:

- The paper proposes a novel causal attention module (CaaM) that self-annotates the confounders in unsupervised fashion and learns to mask them out by minimizing the mutual information between the attended features and the confounding context.
- The paper introduces a self-supervised learning scheme that generates pseudo confounders by randomly cropping and pasting image patches, which can be applied to any attention model without extra supervision.
- The paper demonstrates the effectiveness and generality of CaaM on various OOD benchmarks, including NICO [21], ImageNet-C [12], ImageNet-R [13], and ImageNet-O [15]. The paper shows that CaaM significantly improves the robustness and saliency of attention models in OOD settings.

## Method Summary

[1]: https://arxiv.org/abs/2108.08782 "[2108.08782] Causal Attention for Unbiased Visual Recognition - arXiv.org"
[2]: https://arxiv.org/pdf/2104.08782v1.pdf "arXiv:2104.08782v1 [cs.CL] 18 Apr 2021"
[3]: http://export.arxiv.org/abs/2208.08782v1 "[2208.08782v1] Fair Division of Indivisible Goods: A Survey"

Here is a summary of the method section based on the search results:

- The paper introduces a novel attention mechanism that learns to mask out the confounders by minimizing the mutual information between the attended features and the confounding context. The paper defines confounders as the factors that are correlated with both the input and the output, but do not have causal effects on the output[^1^][2].
- The paper proposes a causal attention module (CaaM) that consists of two components: a confounder annotator and a confounder masker. The confounder annotator learns to generate a binary mask that indicates the confounding regions in the input image. The confounder masker learns to suppress the confounding features by applying the mask to the attention map[^1^][2].
- The paper also introduces a self-supervised learning scheme that generates pseudo confounders by randomly cropping and pasting image patches. The paper uses these pseudo confounders to train CaaM in an unsupervised fashion, without requiring any extra annotations or labels[^1^][2].
- The paper integrates CaaM into two types of attention models: conventional attention CNN and self-attention Vision Transformer. The paper shows that CaaM can be easily plugged into existing models without changing their architectures or parameters[^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image x and a label y
# Output: a prediction y_hat and an attention map A

# Define an attention model M (either CNN or ViT)
M = AttentionModel()

# Define a causal attention module CaaM
CaaM = CausalAttentionModule()

# Generate a pseudo confounder x_c by randomly cropping and pasting image patches
x_c = GeneratePseudoConfounder(x)

# Forward pass the original image and the pseudo confounder through the model
y_hat, A = M(x)
y_hat_c, A_c = M(x_c)

# Compute the loss function L using cross entropy and mutual information
L = CrossEntropy(y_hat, y) + CrossEntropy(y_hat_c, y) - MutualInformation(A, A_c)

# Backward propagate the gradients and update the model parameters
L.backward()
M.update()

# Apply CaaM to the attention map A to mask out the confounders
A = CaaM(A)

# Return the prediction and the attention map
return y_hat, A
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the hyperparameters
batch_size = 64
num_epochs = 100
learning_rate = 0.01
lambda = 0.1 # the weight for the mutual information term

# Define the attention model M (either CNN or ViT)
M = AttentionModel()

# Define the causal attention module CaaM
CaaM = CausalAttentionModule()

# Define the optimizer and the loss function
optimizer = torch.optim.Adam(M.parameters(), lr=learning_rate)
criterion = torch.nn.CrossEntropyLoss()

# Load the dataset and create data loaders
dataset = torchvision.datasets.ImageFolder("path/to/dataset")
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Define a function to generate a pseudo confounder by randomly cropping and pasting image patches
def GeneratePseudoConfounder(x):
  # x is a batch of images of shape (batch_size, 3, height, width)
  # return a batch of images with pseudo confounders of the same shape

  # Randomly select a patch size between 10% and 50% of the image size
  patch_size = np.random.randint(0.1 * height, 0.5 * height)

  # Randomly select a patch location for each image in the batch
  patch_x = np.random.randint(0, height - patch_size, size=batch_size)
  patch_y = np.random.randint(0, width - patch_size, size=batch_size)

  # Randomly select a target location for each image in the batch
  target_x = np.random.randint(0, height - patch_size, size=batch_size)
  target_y = np.random.randint(0, width - patch_size, size=batch_size)

  # Copy the image batch to avoid modifying the original data
  x_c = x.clone()

  # For each image in the batch, crop the patch and paste it to the target location
  for i in range(batch_size):
    patch = x[i, :, patch_x[i]:patch_x[i]+patch_size, patch_y[i]:patch_y[i]+patch_size]
    x_c[i, :, target_x[i]:target_x[i]+patch_size, target_y[i]:target_y[i]+patch_size] = patch

  # Return the batch of images with pseudo confounders
  return x_c

# Define a function to compute the mutual information between two attention maps
def MutualInformation(A, A_c):
  # A and A_c are two batches of attention maps of shape (batch_size, num_heads, height, width)
  # return a scalar value representing the mutual information

  # Compute the joint probability distribution p(A, A_c) by histogram binning
  joint_hist = np.histogram2d(A.flatten(), A_c.flatten(), bins=10)[0]

  # Normalize the joint histogram to get the joint probability distribution
  joint_prob = joint_hist / joint_hist.sum()

  # Compute the marginal probability distributions p(A) and p(A_c) by summing over the joint probability distribution
  marginal_prob_A = joint_prob.sum(axis=1)
  marginal_prob_A_c = joint_prob.sum(axis=0)

  # Compute the mutual information I(A; A_c) using the formula: I(A; A_c) = sum(p(A, A_c) * log(p(A, A_c) / (p(A) * p(A_c))))
  MI = 0
  for i in range(10):
    for j in range(10):
      if joint_prob[i,j] > 0:
        MI += joint_prob[i,j] * np.log(joint_prob[i,j] / (marginal_prob_A[i] * marginal_prob_A_c[j]))

  # Return the mutual information
  return MI

# Train the model for a fixed number of epochs
for epoch in range(num_epochs):

  # Loop over the batches of data
  for x, y in train_loader:

    # Generate a pseudo confounder x_c by randomly cropping and pasting image patches
    x_c = GeneratePseudoConfounder(x)

    # Forward pass the original image and the pseudo confounder through the model
    y_hat, A = M(x)
    y_hat_c, A_c = M(x_c)

    # Compute the loss function L using cross entropy and mutual information
    L_ce = criterion(y_hat, y) + criterion(y_hat_c, y)
    L_mi = MutualInformation(A, A_c)
    L = L_ce - lambda * L_mi

    # Backward propagate the gradients and update the model parameters
    optimizer.zero_grad()
    L.backward()
    optimizer.step()

    # Apply CaaM to the attention map A to mask out the confounders
    A = CaaM(A)

  # Print the epoch number and the loss value
  print(f"Epoch {epoch}, Loss {L.item()}")
```