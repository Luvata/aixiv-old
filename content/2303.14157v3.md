---
title: 2303.14157v3 Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis
date: 2023-03-15
---

# [Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis](http://arxiv.org/abs/2303.14157v3)

authors: Thuan Hoang Nguyen, Thanh Van Le, Anh Tran


## What, Why and How

[1]: https://arxiv.org/abs/2303.14157 "[2303.14157] Efficient Scale-Invariant Generator with Column-Row ..."
[2]: https://arxiv.org/pdf/2303.14157.pdf "Efficient Scale-Invariant Generator with Column-Row Entangled Pixel ..."
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2303.14157v3 "[2303.14157v3] Efficient Scale-Invariant Generator with Column-Row ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new generative model called **Column-Row Entangled Pixel Synthesis (CREPS)** that can synthesize photo-realistic images at any arbitrary resolution without using any spatial convolutions or coarse-to-fine design.
- **Why**: The paper aims to address the limitations of existing GAN-based and INR-based solutions for any-scale image synthesis, which suffer from inconsistency, texture sticking, huge memory footprint, and slow inference when scaling the output resolution.
- **How**: The paper employs a novel bi-line representation that decomposes layer-wise feature maps into separate "thick" column and row encodings. The paper also introduces a column-row entangled pixel synthesis module that can generate pixels by combining the column and row encodings. The paper evaluates the proposed model on various datasets and shows its ability to produce scale-consistent and alias-free images with proper training and inference speed.



## Main Contributions

[1]: https://arxiv.org/abs/2303.14157 "[2303.14157] Efficient Scale-Invariant Generator with Column-Row ..."
[2]: https://arxiv.org/pdf/2303.14157.pdf "Efficient Scale-Invariant Generator with Column-Row Entangled Pixel ..."
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2303.14157v3 "[2303.14157v3] Efficient Scale-Invariant Generator with Column-Row ..."

According to the paper[^1^][1], the main contributions are:

- A new generative model called **CREPS** that can synthesize photo-realistic images at any arbitrary resolution without using any spatial convolutions or coarse-to-fine design.
- A novel bi-line representation that decomposes layer-wise feature maps into separate "thick" column and row encodings, which reduces the memory footprint and makes the system scalable.
- A column-row entangled pixel synthesis module that can generate pixels by combining the column and row encodings, which ensures scale-equivariance and avoids aliasing artifacts.
- Extensive experiments on various datasets that demonstrate the effectiveness and efficiency of the proposed model compared with existing methods.


## Method Summary

[1]: https://arxiv.org/abs/2303.14157 "[2303.14157] Efficient Scale-Invariant Generator with Column-Row ..."
[2]: https://arxiv.org/pdf/2303.14157.pdf "Efficient Scale-Invariant Generator with Column-Row Entangled Pixel ..."
[3]: https://arxiv-export-lb.library.cornell.edu/abs/2303.14157v3 "[2303.14157v3] Efficient Scale-Invariant Generator with Column-Row ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a new generative model called **CREPS** that consists of three main components: a style encoder, a bi-line generator, and a column-row entangled pixel synthesis module.
- The style encoder takes a latent code as input and produces a style vector that controls the appearance of the output image.
- The bi-line generator takes the style vector as input and generates two sets of feature maps: one for the column encoding and one for the row encoding. The column encoding captures the vertical information of the image, while the row encoding captures the horizontal information. The paper uses a novel bi-line representation that decomposes each feature map into separate "thick" column and row encodings, which reduces the memory footprint and makes the system scalable.
- The column-row entangled pixel synthesis module takes the column and row encodings as input and generates pixels by combining them using a bilinear interpolation. The paper shows that this module ensures scale-equivariance and avoids aliasing artifacts by design.
- The paper also introduces a scale-aware discriminator that can distinguish between real and fake images at different scales. The paper uses a multi-scale hinge loss to train the generator and the discriminator in an adversarial manner.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the generator G, the discriminator D, and the hinge loss function L
G = CREPS()
D = ScaleAwareDiscriminator()
L = MultiScaleHingeLoss()

# Define the latent code z, the style vector s, and the output resolution r
z = sample_from_normal_distribution()
s = G.style_encoder(z)
r = choose_any_resolution()

# Generate the column and row encodings using the bi-line generator
c, r = G.bi_line_generator(s)

# Generate the output image using the column-row entangled pixel synthesis module
x = G.pixel_synthesis(c, r)

# Compute the discriminator scores for the real and fake images
y_real = D(x_real, r)
y_fake = D(x, r)

# Compute the generator and discriminator losses using the hinge loss function
loss_G = L(y_fake, True)
loss_D = L(y_real, True) + L(y_fake, False)

# Update the generator and discriminator parameters using gradient descent
G.update_parameters(loss_G)
D.update_parameters(loss_D)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

# Define the hyperparameters
num_layers = 8 # number of layers in the bi-line generator
num_channels = 512 # number of channels in each layer
num_styles = 512 # dimension of the style vector
num_classes = 1000 # number of classes for conditional generation
lambda_adv = 1.0 # weight for the adversarial loss
lambda_fm = 10.0 # weight for the feature matching loss
lambda_cls = 1.0 # weight for the conditional loss
lr = 0.0002 # learning rate
beta1 = 0.5 # beta1 for Adam optimizer
beta2 = 0.999 # beta2 for Adam optimizer

# Define the style encoder network
class StyleEncoder(nn.Module):
    def __init__(self, num_styles, num_classes):
        super(StyleEncoder, self).__init__()
        # Define the convolutional layers with leaky ReLU activation and instance normalization
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3)
        self.in1 = nn.InstanceNorm2d(64)
        self.lrelu1 = nn.LeakyReLU(0.2)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
        self.in2 = nn.InstanceNorm2d(128)
        self.lrelu2 = nn.LeakyReLU(0.2)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)
        self.in3 = nn.InstanceNorm2d(256)
        self.lrelu3 = nn.LeakyReLU(0.2)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)
        self.in4 = nn.InstanceNorm2d(512)
        self.lrelu4 = nn.LeakyReLU(0.2)
        # Define the fully connected layer for style vector output
        self.fc_style = nn.Linear(512 * 4 * 4, num_styles)
        # Define the fully connected layer for class label output (optional)
        self.fc_class = nn.Linear(512 * 4 * 4, num_classes)

    def forward(self, x):
        # Forward pass through the convolutional layers
        x = self.lrelu1(self.in1(self.conv1(x)))
        x = self.lrelu2(self.in2(self.conv2(x)))
        x = self.lrelu3(self.in3(self.conv3(x)))
        x = self.lrelu4(self.in4(self.conv4(x)))
        # Flatten the feature map
        x = x.view(x.size(0), -1)
        # Forward pass through the fully connected layers
        s = self.fc_style(x) # style vector output
        c = self.fc_class(x) # class label output (optional)
        return s, c

# Define the bi-line generator network
class BiLineGenerator(nn.Module):
    def __init__(self, num_layers, num_channels, num_styles):
        super(BiLineGenerator, self).__init__()
        # Define the style modulation layers with learnable affine transformation parameters
        self.style_mods = nn.ModuleList()
        for i in range(num_layers):
            self.style_mods.append(StyleMod(num_channels, num_styles))
        # Define the bi-line layers with column and row encodings
        self.bi_lines = nn.ModuleList()
        for i in range(num_layers):
            self.bi_lines.append(BiLine(num_channels))

    def forward(self, s):
        # Initialize the column and row encodings with zeros
        c = torch.zeros(s.size(0), num_channels, 1).to(s.device) # column encoding
        r = torch.zeros(s.size(0), num_channels).to(s.device) # row encoding
        # Forward pass through the bi-line layers with style modulation
        for i in range(num_layers):
            c, r = self.bi_lines[i](c, r) # update column and row encodings
            c, r = self.style_mods[i](c, r, s) # apply style modulation to column and row encodings
        return c, r

# Define the style modulation layer
class StyleMod(nn.Module):
    def __init__(self, num_channels, num_styles):
        super(StyleMod, self).__init__()
        # Define the fully connected layer to produce the affine transformation parameters
        self.fc = nn.Linear(num_styles, num_channels * 2)
        # Initialize the parameters with ones and zeros
        self.fc.weight.data.fill_(0.0)
        self.fc.bias.data.fill_(1.0)

    def forward(self, c, r, s):
        # Forward pass through the fully connected layer
        y = self.fc(s) # shape: (batch_size, num_channels * 2)
        # Reshape the output to get the scale and bias parameters
        y = y.view(y.size(0), 2, y.size(1) // 2, 1) # shape: (batch_size, 2, num_channels, 1)
        scale = y[:, 0] # shape: (batch_size, num_channels, 1)
        bias = y[:, 1] # shape: (batch_size, num_channels, 1)
        # Apply the affine transformation to the column and row encodings
        c = c * scale + bias # shape: (batch_size, num_channels, 1)
        r = r * scale.squeeze(-1) + bias.squeeze(-1) # shape: (batch_size, num_channels)
        return c, r

# Define the bi-line layer
class BiLine(nn.Module):
    def __init__(self, num_channels):
        super(BiLine, self).__init__()
        # Define the linear layers for column and row encodings
        self.linear_c = nn.Linear(num_channels, num_channels)
        self.linear_r = nn.Linear(num_channels, num_channels)

    def forward(self, c, r):
        # Forward pass through the linear layers
        c_new = self.linear_c(c) # shape: (batch_size, num_channels, 1)
        r_new = self.linear_r(r) # shape: (batch_size, num_channels)
        # Add the column and row encodings element-wise
        c_new = c_new + r.unsqueeze(-1) # shape: (batch_size, num_channels, 1)
        r_new = r_new + c.squeeze(-1) # shape: (batch_size, num_channels)
        return c_new, r_new

# Define the column-row entangled pixel synthesis module
class PixelSynthesis(nn.Module):
    def __init__(self):
        super(PixelSynthesis, self).__init__()

    def forward(self, c, r):
        # Get the batch size and number of channels
        batch_size = c.size(0)
        num_channels = c.size(1)
        # Get the output resolution from the column and row encodings
        height = c.size(2) # height is the length of column encoding
        width = r.size(1) // num_channels # width is the length of row encoding divided by number of channels
        # Reshape the row encoding to match the column encoding
        r = r.view(batch_size, num_channels, width) # shape: (batch_size, num_channels, width)
        # Repeat the column and row encodings along different dimensions
        c = c.repeat(1, 1, width) # shape: (batch_size, num_channels, height * width)
        r = r.repeat(1, height, 1) # shape: (batch_size, num_channels * height, width)
        # Transpose the row encoding to align with the column encoding
        r = r.transpose(1, 2) # shape: (batch_size, width, num_channels * height)
        # Reshape the row encoding to match the column encoding
        r = r.view(batch_size, num_channels, height * width) # shape: (batch_size, num_channels, height * width)
        # Apply bilinear interpolation to combine the column and row encodings
        x = F.bilinear(c.permute(0 ,2 ,1), r.permute(0 ,2 ,1), torch.ones(batch_size ,height ,width).to(c.device)) 
# shape: (batch_size ,height ,width)
# Transpose the output image to match the channel dimension
x = x.transpose(1 ,2) 
# shape: (batch_size ,width ,height)
# Reshape the output image to have three channels
x = x.view(batch_size ,3 ,-1 ,height) 
# shape: (batch_size ,3 ,width ,height)
return x

# Define the scale-aware discriminator network
class ScaleAwareDiscriminator(nn.Module):
    def __init__(self ,num_classes):
super(ScaleAwareDiscriminator ,self).__init__()
# Define a list of scales to use for multi-scale discrimination
self.scales = [4 ,8 ,16]
# Define a dictionary of convolutional blocks for each scale
self.blocks = nn.ModuleDict()
for scale in self.scales:
self.blocks[str(scale)] = ConvBlock(scale)
# Define a