---
title: 2306.00424v1 End-to-end Knowledge Retrieval with Multi-modal Queries
date: 2023-06-01
---

# [End-to-end Knowledge Retrieval with Multi-modal Queries](http://arxiv.org/abs/2306.00424v1)

authors: Man Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, Chitta Baral


## What, Why and How

[1]: https://arxiv.org/pdf/2306.00424v1.pdf "End-to-end Knowledge Retrieval with Multi-modal Queries - arXiv.org"
[2]: https://arxiv.org/pdf/2305.00424v1.pdf "PolicyIterationReinforcementLearningMethodfor Continuous-timeMean ..."
[3]: http://export.arxiv.org/abs/2306.00424 "[2306.00424] End-to-end Knowledge Retrieval with Multi-modal Queries"

Here is a summary of the paper:

- **What**: The paper introduces a new task and dataset for knowledge retrieval with multi-modal queries, i.e. queries that contain both text and image inputs. The paper also proposes a new model called ReViz that can directly process text and image inputs and retrieve relevant knowledge from a large corpus in an end-to-end fashion.
- **Why**: The paper aims to address the challenge of retrieving knowledge that requires integrating information from both text and image inputs, which is different from previous work on cross-modal retrieval that focuses on matching text and image representations. The paper argues that this task is more realistic and useful for human-like communication and learning.
- **How**: The paper curates a new dataset called ReMuQ (Retrieval with Multimodal Queries) that contains 10,000 multimodal queries and 100,000 knowledge snippets from Wikipedia. The paper also introduces a new model called ReViz (Retrieval with Vision) that uses a transformer-based architecture to encode text and image inputs and retrieve relevant knowledge snippets from the corpus. The paper introduces a new pretraining task that is effective for learning knowledge retrieval with multimodal queries and also improves performance on downstream tasks. The paper demonstrates superior performance of ReViz on ReMuQ and another dataset (OK-VQA) under zero-shot settings as well as further improvements when finetuned on these datasets.

## Main Contributions

The paper claims to make the following contributions:

- It introduces a new task and dataset for knowledge retrieval with multimodal queries, which is more challenging and realistic than previous cross-modal retrieval tasks.
- It proposes a new model ReViz that can directly process text and image inputs and retrieve relevant knowledge from a large corpus in an end-to-end fashion without relying on intermediate modules such as object detectors or caption generators.
- It introduces a new pretraining task that is effective for learning knowledge retrieval with multimodal queries and also improves performance on downstream tasks.
- It demonstrates superior performance of ReViz on ReMuQ and OK-VQA datasets under zero-shot settings as well as further improvements when finetuned on these datasets.

## Method Summary

[1]: https://arxiv.org/pdf/2306.00424v1.pdf "End-to-end Knowledge Retrieval with Multi-modal Queries - arXiv.org"
[2]: https://arxiv.org/abs/2304.00424 "[2304.00424] Progressive Random Convolutions for Single Domain ..."
[3]: http://export.arxiv.org/abs/2304.00424v1 "[2304.00424v1] Progressive Random Convolutions for Single Domain ..."

Here is a summary of the method section of the paper:

- The paper describes the proposed model ReViz (Retrieval with Vision) that consists of three main components: a multimodal encoder, a knowledge retriever, and a knowledge decoder.
- The multimodal encoder uses a transformer-based architecture to encode text and image inputs into a joint representation. The encoder uses a cross-attention mechanism to fuse information from both modalities and a self-attention mechanism to capture intra-modal dependencies.
- The knowledge retriever uses a dot-product attention mechanism to compute the similarity scores between the multimodal query representation and the knowledge snippets from the corpus. The retriever selects the top-k most relevant snippets as candidates for the final answer.
- The knowledge decoder uses another transformer-based architecture to decode the selected knowledge snippets into a natural language answer. The decoder uses a cross-attention mechanism to attend to both the multimodal query representation and the knowledge snippets, and a self-attention mechanism to generate coherent and fluent sentences.
- The paper introduces a new pretraining task for ReViz that involves masking some tokens in the text input, some regions in the image input, and some words in the knowledge snippet, and then reconstructing them using the multimodal encoder and the knowledge decoder. The paper argues that this pretraining task can help ReViz learn generalizable visual and linguistic representations and improve its retrieval and generation abilities.

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Define the hyperparameters
num_layers = 12 # number of transformer layers for encoder and decoder
hidden_size = 768 # hidden size of transformer
num_heads = 12 # number of attention heads
dropout_rate = 0.1 # dropout rate for transformer
vocab_size = 30522 # vocabulary size for text input and output
image_size = 224 # image size for image input
image_channels = 3 # number of image channels
image_embed_size = 2048 # image embedding size
knowledge_size = 100000 # number of knowledge snippets in the corpus
knowledge_length = 128 # maximum length of knowledge snippets
top_k = 10 # number of top-k knowledge snippets to select
batch_size = 32 # batch size for training and inference
learning_rate = 1e-4 # learning rate for optimizer

# Define the model components
multimodal_encoder = TransformerEncoder(num_layers, hidden_size, num_heads, dropout_rate) # multimodal encoder
knowledge_retriever = DotProductAttention(hidden_size, knowledge_size) # knowledge retriever
knowledge_decoder = TransformerDecoder(num_layers, hidden_size, num_heads, dropout_rate, vocab_size) # knowledge decoder

# Define the loss function and optimizer
criterion = CrossEntropyLoss() # cross entropy loss for text reconstruction and answer generation
optimizer = AdamW(model.parameters(), lr=learning_rate) # AdamW optimizer

# Define the pretraining and finetuning datasets
pretrain_dataset = ReMuQDataset(mode="pretrain") # pretraining dataset with masked text, image and knowledge inputs
finetune_dataset = ReMuQDataset(mode="finetune") # finetuning dataset with multimodal queries and answers

# Define the pretraining and finetuning dataloaders
pretrain_dataloader = DataLoader(pretrain_dataset, batch_size=batch_size, shuffle=True) # pretraining dataloader
finetune_dataloader = DataLoader(finetune_dataset, batch_size=batch_size, shuffle=True) # finetuning dataloader

# Define the pretraining and finetuning loops
def pretrain():
  # Set the model to training mode
  model.train()
  # Loop over the pretraining batches
  for batch in pretrain_dataloader:
    # Get the inputs and outputs from the batch
    text_input, text_mask, text_output = batch["text_input"], batch["text_mask"], batch["text_output"] # text input, mask and output
    image_input, image_mask, image_output = batch["image_input"], batch["image_mask"], batch["image_output"] # image input, mask and output
    knowledge_input, knowledge_mask, knowledge_output = batch["knowledge_input"], batch["knowledge_mask"], batch["knowledge_output"] # knowledge input, mask and output
    
    # Forward pass through the multimodal encoder
    multimodal_query = multimodal_encoder(text_input, text_mask, image_input, image_mask) # multimodal query representation
    
    # Forward pass through the knowledge decoder for text reconstruction
    text_logits = knowledge_decoder(multimodal_query, text_mask, text_output) # text logits
    
    # Forward pass through the knowledge decoder for image reconstruction
    image_logits = knowledge_decoder(multimodal_query, image_mask, image_output) # image logits
    
    # Forward pass through the knowledge decoder for knowledge reconstruction
    knowledge_logits = knowledge_decoder(multimodal_query, knowledge_mask, knowledge_output) # knowledge logits
    
    # Compute the loss for text reconstruction
    text_loss = criterion(text_logits, text_output) # text loss
    
    # Compute the loss for image reconstruction
    image_loss = criterion(image_logits, image_output) # image loss
    
    # Compute the loss for knowledge reconstruction
    knowledge_loss = criterion(knowledge_logits, knowledge_output) # knowledge loss
    
    # Compute the total loss as the sum of individual losses
    total_loss = text_loss + image_loss + knowledge_loss # total loss
    
    # Backward pass and update the parameters
    optimizer.zero_grad() # zero the gradients
    total_loss.backward() # compute the gradients
    optimizer.step() # update the parameters
    
    # Print the loss every n steps

def finetune():
  # Set the model to training mode
  model.train()
  # Loop over the finetuning batches
  for batch in finetune_dataloader:
    # Get the inputs and outputs from the batch
    text_input, text_mask = batch["text_input"], batch["text_mask"] # text input and mask
    image_input, image_mask = batch["image_input"], batch["image_mask"] # image input and mask
    answer_output = batch["answer_output"] # answer output
    
    # Forward pass through the multimodal encoder
    multimodal_query = multimodal_encoder(text_input, text_mask, image_input, image_mask) # multimodal query representation
    
    # Forward pass through the knowledge retriever
    knowledge_scores, knowledge_indices = knowledge_retriever(multimodal_query) # knowledge scores and indices
    
    # Select the top-k knowledge snippets from the corpus
    knowledge_input, knowledge_mask = select_knowledge(knowledge_indices, top_k) # knowledge input and mask
    
    # Forward pass through the knowledge decoder for answer generation
    answer_logits = knowledge_decoder(multimodal_query, knowledge_mask, answer_output) # answer logits
    
    # Compute the loss for answer generation
    answer_loss = criterion(answer_logits, answer_output) # answer loss
    
    # Backward pass and update the parameters
    optimizer.zero_grad() # zero the gradients
    answer_loss.backward() # compute the gradients
    optimizer.step() # update the parameters
    
    # Print the loss every n steps

# Pretrain the model
pretrain()

# Finetune the model
finetune()
```