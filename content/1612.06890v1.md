---
title: 1612.06890v1 CLEVR  A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning
date: 2016-12-07
---

# [CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](http://arxiv.org/abs/1612.06890v1)

authors: Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick


## What, Why and How

[1]: https://arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."
[2]: https://arxiv.org/pdf/1612.06890v1.pdf "arXiv:1612.06890v1 [cs.CV] 20 Dec 2016"
[3]: http://export.arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."

Here is a summary of the paper:

- **What**: The paper introduces CLEVR, a diagnostic dataset for compositional language and elementary visual reasoning. CLEVR contains images of 3D-rendered objects and questions about these images that require different types of visual reasoning skills to answer.
- **Why**: The paper aims to address the limitations of existing visual question answering (VQA) datasets, which have strong biases that models can exploit without reasoning, and which conflate multiple sources of error, making it hard to pinpoint model weaknesses. The paper also aims to provide a detailed analysis of various modern visual reasoning systems using CLEVR, and to reveal their abilities and limitations.
- **How**: The paper describes how CLEVR was created using a programmatic approach that allows for precise control over the complexity and compositionality of the questions, and how it was annotated with functional programs that specify the reasoning steps needed to answer each question. The paper also presents experiments with several VQA models on CLEVR, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), stacked attention networks (SANs), relational networks (RNs), and neural module networks (NMNs). The paper evaluates these models using accuracy, consistency, and validity metrics, and analyzes their performance across different question categories and reasoning types.

## Main Contributions

[1]: https://arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."
[2]: https://arxiv.org/pdf/1612.06890v1.pdf "arXiv:1612.06890v1 [cs.CV] 20 Dec 2016"
[3]: http://export.arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."

According to the paper[^1^][1], the main contributions are:

- **A new diagnostic dataset for VQA**: The paper introduces CLEVR, a large-scale dataset that contains images of 3D-rendered objects and questions about these images that require different types of visual reasoning abilities to answer. CLEVR has minimal biases and detailed annotations that describe the kind of reasoning each question requires.
- **A programmatic approach to question generation**: The paper describes how CLEVR was created using a programmatic approach that allows for precise control over the complexity and compositionality of the questions, and how it enables the generation of functional programs that specify the reasoning steps needed to answer each question.
- **A comprehensive analysis of visual reasoning systems**: The paper presents experiments with several VQA models on CLEVR, including CNNs, RNNs, SANs, RNs, and NMNs. The paper evaluates these models using accuracy, consistency, and validity metrics, and analyzes their performance across different question categories and reasoning types. The paper also provides novel insights into the abilities and limitations of these models, and highlights the challenges and opportunities for future research on visual reasoning.

## Method Summary

[1]: https://arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."
[2]: https://arxiv.org/pdf/1612.06890v1.pdf "arXiv:1612.06890v1 [cs.CV] 20 Dec 2016"
[3]: http://export.arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."

Here is a summary of the method section of the paper:

- **Dataset creation**: The paper describes how CLEVR was created using a programmatic approach that involves three steps: (1) generating synthetic scenes of 3D-rendered objects with different shapes, sizes, colors, and materials; (2) rendering images of these scenes from different viewpoints and lighting conditions; and (3) generating questions about these images using a grammar that defines templates for different question types and reasoning skills.
- **Dataset annotation**: The paper describes how CLEVR was annotated with functional programs that specify the reasoning steps needed to answer each question. These programs are composed of modular functions that correspond to different reasoning skills, such as filtering objects by attributes, counting objects, comparing attributes, etc. The paper also provides statistics on the distribution of question types, reasoning types, and program lengths in CLEVR.
- **Model evaluation**: The paper describes how various VQA models were evaluated on CLEVR using accuracy, consistency, and validity metrics. Accuracy measures the percentage of questions that a model answers correctly. Consistency measures the agreement between a model's answers to different questions about the same image. Validity measures the percentage of answers that are valid for a given question type. The paper also analyzes the performance of different models across different question categories and reasoning types, and compares them with human performance.

## Pseudo Code - High level

[1]: https://arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."
[2]: https://arxiv.org/pdf/1612.06890v1.pdf "arXiv:1612.06890v1 [cs.CV] 20 Dec 2016"
[3]: http://export.arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."

Here is the high-level pseudo code for the paper:

- **Dataset creation**:
  - For each scene:
    - Sample a random number of objects with different shapes, sizes, colors, and materials
    - Place the objects in the scene without overlapping
    - Render the scene from a random viewpoint and lighting condition
    - Save the image and the scene information
  - For each image:
    - For each question type:
      - Sample a question template from a grammar that defines the question structure and reasoning skills
      - Fill in the template with attributes and relations from the scene information
      - Generate a functional program that corresponds to the question template and the scene information
      - Execute the program on the scene information to get the answer
      - Save the question, the answer, and the program
- **Dataset annotation**:
  - For each question:
    - Parse the question into a list of tokens
    - Parse the program into a list of functions and arguments
    - Annotate each token with its part-of-speech tag, dependency relation, and program alignment
    - Annotate each function with its input type, output type, and question alignment
    - Save the annotations
- **Model evaluation**:
  - For each model:
    - Train the model on a subset of CLEVR images and questions
    - Test the model on a different subset of CLEVR images and questions
    - Compute the accuracy, consistency, and validity metrics for the model
    - Analyze the performance of the model across different question categories and reasoning types
    - Compare the performance of the model with human performance and other models

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."
[2]: https://arxiv.org/pdf/1612.06890v1.pdf "arXiv:1612.06890v1 [cs.CV] 20 Dec 2016"
[3]: http://export.arxiv.org/abs/1612.06890 "[1612.06890] CLEVR: A Diagnostic Dataset for Compositional Language and ..."

Here is the detailed pseudo code to implement this paper:

- **Dataset creation**:
  - Define a set of object attributes (shape, size, color, material) and their possible values
  - Define a set of spatial relations (left, right, front, behind) and their corresponding functions
  - Define a grammar that specifies the structure and vocabulary of different question types and reasoning skills
  - Initialize an empty list of scenes
  - For each scene:
    - Initialize an empty list of objects
    - Sample a random number of objects (N) between 3 and 10
    - For each object:
      - Sample a random shape, size, color, and material from the attribute values
      - Sample a random position (x, y, z) in the scene
      - Check if the object overlaps with any existing object in the scene
      - If yes, resample a new position until there is no overlap
      - Add the object to the list of objects
    - Render the scene from a random viewpoint and lighting condition using Blender
    - Save the image and the scene information (objects, attributes, positions) as a JSON file
    - Add the scene to the list of scenes
  - Initialize an empty list of questions
  - For each image:
    - Load the scene information from the JSON file
    - For each question type:
      - Sample a question template from the grammar that defines the question structure and reasoning skills
      - Fill in the template with attributes and relations from the scene information using placeholders
      - Generate a functional program that corresponds to the question template and the scene information using placeholders
      - Replace the placeholders with actual values from the scene information using random sampling or filtering functions
      - Execute the program on the scene information to get the answer
      - Save the question, the answer, and the program as a JSON file
      - Add the question to the list of questions
- **Dataset annotation**:
  - For each question:
    - Load the question, the answer, and the program from the JSON file
    - Parse the question into a list of tokens using spaCy
    - Parse the program into a list of functions and arguments using a custom parser
    - Annotate each token with its part-of-speech tag, dependency relation, and program alignment using spaCy and heuristic rules
    - Annotate each function with its input type, output type, and question alignment using heuristic rules and a custom type system
    - Save the annotations as a JSON file
- **Model evaluation**:
  - Define a set of models to evaluate (CNNs, RNNs, SANs, RNs, NMNs)
  - Define a set of metrics to evaluate (accuracy, consistency, validity)
  - Split CLEVR into train, val, and test subsets using predefined splits
  - For each model:
    - Train the model on CLEVR train subset using PyTorch or TensorFlow
    - Test the model on CLEVR val and test subsets using PyTorch or TensorFlow
    - Compute the accuracy metric for each question by comparing the model's answer with the ground truth answer
    - Compute the consistency metric for each image by comparing the model's answers to different questions about the same image
    - Compute the validity metric for each question type by checking if the model's answer is valid for that question type
    - Analyze the performance of the model across different question categories and reasoning types using accuracy metric
    - Compare the performance of the model with human performance and other models using all metrics