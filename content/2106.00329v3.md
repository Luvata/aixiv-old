---
title: 2106.00329v3 Consistent Two-Flow Network for Tele-Registration of Point Clouds
date: 2021-06-01
---

# [Consistent Two-Flow Network for Tele-Registration of Point Clouds](http://arxiv.org/abs/2106.00329v3)

authors: Zihao Yan, Zimu Yi, Ruizhen Hu, Niloy J. Mitra, Daniel Cohen-Or, Hui Huang


## What, Why and How

[1]: https://arxiv.org/abs/2106.00329 "Consistent Two-Flow Network for Tele-Registration of Point Clouds"
[2]: https://arxiv.org/pdf/2106.00329v3.pdf "arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2106.00329v3 "[2106.00329v3] Consistent Two-Flow Network for Tele-Registration of ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a learning-based technique for tele-registration of point clouds, which is the problem of registering two partial point clouds that have little or no overlap.
- **Why**: Tele-registration is a challenging and important problem in computer graphics and other fields, as it enables the completion and reconstruction of 3D shapes from partial scans. Existing methods struggle or fail when the overlap between the scans is small or nonexistent.
- **How**: The paper proposes a novel neural network design that combines registration and completion tasks in a way that reinforces each other. The network consists of two coupled flows: one that register-and-complete, and one that complete-and-register. The network learns a prior of a class of shapes and can complete a partial shape. The network is trained to produce consistent results between the two flows, which leads to robust and reliable tele-registration and better point cloud prediction. The paper demonstrates the performance of the network on synthetic and real-world data, and shows that it outperforms state-of-the-art methods in both completion and registration.

## Main Contributions

According to the paper, the main contributions are:

- A novel neural network design that combines registration and completion tasks in a way that reinforces each other, and produces consistent results between two coupled flows.
- A new loss function that encourages the consistency between the two flows and penalizes the deviation from the ground truth shape.
- A comprehensive evaluation of the network on synthetic and real-world data, showing that it outperforms state-of-the-art methods in both completion and registration, and can handle challenging cases of tele-registration with little or no overlap.

## Method Summary

Here is a summary of the method section of the paper:

- The paper proposes a consistent two-flow network (CTFNet) that consists of two coupled flows: one that register-and-complete (R&C), and one that complete-and-register (C&R).
- The R&C flow takes two partial point clouds as input and outputs a registered point cloud and a completed point cloud. The C&R flow takes the same input but outputs a completed point cloud and a registered point cloud. The two flows share the same encoder and decoder modules, but have different alignment modules.
- The encoder module encodes each partial point cloud into a latent feature vector. The decoder module decodes the feature vector into a dense point cloud. The alignment module aligns the partial point clouds or the dense point clouds using a differentiable iterative closest point (ICP) algorithm.
- The network is trained with a loss function that consists of three terms: a completion loss, a registration loss, and a consistency loss. The completion loss measures the distance between the completed point clouds and the ground truth shapes. The registration loss measures the distance between the registered point clouds and the ground truth poses. The consistency loss measures the distance between the outputs of the two flows and encourages them to be identical.
- The paper also introduces a data augmentation technique that randomly rotates and translates the partial point clouds to simulate different scanning poses and increase the diversity of the training data.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder, decoder and alignment modules
encoder = Encoder()
decoder = Decoder()
aligner = Aligner()

# Define the loss function
def loss_function(X1, X2, Y1, Y2, Z1, Z2):
  # X1 and X2 are the input partial point clouds
  # Y1 and Y2 are the output registered point clouds
  # Z1 and Z2 are the output completed point clouds
  completion_loss = chamfer_distance(Z1, Z2) + chamfer_distance(Z1, GT) + chamfer_distance(Z2, GT)
  registration_loss = chamfer_distance(Y1, Y2) + chamfer_distance(Y1, GT) + chamfer_distance(Y2, GT)
  consistency_loss = chamfer_distance(Y1, Z2) + chamfer_distance(Y2, Z1)
  return completion_loss + registration_loss + consistency_loss

# Define the R&C flow
def register_and_complete(X1, X2):
  # Encode the input point clouds
  F1 = encoder(X1)
  F2 = encoder(X2)
  # Align the input point clouds
  Y1, Y2 = aligner(X1, X2)
  # Decode the aligned point clouds
  Z1 = decoder(F1)
  Z2 = decoder(F2)
  return Y1, Y2, Z1, Z2

# Define the C&R flow
def complete_and_register(X1, X2):
  # Encode the input point clouds
  F1 = encoder(X1)
  F2 = encoder(X2)
  # Decode the input point clouds
  Z1 = decoder(F1)
  Z2 = decoder(F2)
  # Align the decoded point clouds
  Y1, Y2 = aligner(Z1, Z2)
  return Y1, Y2, Z1, Z2

# Train the network
for epoch in epochs:
  for batch in batches:
    # Get the input partial point clouds
    X1, X2 = batch
    # Apply data augmentation
    X1, X2 = augment(X1, X2)
    # Run the R&C flow
    Y1_RC, Y2_RC, Z1_RC, Z2_RC = register_and_complete(X1, X2)
    # Run the C&R flow
    Y1_CR, Y2_CR, Z1_CR, Z2_CR = complete_and_register(X1, X2)
    # Compute the loss
    loss_RC = loss_function(X1, X2, Y1_RC, Y2_RC, Z1_RC, Z2_RC)
    loss_CR = loss_function(X1, X2, Y1_CR, Y2_CR, Z1_CR, Z2_CR)
    loss = loss_RC + loss_CR
    # Update the network parameters
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import numpy as np
import random

# Define the hyperparameters
num_points = 2048 # number of points in each point cloud
latent_dim = 1024 # dimension of the latent feature vector
hidden_dim = 512 # dimension of the hidden layer in the encoder and decoder
num_iter = 10 # number of iterations for the ICP algorithm
learning_rate = 0.001 # learning rate for the optimizer
batch_size = 32 # batch size for the training data
num_epochs = 100 # number of epochs for the training

# Define the encoder module
class Encoder(torch.nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Define the linear layers
    self.fc1 = torch.nn.Linear(num_points * 3, hidden_dim)
    self.fc2 = torch.nn.Linear(hidden_dim, latent_dim)
    # Define the activation function
    self.relu = torch.nn.ReLU()

  def forward(self, x):
    # x is a batch of point clouds of shape (batch_size, num_points, 3)
    # Flatten the point clouds
    x = x.view(batch_size, -1)
    # Apply the linear layers and the activation function
    x = self.relu(self.fc1(x))
    x = self.fc2(x)
    return x

# Define the decoder module
class Decoder(torch.nn.Module):
  def __init__(self):
    super(Decoder, self).__init__()
    # Define the linear layers
    self.fc1 = torch.nn.Linear(latent_dim, hidden_dim)
    self.fc2 = torch.nn.Linear(hidden_dim, num_points * 3)
    # Define the activation function
    self.relu = torch.nn.ReLU()

  def forward(self, x):
    # x is a batch of latent feature vectors of shape (batch_size, latent_dim)
    # Apply the linear layers and the activation function
    x = self.relu(self.fc1(x))
    x = self.fc2(x)
    # Reshape the output to point clouds
    x = x.view(batch_size, num_points, 3)
    return x

# Define the alignment module
class Aligner(torch.nn.Module):
  def __init__(self):
    super(Aligner, self).__init__()
  
  def forward(self, x1, x2):
    # x1 and x2 are batches of point clouds of shape (batch_size, num_points, 3)
    # Initialize the transformation matrices as identity matrices
    T1 = torch.eye(3).unsqueeze(0).repeat(batch_size, 1, 1).to(device) # shape: (batch_size, 3, 3)
    T2 = torch.eye(3).unsqueeze(0).repeat(batch_size, 1, 1).to(device) # shape: (batch_size, 3, 3)
    # Iterate for a fixed number of steps
    for i in range(num_iter):
      # Find the nearest neighbors between x1 and x2 using Euclidean distance
      dist = torch.cdist(x1, x2) # shape: (batch_size, num_points, num_points)
      nn_idx = torch.argmin(dist, dim=2) # shape: (batch_size, num_points)
      nn_x2 = torch.gather(x2, 1, nn_idx.unsqueeze(-1).repeat(1, 1, 3)) # shape: (batch_size, num_points, 3)
      # Compute the centroids of x1 and nn_x2
      c1 = torch.mean(x1, dim=1) # shape: (batch_size, 3)
      c2 = torch.mean(nn_x2, dim=1) # shape: (batch_size, 3)
      # Center x1 and nn_x2 by subtracting their centroids
      x1_c = x1 - c1.unsqueeze(1) # shape: (batch_size, num_points, 3)
      nn_x2_c = nn_x2 - c2.unsqueeze(1) # shape: (batch_size, num_points, 3)
      # Compute the cross-covariance matrix between x1_c and nn_x2_c
      H = torch.bmm(x1_c.transpose(1, 2), nn_x2_c) # shape: (batch_size, 3 ,3)
      # Compute the singular value decomposition of H
      U, S ,V = torch.svd(H) 
      # Compute the optimal rotation matrix R using U and V
      R = torch.bmm(V, U.transpose(1, 2)) # shape: (batch_size, 3, 3)
      # Correct R to be a proper rotation matrix if necessary
      det = torch.det(R) # shape: (batch_size,)
      I = torch.eye(3).unsqueeze(0).repeat(batch_size, 1, 1).to(device) # shape: (batch_size, 3, 3)
      I[:, 2, 2] = det # shape: (batch_size, 3, 3)
      R = torch.bmm(V, torch.bmm(I, U.transpose(1, 2))) # shape: (batch_size, 3, 3)
      # Compute the optimal translation vector t using c1 and c2
      t = c2 - torch.bmm(R, c1.unsqueeze(-1)).squeeze(-1) # shape: (batch_size, 3)
      # Update the transformation matrices T1 and T2
      T1 = torch.bmm(R, T1) # shape: (batch_size, 3, 3)
      T2 = torch.bmm(R, T2) + t.unsqueeze(-1) # shape: (batch_size, 3, 3)
      # Apply the transformation to x1 and x2
      x1 = torch.bmm(x1, R.transpose(1, 2)) + t.unsqueeze(1) # shape: (batch_size, num_points, 3)
      x2 = torch.bmm(x2, R.transpose(1, 2)) + t.unsqueeze(1) # shape: (batch_size, num_points, 3)
    return x1, x2

# Define the chamfer distance function
def chamfer_distance(x1, x2):
  # x1 and x2 are batches of point clouds of shape (batch_size, num_points, 3)
  # Compute the pairwise distance matrix between x1 and x2
  dist = torch.cdist(x1, x2) # shape: (batch_size, num_points, num_points)
  # Compute the minimum distance for each point in x1 and x2
  min_dist_x1 = torch.min(dist, dim=2)[0] # shape: (batch_size, num_points)
  min_dist_x2 = torch.min(dist, dim=1)[0] # shape: (batch_size, num_points)
  # Compute the average chamfer distance for each point cloud pair
  chamfer_dist = torch.mean(min_dist_x1 + min_dist_x2, dim=1) # shape: (batch_size,)
  return chamfer_dist

# Define the loss function
def loss_function(X1, X2, Y1, Y2 ,Z1 ,Z2):
  # X1 and X2 are the input partial point clouds of shape (batch_size ,num_points ,3)
  # Y1 and Y2 are the output registered point clouds of shape (batch_size ,num_points ,3)
  # Z1 and Z2 are the output completed point clouds of shape (batch_size ,num_points ,3)
  
  # Get the ground truth shapes and poses from the dataset
  GT_shape = dataset.get_shape(X1 ,X2) # shape: (batch_size ,num_points ,3)
  GT_pose = dataset.get_pose(X1 ,X2) # shape: (batch_size ,num_points ,3)

  # Compute the completion loss as the sum of chamfer distances between Z1 ,Z2 and GT_shape
  completion_loss = chamfer_distance(Z1 ,Z2) + chamfer_distance(Z1 ,GT_shape) + chamfer_distance(Z2 ,GT_shape)

  # Compute the registration loss as the sum of chamfer distances between Y1 ,Y2 and GT_pose
  registration_loss = chamfer_distance(Y1 ,Y2) + chamfer_distance(Y1 ,GT_pose) + chamfer_distance(Y2 ,GT_pose)

  # Compute the consistency loss as the sum of chamfer distances between Y1 ,Z2 and Y2 ,Z1
  consistency_loss = chamfer_distance(Y1 ,Z2) + chamfer_distance(Y2 ,Z1)

  # Return the total loss as the sum of completion loss ,registration loss and consistency loss
  return completion_loss + registration_loss + consistency_loss

# Define the R&C flow
def register_and_complete(X1 ,X2):
  
   # Encode the input point clouds
   F1 = encoder(X1)
   F2 = encoder(X2)

   # Align the input point clouds
   Y1 ,Y2 = aligner(X1 ,X2)

   # Decode the aligned point clouds
   Z1 = decoder(F1)
   Z2 = decoder(F2)

   return Y1 ,Y2 ,Z1 ,Z2

# Define the C&R flow
def complete_and_register(X1 ,X2