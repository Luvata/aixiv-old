---
title: 2306.05426v2 SequenceMatch  Imitation Learning for Autoregressive Sequence Modelling with Backtracking
date: 2023-06-06
---

# [SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking](http://arxiv.org/abs/2306.05426v2)

authors: Chris Cundy, Stefano Ermon


## What, Why and How

[1]: https://arxiv.org/pdf/2306.05426v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2301.05426v2.pdf "arXiv:2301.05426v2 [math.OC] 23 Feb 2023"
[3]: https://arxiv.org/pdf/2306.04422v2.pdf "arXiv:2306.04422v2 [cs.AI] 12 Jun 2023"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- What: The paper proposes **SequenceMatch**, an imitation learning method for autoregressive sequence modelling with backtracking, which can improve the quality of generated sequences such as text or video.
- Why: The paper argues that the maximum-likelihood objective used to train autoregressive models does not match the downstream use-case of generating high-quality sequences, and leads to compounding error and degeneration problems when the model goes out of distribution.
- How: The paper formulates sequence generation as an imitation learning problem, which allows minimizing various divergences between the generated and data distributions, including those that penalize out-of-distribution sequences. The paper also introduces a backspace action that allows the model to backtrack and correct its mistakes during generation. The paper shows that SequenceMatch can be implemented without adversarial training or architectural changes, and empirically demonstrates its effectiveness on text generation with language models.


## Main Contributions

[1]: https://arxiv.org/pdf/2306.05426v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2301.05426v2.pdf "arXiv:2301.05426v2 [math.OC] 23 Feb 2023"
[3]: https://arxiv.org/pdf/2306.04422v2.pdf "arXiv:2306.04422v2 [cs.AI] 12 Jun 2023"

The paper at [^1^][1] has the following main contributions:

- It introduces **SequenceMatch**, a novel imitation learning method for autoregressive sequence modelling with backtracking, which can improve the quality of generated sequences such as text or video.
- It shows that SequenceMatch can minimize various divergences between the generated and data distributions, including those that penalize out-of-distribution sequences, and that it can incorporate a backspace action that allows the model to backtrack and correct its mistakes during generation.
- It identifies the SequenceMatch-χ2 divergence as a more suitable training objective for autoregressive models which are used for generation, and proves its theoretical properties.
- It demonstrates empirically that SequenceMatch training leads to improvements over maximum-likelihood training on text generation with language models, and provides qualitative analysis of the generated texts.


## Method Summary

[1]: https://arxiv.org/pdf/2306.05426v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2301.05426v2.pdf "arXiv:2301.05426v2 [math.OC] 23 Feb 2023"
[3]: https://arxiv.org/pdf/2306.04422v2.pdf "arXiv:2306.04422v2 [cs.AI] 12 Jun 2023"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper formulates sequence generation as an imitation learning problem, where the goal is to learn a policy that matches the distribution of sequences from a dataset.
- The paper defines a divergence measure between two distributions of sequences, which can be decomposed into a sum of divergences between conditional distributions of next tokens given previous tokens.
- The paper proposes SequenceMatch, an algorithm that minimizes the divergence measure by iteratively updating the policy using gradient descent and sampling sequences from the current policy.
- The paper introduces a backspace action that allows the model to backtrack and correct its mistakes during generation, and shows how to incorporate it into the divergence measure and the SequenceMatch algorithm.
- The paper identifies the SequenceMatch-χ2 divergence as a more suitable objective for sequence generation, and proves its theoretical properties such as consistency and robustness.


## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2306.05426v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2301.05426v2.pdf "arXiv:2301.05426v2 [math.OC] 23 Feb 2023"
[3]: https://arxiv.org/pdf/2306.04422v2.pdf "arXiv:2306.04422v2 [cs.AI] 12 Jun 2023"

Here is the high-level pseudo code for the paper at [^1^][1]:

- Define a divergence measure D between two distributions of sequences P and Q
- Initialize a policy π that generates sequences from P
- Repeat until convergence:
  - Sample a batch of sequences S from π
  - Compute the gradient of D(P, Q) with respect to π using S
  - Update π using gradient descent
- Optionally, introduce a backspace action b that allows π to backtrack and correct its mistakes during generation
- Optionally, use the SequenceMatch-χ2 divergence as D, which has desirable theoretical properties


## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2306.05426v2.pdf "Abstract - arXiv.org"
[2]: https://arxiv.org/pdf/2301.05426v2.pdf "arXiv:2301.05426v2 [math.OC] 23 Feb 2023"
[3]: https://arxiv.org/pdf/2306.04422v2.pdf "arXiv:2306.04422v2 [cs.AI] 12 Jun 2023"

Here is the detailed pseudo code to implement the paper at [^1^][1]:

- Define a divergence measure D between two distributions of sequences P and Q
  - D(P, Q) = E_p[log p(x)/q(x)] + E_q[log q(x)/p(x)] + λE_pq[log pq(x)/p(x)q(x)]
  - where p and q are the data and generated distributions, pq is their product distribution, and λ is a hyperparameter
- Initialize a policy π that generates sequences from P
  - π can be any autoregressive model such as a transformer
  - Train π using maximum-likelihood on the data distribution P
- Repeat until convergence:
  - Sample a batch of sequences S from π
    - For each sequence s in S:
      - Initialize s as an empty sequence
      - Repeat until s reaches a maximum length or an end-of-sequence token:
        - Sample a token t from π(s) using a softmax temperature τ
        - Append t to s
  - Compute the gradient of D(P, Q) with respect to π using S
    - For each sequence s in S:
      - Compute the log-probabilities of s under p, q, and pq using π
      - Compute the gradient of D(P, Q) for s using the chain rule
      - Average the gradients over the batch
  - Update π using gradient descent
    - Apply an optimizer such as Adam to update the parameters of π using the gradient
- Optionally, introduce a backspace action b that allows π to backtrack and correct its mistakes during generation
  - Modify the sampling procedure to include b as a possible token
  - Modify the divergence measure to account for b as follows:
    - D(P, Q) = E_p[log p(x)/q(x)] + E_q[log q(x)/p(x)] + λE_pq[log pq(x)/p(x)q(x)] + μE_q[log q(b|x)/p(b|x)]
    - where μ is a hyperparameter and p(b|x) and q(b|x) are the probabilities of backtracking given x under p and q
- Optionally, use the SequenceMatch-χ2 divergence as D, which has desirable theoretical properties such as consistency and robustness
  - Define D as follows:
    - D(P, Q) = E_p[log p(x)/q(x)] + E_q[log q(x)/p(x)] + λE_pq[log pq(x)/p(x)q(x)] + ηE_pq[(p(x) − q(x))^2/pq(x)]
    - where η is a hyperparameter and pq is the product distribution of p and q