---
title: 2104.06820v1 Few-shot Image Generation via Cross-domain Correspondence
date: 2021-04-07
---

# [Few-shot Image Generation via Cross-domain Correspondence](http://arxiv.org/abs/2104.06820v1)

authors: Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli Shechtman, Richard Zhang


## What, Why and How

[1]: https://arxiv.org/pdf/2104.06820v1.pdf "arXiv:2104.06820v1 [cs.CV] 13 Apr 2021"
[2]: https://arxiv.org/abs/2104.06820 "Few-shot Image Generation via Cross-domain Correspondence"
[3]: https://arxiv.org/pdf/2104.06820v1%29%3C%22 "arXiv.org"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method for few-shot image generation via cross-domain correspondence, which can adapt a pre-trained generative model to a target domain with very few examples (e.g., 10) and generate diverse and realistic images.
- **Why**: The paper addresses the problem of overfitting when training generative models on domains with limited data, and seeks to utilize a large source domain for pretraining and transfer the diversity information from source to target.
- **How**: The paper introduces two novel components: a cross-domain distance consistency loss that preserves the relative similarities and differences between instances in the source and target domains, and an anchor-based strategy that encourages different levels of realism over different regions in the latent space. The paper demonstrates the effectiveness of the method on both photorealistic and non-photorealistic domains, and shows that the method can automatically discover correspondences between source and target domains.


## Main Contributions

[1]: https://arxiv.org/pdf/2104.06820v1.pdf "arXiv:2104.06820v1 [cs.CV] 13 Apr 2021"
[2]: https://arxiv.org/abs/2104.06820 "Few-shot Image Generation via Cross-domain Correspondence"
[3]: https://arxiv.org/pdf/2104.06820v1%29%3C%22 "arXiv.org"

According to the paper at [^1^][1], the main contributions are:

- A novel cross-domain distance consistency loss that preserves the relative similarities and differences between instances in the source and target domains, and enables the discovery of one-to-one correspondences between them.
- An anchor-based strategy that encourages different levels of realism over different regions in the latent space, and reduces overfitting by preventing mode collapse and improving diversity.
- Extensive experiments on both photorealistic and non-photorealistic domains, showing that the proposed method can generate more diverse and realistic images than previous methods with very few training examples (e.g., 10).


## Method Summary

[1]: https://arxiv.org/pdf/2104.06820v1.pdf "arXiv:2104.06820v1 [cs.CV] 13 Apr 2021"
[2]: https://arxiv.org/abs/2104.06820 "Few-shot Image Generation via Cross-domain Correspondence"
[3]: https://arxiv.org/pdf/2104.06820v1%29%3C%22 "arXiv.org"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper assumes a pre-trained source model Gs that can generate images from a large source domain S, and a target domain T with very few examples (e.g., 10).
- The paper aims to adapt Gs to a new model Gs!t that can generate images from T, while preserving the diversity and realism of S.
- The paper introduces two novel components: a cross-domain distance consistency loss Lcd and an anchor-based strategy La.
- The cross-domain distance consistency loss Lcd encourages the model to preserve the relative similarities and differences between instances in S and T, by minimizing the distance between corresponding noise vectors in the latent space Z. The paper proposes a simple yet effective way to find such correspondences by sorting the noise vectors according to their L2 norm.
- The anchor-based strategy La encourages the model to generate different levels of realism over different regions in Z, by introducing anchor points that represent realistic images in S and T. The paper defines an anchor loss that penalizes unrealistic images near the anchor points, and a diversity loss that penalizes mode collapse far from the anchor points.
- The paper combines Lcd and La with the standard adversarial loss Ladv and the feature matching loss Lfm to form the final objective function for few-shot adaptation. The paper also introduces a curriculum learning scheme that gradually increases the difficulty of adaptation by changing the anchor points over time.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a pre-trained source model Gs, a source domain S, a target domain T with few examples
# Output: an adapted model Gs!t that can generate images from T

# Initialize the noise vectors z_s for S and z_t for T
z_s = sample_noise_vectors(len(S))
z_t = sample_noise_vectors(len(T))

# Sort z_s and z_t according to their L2 norm
z_s = sort_by_norm(z_s)
z_t = sort_by_norm(z_t)

# Initialize the anchor points a_s and a_t for S and T
a_s = select_anchor_points(S, z_s)
a_t = select_anchor_points(T, z_t)

# Initialize the curriculum learning parameters alpha and beta
alpha = 0 # controls the anchor loss
beta = 1 # controls the diversity loss

# Train Gs!t with the objective function
for epoch in range(num_epochs):
  for batch in range(num_batches):
    # Sample a batch of noise vectors from z_s and z_t
    z_s_batch = sample_batch(z_s)
    z_t_batch = sample_batch(z_t)

    # Generate images from Gs!t using z_s_batch and z_t_batch
    x_s_batch = Gs!t(z_s_batch)
    x_t_batch = Gs!t(z_t_batch)

    # Compute the cross-domain distance consistency loss Lcd
    Lcd = compute_Lcd(z_s_batch, z_t_batch)

    # Compute the anchor loss La_a and the diversity loss La_d
    La_a = compute_La_a(x_s_batch, x_t_batch, a_s, a_t)
    La_d = compute_La_d(x_s_batch, x_t_batch, a_s, a_t)

    # Compute the adversarial loss Ladv and the feature matching loss Lfm
    Ladv = compute_Ladv(x_s_batch, x_t_batch, S, T)
    Lfm = compute_Lfm(x_s_batch, x_t_batch, S, T)

    # Compute the total loss L
    L = Ladv + Lfm + Lcd + alpha * La_a + beta * La_d

    # Update Gs!t using gradient descent
    Gs!t = update_Gs!t(L)

  # Update the curriculum learning parameters alpha and beta
  alpha = update_alpha(alpha)
  beta = update_beta(beta)

  # Update the anchor points a_s and a_t using the current Gs!t
  a_s = update_anchor_points(S, z_s, Gs!t)
  a_t = update_anchor_points(T, z_t, Gs!t)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: a pre-trained source model Gs, a source domain S, a target domain T with few examples
# Output: an adapted model Gs!t that can generate images from T

# Initialize the noise vectors z_s for S and z_t for T
z_s = np.random.normal(0, 1, (len(S), dim_z)) # dim_z is the dimension of the latent space
z_t = np.random.normal(0, 1, (len(T), dim_z))

# Sort z_s and z_t according to their L2 norm
z_s = z_s[np.argsort(np.linalg.norm(z_s, axis=1))]
z_t = z_t[np.argsort(np.linalg.norm(z_t, axis=1))]

# Initialize the anchor points a_s and a_t for S and T
a_s = select_anchor_points(S, z_s)
a_t = select_anchor_points(T, z_t)

# Initialize the curriculum learning parameters alpha and beta
alpha = 0 # controls the anchor loss
beta = 1 # controls the diversity loss

# Train Gs!t with the objective function
for epoch in range(num_epochs):
  for batch in range(num_batches):
    # Sample a batch of noise vectors from z_s and z_t
    z_s_batch = sample_batch(z_s)
    z_t_batch = sample_batch(z_t)

    # Generate images from Gs!t using z_s_batch and z_t_batch
    x_s_batch = Gs!t(z_s_batch)
    x_t_batch = Gs!t(z_t_batch)

    # Compute the cross-domain distance consistency loss Lcd
    Lcd = torch.mean(torch.abs(torch.cdist(z_s_batch, z_t_batch) - torch.cdist(x_s_batch, x_t_batch)))

    # Compute the anchor loss La_a and the diversity loss La_d
    La_a = torch.mean(torch.min(torch.cdist(x_s_batch, a_s), dim=1)[0]) + torch.mean(torch.min(torch.cdist(x_t_batch, a_t), dim=1)[0])
    La_d = -torch.mean(torch.max(torch.cdist(x_s_batch, a_s), dim=1)[0]) - torch.mean(torch.max(torch.cdist(x_t_batch, a_t), dim=1)[0])

    # Compute the adversarial loss Ladv and the feature matching loss Lfm
    Ladv = compute_Ladv(x_s_batch, x_t_batch, S, T) # use any standard GAN loss function
    Lfm = compute_Lfm(x_s_batch, x_t_batch, S, T) # use any standard feature matching loss function

    # Compute the total loss L
    L = Ladv + Lfm + Lcd + alpha * La_a + beta * La_d

    # Update Gs!t using gradient descent
    optimizer.zero_grad()
    L.backward()
    optimizer.step()

  # Update the curriculum learning parameters alpha and beta
  alpha = min(alpha + delta_alpha, alpha_max) # delta_alpha and alpha_max are hyperparameters
  beta = max(beta - delta_beta, beta_min) # delta_beta and beta_min are hyperparameters

  # Update the anchor points a_s and a_t using the current Gs!t
  a_s = update_anchor_points(S, z_s, Gs!t)
  a_t = update_anchor_points(T, z_t, Gs!t)
```