---
title: 2007.06600v4 Closed-Form Factorization of Latent Semantics in GANs
date: 2020-07-07
---

# [Closed-Form Factorization of Latent Semantics in GANs](http://arxiv.org/abs/2007.06600v4)

authors: Yujun Shen, Bolei Zhou


## What, Why and How

[1]: https://arxiv.org/pdf/2007.06600.pdf "arXiv:2007.06600v4 [cs.CV] 3 Apr 2021"
[2]: https://arxiv.org/abs/2007.06600 "Closed-Form Factorization of Latent Semantics in GANs"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a **closed-form factorization algorithm** for **latent semantic discovery** in GANs by directly decomposing the pre-trained weights.
- **Why**: The paper aims to identify the meaningful directions in the latent space of GANs that correspond to human-understandable concepts, such as pose, expression, lighting, etc., without requiring manual annotations or clear definitions of the target attributes.
- **How**: The paper examines the internal representation learned by GANs and exploits the low-rank structure of the weight matrices to obtain the principal components of the latent space. The paper then applies a linear transformation to align the principal components with the interpretable directions and evaluates the quality and diversity of the discovered semantics on various GAN models and datasets.

## Main Contributions

[1]: https://arxiv.org/pdf/2007.06600.pdf "arXiv:2007.06600v4 [cs.CV] 3 Apr 2021"
[2]: https://arxiv.org/abs/2007.06600 "Closed-Form Factorization of Latent Semantics in GANs"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

According to the paper at [^1^][1], the main contributions are:

- **A novel closed-form factorization algorithm** for latent semantic discovery in GANs by directly decomposing the pre-trained weights, which is **unsupervised**, **fast**, and **simple** to implement.
- **A comprehensive analysis** of the generation mechanism of GANs and the low-rank structure of the weight matrices, which reveals the underlying variation factors in the latent space and motivates the proposed factorization method.
- **A thorough evaluation** of the quality and diversity of the discovered semantics on various GAN models and datasets, which demonstrates the effectiveness and versatility of the proposed method compared to the state-of-the-art supervised methods.

## Method Summary

[1]: https://arxiv.org/pdf/2007.06600.pdf "arXiv:2007.06600v4 [cs.CV] 3 Apr 2021"
[2]: https://arxiv.org/abs/2007.06600 "Closed-Form Factorization of Latent Semantics in GANs"
[3]: https://info.arxiv.org/help/bulk_data_s3.html "Full Text via S3 - arXiv info"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper first analyzes the **generation mechanism** of GANs and shows that the output image can be expressed as a linear combination of the **basis vectors** derived from the weight matrices of the generator network.
- The paper then proposes a **closed-form factorization algorithm** to obtain the **principal components** of the latent space by performing singular value decomposition (SVD) on the weight matrices of the generator network.
- The paper further introduces a **linear transformation** to align the principal components with the **interpretable directions** by minimizing the reconstruction error between the original and transformed latent codes.
- The paper finally evaluates the **quality and diversity** of the discovered semantics by measuring the semantic consistency, attribute intensity, and editing diversity on various GAN models and datasets.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Input: a pre-trained GAN model G with latent space Z
# Output: a set of interpretable directions D in Z

# Step 1: Analyze the generation mechanism of G
# For each layer l of G, compute the basis vectors B_l from the weight matrix W_l
# For each latent code z in Z, compute the output image x = G(z) as a linear combination of B_l

# Step 2: Factorize the latent space Z
# For each layer l of G, perform SVD on W_l to obtain the principal components P_l and the singular values S_l
# For each latent code z in Z, project it onto P_l to obtain the principal component scores Q_l

# Step 3: Align the principal components with the interpretable directions
# For each layer l of G, learn a linear transformation matrix T_l that maps Q_l to a new latent code z' with minimal reconstruction error
# For each latent code z in Z, apply T_l to obtain the transformed latent code z' and the interpretable directions D_l

# Step 4: Evaluate the quality and diversity of the discovered semantics
# For each interpretable direction d in D, measure the semantic consistency, attribute intensity, and editing diversity on various output images x = G(z + alpha * d), where alpha is a scalar factor
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Input: a pre-trained GAN model G with latent space Z of dimension N
# Output: a set of interpretable directions D in Z

# Step 1: Analyze the generation mechanism of G
# Let L be the number of layers in G
# Let W_l be the weight matrix of layer l with shape (M_l, N_l), where M_l is the output dimension and N_l is the input dimension
# Let B_l be the basis vectors of layer l with shape (M_l, N)
# Initialize B_l as zero matrices
# For l = L to 1:
  # If l == L:
    # B_l = W_l # The last layer directly maps the latent code to the output image
  # Else:
    # B_l = W_l * B_(l+1) # The previous layers map the latent code to the intermediate features
# For each latent code z in Z:
  # Initialize x as zero vector with shape (M_L,)
  # For l = 1 to L:
    # x = x + B_l * z # The output image is a linear combination of the basis vectors weighted by the latent code

# Step 2: Factorize the latent space Z
# Let P_l be the principal components of layer l with shape (N_l, N)
# Let S_l be the singular values of layer l with shape (N,)
# Let Q_l be the principal component scores of layer l with shape (N, N)
# For each layer l of G:
  # Perform SVD on W_l to obtain U_l, S_l, and V_l, where U_l has shape (M_l, N), S_l has shape (N,), and V_l has shape (N, N_l)
  # P_l = V_l.T # The principal components are the right singular vectors of W_l
  # Q_l = S_l * U_l.T # The principal component scores are the product of the singular values and the left singular vectors of W_l
# For each latent code z in Z:
  # For each layer l of G:
    # Project z onto P_l to obtain q_l = P_l.T * z # The projection is a dot product between z and P_l

# Step 3: Align the principal components with the interpretable directions
# Let T_l be the linear transformation matrix of layer l with shape (N, N)
# Let z' be the transformed latent code with shape (N,)
# Let D_l be the interpretable directions of layer l with shape (N, N)
# For each layer l of G:
  # Initialize T_l as an identity matrix
  # Minimize the reconstruction error E = ||z - T_l * q_l||^2 with respect to T_l using gradient descent or other optimization methods
  # Apply T_l to Q_l to obtain Z' = T_l * Q_l # The transformed latent codes are the product of T_l and Q_l
  # D_l = P_l * T_l # The interpretable directions are the product of P_l and T_l
# For each latent code z in Z:
  # Apply T_L to q_L to obtain z' = T_L * q_L # The transformed latent code is the product of T_L and q_L

# Step 4: Evaluate the quality and diversity of the discovered semantics
# Let d be an interpretable direction in D with shape (N,)
# Let x be an output image generated by G(z) with shape (M_L,)
# Let alpha be a scalar factor that controls the magnitude of editing
# For each interpretable direction d in D:
  # Measure the semantic consistency by computing the cosine similarity between d and a pre-trained attribute classifier on a set of output images x = G(z + alpha * d) for different values of alpha
  # Measure the attribute intensity by computing the mean absolute difference between x and x' = G(z + alpha * d) for a fixed value of alpha
  # Measure the editing diversity by computing the entropy of a pre-trained attribute classifier on a set of output images x = G(z + alpha * d) for different values of z and alpha
```