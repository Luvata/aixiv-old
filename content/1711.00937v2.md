---
title: 1711.00937v2 Neural Discrete Representation Learning
date: 2017-11-01
---

# [Neural Discrete Representation Learning](http://arxiv.org/abs/1711.00937v2)

authors: Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu


## What, Why and How

[1]: https://arxiv.org/abs/1711.00937 "[1711.00937] Neural Discrete Representation Learning - arXiv.org"
[2]: https://arxiv.org/pdf/1711.00937.pdf "Neural Discrete Representation Learning - arXiv.org"

Here is a summary of the paper:

- **What**: The paper proposes a generative model called **Vector Quantised-Variational AutoEncoder (VQ-VAE)** that learns discrete representations of data without supervision[^1^][1] [^2^][2].
- **Why**: The paper aims to overcome the limitations of existing models that use continuous representations, such as VAEs, which suffer from **posterior collapse** and **static prior**[^1^][1] [^2^][2]. The paper also argues that discrete representations are more natural and useful for many domains, such as language, speech and images[^1^][1] [^2^][2].
- **How**: The paper uses ideas from **vector quantisation (VQ)** to learn a discrete latent space, where each latent code is assigned to the nearest vector in a codebook[^1^][1] [^2^][2]. The paper also learns a dynamic prior over the latent codes using an autoregressive model, such as PixelCNN[^1^][1] [^2^][2]. The paper evaluates the model on various tasks, such as image generation, video generation, speech synthesis, speaker conversion and unsupervised phoneme discovery[^1^][1] [^2^][2].

## Main Contributions

Some of the contributions of this paper are:

- It introduces a novel generative model that learns discrete representations without supervision.
- It shows how to use vector quantisation to avoid posterior collapse and learn a meaningful latent space.
- It demonstrates how to learn a dynamic prior over the discrete codes using an autoregressive model.
- It provides empirical evidence of the quality and usefulness of the learnt representations on various domains and tasks.

## Method Summary

The method section of the paper describes the details of the VQ-VAE model and its training procedure. The model consists of three main components: an encoder, a decoder and a codebook. The encoder maps the input data to a latent space of discrete codes, where each code is represented by an embedding vector from the codebook. The decoder reconstructs the input data from the latent codes using a deconvolutional network. The codebook is updated using an exponential moving average of the encoder outputs. The model is trained by minimizing a reconstruction loss and a commitment loss, which encourages the encoder to use the codebook effectively. The paper also explains how to learn a prior distribution over the latent codes using an autoregressive model, such as PixelCNN or WaveNet. The prior model is trained separately from the VQ-VAE model using the latent codes as inputs. The paper also discusses some implementation details and hyperparameters of the model.

## Pseudo Code - High level

Here is a possible pseudo code for the paper:

```python
# Define the encoder network
def encoder(x):
  # Apply convolutional layers to x and get z_e
  z_e = conv_layers(x)
  # Return z_e
  return z_e

# Define the decoder network
def decoder(z_q):
  # Apply deconvolutional layers to z_q and get x_hat
  x_hat = deconv_layers(z_q)
  # Return x_hat
  return x_hat

# Define the codebook
def codebook(K, D):
  # Initialize a matrix of KxD with random values
  e = random_matrix(K, D)
  # Return e
  return e

# Define the vector quantisation function
def vector_quantisation(z_e, e):
  # Compute the L2 distance between z_e and e
  dist = L2_distance(z_e, e)
  # Find the index of the nearest vector for each z_e
  k = argmin(dist, axis=1)
  # Retrieve the corresponding vector from e
  z_q = e[k]
  # Return z_q and k
  return z_q, k

# Define the VQ-VAE model
def VQ_VAE(x, e):
  # Encode x to z_e
  z_e = encoder(x)
  # Quantise z_e to z_q and get k
  z_q, k = vector_quantisation(z_e, e)
  # Decode z_q to x_hat
  x_hat = decoder(z_q)
  # Return x_hat, z_e, z_q and k
  return x_hat, z_e, z_q, k

# Define the reconstruction loss function
def reconstruction_loss(x, x_hat):
  # Compute the mean squared error between x and x_hat
  mse = mean_squared_error(x, x_hat)
  # Return mse
  return mse

# Define the commitment loss function
def commitment_loss(z_e, z_q):
  # Compute the mean squared error between z_e and z_q
  mse = mean_squared_error(z_e, z_q)
  # Return mse
  return mse

# Define the codebook update function
def codebook_update(e, z_e, k, beta):
  # Compute the exponential moving average of z_e for each k
  n = exp_moving_average(z_e, k)
  # Update e by adding beta times (n - e) for each k
  e = e + beta * (n - e)
  # Return e
  return e

# Define the prior model
def prior_model(z):
  # Apply an autoregressive model to z and get p_z
  p_z = autoregressive_model(z)
  # Return p_z
  return p_z

# Define the training procedure for VQ-VAE model
def train_VQ_VAE(data, e, alpha, beta):
  # Loop over the data batches
  for x in data:
    # Forward pass of VQ-VAE model and get outputs
    x_hat, z_e, z_q, k = VQ_VAE(x, e)
    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(x, x_hat)
    # Compute the commitment loss
    com_loss = commitment_loss(z_e, z_q)
    # Compute the total loss as a weighted sum of rec_loss and com_loss
    loss = rec_loss + alpha * com_loss
    # Backpropagate the loss and update the encoder and decoder parameters
    backprop(loss)
    # Update the codebook using z_e, k and beta
    e = codebook_update(e, z_e, k, beta)

# Define the training procedure for prior model
def train_prior_model(data, e):
  # Loop over the data batches
  for x in data:
    # Encode x to z_e using encoder network
    z_e = encoder(x)
    # Quantise z_e to z_q using vector quantisation function
    z_q, _ = vector_quantisation(z_e, e)
    # Forward pass of prior model and get p_z 
    p_z = prior_model(z_q)
    # Compute the negative log-likelihood of p_z 
    nll = negative_log_likelihood(p_z)
    # Backpropagate the nll and update the prior model parameters 
    backprop(nll)

```

## Pseudo Code - Detail

Here is a possible pseudo code to implement the paper:

```python
# Import the necessary libraries
import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

# Define the hyperparameters
batch_size = 32 # The size of each data batch
K = 512 # The number of vectors in the codebook
D = 64 # The dimension of each vector in the codebook
alpha = 0.25 # The weight for the commitment loss
beta = 0.25 # The decay rate for the exponential moving average
learning_rate = 0.001 # The learning rate for the optimizer

# Load the data and preprocess it
data = load_data() # Load the data from a source
data = preprocess_data(data) # Preprocess the data according to the domain

# Define the encoder network as a convolutional neural network
def encoder(x):
  # Apply a convolutional layer with 32 filters, 4x4 kernel, 2x2 stride and ReLU activation to x and get h1
  h1 = tf.nn.conv2d(x, filters=32, kernel_size=4, strides=2, activation=tf.nn.relu)
  # Apply a convolutional layer with 32 filters, 4x4 kernel, 2x2 stride and ReLU activation to h1 and get h2
  h2 = tf.nn.conv2d(h1, filters=32, kernel_size=4, strides=2, activation=tf.nn.relu)
  # Apply a convolutional layer with 64 filters, 4x4 kernel, 2x2 stride and ReLU activation to h2 and get h3
  h3 = tf.nn.conv2d(h2, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)
  # Apply a convolutional layer with D filters, 1x1 kernel, 1x1 stride and linear activation to h3 and get z_e
  z_e = tf.nn.conv2d(h3, filters=D, kernel_size=1, strides=1, activation=None)
  # Return z_e
  return z_e

# Define the decoder network as a deconvolutional neural network
def decoder(z_q):
  # Apply a deconvolutional layer with D filters, 1x1 kernel, 1x1 stride and ReLU activation to z_q and get h4
  h4 = tf.nn.conv2d_transpose(z_q, filters=D, kernel_size=1, strides=1, activation=tf.nn.relu)
  # Apply a deconvolutional layer with 64 filters, 4x4 kernel, 2x2 stride and ReLU activation to h4 and get h5
  h5 = tf.nn.conv2d_transpose(h4, filters=64, kernel_size=4, strides=2, activation=tf.nn.relu)
  # Apply a deconvolutional layer with 32 filters, 4x4 kernel, 2x2 stride and ReLU activation to h5 and get h6
  h6 = tf.nn.conv2d_transpose(h5, filters=32, kernel_size=4, strides=2, activation=tf.nn.relu)
  # Apply a deconvolutional layer with C filters (where C is the number of channels in x), 
    #4x4 kernel, 2x2 stride and sigmoid activation to h6 and get x_hat
    x_hat = tf.nn.conv2d_transpose(h6,
                                   filters=C,
                                   kernel_size=4,
                                   strides=2,
                                   activation=tf.nn.sigmoid)
    # Return x_hat
    return x_hat

# Define the codebook as a variable matrix of KxD with random values
e = tf.Variable(tf.random.normal(shape=(K,D)))

# Define the vector quantisation function using TensorFlow operations
def vector_quantisation(z_e,e):
    # Flatten z_e to a matrix of BxHxWxD (where B is the batch size,
      #H and W are the height and width of z_e)
      z_e_flat = tf.reshape(z_e,(B,-1,D))
      # Compute the L2 distance between z_e_flat and e using broadcasting and reduce_sum 
      dist = tf.reduce_sum((tf.expand_dims(z_e_flat,axis=-2) - tf.expand_dims(e,axis=0))**2,axis=-1)
      # Find the index of the nearest vector for each z_e_flat using argmin 
      k = tf.argmin(dist,axis=-1)
      # Retrieve the corresponding vector from e using gather 
      z_q_flat = tf.gather(e,k)
      # Reshape z_q_flat to the same shape as z_e 
      z_q = tf.reshape(z_q_flat,tf.shape(z_e))
      # Return z_q and k
      return z_q, k

# Define the VQ-VAE model as a function that takes x and e as inputs and returns x_hat, z_e, z_q and k as outputs
def VQ_VAE(x,e):
    # Encode x to z_e using encoder network
    z_e = encoder(x)
    # Quantise z_e to z_q and get k using vector quantisation function
    z_q, k = vector_quantisation(z_e,e)
    # Decode z_q to x_hat using decoder network
    x_hat = decoder(z_q)
    # Return x_hat, z_e, z_q and k
    return x_hat, z_e, z_q, k

# Define the reconstruction loss function as the mean squared error between x and x_hat
def reconstruction_loss(x,x_hat):
    # Compute the mean squared error between x and x_hat using reduce_mean 
    mse = tf.reduce_mean((x-x_hat)**2)
    # Return mse
    return mse

# Define the commitment loss function as the mean squared error between z_e and z_q
def commitment_loss(z_e,z_q):
    # Compute the mean squared error between z_e and z_q using reduce_mean 
    mse = tf.reduce_mean((z_e-z_q)**2)
    # Return mse
    return mse

# Define the codebook update function using TensorFlow operations
def codebook_update(e,z_e,k,beta):
    # Compute the one-hot encoding of k using one_hot 
    one_hot_k = tf.one_hot(k,K)
    # Compute the exponential moving average of z_e for each k using reduce_sum and reduce_mean 
    n = tf.reduce_sum(one_hot_k,axis=[0,1])
    m = tf.reduce_sum(tf.expand_dims(one_hot_k,axis=-1) * tf.expand_dims(z_e_flat,axis=-2),axis=[0,1])
    n = tf.reduce_mean(n,axis=0)
    m = tf.reduce_mean(m,axis=0)
    # Update e by adding beta times (n - e) for each k using assign_add 
    e.assign_add(beta * (n - e))
    # Return e
    return e

# Define the prior model as an autoregressive model such as PixelCNN or WaveNet
prior_model = PixelCNN() # or WaveNet()

# Define the optimizer for the VQ-VAE model as an Adam optimizer with learning rate
optimizer_VQ_VAE = tf.optimizers.Adam(learning_rate)

# Define the optimizer for the prior model as an Adam optimizer with learning rate
optimizer_prior_model = tf.optimizers.Adam(learning_rate)

# Define the training procedure for VQ-VAE model as a function that takes a data batch and e as inputs and returns the loss and e as outputs
@tf.function
def train_VQ_VAE(x,e):
  # Use a gradient tape to record the gradients of the loss with respect to the encoder, decoder and codebook parameters
  with tf.GradientTape() as tape:
      # Forward pass of VQ-VAE model and get outputs
      x_hat, z_e, z_q, k = VQ_VAE(x,e)
      # Compute the reconstruction loss
      rec_loss = reconstruction_loss(x,x_hat)
      # Compute the commitment loss
      com_loss = commitment_loss(z_e,z_q)
      # Compute the total loss as a weighted sum of rec_loss and com_loss
      loss = rec_loss + alpha * com_loss
  # Get the gradients of the loss with respect to the encoder and decoder parameters using tape.gradient 
  gradients_encoder_decoder = tape.gradient(loss,[encoder.trainable_variables,decoder.trainable_variables])
  # Apply the gradients to the encoder and decoder parameters using optimizer_VQ_VAE.apply_gradients 
  optimizer_VQ_VAE.apply_gradients(zip(gradients_encoder_decoder,[encoder.trainable_variables,decoder.trainable_variables]))
  # Update the codebook using z_e, k and beta using codebook_update function 
  e = codebook_update(e,z_e,k,beta)
  # Return loss and e
  return loss, e

# Define the training procedure for prior model as a function that takes a data batch and e as inputs and returns the negative log-likelihood as output
@tf.function
def train_prior_model(x,e):
  # Use a gradient tape to record the gradients of the negative log-likelihood with respect to the prior model parameters
  with tf.GradientTape() as tape:
      # Encode x to z_e using encoder network
      z_e = encoder(x)
      # Quantise z_e to z_q using vector quantisation function
      z_q, _ = vector_quantisation(z_e,e)
      # Forward pass of prior model and get p_z 
      p_z = prior_model(z_q)