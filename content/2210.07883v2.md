---
title: 2210.07883v2 One Model to Edit Them All  Free-Form Text-Driven Image Manipulation with Semantic Modulations
date: 2022-10-08
---

# [One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations](http://arxiv.org/abs/2210.07883v2)

authors: Yiming Zhu, Hongyu Liu, Yibing Song, ziyang Yuan, Xintong Han, Chun Yuan, Qifeng Chen, Jue Wang


## What, Why and How

[1]: https://arxiv.org/abs/2210.07883 "[2210.07883] One Model to Edit Them All: Free-Form Text ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2201.07883v2 "[2201.07883v2] A simple mechanism for stable oscillations in an ..."
[3]: https://arxiv.org/pdf/2210.07883.pdf "arXiv.org"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method named **Free-Form CLIP (FFCLIP)**, which can perform free-form text-driven image manipulation with semantic modulations.
- **Why**: The paper aims to overcome the limitations of existing methods that can only handle fixed text prompts for image manipulation, and to enable users to describe their intentions with natural language.
- **How**: The paper leverages the visual latent space of **StyleGAN** and the text embedding space of **CLIP**, and introduces a cross-modality semantic modulation module that contains semantic alignment and injection. The semantic alignment performs an automatic latent mapping between the two spaces via linear transformations with a cross attention mechanism. The semantic injection adds the text prompt embeddings to the StyleGAN latent space to modulate the image generation. The paper evaluates FFCLIP on three types of images (human portraits, cars, and churches) and shows that it can produce semantically accurate and visually realistic images with various text prompts.

## Main Contributions

[1]: https://arxiv.org/abs/2210.07883 "[2210.07883] One Model to Edit Them All: Free-Form Text ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2201.07883v2 "[2201.07883v2] A simple mechanism for stable oscillations in an ..."
[3]: https://arxiv.org/pdf/2210.07883.pdf "arXiv.org"

According to the paper[^1^][1], the main contributions are:

- **A novel method for free-form text-driven image manipulation** that can handle various text prompts without pre-defining them.
- **A cross-modality semantic modulation module** that can automatically align and inject semantics from text embeddings to StyleGAN latent space.
- **Extensive experiments and evaluations** on three types of images that demonstrate the effectiveness and versatility of FFCLIP.

## Method Summary

[1]: https://arxiv.org/abs/2210.07883 "[2210.07883] One Model to Edit Them All: Free-Form Text ... - arXiv.org"
[2]: http://export.arxiv.org/abs/2201.07883v2 "[2201.07883v2] A simple mechanism for stable oscillations in an ..."
[3]: https://arxiv.org/pdf/2210.07883.pdf "arXiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first introduces the **preliminaries** of StyleGAN and CLIP, which are the two key components of FFCLIP. StyleGAN is a generative adversarial network that can synthesize high-quality images from a latent space. CLIP is a contrastive learning framework that can learn text and image embeddings from a large-scale dataset.
- The paper then describes the **proposed method** of FFCLIP, which consists of three parts: a **text encoder**, a **cross-modality semantic modulation module**, and a **StyleGAN decoder**. The text encoder uses CLIP to encode the text prompt into a vector. The cross-modality semantic modulation module aligns and injects the text vector to the StyleGAN latent space. The StyleGAN decoder generates the manipulated image from the modulated latent vector.
- The paper further details the **semantic alignment** and **semantic injection** processes in the cross-modality semantic modulation module. The semantic alignment learns linear transformations between the text and image embedding spaces with a cross attention mechanism. The semantic injection adds the text vector to the latent vector at different scales to control the image generation at different resolutions.
- The paper finally presents the **training and inference** procedures of FFCLIP. The training objective is to minimize the distance between the text vector and the image vector in the CLIP embedding space, while keeping the image quality high. The inference process is to encode a given text prompt and manipulate a given image with FFCLIP.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the text encoder using CLIP
text_encoder = CLIP()

# Define the StyleGAN decoder using a pre-trained model
stylegan_decoder = StyleGAN()

# Define the cross-modality semantic modulation module
semantic_modulation = SemanticModulation()

# Define the training objective as the distance between text and image embeddings
objective = Distance(text_encoder, stylegan_decoder)

# Train FFCLIP with text prompts and images
for text_prompt, image in data:
  # Encode the text prompt into a vector
  text_vector = text_encoder(text_prompt)
  
  # Encode the image into a latent vector
  latent_vector = stylegan_decoder.encode(image)
  
  # Align and inject the text vector to the latent vector
  modulated_vector = semantic_modulation(text_vector, latent_vector)
  
  # Generate the manipulated image from the modulated vector
  manipulated_image = stylegan_decoder.decode(modulated_vector)
  
  # Compute the loss as the objective function
  loss = objective(text_vector, manipulated_image)
  
  # Update the parameters of FFCLIP
  update_parameters(loss)

# Inference with a new text prompt and a new image
new_text_prompt = input()
new_image = input()

# Encode the new text prompt into a vector
new_text_vector = text_encoder(new_text_prompt)

# Encode the new image into a latent vector
new_latent_vector = stylegan_decoder.encode(new_image)

# Align and inject the new text vector to the new latent vector
new_modulated_vector = semantic_modulation(new_text_vector, new_latent_vector)

# Generate the new manipulated image from the new modulated vector
new_manipulated_image = stylegan_decoder.decode(new_modulated_vector)

# Output the new manipulated image
output(new_manipulated_image)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import stylegan2

# Define the text encoder using CLIP
text_encoder = clip.load("ViT-B/32", device="cuda")

# Define the StyleGAN decoder using a pre-trained model
stylegan_decoder = stylegan2.load("ffhq-config-f", device="cuda")

# Define the cross-modality semantic modulation module
class SemanticModulation(torch.nn.Module):
  def __init__(self, text_dim, latent_dim, num_layers):
    super().__init__()
    # Define the linear transformations for each layer
    self.linear_transforms = torch.nn.ModuleList([torch.nn.Linear(text_dim, latent_dim) for _ in range(num_layers)])
    # Define the cross attention mechanism for each layer
    self.cross_attentions = torch.nn.ModuleList([torch.nn.MultiheadAttention(latent_dim, latent_dim) for _ in range(num_layers)])
  
  def forward(self, text_vector, latent_vector):
    # Loop over each layer
    for i in range(num_layers):
      # Apply the linear transformation to the text vector
      transformed_text_vector = self.linear_transforms[i](text_vector)
      # Apply the cross attention to the latent vector and the transformed text vector
      modulated_vector, _ = self.cross_attentions[i](latent_vector, transformed_text_vector, transformed_text_vector)
      # Add the modulated vector to the latent vector
      latent_vector = latent_vector + modulated_vector
    # Return the final modulated vector
    return latent_vector

# Define the training objective as the distance between text and image embeddings
def objective(text_encoder, stylegan_decoder, text_vector, manipulated_image):
  # Encode the manipulated image into a vector using CLIP
  image_vector = text_encoder.encode_image(manipulated_image)
  # Compute the cosine similarity between the text vector and the image vector
  similarity = torch.cosine_similarity(text_vector, image_vector)
  # Compute the distance as the negative similarity
  distance = -similarity
  # Return the distance as the loss
  return distance

# Initialize the semantic modulation module with appropriate dimensions and number of layers
semantic_modulation = SemanticModulation(512, 512, 18)

# Define an optimizer for updating the parameters of FFCLIP
optimizer = torch.optim.Adam(semantic_modulation.parameters(), lr=0.0001)

# Train FFCLIP with text prompts and images
for epoch in range(epochs):
  for text_prompt, image in data:
    # Encode the text prompt into a vector using CLIP
    text_vector = text_encoder.encode_text(text_prompt)
    
    # Encode the image into a latent vector using StyleGAN
    latent_vector = stylegan_decoder.encode(image)
    
    # Align and inject the text vector to the latent vector using semantic modulation
    modulated_vector = semantic_modulation(text_vector, latent_vector)
    
    # Generate the manipulated image from the modulated vector using StyleGAN
    manipulated_image = stylegan_decoder.decode(modulated_vector)
    
    # Compute the loss as the objective function
    loss = objective(text_encoder, stylegan_decoder, text_vector, manipulated_image)
    
    # Update the parameters of FFCLIP using backpropagation and gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Inference with a new text prompt and a new image
new_text_prompt = input()
new_image = input()

# Encode the new text prompt into a vector using CLIP
new_text_vector = text_encoder.encode_text(new_text_prompt)

# Encode the new image into a latent vector using StyleGAN
new_latent_vector = stylegan_decoder.encode(new_image)

# Align and inject the new text vector to the new latent vector using semantic modulation
new_modulated_vector = semantic_modulation(new_text_vector, new_latent_vector)

# Generate the new manipulated image from the new modulated vector using StyleGAN
new_manipulated_image = stylegan_decoder.decode(new_modulated_vector)

# Output the new manipulated image
output(new_manipulated_image)
```