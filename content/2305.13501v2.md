---
title: 2305.13501v2 LaDI-VTON  Latent Diffusion Textual-Inversion Enhanced Virtual Try-On
date: 2023-05-14
---

# [LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On](http://arxiv.org/abs/2305.13501v2)

authors: Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, Rita Cucchiara


## What, Why and How

[1]: https://arxiv.org/pdf/2305.13501v2.pdf "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On"
[2]: https://arxiv.org/abs/2305.13501 "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On"
[3]: http://export.arxiv.org/pdf/2305.13501 "PDF for 2305.13501 - export.arxiv.org"

Here is a summary of the paper:

- **What**: The paper introduces LaDI-VTON, a novel model for image-based virtual try-on, which can generate realistic images of a target model wearing a given in-shop garment.
- **Why**: The paper aims to enhance the consumer experience in e-commerce and metaverse by leveraging the recent advancements in diffusion models, which can create high-quality images with fine details and textures.
- **How**: The paper proposes a latent diffusion model extended with an additional autoencoder module that uses learnable skip connections to preserve the model's characteristics. The paper also proposes a textual inversion component that can map the visual features of the garment to the CLIP token embedding space and generate pseudo-word token embeddings that can condition the generation process.

The paper claims to outperform the competitors on two datasets: Dress Code and VITON-HD[^1^][1].


## Main Contributions

According to the paper, the main contributions are:

- The first latent diffusion model for the virtual try-on task, which can generate realistic and diverse images of a target model wearing a given in-shop garment.
- A novel additional autoencoder module that exploits learnable skip connections to enhance the generation process and preserve the model's characteristics.
- A novel textual inversion component that can map the visual features of the garment to the CLIP token embedding space and generate pseudo-word token embeddings that can condition the generation process.
- Extensive experiments on two datasets, Dress Code and VITON-HD, showing that the proposed method outperforms the state-of-the-art methods by a large margin on various metrics.


## Method Summary

Here is a summary of the method section of the paper:

- The paper proposes a latent diffusion model for the virtual try-on task, which consists of three main components: a latent diffusion model (LDM), an additional autoencoder module (AAM), and a textual inversion component (TIC).
- The LDM is based on the denoising diffusion probabilistic model (DDPM) , which can generate realistic images by reversing a Markov chain that gradually adds noise to the input image. The LDM takes as input a target model image and a try-on clothing item image, and outputs a latent code that encodes both images. The LDM is conditioned on a set of token embeddings that represent the try-on clothing item.
- The AAM is an encoder-decoder network that takes as input the target model image and outputs a reconstruction of the same image. The AAM uses learnable skip connections to transfer information from the encoder to the decoder, and also to the LDM. The AAM helps to preserve the model's characteristics and enhance the generation quality.
- The TIC is a module that can map the visual features of the try-on clothing item to the CLIP token embedding space . The TIC uses an encoder network to extract features from the clothing item image, and then uses a linear projection layer to generate a set of pseudo-word token embeddings that can condition the LDM. The TIC helps to maintain the texture and details of the clothing item and improve the diversity of the generated images.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: target model image x, try-on clothing item image y
# Output: generated image z of target model wearing try-on clothing item

# Encode target model image x using AAM encoder
x_enc = AAM_encoder(x)

# Encode try-on clothing item image y using TIC encoder
y_enc = TIC_encoder(y)

# Project y_enc to CLIP token embedding space using linear projection layer
y_tokens = linear_projection(y_enc)

# Initialize latent code z_0 with Gaussian noise
z_0 = sample_from_gaussian()

# Reverse the diffusion process from z_0 to z_T using LDM conditioned on y_tokens and x_enc
for t in range(T):
  # Predict the mean and variance of the reverse diffusion process at time step t
  mean_t, var_t = LDM(z_t, y_tokens, x_enc)
  
  # Sample z_t+1 from the predicted distribution
  z_t+1 = sample_from(mean_t, var_t)
  
# Decode z_T using AAM decoder to get the final generated image z
z = AAM_decoder(z_T)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import clip
import numpy as np

# Define the hyperparameters
T = 1000 # number of diffusion steps
beta = 0.0001 # noise level
sigma = np.sqrt(np.cumsum(np.ones(T) * beta)) # noise schedule
alpha = 1 - sigma ** 2 # reverse diffusion coefficients
eps = 1e-5 # small constant for numerical stability

# Define the AAM encoder network
class AAM_Encoder(nn.Module):
  def __init__(self):
    super().__init__()
    # Use a ResNet-50 backbone with pretrained weights
    self.backbone = torchvision.models.resnet50(pretrained=True)
    # Remove the last fully connected layer and the average pooling layer
    self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
    # Add a convolutional layer to reduce the feature dimension to 256
    self.conv = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)
  
  def forward(self, x):
    # Input x is a batch of target model images of size (B, 3, H, W)
    # Output x_enc is a batch of encoded features of size (B, 256, H/32, W/32)
    x_enc = self.backbone(x) # (B, 2048, H/32, W/32)
    x_enc = self.conv(x_enc) # (B, 256, H/32, W/32)
    return x_enc

# Define the AAM decoder network
class AAM_Decoder(nn.Module):
  def __init__(self):
    super().__init__()
    # Use a series of upsampling and convolutional blocks to reconstruct the image
    self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
    self.conv1 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)
    self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
    self.conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)
    self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
    self.conv3 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)
    self.up4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
    self.conv4 = nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1)
    self.up5 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
    self.conv5 = nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1)

  def forward(self, z):
    # Input z is a batch of latent codes of size (B, 256, H/32, W/32)
    # Output x_rec is a batch of reconstructed images of size (B, 3, H, W)
    z = F.relu(self.conv1(self.up1(z))) # (B, 128, H/16, W/16)
    z = F.relu(self.conv2(self.up2(z))) # (B ,64 , H/8 , W/8 )
    z = F.relu(self.conv3(self.up3(z))) # (B ,32 , H/4 , W/4 )
    z = F.relu(self.conv4(self.up4(z))) # (B ,16 , H/2 , W/2 )
    x_rec = torch.tanh(self.conv5(self.up5(z))) # (B ,3 , H , W )
    return x_rec

# Define the LDM network
class LDM(nn.Module):
  def __init__(self):
    super().__init__()
    # Use a series of convolutional and residual blocks to predict the mean and variance of the reverse diffusion process
    self.conv1 = nn.Conv2d(259 + len(y_tokens), 256 + len(y_tokens), kernel_size=3,
                           stride=1,padding=1) 
                           # input channels: target model features + try-on clothing item tokens + time embedding token
                           # output channels: intermediate features + try-on clothing item tokens
    self.res_blocks = nn.ModuleList([ResBlock(256 + len(y_tokens)) for _ in range(10)]) # 10 residual blocks
    self.conv2 = nn.Conv2d(256 + len(y_tokens), 256, kernel_size=3, stride=1, padding=1) # reduce the feature dimension to 256
    self.conv3 = nn.Conv2d(256, 6, kernel_size=1, stride=1, padding=0) # predict the mean and variance for each RGB channel

  def forward(self, z, y_tokens, x_enc):
    # Input z is a batch of latent codes of size (B, 3, H, W)
    # Input y_tokens is a batch of try-on clothing item tokens of size (B, len(y_tokens))
    # Input x_enc is a batch of target model features of size (B, 256, H/32, W/32)
    # Output mean and var are batches of predicted mean and variance of size (B, 3, H, W)
    
    # Upsample z to match the spatial dimension of x_enc
    z = F.interpolate(z, size=(H/32, W/32), mode='bilinear', align_corners=True) # (B, 3, H/32, W/32)

    # Concatenate z and x_enc along the channel dimension
    z_x = torch.cat([z, x_enc], dim=1) # (B, 259, H/32, W/32)

    # Tile y_tokens along the spatial dimension and concatenate with z_x
    y_tokens = y_tokens.unsqueeze(-1).unsqueeze(-1) # (B, len(y_tokens), 1 , 1 )
    y_tokens = y_tokens.repeat(1 , 1 , H/32 , W/32) # (B , len(y_tokens) , H/32 , W/32)
    z_x_y = torch.cat([z_x , y_tokens] , dim=1) # (B , 259 + len(y_tokens) , H/32 , W/32)

    # Add a time embedding token to z_x_y
    t = torch.randint(0 , T , size=(B ,)) # sample a random time step for each sample in the batch
    t_token = clip.tokenize([str(t_i.item()) for t_i in t]) # convert the time step to a CLIP token
    t_token = t_token.to(device) # move the token to the device
    t_embed = clip_model.encode_text(t_token) # get the CLIP token embedding
    t_embed = t_embed.unsqueeze(-1).unsqueeze(-1) # (B , 1 , 1 , 1 )
    t_embed = t_embed.repeat(1 , 1 , H/32 , W/32) # (B , 1 , H/32 , W/32)
    z_x_y_t = torch.cat([z_x_y , t_embed] , dim=1) # (B , 260 + len(y_tokens) , H/32 , W/32)

    # Pass z_x_y_t through the LDM network
    out = F.relu(self.conv1(z_x_y_t)) # (B , 256 + len(y_tokens) , H/32 , W/32)
    for res_block in self.res_blocks:
      out = res_block(out) # (B , 256 + len(y_tokens) , H/32 , W/32)
    out = F.relu(self.conv2(out)) # (B , 256 , H/32 , W/32)
    out = self.conv3(out) # (B , 6 , H/32 , W/32)

    # Downsample out to match the spatial dimension of z
    out = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=True) # (B, 6, H, W)

    # Split out into mean and variance
    mean = out[:, :3] # (B, 3, H, W)
    var = F.softplus(out[:, 3:]) + eps # (B, 3, H, W)

    return mean, var

# Define the ResBlock module
class ResBlock(nn.Module):
  def __init__(self, channels):
    super().__init__()
    self.conv1 = nn.Conv2d(channels, channels // 2,
                           kernel_size=1,stride=1,padding=0)
                           # reduce the channel dimension by half
    self.conv2 = nn.Conv2d(channels // 2,
                           channels // 2,kernel_size=3,stride=1,padding=1)
                           # keep the same channel dimension
    self.conv3 = nn.Conv2d(channels // 2,
                           channels,kernel_size=