---
title: 2004.02546v3 GANSpace  Discovering Interpretable GAN Controls
date: 2020-04-03
---

# [GANSpace: Discovering Interpretable GAN Controls](http://arxiv.org/abs/2004.02546v3)

authors: Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris


## What, Why and How

[1]: https://arxiv.org/abs/2004.02546 "[2004.02546] GANSpace: Discovering Interpretable GAN Controls - arXiv.org"
[2]: https://arxiv-export-lb.library.cornell.edu/abs/2004.02546 "[2004.02546] GANSpace: Discovering Interpretable GAN Controls"
[3]: https://arxiv.org/pdf/2004.02546.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day[^1^][1] [^2^][2].
- **Why**: The paper aims to provide a better understanding of the latent space and feature space of GANs, and to enable users to manipulate the generated images in intuitive and meaningful ways[^1^][1] [^2^][2].
- **How**: The paper identifies important latent directions based on Principal Components Analysis (PCA) applied either in latent space or feature space. Then, it shows that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, it shows that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. The paper demonstrates the results on different GANs trained on various datasets, and compares them with previous supervised approaches[^1^][1] [^2^][2].

## Main Contributions

[1]: https://wandb.ai/captain-pool/ganspace/reports/GANspace-An-Overview-of-Generative-Adversarial-Networks--VmlldzozODczMDE "GANspace: An Overview of Generative Adversarial Networks"
[2]: https://dvelopery0115.github.io/2021/08/22/GANSpace.html "Summary of 'GANSpace: Discovering Interpretable GAN Controls'"
[3]: https://analyticsindiamag.com/guide-to-ganspace-discovering-interpretable-gan-control/ "Guide to GANSpace: Discovering Interpretable GAN Control"
[4]: https://github.com/harskish/ganspace "GANSpace: Discovering Interpretable GAN Controls - GitHub"

Some of the contributions of this paper are:

- It proposes a simple technique to **analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis**, such as change of viewpoint, aging, lighting, and time of day[^1^][1] [^2^][2] [^3^][4].
- It shows that **semantically meaningful directions in GAN latent spaces can be discovered by applying Principal Components Analysis (PCA)** in latent space for StyleGAN, and feature space for BigGAN[^1^][1] [^2^][2] [^4^][3] [^3^][4].
- It shows how **BigGAN can be modified to allow StyleGAN-like layer-wise style mixing and control without retraining**[^1^][1] [^2^][2] [^3^][4].
- It demonstrates the results on different GANs trained on various datasets, and compares them with previous supervised approaches[^1^][1] [^2^][2] [^3^][4].

## Method Summary

[1]: https://arxiv.org/pdf/2305.14551v1.pdf "E S VARIATIONS IN GAN LATENT S M FACTORIZATION - arXiv.org"
[2]: https://github.com/harskish/ganspace "GANSpace: Discovering Interpretable GAN Controls - GitHub"
[3]: https://arxiv.org/abs/2004.02546 "[2004.02546] GANSpace: Discovering Interpretable GAN Controls - arXiv.org"

Here is a summary of the method section of the paper:

- The paper proposes a simple technique to **analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis**, such as change of viewpoint, aging, lighting, and time of day[^1^][2] [^2^][3].
- The paper shows that **semantically meaningful directions in GAN latent spaces can be discovered by applying Principal Components Analysis (PCA)** in latent space for StyleGAN, and feature space for BigGAN[^1^][2] [^2^][3].
- The paper explains how to perform PCA in different spaces and how to apply the resulting edit directions in a layer-wise manner to achieve fine-grained control over the generated images[^1^][2].
- The paper also shows how **BigGAN can be modified to allow StyleGAN-like layer-wise style mixing and control without retraining** by using a simple linear transformation[^1^][2].

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the GAN model and the latent space
GAN = StyleGAN or BigGAN
latent_space = Z or W or F

# Sample latent vectors from the latent space
latent_vectors = sample(GAN, latent_space, n)

# Apply PCA to the latent vectors and get the principal components
principal_components = PCA(latent_vectors)

# Define the layer range for applying the edit directions
layer_range = all_layers or some_layers

# For each principal component, apply it as an edit direction to the latent vectors in the layer range
for pc in principal_components:
  edited_vectors = latent_vectors + pc * sigma * layer_mask(layer_range)
  # Generate and save images from the edited vectors
  edited_images = GAN(edited_vectors)
  save_images(edited_images, pc, sigma, layer_range)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torchvision
import sklearn.decomposition

# Define the GAN model and the latent space
GAN = StyleGAN or BigGAN # load the pretrained model
latent_space = Z or W or F # choose the latent space according to the model

# Define the number of samples, components, and sigma values
n = 1000000 # number of latent vectors to sample
c = 80 # number of principal components to keep
sigmas = [-6, -4, -2, 0, 2, 4, 6] # list of sigma values for scaling the edit directions

# Define the layer range for applying the edit directions
layer_range = all_layers or some_layers # choose the layer range according to the model and the desired granularity

# Sample latent vectors from the latent space
latent_vectors = sample(GAN, latent_space, n) # use torch.randn or GAN.mapping for Z or W space, use GAN.truncation for F space

# Apply PCA to the latent vectors and get the principal components
pca = sklearn.decomposition.PCA(n_components=c)
pca.fit(latent_vectors)
principal_components = pca.components_

# Define a function to create a layer mask for applying the edit directions
def layer_mask(layer_range):
  # Create a boolean mask with True values for the layers in the layer range and False values for the others
  mask = torch.zeros(GAN.num_layers, dtype=torch.bool)
  mask[layer_range] = True
  # Repeat the mask along the batch dimension and return it
  return mask.repeat(n, 1)

# For each principal component, apply it as an edit direction to the latent vectors in the layer range
for i, pc in enumerate(principal_components):
  # Reshape the principal component to match the shape of the latent vectors
  pc = pc.reshape(latent_vectors.shape)
  # For each sigma value, scale the principal component and add it to the latent vectors in the layer range
  for sigma in sigmas:
    edited_vectors = latent_vectors.clone()
    edited_vectors[:, layer_mask] += pc[:, layer_mask] * sigma
    # Generate and save images from the edited vectors
    edited_images = GAN(edited_vectors) # use GAN.synthesis for Z or W space, use GAN for F space
    save_images(edited_images, i, sigma, layer_range) # use torchvision.utils.save_image or any other image saving function
```