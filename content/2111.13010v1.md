---
title: 2111.13010v1 Attribute-specific Control Units in StyleGAN for Fine-grained Image Manipulation
date: 2021-11-14
---

# [Attribute-specific Control Units in StyleGAN for Fine-grained Image Manipulation](http://arxiv.org/abs/2111.13010v1)

authors: Rui Wang, Jian Chen, Gang Yu, Li Sun, Changqian Yu, Changxin Gao, Nong Sang


## What, Why and How

[1]: https://arxiv.org/pdf/2111.13010v1.pdf "Attribute-specific Control Units in StyleGAN for Fine-grained Image ..."
[2]: https://arxiv.org/abs/2111.13010 "[2111.13010] Attribute-specific Control Units in StyleGAN for Fine ..."
[3]: http://export.arxiv.org/abs/2011.13010v1 "[2011.13010v1] Quantum Correlations in Neutrino Oscillation: Coherence ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to discover and manipulate attribute-specific control units in StyleGAN for fine-grained image manipulation, such as local attribute translation.
- **Why**: The paper aims to address the issue of limited semantic and spatial manipulation precision in the existing latent spaces of StyleGAN, which hinders fine-grained image manipulation tasks.
- **How**: The paper introduces the concept of control units, which consist of multiple channels of feature maps and modulation styles that are related to a specific attribute. The paper also proposes a simple yet effective method to detect and manipulate these control units by moving the modulation style along a sparse direction vector and replacing the filter-wise styles used to compute the feature maps. The paper evaluates the proposed method on various face attribute manipulation tasks and compares it with state-of-the-art methods. The paper also shows the manipulation results of real images using the proposed method.

## Main Contributions

The paper claims the following contributions:

- It discovers attribute-specific control units in StyleGAN, which consist of multiple channels of feature maps and modulation styles that are related to a specific attribute.
- It proposes a simple yet effective method to detect and manipulate these control units by moving the modulation style along a sparse direction vector and replacing the filter-wise styles used to compute the feature maps.
- It demonstrates that the proposed method can achieve semantic and spatial disentangled controls for fine-grained image manipulation tasks, such as local attribute translation.
- It shows that the proposed method performs favorably against the state-of-the-art methods in terms of qualitative and quantitative evaluations.
- It applies the proposed method to manipulate real images and shows the effectiveness of the method.

## Method Summary

The method section of the paper consists of three subsections: Control Units in StyleGAN, Control Unit Detection and Manipulation, and Implementation Details. Here is a summary of each subsection:

- Control Units in StyleGAN: This subsection introduces the concept of control units, which are groups of channels of feature maps and modulation styles that are related to a specific attribute. The subsection also explains how StyleGAN generates images using the style-based generator and the adaptive instance normalization (AdaIN) operation. The subsection shows that manipulating a single channel of modulation styles or feature maps is not sufficient to achieve fine-grained image manipulation, and that manipulating multiple channels of both modulation styles and feature maps is necessary to obtain semantic and spatial disentangled controls.
- Control Unit Detection and Manipulation: This subsection proposes a method to detect and manipulate the control units in StyleGAN. The method consists of two steps: sparse direction vector estimation and filter-wise style replacement. The sparse direction vector estimation step aims to find a sparse direction vector in the modulation style space that corresponds to a specific attribute. The filter-wise style replacement step aims to replace the filter-wise styles used to compute the feature maps with the ones moved along the sparse direction vector. The subsection also discusses how to select the layers and channels for manipulation, and how to adjust the manipulation intensity.
- Implementation Details: This subsection provides the details of the implementation of the proposed method, such as the network architecture, the training data, the hyperparameters, and the evaluation metrics. The subsection also describes how to apply the proposed method to real images using an encoder network.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a latent code w and an attribute a
# Output: a manipulated image I'

# Step 1: Control Unit Detection
# Find a sparse direction vector d_a in the modulation style space that corresponds to the attribute a
d_a = estimate_sparse_direction(w, a)

# Step 2: Control Unit Manipulation
# For each layer l in the selected layers L:
  # Move the modulation style w_l along the direction vector d_a with a scale factor alpha
  w_l' = w_l + alpha * d_a
  
  # Replace the filter-wise styles used to compute the feature maps with w_l'
  f_l' = AdaIN(f_l, w_l')
  
# Step 3: Image Generation
# Generate the manipulated image I' using the modified feature maps f_l'
I' = generate_image(f_l')
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a latent code w and an attribute a
# Output: a manipulated image I'

# Define the StyleGAN generator G and the encoder network E
G = StyleGAN()
E = Encoder()

# Define the attribute classifier C and the attribute labels Y
C = Classifier()
Y = {0: 'no beard', 1: 'full beard'}

# Define the selected layers L and the number of channels N for each layer
L = [4, 6, 8, 10, 12, 14]
N = [16, 32, 64, 128, 256, 512]

# Define the scale factor alpha and the threshold tau for manipulation intensity and sparsity
alpha = 0.5
tau = 0.01

# Step 1: Control Unit Detection
# Find a sparse direction vector d_a in the modulation style space that corresponds to the attribute a
def estimate_sparse_direction(w, a):
  # Initialize d_a as a zero vector with the same dimension as w
  d_a = zeros_like(w)
  
  # For each layer l in L:
    # Initialize d_l as a zero vector with N[l] elements
    d_l = zeros(N[l])
    
    # For each channel c in range(N[l]):
      # Move w_l[c] along the positive and negative unit directions e_c with a small step size epsilon
      w_l_pos = w_l.copy()
      w_l_neg = w_l.copy()
      w_l_pos[c] += epsilon * e_c
      w_l_neg[c] -= epsilon * e_c
      
      # Generate two images I_pos and I_neg using G with the modified w_l
      I_pos = G(w_l_pos)
      I_neg = G(w_l_neg)
      
      # Compute the attribute scores S_pos and S_neg using C for both images
      S_pos = C(I_pos)[a]
      S_neg = C(I_neg)[a]
      
      # Compute the difference D_c between S_pos and S_neg
      D_c = S_pos - S_neg
      
      # If D_c is larger than tau, set d_l[c] to D_c
      if D_c > tau:
        d_l[c] = D_c
        
    # Normalize d_l to have unit norm
    d_l = normalize(d_l)
    
    # Assign d_l to d_a[l]
    d_a[l] = d_l
    
  # Return d_a
  return d_a

# Step 2: Control Unit Manipulation
# For each layer l in the selected layers L:
  # Move the modulation style w_l along the direction vector d_a with a scale factor alpha
  w_l' = w_l + alpha * d_a[l]
  
  # Replace the filter-wise styles used to compute the feature maps with w_l'
  f_l' = AdaIN(f_l, w_l')
  
# Step 3: Image Generation
# Generate the manipulated image I' using the modified feature maps f_l'
I' = G(f_l')

# If the input is a real image I instead of a latent code w:
# Use E to encode I into w and then apply the same steps as above
w = E(I)
I' = manipulate_image(w, a)
```