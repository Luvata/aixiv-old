---
title: 2102.02766v1 Designing an Encoder for StyleGAN Image Manipulation
date: 2021-02-03
---

# [Designing an Encoder for StyleGAN Image Manipulation](http://arxiv.org/abs/2102.02766v1)

authors: Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2102.02766 "Designing an Encoder for StyleGAN Image Manipulation"
[2]: https://arxiv.org/pdf/2102.02766.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2103.02766v1 "Title: PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a new encoder for StyleGAN image manipulation that balances the tradeoffs between reconstruction accuracy, editability, and perceptual quality.
- **Why**: The paper aims to address the challenge of editing real images using pre-trained unconditional generators, such as StyleGAN, which require the inversion of the images into their latent space. Existing methods either produce distorted inversions that are easy to edit, or accurate inversions that are hard to edit or look unnatural.
- **How**: The paper studies the latent space of StyleGAN and identifies two principles for designing encoders: (1) using a multi-scale architecture that leverages both coarse and fine features, and (2) using a perceptual loss that encourages the inversions to be close to regions that StyleGAN was trained on. The paper then presents an encoder based on these principles and evaluates its performance on various domains and editing tasks.

## Main Contributions

[1]: https://arxiv.org/abs/2102.02766 "Designing an Encoder for StyleGAN Image Manipulation"
[2]: https://arxiv.org/pdf/2102.02766.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2103.02766v1 "Title: PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds"

According to the paper[^1^][1], the main contributions are:

- **Identifying and analyzing** the tradeoffs between distortion, editability, and perception in the StyleGAN latent space, and showing how they affect the quality of real-image editing.
- **Proposing two principles** for designing encoders that can control the proximity of the inversions to regions that StyleGAN was trained on, and thus balance the tradeoffs.
- **Presenting a new encoder** based on the proposed principles that achieves superior real-image editing quality with only a small reconstruction accuracy drop, compared to existing methods.
- **Evaluating the encoder** on various domains and editing tasks, such as style mixing, attribute manipulation, and semantic editing, and demonstrating its effectiveness and robustness.

## Method Summary

[1]: https://arxiv.org/abs/2102.02766 "Designing an Encoder for StyleGAN Image Manipulation"
[2]: https://arxiv.org/pdf/2102.02766.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2103.02766v1 "Title: PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first introduces the **StyleGAN architecture** and its latent space, which consists of two levels: the **W space** and the **S space**. The W space is a disentangled space that controls the high-level attributes of the image, while the S space is an entangled space that controls the low-level details of the image. The paper also defines the **W+ space**, which is an extended version of the W space that allows for finer control over the image features.
- The paper then analyzes the **tradeoffs** between distortion, editability, and perception in the StyleGAN latent space. Distortion measures how well the inversion reconstructs the input image, editability measures how well the inversion preserves the semantic meaning of the image after editing, and perception measures how natural and realistic the inversion looks. The paper shows that these tradeoffs depend on which latent space and which loss function are used for inversion. Specifically, the paper shows that:
  - Inverting to the W space leads to low distortion but low editability and low perception, as the inversions tend to be blurry and lose fine details.
  - Inverting to the S space leads to high distortion but high editability and high perception, as the inversions tend to be sharp and preserve fine details, but also introduce artifacts and noise that are hard to remove.
  - Inverting to the W+ space leads to a balance between distortion and editability, but low perception, as the inversions tend to be overfitted and deviate from regions that StyleGAN was trained on.
  - Using a pixel-wise loss function leads to low distortion but low editability and low perception, as it encourages exact reconstruction but ignores semantic consistency and naturalness.
  - Using a perceptual loss function leads to high distortion but high editability and high perception, as it encourages semantic consistency and naturalness but ignores exact reconstruction.
- Based on these tradeoffs, the paper proposes two principles for designing encoders that can balance them:
  - **Principle 1**: Use a multi-scale architecture that leverages both coarse and fine features from different layers of StyleGAN. This allows for capturing both global and local information of the image, and thus improving both distortion and editability.
  - **Principle 2**: Use a perceptual loss function that encourages the inversions to be close to regions that StyleGAN was trained on. This allows for improving perception by avoiding overfitting and generating natural-looking images.
- Based on these principles, the paper presents a new encoder called **pSp** (perceptual StyleGAN encoder). The encoder consists of three components: (1) a feature extractor that extracts multi-scale features from an input image using a pre-trained VGG network, (2) a mapping network that maps each feature map to a corresponding latent code in W+ space using a fully connected layer, and (3) a perceptual loss function that computes the distance between the feature maps of the input image and the reconstructed image using a pre-trained VGG network. The encoder is trained by minimizing this perceptual loss function using gradient descent.
- The paper evaluates the performance of pSp on various domains and editing tasks, such as style mixing, attribute manipulation, and semantic editing. The paper compares pSp with existing methods, such as e4e (encoder for editing), ReStyle (refined StyleGAN inversion), IDInvert (in-domain GAN inversion), and SEAN (semantic region-adaptive normalization). The paper shows that pSp achieves superior real-image editing quality with only a small reconstruction accuracy drop.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the StyleGAN architecture and its latent spaces
StyleGAN = load_pretrained_model()
W_space = StyleGAN.get_W_space()
S_space = StyleGAN.get_S_space()
W_plus_space = StyleGAN.get_W_plus_space()

# Define the feature extractor using a pre-trained VGG network
VGG = load_pretrained_model()
feature_extractor = VGG.get_feature_maps()

# Define the mapping network that maps each feature map to a latent code
mapping_network = FullyConnectedLayer()

# Define the perceptual loss function using a pre-trained VGG network
perceptual_loss = VGG.get_perceptual_distance()

# Define the encoder that combines the feature extractor, the mapping network, and the perceptual loss function
encoder = pSp(feature_extractor, mapping_network, perceptual_loss)

# Train the encoder by minimizing the perceptual loss function
for input_image in training_data:
  # Extract multi-scale features from the input image
  features = feature_extractor(input_image)
  # Map each feature map to a latent code in W+ space
  latent_code = mapping_network(features)
  # Reconstruct the image using StyleGAN and the latent code
  reconstructed_image = StyleGAN(latent_code)
  # Compute the perceptual loss between the input image and the reconstructed image
  loss = perceptual_loss(input_image, reconstructed_image)
  # Update the encoder parameters using gradient descent
  encoder.update(loss)

# Use the encoder for real-image editing tasks
for input_image in test_data:
  # Invert the input image to W+ space using the encoder
  latent_code = encoder(input_image)
  # Perform editing on the latent code using various techniques, such as style mixing, attribute manipulation, or semantic editing
  edited_latent_code = edit(latent_code)
  # Generate the edited image using StyleGAN and the edited latent code
  edited_image = StyleGAN(edited_latent_code)
  # Display the edited image
  show(edited_image)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the StyleGAN architecture and its latent spaces
StyleGAN = load_pretrained_model() # Load a pre-trained StyleGAN model from https://github.com/NVlabs/stylegan2-ada-pytorch
W_space = StyleGAN.get_W_space() # Get the W space of StyleGAN, which is a 512-dimensional disentangled space
S_space = StyleGAN.get_S_space() # Get the S space of StyleGAN, which is a 18x512-dimensional entangled space
W_plus_space = StyleGAN.get_W_plus_space() # Get the W+ space of StyleGAN, which is an extended version of W space that allows for finer control over the image features

# Define the feature extractor using a pre-trained VGG network
VGG = torchvision.models.vgg16(pretrained=True) # Load a pre-trained VGG-16 model from https://pytorch.org/vision/stable/models.html
feature_extractor = VGG.features # Get the feature maps of VGG-16, which are 64x224x224, 64x112x112, 128x56x56, 256x28x28, 512x14x14, and 512x7x7

# Define the mapping network that maps each feature map to a latent code
mapping_network = torch.nn.ModuleDict() # Create a module dictionary to store the mapping layers for each feature map
for i in range(6): # Loop over the six feature maps
  mapping_network[str(i)] = torch.nn.Linear(feature_extractor[i].num_features, W_space.dim) # Create a fully connected layer that maps the number of features of each feature map to the dimension of W space

# Define the perceptual loss function using a pre-trained VGG network
perceptual_loss = torch.nn.L1Loss() # Use the L1 loss function to compute the perceptual distance between two images

# Define the encoder that combines the feature extractor, the mapping network, and the perceptual loss function
encoder = pSp(feature_extractor, mapping_network, perceptual_loss) # Create an instance of pSp class

# Train the encoder by minimizing the perceptual loss function
optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01) # Use Adam optimizer with learning rate 0.01
training_data = load_training_data() # Load the training data from https://github.com/omertov/encoder4editing/tree/master/data
for epoch in range(100): # Loop over 100 epochs
  for input_image in training_data: # Loop over each input image in the training data
    optimizer.zero_grad() # Zero out the gradients
    # Extract multi-scale features from the input image
    features = feature_extractor(input_image)
    # Map each feature map to a latent code in W+ space
    latent_code = []
    for i in range(6): # Loop over the six feature maps
      latent_code.append(mapping_network[str(i)](features[i])) # Apply the mapping layer to each feature map and append it to the latent code list
    latent_code = torch.stack(latent_code) # Stack the latent code list into a tensor of shape 18x512
    # Reconstruct the image using StyleGAN and the latent code
    reconstructed_image = StyleGAN(latent_code)
    # Compute the perceptual loss between the input image and the reconstructed image
    loss = perceptual_loss(input_image, reconstructed_image)
    # Update the encoder parameters using gradient descent
    loss.backward() # Compute the gradients
    optimizer.step() # Update the parameters

# Use the encoder for real-image editing tasks
test_data = load_test_data() # Load the test data from https://github.com/omertov/encoder4editing/tree/master/data/test_images
for input_image in test_data: # Loop over each input image in the test data
  # Invert the input image to W+ space using the encoder
  latent_code = encoder(input_image)
  # Perform editing on the latent code using various techniques, such as style mixing, attribute manipulation, or semantic editing
  edited_latent_code = edit(latent_code)
  # Generate the edited image using StyleGAN and the edited latent code
  edited_image = StyleGAN(edited_latent_code)
  # Display the edited image
  show(edited_image)
```