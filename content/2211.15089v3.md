---
title: 2211.15089v3 Continuous diffusion for categorical data
date: 2022-11-16
---

# [Continuous diffusion for categorical data](http://arxiv.org/abs/2211.15089v3)

authors: Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, RÃ©mi Leblond, Will Grathwohl, Jonas Adler


## What, Why and How

[1]: https://arxiv.org/abs/2211.15089 "[2211.15089] Continuous diffusion for categorical data - arXiv.org"
[2]: https://arxiv.org/pdf/2211.15089v3.pdf "arXiv:2211.15089v3 [cs.CL] 15 Dec 2022"
[3]: http://arxiv-export3.library.cornell.edu/abs/2211.15089 "[2211.15089] Continuous diffusion for categorical data"

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a framework for modelling categorical data (such as language) with diffusion models that are continuous both in time and input space. The framework is called CDCD (Continuous Diffusion for Categorical Data).
- **Why**: The paper argues that diffusion models are a powerful paradigm for generative modelling of perceptual signals (such as images and sound) that are continuous in nature, but existing diffusion-inspired alternatives for discrete and categorical data have limitations. The paper aims to preserve the benefits of continuous diffusion models while adapting them to categorical data.
- **How**: The paper introduces a novel way of encoding categorical data into continuous vectors using a learnable embedding function, and a way of decoding them back using a softmax function. The paper also proposes a way of defining the noise process and the reverse process for continuous diffusion models on categorical data, and a way of training them using denoising score matching. The paper demonstrates the efficacy of CDCD on several language modelling tasks, such as character-level text generation, word-level text generation, and code completion.

## Main Contributions

According to the paper at , the main contributions are:

- A novel framework for modelling categorical data with continuous diffusion models, called CDCD.
- A learnable embedding function that maps categorical data to continuous vectors, and a softmax function that maps them back.
- A noise process and a reverse process for continuous diffusion models on categorical data, based on the entropy of the model predictions.
- A training method for CDCD using denoising score matching, with a regularization term to encourage diversity in the embeddings.
- An empirical evaluation of CDCD on several language modelling tasks, showing that it outperforms existing diffusion-inspired alternatives and achieves competitive results with state-of-the-art autoregressive models.

## Method Summary

The method section of the paper at  describes the following steps:

- Encoding and decoding categorical data: The paper defines a learnable embedding function that maps each possible value of a categorical variable to a continuous vector, and a softmax function that maps each continuous vector to a probability distribution over the possible values. The paper also defines a temperature parameter that controls the sharpness of the softmax function.
- Noise process and reverse process: The paper defines a noise process that adds Gaussian noise to the continuous vectors at each diffusion step, and a reverse process that removes the noise at each denoising step. The paper also defines a heuristic to determine the variance of the noise at each step, based on the entropy of the model predictions.
- Training CDCD: The paper proposes to train CDCD using denoising score matching, which is a method that minimizes the squared difference between the score (gradient of the log-density) of the data and the score of the model. The paper also adds a regularization term to the loss function that encourages diversity in the embeddings by penalizing similar embeddings for different values.
- Evaluation: The paper evaluates CDCD on several language modelling tasks, such as character-level text generation, word-level text generation, and code completion. The paper compares CDCD with existing diffusion-inspired alternatives, such as discrete diffusion models and variational diffusion models, and with state-of-the-art autoregressive models, such as GPT-3 and Transformer-XL. The paper reports results on various metrics, such as perplexity, bits per character, bits per word, accuracy, and diversity.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the embedding function E and the softmax function S
E = learnable_function()
S = softmax_function()

# Define the noise process and the reverse process
def noise_process(x, t):
  # x is a continuous vector, t is the diffusion step
  # sigma_t is the variance of the noise at step t
  # eta_t is a Gaussian random variable with zero mean and unit variance
  sigma_t = heuristic_function(S(x), t)
  return x + sqrt(sigma_t) * eta_t

def reverse_process(y, t):
  # y is a noisy continuous vector, t is the denoising step
  # sigma_t is the variance of the noise at step t
  return (y - sqrt(sigma_t) * eta_t) / (1 - sigma_t)

# Define the score function F
F = learnable_function()

# Define the loss function L
def L(x, y, t):
  # x is a continuous vector, y is a noisy continuous vector, t is the diffusion/denoising step
  # lambda is a regularization coefficient
  return (F(y, t) - (y - x) / sigma_t) ** 2 + lambda * regularization_term(E)

# Train CDCD using denoising score matching
for each batch of categorical data:
  # Encode the categorical data into continuous vectors using E
  x = E(data)
  # Apply the noise process to get noisy continuous vectors
  y = noise_process(x, t)
  # Compute the loss and update the parameters of E and F using gradient descent
  loss = L(x, y, t)
  update_parameters(E, F, loss)

# Generate categorical data using CDCD
# Initialize a continuous vector with Gaussian noise
z = Gaussian_noise()
# Apply the reverse process for T steps to get a denoised continuous vector
for t in range(T):
  z = reverse_process(z, t)
# Decode the continuous vector into a categorical variable using S
data = S(z)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define the hyperparameters
T = 100 # number of diffusion/denoising steps
N = 256 # number of possible values for each categorical variable
D = 64 # dimension of the continuous vectors
K = 16 # dimension of the keys and queries for the attention mechanism
H = 256 # dimension of the hidden states for the LSTM cells
L = 4 # number of layers for the LSTM cells
B = 32 # batch size
LR = 0.001 # learning rate
LAMBDA = 0.01 # regularization coefficient

# Define the embedding function E and the softmax function S
class Embedding(nn.Module):
  def __init__(self):
    super(Embedding, self).__init__()
    # Initialize a learnable embedding matrix with shape (N, D)
    self.embedding_matrix = nn.Parameter(torch.randn(N, D))
  
  def forward(self, data):
    # data is a tensor of shape (B, L) containing categorical variables
    # Return a tensor of shape (B, L, D) containing continuous vectors
    return self.embedding_matrix[data]

class Softmax(nn.Module):
  def __init__(self):
    super(Softmax, self).__init__()
    # Initialize a temperature parameter with a small positive value
    self.temperature = nn.Parameter(torch.tensor(0.1))
  
  def forward(self, z):
    # z is a tensor of shape (B, L, D) containing continuous vectors
    # Return a tensor of shape (B, L, N) containing probability distributions over possible values
    return torch.softmax(z / self.temperature, dim=-1)

E = Embedding()
S = Softmax()

# Define the noise process and the reverse process
def noise_process(x, t):
  # x is a tensor of shape (B, L, D) containing continuous vectors, t is an integer representing the diffusion step
  # sigma_t is a tensor of shape (B, L) containing the variance of the noise at each step
  # eta_t is a tensor of shape (B, L, D) containing Gaussian random variables with zero mean and unit variance
  sigma_t = heuristic_function(S(x), t)
  eta_t = torch.randn_like(x)
  return x + torch.sqrt(sigma_t.unsqueeze(-1)) * eta_t

def reverse_process(y, t):
  # y is a tensor of shape (B, L, D) containing noisy continuous vectors, t is an integer representing the denoising step
  # sigma_t is a tensor of shape (B, L) containing the variance of the noise at each step
  sigma_t = heuristic_function(S(y), t)
  return (y - torch.sqrt(sigma_t.unsqueeze(-1)) * eta_t) / (1 - sigma_t.unsqueeze(-1))

# Define the heuristic function to compute sigma_t
def heuristic_function(p, t):
  # p is a tensor of shape (B, L, N) containing probability distributions over possible values, t is an integer representing the diffusion/denoising step
  # alpha is a scalar representing the target entropy increase per step
  alpha = np.log(N) / T
  # H is a tensor of shape (B, L) containing the entropy of p at each position
  H = -torch.sum(p * torch.log(p + 1e-9), dim=-1)
  # Return a tensor of shape (B, L) containing sigma_t at each position
  return torch.exp(2 * alpha * t - H)

# Define the score function F using an LSTM-based neural network with attention mechanism
class Score(nn.Module):
  def __init__(self):
    super(Score, self).__init__()
    # Initialize four LSTM cells with input size D and hidden size H for each layer
    self.lstm_cells = [nn.LSTMCell(D, H) for _ in range(L)]
    # Initialize four linear layers with input size H and output size K for each layer to compute the keys for the attention mechanism
    self.key_layers = [nn.Linear(H, K) for _ in range(L)]
    # Initialize four linear layers with input size H and output size K for each layer to compute the queries for the attention mechanism
    self.query_layers = [nn.Linear(H, K) for _ in range(L)]
    # Initialize four linear layers with input size H and output size H for each layer to compute the values for the attention mechanism
    self.value_layers = [nn.Linear(H, H) for _ in range(L)]
    # Initialize a linear layer with input size H and output size D to compute the final score
    self.output_layer = nn.Linear(H, D)
  
  def forward(self, y, t):
    # y is a tensor of shape (B, L, D) containing noisy continuous vectors, t is an integer representing the denoising step
    # h and c are lists of tensors of shape (B, H) containing the hidden states and cell states for each layer
    h = [torch.zeros(B, H) for _ in range(L)]
    c = [torch.zeros(B, H) for _ in range(L)]
    # score is a tensor of shape (B, L, D) containing the score for each position
    score = torch.zeros(B, L, D)
    # Loop over the positions from left to right
    for i in range(L):
      # x is a tensor of shape (B, D) containing the input for the LSTM cells at position i
      x = y[:, i, :]
      # Loop over the layers from bottom to top
      for j in range(L):
        # Update the hidden state and cell state for layer j using the LSTM cell
        h[j], c[j] = self.lstm_cells[j](x, (h[j], c[j]))
        # Compute the key, query and value for layer j using the linear layers
        key = self.key_layers[j](h[j])
        query = self.query_layers[j](h[j])
        value = self.value_layers[j](h[j])
        # Compute the attention weight for layer j using the dot product between the key and the query
        weight = torch.exp(torch.sum(key * query, dim=-1))
        # Normalize the weight across all positions using softmax
        weight = weight / torch.sum(weight, dim=-1)
        # Compute the context vector for layer j using the weighted sum of the values
        context = torch.sum(weight.unsqueeze(-1) * value.unsqueeze(1), dim=0)
        # Compute the output for layer j using a residual connection and a tanh activation
        output = torch.tanh(x + context)
        # Update the input for the next layer with the output
        x = output
      # Compute the score for position i using the output layer
      score[:, i, :] = self.output_layer(x)
    # Return the score tensor
    return score

F = Score()

# Define the loss function L
def L(x, y, t):
  # x is a tensor of shape (B, L, D) containing continuous vectors, y is a tensor of shape (B, L, D) containing noisy continuous vectors, t is an integer representing the diffusion/denoising step
  # sigma_t is a tensor of shape (B, L) containing the variance of the noise at each step
  sigma_t = heuristic_function(S(x), t)
  # Return a scalar representing the loss value
  return torch.mean((F(y, t) - (y - x) / sigma_t.unsqueeze(-1)) ** 2 + LAMBDA * regularization_term(E))

# Define the regularization term to encourage diversity in the embeddings
def regularization_term(E):
  # E is an instance of Embedding class
  # Return a scalar representing the regularization term
  return torch.mean(torch.pdist(E.embedding_matrix))

# Initialize an optimizer to update the parameters of E and F using Adam algorithm
optimizer = optim.Adam([E.parameters(), F.parameters()], lr=LR)

# Train CDCD using denoising score matching
for each epoch:
  for each batch of categorical data:
    # Encode the categorical data into continuous vectors using E
    x = E(data)
    # Sample a random diffusion/denoising step t from {0,...,T-1}
    t = np.random.randint(T)
    # Apply the noise process to get noisy continuous vectors
    y = noise_process(x, t)
    # Compute the loss and update the parameters of E and F using gradient descent
    loss = L(x, y, t)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Generate categorical data using CDCD
# Initialize a continuous vector with Gaussian noise of shape (B, L, D)
z = torch.randn(B, L, D)
# Apply the reverse process for T steps to get a denoised continuous vector
for t in range(T-1,-1,-1):
  z = reverse_process(z, t)
# Decode the continuous vector into a categorical variable using S and argmax function
data = torch.argmax(S(z), dim=-1)
```