---
title: 2209.13430v2 UniCLIP  Unified Framework for Contrastive Language-Image Pre-training
date: 2022-09-14
---

# [UniCLIP: Unified Framework for Contrastive Language-Image Pre-training](http://arxiv.org/abs/2209.13430v2)

authors: Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, Junmo Kim


## What, Why and How

[1]: https://arxiv.org/abs/2209.13430 "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training"
[2]: https://arxiv.org/pdf/2209.13430.pdf "arXiv:2209.13430v2 [cs.CV] 31 Oct 2022"
[3]: http://export.arxiv.org/abs/2209.13430 "[2209.13430] UniCLIP: Unified Framework for Contrastive Language-Image ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes UniCLIP, a unified framework for contrastive language-image pre-training that integrates the contrastive loss of both inter-domain pairs (image-text) and intra-domain pairs (image-image) into a single universal space.
- **Why**: The paper aims to overcome the limitation of previous vision-language pre-training methods that define the contrastive loss for inter-domain and intra-domain pairs separately, which overlooks many feasible combinations of supervision and leads to suboptimal performance.
- **How**: The paper resolves the discrepancies that occur when integrating contrastive loss between different domains by the three key components of UniCLIP: (1) augmentation-aware feature embedding, which embeds different augmentations of the same image or text into similar vectors, (2) MP-NCE loss, which maximizes the mutual information between positive pairs across domains while minimizing it between negative pairs within domains, and (3) domain dependent similarity measure, which adjusts the similarity scores between different domains based on their distributions. The paper demonstrates that UniCLIP outperforms previous methods on various single- and multi-modality downstream tasks.

## Main Contributions

[1]: https://arxiv.org/abs/2209.13430 "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training"
[2]: https://arxiv.org/pdf/2209.13430.pdf "arXiv:2209.13430v2 [cs.CV] 31 Oct 2022"
[3]: http://export.arxiv.org/abs/2209.13430 "[2209.13430] UniCLIP: Unified Framework for Contrastive Language-Image ..."

According to the paper at [^1^][1], the main contributions are:

- **UniCLIP**, a unified framework for contrastive language-image pre-training that integrates the contrastive loss of both inter-domain pairs and intra-domain pairs into a single universal space.
- **Augmentation-aware feature embedding**, which embeds different augmentations of the same image or text into similar vectors, enabling the model to learn robust and invariant features across domains.
- **MP-NCE loss**, which maximizes the mutual information between positive pairs across domains while minimizing it between negative pairs within domains, leading to a more data-efficient and scalable objective.
- **Domain dependent similarity measure**, which adjusts the similarity scores between different domains based on their distributions, alleviating the domain discrepancy problem and improving the alignment quality.
- **Empirical results** that show UniCLIP outperforms previous vision-language pre-training methods on various single- and multi-modality downstream tasks, such as image-text retrieval, image captioning, visual question answering, and visual entailment.

## Method Summary

[1]: https://arxiv.org/abs/2209.13430 "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training"
[2]: https://arxiv.org/pdf/2209.13430.pdf "arXiv:2209.13430v2 [cs.CV] 31 Oct 2022"
[3]: http://export.arxiv.org/abs/2209.13430 "[2209.13430] UniCLIP: Unified Framework for Contrastive Language-Image ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper adopts a vision-language encoder that consists of a convolutional neural network (CNN) for image feature extraction and a transformer network for text feature extraction, followed by a projection head that maps the features into a common embedding space.
- The paper defines the contrastive loss for inter-domain pairs (image-text) and intra-domain pairs (image-image) in the same embedding space, using different augmentations of the same image or text as positive pairs and different images or texts as negative pairs.
- The paper introduces three key components to improve the contrastive learning in the unified space: (1) augmentation-aware feature embedding, which embeds different augmentations of the same image or text into similar vectors by adding an auxiliary loss that minimizes their distance, (2) MP-NCE loss, which maximizes the mutual information between positive pairs across domains while minimizing it between negative pairs within domains by using multiple positives and negatives per sample, and (3) domain dependent similarity measure, which adjusts the similarity scores between different domains based on their distributions by using a learnable scaling factor.
- The paper pre-trains UniCLIP on a large-scale uncurated dataset of image-text pairs collected from the web, and evaluates it on various single- and multi-modality downstream tasks.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a vision-language encoder with a CNN and a transformer
encoder = VisionLanguageEncoder(CNN, transformer)

# Define a projection head that maps the features into a common embedding space
projection_head = ProjectionHead()

# Define an augmentation-aware feature embedding module that minimizes the distance between different augmentations of the same image or text
augmentation_aware_embedding = AugmentationAwareEmbedding()

# Define an MP-NCE loss that maximizes the mutual information between positive pairs across domains and minimizes it between negative pairs within domains
mp_nce_loss = MPNCELoss()

# Define a domain dependent similarity measure that adjusts the similarity scores between different domains based on their distributions
domain_dependent_similarity = DomainDependentSimilarity()

# Pre-train UniCLIP on a large-scale uncurated dataset of image-text pairs
for image, text in dataset:
  # Apply different augmentations to the image and text
  image1, image2 = augment_image(image)
  text1, text2 = augment_text(text)

  # Encode the image and text features using the encoder
  image_feature1 = encoder(image1)
  image_feature2 = encoder(image2)
  text_feature1 = encoder(text1)
  text_feature2 = encoder(text2)

  # Project the features into a common embedding space using the projection head
  image_embedding1 = projection_head(image_feature1)
  image_embedding2 = projection_head(image_feature2)
  text_embedding1 = projection_head(text_feature1)
  text_embedding2 = projection_head(text_feature2)

  # Embed different augmentations of the same image or text into similar vectors using the augmentation-aware feature embedding module
  augmentation_aware_embedding(image_embedding1, image_embedding2)
  augmentation_aware_embedding(text_embedding1, text_embedding2)

  # Compute the contrastive loss for inter-domain and intra-domain pairs using the MP-NCE loss and the domain dependent similarity measure
  inter_domain_loss = mp_nce_loss(domain_dependent_similarity(image_embedding1, text_embedding1), domain_dependent_similarity(image_embedding2, text_embedding2))
  intra_domain_loss = mp_nce_loss(domain_dependent_similarity(image_embedding1, image_embedding2), domain_dependent_similarity(text_embedding1, text_embedding2))

  # Optimize the model parameters to minimize the total contrastive loss
  total_loss = inter_domain_loss + intra_domain_loss
  optimizer.zero_grad()
  total_loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
batch_size = 256 # The number of image-text pairs per batch
image_size = 224 # The size of the input image
text_length = 64 # The maximum length of the input text
embedding_dim = 512 # The dimension of the common embedding space
temperature = 0.07 # The temperature parameter for the contrastive loss
num_positives = 2 # The number of positive samples per sample
num_negatives = 255 # The number of negative samples per sample
alpha = 0.2 # The scaling factor for the domain dependent similarity measure

# Define a vision-language encoder with a CNN and a transformer
# Use a ResNet-50 model for the CNN and a BERT model for the transformer
encoder = VisionLanguageEncoder(torchvision.models.resnet50(pretrained=True), transformers.BertModel.from_pretrained('bert-base-uncased'))

# Define a projection head that maps the features into a common embedding space
# Use a linear layer followed by a layer normalization
projection_head = ProjectionHead(torch.nn.Linear(encoder.output_dim, embedding_dim), torch.nn.LayerNorm(embedding_dim))

# Define an augmentation-aware feature embedding module that minimizes the distance between different augmentations of the same image or text
# Use a mean squared error loss as the auxiliary loss
augmentation_aware_embedding = AugmentationAwareEmbedding(torch.nn.MSELoss())

# Define an MP-NCE loss that maximizes the mutual information between positive pairs across domains and minimizes it between negative pairs within domains
# Use a softmax function to compute the probabilities of positive and negative pairs
mp_nce_loss = MPNCELoss()

# Define a domain dependent similarity measure that adjusts the similarity scores between different domains based on their distributions
# Use a learnable scaling factor to multiply the similarity scores between image and text embeddings
domain_dependent_similarity = DomainDependentSimilarity(torch.nn.Parameter(torch.tensor(alpha)))

# Define an optimizer to update the model parameters
# Use Adam with a learning rate of 1e-4 and weight decay of 1e-2
optimizer = torch.optim.Adam(encoder.parameters() + projection_head.parameters() + domain_dependent_similarity.parameters(), lr=1e-4, weight_decay=1e-2)

# Load a large-scale uncurated dataset of image-text pairs from the web
# Use COCO Captions as an example dataset
dataset = torchvision.datasets.CocoCaptions(root='data/coco/train2017', annFile='data/coco/annotations/captions_train2017.json', transform=torchvision.transforms.ToTensor())

# Define a data loader to iterate over the dataset in batches
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Pre-train UniCLIP on the dataset for a number of epochs
for epoch in range(num_epochs):
  # Loop over the batches of image-text pairs
  for image, text in dataloader:
    # Resize and normalize the image to match the CNN input
    image = torchvision.transforms.Resize(image_size)(image)
    image = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)

    # Tokenize and pad the text to match the transformer input
    text = transformers.BertTokenizer.from_pretrained('bert-base-uncased').batch_encode_plus(text, padding=True, truncation=True, max_length=text_length, return_tensors='pt')

    # Apply different augmentations to the image and text
    # Use random cropping, horizontal flipping, color jittering, and Gaussian noise for the image
    # Use random shuffling, word dropping, word masking, and word replacing for the text
    image1 = torchvision.transforms.RandomCrop(image_size)(image)
    image2 = torchvision.transforms.RandomHorizontalFlip()(image)
    image1 = torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)(image1)
    image2 = torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)(image2)
    image1 = image1 + torch.randn_like(image1) * 0.1
    image2 = image2 + torch.randn_like(image2) * 0.1

    text1 = transformers.BertTokenizer.from_pretrained('bert-base-uncased').batch_decode(transformers.data.data_collator.DataCollatorForLanguageModeling(tokenizer=transformers.BertTokenizer.from_pretrained('bert-base-uncased'), mlm_probability=0.15)(text)['input_ids'], skip_special_tokens=True)
    text2 = transformers.BertTokenizer.from_pretrained('bert-base-uncased').batch_decode(transformers.data.data_collator.DataCollatorForLanguageModeling(tokenizer=transformers.BertTokenizer.from_pretrained('bert-base-uncased'), mlm_probability=0.15)(text)['input_ids'], skip_special_tokens=True)

    # Encode the image and text features using the encoder
    image_feature1 = encoder(image1)
    image_feature2 = encoder(image2)
    text_feature1 = encoder(text1)
    text_feature2 = encoder(text2)

    # Project the features into a common embedding space using the projection head
    image_embedding1 = projection_head(image_feature1)
    image_embedding2 = projection_head(image_feature2)
    text_embedding1 = projection_head(text_feature1)
    text_embedding2 = projection_head(text_feature2)

    # Embed different augmentations of the same image or text into similar vectors using the augmentation-aware feature embedding module
    augmentation_aware_embedding(image_embedding1, image_embedding2)
    augmentation_aware_embedding(text_embedding1, text_embedding2)

    # Compute the contrastive loss for inter-domain and intra-domain pairs using the MP-NCE loss and the domain dependent similarity measure
    inter_domain_loss = mp_nce_loss(domain_dependent_similarity(image_embedding1, text_embedding1), domain_dependent_similarity(image_embedding2, text_embedding2))
    intra_domain_loss = mp_nce_loss(domain_dependent_similarity(image_embedding1, image_embedding2), domain_dependent_similarity(text_embedding1, text_embedding2))

    # Optimize the model parameters to minimize the total contrastive loss
    total_loss = inter_domain_loss + intra_domain_loss
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```