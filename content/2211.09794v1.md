---
title: 2211.09794v1 Null-text Inversion for Editing Real Images using Guided Diffusion Models
date: 2022-11-10
---

# [Null-text Inversion for Editing Real Images using Guided Diffusion Models](http://arxiv.org/abs/2211.09794v1)

authors: Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/abs/2211.09794 "[2211.09794] Null-text Inversion for Editing Real Images using Guided ..."
[2]: https://arxiv.org/abs/2111.09794 "[2111.09794] A Survey of Zero-shot Generalisation in Deep ... - arXiv.org"
[3]: https://arxiv-export1.library.cornell.edu/abs/2211.09794?context=cs.CV "[2211.09794] Null-text Inversion for Editing Real Images using Guided ..."

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces an accurate inversion technique for editing real images using text-guided diffusion models.
- **Why**: The paper aims to facilitate an intuitive text-based modification of real images using state-of-the-art text-guided diffusion models, which require inverting the image with a meaningful text prompt into the pretrained model's domain.
- **How**: The paper proposes two novel key components for the inversion technique: (i) Pivotal inversion for diffusion models, which uses a single pivotal noise vector for each timestamp and optimizes around it; and (ii) NULL-text optimization, which only modifies the unconditional textual embedding that is used for classifier-free guidance, rather than the input text embedding. The paper evaluates the proposed technique on a variety of images and prompt editing, showing high-fidelity editing of real images.

## Main Contributions

According to the paper at , the main contributions are:

- A novel pivotal inversion technique for diffusion models, which improves the accuracy and stability of the inversion process.
- A novel NULL-text optimization technique, which enables prompt-based editing of real images without modifying the model's weights or the conditional embedding.
- A comprehensive evaluation of the proposed technique on various images and prompt editing, demonstrating its effectiveness and versatility.

## Method Summary

[1]: https://arxiv.org/abs/2211.09794 "[2211.09794] Null-text Inversion for Editing Real Images ... - arXiv.org"
[2]: https://arxiv.org/abs/2111.09794v1 "A Survey of Generalisation in Deep Reinforcement Learning"
[3]: https://arxiv-export1.library.cornell.edu/abs/2211.09794?context=cs.CV "[2211.09794] Null-text Inversion for Editing Real Images using Guided ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a two-step inversion technique for editing real images using text-guided diffusion models, which are generative models that sample images by reversing a Markov chain of noisy diffusion steps.
- The first step is **pivotal inversion**, which aims to find a pivotal noise vector for each timestamp of the diffusion process that best matches the input image. This is done by optimizing a loss function that measures the distance between the input image and the reconstructed image from the noise vector, as well as a regularization term that encourages smoothness and diversity of the noise vectors. The paper shows that this step improves the accuracy and stability of the inversion compared to existing methods that use random noise samples.
- The second step is **NULL-text optimization**, which aims to modify the unconditional textual embedding that is used for classifier-free guidance in text-guided diffusion models, rather than the input text embedding. This allows for keeping both the model weights and the conditional embedding intact and hence enables applying prompt-based editing while avoiding the cumbersome tuning of the model's weights. The paper shows that this step enables high-fidelity editing of real images with various prompts.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a real image x and a text prompt t
# Output: an edited image x' that matches the text prompt t

# Step 1: Pivotal inversion
# Initialize a pivotal noise vector z_pivot for each timestamp t of the diffusion process
# Initialize a noise vector z for each timestamp t of the diffusion process
# Repeat until convergence:
  # For each timestamp t in reverse order:
    # Update z[t] by adding a small perturbation to z_pivot[t]
    # Update z_pivot[t] by minimizing the loss function L(x, z[t]) + R(z[t], z_pivot[t])
# Return the final noise vector z

# Step 2: NULL-text optimization
# Initialize an unconditional textual embedding e_null
# Repeat until convergence:
  # Update e_null by minimizing the loss function L(t, e_null)
# Return the final textual embedding e_null

# Step 3: Prompt-based editing
# Use the text-guided diffusion model with the noise vector z and the textual embedding e_null to generate an edited image x' that matches the text prompt t
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: a real image x and a text prompt t
# Output: an edited image x' that matches the text prompt t

# Step 1: Pivotal inversion
# Define the diffusion process parameters: number of timestamps T, noise level schedule beta[t], diffusion model D
# Define the loss function L(x, z[t]) = ||x - D(z[t], t)||^2
# Define the regularization term R(z[t], z_pivot[t]) = lambda * (||z[t] - z_pivot[t]||^2 + ||z[t+1] - z_pivot[t+1]||^2)
# Define the learning rate alpha and the number of iterations N
# Initialize a pivotal noise vector z_pivot for each timestamp t of the diffusion process by sampling from N(0, I)
# Initialize a noise vector z for each timestamp t of the diffusion process by copying z_pivot
# Repeat N times:
  # For each timestamp t in reverse order from T to 1:
    # Update z[t] by adding a small Gaussian noise to z_pivot[t]
    # Update z_pivot[t] by gradient descent: z_pivot[t] = z_pivot[t] - alpha * dL/dz_pivot[t]
# Return the final noise vector z

# Step 2: NULL-text optimization
# Define the text-guided diffusion model parameters: text encoder E, unconditional textual embedding e_0, conditional textual embedding e_t
# Define the loss function L(t, e_null) = KL(E(t) || e_null) + KL(e_null || e_0)
# Define the learning rate beta and the number of iterations M
# Initialize an unconditional textual embedding e_null by copying e_0
# Repeat M times:
  # Update e_null by gradient descent: e_null = e_null - beta * dL/de_null
# Return the final textual embedding e_null

# Step 3: Prompt-based editing
# Use the text-guided diffusion model with the noise vector z and the textual embedding e_null to generate an edited image x' that matches the text prompt t by reversing the diffusion process:
  # For each timestamp t from 1 to T:
    # Update x' by denoising: x' = (x' + sqrt(beta[t]) * z[t]) / (1 + beta[t])
    # Update x' by conditioning: x' = x' + D(e_null, t) * (e_t - e_0)
# Return the final edited image x'
```