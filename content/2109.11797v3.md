---
title: 2109.11797v3 CPT  Colorful Prompt Tuning for Pre-trained Vision-Language Models
date: 2021-09-12
---

# [CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models](http://arxiv.org/abs/2109.11797v3)

authors: Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun


## What, Why and How

[1]: https://arxiv.org/abs/2109.11797 "[2109.11797] CPT: Colorful Prompt Tuning for Pre-trained Vision ..."
[2]: https://www.arxiv-vanity.com/ "arXiv Vanity â€“ Read academic papers from arXiv as web pages"
[3]: https://arxiv.org/pdf/2109.11797v3.pdf "arXiv.org e-Print archive"
[4]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the paper:

- **What**: The paper presents a novel paradigm for tuning pre-trained vision-language models (VL-PTMs) for cross-modal tasks, such as image captioning, visual question answering, and referring expression comprehension. The paradigm is called Cross-modal Prompt Tuning (CPT), or Colorful Prompt Tuning, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text.
- **Why**: The paper argues that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for large amounts of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. CPT aims to address this challenge by maximally mitigating the gap and enabling strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs.
- **How**: The paper proposes to use color-based co-referential markers to create prompts for both image and text inputs, such as adding colored boxes around objects in the image and using corresponding colored words in the text. For example, given an image of a dog and a cat, and a text input "The [red] is bigger than the [blue]", CPT would add red and blue boxes around the dog and the cat respectively, and ask the model to fill in the blanks with the correct words. The paper also introduces a prompt generator that can automatically create diverse and informative prompts for different tasks and datasets. The paper evaluates CPT on three cross-modal tasks and shows that it outperforms fine-tuning by a large margin, especially in few-shot and zero-shot settings.

[^1^][1] [^2^][2] [^3^][3]

## Main Contributions

[1]: https://arxiv.org/pdf/2109.11797.pdf "arXiv:2109.11797v3 [cs.CV] 20 May 2022"
[2]: https://arxiv.org/abs/2109.11797 "[2109.11797] CPT: Colorful Prompt Tuning for Pre-trained Vision ..."
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

According to the paper[^1^][1], the main contributions are:

- **A novel paradigm for tuning VL-PTMs**: The paper proposes CPT, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, bridging the gap between pre-training and fine-tuning objectives of VL-PTMs.
- **A prompt generator for diverse and informative prompts**: The paper introduces a prompt generator that can automatically create prompts for different tasks and datasets, based on a set of heuristics and rules. The prompt generator can also handle multiple objects and relations in the image and text inputs.
- **Comprehensive experiments on three cross-modal tasks**: The paper evaluates CPT on image captioning, visual question answering, and referring expression comprehension tasks, using four benchmark datasets. The paper shows that CPT outperforms fine-tuning by a large margin, especially in few-shot and zero-shot settings. The paper also conducts ablation studies and qualitative analysis to demonstrate the effectiveness of CPT.

## Method Summary

[1]: https://arxiv.org/pdf/2109.11797.pdf "arXiv:2109.11797v3 [cs.CV] 20 May 2022"
[2]: https://arxiv.org/abs/2109.11797 "[2109.11797] CPT: Colorful Prompt Tuning for Pre-trained Vision ..."
[3]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"

Here is a summary of the method section of the paper:

- **The CPT framework**: The paper describes the general framework of CPT, which consists of three steps: prompt generation, prompt tuning, and prompt inference. In prompt generation, color-based co-referential markers are added to both image and text inputs to create prompts. In prompt tuning, the VL-PTM is tuned on the prompts using a masked language modeling objective. In prompt inference, the tuned VL-PTM is used to fill in the blanks in the prompts and generate outputs for downstream tasks.
- **The prompt generator**: The paper introduces a prompt generator that can automatically create diverse and informative prompts for different tasks and datasets. The prompt generator follows a set of heuristics and rules to select objects and relations in the image and text inputs, assign colors to them, and mask some words in the text input. The paper also describes how the prompt generator can handle multiple objects and relations in the image and text inputs.
- **The experimental setup**: The paper evaluates CPT on three cross-modal tasks: image captioning, visual question answering, and referring expression comprehension. The paper uses four benchmark datasets: COCO (Lin et al., 2014), VQA (Antol et al., 2015), RefCOCO (Yu et al., 2016), and RefCOCO+ (Kazemzadeh et al., 2014). The paper compares CPT with fine-tuning on different VL-PTMs: ViLBERT (Lu et al., 2019), LXMERT (Tan and Bansal, 2019), UNITER (Chen et al., 2020), OSCAR (Li et al., 2020), CLIP (Radford et al., 2021), and ALIGN (Jia et al., 2021). The paper also conducts ablation studies and qualitative analysis to demonstrate the effectiveness of CPT.

[^1^][1] [^2^][2] [^3^][3]

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define a VL-PTM and load pre-trained weights
model = VL-PTM()
model.load_pretrained_weights()

# Define a prompt generator
prompt_generator = PromptGenerator()

# Define a task and a dataset
task = "image captioning"
dataset = "COCO"

# Loop over the dataset
for image, text in dataset:

  # Generate a prompt with color-based co-referential markers
  prompt_image, prompt_text = prompt_generator.generate_prompt(image, text, task)

  # Mask some words in the prompt text
  masked_prompt_text, mask_indices = mask_words(prompt_text)

  # Tune the model on the prompt using MLM objective
  model.tune(prompt_image, masked_prompt_text, mask_indices)

# Loop over the test set
for image, text in test_set:

  # Generate a prompt with color-based co-referential markers
  prompt_image, prompt_text = prompt_generator.generate_prompt(image, text, task)

  # Mask some words in the prompt text
  masked_prompt_text, mask_indices = mask_words(prompt_text)

  # Infer the output from the model by filling in the blanks
  output = model.infer(prompt_image, masked_prompt_text, mask_indices)

  # Evaluate the output using task-specific metrics
  evaluate(output, text)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import transformers
import numpy as np
import random

# Define a VL-PTM and load pre-trained weights
# You can use any of the VL-PTMs mentioned in the paper, such as ViLBERT, LXMERT, UNITER, OSCAR, CLIP, or ALIGN
model = VL-PTM()
model.load_pretrained_weights()

# Define a prompt generator
class PromptGenerator:

  # Initialize the prompt generator with some parameters
  def __init__(self, num_colors=10, max_objects=5, max_relations=3):

    # Define a list of colors to use as co-referential markers
    self.colors = ["red", "green", "blue", "yellow", "orange", "purple", "pink", "brown", "black", "white"]

    # Shuffle the colors to avoid bias
    random.shuffle(self.colors)

    # Define the maximum number of objects and relations to handle in the image and text inputs
    self.max_objects = max_objects
    self.max_relations = max_relations

  # Generate a prompt with color-based co-referential markers for a given image and text input and a task
  def generate_prompt(self, image, text, task):

    # Extract the objects and relations from the image using a pre-trained object detector and relation detector
    # You can use any of the detectors mentioned in the paper, such as Faster R-CNN, Mask R-CNN, or DETR
    objects = object_detector(image)
    relations = relation_detector(image)

    # Select the top k objects and relations based on their confidence scores and relevance to the task
    # You can use different heuristics and rules for different tasks and datasets as described in the paper
    objects = select_top_k(objects, self.max_objects, task)
    relations = select_top_k(relations, self.max_relations, task)

    # Assign a color to each object and relation based on their order in the list
    # Use a different color for each object and relation
    for i in range(len(objects)):
      objects[i].color = self.colors[i]

    for i in range(len(relations)):
      relations[i].color = self.colors[i + len(objects)]

    # Add colored boxes around the objects and relations in the image
    prompt_image = add_colored_boxes(image, objects + relations)

    # Replace the words corresponding to the objects and relations in the text with colored words
    # Use square brackets to indicate masked words
    prompt_text = replace_with_colored_words(text, objects + relations)

    # Return the prompt image and text
    return prompt_image, prompt_text

# Define a function to mask some words in the prompt text based on the task
def mask_words(prompt_text, task):

  # Split the prompt text into tokens using a pre-trained tokenizer
  # You can use any of the tokenizers compatible with the VL-PTM you are using, such as BERTTokenizer or GPT2Tokenizer
  tokenizer = Tokenizer()
  tokens = tokenizer.tokenize(prompt_text)

  # Mask some tokens based on the task
  # You can use different masking strategies for different tasks as described in the paper
  if task == "image captioning":
    # Mask all colored words in the text
    mask_indices = [i for i in range(len(tokens)) if tokens[i].startswith("[") and tokens[i].endswith("]")]
  elif task == "visual question answering":
    # Mask the last token in the text if it is a question mark
    if tokens[-1] == "?":
      mask_indices = [-1]
    else:
      mask_indices = []
  elif task == "referring expression comprehension":
    # Mask one random colored word in the text that corresponds to an object or relation in the image
    mask_indices = [random.choice([i for i in range(len(tokens)) if tokens[i].startswith("[") and tokens[i].endswith("]")])]

  # Replace the masked tokens with a special token [MASK]
  masked_tokens = tokens.copy()
  for i in mask_indices:
    masked_tokens[i] = "[MASK]"

  # Convert the masked tokens back to text using the tokenizer
  masked_prompt_text = tokenizer.convert_tokens_to_string(masked_tokens)

  # Return the masked prompt text and the mask indices
  return masked_prompt_text, mask_indices

# Define a function to tune the model on the prompt using MLM objective
def tune(prompt_image, masked_prompt_text, mask_indices):

  # Convert the prompt image and text into features using a pre-trained feature extractor
  # You can use any of the feature extractors compatible with the VL-PTM you are using, such as ResNet or ViT
  feature_extractor = FeatureExtractor()
  image_features = feature_extractor(prompt_image)
  text_features = feature_extractor(masked_prompt_text)

  # Concatenate the image and text features and pass them to the model
  # The model should output the logits for each token in the text
  features = torch.cat([image_features, text_features], dim=0)
  logits = model(features)

  # Compute the loss using a cross-entropy criterion
  # Only consider the logits and labels for the masked tokens
  criterion = torch.nn.CrossEntropyLoss()
  logits = logits[mask_indices]
  labels = tokenizer.convert_tokens_to_ids(tokens[mask_indices])
  loss = criterion(logits, labels)

  # Backpropagate the loss and update the model parameters
  loss.backward()
  optimizer.step()

# Define a function to infer the output from the model by filling in the blanks
def infer(prompt_image, masked_prompt_text, mask_indices):

  # Convert the prompt image and text into features using a pre-trained feature extractor
  # You can use any of the feature extractors compatible with the VL-PTM you are using, such as ResNet or ViT
  feature_extractor = FeatureExtractor()
  image_features = feature_extractor(prompt_image)
  text_features = feature_extractor(masked_prompt_text)

  # Concatenate the image and text features and pass them to the model
  # The model should output the logits for each token in the text
  features = torch.cat([image_features, text_features], dim=0)
  logits = model(features)

  # Get the predicted tokens for the masked tokens by taking the argmax of the logits
  # Convert the predicted tokens back to words using the tokenizer
  predicted_tokens = logits[mask_indices].argmax(dim=-1)
  predicted_words = tokenizer.convert_ids_to_tokens(predicted_tokens)

  # Replace the masked words in the prompt text with the predicted words
  output_tokens = tokens.copy()
  for i in range(len(mask_indices)):
    output_tokens[mask_indices[i]] = predicted_words[i]
  
  # Convert the output tokens back to text using the tokenizer
  output = tokenizer.convert_tokens_to_string(output_tokens)

  # Return the output
  return output

# Define a function to evaluate the output using task-specific metrics
def evaluate(output, text):

  # Compute different metrics based on the task
  if task == "image captioning":
    # Compute BLEU, METEOR, ROUGE, CIDEr, and SPICE scores between the output and the text
    bleu_score = bleu(output, text)
    meteor_score = meteor(output, text)
    rouge_score = rouge(output, text)
    cider_score = cider(output, text)
    spice_score = spice(output, text)

    # Return a dictionary of scores
    return {"BLEU": bleu_score, "METEOR": meteor_score, "ROUGE": rouge_score, "CIDEr": cider_score, "SPICE": spice_score}

  elif task == "visual question answering":
    # Compute accuracy between the output and the text
    accuracy = int(output == text)

    # Return a dictionary of scores
    return {"Accuracy": accuracy}

  elif task == "referring expression comprehension":
    # Compute IoU between the output and the text
    iou = iou(output, text)

    # Return a dictionary of scores
    return {"IoU": iou}
```