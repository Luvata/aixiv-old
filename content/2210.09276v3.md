---
title: 2210.09276v3 Imagic  Text-Based Real Image Editing with Diffusion Models
date: 2022-10-10
---

# [Imagic: Text-Based Real Image Editing with Diffusion Models](http://arxiv.org/abs/2210.09276v3)

authors: Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani


## What, Why and How

[1]: https://arxiv-export1.library.cornell.edu/abs/2210.09276v1 "[2210.09276v1] Imagic: Text-Based Real Image Editing with Diffusion Models"
[2]: https://arxiv.org/pdf/2210.09276v3 "arXiv.org"
[3]: https://arxiv.org/pdf/2210.09276v3.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper presents a method for text-based real image editing with diffusion models, called Imagic. It can apply complex semantic edits to a single real image, such as changing the posture and composition of objects, while preserving the original characteristics.
- **Why**: The paper aims to overcome the limitations of previous methods that are either restricted to specific editing types, or require synthetic images or multiple inputs, or cannot handle non-rigid transformations. The paper also demonstrates the potential of text-to-image diffusion models for image editing tasks.
- **How**: The paper leverages a pre-trained text-to-image diffusion model that generates images conditioned on text descriptions. It fine-tunes the model on a single input image and a target text, and produces a text embedding that aligns with both. It then uses the text embedding and the input image to generate the edited image through a diffusion process. The paper evaluates the method on various domains and shows high-quality results.

## Main Contributions

[1]: https://arxiv-export1.library.cornell.edu/abs/2210.09276v1 "[2210.09276v1] Imagic: Text-Based Real Image Editing with Diffusion Models"
[2]: https://arxiv.org/pdf/2210.09276v3 "arXiv.org"
[3]: https://arxiv.org/pdf/2210.09276v3.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- **The first method** to perform complex text-guided semantic edits on a single real image, such as changing the posture and composition of objects, without requiring any additional inputs or masks.
- **A novel approach** to leverage a pre-trained text-to-image diffusion model for image editing, by fine-tuning it on a single input image and a target text, and producing a text embedding that aligns with both.
- **A comprehensive evaluation** of the method on various domains and editing types, showing high-quality results that outperform existing methods and preserve the original image characteristics.

## Method Summary

[1]: https://arxiv-export1.library.cornell.edu/abs/2210.09276v1 "[2210.09276v1] Imagic: Text-Based Real Image Editing with Diffusion Models"
[2]: https://arxiv.org/pdf/2210.09276v3 "arXiv.org"
[3]: https://arxiv.org/pdf/2210.09276v3.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- The paper uses a text-to-image diffusion model that generates images conditioned on text descriptions. The model is based on the unconditional diffusion model of , which reverses the generative process of a diffusion denoising autoencoder . The model starts from a noisy image and gradually refines it by applying Gaussian noise and denoising steps until it reaches the final image. The model is conditioned on text by using a CLIP  encoder that produces a text embedding, which is concatenated with the image features at each step.
- The paper proposes a method to fine-tune the pre-trained text-to-image diffusion model on a single input image and a target text, and produce a text embedding that aligns with both. The method consists of three steps: (1) Text embedding optimization: The method optimizes the text embedding to minimize the reconstruction loss between the input image and the generated image, while maximizing the CLIP score between the text embedding and the input image. (2) Diffusion model fine-tuning: The method fine-tunes the diffusion model parameters to capture the image-specific appearance and details, while keeping the text embedding fixed. (3) Image editing: The method uses the fine-tuned model and the optimized text embedding to generate the edited image through the diffusion process, starting from a random noise image.
- The paper also introduces some techniques to improve the quality and diversity of the generated images, such as using multiple text embeddings, adding noise to the text embeddings, and using style mixing.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a real image x and a target text t
# Output: an edited image y that matches t

# Load the pre-trained text-to-image diffusion model
model = load_model()

# Initialize the text embedding z randomly
z = random_embedding()

# Step 1: Text embedding optimization
for k in range(K):
  # Generate an image x_hat from z and x using the diffusion model
  x_hat = model.generate(z, x)
  # Compute the reconstruction loss between x and x_hat
  L_rec = reconstruction_loss(x, x_hat)
  # Compute the CLIP score between z and x
  S_clip = clip_score(z, x)
  # Update z by gradient descent to minimize L_rec and maximize S_clip
  z = z - lr * grad(L_rec - lambda * S_clip, z)

# Step 2: Diffusion model fine-tuning
for i in range(I):
  # Sample a batch of noise images e from a Gaussian distribution
  e = sample_noise(batch_size)
  # Generate a batch of images x_tilde from z and e using the diffusion model
  x_tilde = model.generate(z, e)
  # Compute the reconstruction loss between x and x_tilde
  L_rec = reconstruction_loss(x, x_tilde)
  # Update the model parameters by gradient descent to minimize L_rec
  model = model - lr * grad(L_rec, model)

# Step 3: Image editing
# Sample a noise image e from a Gaussian distribution
e = sample_noise()
# Generate an edited image y from z and e using the fine-tuned model
y = model.generate(z, e)
# Return y as the output
return y
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define some hyperparameters
K = 100 # number of iterations for text embedding optimization
I = 100 # number of iterations for diffusion model fine-tuning
batch_size = 16 # batch size for fine-tuning
lr = 0.01 # learning rate for gradient descent
lambda = 0.1 # weight for the CLIP score term
T = 1000 # number of diffusion steps
beta = 0.0001 # noise level for diffusion

# Load the pre-trained text-to-image diffusion model
model = torch.load("model.pth")

# Load the CLIP encoder and tokenizer
encoder = clip.load("ViT-B/32", device="cuda")
tokenizer = clip.tokenize

# Define a function to compute the reconstruction loss between two images
def reconstruction_loss(x, x_hat):
  # Use the L2 norm as the loss function
  return torch.norm(x - x_hat, p=2)

# Define a function to compute the CLIP score between a text embedding and an image
def clip_score(z, x):
  # Encode the image using the CLIP encoder
  x_enc = encoder.encode_image(x)
  # Normalize the text embedding and the image encoding
  z = z / z.norm(dim=-1, keepdim=True)
  x_enc = x_enc / x_enc.norm(dim=-1, keepdim=True)
  # Compute the cosine similarity between them
  return torch.sum(z * x_enc, dim=-1)

# Define a function to sample a batch of noise images from a Gaussian distribution
def sample_noise(batch_size):
  # Sample a batch of random vectors from a standard normal distribution
  e = torch.randn(batch_size, 3, 256, 256)
  # Return the noise images
  return e

# Define a function to generate an image from a text embedding and a noise image using the diffusion model
def model.generate(z, e):
  # Concatenate the text embedding with the noise image along the channel dimension
  x_0 = torch.cat([z, e], dim=1)
  # Initialize the generated image as the noise image
  x_t = x_0.clone()
  # Loop over the diffusion steps from T to 1
  for t in range(T, 0, -1):
    # Compute the noise level alpha_t for this step
    alpha_t = (1 - t/T) * (1 - beta) + beta
    # Apply Gaussian noise to the generated image with variance alpha_t
    x_t = x_t + torch.sqrt(alpha_t) * torch.randn_like(x_t)
    # Apply a denoising step to the generated image using the diffusion model
    x_t = model.denoise(x_t, t, x_0)
  # Return the final generated image
  return x_t

# Input: a real image x and a target text t
# Output: an edited image y that matches t

# Load the real image and resize it to 256x256 pixels
x = torchvision.io.read_image("input.jpg")
x = torchvision.transforms.Resize((256,256))(x)

# Tokenize the target text and encode it using the CLIP encoder
t = "Make the dog jump"
t = tokenizer(t)
z = encoder.encode_text(t)

# Step 1: Text embedding optimization
# Create an optimizer for z using gradient descent with momentum
optimizer_z = torch.optim.SGD([z], lr=lr, momentum=0.9)
# Loop over K iterations
for k in range(K):
  # Generate an image x_hat from z and x using the diffusion model
  x_hat = model.generate(z, x)
  # Compute the reconstruction loss between x and x_hat
  L_rec = reconstruction_loss(x, x_hat)
  # Compute the CLIP score between z and x
  S_clip = clip_score(z, x)
  # Compute the total loss as a weighted combination of L_rec and S_clip
  L_total = L_rec - lambda * S_clip
  # Update z by gradient descent to minimize L_total
  optimizer_z.zero_grad()
  L_total.backward()
  optimizer_z.step()

# Step 2: Diffusion model fine-tuning
# Create an optimizer for model using gradient descent with momentum
optimizer_model = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)
# Loop over I iterations
for i in range(I):
  # Sample a batch of noise images e from a Gaussian distribution
  e = sample_noise(batch_size)
  # Generate a batch of images x_tilde from z and e using the diffusion model
  x_tilde = model.generate(z, e)
  # Compute the reconstruction loss between x and x_tilde
  L_rec = reconstruction_loss(x, x_tilde)
  # Update the model parameters by gradient descent to minimize L_rec
  optimizer_model.zero_grad()
  L_rec.backward()
  optimizer_model.step()

# Step 3: Image editing
# Sample a noise image e from a Gaussian distribution
e = sample_noise()
# Generate an edited image y from z and e using the fine-tuned model
y = model.generate(z, e)
# Save y as the output image
torchvision.io.write_image("output.jpg", y)
```