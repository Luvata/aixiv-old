---
title: 2303.18242v1 $\infty$-Diff  Infinite Resolution Diffusion with Subsampled Mollified States
date: 2023-03-19
---

# [$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States](http://arxiv.org/abs/2303.18242v1)

authors: Sam Bond-Taylor, Chris G. Willcocks


## What, Why and How

[1]: https://arxiv.org/abs/2303.18242 "[2303.18242] $\\infty$-Diff: Infinite Resolution Diffusion with ..."
[2]: https://arxiv.org/pdf/2303.18242v1.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2305.18242v1 "[2305.18242v1] Dataset for neutron and gamma-ray pulse shape discrimination"

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces $\\infty$-Diff, a generative diffusion model that can produce infinite resolution images from raw data.
- **Why**: The paper aims to overcome the limitations of existing infinite resolution generative models, such as requiring latent vector compression, hypernetworks, or discrete components, which degrade the sample quality and scalability.
- **How**: The paper proposes to randomly sample subsets of coordinates during training and learn to denoise the content at those coordinates, resulting in a continuous function that can generate images at arbitrary resolutions. The paper also introduces a subsampled mollified state (SMS) technique to improve the stability and efficiency of the diffusion process. The paper evaluates the proposed model on several image datasets and shows that it achieves lower FID scores and higher resolution scalability than previous methods.

## Main Contributions

[1]: https://arxiv.org/abs/2303.18242 "[2303.18242] $\\infty$-Diff: Infinite Resolution Diffusion with ..."
[2]: https://arxiv.org/pdf/2303.18242v1.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2305.18242v1 "[2305.18242v1] Dataset for neutron and gamma-ray pulse shape discrimination"

The paper claims the following contributions[^1^][1]:

- A novel generative diffusion model that directly operates on infinite resolution data, without requiring latent vector compression, hypernetworks, or discrete components.
- A subsampled mollified state (SMS) technique that improves the stability and efficiency of the diffusion process by applying a mollifier function to the sampled coordinates.
- An empirical evaluation of the proposed model on several image datasets, showing that it outperforms previous methods in terms of FID scores and resolution scalability.

## Method Summary

[1]: https://arxiv.org/abs/2303.18242 "[2303.18242] $\\infty$-Diff: Infinite Resolution Diffusion with ..."
[2]: https://arxiv.org/pdf/2303.18242v1.pdf "arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2305.18242v1 "[2305.18242v1] Dataset for neutron and gamma-ray pulse shape discrimination"

Here is a summary of the method section of the paper[^1^][1]:

- The paper adopts the diffusion probabilistic model framework, which defines a Markov chain that transforms a data distribution into a simple prior distribution through a sequence of Gaussian noise additions.
- The paper proposes to randomly sample subsets of coordinates from the data during training and learn a denoising function that predicts the original content at those coordinates given the noisy data and the noise level. The denoising function is parameterized by a neural network with residual blocks and self-attention layers.
- The paper introduces a subsampled mollified state (SMS) technique that applies a mollifier function to the sampled coordinates before adding noise, which smooths out the high-frequency components and reduces the variance of the noise. The paper shows that this technique improves the stability and efficiency of the diffusion process and allows for larger noise levels.
- The paper also describes how to generate samples from the learned model by reversing the diffusion process, starting from a random sample from the prior distribution and iteratively applying the denoising function to refine the image at different resolutions. The paper shows that this procedure can generate images at arbitrary resolutions, even higher than the training data, by using bilinear interpolation to upscale the image before applying the denoising function.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the diffusion probabilistic model
def diffusion_model(x):
  # Initialize the noise level and the prior distribution
  beta = 0
  prior = Normal(0, 1)
  # Loop over the diffusion steps
  for t in range(T):
    # Sample a subset of coordinates from x
    S = sample_subset(x.shape)
    # Apply a mollifier function to the sampled coordinates
    x_S = mollify(x[S])
    # Add Gaussian noise to x_S
    epsilon = Normal(0, sqrt(beta))
    x_S = x_S + epsilon
    # Update the noise level
    beta = beta + alpha_t
  # Return the final noisy image and the prior sample
  return x_S, prior.sample(x.shape)

# Define the denoising function
def denoise(x, beta):
  # Initialize a neural network with residual blocks and self-attention layers
  net = ResNet()
  # Concatenate x and beta as the input
  input = concat(x, beta)
  # Predict the original content at the sampled coordinates
  output = net(input)
  return output

# Train the model
def train(data):
  # Loop over the data batches
  for x in data:
    # Apply the diffusion model to x
    x_S, z = diffusion_model(x)
    # Compute the loss as the negative log-likelihood of x given x_S and beta
    loss = -log_likelihood(x, denoise(x_S, beta))
    # Update the model parameters using gradient descent
    update_parameters(loss)

# Generate samples from the model
def sample():
  # Sample a random image from the prior distribution
  z = prior.sample()
  # Loop over the diffusion steps in reverse order
  for t in range(T-1, -1, -1):
    # Get the current noise level
    beta = beta - alpha_t
    # Upscale z to the desired resolution using bilinear interpolation
    z = upscale(z)
    # Apply the denoising function to z
    z = denoise(z, beta)
  # Return the final image
  return z

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np

# Define the hyperparameters
T = 1000 # number of diffusion steps
alpha_0 = 0.001 # initial noise level
alpha_T = 0.1 # final noise level
gamma = 0.995 # noise level decay factor
batch_size = 64 # batch size for training and sampling
lr = 0.0001 # learning rate for gradient descent
num_epochs = 100 # number of epochs for training
image_size = 256 # image size for training and sampling

# Define the diffusion probabilistic model
def diffusion_model(x):
  # Initialize the noise level and the prior distribution
  beta = alpha_0
  prior = torch.distributions.Normal(0, 1)
  # Loop over the diffusion steps
  for t in range(T):
    # Sample a subset of coordinates from x
    S = sample_subset(x.shape)
    # Apply a mollifier function to the sampled coordinates
    x_S = mollify(x[S])
    # Add Gaussian noise to x_S
    epsilon = torch.distributions.Normal(0, torch.sqrt(beta))
    x_S = x_S + epsilon.sample(x_S.shape)
    # Update the noise level
    beta = beta * gamma + alpha_T * (1 - gamma)
  # Return the final noisy image and the prior sample
  return x_S, prior.sample(x.shape)

# Define the mollifier function
def mollify(x):
  # Define a Gaussian kernel with standard deviation sigma
  sigma = 1.0
  kernel_size = int(4 * sigma + 1)
  kernel = np.exp(-np.linspace(-2, 2, kernel_size) ** 2 / (2 * sigma ** 2))
  kernel = kernel / np.sum(kernel)
  kernel = torch.tensor(kernel, dtype=torch.float32)
  # Apply the kernel to x along each dimension using convolution
  x = F.conv1d(x.unsqueeze(1), kernel.unsqueeze(0).unsqueeze(1), padding=kernel_size // 2).squeeze(1)
  x = F.conv1d(x.unsqueeze(2), kernel.unsqueeze(0).unsqueeze(1), padding=kernel_size // 2).squeeze(2)
  # Return the mollified x
  return x

# Define the denoising function
def denoise(x, beta):
  # Initialize a neural network with residual blocks and self-attention layers
  net = ResNet()
  # Concatenate x and beta as the input
  input = torch.cat([x, beta.expand_as(x)], dim=1)
  # Predict the original content at the sampled coordinates
  output = net(input)
  return output

# Define the ResNet class
class ResNet(nn.Module):
  def __init__(self):
    super().__init__()
    # Define the input layer with a convolutional layer and a normalization layer
    self.input_layer = nn.Sequential(
      nn.Conv2d(4, 64, kernel_size=3, padding=1),
      nn.BatchNorm2d(64)
    )
    # Define a list of residual blocks with convolutional layers, normalization layers, and self-attention layers
    self.res_blocks = nn.ModuleList([
      ResBlock(64) for _ in range(8)
    ])
    # Define the output layer with a convolutional layer and a tanh activation function
    self.output_layer = nn.Sequential(
      nn.Conv2d(64, 3, kernel_size=3, padding=1),
      nn.Tanh()
    )

  def forward(self, x):
    # Apply the input layer to x
    x = self.input_layer(x)
    # Apply each residual block to x and add the residual connection
    for block in self.res_blocks:
      x = x + block(x)
    # Apply the output layer to x and return it
    x = self.output_layer(x)
    return x

# Define the ResBlock class
class ResBlock(nn.Module):
  def __init__(self, channels):
    super().__init__()
    # Define the first convolutional layer and the normalization layer
    self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
    self.norm1 = nn.BatchNorm2d(channels)
    # Define the second convolutional layer and the normalization layer
    self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
    self.norm2 = nn.BatchNorm2d(channels)
    # Define the self-attention layer
    self.attn = SelfAttention(channels)

  def forward(self, x):
    # Apply the first convolutional layer and the normalization layer to x
    x = self.conv1(x)
    x = self.norm1(x)
    # Apply the ReLU activation function to x
    x = F.relu(x)
    # Apply the second convolutional layer and the normalization layer to x
    x = self.conv2(x)
    x = self.norm2(x)
    # Apply the self-attention layer to x
    x = self.attn(x)
    # Return x
    return x

# Define the SelfAttention class
class SelfAttention(nn.Module):
  def __init__(self, channels):
    super().__init__()
    # Define the query, key, and value projection layers
    self.query = nn.Conv2d(channels, channels // 8, kernel_size=1)
    self.key = nn.Conv2d(channels, channels // 8, kernel_size=1)
    self.value = nn.Conv2d(channels, channels, kernel_size=1)
    # Define the output projection layer
    self.output = nn.Conv2d(channels, channels, kernel_size=1)

  def forward(self, x):
    # Get the batch size and the spatial dimensions of x
    batch_size, _, height, width = x.shape
    # Project x to query, key, and value tensors
    query = self.query(x).view(batch_size, -1, height * width).permute(0, 2, 1)
    key = self.key(x).view(batch_size, -1, height * width)
    value = self.value(x).view(batch_size, -1, height * width).permute(0, 2, 1)
    # Compute the attention scores by matrix multiplication and scaling
    scores = torch.matmul(query, key) / (channels ** 0.5)
    # Apply the softmax function to get the attention weights
    weights = F.softmax(scores, dim=-1)
    # Compute the weighted sum of value tensors
    output = torch.matmul(weights, value).permute(0, 2, 1).view(batch_size, -1, height, width)
    # Project the output tensor and add the residual connection
    output = self.output(output) + x
    # Return the output tensor
    return output

# Train the model
def train(data):
  # Initialize the model and the optimizer
  model = ResNet()
  optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  # Loop over the epochs
  for epoch in range(num_epochs):
    # Loop over the data batches
    for x in data:
      # Resize x to the image size
      x = F.interpolate(x, size=image_size)
      # Apply the diffusion model to x
      x_S, z = diffusion_model(x)
      # Compute the loss as the negative log-likelihood of x given x_S and beta
      loss = -torch.distributions.Normal(denoise(x_S, beta), torch.sqrt(beta)).log_prob(x).mean()
      # Update the model parameters using gradient descent
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    # Print the epoch and the loss
    print(f"Epoch {epoch}, Loss {loss.item()}")

# Generate samples from the model
def sample():
  # Initialize a random image from the prior distribution
  z = prior.sample((batch_size, 3, image_size, image_size))
  # Loop over the diffusion steps in reverse order
  for t in range(T-1, -1, -1):
    # Get the current noise level
    beta = beta - alpha_t * gamma + alpha_T * (1 - gamma)
    # Upscale z to the desired resolution using bilinear interpolation
    z = F.interpolate(z, scale_factor=2, mode="bilinear", align_corners=False)
    # Apply the denoising function to z
    z = denoise(z, beta)
  # Return the final image
  return z

```