---
title: 2211.13221v2 Latent Video Diffusion Models for High-Fidelity Long Video Generation
date: 2022-11-14
---

# [Latent Video Diffusion Models for High-Fidelity Long Video Generation](http://arxiv.org/abs/2211.13221v2)

authors: Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen


## What, Why and How

[1]: https://arxiv.org/pdf/2211.13221v2 "Latent Video Diffusion Models for High-Fidelity Long Video Generation"
[2]: https://arxiv.org/pdf/2211.13221v2.pdf "arXiv.org e-Print archive"
[3]: https://scholar.archive.org/work/ctyixe4tn5ezjgrjw22mvtdhke "Latent Video Diffusion Models for High-Fidelity Long Video Generation"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes **Latent Video Diffusion Models (LVDM)**, a framework for high-fidelity long video generation based on diffusion models.
- **Why**: The paper aims to overcome the limitations of existing generative models for video synthesis, such as low quality, short length, mode collapse, and high computational cost.
- **How**: The paper introduces several techniques to improve the performance and efficiency of video diffusion models, such as:
    - Leveraging a low-dimensional 3D latent space to compress videos and reduce memory consumption.
    - Applying hierarchical diffusion in the latent space to generate longer videos with more than one thousand frames.
    - Employing conditional latent perturbation and unconditional guidance to mitigate the accumulated errors during the extension of video length.
    - Extending the framework to text-to-video generation by conditioning on text embeddings.

## Main Contributions

According to the paper, the contributions are:

- The introduction of LVDM, an efficient diffusion-based baseline approach for video generation by firstly compressing videos into tight latents.
- The proposal of hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced.
- The development of conditional latent perturbation and unconditional guidance that effectively mitigate the performance degradation issue for long video generation.
- The extension to large-scale text-to-video generation to demonstrate the superiority of the proposed framework.

## Method Summary

[1]: https://arxiv.org/pdf/2211.13221v2 "Latent Video Diffusion Models for High-Fidelity Long Video Generation"
[2]: https://arxiv.org/pdf/2211.13221v2.pdf "arXiv.org e-Print archive"
[3]: https://scholar.archive.org/work/ctyixe4tn5ezjgrjw22mvtdhke "Latent Video Diffusion Models for High-Fidelity Long Video Generation"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first introduces the **background** of diffusion models and video compression, and defines the notation and problem formulation.
- The paper then presents the **main method** of LVDM, which consists of three components: a video encoder-decoder network, a latent diffusion model, and a video reconstruction network.
- The paper describes how to **train** the LVDM using a variational lower bound on the log-likelihood of the video data, and how to **sample** from the LVDM using a reverse diffusion process with noise scheduling and denoising score matching.
- The paper also explains how to **extend** the LVDM to generate longer videos using hierarchical diffusion in the latent space, and how to improve the quality and diversity of long video generation using conditional latent perturbation and unconditional guidance.
- The paper further shows how to **adapt** the LVDM to text-to-video generation by conditioning on text embeddings extracted from a pre-trained CLIP model.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the video encoder-decoder network
video_encoder = Encoder()
video_decoder = Decoder()

# Define the latent diffusion model
latent_diffusion = DiffusionModel()

# Define the video reconstruction network
video_reconstruction = ReconstructionModel()

# Train the LVDM
for each batch of videos x:
  # Encode the videos into latents z
  z = video_encoder(x)
  # Apply latent diffusion to z
  z_tilde, eps = latent_diffusion(z)
  # Decode the latents back to videos x_tilde
  x_tilde = video_decoder(z_tilde)
  # Reconstruct the original videos x_hat from x_tilde
  x_hat = video_reconstruction(x_tilde)
  # Compute the variational lower bound loss
  loss = reconstruction_loss(x, x_hat) + kl_loss(z, z_tilde, eps)
  # Update the model parameters
  optimizer.step(loss)

# Sample from the LVDM
# Initialize a random latent z_0
z_0 = torch.randn(batch_size, latent_dim)
# Reverse the latent diffusion process from z_0 to z_T
z_T = latent_diffusion.reverse(z_0)
# Decode the final latent z_T to video x_T
x_T = video_decoder(z_T)
# Reconstruct the video x_hat from x_T
x_hat = video_reconstruction(x_T)
# Return the sampled video x_hat
return x_hat

# Extend the LVDM to generate longer videos using hierarchical diffusion
# Define a coarse-to-fine latent hierarchy with different scales and resolutions
latent_hierarchy = [z_1, z_2, ..., z_L]
# For each scale l in the hierarchy:
for l in range(1, L+1):
  # Encode the videos into latents z_l
  z_l = video_encoder(x, scale=l)
  # Apply latent diffusion to z_l
  z_l_tilde, eps_l = latent_diffusion(z_l, scale=l)
  # Decode the latents back to videos x_l_tilde
  x_l_tilde = video_decoder(z_l_tilde, scale=l)
  # Reconstruct the original videos x_l_hat from x_l_tilde
  x_l_hat = video_reconstruction(x_l_tilde, scale=l)
  # Compute the variational lower bound loss for scale l
  loss_l = reconstruction_loss(x, x_l_hat) + kl_loss(z_l, z_l_tilde, eps_l)
  # Update the model parameters for scale l
  optimizer.step(loss_l)

# Sample from the extended LVDM using hierarchical diffusion
# Initialize a random latent z_0 at the coarsest scale L
z_0_L = torch.randn(batch_size, latent_dim_L)
# Reverse the latent diffusion process from z_0_L to z_T_L
z_T_L = latent_diffusion.reverse(z_0_L, scale=L)
# Decode the final latent z_T_L to video x_T_L at the coarsest resolution
x_T_L = video_decoder(z_T_L, scale=L)
# For each finer scale l in reverse order:
for l in range(L-1, 0, -1):
  # Upsample the previous video x_T_l+1 to match the resolution of scale l
  x_T_l+1_upsampled = upsample(x_T_l+1)
  # Encode the upsampled video into latents z_0_l
  z_0_l = video_encoder(x_T_l+1_upsampled, scale=l)
  # Reverse the latent diffusion process from z_0_l to z_T_l with conditional perturbation and unconditional guidance from z_T_l+1
  z_T_l = latent_diffusion.reverse(z_0_l, scale=l, condition=z_T_l+1)
  # Decode the final latent z_T_l to video x_T_l at scale l resolution
  x_T_l = video_decoder(z_T_l, scale=l)
# Reconstruct the final video x_hat from x_T_1 at the finest resolution
x_hat = video_reconstruction(x_T_1)
# Return the sampled long video x_hat
return x_hat

# Adapt the LVDM to text-to-video generation by conditioning on text embeddings
# Define a pre-trained CLIP model to extract text embeddings
clip_model = CLIPModel()
# For each batch of videos x and texts t:
for each batch of videos x and texts t:
  # Extract text embeddings e from t using CLIP model
  e = clip_model(t)
  # Encode the videos into latents z with text embeddings as condition
  z = video_encoder(x, condition=e)
  # Apply latent diffusion to z with text embeddings as condition
  z_tilde, eps = latent_diffusion(z, condition=e)
  # Decode the latents back to videos x_tilde with text embeddings as condition
  x_tilde = video_decoder(z_tilde, condition=e)
  # Reconstruct the original videos x_hat from x_tilde with text embeddings as condition
  x_hat = video_reconstruction(x_tilde, condition=e)
  # Compute the variational lower bound loss
  loss = reconstruction_loss(x, x_hat) + kl_loss(z, z_tilde, eps)
  # Update the model parameters
  optimizer.step(loss)

# Sample from the adapted LVDM by conditioning on text embeddings
# Given a text t:
# Extract text embeddings e from t using CLIP model
e = clip_model(t)
# Initialize a random latent z_0
z_0 = torch.randn(batch_size, latent_dim)
# Reverse the latent diffusion process from z_0 to z_T with text embeddings as condition
z_T = latent_diffusion.reverse(z_0, condition=e)
# Decode the final latent z_T to video x_T with text embeddings as condition
x_T = video_decoder(z_T, condition=e)
# Reconstruct the video x_hat from x_T with text embeddings as condition
x_hat = video_reconstruction(x_T, condition=e)
# Return the sampled video x_hat conditioned on text t
return x_hat
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import clip

# Define the hyperparameters
latent_dim = 256 # the dimension of the latent space
latent_dim_L = 64 # the dimension of the latent space at the coarsest scale
num_scales = 4 # the number of scales in the latent hierarchy
num_timesteps = 1000 # the number of timesteps in the diffusion process
beta_min = 0.0001 # the minimum noise level
beta_max = 0.02 # the maximum noise level
video_length = 32 # the length of the input videos
video_channels = 3 # the number of channels of the input videos
video_height = 256 # the height of the input videos
video_width = 256 # the width of the input videos

# Define a helper function to compute the noise levels for each timestep
def get_noise_levels(num_timesteps, beta_min, beta_max):
  betas = torch.exp(torch.linspace(torch.log(beta_min), torch.log(beta_max), num_timesteps)) # geometrically spaced betas
  alphas = 1. - betas # noise-removal coefficients
  alphas_cumprod = torch.cumprod(alphas, dim=0) # cumulative product of alphas
  alphas_cumprod_prev = torch.cat([torch.tensor([1.]), alphas_cumprod[:-1]]) # shifted cumulative product of alphas
  sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod) # square root of cumulative product of alphas
  sqrt_alphas_cumprod_prev = torch.sqrt(alphas_cumprod_prev) # square root of shifted cumulative product of alphas
  sqrt_recip_alphas_cumprod = torch.rsqrt(alphas_cumprod) # reciprocal square root of cumulative product of alphas
  sqrt_recip_alphas_cumprod_prev = torch.rsqrt(alphas_cumprod_prev) # reciprocal square root of shifted cumulative product of alphas
  return betas, alphas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod, sqrt_alphas_cumprod_prev, sqrt_recip_alphas_cumprod, sqrt_recip_alphas_cumprod_prev

# Compute the noise levels for each timestep
betas, alphas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod, sqrt_alphas_cumprod_prev, sqrt_recip_alphas_cumprod, sqrt_recip_alphas_cumprod_prev = get_noise_levels(num_timesteps, beta_min, beta_max)

# Define a helper function to compute the KL loss term for each scale and timestep
def kl_loss(z_0, z_tilde_0, eps_0):
  sigma_0 = torch.exp(0.5 * z_0[:, -1]) # standard deviation of z_0 from its last channel
  sigma_tilde_0 = torch.exp(0.5 * z_tilde_0[:, -1]) # standard deviation of z_tilde_0 from its last channel
  mu_0 = z_0[:, :-1] # mean of z_0 from all but its last channel
  mu_tilde_0 = z_tilde_0[:, :-1] # mean of z_tilde_0 from all but its last channel
  log_sigma_0_sq = torch.log(sigma_0 ** 2 + 1e-9) # log variance of z_0 with a small constant for numerical stability
  log_sigma_tilde_0_sq = torch.log(sigma_tilde_0 ** 2 + 1e-9) # log variance of z_tilde_0 with a small constant for numerical stability
  
  kl_divergence_per_scale_per_timestep = -log_sigma_tilde_0_sq + log_sigma_0_sq + (sigma_tilde_0 ** 2 + (mu_tilde_0 - mu_0) ** 2) / (sigma_0 ** 2 + eps_0 **2) -1. # KL divergence formula from Appendix B.2
  
  kl_divergence_per_scale_per_timestep /= num_scales * num_timesteps # normalize by number of scales and timesteps
  
  kl_divergence_per_scale_per_timestep *= (latent_dim -1.) / latent_dim # multiply by a factor to account for the last channel being used for standard deviation
  
  kl_divergence_per_scale_per_timestep += torch.mean(torch.log(eps_0 **2 + sigma_tilde_0 **2)) / num_scales / num_timesteps # add a term to account for the last channel being used for standard deviation
  
  return kl_divergence_per_scale_per_timestep

# Define the video encoder-decoder network
class VideoEncoderDecoder(nn.Module):
  def __init__(self, latent_dim, num_scales):
    super().__init__()
    self.latent_dim = latent_dim
    self.num_scales = num_scales
    # Define a 3D convolutional network for encoding videos into latents
    self.encoder = nn.Sequential(
      nn.Conv3d(video_channels, 32, kernel_size=3, stride=1, padding=1),
      nn.ReLU(),
      nn.Conv3d(32, 64, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv3d(128, 256, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Conv3d(256, 512, kernel_size=3, stride=2, padding=1),
      nn.ReLU(),
      nn.Flatten(),
      nn.Linear(512 * (video_length // 16) * (video_height // 16) * (video_width // 16), latent_dim + 1) # output a latent vector with an extra channel for standard deviation
    )
    # Define a 3D convolutional network for decoding latents into videos
    self.decoder = nn.Sequential(
      nn.Linear(latent_dim + 1, 512 * (video_length // 16) * (video_height // 16) * (video_width // 16)),
      nn.Unflatten(1, (512, video_length // 16, video_height // 16, video_width // 16)),
      nn.ReLU(),
      nn.ConvTranspose3d(512, 256, kernel_size=4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose3d(64, 32, kernel_size=4, stride=2, padding=1),
      nn.ReLU(),
      nn.ConvTranspose3d(32, video_channels + num_scales - 1 , kernel_size=3, stride=1, padding=1) # output a video tensor with extra channels for scale selection
    )
  
  def forward(self, x, scale=None):
    # Encode the videos into latents
    z = self.encoder(x)
    # If scale is specified (during training), select the corresponding channel from the decoder output
    if scale is not None:
      x_tilde = self.decoder(z)[..., scale]
    # If scale is not specified (during sampling), use softmax to combine the channels from the decoder output
    else:
      x_tilde = torch.sum(F.softmax(self.decoder(z), dim=-1) * x.unsqueeze(-1), dim=-1)
    return z, x_tilde

# Instantiate the video encoder-decoder network
video_encoder_decoder = VideoEncoderDecoder(latent_dim, num_scales)

# Define the latent diffusion model
class LatentDiffusion(nn.Module):
  def __init__(self):
    super().__init__()
    # Define a fully connected network for predicting the mean and variance of the Gaussian distribution of z_t given z_0 and t
    self.predictor = nn.Sequential(
      nn.Linear(latent_dim + num_scales + num_timesteps + latent_dim_L + clip.text_features_dim , latent_dim * 4),
      nn.ReLU(),
      nn.Linear(latent_dim * 4 , latent_dim * 2) # output a vector of size latent_dim * 2 for mean and variance
    )
  
  def forward(self, z_0 , t , condition=None , scale=None):
    # Concatenate z_0 , t , condition , and scale as input to the predictor network
    input = torch.cat([z_0 , t , condition , scale] , dim=-1)
    # Predict the mean and variance of z_t given the input
    mean , log_var = torch.chunk(self.predictor(input) , chunks=2 , dim=-1)
    var = torch.exp(log_var)
    # Sample z_t from the Gaussian distribution
    z_t = mean + torch.randn_like(mean) * torch.sqrt(var)
    return z_t
  
  def reverse(self , z_0 , condition=None):
    # Initialize a list to store the reversed latents
    reversed_latents = []
    # For each timestep t in reverse order:
    for t in range(num_timesteps -1 , -