---
title: 2211.16961v4 Pattern Attention Transformer with Doughnut Kernel
date: 2022-11-17
---

# [Pattern Attention Transformer with Doughnut Kernel](http://arxiv.org/abs/2211.16961v4)

authors: WenYuan Sheng


## What, Why and How

[1]: https://arxiv.org/pdf/2211.16961v4.pdf "(PAT) arXiv:2211.16961v4 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/abs/2211.16961 "[2211.16961] Pattern Attention Transformer with Doughnut Kernel - arXiv.org"
[3]: http://arxiv-export2.library.cornell.edu/abs/2211.16961v4 "[2211.16961v4] Pattern Attention Transformer with Doughnut Kernel"

Here is a summary of the paper:

- **What**: The paper proposes a new architecture, the Pattern Attention Transformer (PAT), that uses a new doughnut kernel to enhance the design of patches for Transformer in computer vision[^1^][1].
- **Why**: The paper aims to address the problem of handling the high resolution of pixels in images with Transformer, which is originally designed for natural language processing. The paper argues that the existing patching methods, such as ViT and Swin Transformer, have drawbacks such as fixed boundaries and overlapping features[^1^][1].
- **How**: The paper introduces the doughnut kernel, which replaces the line-cut boundaries with two types of areas: sensor and updating. The sensor area provides ample local units for self-attention, while the updating area updates limited core units. The paper also explores different shapes of kernels beyond square, such as regular octagon. The paper designs PAT with Transformer blocks of regular octagon shape doughnut kernels and evaluates its performance on ImageNet 1K dataset[^1^][1].

## Main Contributions

According to the paper, the contributions are:

- A new kernel, the doughnut kernel, that enhances the design of patches for Transformer in computer vision by using two types of areas: sensor and updating.
- A new topic about the shape of kernels for self-attention, and a demonstration of regular octagon shape doughnut kernels.
- A new architecture, PAT, that is composed of Transformer blocks of regular octagon shape doughnut kernels and achieves higher performance than Swin Transformer on ImageNet 1K.

## Method Summary

[1]: https://arxiv.org/pdf/2211.16961v4.pdf "(PAT) arXiv:2211.16961v4 [cs.CV] 1 Jun 2023"
[2]: https://arxiv.org/pdf/2211.16961.pdf "(PAT) arXiv:2211.16961v3 [cs.CV] 17 Mar 2023"
[3]: http://export.arxiv.org/abs/2203.16961v4 "[2203.16961v4] Sub-additivity of measure-theoretic entropies of ..."

Here is a summary of the method section of the paper:

- The paper first introduces the **QKVA grid**, which is a way of comprehending self-attention with three matrices: queue (Q), key (K), and value (V). The paper shows how QKVA grid can be used to visualize the self-attention process and its output matrix[^1^][1].
- The paper then presents the **doughnut kernel**, which is a new kernel that adapts self-attention with QKVA grid. The doughnut kernel has two types of areas: sensor and updating. The sensor area covers a wider range of units for self-attention, while the updating area updates limited core units. The paper explains how the doughnut kernel can avoid fixed boundaries and overlapping features that exist in existing patching methods[^1^][1].
- The paper also explores the **shape of kernels** for self-attention, and proposes regular octagon as a suitable shape for doughnut kernels. The paper compares regular octagon with square and circle in terms of coverage ratio and boundary length, and shows that regular octagon has advantages over both[^1^][1].
- The paper finally describes the **PAT architecture**, which is composed of Transformer blocks of regular octagon shape doughnut kernels. The paper details the components of PAT, such as pattern attention layer, pattern normalization layer, pattern feed-forward layer, and pattern pooling layer. The paper also discusses the implementation details and hyperparameters of PAT[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the doughnut kernel with sensor and updating areas
def doughnut_kernel(patch_size, sensor_size, updating_size):
  # Initialize the kernel with zeros
  kernel = np.zeros((patch_size, patch_size))
  # Set the sensor area to ones
  kernel[:sensor_size, :sensor_size] = 1
  kernel[:sensor_size, -sensor_size:] = 1
  kernel[-sensor_size:, :sensor_size] = 1
  kernel[-sensor_size:, -sensor_size:] = 1
  # Set the updating area to ones
  kernel[(patch_size-updating_size)//2:(patch_size+updating_size)//2,
         (patch_size-updating_size)//2:(patch_size+updating_size)//2] = 1
  return kernel

# Define the pattern attention layer with doughnut kernels
def pattern_attention_layer(x, num_heads, head_dim):
  # Split x into patches of size patch_size x patch_size
  patches = split_patches(x, patch_size)
  # Apply doughnut kernels to patches to get sensor and updating areas
  sensor_patches = patches * doughnut_kernel(patch_size, sensor_size, 0)
  updating_patches = patches * doughnut_kernel(patch_size, 0, updating_size)
  # Reshape patches into tokens of size head_dim
  sensor_tokens = reshape(sensor_patches, (batch_size, num_heads, num_sensor_tokens, head_dim))
  updating_tokens = reshape(updating_patches, (batch_size, num_heads, num_updating_tokens, head_dim))
  # Compute Q, K, V matrices from tokens
  Q = linear(updating_tokens)
  K = linear(sensor_tokens)
  V = linear(sensor_tokens)
  # Compute attention scores from Q and K matrices
  scores = softmax(Q @ K.transpose(-1, -2) / sqrt(head_dim))
  # Compute output tokens from scores and V matrix
  output_tokens = scores @ V
  # Reshape output tokens into patches of size patch_size x patch_size
  output_patches = reshape(output_tokens, (batch_size, num_heads, patch_size, patch_size))
  # Apply doughnut kernels to output patches to get output areas
  output_areas = output_patches * doughnut_kernel(patch_size, sensor_size-updating_size//2,
                                                  updating_size+updating_size//2)
  # Merge output areas into output x of size H x W x C
  output_x = merge_areas(output_areas)
  return output_x

# Define the pattern normalization layer with layer normalization
def pattern_normalization_layer(x):
  # Apply layer normalization to x along the last dimension
  normalized_x = layer_norm(x)
  return normalized_x

# Define the pattern feed-forward layer with two linear layers and GELU activation
def pattern_feed_forward_layer(x):
  # Apply the first linear layer to x with expansion factor d_ff
  hidden_x = linear(x, out_features=d_ff)
  # Apply GELU activation to hidden_x
  activated_x = gelu(hidden_x)
  # Apply the second linear layer to activated_x with reduction factor d_model
  output_x = linear(activated_x, out_features=d_model)
  return output_x

# Define the pattern pooling layer with average pooling and linear projection
def pattern_pooling_layer(x):
  # Apply average pooling to x with kernel size k and stride s
  pooled_x = avg_pool(x, k, s)
  # Apply linear projection to pooled_x with reduction factor d_model // r
  projected_x = linear(pooled_x, out_features=d_model // r)
  return projected_x

# Define the PAT architecture with four stages of Transformer blocks and a classification head
def PAT(x):
  # Initialize x with a learnable embedding matrix E of size H x W x C
  x = E * x
  # Apply a pattern normalization layer to x
  x = pattern_normalization_layer(x)
  
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the hyperparameters
batch_size = 64 # batch size
H = 224 # image height
W = 224 # image width
C = 3 # image channels
d_model = 96 # model dimension
d_ff = 384 # feed-forward dimension
num_heads = 3 # number of attention heads
head_dim = d_model // num_heads # head dimension
num_stages = 4 # number of stages
num_blocks = [2, 2, 6, 2] # number of blocks per stage
patch_size = [7, 7, 7, 7] # patch size per stage
sensor_size = [5, 5, 5, 5] # sensor size per stage
updating_size = [3, 3, 3, 3] # updating size per stage
num_classes = 1000 # number of classes for classification

# Define the doughnut kernel with sensor and updating areas
def doughnut_kernel(patch_size, sensor_size, updating_size):
  # Initialize the kernel with zeros
  kernel = np.zeros((patch_size, patch_size))
  # Set the sensor area to ones
  kernel[:sensor_size, :sensor_size] = 1
  kernel[:sensor_size, -sensor_size:] = 1
  kernel[-sensor_size:, :sensor_size] = 1
  kernel[-sensor_size:, -sensor_size:] = 1
  # Set the updating area to ones
  kernel[(patch_size-updating_size)//2:(patch_size+updating_size)//2,
         (patch_size-updating_size)//2:(patch_size+updating_size)//2] = 1
  return kernel

# Define the split patches function to split x into patches of size patch_size x patch_size
def split_patches(x, patch_size):
  # Unfold x along the height and width dimensions with kernel size and stride equal to patch_size
  patches = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
  # Reshape patches to have shape (batch_size, C, num_patches_h, num_patches_w, patch_size, patch_size)
  patches = patches.contiguous().view(batch_size, C, -1, patch_size * patch_size)
  # Permute patches to have shape (batch_size, num_patches_h * num_patches_w, C * patch_size * patch_size)
  patches = patches.permute(0, 2, 1).contiguous()
  
```