---
title: 2209.06970v2 Generative Visual Prompt  Unifying Distributional Control of Pre-Trained Generative Models
date: 2022-09-07
---

# [Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models](http://arxiv.org/abs/2209.06970v2)

authors: Chen Henry Wu, Saman Motamed, Shaunak Srivastava, Fernando De la Torre


## What, Why and How

[1]: https://arxiv.org/abs/2209.06970v2 "[2209.06970v2] Generative Visual Prompt: Unifying Distributional ..."
[2]: https://arxiv.org/abs/2209.00796 "[2209.00796] Diffusion Models: A Comprehensive Survey of Methods and ..."
[3]: http://export.arxiv.org/abs/2108.06970v2 "[2108.06970v2] Branching geodesics of the Gromov--Hausdorff distance"

Here is a summary of the paper:

- **What**: The paper proposes a framework called Generative Visual Prompt (PromptGen) for controlling the output distribution of pre-trained generative models by using other off-the-shelf models as energy-based models (EBMs).
- **Why**: The paper aims to address the challenge of sampling from a specific region of the output space or sampling evenly over a range of characteristics, which is often required in many applications of generative models.
- **How**: The paper leverages invertible neural networks to approximate the EBMs and samples images in a feed-forward manner, avoiding optimization at inference. The paper demonstrates how PromptGen can use various off-the-shelf models (such as CLIP, image classifiers, and inverse graphics models) as control for different unconditional generative models (such as StyleGAN2, StyleNeRF, diffusion autoencoder, and NVAE). The paper also shows how PromptGen can de-bias the controlled distribution by iteratively applying PromptGen with different control models.

The summary is based on the abstract and introduction of the paper[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2209.06970v2 "[2209.06970v2] Generative Visual Prompt: Unifying Distributional ..."
[2]: https://arxiv.org/abs/2209.00796 "[2209.00796] Diffusion Models: A Comprehensive Survey of Methods and ..."
[3]: http://export.arxiv.org/abs/2108.06970v2 "[2108.06970v2] Branching geodesics of the Gromov--Hausdorff distance"

According to the paper, the main contributions are:

- **A novel framework for distributional control over pre-trained generative models** by using other off-the-shelf models as energy-based models (EBMs).
- **A feed-forward sampling method** that approximates the EBMs with invertible neural networks, avoiding optimization at inference and enabling efficient sampling.
- **A comprehensive evaluation of PromptGen** on several unconditional generative models and control models, demonstrating its effectiveness and versatility in various scenarios of controlled and de-biased sampling.
- **A discovery of the reporting bias of the CLIP model** when used as control, and a method to iteratively de-bias the controlled distribution with PromptGen.

The contributions are based on section 1.2 of the paper[^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/2209.06970v2 "[2209.06970v2] Generative Visual Prompt: Unifying Distributional ..."
[2]: https://arxiv.org/abs/2209.00796 "[2209.00796] Diffusion Models: A Comprehensive Survey of Methods and ..."
[3]: http://export.arxiv.org/abs/2108.06970v2 "[2108.06970v2] Branching geodesics of the Gromov--Hausdorff distance"

Here is a summary of the method section of the paper:

- The paper first introduces the concept of **generative visual prompt**, which is a function that maps a latent code of a pre-trained generative model to an energy value that indicates how well the generated image matches a desired distribution.
- The paper then describes how to **approximate the generative visual prompt** with an invertible neural network, which allows for efficient sampling by applying the inverse function to a noise vector sampled from a Gaussian distribution with the same energy value.
- The paper also explains how to **train the invertible neural network** by minimizing the KL divergence between the target distribution and the approximated distribution, using a Monte Carlo estimator and a gradient penalty term.
- The paper finally presents several **applications of generative visual prompt** for controlling and de-biasing the output distribution of various unconditional generative models, using different off-the-shelf models as control.

The summary is based on section 3 of the paper[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the pre-trained generative model G and the control model C
G = load_pretrained_generative_model()
C = load_off_the_shelf_control_model()

# Define the generative visual prompt function f
def f(z):
  # Generate an image x from the latent code z
  x = G(z)
  # Compute the energy value of x according to the control model C
  e = C(x)
  # Return the negative energy value as the generative visual prompt
  return -e

# Approximate the generative visual prompt function f with an invertible neural network h
h = InvertibleNeuralNetwork()
# Train h by minimizing the KL divergence between f and h
for epoch in range(num_epochs):
  # Sample a batch of latent codes z from the prior distribution of G
  z = sample_from_prior(G)
  # Compute the loss function as the KL divergence plus a gradient penalty term
  loss = KL_divergence(f(z), h(z)) + gradient_penalty(h)
  # Update the parameters of h by gradient descent
  h.update_parameters(loss)

# Sample images from the approximated generative visual prompt function h
for i in range(num_samples):
  # Sample a noise vector n from a standard Gaussian distribution
  n = sample_from_gaussian()
  # Apply the inverse function of h to n to get a latent code z
  z = h.inverse(n)
  # Generate an image x from z using G
  x = G(z)
  # Save or display the image x
  save_or_display(x)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import numpy as np

# Define the pre-trained generative model G and the control model C
# For example, G can be StyleGAN2 and C can be CLIP
G = load_pretrained_generative_model()
C = load_off_the_shelf_control_model()

# Define the generative visual prompt function f
def f(z):
  # Generate an image x from the latent code z
  x = G(z)
  # Normalize x to the range [0, 1]
  x = (x + 1) / 2
  # Resize x to the input size of C
  x = F.interpolate(x, size=C.input_size)
  # Compute the energy value of x according to the control model C
  # For example, if C is CLIP, we can use a text query as the target distribution
  text_query = "a cute cat"
  text_embedding = C.encode_text(text_query)
  image_embedding = C.encode_image(x)
  e = torch.cosine_similarity(text_embedding, image_embedding, dim=-1)
  # Return the negative energy value as the generative visual prompt
  return -e

# Define the invertible neural network h
# For example, h can be a Glow model with affine coupling layers
h = InvertibleNeuralNetwork()

# Define the optimizer for h
# For example, we can use Adam with a learning rate of 0.001
optimizer = torch.optim.Adam(h.parameters(), lr=0.001)

# Define the hyperparameters for training h
num_epochs = 100 # The number of epochs to train h
batch_size = 16 # The batch size for training h
lambda_ = 10 # The coefficient for the gradient penalty term

# Train h by minimizing the KL divergence between f and h
for epoch in range(num_epochs):
  # Shuffle the latent codes z from the prior distribution of G
  z = shuffle(sample_from_prior(G))
  # Loop over mini-batches of z
  for i in range(0, len(z), batch_size):
    # Get a mini-batch of z
    z_batch = z[i:i+batch_size]
    # Zero the gradients of h
    optimizer.zero_grad()
    # Compute the output of h on z_batch
    h_z_batch = h(z_batch)
    # Compute the KL divergence loss as the difference between f and h
    kl_loss = torch.mean(f(z_batch) - h_z_batch)
    # Compute the gradient penalty loss as the norm of the gradient of h
    grad_h_z_batch = torch.autograd.grad(outputs=h_z_batch, inputs=z_batch,
                                         grad_outputs=torch.ones_like(h_z_batch),
                                         create_graph=True, retain_graph=True)[0]
    grad_penalty_loss = lambda_ * torch.mean(torch.norm(grad_h_z_batch, dim=-1) ** 2)
    # Compute the total loss as the sum of kl_loss and grad_penalty_loss
    loss = kl_loss + grad_penalty_loss
    # Backpropagate the loss and update the parameters of h
    loss.backward()
    optimizer.step()
    # Print or log the loss values

# Define the hyperparameter for sampling images from h
num_samples = 100 # The number of images to sample from h

# Sample images from the approximated generative visual prompt function h
for i in range(num_samples):
  # Sample a noise vector n from a standard Gaussian distribution
  n = sample_from_gaussian()
  # Apply the inverse function of h to n to get a latent code z
  z = h.inverse(n)
  # Generate an image x from z using G
  x = G(z)
  # Normalize x to the range [0, 1]
  x = (x + 1) / 2
  # Save or display the image x
  save_or_display(x)
```