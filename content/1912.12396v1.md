---
title: 1912.12396v1 MulGAN  Facial Attribute Editing by Exemplar
date: 2019-12-13
---

# [MulGAN: Facial Attribute Editing by Exemplar](http://arxiv.org/abs/1912.12396v1)

authors: Jingtao Guo, Zhenzhen Qian, Zuowei Zhou, Yi Liu


## What, Why and How

[1]: https://arxiv.org/abs/1912.12396v1 "[1912.12396v1] MulGAN: Facial Attribute Editing by Exemplar - arXiv.org"
[2]: https://arxiv.org/pdf/1912.12396.pdf "arXiv:1912.12396v1 [cs.CV] 28 Dec 2019"
[3]: https://arxiv.org/pdf/1912.12396v1.pdf "arXiv.org e-Print archive"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- What: The paper proposes a novel method called **MulGAN** for facial attribute editing by exemplar, which can transfer multiple attributes of the exemplar to the source image while preserving attribute-excluding details.
- Why: The paper aims to overcome the limitations of existing methods for facial attribute editing by exemplar, which require opposite-attribute pairs as input, have weak capability of editing multiple attributes by exemplars, and produce poor quality of generated images.
- How: The paper introduces a new model structure that consists of an encoder, a decoder, and an attribute classifier. The encoder extracts the attribute-related and attribute-excluding features from the source image and the exemplar into different regions of the latent feature space. The decoder reconstructs the output image by combining the attribute-related features from the exemplar and the attribute-excluding features from the source image. The attribute classifier ensures that the attribute-related features match the attribute labels of the exemplar. The model is trained with an adversarial loss, a reconstruction loss, and an attribute classification loss.



## Main Contributions

[1]: https://arxiv.org/abs/1912.12396v1 "[1912.12396v1] MulGAN: Facial Attribute Editing by Exemplar - arXiv.org"
[2]: https://arxiv.org/pdf/1912.12396.pdf "arXiv:1912.12396v1 [cs.CV] 28 Dec 2019"
[3]: https://arxiv.org/pdf/1912.12396v1.pdf "arXiv.org e-Print archive"

According to the paper[^1^][1], the main contributions are:

- The paper proposes a novel method called **MulGAN** that can edit multiple attributes by exemplars without requiring opposite-attribute pairs as input.
- The paper introduces a new model structure that consists of an encoder, a decoder, and an attribute classifier, which can enhance attribute transfer capabilities by exemplars while improve the quality of the generated image.
- The paper demonstrates the effectiveness of the proposed method on overcoming the limitations of existing methods by comparing with them on the CelebA dataset.


## Method Summary

[1]: https://arxiv.org/abs/1912.12396v1 "[1912.12396v1] MulGAN: Facial Attribute Editing by Exemplar - arXiv.org"
[2]: https://arxiv.org/pdf/1912.12396.pdf "arXiv:1912.12396v1 [cs.CV] 28 Dec 2019"
[3]: https://arxiv.org/pdf/1912.12396v1.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper[^1^][1]:

- The paper introduces a new model structure that consists of an encoder, a decoder, and an attribute classifier. The encoder extracts the attribute-related and attribute-excluding features from the source image and the exemplar into different regions of the latent feature space. The decoder reconstructs the output image by combining the attribute-related features from the exemplar and the attribute-excluding features from the source image. The attribute classifier ensures that the attribute-related features match the attribute labels of the exemplar.
- The paper defines three losses to train the model: an adversarial loss, a reconstruction loss, and an attribute classification loss. The adversarial loss encourages the output image to be realistic and indistinguishable from real images. The reconstruction loss measures the similarity between the output image and the source image in terms of attribute-excluding details. The attribute classification loss makes sure that the output image has the same attributes as the exemplar.
- The paper describes how to generate multiple attributes by exemplars using the proposed model. The paper first encodes the source image and multiple exemplars into latent feature vectors. Then, it selects the attribute-related features from each exemplar according to their attribute labels and concatenates them into a single vector. Finally, it combines this vector with the attribute-excluding features from the source image and decodes it into an output image.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```
# Define the encoder, decoder, and attribute classifier networks
encoder = Encoder()
decoder = Decoder()
classifier = Classifier()

# Define the adversarial loss, reconstruction loss, and attribute classification loss functions
adv_loss = AdversarialLoss()
rec_loss = ReconstructionLoss()
att_loss = AttributeClassificationLoss()

# Define the optimizer for the model parameters
optimizer = Optimizer()

# Loop over the training data
for source_image, exemplar_image, exemplar_label in data:

  # Encode the source image and the exemplar image into latent feature vectors
  source_feature = encoder(source_image)
  exemplar_feature = encoder(exemplar_image)

  # Split the latent feature vectors into attribute-related and attribute-excluding regions
  source_att_feature, source_exc_feature = split(source_feature)
  exemplar_att_feature, exemplar_exc_feature = split(exemplar_feature)

  # Combine the attribute-related feature from the exemplar and the attribute-excluding feature from the source
  output_feature = concat(exemplar_att_feature, source_exc_feature)

  # Decode the output feature into an output image
  output_image = decoder(output_feature)

  # Classify the output image and the exemplar image by their attributes
  output_label = classifier(output_image)
  exemplar_label_pred = classifier(exemplar_image)

  # Compute the losses
  adv_loss_value = adv_loss(output_image)
  rec_loss_value = rec_loss(output_image, source_image)
  att_loss_value = att_loss(output_label, exemplar_label) + att_loss(exemplar_label_pred, exemplar_label)

  # Compute the total loss as a weighted sum of the losses
  total_loss = alpha * adv_loss_value + beta * rec_loss_value + gamma * att_loss_value

  # Update the model parameters using the optimizer
  optimizer.step(total_loss)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```
# Define the encoder network as a convolutional neural network with residual blocks
encoder = CNN(res_blocks = True)

# Define the decoder network as a deconvolutional neural network with residual blocks and skip connections
decoder = DeCNN(res_blocks = True, skip_connections = True)

# Define the attribute classifier network as a convolutional neural network with a fully connected layer
classifier = CNN(fc_layer = True)

# Define the adversarial loss function as the hinge loss between the output image and the real/fake labels
adv_loss = HingeLoss(output_image, real_fake_label)

# Define the reconstruction loss function as the L1 loss between the output image and the source image
rec_loss = L1Loss(output_image, source_image)

# Define the attribute classification loss function as the cross entropy loss between the predicted and true attribute labels
att_loss = CrossEntropyLoss(predicted_label, true_label)

# Define the optimizer for the model parameters as Adam with a learning rate of 0.0002 and beta1 of 0.5
optimizer = Adam(lr = 0.0002, beta1 = 0.5)

# Define the hyperparameters for the loss weights as alpha = 1, beta = 10, and gamma = 1
alpha = 1
beta = 10
gamma = 1

# Loop over the training data for 200 epochs
for epoch in range(200):

  # Loop over the training data in batches of size 16
  for source_image, exemplar_image, exemplar_label in data.batch(16):

    # Encode the source image and the exemplar image into latent feature vectors of size 256 x 16 x 16
    source_feature = encoder(source_image) # shape: [batch_size, 256, 16, 16]
    exemplar_feature = encoder(exemplar_image) # shape: [batch_size, 256, 16, 16]

    # Split the latent feature vectors into attribute-related and attribute-excluding regions by slicing along the channel dimension
    source_att_feature = source_feature[:, :128, :, :] # shape: [batch_size, 128, 16, 16]
    source_exc_feature = source_feature[:, 128:, :, :] # shape: [batch_size, 128, 16, 16]
    exemplar_att_feature = exemplar_feature[:, :128, :, :] # shape: [batch_size, 128, 16, 16]
    exemplar_exc_feature = exemplar_feature[:, 128:, :, :] # shape: [batch_size, 128, 16, 16]

    # Combine the attribute-related feature from the exemplar and the attribute-excluding feature from the source by concatenating along the channel dimension
    output_feature = concat(exemplar_att_feature, source_exc_feature) # shape: [batch_size, 256, 16, 16]

    # Decode the output feature into an output image of size 3 x 256 x 256
    output_image = decoder(output_feature) # shape: [batch_size, 3, 256, 256]

    # Classify the output image and the exemplar image by their attributes using a binary vector of size 40
    output_label = classifier(output_image) # shape: [batch_size, 40]
    exemplar_label_pred = classifier(exemplar_image) # shape: [batch_size, 40]

    # Compute the losses using the defined loss functions
    adv_loss_value = adv_loss(output_image)
    rec_loss_value = rec_loss(output_image, source_image)
    att_loss_value = att_loss(output_label, exemplar_label) + att_loss(exemplar_label_pred, exemplar_label)

    # Compute the total loss as a weighted sum of the losses
    total_loss = alpha * adv_loss_value + beta * rec_loss_value + gamma * att_loss_value

    # Update the model parameters using the optimizer
    optimizer.step(total_loss)
```