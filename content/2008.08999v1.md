---
title: 2008.08999v1 Object Properties Inferring from and Transfer for Human Interaction Motions
date: 2020-08-09
---

# [Object Properties Inferring from and Transfer for Human Interaction Motions](http://arxiv.org/abs/2008.08999v1)

authors: Qian Zheng, Weikai Wu, Hanting Pan, Niloy Mitra, Daniel Cohen-Or, Hui Huang


## What, Why and How

[1]: https://arxiv.org/abs/2008.08999v1 "[2008.08999v1] Object Properties Inferring from and Transfer for Human ..."
[2]: https://arxiv.org/pdf/2008.08999 "arXiv.org"
[3]: http://export.arxiv.org/abs/2303.08999v1 "[2303.08999v1] A High-Performance Accelerator for Super-Resolution ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method to infer object properties (such as weight, fragility, or delicacy) from human interaction motions alone, without seeing the interacting object itself.
- **Why**: The paper aims to reveal the latent correlation between human motions and object properties, and to enable new synthesis possibilities for human interaction motions by transferring object properties to a given motion.
- **How**: The paper collects a large dataset of videos and 3D skeletal motions of actors performing different actions with different objects. The paper then uses a fine-grained action recognition network to classify the actions and estimate the object properties from the 3D skeleton sequences. The paper also proposes a motion disentanglement and transfer framework to manipulate the object properties of a given motion.

## Main Contributions

[1]: https://arxiv.org/abs/2008.08999v1 "[2008.08999v1] Object Properties Inferring from and Transfer for Human ..."
[2]: https://arxiv.org/pdf/2008.08999 "arXiv.org"
[3]: http://export.arxiv.org/abs/2303.08999v1 "[2303.08999v1] A High-Performance Accelerator for Super-Resolution ..."

According to the paper[^1^][1], the main contributions are:

- **A novel problem formulation** of inferring object properties from human interaction motions alone, without seeing the interacting object itself.
- **A large-scale dataset** of videos and 3D skeletal motions of actors performing different actions with different objects, annotated with fine-grained action labels and object properties.
- **A fine-grained action recognition network** that learns to classify the actions and estimate the object properties from the 3D skeleton sequences, achieving high accuracy and generalization.
- **A motion disentanglement and transfer framework** that allows manipulating the object properties of a given motion, leading to new synthesis possibilities for human interaction motions.

## Method Summary

[1]: https://arxiv.org/abs/2008.08999v1 "[2008.08999v1] Object Properties Inferring from and Transfer for Human ..."
[2]: https://arxiv.org/pdf/2008.08999 "arXiv.org"
[3]: http://export.arxiv.org/abs/2303.08999v1 "[2303.08999v1] A High-Performance Accelerator for Super-Resolution ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper proposes a **two-stage framework** for inferring object properties from human interaction motions and transferring them to a given motion.
- The first stage is **fine-grained action recognition**, which aims to classify the actions and estimate the object properties from the 3D skeleton sequences. The paper uses a **3D convolutional neural network** (3D-CNN) to extract spatio-temporal features from the skeleton sequences, and a **multi-task learning** (MTL) scheme to jointly predict the action labels and object properties. The paper also introduces a **motion similarity loss** (MSL) to enforce the network to learn subtle differences among similar actions.
- The second stage is **motion disentanglement and transfer**, which aims to manipulate the object properties of a given motion. The paper uses a **variational autoencoder** (VAE) to encode the motion features into a latent space, where the motion can be disentangled into two components: one related to the action type and the other related to the object property. The paper then uses a **conditional generative adversarial network** (cGAN) to generate new motion features by conditioning on the desired object property. The paper also introduces a **motion reconstruction loss** (MRL) and a **motion adversarial loss** (MAL) to ensure the quality and diversity of the generated motions.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a 3D skeleton sequence S of length T
# Output: a new 3D skeleton sequence S' of length T with a different object property P'

# Fine-grained action recognition
# Define a 3D-CNN network F with MTL scheme
# Train F on a large dataset of skeleton sequences with action labels and object properties
# Predict the action label A and the object property P of S using F

# Motion disentanglement and transfer
# Define a VAE network E to encode S into a latent vector Z
# Train E on the same dataset of skeleton sequences
# Encode S into Z using E
# Split Z into two components: Z_a related to A and Z_p related to P
# Define a cGAN network G to generate a new latent vector Z' conditioned on a desired object property P'
# Train G on the same dataset of skeleton sequences
# Generate Z' using G and P'
# Concatenate Z_a and Z' to form a new latent vector Z''
# Define a decoder network D to decode Z'' into a new skeleton sequence S'
# Train D on the same dataset of skeleton sequences
# Decode Z'' into S' using D

# Return S'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define hyperparameters
batch_size = 32 # the number of skeleton sequences in a batch
T = 64 # the length of a skeleton sequence
N = 25 # the number of joints in a skeleton
D = 3 # the dimension of a joint position
C = 10 # the number of action classes
K = 3 # the number of object properties
L = 128 # the dimension of the latent vector
M = 64 # the dimension of the action-related component
N = 64 # the dimension of the property-related component

# Define a 3D-CNN network F with MTL scheme
class F(nn.Module):
    def __init__(self):
        super(F, self).__init__()
        # Define convolutional layers
        self.conv1 = nn.Conv3d(D, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, padding=1)
        self.conv4 = nn.Conv3d(64, 128, kernel_size=3, padding=1)
        # Define pooling layers
        self.pool1 = nn.MaxPool3d(kernel_size=(2,2,2))
        self.pool2 = nn.MaxPool3d(kernel_size=(2,2,2))
        self.pool3 = nn.MaxPool3d(kernel_size=(2,2,2))
        self.pool4 = nn.MaxPool3d(kernel_size=(2,2,2))
        # Define fully connected layers for action classification
        self.fc1_a = nn.Linear(128 * (T//16) * (N//16) * (D//16), 256)
        self.fc2_a = nn.Linear(256, C)
        # Define fully connected layers for object property estimation
        self.fc1_p = nn.Linear(128 * (T//16) * (N//16) * (D//16), 256)
        self.fc2_p = nn.Linear(256, K)

    def forward(self, x):
        # x is a batch of skeleton sequences of shape (batch_size, D, T, N, D)
        x = F.relu(self.conv1(x)) # shape: (batch_size, 16, T, N, D)
        x = self.pool1(x) # shape: (batch_size, 16, T//2, N//2, D//2)
        x = F.relu(self.conv2(x)) # shape: (batch_size, 32, T//2, N//2, D//2)
        x = self.pool2(x) # shape: (batch_size, 32, T//4, N//4, D//4)
        x = F.relu(self.conv3(x)) # shape: (batch_size, 64, T//4, N//4, D//4)
        x = self.pool3(x) # shape: (batch_size, 64, T//8, N//8 ,D//8)
        x = F.relu(self.conv4(x)) # shape: (batch_size ,128 ,T//8 ,N//8 ,D//8)
        x = self.pool4(x) # shape: (batch_size ,128 ,T//16 ,N//16 ,D//16)
        x = x.view(batch_size , -1) # shape: (batch_size ,128 * (T//16) * (N//16) * (D//16))
        # For action classification
        x_a = F.relu(self.fc1_a(x)) # shape: (batch_size ,256)
        x_a = F.softmax(self.fc2_a(x_a), dim=1) # shape: (batch_size ,C)
        # For object property estimation
        x_p = F.relu(self.fc1_p(x)) # shape: (batch_size ,256)
        x_p = F.softmax(self.fc2_p(x_p), dim=1) # shape: (batch_size ,K)

        return x_a ,x_p

# Define a VAE network E to encode S into a latent vector Z
class E(nn.Module):
    def __init__(self):
        super(E ,self).__init__()
        # Define convolutional layers
        self.conv1 = nn.Conv3d(D ,16 ,kernel_size=3 ,padding=1)
        self.conv2 = nn.Conv3d(16 ,32 ,kernel_size=3 ,padding=1)
        self.conv3 = nn.Conv3d(32 ,64 ,kernel_size=3 ,padding=1)
        self.conv4 = nn.Conv3d(64 ,128 ,kernel_size=3 ,padding=1)
        # Define pooling layers
        self.pool1 = nn.MaxPool3d(kernel_size=(2,2,2))
        self.pool2 = nn.MaxPool3d(kernel_size=(2,2,2))
        self.pool3 = nn.MaxPool3d(kernel_size=(2,2,2))
        self.pool4 = nn.MaxPool3d(kernel_size=(2,2,2))
        # Define fully connected layers for mean and log variance of Z
        self.fc1_mu = nn.Linear(128 * (T//16) * (N//16) * (D//16), L)
        self.fc1_logvar = nn.Linear(128 * (T//16) * (N//16) * (D//16), L)

    def forward(self, x):
        # x is a batch of skeleton sequences of shape (batch_size, D, T, N, D)
        x = F.relu(self.conv1(x)) # shape: (batch_size, 16, T, N, D)
        x = self.pool1(x) # shape: (batch_size, 16, T//2, N//2, D//2)
        x = F.relu(self.conv2(x)) # shape: (batch_size, 32, T//2, N//2, D//2)
        x = self.pool2(x) # shape: (batch_size, 32, T//4, N//4, D//4)
        x = F.relu(self.conv3(x)) # shape: (batch_size, 64, T//4, N//4, D//4)
        x = self.pool3(x) # shape: (batch_size, 64, T//8, N//8 ,D//8)
        x = F.relu(self.conv4(x)) # shape: (batch_size ,128 ,T//8 ,N//8 ,D//8)
        x = self.pool4(x) # shape: (batch_size ,128 ,T//16 ,N//16 ,D//16)
        x = x.view(batch_size , -1) # shape: (batch_size ,128 * (T//16) * (N//16) * (D//16))
        # For mean and log variance of Z
        mu = self.fc1_mu(x) # shape: (batch_size ,L)
        logvar = self.fc1_logvar(x) # shape: (batch_size ,L)

        return mu ,logvar

# Define a decoder network D to decode Z into a new skeleton sequence S
class D(nn.Module):
    def __init__(self):
        super(D ,self).__init__()
        # Define fully connected layer for latent vector
        self.fc1 = nn.Linear(L ,128 * (T//16) * (N//16) * (D//16))
        # Define deconvolutional layers
        self.deconv1 = nn.ConvTranspose3d(128 ,64 ,kernel_size=3 ,padding=1)
        self.deconv2 = nn.ConvTranspose3d(64 ,32 ,kernel_size=3 ,padding=1)
        self.deconv3 = nn.ConvTranspose3d(32 ,16 ,kernel_size=3 ,padding=1)
        self.deconv4 = nn.ConvTranspose3d(16 ,D ,kernel_size=3 ,padding=1)
        # Define unpooling layers
        self.unpool1 = nn.Upsample(scale_factor=(2,2,2))
        self.unpool2 = nn.Upsample(scale_factor=(2,2,2))
        self.unpool3 = nn.Upsample(scale_factor=(2,2,2))
        self.unpool4 = nn.Upsample(scale_factor=(2,2,2))

    def forward(self, z):
        # z is a batch of latent vectors of shape (batch_size ,L)
        z = F.relu(self.fc1(z)) # shape: (batch_size ,128 * (T//16) * (N//16) * (D//16))
        z = z.view(batch_size ,-1 ,(T // 16), (N // 16), (D // 16)) # shape: (batch_size ,-128 ,-T // 16 ,-N // 16 ,-D // 16)
        z = F.relu(self.deconv1(z)) # shape: (batch_size ,-64 ,-T // 8 ,-N // 8 ,-D // 8)
        z = self.unpool1(z) # shape: (batch