---
title: 2112.04482v3 FLAVA  A Foundational Language And Vision Alignment Model
date: 2021-12-05
---

# [FLAVA: A Foundational Language And Vision Alignment Model](http://arxiv.org/abs/2112.04482v3)

authors: Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, Douwe Kiela


## What, Why and How

[1]: https://arxiv.org/abs/2112.04482 "FLAVA: A Foundational Language And Vision Alignment Model"
[2]: https://arxiv.org/pdf/2112.04482.pdf "FLAVA: A Foundational Language And Vision Alignment Model - arXiv.org"
[3]: http://export.arxiv.org/pdf/1909.04482 "arXiv:1909.04482v3 [math.CO] 12 Jan 2022"

Here is a summary of the paper:

- **What**: The paper introduces FLAVA, a Foundational Language And Vision Alignment Model, that learns from multimodal (image-text pairs) and unimodal (unpaired images and text) data and can be applied to a broad scope of tasks from three domains: visual recognition, language understanding, and multimodal reasoning[^1^][1].
- **Why**: The paper aims to overcome the limitations of existing vision and vision-and-language models that are either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both, and that often only target specific modalities or tasks. The paper argues that a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks[^1^][1].
- **How**: The paper proposes a single holistic universal model architecture that combines global contrastive learning with multimodal masked language modeling (MMM), multimodal image modeling (MIM), and masked language modeling (MLM). The paper also introduces a new pretraining dataset called FLAVA-Data that contains 1.2 billion image-text pairs collected from the web. The paper evaluates FLAVA on 35 tasks spanning the three target domains and shows that it outperforms or matches state-of-the-art models on most of them[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2112.04482 "FLAVA: A Foundational Language And Vision Alignment Model"
[2]: https://arxiv.org/pdf/2112.04482.pdf "FLAVA: A Foundational Language And Vision Alignment Model - arXiv.org"

According to the paper[^1^][1], the main contributions are:

- **A new model architecture**: FLAVA combines global contrastive learning with multimodal masked language modeling (MMM), multimodal image modeling (MIM), and masked language modeling (MLM) in a single holistic universal model that targets all modalities at once[^1^][1].
- **A new pretraining dataset**: FLAVA-Data contains 1.2 billion image-text pairs collected from the web using a novel data collection pipeline that leverages natural language queries and image captions[^1^][1].
- **A comprehensive evaluation**: FLAVA is evaluated on 35 tasks spanning visual recognition, language understanding, and multimodal reasoning domains, and shows that it outperforms or matches state-of-the-art models on most of them[^1^][1].

## Method Summary

[1]: https://paperswithcode.com/method/flava "FLAVA Explained | Papers With Code"
[2]: https://arxiv.org/pdf/2112.04482.pdf "FLAVA: A Foundational Language And Vision Alignment Model - arXiv.org"

Here is a summary of the method section of the paper:

- **Model architecture**: FLAVA consists of three transformers: an image encoder transformer, a text encoder transformer, and a multimodal encoder transformer. The image encoder takes as input a sequence of image patches and outputs unimodal image representations. The text encoder takes as input a sequence of tokens and outputs unimodal text representations. The multimodal encoder takes as input the encoded unimodal image and text and integrates their representations for multimodal reasoning[^1^][2].
- **Pretraining objectives**: FLAVA uses four types of pretraining objectives: global contrastive learning, multimodal masked language modeling (MMM), multimodal image modeling (MIM), and masked language modeling (MLM). Global contrastive learning aims to align the image and text representations across modalities. MMM and MIM aim to predict the masked tokens or patches in the image-text pairs. MLM aims to predict the masked tokens in the unpaired text[^1^][2].
- **Pretraining dataset**: FLAVA uses a new pretraining dataset called FLAVA-Data that contains 1.2 billion image-text pairs collected from the web using a novel data collection pipeline that leverages natural language queries and image captions. FLAVA-Data covers diverse topics and domains such as animals, sports, celebrities, etc[^1^][2].
- **Downstream tasks**: FLAVA is evaluated on 35 tasks spanning visual recognition, language understanding, and multimodal reasoning domains. For visual recognition tasks, such as ImageNet or COCO detection, FLAVA uses the output from the image encoder and adds a task-specific head. For language understanding tasks, such as GLUE or SQuAD, FLAVA uses the output from the text encoder and adds a task-specific head. For multimodal reasoning tasks, such as VQA or NLVR2, FLAVA uses the output from the multimodal encoder and adds a task-specific head[^1^][2].

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```
# Define the model architecture
FLAVA = Transformer(image_encoder, text_encoder, multimodal_encoder)

# Define the pretraining objectives
contrastive_loss = Contrastive(image_encoder, text_encoder)
mmm_loss = MaskedLanguageModeling(multimodal_encoder)
mim_loss = MaskedImageModeling(multimodal_encoder)
mlm_loss = MaskedLanguageModeling(text_encoder)

# Load the pretraining dataset
FLAVA_Data = LoadData(image_text_pairs, unpaired_images, unpaired_text)

# Pretrain the model on FLAVA-Data
for each batch in FLAVA_Data:
  # Get the inputs and labels
  image_text_pair, unpaired_image, unpaired_text = batch
  image, text = image_text_pair
  # Forward pass
  image_rep = image_encoder(image)
  text_rep = text_encoder(text)
  multimodal_rep = multimodal_encoder(image_rep, text_rep)
  # Compute the losses
  contrastive_loss = contrastive_loss(image_rep, text_rep)
  mmm_loss = mmm_loss(multimodal_rep, text)
  mim_loss = mim_loss(multimodal_rep, image)
  mlm_loss = mlm_loss(text_rep, text)
  # Backward pass and update parameters
  total_loss = contrastive_loss + mmm_loss + mim_loss + mlm_loss
  total_loss.backward()
  optimizer.step()

# Evaluate the model on downstream tasks
for each task in tasks:
  # Load the task-specific data and head
  task_data, task_head = LoadTask(task)
  # Choose the encoder output based on the task modality
  if task is visual recognition:
    encoder_output = image_rep
  elif task is language understanding:
    encoder_output = text_rep
  elif task is multimodal reasoning:
    encoder_output = multimodal_rep
  # Fine-tune the model on the task data
  for each batch in task_data:
    # Get the inputs and labels
    input, label = batch
    # Forward pass
    encoder_output = FLAVA(input)
    output = task_head(encoder_output)
    # Compute the loss
    task_loss = Loss(output, label)
    # Backward pass and update parameters
    task_loss.backward()
    optimizer.step()
```


## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the model architecture
class FLAVA(torch.nn.Module):
  def __init__(self, image_encoder_config, text_encoder_config, multimodal_encoder_config):
    super(FLAVA, self).__init__()
    # Initialize the image encoder transformer
    self.image_encoder = transformers.ViTModel(image_encoder_config)
    # Initialize the text encoder transformer
    self.text_encoder = transformers.BertModel(text_encoder_config)
    # Initialize the multimodal encoder transformer
    self.multimodal_encoder = transformers.BertModel(multimodal_encoder_config)

  def forward(self, image, text):
    # Encode the image patches
    image_rep = self.image_encoder(image)
    # Encode the text tokens
    text_rep = self.text_encoder(text)
    # Concatenate the image and text representations
    multimodal_input = torch.cat([image_rep, text_rep], dim=1)
    # Encode the multimodal input
    multimodal_rep = self.multimodal_encoder(multimodal_input)
    # Return the encoder outputs
    return image_rep, text_rep, multimodal_rep

# Define the pretraining objectives
class Contrastive(torch.nn.Module):
  def __init__(self, temperature):
    super(Contrastive, self).__init__()
    # Initialize the temperature parameter
    self.temperature = temperature

  def forward(self, image_rep, text_rep):
    # Compute the cosine similarity matrix between image and text representations
    sim_matrix = torch.matmul(image_rep, text_rep.t()) / self.temperature
    # Compute the contrastive loss using cross entropy with hard negatives
    image_loss = torch.nn.functional.cross_entropy(sim_matrix, torch.arange(sim_matrix.size(0)))
    text_loss = torch.nn.functional.cross_entropy(sim_matrix.t(), torch.arange(sim_matrix.size(0)))
    contrastive_loss = (image_loss + text_loss) / 2
    # Return the contrastive loss
    return contrastive_loss

class MaskedLanguageModeling(torch.nn.Module):
  def __init__(self, encoder_config, vocab_size):
    super(MaskedLanguageModeling, self).__init__()
    # Initialize the linear layer for prediction
    self.linear = torch.nn.Linear(encoder_config.hidden_size, vocab_size)

  def forward(self, encoder_output, labels):
    # Predict the logits for each token
    logits = self.linear(encoder_output)
    # Compute the masked language modeling loss using cross entropy with ignore index
    mlm_loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1), ignore_index=-100)
    # Return the masked language modeling loss
    return mlm_loss

class MaskedImageModeling(torch.nn.Module):
  def __init__(self, encoder_config):
    super(MaskedImageModeling, self).__init__()
    # Initialize the linear layer for prediction
    self.linear = torch.nn.Linear(encoder_config.hidden_size, encoder_config.hidden_size)

  def forward(self, encoder_output, labels):
    # Predict the logits for each patch
    logits = self.linear(encoder_output)
    # Compute the masked image modeling loss using mean squared error with ignore index
    mask = (labels != -100).unsqueeze(-1)
    mim_loss = torch.nn.functional.mse_loss(logits[mask], labels[mask], reduction='mean')
    # Return the masked image modeling loss
    return mim_loss

# Load the pretraining dataset
FLAVA_Data = LoadData(image_text_pairs, unpaired_images, unpaired_text)

# Initialize the model and the objectives
FLAVA = FLAVA(image_encoder_config, text_encoder_config, multimodal_encoder_config)
contrastive_loss = Contrastive(temperature)
mmm_loss = MaskedLanguageModeling(multimodal_encoder_config, vocab_size)
mim_loss = MaskedImageModeling(multimodal_encoder_config)
mlm_loss = MaskedLanguageModeling(text_encoder_config, vocab_size)

# Initialize the optimizer and the learning rate scheduler
optimizer = transformers.AdamW(FLAVA.parameters(), lr=learning_rate)
scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)

# Pretrain the model on FLAVA-Data
for epoch in range(num_epochs):
  for step, batch in enumerate(FLAVA_Data):
    # Get the inputs and labels
    image_text_pair, unpaired_image, unpaired_text = batch
    image, text = image_text_pair
    # Move the inputs and labels to device (GPU or CPU)
    image = image.to(device)
    text = text.to(device)
    unpaired_image = unpaired_image.to(device)
    unpaired_text = unpaired_text.to(device)
    # Forward pass
    image_rep, text_rep, multimodal_rep = FLAVA(image, text)
    # Compute the losses
    contrastive_loss = contrastive_loss(image_rep, text_rep)
    mmm_loss = mmm_loss(multimodal_rep, text)
    mim_loss = mim_loss(multimodal_rep, image)
    mlm_loss = mlm_loss(text_rep, text)
    # Backward pass and update parameters
    total_loss = contrastive_loss + mmm_loss + mim_loss + mlm_loss
    total_loss.backward()
    optimizer.step()
    scheduler.step()
    FLAVA.zero_grad()
    # Print the loss every logging_steps
    if step % logging_steps == 0:
      print(f"Epoch {epoch}, Step {step}, Loss {total_loss.item()}")

# Save the pretrained model
torch.save(FLAVA.state_dict(), "FLAVA.pth")

# Evaluate the model on downstream tasks
for task in tasks:
  # Load the task-specific data and head
  task_data, task_head = LoadTask(task)
  # Choose the encoder output based on the task modality
  if task is visual recognition:
    encoder_output = image_rep
  elif task is language understanding:
    encoder_output = text_rep
  elif task is multimodal reasoning:
    encoder_output = multimodal_rep
  # Fine-tune the model on the task data
  for epoch in range(num_epochs):
    for step, batch in enumerate(task_data):
      # Get the inputs and labels
      input, label = batch
      # Move the inputs and labels to device (GPU or CPU)
      input = input.to(device)
      label = label.to(device)
      # Forward pass
      encoder_output = FLAVA(input)
      output = task_head(encoder_output)
      # Compute the task loss using cross entropy or other appropriate loss function
      task_loss = Loss(output, label)
      # Backward pass and update parameters
      task_loss.backward()
      optimizer.step()
      scheduler.step()
      FLAVA.zero_grad()
      # Print the loss every logging_steps
      if step % logging_steps == 0:
        print(f"Epoch {epoch}, Step {step}, Loss {task_loss.item()}")
  # Save the fine-tuned model for each task
  torch.save(FLAVA.state_dict(), f"FLAVA_{task}.pth")
```