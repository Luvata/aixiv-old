---
title: 2112.06825v2 VL-Adapter  Parameter-Efficient Transfer Learning for Vision-and-Language Tasks
date: 2021-12-07
---

# [VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks](http://arxiv.org/abs/2112.06825v2)

authors: Yi-Lin Sung, Jaemin Cho, Mohit Bansal


## What, Why and How

[1]: https://arxiv.org/abs/2112.06825 "[2112.06825] VL-Adapter: Parameter-Efficient Transfer Learning for ..."
[2]: https://arxiv.org/pdf/2112.06825v2.pdf "fylsung,jmincho,mbansalg@cs.unc.edu arXiv:2112.06825v2 [cs.CV] 24 Mar 2022"
[3]: http://export.arxiv.org/abs/2112.06825 "[2112.06825] VL-Adapter: Parameter-Efficient Transfer Learning for ..."

Here is a summary of the paper[^1^][1]:

- **What**: The paper introduces adapter-based parameter-efficient transfer learning techniques to vision-and-language (V&L) models such as VL-BART and VLT5. It evaluates the methods on both image-text and video-text benchmarks, and compares them with full fine-tuning and prompt-tuning approaches. It also enhances the efficiency and performance of adapters by sharing their weights across tasks.
- **Why**: The paper aims to address the challenge of fine-tuning large-scale pre-trained V&L models for downstream tasks, which can be costly in terms of memory and storage. It also seeks to improve the generalization and adaptability of V&L models to diverse tasks and domains.
- **How**: The paper adopts three popular adapter-based methods (Adapter, Hyperformer, Compacter) that insert small modules between the layers of pre-trained models and only update these modules during fine-tuning. It also proposes a weight-sharing technique that allows adapters to share parameters across different tasks. It conducts experiments on eight V&L datasets covering image-text and video-text tasks, and analyzes the results in terms of accuracy, efficiency, and ablation studies.

## Main Contributions

[1]: https://arxiv.org/abs/2112.06825 "[2112.06825] VL-Adapter: Parameter-Efficient Transfer Learning for ..."
[2]: https://arxiv.org/pdf/2112.06825.pdf "fylsung,jmincho,mbansalg@cs.unc.edu arXiv:2112.06825v2 [cs.CV] 24 Mar 2022"
[3]: https://openaccess.thecvf.com/content/CVPR2022/papers/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.pdf "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and ..."

The paper claims the following contributions[^1^][1]:

- It introduces adapter-based parameter-efficient transfer learning techniques to V&L models such as VL-BART and VLT5, which can reduce the number of updated parameters by more than 95% compared to full fine-tuning.
- It evaluates the methods on a unified multi-task setup on both image-text and video-text benchmarks, covering eight diverse V&L datasets and tasks.
- It benchmarks three popular adapter-based methods (Adapter, Hyperformer, Compacter) against the standard full fine-tuning and the recently proposed prompt-tuning approach, and shows that adapters can match or outperform them in most cases.
- It enhances the efficiency and performance of adapters by sharing their weights across different tasks, and demonstrates that weight-sharing can improve the generalization and adaptability of V&L models.
- It presents a comprehensive analysis of the combination of adapter and task-specific prompts, the impact of V&L pre-training on adapters, and the ablation studies of different adapter settings.

## Method Summary

[1]: https://arxiv.org/abs/2112.06825 "[2112.06825] VL-Adapter: Parameter-Efficient Transfer Learning for ..."
[2]: https://arxiv.org/pdf/2112.06825.pdf "fylsung,jmincho,mbansalg@cs.unc.edu arXiv:2112.06825v2 [cs.CV] 24 Mar 2022"
[3]: https://openaccess.thecvf.com/content/CVPR2022/papers/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.pdf "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper adopts three popular adapter-based methods (Adapter, Hyperformer, Compacter) that insert small modules between the layers of pre-trained models and only update these modules during fine-tuning. The modules consist of two linear transformations with a non-linear activation function in between. The paper also proposes a weight-sharing technique that allows adapters to share parameters across different tasks by using a task embedding vector as an input to the adapter modules.
- The paper uses two V&L models as base models: VL-BART and VLT5, which are pre-trained on large-scale image-text and video-text data respectively. The paper also uses CLIP [34] as a vision encoder to extract visual features from images and videos. The paper fine-tunes the V&L models with adapters on eight V&L datasets covering four image-text tasks (VQAv2, GQA, NLVR2, MSCOCO image captioning) and four video-text tasks (TVQA, How2QA, TVC, YC2C).
- The paper compares the performance and efficiency of adapter-based methods with full fine-tuning and prompt-tuning approaches. Full fine-tuning updates all the parameters of the pre-trained models, while prompt-tuning only updates a small set of task-specific tokens that are prepended or appended to the input sequence. The paper measures the accuracy, inference speed, and parameter size of each method on each task. The paper also conducts ablation studies to analyze the impact of different adapter settings, such as the number of adapter layers, the adapter dimension, and the weight-sharing scheme.

## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the base V&L model (VL-BART or VLT5)
base_model = VLBART() or VLT5()

# Define the adapter modules
adapter_modules = [Adapter(), Hyperformer(), Compacter()]

# Define the task embedding vector
task_embedding = Embedding(num_tasks, embedding_dim)

# Define the loss function and optimizer
loss_fn = CrossEntropyLoss()
optimizer = AdamW(adapter_modules.parameters())

# Loop over the tasks and datasets
for task, dataset in tasks_and_datasets:

  # Loop over the batches of data
  for batch in dataset:

    # Get the input sequence and the label
    input_seq = batch["input_seq"]
    label = batch["label"]

    # Prepend or append the task-specific prompt to the input sequence
    input_seq = add_prompt(input_seq, task)

    # Get the task embedding vector
    task_emb = task_embedding(task)

    # Pass the input sequence and the task embedding vector to the base model
    output = base_model(input_seq, task_emb)

    # Compute the loss
    loss = loss_fn(output, label)

    # Update the adapter modules
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from transformers import VLBartModel, VLT5Model, CLIPModel

# Define the adapter module class
class Adapter(nn.Module):

  # Initialize the adapter module with the input dimension and the adapter dimension
  def __init__(self, input_dim, adapter_dim):
    super(Adapter, self).__init__()

    # Define the two linear transformations with a ReLU activation in between
    self.linear1 = nn.Linear(input_dim, adapter_dim)
    self.linear2 = nn.Linear(adapter_dim, input_dim)
    self.relu = nn.ReLU()

  # Forward pass the input and the task embedding vector through the adapter module
  def forward(self, x, task_emb):

    # Concatenate the input and the task embedding vector along the last dimension
    x = torch.cat([x, task_emb], dim=-1)

    # Pass the concatenated vector through the two linear transformations and the ReLU activation
    x = self.linear1(x)
    x = self.relu(x)
    x = self.linear2(x)

    # Return the output of the adapter module
    return x

# Define the hyperformer module class (similar to adapter but with an additional attention layer)
class Hyperformer(nn.Module):

  # Initialize the hyperformer module with the input dimension, the adapter dimension, and the number of attention heads
  def __init__(self, input_dim, adapter_dim, num_heads):
    super(Hyperformer, self).__init__()

    # Define the two linear transformations with a ReLU activation in between
    self.linear1 = nn.Linear(input_dim, adapter_dim)
    self.linear2 = nn.Linear(adapter_dim, input_dim)
    self.relu = nn.ReLU()

    # Define the multi-head attention layer
    self.attention = nn.MultiheadAttention(adapter_dim, num_heads)

  # Forward pass the input and the task embedding vector through the hyperformer module
  def forward(self, x, task_emb):

    # Concatenate the input and the task embedding vector along the last dimension
    x = torch.cat([x, task_emb], dim=-1)

    # Pass the concatenated vector through the first linear transformation and the ReLU activation
    x = self.linear1(x)
    x = self.relu(x)

    # Pass the output through the multi-head attention layer
    x = self.attention(x, x, x)[0]

    # Pass the output through the second linear transformation
    x = self.linear2(x)

    # Return the output of the hyperformer module
    return x

# Define the compacter module class (similar to adapter but with a smaller adapter dimension)
class Compacter(nn.Module):

  # Initialize the compacter module with the input dimension and the compacter dimension (smaller than adapter dimension)
  def __init__(self, input_dim, compacter_dim):
    super(Compacter, self).__init__()

    # Define the two linear transformations with a ReLU activation in between
    self.linear1 = nn.Linear(input_dim, compacter_dim)
    self.linear2 = nn.Linear(compacter_dim, input_dim)
    self.relu = nn.ReLU()

  # Forward pass the input and the task embedding vector through the compacter module
  def forward(self, x, task_emb):

    # Concatenate the input and the task embedding vector along the last dimension
    x = torch.cat([x, task_emb], dim=-1)

    # Pass the concatenated vector through the two linear transformations and the ReLU activation
    x = self.linear1(x)
    x = self.relu(x)
    x = self.linear2(x)

    # Return the output of the compacter module
    return x

# Define a dictionary of adapter modules
adapter_modules = {"Adapter": Adapter(), "Hyperformer": Hyperformer(), "Compacter": Compacter()}

# Define a dictionary of base V&L models (VL-BART or VLT5) and load their pre-trained weights
base_models = {"VL-BART": VLBartModel.from_pretrained("facebook/vl-bart-large-coco"), "VLT5": VLT5Model.from_pretrained("allenai/vlt5-large")}

# Define a CLIP model as a vision encoder and load its pre-trained weights
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

# Define a task embedding vector with an embedding layer
task_embedding = nn.Embedding(num_tasks, embedding_dim)

# Define a cross entropy loss function
loss_fn = nn.CrossEntropyLoss()

# Define an AdamW optimizer for the adapter modules
optimizer = optim.AdamW(adapter_modules.parameters())

# Loop over the tasks and datasets
for task, dataset in tasks_and_datasets:

  # Get the base V&L model for the task
  base_model = base_models[task]

  # Get the adapter module for the task
  adapter_module = adapter_modules[task]

  # Loop over the batches of data
  for batch in dataset:

    # Get the input sequence and the label
    input_seq = batch["input_seq"]
    label = batch["label"]

    # Prepend or append the task-specific prompt to the input sequence
    input_seq = add_prompt(input_seq, task)

    # Get the task embedding vector
    task_emb = task_embedding(task)

    # Extract the visual features from the images or videos using CLIP model
    visual_features = clip_model(batch["images"] or batch["videos"])[0]

    # Pass the input sequence, the visual features, and the task embedding vector to the base model
    output = base_model(input_seq, visual_features, task_emb)

    # Pass the output and the task embedding vector to the adapter module
    output = adapter_module(output, task_emb)

    # Compute the loss
    loss = loss_fn(output, label)

    # Update the adapter module
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```