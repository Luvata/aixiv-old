---
title: 2305.17262v2 Im-Promptu  In-Context Composition from Image Prompts
date: 2023-05-18
---

# [Im-Promptu: In-Context Composition from Image Prompts](http://arxiv.org/abs/2305.17262v2)

authors: Bhishma Dedhia, Michael Chang, Jake C. Snell, Thomas L. Griffiths, Niraj K. Jha


## What, Why and How

[1]: https://arxiv.org/abs/2305.17262v2 "Im-Promptu: In-Context Composition from Image Prompts"
[2]: https://arxiv.org/abs/2211.17262v2 "[2211.17262v2] Non-Deterministic Approximation Fixpoint ... - arXiv.org"
[3]: http://export.arxiv.org/pdf/2305.17262 "export.arxiv.org"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces a meta-learning framework called **Im-Promptu** that can perform **in-context composition** from image prompts using analogical reasoning and large language models.
- **Why**: The paper aims to test the generalization properties of a visual in-context learner and to explore the tradeoffs between extrapolation abilities and the degree of compositionality in different visual representations.
- **How**: The paper proposes a notion of an analogy-based in-context learner and uses it to design Im-Promptu, which consists of a tokenizer, an encoder, a cross-attention module, and a decoder. The paper also introduces a suite of three benchmarks to evaluate Im-Promptu on various tasks such as image completion, image transformation, and image generation. The paper compares Im-Promptu with different levels of compositionality, including vector representations, patch representations, and object slots.


## Main Contributions

[1]: https://arxiv.org/abs/2305.17262v2 "Im-Promptu: In-Context Composition from Image Prompts"
[2]: https://arxiv.org/abs/2211.17262v2 "[2211.17262v2] Non-Deterministic Approximation Fixpoint ... - arXiv.org"
[3]: http://export.arxiv.org/pdf/2305.17262 "export.arxiv.org"

According to the paper at [^1^][1], the main contributions are:

- Introducing a suite of three benchmarks to test the generalization properties of a visual in-context learner on tasks such as image completion, image transformation, and image generation.
- Formalizing the notion of an analogy-based in-context learner and using it to design a meta-learning framework called Im-Promptu that can perform in-context composition from image prompts using large language models.
- Exploring the tradeoffs between extrapolation abilities and the degree of compositionality in different visual representations, such as vector representations, patch representations, and object slots.
- Demonstrating a use case of Im-Promptu as an intuitive programming interface for image generation.


## Method Summary

[1]: https://arxiv.org/abs/2305.17262v2 "Im-Promptu: In-Context Composition from Image Prompts"
[2]: https://arxiv.org/abs/2211.17262v2 "[2211.17262v2] Non-Deterministic Approximation Fixpoint ... - arXiv.org"
[3]: http://export.arxiv.org/pdf/2305.17262 "export.arxiv.org"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper formalizes the notion of an **analogy-based in-context learner** that can perform in-context composition from image prompts using large language models. The learner takes as input a set of analogies and a prompt, and outputs a composed image that satisfies the prompt. The learner is trained to minimize the distance between the output image and a target image using a meta-learning objective.
- The paper proposes a meta-learning framework called **Im-Promptu** that implements the analogy-based in-context learner. Im-Promptu consists of four components: a **tokenizer** that converts images into tokens, an **encoder** that embeds the tokens into a latent space, a **cross-attention module** that attends to the analogies and the prompt, and a **decoder** that reconstructs an image from the latent representation.
- The paper introduces a suite of three benchmarks to evaluate Im-Promptu on various tasks such as image completion, image transformation, and image generation. The benchmarks are designed to test the generalization properties of Im-Promptu across different domains, compositions, and transformations. The benchmarks are based on existing datasets such as CLEVR, ShapeWorld, and COCO-Stuff.
- The paper compares Im-Promptu with different levels of compositionality in the tokenizer, such as vector representations, patch representations, and object slots. The paper also compares Im-Promptu with other methods such as CLIP and DALL-E on some tasks.


## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```
# Define the tokenizer, encoder, cross-attention module, and decoder
tokenizer = Tokenizer()
encoder = Encoder()
cross_attention = CrossAttention()
decoder = Decoder()

# Define the meta-learning objective
def meta_objective(analogies, prompt, target):
  # Tokenize the analogies, prompt, and target
  analogy_tokens = tokenizer(analogies)
  prompt_tokens = tokenizer(prompt)
  target_tokens = tokenizer(target)

  # Encode the tokens into latent representations
  analogy_latents = encoder(analogy_tokens)
  prompt_latents = encoder(prompt_tokens)
  target_latents = encoder(target_tokens)

  # Apply cross-attention to the analogy and prompt latents
  composed_latents = cross_attention(analogy_latents, prompt_latents)

  # Decode the composed latents into an output image
  output_image = decoder(composed_latents)

  # Compute the distance between the output image and the target image
  distance = distance_function(output_image, target_image)

  # Return the distance as the meta-objective
  return distance

# Train Im-Promptu using a meta-learning algorithm
for episode in meta_learning_episodes:
  # Sample a set of analogies, a prompt, and a target from a task distribution
  analogies, prompt, target = sample_task()

  # Update Im-Promptu parameters to minimize the meta-objective
  update_parameters(meta_objective(analogies, prompt, target))
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```
# Define the tokenizer, encoder, cross-attention module, and decoder
tokenizer = Tokenizer(
  # Choose the level of compositionality: vector, patch, or object
  mode = "object",

  # Define the parameters for the tokenizer
  num_tokens = 8192,
  token_size = 32,
  stride = 16,
  vocab_size = 256
)

encoder = Encoder(
  # Use a transformer encoder with self-attention
  num_layers = 12,
  num_heads = 16,
  hidden_size = 1024,
  dropout_rate = 0.1
)

cross_attention = CrossAttention(
  # Use a transformer decoder with cross-attention
  num_layers = 6,
  num_heads = 16,
  hidden_size = 1024,
  dropout_rate = 0.1
)

decoder = Decoder(
  # Use a convolutional decoder with upsampling
  num_layers = 6,
  num_filters = [512, 256, 128, 64, 32, 3],
  kernel_size = [3, 3, 3, 3, 3, 1],
  stride = [2, 2, 2, 2, 2, 1],
)

# Define the meta-learning objective
def meta_objective(analogies, prompt, target):
  
   # Tokenize the analogies, prompt, and target
   analogy_tokens = tokenizer(analogies) # shape: [batch_size, num_analogies, num_tokens]
   prompt_tokens = tokenizer(prompt) # shape: [batch_size, num_tokens]
   target_tokens = tokenizer(target) # shape: [batch_size, num_tokens]

   # Encode the tokens into latent representations
   analogy_latents = encoder(analogy_tokens) # shape: [batch_size, num_analogies * num_tokens, hidden_size]
   prompt_latents = encoder(prompt_tokens) # shape: [batch_size, num_tokens, hidden_size]
   target_latents = encoder(target_tokens) # shape: [batch_size, num_tokens, hidden_size]

   # Apply cross-attention to the analogy and prompt latents
   composed_latents = cross_attention(analogy_latents, prompt_latents) # shape: [batch_size, num_tokens * hidden_size]

   # Decode the composed latents into an output image
   output_image = decoder(composed_latents) # shape: [batch_size, image_height, image_width]

   # Compute the distance between the output image and the target image
   distance = mean_squared_error(output_image, target_image) # scalar

   # Return the distance as the meta-objective
   return distance

# Train Im-Promptu using a meta-learning algorithm (e.g. MAML)
for episode in meta_learning_episodes:
  
   # Sample a set of analogies, a prompt, and a target from a task distribution
   analogies_train, prompt_train, target_train = sample_task()
   analogies_test , prompt_test , target_test = sample_task()

   # Update Im-Promptu parameters to minimize the meta-objective on the training task
   params_train = update_parameters(meta_objective(analogies_train , prompt_train , target_train))

   # Evaluate Im-Promptu parameters on the meta-objective on the test task
   params_test = evaluate_parameters(meta_objective(analogies_test , prompt_test , target_test))

   # Update Im-Promptu parameters using meta-gradients from the test task
   update_parameters(params_test)
```