---
title: 2306.02018v2 VideoComposer  Compositional Video Synthesis with Motion Controllability
date: 2023-06-03
---

# [VideoComposer: Compositional Video Synthesis with Motion Controllability](http://arxiv.org/abs/2306.02018v2)

authors: Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, Jingren Zhou


## What, Why and How

[1]: https://arxiv.org/abs/2306.02018 "[2306.02018] VideoComposer: Compositional Video Synthesis with Motion ..."
[2]: http://arxiv-export3.library.cornell.edu/abs/2306.02018v2 "[2306.02018v2] VideoComposer: Compositional Video Synthesis with Motion ..."
[3]: https://arxiv.org/pdf/2305.02018v2.pdf "arXiv:2305.02018v2 [quant-ph] 16 May 2023"

Here is a summary of the paper:

- **What**: The paper proposes a novel framework called **VideoComposer** for compositional video synthesis with motion controllability. It can generate realistic and diverse videos of human actions based on user-specified motion and appearance inputs[^1^][1].
- **Why**: The paper aims to address the challenges of controllable video synthesis, such as the large variation of temporal dynamics, the requirement of cross-frame temporal consistency, and the difficulty of disentangling motion and appearance factors[^1^][1].
- **How**: The paper introduces a two-stage pipeline that consists of a **motion composer** and an **appearance composer**. The motion composer synthesizes a coarse video with desired motion trajectories from a motion code and a reference image. The appearance composer refines the coarse video with realistic details and consistent appearance from an appearance code and the same reference image[^1^][1]. The paper also proposes a novel loss function called **motion consistency loss** to enforce temporal coherence and motion fidelity in the synthesized videos[^1^][1].


## Main Contributions

[1]: https://arxiv.org/abs/2306.02018 "[2306.02018] VideoComposer: Compositional Video Synthesis with Motion ..."
[2]: http://arxiv-export3.library.cornell.edu/abs/2306.02018v2 "[2306.02018v2] VideoComposer: Compositional Video Synthesis with Motion ..."
[3]: https://arxiv.org/pdf/2305.02018v2.pdf "arXiv:2305.02018v2 [quant-ph] 16 May 2023"

According to the paper[^1^][1], the main contributions are:

- It proposes a novel framework for compositional video synthesis with motion controllability, which can generate realistic and diverse videos of human actions based on user-specified motion and appearance inputs.
- It introduces a two-stage pipeline that consists of a motion composer and an appearance composer, which can effectively disentangle and manipulate the motion and appearance factors in video synthesis.
- It proposes a novel loss function called motion consistency loss, which can enforce temporal coherence and motion fidelity in the synthesized videos.
- It demonstrates the effectiveness and superiority of the proposed framework on various datasets and tasks, such as video interpolation, video extrapolation, video inpainting, and video style transfer.


## Method Summary

[1]: https://arxiv.org/abs/2306.02018 "[2306.02018] VideoComposer: Compositional Video Synthesis with Motion ..."
[2]: http://arxiv-export3.library.cornell.edu/abs/2306.02018v2 "[2306.02018v2] VideoComposer: Compositional Video Synthesis with Motion ..."
[3]: https://arxiv.org/pdf/2305.02018v2.pdf "arXiv:2305.02018v2 [quant-ph] 16 May 2023"

Here is a summary of the method section of the paper:

- The paper introduces a two-stage pipeline for compositional video synthesis with motion controllability, which consists of a **motion composer** and an **appearance composer**[^1^][1].
- The motion composer takes a motion code and a reference image as inputs, and synthesizes a coarse video with desired motion trajectories. The motion code is a latent vector that encodes the motion information of a source video, which can be extracted by a pre-trained motion encoder or randomly sampled from a prior distribution. The reference image is a single frame that provides the appearance information of the target subject[^1^][1].
- The appearance composer takes an appearance code and the same reference image as inputs, and refines the coarse video with realistic details and consistent appearance. The appearance code is another latent vector that encodes the appearance information of the reference image, which can be extracted by a pre-trained appearance encoder or randomly sampled from a prior distribution[^1^][1].
- The paper also proposes a novel loss function called **motion consistency loss**, which can enforce temporal coherence and motion fidelity in the synthesized videos. The motion consistency loss consists of two terms: a **motion cycle-consistency loss** and a **motion reconstruction loss**. The motion cycle-consistency loss ensures that the motion code can be recovered from the synthesized video by applying the motion encoder. The motion reconstruction loss ensures that the synthesized video can be reconstructed from the motion code by applying the motion decoder[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the motion encoder, motion decoder, appearance encoder, appearance decoder, and discriminator networks
motion_encoder = MotionEncoder()
motion_decoder = MotionDecoder()
appearance_encoder = AppearanceEncoder()
appearance_decoder = AppearanceDecoder()
discriminator = Discriminator()

# Define the motion consistency loss function
def motion_consistency_loss(motion_code, coarse_video):
  # Apply the motion encoder to the coarse video to get the reconstructed motion code
  reconstructed_motion_code = motion_encoder(coarse_video)
  # Apply the motion decoder to the motion code to get the reconstructed video
  reconstructed_video = motion_decoder(motion_code)
  # Compute the motion cycle-consistency loss as the L1 distance between the motion code and the reconstructed motion code
  motion_cycle_loss = L1(motion_code, reconstructed_motion_code)
  # Compute the motion reconstruction loss as the L1 distance between the coarse video and the reconstructed video
  motion_recon_loss = L1(coarse_video, reconstructed_video)
  # Return the weighted sum of the two losses
  return lambda_cycle * motion_cycle_loss + lambda_recon * motion_recon_loss

# Define the other loss functions such as adversarial loss, perceptual loss, etc.

# Define the training procedure
def train():
  # Loop over the training data
  for source_video, reference_image in data_loader:
    # Extract the motion code from the source video using the motion encoder
    motion_code = motion_encoder(source_video)
    # Extract the appearance code from the reference image using the appearance encoder
    appearance_code = appearance_encoder(reference_image)
    # Synthesize a coarse video with desired motion trajectories using the motion decoder
    coarse_video = motion_decoder(motion_code)
    # Refine the coarse video with realistic details and consistent appearance using the appearance decoder
    refined_video = appearance_decoder(appearance_code, coarse_video)
    # Compute the discriminator outputs for the real and fake videos
    real_output = discriminator(source_video)
    fake_output = discriminator(refined_video)
    # Compute the generator and discriminator losses using various loss functions
    generator_loss = adversarial_loss(fake_output) + perceptual_loss(source_video, refined_video) + motion_consistency_loss(motion_code, coarse_video) + ...
    discriminator_loss = adversarial_loss(real_output, fake_output) + ...
    # Update the generator and discriminator parameters using gradient descent
    update_parameters(generator_loss, discriminator_loss)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import cv2

# Define some hyperparameters
batch_size = 8 # The number of videos/images in a batch
num_frames = 16 # The number of frames in a video
image_size = 256 # The size of the image/video frame
motion_dim = 128 # The dimension of the motion code
appearance_dim = 128 # The dimension of the appearance code
lambda_cycle = 10 # The weight for the motion cycle-consistency loss
lambda_recon = 10 # The weight for the motion reconstruction loss
lambda_adv = 1 # The weight for the adversarial loss
lambda_perceptual = 10 # The weight for the perceptual loss

# Define the motion encoder network
class MotionEncoder(nn.Module):
  def __init__(self):
    super(MotionEncoder, self).__init__()
    # Use a pre-trained ResNet-18 model as the backbone
    self.backbone = models.resnet18(pretrained=True)
    # Remove the last fully connected layer and the average pooling layer
    self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
    # Add a temporal convolution layer to capture the temporal information across frames
    self.temporal_conv = nn.Conv3d(512, 512, kernel_size=(3, 1, 1), padding=(1, 0, 0))
    # Add a global average pooling layer to reduce the spatial dimensions
    self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
    # Add a fully connected layer to output the motion code
    self.fc = nn.Linear(512, motion_dim)

  def forward(self, x):
    # x is a batch of videos of shape (batch_size, num_frames, 3, image_size, image_size)
    # Reshape x to (batch_size * num_frames, 3, image_size, image_size) and pass it through the backbone
    x = x.view(-1, 3, image_size, image_size)
    x = self.backbone(x)
    # Reshape x back to (batch_size, num_frames, 512, h, w) where h and w are the output sizes of the backbone
    x = x.view(batch_size, num_frames, 512, -1, -1)
    # Apply the temporal convolution layer
    x = self.temporal_conv(x)
    # Apply the global average pooling layer
    x = self.global_avg_pool(x)
    # Reshape x to (batch_size, 512)
    x = x.view(batch_size, -1)
    # Apply the fully connected layer to get the motion code of shape (batch_size, motion_dim)
    x = self.fc(x)
    return x

# Define the motion decoder network
class MotionDecoder(nn.Module):
  def __init__(self):
    super(MotionDecoder, self).__init__()
    # Use a pre-trained ResNet-18 model as the backbone
    self.backbone = models.resnet18(pretrained=True)
    # Remove the last fully connected layer and the average pooling layer
    self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])
    # Add a temporal convolution layer to capture the temporal information across frames
    self.temporal_conv = nn.Conv3d(512, 512, kernel_size=(3, 1, 1), padding=(1, 0, 0))
    # Add a global average pooling layer to reduce the spatial dimensions
    self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))
    # Add a fully connected layer to map the motion code to a latent vector of shape (batch_size * num_frames * h * w)
    self.fc = nn.Linear(motion_dim + appearance_dim + num_frames * h * w)