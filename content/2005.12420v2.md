---
title: 2005.12420v2 Network Bending  Expressive Manipulation of Deep Generative Models
date: 2020-05-13
---

# [Network Bending: Expressive Manipulation of Deep Generative Models](http://arxiv.org/abs/2005.12420v2)

authors: Terence Broad, Frederic Fol Leymarie, Mick Grierson


## What, Why and How

[1]: https://arxiv.org/abs/2005.12420 "[2005.12420] Network Bending: Expressive Manipulation of Deep ..."
[2]: https://arxiv.org/pdf/2005.12420 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2106.12420v2 "[2106.12420v2] Radii of Young Star Clusters in Nearby Galaxies"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- What: The paper introduces a new framework for manipulating and interacting with deep generative models that the authors call **network bending**. Network bending consists of inserting deterministic transformations as distinct layers into the computational graph of a trained generative neural network and applying them during inference. The paper also presents a novel algorithm for analysing the deep generative model and clustering features based on their spatial activation maps.
- Why: The paper aims to provide a more expressive and intuitive way of controlling and exploring the latent space of deep generative models, as well as to reveal the internal structure and semantics of the generative process. The paper claims that network bending allows for the direct manipulation of semantically meaningful aspects of the generated images, such as shape, texture, color, and style, as well as enabling a broad range of creative outcomes.
- How: The paper demonstrates network bending on several state-of-the-art deep generative models trained on different image datasets, such as StyleGAN2, BigGAN, and VQ-VAE. The paper shows how different types of transformations, such as scaling, rotation, translation, shearing, cropping, masking, and blending, can be applied to different layers of the generative model to achieve various effects. The paper also shows how the proposed feature clustering algorithm can group together features that correspond to the generation of similar spatial patterns in the images. The paper evaluates network bending qualitatively by showing examples of generated images and quantitatively by measuring the diversity and quality of the images using metrics such as FID and LPIPS.

## Main Contributions

The paper claims to make the following contributions:

- It introduces network bending, a new framework for manipulating and interacting with deep generative models that allows for expressive and intuitive control over the latent space and the generative process.
- It presents a comprehensive set of deterministic transformations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference.
- It proposes a novel algorithm for analysing the deep generative model and clustering features based on their spatial activation maps, which enables the meaningful manipulation of sets of features that correspond to the generation of semantically significant aspects of the images.
- It demonstrates network bending on several state-of-the-art deep generative models trained on different image datasets, showing its versatility and effectiveness in producing diverse and high-quality images.

## Method Summary

[1]: https://arxiv.org/abs/2005.12420 "[2005.12420] Network Bending: Expressive Manipulation of Deep ..."
[2]: https://arxiv.org/pdf/2005.12420 "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2106.12420v2 "[2106.12420v2] Radii of Young Star Clusters in Nearby Galaxies"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the network bending framework, which consists of two main components: **transformations** and **feature clustering**.
- Transformations are deterministic operations that can be inserted as distinct layers into the computational graph of a trained generative neural network and applied during inference. The paper defines a set of transformations that can be applied to different types of layers, such as convolutional, fully connected, and normalization layers. The paper also explains how to implement these transformations using PyTorch and TensorFlow frameworks. The paper categorizes the transformations into three types: **affine**, **non-affine**, and **blending**. Affine transformations include scaling, rotation, translation, and shearing. Non-affine transformations include cropping and masking. Blending transformations include linear and nonlinear blending of different layers or models.
- Feature clustering is a novel algorithm that analyzes the deep generative model and clusters features based on their spatial activation maps. The paper defines a feature as a single channel of a convolutional layer or a single neuron of a fully connected layer. The paper proposes a method to compute the spatial activation map of each feature, which is a 2D representation of how much each feature contributes to the generation of each pixel in the output image. The paper then applies a clustering algorithm (DBSCAN) to group together features that have similar spatial activation maps, resulting in clusters that correspond to the generation of semantically meaningful aspects of the images, such as shape, texture, color, and style. The paper also provides a way to visualize the clusters by generating images that highlight the contribution of each cluster to the output image.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the network bending framework
class NetworkBending:
  # Initialize the framework with a trained generative model
  def __init__(self, model):
    self.model = model # a deep generative neural network
    self.layers = model.layers # a list of layers in the model
    self.transformations = [] # a list of transformations to be applied
    self.clusters = {} # a dictionary of feature clusters
  
  # Add a transformation to a layer of the model
  def add_transformation(self, layer, transformation):
    # Check if the layer and the transformation are compatible
    if self.is_compatible(layer, transformation):
      # Insert the transformation as a distinct layer into the model
      self.insert_layer(layer, transformation)
      # Add the transformation to the list of transformations
      self.transformations.append(transformation)
  
  # Remove a transformation from a layer of the model
  def remove_transformation(self, layer, transformation):
    # Check if the layer and the transformation are compatible
    if self.is_compatible(layer, transformation):
      # Remove the transformation from the list of transformations
      self.transformations.remove(transformation)
      # Remove the transformation as a distinct layer from the model
      self.remove_layer(layer, transformation)
  
  # Cluster features based on their spatial activation maps
  def cluster_features(self):
    # For each layer in the model
    for layer in self.layers:
      # If the layer is convolutional or fully connected
      if self.is_feature_layer(layer):
        # Compute the spatial activation maps of each feature in the layer
        activation_maps = self.compute_activation_maps(layer)
        # Apply a clustering algorithm (DBSCAN) to group features with similar activation maps
        clusters = self.apply_clustering(activation_maps)
        # Store the clusters in the dictionary of feature clusters
        self.clusters[layer] = clusters
  
  # Generate an image from a latent vector using the model with transformations and feature clusters
  def generate_image(self, latent_vector):
    # Pass the latent vector through the model with transformations
    image = self.model(latent_vector)
    # For each layer in the model
    for layer in self.layers:
      # If the layer has feature clusters
      if layer in self.clusters:
        # For each cluster in the layer
        for cluster in self.clusters[layer]:
          # Apply a mask to the image based on the cluster's activation map
          image = self.apply_mask(image, cluster)
    # Return the generated image
    return image

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # PyTorch framework
import tensorflow as tf # TensorFlow framework
import numpy as np # NumPy library for numerical operations
import cv2 # OpenCV library for image processing
import sklearn.cluster # Scikit-learn library for clustering algorithms

# Define the network bending framework
class NetworkBending:
  # Initialize the framework with a trained generative model
  def __init__(self, model):
    self.model = model # a deep generative neural network
    self.layers = model.layers # a list of layers in the model
    self.transformations = [] # a list of transformations to be applied
    self.clusters = {} # a dictionary of feature clusters
  
  # Add a transformation to a layer of the model
  def add_transformation(self, layer, transformation):
    # Check if the layer and the transformation are compatible
    if self.is_compatible(layer, transformation):
      # Insert the transformation as a distinct layer into the model
      self.insert_layer(layer, transformation)
      # Add the transformation to the list of transformations
      self.transformations.append(transformation)
  
  # Remove a transformation from a layer of the model
  def remove_transformation(self, layer, transformation):
    # Check if the layer and the transformation are compatible
    if self.is_compatible(layer, transformation):
      # Remove the transformation from the list of transformations
      self.transformations.remove(transformation)
      # Remove the transformation as a distinct layer from the model
      self.remove_layer(layer, transformation)
  
  # Check if a layer and a transformation are compatible
  def is_compatible(self, layer, transformation):
    # Get the type of the layer and the transformation
    layer_type = type(layer)
    transformation_type = type(transformation)
    # If the layer is convolutional or fully connected
    if layer_type in [torch.nn.Conv2d, torch.nn.Linear, tf.keras.layers.Conv2D, tf.keras.layers.Dense]:
      # If the transformation is affine or non-affine
      if transformation_type in [AffineTransformation, NonAffineTransformation]:
        return True
      else:
        return False
    # If the layer is normalization or activation
    elif layer_type in [torch.nn.BatchNorm2d, torch.nn.ReLU, tf.keras.layers.BatchNormalization, tf.keras.layers.ReLU]:
      # If the transformation is blending
      if transformation_type == BlendingTransformation:
        return True
      else:
        return False
    else:
      return False
  
  # Insert a transformation as a distinct layer into the model
  def insert_layer(self, layer, transformation):
    # Get the index of the layer in the model
    index = self.layers.index(layer)
    # Insert the transformation after the layer in the model
    self.layers.insert(index + 1, transformation)
  
  # Remove a transformation as a distinct layer from the model
  def remove_layer(self, layer, transformation):
    # Get the index of the layer in the model
    index = self.layers.index(layer)
    # Remove the transformation after the layer in the model
    self.layers.pop(index + 1)
  
  # Cluster features based on their spatial activation maps
  def cluster_features(self):
    # For each layer in the model
    for layer in self.layers:
      # If the layer is convolutional or fully connected
      if self.is_feature_layer(layer):
        # Compute the spatial activation maps of each feature in the layer
        activation_maps = self.compute_activation_maps(layer)
        # Apply a clustering algorithm (DBSCAN) to group features with similar activation maps
        clusters = self.apply_clustering(activation_maps)
        # Store the clusters in the dictionary of feature clusters
        self.clusters[layer] = clusters
  
  # Check if a layer is convolutional or fully connected
  def is_feature_layer(self, layer):
    # Get the type of the layer 
    layer_type = type(layer)
    # If the layer is convolutional or fully connected 
    if layer_type in [torch.nn.Conv2d, torch.nn.Linear, tf.keras.layers.Conv2D, tf.keras.layers.Dense]:
      return True 
    else: 
      return False
  
  # Compute the spatial activation maps of each feature in a layer 
  def compute_activation_maps(self, layer): 
    # Get the number of features in the layer 
    num_features = layer.out_channels if type(layer) == torch.nn.Conv2d else layer.out_features 
    num_features = layer.filters if type(layer) == tf.keras.layers.Conv2D else layer.units 
    # Initialize an empty list to store the activation maps 
    activation_maps = [] 
    # For each feature in the layer 
    for i in range(num_features): 
      # Create a one-hot vector of size num_features with 1 at the i-th position and 0 elsewhere 
      one_hot = np.zeros(num_features) 
      one_hot[i] = 1 
      # Convert the one-hot vector to a tensor 
      one_hot = torch.tensor(one_hot) if type(layer) in [torch.nn.Conv2d, torch.nn.Linear] else tf.convert_to_tensor(one_hot) 
      # Pass the one-hot vector through the layer and get the output tensor 
      output = layer(one_hot) 
      # Reshape the output tensor to a 2D array of size (height, width) 
      height = output.shape[2] if type(layer) == torch.nn.Conv2d else 1 
      width = output.shape[3] if type(layer) == torch.nn.Conv2d else output.shape[1] 
      height = output.shape[1] if type(layer) == tf.keras.layers.Conv2D else 1 
      width = output.shape[2] if type(layer) == tf.keras.layers.Conv2D else output.shape[0] 
      output = output.reshape(height, width) 
      # Normalize the output array to have values between 0 and 1 
      output = (output - output.min()) / (output.max() - output.min()) 
      # Append the output array to the list of activation maps 
      activation_maps.append(output) 
    # Return the list of activation maps 
    return activation_maps 
  
  # Apply a clustering algorithm (DBSCAN) to group features with similar activation maps
  def apply_clustering(self, activation_maps):
    # Flatten each activation map to a 1D vector
    vectors = [map.flatten() for map in activation_maps]
    # Convert the list of vectors to a 2D array of size (num_features, num_pixels)
    vectors = np.array(vectors)
    # Apply DBSCAN algorithm to cluster the vectors based on their similarity
    db = sklearn.cluster.DBSCAN(eps=0.5, min_samples=5).fit(vectors)
    # Get the labels of each vector
    labels = db.labels_
    # Initialize an empty dictionary to store the clusters
    clusters = {}
    # For each label in the set of unique labels
    for label in set(labels):
      # If the label is not -1 (which means noise)
      if label != -1:
        # Get the indices of the vectors that belong to this cluster
        indices = np.where(labels == label)[0]
        # Store the indices in the dictionary of clusters
        clusters[label] = indices
    # Return the dictionary of clusters
    return clusters
  
  # Generate an image from a latent vector using the model with transformations and feature clusters
  def generate_image(self, latent_vector):
    # Pass the latent vector through the model with transformations
    image = self.model(latent_vector)
    # For each layer in the model
    for layer in self.layers:
      # If the layer has feature clusters
      if layer in self.clusters:
        # For each cluster in the layer
        for cluster in self.clusters[layer]:
          # Apply a mask to the image based on the cluster's activation map
          image = self.apply_mask(image, cluster)
    # Return the generated image
    return image
  
  # Apply a mask to an image based on a cluster's activation map
  def apply_mask(self, image, cluster):
    # Get the layer and the indices of the features that belong to this cluster
    layer, indices = cluster
    # Initialize an empty array to store the mask
    mask = np.zeros_like(image)
    # For each index in the indices
    for index in indices:
      # Get the activation map of the feature at this index
      activation_map = self.compute_activation_map(layer, index)
      # Resize the activation map to match the size of the image
      activation_map = cv2.resize(activation_map, (image.shape[1], image.shape[0]))
      # Add the activation map to the mask
      mask += activation_map
    # Normalize the mask to have values between 0 and 1
    mask = (mask - mask.min()) / (mask.max() - mask.min())
    # Multiply the image by the mask element-wise
    image = image * mask
    # Return the masked image
    return image

```