---
title: 2306.00986v1 Diffusion Self-Guidance for Controllable Image Generation
date: 2023-06-01
---

# [Diffusion Self-Guidance for Controllable Image Generation](http://arxiv.org/abs/2306.00986v1)

authors: Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, Aleksander Holynski


## What, Why and How

[1]: https://arxiv.org/abs/2306.00986 "[2306.00986] Diffusion Self-Guidance for Controllable Image Generation"
[2]: https://arxiv.org/pdf/2304.00986v1 "arXiv:2304.00986v1 [cs.AI] 30 Mar 2023"
[3]: http://export.arxiv.org/abs/2112.00986v1 "[2112.00986v1] On the existence of vortex-wave systems to inviscid gSQG ..."

Here is a summary of the paper:

- **What**: The paper introduces a method called **self-guidance** that provides greater control over generated images by guiding the internal representations of diffusion models[^1^][1].
- **Why**: The paper aims to solve the problem that many aspects of an image are difficult or impossible to convey through text, and existing methods for image manipulation are either limited or require additional models or training[^1^][1].
- **How**: The paper demonstrates that properties such as the shape, location, and appearance of objects can be extracted from these representations and used to steer sampling. Self-guidance works similarly to classifier guidance, but uses signals present in the pretrained model itself, requiring no additional models or training. The paper shows how a simple set of properties can be composed to perform challenging image manipulations, such as modifying the position or size of objects, merging the appearance of objects in one image with the layout of another, composing objects from many images into one, and more. The paper also shows that self-guidance can be used to edit real images[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- It introduces **self-guidance**, a novel method for controlling image generation by manipulating the internal representations of diffusion models.
- It shows that self-guidance can extract and modify properties such as shape, location, and appearance of objects without requiring any additional models or training.
- It demonstrates that self-guidance can perform various image manipulation tasks that are challenging or impossible for existing methods, such as object resizing, appearance transfer, image composition, and real image editing.
- It provides an interactive demo and a project page with more results and details.

## Method Summary

[1]: https://arxiv.org/abs/2306.00986 "[2306.00986] Diffusion Self-Guidance for Controllable Image Generation"
[2]: https://arxiv.org/pdf/2006.00986v1.pdf "arXiv:2006.00986v1 [cs.LG] 1 Jun 2020"
[3]: http://export.arxiv.org/pdf/2206.00986v1 "arXiv:2206.00986v1 [math.FA] 2 Jun 2022"

Here is a summary of the method section of the paper:

- The paper uses **diffusion models** as the base generative model, which are a class of models that generate images by reversing a Markov chain that gradually adds noise to an image until reaching a uniform distribution[^1^][1].
- The paper proposes **self-guidance**, a method that leverages the internal representations of diffusion models to extract and modify properties of objects in images. Self-guidance consists of two steps: **property extraction** and **property modification**[^1^][1].
- In property extraction, the paper uses **contrastive learning** to learn a latent space that captures properties such as shape, location, and appearance of objects. The paper uses a pretrained diffusion model to encode images into latent vectors, and then trains a contrastive loss function to pull together latent vectors that share the same property and push apart latent vectors that differ in that property[^1^][1].
- In property modification, the paper uses **gradient-based sampling** to steer the sampling process of diffusion models towards desired properties. The paper computes the gradient of a property score function with respect to the latent vector, and then applies the gradient update to the latent vector at each sampling step. The property score function can be either learned from data or defined by hand[^1^][1].
- The paper shows how self-guidance can be applied to various image manipulation tasks by combining different property score functions and latent vector operations. The paper also shows how self-guidance can be extended to edit real images by using an encoder network to map real images into the latent space of diffusion models[^1^][1].

## Pseudo Code

Here is a possible pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import numpy as np

# Define hyperparameters
num_epochs = 100
batch_size = 64
learning_rate = 0.001
temperature = 0.1
num_properties = 3 # shape, location, appearance
num_samples = 10 # number of samples to generate for each task

# Load data
dataset = torchvision.datasets.ImageFolder(root='data', transform=torchvision.transforms.ToTensor())
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Load pretrained diffusion model
diffusion_model = torch.load('diffusion_model.pth')

# Define encoder network to map real images to latent space of diffusion model
encoder = torch.nn.Sequential(
    torch.nn.Conv2d(3, 64, 3, padding=1),
    torch.nn.ReLU(),
    torch.nn.MaxPool2d(2),
    torch.nn.Conv2d(64, 128, 3, padding=1),
    torch.nn.ReLU(),
    torch.nn.MaxPool2d(2),
    torch.nn.Conv2d(128, 256, 3, padding=1),
    torch.nn.ReLU(),
    torch.nn.MaxPool2d(2),
    torch.nn.Flatten(),
    torch.nn.Linear(256*8*8, 1024)
)

# Define property score networks to learn property scores from latent vectors
property_scores = [torch.nn.Linear(1024, 1) for _ in range(num_properties)]

# Define optimizer and loss function
optimizer = torch.optim.Adam([encoder.parameters()] + [p.parameters() for p in property_scores], lr=learning_rate)
loss_function = torch.nn.BCEWithLogitsLoss()

# Train encoder and property score networks using contrastive learning
for epoch in range(num_epochs):
    for images, labels in dataloader:
        # Encode images into latent vectors using diffusion model and encoder network
        latents = encoder(diffusion_model.encode(images))

        # Compute pairwise distances between latent vectors in the batch
        distances = torch.cdist(latents, latents)

        # Compute masks for positive and negative pairs for each property
        positive_masks = []
        negative_masks = []
        for i in range(num_properties):
            # Define a function to check if two images share the same property i
            def same_property(i, image1, image2):
                # For example, if property i is shape, return True if image1 and image2 have the same shape of object
                pass

            # Compute a mask of size (batch_size, batch_size) where mask[i][j] is True if images[i] and images[j] share the same property i
            positive_mask = torch.tensor([[same_property(i, images[k], images[l]) for l in range(batch_size)] for k in range(batch_size)])

            # Compute a mask of size (batch_size, batch_size) where mask[i][j] is True if images[i] and images[j] differ in property i
            negative_mask = ~positive_mask

            # Append the masks to the lists
            positive_masks.append(positive_mask)
            negative_masks.append(negative_mask)

        # Compute loss for each property using contrastive learning
        losses = []
        for i in range(num_properties):
            # Compute property scores for latent vectors using property score network i
            scores = property_scores[i](latents)

            # Compute logits for positive and negative pairs using distances and scores
            positive_logits = -distances[positive_masks[i]] + scores[positive_masks[i]]
            negative_logits = -distances[negative_masks[i]] + scores[negative_masks[i]]

            # Compute labels for positive and negative pairs (1 for positive, 0 for negative)
            positive_labels = torch.ones(positive_logits.size())
            negative_labels = torch.zeros(negative_logits.size())

            # Concatenate logits and labels
            logits = torch.cat([positive_logits, negative_logits])
            labels = torch.cat([positive_labels, negative_labels])

            # Compute loss using binary cross entropy with logits
            loss = loss_function(logits / temperature, labels)

            # Append loss to the list
            losses.append(loss)

        # Compute total loss as the sum of losses for each property
        total_loss = sum(losses)

        # Backpropagate and update parameters
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

        # Print loss
        print(f'Epoch {epoch}, Loss {total_loss.item()}')

# Save encoder and property score networks
torch.save(encoder, 'encoder.pth')
torch.save(property_scores, 'property_scores.pth')

# Define a function to perform property modification using gradient-based sampling
def property_modification(image, property_index, target_score, num_steps):
    # Encode image into latent vector using diffusion model and encoder network
    latent = encoder(diffusion_model.encode(image))

    # Perform gradient-based sampling for num_steps
    for t in range(num_steps):
        # Compute property score for latent vector using property score network
        score = property_scores[property_index](latent)

        # Compute loss as the squared difference between score and target score
        loss = (score - target_score) ** 2

        # Compute gradient of loss with respect to latent vector
        gradient = torch.autograd.grad(loss, latent)[0]

        # Update latent vector by applying gradient descent
        latent = latent - learning_rate * gradient

        # Decode latent vector into image using diffusion model
        image = diffusion_model.decode(latent, t)

    # Return the final image
    return image

# Define a function to perform appearance transfer by swapping latent vectors of two images
def appearance_transfer(image1, image2):
    # Encode images into latent vectors using diffusion model and encoder network
    latent1 = encoder(diffusion_model.encode(image1))
    latent2 = encoder(diffusion_model.encode(image2))

    # Swap the latent vectors
    latent1, latent2 = latent2, latent1

    # Decode the swapped latent vectors into images using diffusion model
    image1 = diffusion_model.decode(latent1, 0)
    image2 = diffusion_model.decode(latent2, 0)

    # Return the swapped images
    return image1, image2

# Define a function to perform image composition by concatenating latent vectors of multiple images
def image_composition(images):
    # Encode images into latent vectors using diffusion model and encoder network
    latents = [encoder(diffusion_model.encode(image)) for image in images]

    # Concatenate the latent vectors along the feature dimension
    latent = torch.cat(latents, dim=1)

    # Decode the concatenated latent vector into an image using diffusion model
    image = diffusion_model.decode(latent, 0)

    # Return the composed image
    return image

# Define a function to perform real image editing by mapping real images to latent space and applying property modification
def real_image_editing(image, property_index, target_score, num_steps):
    # Map real image to latent space using encoder network
    latent = encoder(image)

    # Perform property modification using gradient-based sampling
    image = property_modification(image, property_index, target_score, num_steps)

    # Return the edited image
    return image

# Load test images
test_images = torchvision.datasets.ImageFolder(root='test', transform=torchvision.transforms.ToTensor())

# Perform different image manipulation tasks using self-guidance and save the results
for i in range(num_samples):
    # Sample two images from test set
    image1, label1 = test_images[i]
    image2, label2 = test_images[i + num_samples]

    # Perform property modification for each property and save the results
    for j in range(num_properties):
        # Define a target score for property j (higher score means more of that property)
        target_score = 10.0

        # Perform property modification for image1 and image2 using property j and target score
        modified_image1 = property_modification(image1, j, target_score, num_steps)
        modified_image2 = property_modification(image2, j, target_score, num_steps)

        # Save the modified images
        torchvision.utils.save_image(modified_image1, f'results/modified_image1_{j}.png')
        torchvision.utils.save_image(modified_image2, f'results/modified_image2_{j}.png')

    # Perform appearance transfer for image1 and image2 and save the results
    swapped_image1, swapped_image2 = appearance_transfer(image1, image2)

    # Save the swapped images
    torchvision.utils.save_image(swapped_image1, f'results/swapped_image1.png')
    torchvision.utils.save_image(swapped_image2, f'results/swapped_image2.png')

    # Perform image composition for multiple images and save the result
    composed_image = image_composition(test_images[i:i + 4])

    # Save the composed image
    torchvision.utils.save_image(composed_image, f'results/composed_image.png')

# Load real images
real_images = torchvision.datasets.ImageFolder(root='real', transform=torchvision.transforms.ToTensor())

# Perform real image editing using self-guidance and save the results
for i in range(num_samples):
    # Sample an image from real set
    real_image, label = real_images[i]

    # Perform property modification for each property and save the results
    for j in range(num_properties):
        # Define a target score for property j (higher score means more of that property)
        target_score

```