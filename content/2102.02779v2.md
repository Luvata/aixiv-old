---
title: 2102.02779v2 Unifying Vision-and-Language Tasks via Text Generation
date: 2021-02-03
---

# [Unifying Vision-and-Language Tasks via Text Generation](http://arxiv.org/abs/2102.02779v2)

authors: Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal


## What, Why and How

[1]: https://arxiv.org/abs/2102.02779 "Unifying Vision-and-Language Tasks via Text Generation"
[2]: https://arxiv.org/pdf/2102.02779.pdf "Unifying Vision-and-Language Tasks via Text Generation - arXiv.org"
[3]: http://export.arxiv.org/abs/2102.02779 "[2102.02779] Unifying Vision-and-Language Tasks via Text Generation"

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a unified framework for learning different vision-and-language tasks with the same language modeling objective, i.e., multimodal conditional text generation, where the models learn to generate labels in text based on the visual and textual inputs[^1^][1].
- **Why**: The paper aims to alleviate the hassles of designing task-specific architectures and objectives for each vision-and-language task, which can be inefficient and inconsistent[^1^][1].
- **How**: The paper uses a pre-trained vision-and-language transformer model (VL-T5) as the backbone and fine-tunes it on different downstream tasks by generating text labels conditioned on multimodal inputs. The paper also introduces a span prediction task to improve the pre-training of VL-T5[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2102.02779 "Unifying Vision-and-Language Tasks via Text Generation"
[2]: https://arxiv.org/pdf/2102.02779.pdf "Unifying Vision-and-Language Tasks via Text Generation - arXiv.org"
[3]: https://arxiv.org/pdf/2102.02779v1.pdf "Abstract arXiv:2102.02779v1 [cs.CL] 4 Feb 2021"
[4]: https://www.arxiv-vanity.com/papers/2102.02779/ "Unifying Vision-and-Language Tasks via Text Generation"

According to the paper[^1^][1], the main contributions are:

- **A unified framework** for learning different vision-and-language tasks with the same language modeling objective, i.e., multimodal conditional text generation, where the models learn to generate labels in text based on the visual and textual inputs.
- **A span prediction task** to improve the pre-training of vision-and-language transformer models (VL-T5 and VL-BART) by generating text spans that describe visual regions in images.
- **Empirical results** on 7 popular vision-and-language benchmarks, showing that the generative approach reaches comparable performance to recent task-specific state-of-the-art models, and shows better generalization ability on questions that have rare answers.
- **A multi-task learning framework** that allows learning different vision-and-language tasks in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models.

## Method Summary

[1]: https://arxiv.org/abs/2102.02779 "Unifying Vision-and-Language Tasks via Text Generation"
[2]: https://arxiv.org/pdf/2102.02779.pdf "Unifying Vision-and-Language Tasks via Text Generation - arXiv.org"
[3]: https://github.com/ilknuraktemur/Unifying-Vision-and-Language-Tasks-via-Text-Generation "Unifying-Vision-and-Language-Tasks-via-Text-Generation"
[4]: https://www.researchgate.net/publication/366063398_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing "(PDF) Unifying Vision, Text, and Layout for Universal ... - ResearchGate"

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses two pre-trained language models, **T5** and **BART**, and extends them with visual understanding ability by adding a **visual encoder** that takes image features as input and outputs a sequence of visual tokens.
- The paper introduces a new pre-training task, **span prediction**, where the models learn to generate text spans that describe visual regions in images. The paper uses a large-scale image-text corpus, **Conceptual Captions**, for pre-training the models on this task.
- The paper fine-tunes the pre-trained models on different downstream vision-and-language tasks by generating text labels conditioned on multimodal inputs. The paper uses a unified format for different tasks, where the text input consists of a **task prefix** and a **question**, and the text output consists of an **answer**. For example, for visual question answering, the text input is `vqa: what is the man jumping over?` and the text output is `fire hydrant`.
- The paper also shows how to perform multi-task learning with the same architecture and parameters by simply changing the task prefix in the text input. The paper uses a weighted sum of task-specific losses for multi-task learning.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the visual encoder
visual_encoder = TransformerEncoder(image_features_dim, num_layers, num_heads, hidden_dim)

# Define the language model (T5 or BART)
language_model = TransformerModel(vocab_size, num_layers, num_heads, hidden_dim)

# Define the span prediction loss function
span_prediction_loss = CrossEntropyLoss()

# Define the text generation loss function
text_generation_loss = CrossEntropyLoss()

# Pre-train the model on span prediction task
for image, text in conceptual_captions:
  # Extract image features using a pre-trained CNN
  image_features = CNN(image)
  # Encode image features into visual tokens
  visual_tokens = visual_encoder(image_features)
  # Concatenate visual tokens and text tokens as input
  input_tokens = [visual_tokens; text_tokens]
  # Mask out some spans in the text tokens as output
  output_tokens = mask_spans(text_tokens)
  # Forward pass through the language model
  logits = language_model(input_tokens)
  # Compute the span prediction loss
  loss = span_prediction_loss(logits, output_tokens)
  # Backpropagate and update the parameters
  loss.backward()
  optimizer.step()

# Fine-tune the model on downstream tasks
for task in tasks:
  for image, question, answer in task_data:
    # Extract image features using a pre-trained CNN
    image_features = CNN(image)
    # Encode image features into visual tokens
    visual_tokens = visual_encoder(image_features)
    # Concatenate visual tokens and question tokens as input
    input_tokens = [visual_tokens; task_prefix; question_tokens]
    # Use answer tokens as output
    output_tokens = answer_tokens
    # Forward pass through the language model
    logits = language_model(input_tokens)
    # Compute the text generation loss
    loss = text_generation_loss(logits, output_tokens)
    # Backpropagate and update the parameters
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision
import transformers

# Define the hyperparameters
image_features_dim = 2048 # dimension of image features from CNN
num_layers = 12 # number of transformer layers
num_heads = 12 # number of attention heads
hidden_dim = 768 # dimension of hidden states
vocab_size = 32000 # size of vocabulary
max_length = 512 # maximum length of input and output sequences
batch_size = 32 # batch size for training
num_epochs = 10 # number of epochs for training
learning_rate = 1e-4 # learning rate for optimizer

# Define the visual encoder
class VisualEncoder(nn.Module):
  def __init__(self, image_features_dim, num_layers, num_heads, hidden_dim):
    super(VisualEncoder, self).__init__()
    # Define the linear projection layer
    self.linear = nn.Linear(image_features_dim, hidden_dim)
    # Define the transformer encoder layer
    self.transformer = nn.TransformerEncoderLayer(hidden_dim, num_heads)
    # Define the transformer encoder block
    self.encoder = nn.TransformerEncoder(self.transformer, num_layers)

  def forward(self, image_features):
    # Project image features into hidden dimension
    x = self.linear(image_features)
    # Encode image features into visual tokens
    x = self.encoder(x)
    return x

# Define the language model (T5 or BART)
language_model = transformers.T5ForConditionalGeneration.from_pretrained('t5-base')
# or language_model = transformers.BartForConditionalGeneration.from_pretrained('facebook/bart-base')

# Define the span prediction loss function
span_prediction_loss = nn.CrossEntropyLoss(ignore_index=-100)

# Define the text generation loss function
text_generation_loss = nn.CrossEntropyLoss(ignore_index=-100)

# Define the optimizer
optimizer = torch.optim.Adam(language_model.parameters(), lr=learning_rate)

# Load the conceptual captions dataset
conceptual_captions = load_conceptual_captions()

# Pre-train the model on span prediction task
for epoch in range(num_epochs):
  for batch in conceptual_captions.batch(batch_size):
    # Extract image and text from batch
    image = batch['image']
    text = batch['text']
    # Extract image features using a pre-trained CNN (e.g., ResNet-50)
    image_features = torchvision.models.resnet50(pretrained=True)(image)
    # Encode image features into visual tokens
    visual_tokens = visual_encoder(image_features)
    # Tokenize text using a pre-trained tokenizer (e.g., T5Tokenizer)
    tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base')
    text_tokens = tokenizer(text, return_tensors='pt', padding=True)
    # Concatenate visual tokens and text tokens as input_ids and attention_mask
    input_ids = torch.cat([visual_tokens, text_tokens['input_ids']], dim=1)
    attention_mask = torch.cat([torch.ones_like(visual_tokens), text_tokens['attention_mask']], dim=1)
    # Mask out some spans in the text tokens as labels (use -100 for padding and masked tokens)
    labels = mask_spans(text_tokens['input_ids'])
    # Forward pass through the language model
    outputs = language_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    # Extract the loss from outputs
    loss = outputs.loss
    # Compute the span prediction loss (optional)
    # logits = outputs.logits[:, len(visual_tokens):]
    # loss = span_prediction_loss(logits.reshape(-1, vocab_size), labels.reshape(-1))
    # Backpropagate and update the parameters
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# Load the downstream tasks datasets (e.g., VQA, GQA, RefCOCOg, etc.)
tasks = load_tasks()

# Fine-tune the model on downstream tasks
for task in tasks:
  for epoch in range(num_epochs):
    for batch in task.batch(batch_size):
      # Extract image, question, and answer from batch
      image = batch['image']
      question = batch['question']
      answer = batch['answer']
      # Extract image features using a pre-trained CNN (e.g., ResNet-50)
      image_features = torchvision.models.resnet50(pretrained=True)(image)
      # Encode image features into visual tokens
      visual_tokens = visual_encoder(image_features)
      # Tokenize question and answer using a pre-trained tokenizer (e.g., T5Tokenizer)
      tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base')
      question_tokens = tokenizer(task_prefix + question, return_tensors='pt', padding=True)
      answer_tokens = tokenizer(answer, return_tensors='pt', padding=True)
      # Concatenate visual tokens and question tokens as input_ids and attention_mask
      input_ids = torch.cat([visual_tokens, question_tokens['input_ids']], dim=1)
      attention_mask = torch.cat([torch.ones_like(visual_tokens), question_tokens['attention_mask']], dim=1)
      # Use answer tokens as labels (use -100 for padding tokens)
      labels = answer_tokens['input_ids']
      # Forward pass through the language model
      outputs = language_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
      # Extract the loss from outputs
      loss = outputs.loss
      # Compute the text generation loss (optional)
      # logits = outputs.logits
      # loss = text_generation_loss(logits.reshape(-1, vocab_size), labels.reshape(-1))
      # Backpropagate and update the parameters
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
```