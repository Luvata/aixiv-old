---
title: 2211.17256v2 CLIPascene  Scene Sketching with Different Types and Levels of Abstraction
date: 2022-11-18
---

# [CLIPascene: Scene Sketching with Different Types and Levels of Abstraction](http://arxiv.org/abs/2211.17256v2)

authors: Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, Ariel Shamir


## What, Why and How

[1]: https://arxiv.org/abs/2211.17256v2 "[2211.17256v2] CLIPascene: Scene Sketching with Different Types and ..."
[2]: https://arxiv.org/abs/2211.17256 "[2211.17256] CLIPascene: Scene Sketching with Different Types and ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2211.17256 "[2211.17256] CLIPascene: Scene Sketching with Different Types and ..."

Here is a summary of the paper:

- **What**: The paper presents a method for converting a given scene image into a sketch using different types and multiple levels of abstraction.
- **Why**: The paper aims to provide users additional control over selecting the desired sketch based on their personal goals and preferences, and to generate sketches of complex scenes that preserve their recognizability and semantics.
- **How**: The paper trains two MLP networks. The first network learns the desired placement of strokes, while the second network learns to gradually remove strokes from the sketch without harming its recognizability and semantics. The paper uses an explicit disentanglement into two abstraction axes: fidelity and simplicity. Fidelity varies the representation from a more precise portrayal of the input to a looser depiction. Simplicity moves from a detailed depiction to a sparse sketch.

This summary is based on the abstract of the paper[^1^][1] [^2^][2] [^3^][3].


## Main Contributions

[1]: https://arxiv.org/abs/2211.17256v2 "[2211.17256v2] CLIPascene: Scene Sketching with Different Types and ..."
[2]: https://arxiv.org/abs/2211.17256 "[2211.17256] CLIPascene: Scene Sketching with Different Types and ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2211.17256 "[2211.17256] CLIPascene: Scene Sketching with Different Types and ..."

According to the paper[^1^][1], the main contributions are:

- **A novel method for scene sketching with different types and levels of abstraction**. The method can convert any scene image into a sketch that varies in fidelity and simplicity, providing users more control over the sketching process and outcome.
- **A two-stage network architecture for sketch generation**. The first network learns the desired placement of strokes, while the second network learns to gradually remove strokes from the sketch without harming its recognizability and semantics.
- **A large-scale dataset of scene sketches with multiple levels of abstraction**. The dataset contains over 100K sketches of scenes with complex backgrounds and subjects, annotated with fidelity and simplicity levels.
- **An extensive evaluation of the proposed method**. The evaluation includes quantitative and qualitative comparisons with existing methods, ablation studies, user studies, and applications.

## Method Summary

[1]: https://arxiv.org/abs/2211.17256v2 "[2211.17256v2] CLIPascene: Scene Sketching with Different Types and ..."
[2]: https://arxiv.org/abs/2211.17256 "[2211.17256] CLIPascene: Scene Sketching with Different Types and ..."
[3]: https://arxiv-export2.library.cornell.edu/abs/2211.17256 "[2211.17256] CLIPascene: Scene Sketching with Different Types and ..."

Here is a summary of the method section of the paper:

- **Overview**: The paper proposes a two-stage network architecture for scene sketching with different types and levels of abstraction. The first stage is a **stroke placement network** that learns to generate a sketch with high fidelity and detail. The second stage is a **stroke removal network** that learns to remove strokes from the sketch while maintaining its recognizability and semantics. The paper also introduces a **disentanglement scheme** that allows users to control the fidelity and simplicity of the sketch by adjusting two parameters.
- **Stroke placement network**: The paper uses a pre-trained CLIP model  to encode the input scene image and the target sketch into a common latent space. The paper then trains an MLP network to map the image embedding to a sketch embedding that preserves the scene semantics. The paper also trains another MLP network to decode the sketch embedding into a set of strokes using a differentiable renderer . The paper optimizes the networks using a combination of reconstruction loss, perceptual loss, and CLIP loss.
- **Stroke removal network**: The paper trains another MLP network to remove strokes from the sketch generated by the stroke placement network. The paper uses an attention mechanism to select which strokes to remove based on their importance and relevance. The paper also uses a CLIP loss to ensure that the simplified sketch remains recognizable and semantically consistent with the input image.
- **Disentanglement scheme**: The paper defines two abstraction axes: fidelity and simplicity. Fidelity measures how closely the sketch resembles the input image, while simplicity measures how sparse the sketch is. The paper uses two parameters, alpha and beta, to control the level of abstraction along each axis. Alpha controls the fidelity by interpolating between the image embedding and the sketch embedding. Beta controls the simplicity by determining how many strokes to remove from the sketch. The paper shows that varying alpha and beta can produce sketches with different types and levels of abstraction.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: scene image I, fidelity parameter alpha, simplicity parameter beta
# Output: sketch S

# Encode the image using CLIP
image_embedding = CLIP.encode(I)

# Generate a sketch with high fidelity and detail
sketch_embedding = MLP1(image_embedding) # map image embedding to sketch embedding
strokes = MLP2(sketch_embedding) # decode sketch embedding to strokes
sketch = Renderer(strokes) # render strokes to sketch

# Simplify the sketch by removing strokes
attention_scores = MLP3(sketch_embedding, strokes) # compute attention scores for each stroke
selected_strokes = RemoveStrokes(strokes, attention_scores, beta) # remove strokes based on scores and beta
simplified_sketch = Renderer(selected_strokes) # render selected strokes to simplified sketch

# Adjust the fidelity of the sketch by interpolating embeddings
interpolated_embedding = alpha * image_embedding + (1 - alpha) * sketch_embedding # interpolate between image and sketch embeddings
adjusted_strokes = MLP2(interpolated_embedding) # decode interpolated embedding to strokes
adjusted_sketch = Renderer(adjusted_strokes) # render adjusted strokes to adjusted sketch

# Return the final sketch
S = adjusted_sketch + simplified_sketch # combine the adjusted and simplified sketches
return S
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np
import cv2

# Define the hyperparameters
IMAGE_SIZE = 224 # size of the input image
SKETCH_SIZE = 256 # size of the output sketch
EMBEDDING_DIM = 512 # dimension of the CLIP embeddings
STROKE_DIM = 5 # dimension of each stroke (x, y, r, g, b)
MAX_STROKES = 1000 # maximum number of strokes per sketch
RENDERER_SIGMA = 0.5 # standard deviation of the Gaussian kernel for rendering
MLP1_DIMS = [EMBEDDING_DIM, 1024, EMBEDDING_DIM] # dimensions of MLP1 layers
MLP2_DIMS = [EMBEDDING_DIM, 1024, MAX_STROKES * STROKE_DIM] # dimensions of MLP2 layers
MLP3_DIMS = [EMBEDDING_DIM + STROKE_DIM, 1024, 1] # dimensions of MLP3 layers
LAMBDA1 = 10 # weight of the reconstruction loss
LAMBDA2 = 10 # weight of the perceptual loss
LAMBDA3 = 1 # weight of the CLIP loss

# Load the pre-trained CLIP model and tokenizer
clip_model, clip_tokenizer = clip.load("ViT-B/32")

# Define the MLP networks
def MLP(dims):
    # Create a multi-layer perceptron with ReLU activations and skip connections
    layers = []
    for i in range(len(dims) - 1):
        layers.append(torch.nn.Linear(dims[i], dims[i + 1]))
        if i < len(dims) - 2:
            layers.append(torch.nn.ReLU())
            if dims[i] == dims[i + 1]:
                layers.append(torch.nn.Identity()) # skip connection
    return torch.nn.Sequential(*layers)

MLP1 = MLP(MLP1_DIMS) # map image embedding to sketch embedding
MLP2 = MLP(MLP2_DIMS) # decode sketch embedding to strokes
MLP3 = MLP(MLP3_DIMS) # compute attention scores for each stroke

# Define the renderer
def Renderer(strokes):
    # Render a set of strokes to a sketch using a differentiable renderer
    # strokes: a tensor of shape (batch_size, MAX_STROKES, STROKE_DIM)
    # return: a tensor of shape (batch_size, SKETCH_SIZE, SKETCH_SIZE, 3)

    # Normalize the stroke coordinates to [0, 1]
    strokes[:, :, :2] = torch.sigmoid(strokes[:, :, :2])

    # Create a mesh grid for rendering
    grid_x, grid_y = torch.meshgrid(torch.linspace(0, 1, SKETCH_SIZE), torch.linspace(0, 1, SKETCH_SIZE))
    grid_x = grid_x.unsqueeze(0).unsqueeze(-1).repeat(strokes.shape[0], 1, 1, MAX_STROKES)
    grid_y = grid_y.unsqueeze(0).unsqueeze(-1).repeat(strokes.shape[0], 1, 1, MAX_STROKES)

    # Compute the distance from each pixel to each stroke center
    dist_x = grid_x - strokes[:, None, None, :, 0]
    dist_y = grid_y - strokes[:, None, None, :, 1]
    dist = torch.sqrt(dist_x ** 2 + dist_y ** 2)

    # Compute the Gaussian kernel for rendering
    kernel = torch.exp(-dist ** 2 / (2 * RENDERER_SIGMA ** 2))

    # Compute the color intensity for each pixel and each stroke
    intensity = strokes[:, None, None, :, 2:]

    # Render the sketch by multiplying the kernel and the intensity and summing over the strokes
    sketch = torch.sum(kernel * intensity, dim=-2)

    # Normalize the sketch to [0, 1]
    sketch = torch.clamp(sketch / (MAX_STROKES * RENDERER_SIGMA), min=0.0 , max=1.0)

    return sketch

# Define the loss functions
def ReconstructionLoss(sketch1, sketch2):
    # Compute the reconstruction loss between two sketches using L1 norm
    # sketch1: a tensor of shape (batch_size, SKETCH_SIZE, SKETCH_SIZE, 3)
    # sketch2: a tensor of shape (batch_size, SKETCH_SIZE, SKETCH_SIZE, 3)
    # return: a scalar tensor

    return torch.mean(torch.abs(sketch1 - sketch2))

def PerceptualLoss(sketch1, sketch2):
    # Compute the perceptual loss between two sketches using CLIP embeddings
    # sketch1: a tensor of shape (batch_size, SKETCH_SIZE, SKETCH_SIZE, 3)
    # sketch2: a tensor of shape (batch_size, SKETCH_SIZE, SKETCH_SIZE, 3)
    # return: a scalar tensor

    # Resize the sketches to the CLIP input size
    sketch1 = torchvision.transforms.Resize(IMAGE_SIZE)(sketch1)
    sketch2 = torchvision.transforms.Resize(IMAGE_SIZE)(sketch2)

    # Encode the sketches using CLIP
    sketch1_embedding = clip_model.encode_image(sketch1)
    sketch2_embedding = clip_model.encode_image(sketch2)

    # Compute the cosine similarity between the embeddings
    similarity = torch.nn.CosineSimilarity(dim=-1)(sketch1_embedding, sketch2_embedding)

    # Compute the perceptual loss as the negative mean similarity
    return -torch.mean(similarity)

def CLIPLoss(image, sketch, text):
    # Compute the CLIP loss between an image, a sketch, and a text
    # image: a tensor of shape (batch_size, IMAGE_SIZE, IMAGE_SIZE, 3)
    # sketch: a tensor of shape (batch_size, SKETCH_SIZE, SKETCH_SIZE, 3)
    # text: a tensor of shape (batch_size,)
    # return: a scalar tensor

    # Resize the sketch to the CLIP input size
    sketch = torchvision.transforms.Resize(IMAGE_SIZE)(sketch)

    # Encode the image, the sketch, and the text using CLIP
    image_embedding = clip_model.encode_image(image)
    sketch_embedding = clip_model.encode_image(sketch)
    text_embedding = clip_model.encode_text(clip_tokenizer(text))

    # Compute the cosine similarity between the embeddings
    image_sketch_similarity = torch.nn.CosineSimilarity(dim=-1)(image_embedding, sketch_embedding)
    image_text_similarity = torch.nn.CosineSimilarity(dim=-1)(image_embedding, text_embedding)
    sketch_text_similarity = torch.nn.CosineSimilarity(dim=-1)(sketch_embedding, text_embedding)

    # Compute the CLIP loss as the mean squared error between the similarities
    return torch.mean((image_sketch_similarity - image_text_similarity) ** 2) + torch.mean((image_sketch_similarity - sketch_text_similarity) ** 2)

# Define the training loop
def Train(image, text, alpha, beta):
    # Train the networks for one iteration given an image, a text, and abstraction parameters
    # image: a tensor of shape (batch_size, IMAGE_SIZE, IMAGE_SIZE, 3)
    # text: a tensor of shape (batch_size,)
    # alpha: a scalar tensor in [0, 1]
    # beta: a scalar tensor in [0, 1]
    
    # Encode the image using CLIP
    image_embedding = CLIP.encode(image)

    # Generate a sketch with high fidelity and detail
    sketch_embedding = MLP1(image_embedding) # map image embedding to sketch embedding
    strokes = MLP2(sketch_embedding) # decode sketch embedding to strokes
    sketch = Renderer(strokes) # render strokes to sketch

    # Simplify the sketch by removing strokes
    attention_scores = MLP3(torch.cat([sketch_embedding.unsqueeze(1).repeat(1, MAX_STROKES, 1), strokes], dim=-1)) # compute attention scores for each stroke
    selected_strokes = RemoveStrokes(strokes, attention_scores, beta) # remove strokes based on scores and beta
    simplified_sketch = Renderer(selected_strokes) # render selected strokes to simplified sketch

    # Adjust the fidelity of the sketch by interpolating embeddings
    interpolated_embedding = alpha * image_embedding + (1 - alpha) * sketch_embedding # interpolate between image and sketch embeddings
    adjusted_strokes = MLP2(interpolated_embedding) # decode interpolated embedding to strokes
    adjusted_sketch = Renderer(adjusted_strokes) # render adjusted strokes to adjusted sketch

    # Combine the adjusted and simplified sketches
    final_sketch = adjusted_sketch + simplified_sketch

    # Compute the losses
    reconstruction_loss = ReconstructionLoss(sketch, final_sketch)
    perceptual_loss = PerceptualLoss(sketch, final_sketch)
    clip_loss = CLIPLoss(image, final_sketch, text)

    total_loss = LAMBDA1 * reconstruction_loss + LAMBDA2 * perceptual_loss + LAMBDA3 * clip_loss

    # Update the networks using gradient descent
     total_loss.backward()
     optimizer.step()
     optimizer.zero_grad()

# Define the inference loop
def Inference(image, alpha, beta):
     # Generate a sketch given an image and abstraction parameters