---
title: 2112.05142v2 HairCLIP  Design Your Hair by Text and Reference Image
date: 2021-12-06
---

# [HairCLIP: Design Your Hair by Text and Reference Image](http://arxiv.org/abs/2112.05142v2)

authors: Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, Nenghai Yu


## What, Why and How

[1]: https://arxiv.org/abs/2112.05142 "Title: HairCLIP: Design Your Hair by Text and Reference Image - arXiv.org"
[2]: https://arxiv.org/pdf/2110.05142v2.pdf "arXiv:2110.05142v2 [math.LO] 5 Feb 2022"
[3]: https://arxiv.org/pdf/2112.05142v2.pdf "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/1705.05142v2 "[1705.05142v2] Adapting a General Purpose Social Robot for Paediatric ..."

Here is a summary of the paper based on the first search result[^1^][1]:

- **What**: The paper proposes a new hair editing interaction mode, which enables manipulating hair attributes individually or jointly based on the texts or reference images provided by users.
- **Why**: The paper aims to free users from the tedious interaction process of existing methods that require well-drawn sketches or masks as conditional inputs for editing.
- **How**: The paper encodes the image and text conditions in a shared embedding space and proposes a unified hair editing framework by leveraging the powerful image text representation capability of the Contrastive Language-Image Pre-Training (CLIP) model. The paper also designs network structures and loss functions to perform high-quality hair editing in a disentangled manner.

## Main Contributions

[1]: https://arxiv.org/abs/2112.05142 "Title: HairCLIP: Design Your Hair by Text and Reference Image - arXiv.org"
[2]: https://arxiv.org/pdf/2110.05142v2.pdf "arXiv:2110.05142v2 [math.LO] 5 Feb 2022"
[3]: https://arxiv.org/pdf/2112.05142v2.pdf "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/1705.05142v2 "[1705.05142v2] Adapting a General Purpose Social Robot for Paediatric ..."

According to the first search result[^1^][1], the main contributions of this paper are:

- **Proposing a new hair editing interaction mode** that allows users to manipulate hair attributes by text and reference image, which is more intuitive and efficient than existing methods that require sketches or masks.
- **Encoding the image and text conditions in a shared embedding space** and leveraging the CLIP model to achieve a unified hair editing framework that can handle various hair editing scenarios.
- **Designing network structures and loss functions** to perform high-quality hair editing in a disentangled manner, which preserves irrelevant attributes and enhances visual realism of editing results.
- **Conducting extensive experiments** to demonstrate the superiority of the proposed approach over existing methods in terms of manipulation accuracy, visual realism, and irrelevant attribute preservation.

## Method Summary

[1]: https://arxiv.org/abs/2112.05142 "Title: HairCLIP: Design Your Hair by Text and Reference Image - arXiv.org"
[2]: https://arxiv.org/pdf/2110.05142v2.pdf "arXiv:2110.05142v2 [math.LO] 5 Feb 2022"
[3]: https://arxiv.org/pdf/2112.05142v2.pdf "arXiv.org e-Print archive"
[4]: http://export.arxiv.org/abs/1705.05142v2 "[1705.05142v2] Adapting a General Purpose Social Robot for Paediatric ..."

Here is a summary of the method section of the paper based on the first search result[^1^][1]:

- The paper first introduces the **problem formulation** of hair editing by text and reference image, and defines the hair attributes to be manipulated, such as color, shape, length, and style.
- The paper then presents the **network architecture** of the proposed framework, which consists of three modules: a CLIP encoder, a hair attribute encoder, and a hair editing generator. The CLIP encoder maps the image and text conditions to a shared embedding space, where the similarity between them can be measured. The hair attribute encoder extracts hair attributes from the input image and the reference image. The hair editing generator takes the input image, the text condition, and the extracted hair attributes as inputs, and outputs a realistic edited image that satisfies the text condition and preserves irrelevant attributes.
- The paper also describes the **loss functions** used to train the framework, which include a CLIP loss, an attribute loss, an identity loss, an adversarial loss, and a reconstruction loss. The CLIP loss enforces the similarity between the output image and the text condition in the embedding space. The attribute loss ensures that the output image has the desired hair attributes. The identity loss preserves the identity of the input image. The adversarial loss enhances the visual realism of the output image. The reconstruction loss encourages the output image to be consistent with the input image when there is no text condition.


## Pseudo Code - High level

Here is a possible high-level pseudo code for this paper:

```python
# Define the CLIP encoder, the hair attribute encoder, and the hair editing generator
clip_encoder = CLIP_Encoder()
hair_attribute_encoder = Hair_Attribute_Encoder()
hair_editing_generator = Hair_Editing_Generator()

# Define the loss functions
clip_loss = CLIP_Loss()
attribute_loss = Attribute_Loss()
identity_loss = Identity_Loss()
adversarial_loss = Adversarial_Loss()
reconstruction_loss = Reconstruction_Loss()

# Define the optimizer
optimizer = Optimizer()

# Loop over the training data
for input_image, text_condition, reference_image in training_data:

  # Encode the image and text conditions using the CLIP encoder
  input_embedding = clip_encoder(input_image)
  text_embedding = clip_encoder(text_condition)

  # Extract the hair attributes from the input image and the reference image using the hair attribute encoder
  input_attribute = hair_attribute_encoder(input_image)
  reference_attribute = hair_attribute_encoder(reference_image)

  # Generate the output image using the hair editing generator
  output_image = hair_editing_generator(input_image, text_condition, input_attribute, reference_attribute)

  # Compute the losses
  clip_loss_value = clip_loss(output_image, text_condition, input_embedding, text_embedding)
  attribute_loss_value = attribute_loss(output_image, reference_attribute)
  identity_loss_value = identity_loss(output_image, input_image)
  adversarial_loss_value = adversarial_loss(output_image)
  reconstruction_loss_value = reconstruction_loss(output_image, input_image)

  # Compute the total loss
  total_loss = clip_loss_value + attribute_loss_value + identity_loss_value + adversarial_loss_value + reconstruction_loss_value

  # Update the parameters using the optimizer
  optimizer.update(total_loss)
```

## Pseudo Code - Detail

Here is a possible detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define the hyperparameters
batch_size = 16
learning_rate = 0.0002
num_epochs = 100
lambda_clip = 1.0
lambda_attribute = 1.0
lambda_identity = 10.0
lambda_adv = 1.0
lambda_rec = 10.0

# Define the CLIP encoder
clip_encoder = clip.load("ViT-B/32", jit=False)[0].eval()

# Define the hair attribute encoder
class Hair_Attribute_Encoder(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pre-trained ResNet-18 model as the backbone
    self.backbone = torchvision.models.resnet18(pretrained=True)
    # Remove the last fully connected layer and the average pooling layer
    self.backbone = torch.nn.Sequential(*list(self.backbone.children())[:-2])
    # Add a global average pooling layer and a fully connected layer to output a 128-dimensional vector for each attribute
    self.gap = torch.nn.AdaptiveAvgPool2d((1, 1))
    self.fc_color = torch.nn.Linear(512, 128)
    self.fc_shape = torch.nn.Linear(512, 128)
    self.fc_length = torch.nn.Linear(512, 128)
    self.fc_style = torch.nn.Linear(512, 128)

  def forward(self, x):
    # x is a batch of images with shape (batch_size, 3, 256, 256)
    # Extract the features using the backbone
    features = self.backbone(x) # shape: (batch_size, 512, 8, 8)
    # Apply the global average pooling layer
    features = self.gap(features).squeeze() # shape: (batch_size, 512)
    # Apply the fully connected layers to get the attribute vectors
    color = self.fc_color(features) # shape: (batch_size, 128)
    shape = self.fc_shape(features) # shape: (batch_size, 128)
    length = self.fc_length(features) # shape: (batch_size, 128)
    style = self.fc_style(features) # shape: (batch_size, 128)
    # Return the attribute vectors as a dictionary
    return {"color": color, "shape": shape, "length": length, "style": style}

# Define the hair editing generator
class Hair_Editing_Generator(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pre-trained StyleGAN2 model as the backbone
    self.backbone = StyleGAN2(pretrained=True)
    # Add four mapping networks to map the attribute vectors to latent codes
    self.map_color = MappingNetwork(128, 512)
    self.map_shape = MappingNetwork(128, 512)
    self.map_length = MappingNetwork(128, 512)
    self.map_style = MappingNetwork(128, 512)

  def forward(self, x, t, a_in, a_ref):
    # x is a batch of input images with shape (batch_size, 3, 256, 256)
    # t is a batch of text conditions with shape (batch_size,)
    # a_in is a dictionary of input attribute vectors with shape (batch_size, 128) for each key
    # a_ref is a dictionary of reference attribute vectors with shape (batch_size, 128) for each key
    # Encode the input images to latent codes using the backbone encoder
    w_in = self.backbone.encoder(x) # shape: (batch_size, num_layers, latent_dim)
    # Encode the text conditions to latent codes using the CLIP encoder and a linear projection layer
    t_emb = clip_encoder.encode_text(t) # shape: (batch_size, clip_dim)
    t_emb = self.backbone.projector(t_emb) # shape: (batch_size, latent_dim)
    t_emb = t_emb.unsqueeze(1).repeat(1, num_layers, 1) # shape: (batch_size, num_layers, latent_dim)
    # Map the attribute vectors to latent codes using the mapping networks
    w_color = self.map_color(a_ref["color"]) # shape: (batch_size, num_layers, latent_dim)
    w_shape = self.map_shape(a_ref["shape"]) # shape: (batch_size, num_layers, latent_dim)
    w_length = self.map_length(a_ref["length"]) # shape: (batch_size, num_layers, latent_dim)
    w_style = self.map_style(a_ref["style"]) # shape: (batch_size, num_layers, latent_dim)
    # Combine the latent codes using a weighted sum
    w_out = w_in * 0.2 + t_emb * 0.2 + w_color * 0.2 + w_shape * 0.2 + w_length * 0.1 + w_style * 0.1 # shape: (batch_size, num_layers, latent_dim)
    # Generate the output images using the backbone generator
    y = self.backbone.generator(w_out) # shape: (batch_size, 3, 256, 256)
    # Return the output images
    return y

# Define the CLIP loss
class CLIP_Loss(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use the cosine similarity as the similarity measure
    self.cosine_similarity = torch.nn.CosineSimilarity(dim=-1)

  def forward(self, y, t, x_emb, t_emb):
    # y is a batch of output images with shape (batch_size, 3, 256, 256)
    # t is a batch of text conditions with shape (batch_size,)
    # x_emb is a batch of input image embeddings with shape (batch_size, clip_dim)
    # t_emb is a batch of text condition embeddings with shape (batch_size, clip_dim)
    # Encode the output images using the CLIP encoder
    y_emb = clip_encoder.encode_image(y) # shape: (batch_size, clip_dim)
    # Compute the cosine similarity between the output image embeddings and the text condition embeddings
    sim_yt = self.cosine_similarity(y_emb, t_emb) # shape: (batch_size,)
    # Compute the cosine similarity between the input image embeddings and the text condition embeddings
    sim_xt = self.cosine_similarity(x_emb, t_emb) # shape: (batch_size,)
    # Compute the CLIP loss as the negative log likelihood of the softmax over the similarities
    clip_loss = -torch.log(torch.exp(sim_yt) / (torch.exp(sim_yt) + torch.exp(sim_xt))) # shape: (batch_size,)
    # Return the mean CLIP loss over the batch
    return torch.mean(clip_loss)

# Define the attribute loss
class Attribute_Loss(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use the mean squared error as the loss function
    self.mse = torch.nn.MSELoss()

  def forward(self, y, a_ref):
    # y is a batch of output images with shape (batch_size, 3, 256, 256)
    # a_ref is a dictionary of reference attribute vectors with shape (batch_size, 128) for each key
    # Extract the hair attributes from the output images using the hair attribute encoder
    a_out = hair_attribute_encoder(y) # a dictionary of output attribute vectors with shape (batch_size, 128) for each key
    # Compute the attribute loss as the mean squared error between the output attribute vectors and the reference attribute vectors
    attribute_loss = 0
    for key in a_ref.keys():
      attribute_loss += self.mse(a_out[key], a_ref[key])
    # Return the attribute loss
    return attribute_loss

# Define the identity loss
class Identity_Loss(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pre-trained VGG-19 model as the feature extractor
    self.vgg = torchvision.models.vgg19(pretrained=True).eval()
    # Use the mean squared error as the loss function
    self.mse = torch.nn.MSELoss()

  def forward(self, y, x):
    # y is a batch of output images with shape (batch_size, 3, 256, 256)
    # x is a batch of input images with shape (batch_size, 3, 256, 256)
    # Extract the features from the output images and the input images using the VGG model
    y_features = self.vgg(y) # shape: (batch_size, num_features)
    x_features = self.vgg(x) # shape: (batch_size, num_features)
    # Compute the identity loss as the mean squared error between the output features and the input features
    identity_loss = self.mse(y_features, x_features)
    # Return the identity loss
    return identity_loss

# Define the adversarial loss
class Adversarial_Loss(torch.nn.Module):
  def __init__(self):
    super().__init__()
    # Use a pre-trained StyleGAN2 discriminator as the discriminator
    self.discriminator = StyleGAN2_Discriminator(pretrained=True)

  def forward(self, y):
     # y is a batch of output images with shape (batch_size, 3,