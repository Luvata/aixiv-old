---
title: 2101.00190v1 Prefix-Tuning  Optimizing Continuous Prompts for Generation
date: 2021-01-01
---

# [Prefix-Tuning: Optimizing Continuous Prompts for Generation](http://arxiv.org/abs/2101.00190v1)

authors: Xiang Lisa Li, Percy Liang


## What, Why and How

[1]: https://arxiv.org/abs/2101.00190 "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
[2]: https://arxiv.org/pdf/2101.00190v1.pdf "Xiang Lisa Li Abstract - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2101.00190v1 "[2101.00190v1] Prefix-Tuning: Optimizing Continuous Prompts for Generation"

Here is a summary of the paper:

- **What**: The paper proposes prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix).
- **Why**: The paper aims to address the problem of storing a full copy of the large pretrained language model for each task, which can be prohibitively expensive and inefficient. Prefix-tuning only requires storing the prefix for each task, making it modular and space-efficient.
- **How**: The paper draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". The paper applies prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. The paper evaluates prefix-tuning on various metrics and compares it with fine-tuning and other baselines. The paper finds that prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training.

## Main Contributions

[1]: https://arxiv.org/pdf/2101.00190v1.pdf "Xiang Lisa Li Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2101.00190 "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2101.00190v1 "[2101.00190v1] Prefix-Tuning: Optimizing Continuous Prompts for Generation"

The paper lists the following contributions:

- **A novel method for lightweight fine-tuning of large pretrained language models for natural language generation tasks, called prefix-tuning, which only optimizes a small continuous task-specific vector (called the prefix) and keeps the language model parameters frozen**[^1^][1] [^2^][2] [^3^][3].
- **A theoretical analysis of prefix-tuning, showing that it can be seen as a form of adaptive prompting that learns optimal prompts for each task**[^1^][1].
- **An empirical evaluation of prefix-tuning on two natural language generation tasks: table-to-text generation and summarization, using two large pretrained language models: GPT-2 and BART**[^1^][1] [^2^][2].
- **A comparison of prefix-tuning with fine-tuning and other baselines on various metrics, such as performance, parameter efficiency, generalization, and robustness**[^1^][1] [^2^][2].

## Method Summary

[1]: https://arxiv.org/pdf/2101.00190v1.pdf "Xiang Lisa Li Abstract - arXiv.org"
[2]: https://arxiv.org/abs/2101.09545 "[2101.09545] Acceleration Methods - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2101.00190v1 "[2101.00190v1] Prefix-Tuning: Optimizing Continuous Prompts for Generation"

Here is a summary of the method section of the paper:

- **The paper describes the prefix-tuning method, which consists of three main components: the prefix, the language model, and the objective function**[^1^][1].
- **The prefix is a small continuous task-specific vector that is prepended to the input sequence and optimized during training. The prefix can be seen as a form of adaptive prompting that learns optimal prompts for each task**[^1^][1].
- **The language model is a large pretrained language model (such as GPT-2 or BART) that is frozen during training and used to generate the output sequence. The language model treats the prefix as "virtual tokens" and allows subsequent tokens to attend to it**[^1^][1].
- **The objective function is a task-specific loss function (such as cross-entropy or ROUGE) that measures the quality of the generated output and guides the optimization of the prefix**[^1^][1].
- **The paper also provides a theoretical analysis of prefix-tuning, showing that it can be seen as a form of adaptive prompting that learns optimal prompts for each task. The paper proves that prefix-tuning can approximate any fine-tuned language model under mild assumptions**[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Prefix-tuning algorithm
Input: a pretrained language model LM, a task-specific dataset D, a task-specific loss function L
Output: a task-specific prefix vector p
Initialize p randomly
for each epoch do
  for each (x,y) in D do
    # x is the input sequence, y is the output sequence
    # prepend p to x and generate y_hat from LM
    y_hat = LM(p + x)
    # compute the loss between y_hat and y
    loss = L(y_hat, y)
    # update p by gradient descent
    p = p - lr * grad(loss, p)
  end for
end for
return p
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Prefix-tuning algorithm
Input: a pretrained language model LM, a task-specific dataset D, a task-specific loss function L
Output: a task-specific prefix vector p
# get the hidden size and vocabulary size of LM
h = LM.hidden_size
v = LM.vocab_size
# initialize p as a matrix of shape (prefix_length, h)
p = torch.randn(prefix_length, h)
# make p trainable
p.requires_grad = True
# create an optimizer for p
optimizer = torch.optim.Adam([p], lr=lr)
for each epoch do
  # shuffle the dataset
  D = D.shuffle()
  for each (x,y) in D do
    # x is the input sequence, y is the output sequence
    # convert x and y to tensors of shape (seq_length,)
    x = torch.tensor(x)
    y = torch.tensor(y)
    # prepend p to x and get the embeddings of shape (prefix_length + seq_length, h)
    x = torch.cat([p, LM.embeddings(x)], dim=0)
    # generate y_hat from LM by decoding from x
    y_hat = LM.decode(x)
    # compute the loss between y_hat and y using L
    loss = L(y_hat, y)
    # zero the gradients
    optimizer.zero_grad()
    # compute the gradients of loss with respect to p
    loss.backward()
    # update p by gradient descent
    optimizer.step()
  end for
end for
return p
```