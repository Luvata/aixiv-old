---
title: 2302.14045v2 Language Is Not All You Need  Aligning Perception with Language Models
date: 2023-02-15
---

# [Language Is Not All You Need: Aligning Perception with Language Models](http://arxiv.org/abs/2302.14045v2)

authors: Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei


## What, Why and How

[1]: https://arxiv.org/abs/2302.14045 "[2302.14045] Language Is Not All You Need: Aligning Perception with ..."
[2]: https://arxiv.org/pdf/2302.14045v2 "Language Is Not All You Need: Aligning Perception with ... - arXiv.org"
[3]: http://arxiv-export3.library.cornell.edu/abs/2302.14045v2 "[2302.14045v2] Language Is Not All You Need: Aligning Perception with ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot).
- **Why**: The paper aims to achieve a big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence.
- **How**: The paper trains Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. The paper evaluates Kosmos-1 on various settings and tasks without any gradient updates or finetuning. The paper also introduces a dataset of Raven IQ test to diagnose the nonverbal reasoning capability of MLLMs.

## Main Contributions

[1]: https://arxiv.org/pdf/2302.14045v2 "Language Is Not All You Need: Aligning Perception with ... - arXiv.org"
[2]: https://arxiv.org/abs/2302.14045 "[2302.14045] Language Is Not All You Need: Aligning Perception with ..."
[3]: http://arxiv-export3.library.cornell.edu/abs/2302.14045v2 "[2302.14045v2] Language Is Not All You Need: Aligning Perception with ..."

The paper claims the following contributions[^1^][1]:

- It introduces Kosmos-1, a MLLM that can perceive general modalities, learn in context, and follow instructions.
- It trains Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data.
- It evaluates Kosmos-1 on various settings and tasks without any gradient updates or finetuning, and shows that it achieves impressive performance on language, perception-language, and vision tasks.
- It shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language.
- It introduces a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.

## Method Summary

[1]: https://arxiv.org/pdf/2302.14045v2 "Language Is Not All You Need: Aligning Perception with ... - arXiv.org"
[2]: https://arxiv.org/abs/2302.14045 "[2302.14045] Language Is Not All You Need: Aligning Perception with ..."
[3]: http://arxiv-export3.library.cornell.edu/abs/2302.14045v2 "[2302.14045v2] Language Is Not All You Need: Aligning Perception with ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper describes the architecture of Kosmos-1, which consists of a multimodal encoder and a multimodal decoder. The encoder is based on the Transformer-XL model, and the decoder is based on the UniLM model. The encoder and decoder share the same vocabulary and embeddings for text and images.
- The paper introduces a novel multimodal embedding layer that can embed both text and images into a common semantic space. The embedding layer uses a hybrid approach that combines discrete and continuous representations for text and images. The discrete representation is based on byte-pair encoding (BPE) for text and patch-based encoding (PBE) for images. The continuous representation is based on a position-aware projection layer that maps the discrete tokens to a high-dimensional vector space.
- The paper details the training procedure of Kosmos-1, which involves pre-training on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. The paper uses three pre-training objectives: masked language modeling (MLM), masked image modeling (MIM), and multimodal alignment modeling (MAM). The paper also applies several data augmentation techniques, such as random cropping, rotation, flipping, and masking for images, and shuffling, dropping, and masking for text.
- The paper explains how to use Kosmos-1 for various tasks and settings, such as zero-shot, few-shot, and multimodal chain-of-thought prompting. The paper uses natural language instructions to specify the task and modality, and uses special tokens to separate different segments of input and output. The paper also introduces a novel multimodal prompt tuning method that can fine-tune Kosmos-1 on a small amount of task-specific data using gradient updates.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the multimodal encoder and decoder
encoder = TransformerXL(num_layers=24, num_heads=16, hidden_size=1024)
decoder = UniLM(num_layers=24, num_heads=16, hidden_size=1024)

# Define the multimodal embedding layer
embedding_layer = MultimodalEmbeddingLayer(vocab_size=65536, embed_size=1024)

# Define the pre-training objectives
mlm = MaskedLanguageModeling()
mim = MaskedImageModeling()
mam = MultimodalAlignmentModeling()

# Load the web-scale multimodal corpora
corpora = load_multimodal_corpora()

# Pre-train Kosmos-1 on the corpora
for batch in corpora:
  # Apply data augmentation techniques
  batch = augment_data(batch)
  # Embed the text and images into a common semantic space
  batch = embedding_layer(batch)
  # Encode the multimodal input
  encoder_output = encoder(batch)
  # Decode the multimodal output
  decoder_output = decoder(encoder_output)
  # Compute the pre-training losses
  mlm_loss = mlm(decoder_output, batch)
  mim_loss = mim(decoder_output, batch)
  mam_loss = mam(encoder_output, batch)
  # Update the model parameters
  loss = mlm_loss + mim_loss + mam_loss
  loss.backward()
  optimizer.step()

# Use Kosmos-1 for various tasks and settings
def use_kosmos_1(input):
  # Parse the natural language instructions to specify the task and modality
  task, modality = parse_instructions(input)
  # Use special tokens to separate different segments of input and output
  input = add_special_tokens(input)
  # Embed the text and images into a common semantic space
  input = embedding_layer(input)
  # Encode the multimodal input
  encoder_output = encoder(input)
  # Decode the multimodal output
  decoder_output = decoder(encoder_output)
  # Generate the output according to the task and modality
  output = generate_output(decoder_output, task, modality)
  return output

# Optionally, fine-tune Kosmos-1 on a small amount of task-specific data using gradient updates
def fine_tune_kosmos_1(data):
  for batch in data:
    # Use multimodal prompt tuning method to fine-tune Kosmos-1 on the batch
    output = use_kosmos_1(batch)
    loss = compute_loss(output, batch)
    loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
num_layers = 24
num_heads = 16
hidden_size = 1024
vocab_size = 65536
embed_size = 1024
patch_size = 32
image_size = 256
num_patches = (image_size // patch_size) ** 2
max_length = 512
batch_size = 64
learning_rate = 1e-4

# Define the multimodal encoder and decoder
encoder = transformers.TransformerXLModel(
  d_model=hidden_size,
  nhead=num_heads,
  num_encoder_layers=num_layers,
  dropout=0.1,
)

decoder = transformers.UniLMModel(
  d_model=hidden_size,
  nhead=num_heads,
  num_decoder_layers=num_layers,
  dropout=0.1,
)

# Define the multimodal embedding layer
class MultimodalEmbeddingLayer(torch.nn.Module):
  def __init__(self, vocab_size, embed_size):
    super().__init__()
    # Define the text embedding layer
    self.text_embed = torch.nn.Embedding(vocab_size, embed_size)
    # Define the image embedding layer
    self.image_embed = torchvision.models.vit16(pretrained=True).embeddings.patch_embeddings
    # Define the position-aware projection layer
    self.proj_layer = torch.nn.Linear(embed_size + max_length, embed_size)
    # Define the positional embeddings for text and images
    self.text_pos_embed = torch.nn.Parameter(torch.randn(1, max_length, embed_size))
    self.image_pos_embed = torch.nn.Parameter(torch.randn(1, num_patches, embed_size))
  
  def forward(self, input):
    # Split the input into text and images segments
    text_input, image_input = split_input(input)
    # Embed the text tokens using BPE
    text_embed = self.text_embed(text_input)
    # Embed the image patches using PBE
    image_embed = self.image_embed(image_input)
    # Add the positional embeddings to the text and image embeddings
    text_embed += self.text_pos_embed[:, :text_input.size(1), :]
    image_embed += self.image_pos_embed[:, :image_input.size(1), :]
    # Concatenate the text and image embeddings along the sequence dimension
    embed = torch.cat([text_embed, image_embed], dim=1)
    # Project the embeddings to a high-dimensional vector space using a position-aware projection layer
    pos_ids = torch.arange(embed.size(1), device=embed.device).unsqueeze(0).expand_as(embed)
    proj_input = torch.cat([embed, pos_ids], dim=-1)
    proj_output = self.proj_layer(proj_input)
    return proj_output

# Define the pre-training objectives

# Masked language modeling (MLM)
mlm = transformers.BertForMaskedLM.from_pretrained("bert-base-uncased")

# Masked image modeling (MIM)
mim = torchvision.models.vit16(pretrained=True).head

# Multimodal alignment modeling (MAM)
mam = torch.nn.CosineSimilarity(dim=-1)

# Define the loss functions

# Cross entropy loss for MLM and MIM
ce_loss = torch.nn.CrossEntropyLoss()

# Mean squared error loss for MAM
mse_loss = torch.nn.MSELoss()

# Load the web-scale multimodal corpora
corpora = load_multimodal_corpora()

# Pre-train Kosmos-1 on the corpora

# Create an optimizer for updating the model parameters
optimizer = torch.optim.AdamW([
  {"params": encoder.parameters()},
  {"params": decoder.parameters()},
  {"params": embedding_layer.parameters()},
  {"params": mlm.parameters()},
  {"params": mim.parameters()},
], lr=learning_rate)

for epoch in range(num_epochs):
  for batch in corpora:
    # Apply data augmentation techniques
    batch = augment_data(batch)
    # Embed the text and images into a common semantic space
    batch = embedding_layer(batch)
    # Encode the multimodal input
    encoder_output = encoder(batch)
    # Decode the multimodal output
    decoder_output = decoder(encoder_output)
    # Compute the pre-training losses

    # MLM loss: predict the masked text tokens using the decoder output and the MLM head
    mlm_logits = mlm(decoder_output).logits
    mlm_labels = get_mlm_labels(batch)
    mlm_loss = ce_loss(mlm_logits, mlm_labels)

    # MIM loss: predict the masked image patches using the decoder output and the MIM head
    mim_logits = mim(decoder_output).logits
    mim_labels = get_mim_labels(batch)
    mim_loss = ce_loss(mim_logits, mim_labels)

    # MAM loss: align the encoder output and the decoder output using the MAM head
    mam_score = mam(encoder_output, decoder_output)
    mam_target = torch.ones_like(mam_score)
    mam_loss = mse_loss(mam_score, mam_target)

    # Update the model parameters
    loss = mlm_loss + mim_loss + mam_loss
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# Use Kosmos-1 for various tasks and settings
def use_kosmos_1(input):
  # Parse the natural language instructions to specify the task and modality
  task, modality = parse_instructions(input)
  # Use special tokens to separate different segments of input and output
  input = add_special_tokens(input)
  # Embed the text and images into a common semantic space
  input = embedding_layer(input)
  # Encode the multimodal input
  encoder_output = encoder(input)
  # Decode the multimodal output
  decoder_output = decoder(encoder_output)
  # Generate the output according to the task and modality
  output = generate_output(decoder_output, task, modality)
  return output

# Optionally, fine-tune Kosmos-1 on a small amount of task-specific data using gradient updates
def fine_tune_kosmos_1(data):
  for batch in data:
    # Use multimodal prompt tuning method to fine-tune Kosmos-1 on the batch
    output = use_kosmos_1(batch)
    loss = compute_loss(output, batch)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```