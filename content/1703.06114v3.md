---
title: 1703.06114v3 Deep Sets
date: 2017-03-07
---

# [Deep Sets](http://arxiv.org/abs/1703.06114v3)

authors: Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, Alexander Smola


## What, Why and How

[1]: https://arxiv.org/abs/1703.06114 "[1703.06114] Deep Sets - arXiv.org"
[2]: https://arxiv.org/pdf/1703.06114.pdf "Deep Sets - arXiv.org"
[3]: http://export.arxiv.org/abs/1703.06114 "[1703.06114] Deep Sets - export.arxiv.org"

Here is a summary of the paper[^1^][1]:

- **What**: The paper studies the problem of designing models for machine learning tasks defined on sets, which are invariant to permutations of the set elements.
- **Why**: The paper argues that such problems are widespread and challenging, and that existing approaches are either limited or inefficient.
- **How**: The paper provides a characterization of permutation invariant functions and a family of functions to which any permutation invariant objective function must belong. The paper also proposes a deep network architecture that can operate on sets and handle both unsupervised and supervised learning tasks. The paper demonstrates the applicability of the method on various problems such as population statistic estimation, point cloud classification, set expansion, and outlier detection.

## Main Contributions

[1]: https://arxiv.org/abs/1703.06114 "[1703.06114] Deep Sets - arXiv.org"
[2]: https://arxiv.org/pdf/1703.06114.pdf "Deep Sets - arXiv.org"
[3]: http://export.arxiv.org/abs/1703.06114 "[1703.06114] Deep Sets - export.arxiv.org"

According to the paper[^1^][1], the main contributions are:

- A characterization of permutation invariant functions and a family of functions to which any permutation invariant objective function must belong.
- A deep network architecture that can operate on sets and handle both unsupervised and supervised learning tasks, based on the proposed family of functions.
- A derivation of the necessary and sufficient conditions for permutation equivariance in deep models.
- An empirical evaluation of the method on various problems such as population statistic estimation, point cloud classification, set expansion, and outlier detection.

## Method Summary

[1]: https://arxiv.org/abs/1703.06114 "[1703.06114] Deep Sets - arXiv.org"
[2]: https://arxiv.org/pdf/1703.06114.pdf "Deep Sets - arXiv.org"
[3]: http://export.arxiv.org/abs/1703.06114 "[1703.06114] Deep Sets - export.arxiv.org"

Here is a summary of the method section of the paper[^1^][1]:

- The paper first introduces some notation and definitions for sets, functions, and neural networks.
- The paper then presents the main theorem, which states that any permutation invariant function can be decomposed into a sum of functions that depend only on the cardinality of the set and a function that depends only on the sum of the elements in the set.
- The paper also shows that any permutation equivariant function can be decomposed into a sum of functions that depend only on the cardinality of the set and a function that depends only on the sum and the difference of the elements in the set.
- The paper then proposes a deep network architecture that can operate on sets, based on the proposed decomposition. The architecture consists of two modules: a $\rho$ module that maps each element in the set to a latent representation, and a $\phi$ module that aggregates the latent representations into an output. The paper shows how to choose $\rho$ and $\phi$ to ensure permutation invariance or equivariance.
- The paper then describes how to apply the proposed architecture to various problems, such as population statistic estimation, point cloud classification, set expansion, and outlier detection. The paper also discusses some extensions and variations of the architecture, such as using attention mechanisms, recurrent neural networks, or convolutional neural networks.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define a function rho that maps each element in the set to a latent representation
def rho(x):
  # Choose a suitable neural network architecture for rho
  # For example, a multilayer perceptron
  return MLP(x)

# Define a function phi that aggregates the latent representations into an output
def phi(Z):
  # Choose a suitable neural network architecture for phi
  # For example, a sum-pooling layer followed by a multilayer perceptron
  return MLP(sum(Z))

# Define a function f that operates on sets
def f(X):
  # X is a set of elements
  # Apply rho to each element in X and obtain a set of latent representations Z
  Z = {rho(x) for x in X}
  # Apply phi to Z and obtain the output y
  y = phi(Z)
  return y

# Apply f to various problems, such as population statistic estimation, point cloud classification, set expansion, and outlier detection
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import some libraries
import numpy as np
import tensorflow as tf

# Define some hyperparameters
num_hidden = 128 # Number of hidden units in rho and phi
num_output = 10 # Number of output units in phi
learning_rate = 0.01 # Learning rate for gradient descent
num_epochs = 100 # Number of training epochs
batch_size = 32 # Batch size for mini-batch gradient descent

# Define a function rho that maps each element in the set to a latent representation
def rho(x):
  # x is a vector of input features
  # Use a multilayer perceptron with one hidden layer and ReLU activation as rho
  W1 = tf.Variable(tf.random.normal([x.shape[1], num_hidden])) # Weight matrix for the first layer
  b1 = tf.Variable(tf.zeros([num_hidden])) # Bias vector for the first layer
  W2 = tf.Variable(tf.random.normal([num_hidden, num_hidden])) # Weight matrix for the second layer
  b2 = tf.Variable(tf.zeros([num_hidden])) # Bias vector for the second layer
  h1 = tf.nn.relu(tf.matmul(x, W1) + b1) # Hidden layer output
  h2 = tf.nn.relu(tf.matmul(h1, W2) + b2) # Latent representation output
  return h2

# Define a function phi that aggregates the latent representations into an output
def phi(Z):
  # Z is a matrix of latent representations, one per row
  # Use a sum-pooling layer followed by a multilayer perceptron with one hidden layer and softmax activation as phi
  W3 = tf.Variable(tf.random.normal([num_hidden, num_hidden])) # Weight matrix for the third layer
  b3 = tf.Variable(tf.zeros([num_hidden])) # Bias vector for the third layer
  W4 = tf.Variable(tf.random.normal([num_hidden, num_output])) # Weight matrix for the fourth layer
  b4 = tf.Variable(tf.zeros([num_output])) # Bias vector for the fourth layer
  s = tf.reduce_sum(Z, axis=0) # Sum-pooling output
  h3 = tf.nn.relu(tf.matmul(s, W3) + b3) # Hidden layer output
  y = tf.nn.softmax(tf.matmul(h3, W4) + b4) # Output layer output
  return y

# Define a function f that operates on sets
def f(X):
  # X is a matrix of elements in the set, one per row
  # Apply rho to each element in X and obtain a matrix of latent representations Z
  Z = tf.map_fn(rho, X) # Z has the same shape as X
  # Apply phi to Z and obtain the output y
  y = phi(Z)
  return y

# Define a loss function to measure the error between f(X) and the true label y_true
def loss(X, y_true):
  y_pred = f(X) # Predicted label
  return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_true, y_pred)) # Cross-entropy loss

# Define an optimizer to minimize the loss function using gradient descent
optimizer = tf.optimizers.Adam(learning_rate)

# Define a function to compute the accuracy of f(X) and the true label y_true
def accuracy(X, y_true):
  y_pred = f(X) # Predicted label
  return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_pred, axis=1)), tf.float32)) # Accuracy

# Load some data for training and testing (e.g. point cloud classification data)
X_train, y_train = load_train_data() # X_train is a list of matrices of points in each point cloud, y_train is a list of one-hot vectors of labels
X_test, y_test = load_test_data() # X_test is a list of matrices of points in each point cloud, y_test is a list of one-hot vectors of labels

# Train the model using mini-batch gradient descent
for epoch in range(num_epochs):
  # Shuffle the training data
  perm = np.random.permutation(len(X_train))
  X_train = [X_train[i] for i in perm]
  y_train = [y_train[i] for i in perm]
  
  # Loop over batches of training data
  for i in range(0, len(X_train), batch_size):
    X_batch = X_train[i:i+batch_size] # Batch of input sets
    y_batch = y_train[i:i+batch_size] # Batch of output labels
    
    # Compute the gradients and update the parameters
    with tf.GradientTape() as tape:
      tape.watch([W1, b1, W2, b2, W3, b3, W4, b4]) # Watch the variables
      l = loss(X_batch, y_batch) # Compute the loss
    grads = tape.gradient(l, [W1, b1, W2, b2, W3, b3, W4, b4]) # Compute the gradients
    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3, W4, b4])) # Update the parameters
  
  # Evaluate the model on the training and testing data
  train_loss = loss(X_train, y_train) # Training loss
  train_acc = accuracy(X_train, y_train) # Training accuracy
  test_loss = loss(X_test, y_test) # Testing loss
  test_acc = accuracy(X_test, y_test) # Testing accuracy
  
  # Print the results
  print(f"Epoch {epoch+1}, Train Loss: {train_loss}, Train Acc: {train_acc}, Test Loss: {test_loss}, Test Acc: {test_acc}")
```