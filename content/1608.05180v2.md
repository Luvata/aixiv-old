---
title: 1608.05180v2 A Holistic Approach for Data-Driven Object Cutout
date: 2016-08-06
---

# [A Holistic Approach for Data-Driven Object Cutout](http://arxiv.org/abs/1608.05180v2)

authors: Huayong Xu, Yangyan Li, Wenzheng Chen, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen


## What, Why and How

[1]: https://arxiv.org/abs/1608.05180v2 "[1608.05180v2] A Holistic Approach for Data-Driven Object Cutout"
[2]: https://arxiv.org/pdf/1608.05180v2 "arXiv:1608.05180v2 [cs.CV] 16 Sep 2016"
[3]: https://www.quantumlah.org/publications/software/QSampling/QSampling.pdf "Random samples of quantum states: Online resources arXiv:1612.05180v2 ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a holistic approach for data-driven object cutout, which leverages a deep neural network (DNN) trained for objects of a particular class (chairs) to output a probability map (P-map) that indicates how likely each pixel inside a rectangle is to be contained inside an object from the class of interest.
- **Why**: The paper aims to automate the object cutout operation, which is a fundamental task for image editing and manipulation, but is extremely challenging in real-world images with cluttered backgrounds and similar objects. Existing cutout methods are based mainly on low-level image analysis, which may fail to capture the entire shape of the object of interest.
- **How**: The paper uses a DNN trained on a large dataset of chair images to learn global shape priors for chairs. Given a rectangular image region, the DNN outputs a P-map that reflects the likelihood of each pixel belonging to a chair. The paper then uses the P-maps to evaluate and rank rectangle proposals that contain an instance of the class, and further processes good proposals to produce an accurate object cutout mask. The paper evaluates the approach on segmentation benchmark datasets and shows that it significantly outperforms the state-of-the-art on them.

## Main Contributions

The paper claims to make the following contributions:

- It introduces a novel holistic approach for data-driven object cutout, which considers the entire shape of the object of interest by leveraging higher-level image analysis and learnt global shape priors.
- It demonstrates how to use a DNN trained for objects of a particular class (chairs) to output a P-map that indicates how likely each pixel inside a rectangle is to be contained inside an object from the class of interest.
- It shows how to use the P-maps to evaluate and rank rectangle proposals that contain an instance of the class, and further process good proposals to produce an accurate object cutout mask.
- It evaluates the approach on segmentation benchmark datasets and shows that it significantly outperforms the state-of-the-art on them.

## Method Summary

[1]: https://arxiv.org/abs/1608.05180v2 "[1608.05180v2] A Holistic Approach for Data-Driven Object Cutout"
[2]: https://arxiv.org/pdf/1608.05180v2 "arXiv:1608.05180v2 [cs.CV] 16 Sep 2016"
[3]: https://www.quantumlah.org/publications/software/QSampling/QSampling.pdf "Random samples of quantum states: Online resources arXiv:1612.05180v2 ..."

Here is a summary of the method section of the paper[^1^][2]:

- The paper describes the main components of the proposed approach, which are: (1) a DNN that outputs a P-map for a given rectangle; (2) a rectangle proposal generation and ranking scheme that uses the P-maps; and (3) a cutout mask refinement procedure that applies GrabCut [28] on the best proposals.
- The paper explains how the DNN is trained on a large dataset of chair images with segmentation masks, using a fully convolutional network (FCN) [23] architecture. The paper also discusses how to handle multiple scales and orientations of chairs in the input images, and how to deal with occlusions and truncations of chairs.
- The paper details how the rectangle proposals are generated using Edge Boxes [39], and how they are ranked using a scoring function that combines the P-map values and the rectangle size. The paper also describes how to filter out redundant proposals using non-maximum suppression (NMS).
- The paper presents how the cutout mask is refined by applying GrabCut [28] on the best proposal, using the P-map as an initialization for the foreground and background models. The paper also shows how to handle cases where the best proposal is not satisfactory, by using a second-best proposal or asking for user feedback.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image I containing one or more chairs
# Output: a cutout mask M for each chair in I

# Train a DNN on a large dataset of chair images with segmentation masks
DNN = train_DNN(dataset)

# Generate rectangle proposals using Edge Boxes
proposals = edge_boxes(I)

# For each proposal, compute a P-map using the DNN
for p in proposals:
  p.P_map = DNN(p)

# Rank the proposals using a scoring function that combines the P-map values and the rectangle size
proposals = rank_proposals(proposals)

# Filter out redundant proposals using non-maximum suppression
proposals = NMS(proposals)

# Initialize an empty list of cutout masks
cutout_masks = []

# For each proposal, refine the cutout mask using GrabCut with the P-map as initialization
for p in proposals:
  M = grabcut(I, p, p.P_map)
  # If the cutout mask is satisfactory, add it to the list and continue
  if is_satisfactory(M):
    cutout_masks.append(M)
    continue
  # Otherwise, try the second-best proposal or ask for user feedback
  else:
    p2 = get_second_best(proposals)
    M2 = grabcut(I, p2, p2.P_map)
    # If the second-best proposal is satisfactory, add it to the list and continue
    if is_satisfactory(M2):
      cutout_masks.append(M2)
      continue
    # Otherwise, ask the user to adjust the rectangle or provide scribbles
    else:
      user_input = get_user_input()
      M3 = grabcut(I, user_input)
      # Add the final cutout mask to the list and continue
      cutout_masks.append(M3)
      continue

# Return the list of cutout masks
return cutout_masks
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import cv2
import torch
import torchvision

# Define some constants
NUM_PROPOSALS = 1000 # the number of rectangle proposals to generate
NMS_THRESHOLD = 0.7 # the threshold for non-maximum suppression
SATISFACTION_THRESHOLD = 0.9 # the threshold for cutout mask satisfaction
SCALE_FACTOR = 1.5 # the factor to scale up the input image
ANGLE_STEP = 15 # the step size for rotating the input image

# Define the DNN architecture using a fully convolutional network (FCN)
class DNN(torch.nn.Module):
  def __init__(self):
    super(DNN, self).__init__()
    # Use a pre-trained VGG16 network as the backbone
    self.backbone = torchvision.models.vgg16(pretrained=True).features
    # Replace the last max pooling layer with a convolutional layer
    self.backbone[-1] = torch.nn.Conv2d(512, 512, kernel_size=3, padding=1)
    # Add a deconvolutional layer to upsample the feature map
    self.deconv = torch.nn.ConvTranspose2d(512, 1, kernel_size=64, stride=32, padding=16)
    # Add a sigmoid layer to output a probability map
    self.sigmoid = torch.nn.Sigmoid()

  def forward(self, x):
    # Pass the input through the backbone
    x = self.backbone(x)
    # Pass the output through the deconvolutional layer
    x = self.deconv(x)
    # Pass the output through the sigmoid layer
    x = self.sigmoid(x)
    return x

# Define a function to train the DNN on a large dataset of chair images with segmentation masks
def train_DNN(dataset):
  # Initialize a DNN model
  model = DNN()
  # Move the model to GPU if available
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  model.to(device)
  # Define a loss function as binary cross entropy
  criterion = torch.nn.BCELoss()
  # Define an optimizer as stochastic gradient descent with momentum
  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
  # Define a number of epochs to train
  num_epochs = 10
  # Loop over the epochs
  for epoch in range(num_epochs):
    # Initialize the running loss
    running_loss = 0.0
    # Loop over the dataset in batches
    for i, data in enumerate(dataset):
      # Get the inputs and labels from the data
      inputs, labels = data
      # Move the inputs and labels to GPU if available
      inputs = inputs.to(device)
      labels = labels.to(device)
      # Zero the parameter gradients
      optimizer.zero_grad()
      # Forward pass the inputs through the model
      outputs = model(inputs)
      # Compute the loss using the criterion
      loss = criterion(outputs, labels)
      # Backward pass and optimize
      loss.backward()
      optimizer.step()
      # Update the running loss
      running_loss += loss.item()
      # Print statistics every 200 batches
      if i % 200 == 199:
        print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))
        running_loss = 0.0

  print('Finished training')
  return model

# Define a function to generate rectangle proposals using Edge Boxes [39]
def edge_boxes(I):
  # Initialize an EdgeBoxes object with parameters tuned for chairs [39]
  edge_boxes = cv2.ximgproc.createEdgeBoxes(maxBoxes=NUM_PROPOSALS, alpha=0.65, beta=0.75, minScore=0.01,
                                             maxAspectRatio=3.0, minBoxArea=1000.0, gamma=2.0,
                                             kappa=1.5)
  
  # Convert the image to grayscale and compute edges using Canny edge detector [8]
  gray = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)
  edges = cv2.Canny(gray, threshold1=100, threshold2=200)

  # Generate rectangle proposals using edge boxes and return them as a list of (x,y,w,h) tuples
  proposals = edge_boxes.getBoundingBoxes(edges, gray)
  
  return proposals

# Define a function to compute a P-map for a given rectangle using the DNN
def compute_P_map(I, p, model):
  # Crop the image according to the rectangle coordinates
  x, y, w, h = p
  cropped = I[y:y+h, x:x+w]
  # Resize the cropped image to 224x224 as required by the DNN
  resized = cv2.resize(cropped, (224, 224))
  # Convert the resized image to a tensor and normalize it
  tensor = torchvision.transforms.ToTensor()(resized)
  tensor = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(tensor)
  # Add a batch dimension to the tensor
  tensor = tensor.unsqueeze(0)
  # Move the tensor to GPU if available
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  tensor = tensor.to(device)
  # Pass the tensor through the DNN and get the output P-map
  P_map = model(tensor)
  # Remove the batch dimension and squeeze the P-map
  P_map = P_map.squeeze()
  # Move the P-map to CPU and convert it to a numpy array
  P_map = P_map.cpu().detach().numpy()
  # Resize the P-map to match the original rectangle size
  P_map = cv2.resize(P_map, (w, h))
  
  return P_map

# Define a function to rank the proposals using a scoring function that combines the P-map values and the rectangle size
def rank_proposals(proposals):
  # Initialize an empty list of scores
  scores = []
  # Loop over the proposals
  for p in proposals:
    # Get the P-map values and the rectangle size from the proposal
    P_values = p.P_map.flatten()
    w, h = p[2], p[3]
    # Compute the score as the mean of the P-values times the square root of the rectangle area
    score = np.mean(P_values) * np.sqrt(w * h)
    # Append the score to the list of scores
    scores.append(score)
  
  # Sort the proposals by their scores in descending order and return them as a list of (x,y,w,h) tuples
  sorted_proposals = [p for _, p in sorted(zip(scores, proposals), reverse=True)]
  
  return sorted_proposals

# Define a function to filter out redundant proposals using non-maximum suppression (NMS)
def NMS(proposals):
  # Initialize an empty list of filtered proposals
  filtered_proposals = []
  # Loop over the proposals
  for p in proposals:
    # Initialize a flag to indicate if the proposal is redundant or not
    redundant = False
    # Loop over the filtered proposals
    for f in filtered_proposals:
      # Compute the intersection over union (IoU) between the proposal and the filtered proposal
      IoU = compute_IoU(p, f)
      # If the IoU is above a threshold, mark the proposal as redundant and break the loop
      if IoU > NMS_THRESHOLD:
        redundant = True
        break
    
    # If the proposal is not redundant, append it to the filtered proposals list
    if not redundant:
      filtered_proposals.append(p)

  return filtered_proposals

# Define a helper function to compute the intersection over union (IoU) between two rectangles
def compute_IoU(p1, p2):
  # Get the coordinates of the two rectangles
  x1, y1, w1, h1 = p1
  x2, y2, w2, h2 = p2
  # Compute the area of each rectangle
  area1 = w1 * h1
  area2 = w2 * h2
  # Compute the coordinates of the intersection rectangle
  x_inter = max(x1, x2)
  y_inter = max(y1, y2)
  w_inter = min(x1 + w1, x2 + w2) - x_inter
  h_inter = min(y1 + h1, y2 + h2) - y_inter
  # If there is no intersection, return zero
  if w_inter <=0 or h_inter <=0:
    return 0.0
  # Otherwise, compute the area of the intersection rectangle
  area_inter = w_inter * h_inter
  # Compute the area of the union rectangle as the sum of areas minus the intersection area
  area_union = area1 + area2 - area_inter
  # Compute and return the IoU as the ratio of intersection area to union area
  IoU = area_inter / area_union
  
  
# Define a function to refine the cutout mask using GrabCut [28] with the P-map as initialization 
def grabcut(I, p, P_map):
  
```