---
title: 2205.00823v1 CenterCLIP  Token Clustering for Efficient Text-Video Retrieval
date: 2022-05-01
---

# [CenterCLIP: Token Clustering for Efficient Text-Video Retrieval](http://arxiv.org/abs/2205.00823v1)

authors: Shuai Zhao, Linchao Zhu, Xiaohan Wang, Yi Yang


## What, Why and How

[1]: https://arxiv.org/pdf/2205.00823v1.pdf "CenterCLIP: Token Clustering for Efficient Text-Video Retrieval - arXiv.org"
[2]: https://arxiv.org/abs/2205.00823 "CenterCLIP: Token Clustering for Efficient Text-Video Retrieval"
[3]: http://export.arxiv.org/abs/2205.00823 "[2205.00823] CenterCLIP: Token Clustering for Efficient Text-Video ..."

Here is a summary of the paper:

- **What**: The paper proposes a method called CenterCLIP for efficient text-video retrieval, which reduces the number of redundant video tokens by clustering them into segments and selecting the most representative ones.
- **Why**: The paper aims to address the problem of high computation costs and low efficiency of existing text-video retrieval methods, which use vision transformers to generate discrete visual token sequences from videos. These sequences often contain many homogeneous tokens due to the redundancy nature of consecutive and similar frames in videos, which hinders the deployment of video retrieval models in web applications.
- **How**: The paper designs a multi-segment token clustering algorithm to find the most representative tokens and drop the non-essential ones. The algorithm divides videos into multiple segments and conducts segment-level clustering using either deterministic medoids or iterative partitioning. Center tokens from each segment are later concatenated into a new sequence, while their original spatial-temporal relations are well maintained. The paper evaluates the proposed method on typical text-video benchmarks and shows that it surpasses existing state-of-the-art by a large margin, while reducing the training memory cost by 35% and accelerating the inference speed by 14% at the best case[^1^][1] [^2^][2].

## Main Contributions

The paper claims to make the following contributions:

- It proposes a novel token clustering algorithm for efficient text-video retrieval, which reduces the number of redundant video tokens and enhances the segment-level semantic alignment between video and text representations.
- It instantiates two clustering algorithms to efficiently find deterministic medoids and iteratively partition groups in high dimensional space, and compares their performance and efficiency.
- It conducts extensive experiments on typical text-video benchmarks and demonstrates that the proposed method outperforms existing state-of-the-art methods by a large margin, while reducing the computation costs and improving the inference speed.

## Method Summary

Here is a summary of the method section of the paper:

- The paper adopts the CLIP framework for text-video retrieval, which consists of a vision transformer and a text transformer that encode video and text inputs into latent embeddings, and a contrastive loss function that maximizes the cosine similarity between matched video-text pairs and minimizes it between unmatched pairs.
- The paper introduces a token clustering algorithm to reduce the number of video tokens generated by the vision transformer, which uses a convolutional neural network to extract frame-level features and then applies a linear projection to produce visual tokens. The algorithm divides the video tokens into multiple segments according to their temporal order, and then clusters the tokens within each segment using either deterministic medoids or iterative partitioning. The algorithm selects the center token from each cluster as the representative token and discards the rest. The center tokens from each segment are concatenated into a new sequence and fed into the vision transformer for further encoding.
- The paper explains the two clustering algorithms in detail. The deterministic medoids algorithm finds the medoid of each cluster by computing the pairwise distances between tokens and selecting the token with the smallest sum of distances to other tokens. The iterative partitioning algorithm iteratively splits each cluster into two sub-clusters based on the distance to a randomly chosen pivot token, until the desired number of clusters is reached. The paper compares the two algorithms in terms of complexity, stability, and performance.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the vision transformer and the text transformer
vision_transformer = VisionTransformer()
text_transformer = TextTransformer()

# Define the contrastive loss function
contrastive_loss = ContrastiveLoss()

# Define the token clustering algorithm
token_clustering = TokenClustering()

# For each batch of video-text pairs
for video, text in data_loader:

  # Extract frame-level features from video using CNN
  frame_features = CNN(video)

  # Project frame features into visual tokens using linear layer
  visual_tokens = Linear(frame_features)

  # Divide visual tokens into segments according to temporal order
  segments = segment(visual_tokens)

  # Cluster tokens within each segment using either deterministic medoids or iterative partitioning
  clusters = token_clustering(segments)

  # Select center token from each cluster and concatenate them into a new sequence
  center_tokens = select_and_concat(clusters)

  # Encode center tokens and text into latent embeddings using vision transformer and text transformer
  video_embedding = vision_transformer(center_tokens)
  text_embedding = text_transformer(text)

  # Compute contrastive loss between video and text embeddings
  loss = contrastive_loss(video_embedding, text_embedding)

  # Update model parameters using gradient descent
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torchvision
import numpy as np

# Define the hyperparameters
batch_size = 64 # the number of video-text pairs in each batch
num_segments = 8 # the number of segments to divide each video into
num_clusters = 4 # the number of clusters to form within each segment
clustering_method = "medoids" # the clustering algorithm to use, either "medoids" or "partitioning"
temperature = 0.07 # the temperature parameter for the contrastive loss function
learning_rate = 1e-4 # the learning rate for the optimizer

# Define the vision transformer and the text transformer
# We use the pre-trained models from https://github.com/openai/CLIP
vision_transformer = torch.hub.load('openai/CLIP', 'ViT-B/32', jit=False)
text_transformer = torch.hub.load('openai/CLIP', 'RN50x4', jit=False)

# Define the contrastive loss function
# We use the implementation from https://github.com/openai/CLIP/blob/main/clip/loss.py
contrastive_loss = nn.CrossEntropyLoss()

# Define the token clustering algorithm
class TokenClustering(nn.Module):
  def __init__(self, num_segments, num_clusters, method):
    super(TokenClustering, self).__init__()
    self.num_segments = num_segments # the number of segments to divide each video into
    self.num_clusters = num_clusters # the number of clusters to form within each segment
    self.method = method # the clustering algorithm to use, either "medoids" or "partitioning"

  def forward(self, tokens):
    # tokens: a tensor of shape (batch_size, num_frames, token_dim)
    batch_size, num_frames, token_dim = tokens.size()

    # Divide tokens into segments according to temporal order
    segments = tokens.view(batch_size, self.num_segments, -1, token_dim) # shape: (batch_size, num_segments, num_frames // num_segments, token_dim)

    # Cluster tokens within each segment using either deterministic medoids or iterative partitioning
    if self.method == "medoids":
      clusters = self.medoids(segments) # shape: (batch_size, num_segments, num_clusters)
    elif self.method == "partitioning":
      clusters = self.partitioning(segments) # shape: (batch_size, num_segments, num_clusters)
    else:
      raise ValueError("Invalid clustering method")

    # Select center token from each cluster and concatenate them into a new sequence
    center_tokens = torch.gather(segments, 2, clusters.unsqueeze(-1).expand(-1,-1,-1,token_dim)) # shape: (batch_size, num_segments, num_clusters, token_dim)
    center_tokens = center_tokens.view(batch_size, -1, token_dim) # shape: (batch_size, num_segments * num_clusters, token_dim)

    return center_tokens

  def medoids(self, segments):
    # segments: a tensor of shape (batch_size, num_segments, segment_length, token_dim)
    batch_size, num_segments, segment_length, token_dim = segments.size()

    # Compute pairwise distances between tokens within each segment
    distances = torch.cdist(segments, segments) # shape: (batch_size, num_segments, segment_length, segment_length)

    # Sum distances along one dimension to get the total distance for each token
    distances = distances.sum(dim=3) # shape: (batch_size, num_segments, segment_length)

    # Find the medoid of each cluster by selecting the token with the smallest total distance
    _, medoids = distances.topk(self.num_clusters,dim=2,largest=False) # shape: (batch_size,num_segments,num_clusters)

    return medoids

  def partitioning(self, segments):
    # segments: a tensor of shape (batch_size,num_segments,num_frames // num_segments,dim)
    batch_size,num_segments,num_frames // num_segments,dim=segments.size()

    # Initialize an empty list to store the cluster indices for each segment
    clusters=[]

    # For each segment in each batch
    for i in range(batch_size):
      for j in range(num_segments):

        # Get the tokens for the current segment
        tokens=segments[i,j,:,:] # shape: (num_frames // num_segments,dim)

        # Initialize a list to store the cluster indices for the current segment
        cluster=[]

        # Initialize a set to store the indices of unassigned tokens
        unassigned=set(range(num_frames // num_segments))

        # While there are still unassigned tokens and the cluster size is smaller than the desired number of clusters
        while unassigned and len(cluster) < self.num_clusters:

          # Randomly choose a pivot token from the unassigned tokens
          pivot=np.random.choice(list(unassigned))

          # Compute the distances from the pivot token to all other tokens
          distances=torch.norm(tokens-tokens[pivot,:],dim=1) # shape: (num_frames // num_segments,)

          # Find the closest token to the pivot token
          _,closest=distances.min(dim=0)

          # Add the closest token to the cluster
          cluster.append(closest.item())

          # Remove the closest token from the unassigned tokens
          unassigned.remove(closest.item())

        # Convert the cluster list to a tensor and append it to the clusters list
        cluster=torch.tensor(cluster) # shape: (num_clusters,)
        clusters.append(cluster)

    # Reshape the clusters list to a tensor of shape (batch_size,num_segments,num_clusters)
    clusters=torch.stack(clusters).view(batch_size,num_segments,self.num_clusters)

    return clusters

# Define the data loader that provides batches of video-text pairs
# We use the HowTo100M dataset from https://www.di.ens.fr/willow/research/howto100m/
data_loader = HowTo100MDataLoader(batch_size=batch_size)

# Define the optimizer that updates the model parameters using gradient descent
# We use the Adam optimizer from https://pytorch.org/docs/stable/optim.html
optimizer = torch.optim.Adam(params=list(vision_transformer.parameters()) + list(text_transformer.parameters()), lr=learning_rate)

# For each epoch
for epoch in range(num_epochs):

  # For each batch of video-text pairs
  for video, text in data_loader:

    # Extract frame-level features from video using CNN
    frame_features = torchvision.models.resnet50(pretrained=True)(video) # shape: (batch_size, num_frames, 2048)

    # Project frame features into visual tokens using linear layer
    visual_tokens = nn.Linear(2048, 512)(frame_features) # shape: (batch_size, num_frames, 512)

    # Cluster tokens using token clustering algorithm
    center_tokens = TokenClustering(num_segments, num_clusters, clustering_method)(visual_tokens) # shape: (batch_size, num_segments * num_clusters, 512)

    # Encode center tokens and text into latent embeddings using vision transformer and text transformer
    video_embedding = vision_transformer.encode_image(center_tokens) # shape: (batch_size, 512)
    text_embedding = text_transformer.encode_text(text) # shape: (batch_size, 512)

    # Compute contrastive loss between video and text embeddings
    logits = torch.matmul(video_embedding / temperature, text_embedding.t()) / temperature # shape: (batch_size, batch_size)
    labels = torch.arange(batch_size) # shape: (batch_size,)
    loss = contrastive_loss(logits, labels)

    # Update model parameters using gradient descent
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```