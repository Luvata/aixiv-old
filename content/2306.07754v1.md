---
title: 2306.07754v1 Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis
date: 2023-06-08
---

# [Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis](http://arxiv.org/abs/2306.07754v1)

authors: Yihan Ma, Zhengyu Zhao, Xinlei He, Zheng Li, Michael Backes, Yang Zhang


## What, Why and How

[1]: https://arxiv.org/pdf/2306.03751 "arXiv:2306.03751v1 [hep-th] 6 Jun 2023"
[2]: http://export.arxiv.org/abs/2306.07754 "[2306.07754] Generative Watermarking Against Unauthorized Subject ..."
[3]: https://arxiv.org/pdf/2006.07754v1 "PDF for 2006.07754v1 - arXiv.org"

Here is a summary of the paper[^1^][2] in terms of What, Why, and How:

- What: The paper proposes a novel method of generative watermarking that embeds a hidden watermark into the synthesized images by a subject-driven text-to-image model. The watermark can be extracted by a secret key and used to verify the ownership and authenticity of the images.
- Why: The paper aims to address the problem of unauthorized subject manipulation, where an attacker can modify the subject images or texts and generate fake images that look realistic and consistent with the original subject. The paper argues that such manipulation can pose serious threats to the privacy and security of the subject owners and users, and that existing watermarking methods are not effective or robust enough for this scenario.
- How: The paper introduces a generative watermarking framework that consists of three components: a subject-driven text-to-image model, a watermark generator, and a watermark extractor. The subject-driven text-to-image model is fine-tuned with a few subject images and can generate images conditioned on both text and subject. The watermark generator produces a random watermark pattern based on a secret key and embeds it into the latent space of the text-to-image model. The watermark extractor recovers the watermark pattern from the generated images by using the same secret key and compares it with the original one. The paper evaluates the proposed method on two datasets (CelebA-HQ and WikiArt) and demonstrates its effectiveness and robustness against various attacks, such as cropping, scaling, compression, noise, blurring, and style transfer.


## Main Contributions

[1]: https://arxiv.org/pdf/2306.03751 "arXiv:2306.03751v1 [hep-th] 6 Jun 2023"
[2]: https://arxiv.org/abs/2306.07861 "[2306.07861] Efficient Reionization in a Large Hydrodynamic Galaxy ..."
[3]: http://export.arxiv.org/abs/2306.07754 "[2306.07754] Generative Watermarking Against Unauthorized Subject ..."

According to the paper[^1^][3], the main contributions are:

- The paper proposes a novel method of generative watermarking that embeds a hidden watermark into the synthesized images by a subject-driven text-to-image model. The watermark can be extracted by a secret key and used to verify the ownership and authenticity of the images.
- The paper introduces a generative watermarking framework that consists of three components: a subject-driven text-to-image model, a watermark generator, and a watermark extractor. The paper provides the details of each component and how they work together.
- The paper evaluates the proposed method on two datasets (CelebA-HQ and WikiArt) and demonstrates its effectiveness and robustness against various attacks, such as cropping, scaling, compression, noise, blurring, and style transfer. The paper also compares the proposed method with existing watermarking methods and shows its advantages.


## Method Summary

[1]: https://arxiv.org/pdf/2306.03751 "arXiv:2306.03751v1 [hep-th] 6 Jun 2023"
[2]: https://arxiv.org/abs/2306.07861 "[2306.07861] Efficient Reionization in a Large Hydrodynamic Galaxy ..."
[3]: http://export.arxiv.org/abs/2306.07754 "[2306.07754] Generative Watermarking Against Unauthorized Subject ..."

Here is a summary of the method section of the paper[^1^][3]:

- The paper introduces a generative watermarking framework that consists of three components: a subject-driven text-to-image model, a watermark generator, and a watermark extractor.
- The subject-driven text-to-image model is fine-tuned with a few subject images and can generate images conditioned on both text and subject. The paper uses the DALL-E model as the base model and adapts it to different subjects by adding a subject embedding layer and a subject classifier layer. The paper also introduces a subject consistency loss to ensure that the generated images are consistent with the original subject.
- The watermark generator produces a random watermark pattern based on a secret key and embeds it into the latent space of the text-to-image model. The paper uses a hash function to generate the watermark pattern and a convolutional neural network to embed it into the latent codes. The paper also introduces a watermark loss to ensure that the watermark pattern is preserved in the generated images.
- The watermark extractor recovers the watermark pattern from the generated images by using the same secret key and compares it with the original one. The paper uses an inverse convolutional neural network to extract the latent codes from the generated images and a hash function to recover the watermark pattern. The paper also introduces a similarity metric to measure the similarity between the extracted and original watermark patterns.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the subject-driven text-to-image model
model = DALL-E()
model.add_subject_embedding_layer()
model.add_subject_classifier_layer()

# Fine-tune the model with a few subject images
for image, text, subject in subject_data:
  output = model(image, text, subject)
  loss = reconstruction_loss(output, image) + subject_consistency_loss(output, subject)
  loss.backward()
  optimizer.step()

# Define the watermark generator
watermark_generator = CNN()
hash_function = SHA256()

# Generate a random watermark pattern based on a secret key
watermark_pattern = hash_function(secret_key)

# Embed the watermark pattern into the latent space of the model
latent_codes = model.get_latent_codes(text, subject)
watermarked_codes = watermark_generator(latent_codes, watermark_pattern)
model.set_latent_codes(watermarked_codes)

# Define the watermark extractor
watermark_extractor = inverse_CNN()
hash_function = SHA256()

# Extract the watermark pattern from the generated images
generated_images = model.generate_images(text, subject)
extracted_codes = watermark_extractor(generated_images)
extracted_pattern = hash_function(extracted_codes)

# Compare the extracted pattern with the original one
similarity = similarity_metric(extracted_pattern, watermark_pattern)
if similarity > threshold:
  print("The image is authentic and belongs to the subject owner.")
else:
  print("The image is fake or manipulated.")
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import hashlib

# Define the hyperparameters
num_subjects = 10 # the number of subjects to fine-tune the model
num_images_per_subject = 5 # the number of images per subject to fine-tune the model
image_size = 256 # the size of the input and output images
text_length = 32 # the length of the input text
subject_length = 16 # the length of the subject embedding
latent_size = 512 # the size of the latent codes
watermark_size = 256 # the size of the watermark pattern
secret_key = "some_random_string" # the secret key to generate and extract the watermark pattern
threshold = 0.9 # the threshold to determine the similarity between watermark patterns

# Load the pre-trained DALL-E model
model = torch.hub.load('openai/DALL-E', 'DALL-E')

# Add a subject embedding layer to the model
subject_embedding = torch.nn.Embedding(num_subjects, subject_length)
model.add_module("subject_embedding", subject_embedding)

# Add a subject classifier layer to the model
subject_classifier = torch.nn.Linear(latent_size, num_subjects)
model.add_module("subject_classifier", subject_classifier)

# Define the reconstruction loss function
reconstruction_loss = torch.nn.MSELoss()

# Define the subject consistency loss function
subject_consistency_loss = torch.nn.CrossEntropyLoss()

# Define the optimizer for fine-tuning the model
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Load the subject data (images, texts, and labels)
subject_data = load_subject_data(num_subjects, num_images_per_subject, image_size, text_length)

# Fine-tune the model with a few subject images
for epoch in range(10):
  for image, text, subject in subject_data:
    # Resize and normalize the image
    image = torchvision.transforms.Resize(image_size)(image)
    image = torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(image)

    # Convert the text and subject to tensors
    text = torch.tensor(text)
    subject = torch.tensor(subject)

    # Forward pass the model
    output = model(image, text, subject)

    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(output, image)

    # Compute the subject consistency loss
    subject_logits = model.subject_classifier(model.get_latent_codes(text, subject))
    sub_loss = subject_consistency_loss(subject_logits, subject)

    # Compute the total loss
    loss = rec_loss + sub_loss

    # Backward pass and update the model parameters
    loss.backward()
    optimizer.step()

# Define the watermark generator as a convolutional neural network
watermark_generator = torch.nn.Sequential(
  torch.nn.Conv2d(latent_size, watermark_size, kernel_size=3, padding=1),
  torch.nn.ReLU(),
  torch.nn.Conv2d(watermark_size, latent_size, kernel_size=3, padding=1),
)

# Define the hash function as SHA256
hash_function = hashlib.sha256

# Generate a random watermark pattern based on a secret key
watermark_pattern = hash_function(secret_key.encode()).digest()
watermark_pattern = np.array([int(b) for b in watermark_pattern])
watermark_pattern = watermark_pattern.reshape(watermark_size, watermark_size)
watermark_pattern = torch.tensor(watermark_pattern).float()

# Embed the watermark pattern into the latent space of the model
latent_codes = model.get_latent_codes(text, subject)
watermarked_codes = watermark_generator(latent_codes, watermark_pattern)
model.set_latent_codes(watermarked_codes)

# Define the watermark extractor as an inverse convolutional neural network
watermark_extractor = torch.nn.Sequential(
  torch.nn.ConvTranspose2d(3, latent_size, kernel_size=4, stride=2),
  torch.nn.ReLU(),
  torch.nn.ConvTranspose2d(latent_size, latent_size, kernel_size=4, stride=2),
)

# Extract the watermark pattern from the generated images
generated_images = model.generate_images(text, subject)
extracted_codes = watermark_extractor(generated_images)
extracted_pattern = hash_function(extracted_codes.flatten().numpy()).digest()
extracted_pattern = np.array([int(b) for b in extracted_pattern])
extracted_pattern = extracted_pattern.reshape(watermark_size, watermark_size)
extracted_pattern = torch.tensor(extracted_pattern).float()

# Compare the extracted pattern with the original one using cosine similarity
similarity = torch.nn.functional.cosine_similarity(extracted_pattern, watermark_pattern, dim=0)
if similarity > threshold:
  print("The image is authentic and belongs to the subject owner.")
else:
  print("The image is fake or manipulated.")
```