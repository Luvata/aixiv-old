---
title: 2207.10642v1 Generative Multiplane Images  Making a 2D GAN 3D-Aware
date: 2022-07-11
---

# [Generative Multiplane Images: Making a 2D GAN 3D-Aware](http://arxiv.org/abs/2207.10642v1)

authors: Xiaoming Zhao, Fangchang Ma, David GÃ¼era, Zhile Ren, Alexander G. Schwing, Alex Colburn


## What, Why and How

[1]: https://arxiv.org/abs/2207.10642v1 "Generative Multiplane Images: Making a 2D GAN 3D-Aware"
[2]: https://arxiv.org/pdf/2207.10642v1.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2302.10642v1 "[2302.10642v1] Dynamical environments of (486958) Arrokoth: prior ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper proposes a method to make a 2D generative adversarial network (GAN) 3D-aware by generating multiplane images (MPIs) that can be rendered from different viewpoints.
- **Why**: The paper aims to address the limitations of existing 2D GANs that cannot produce view-consistent and realistic 3D images, and the challenges of training 3D GANs that require large memory and computation resources.
- **How**: The paper modifies a classical 2D GAN, i.e., StyleGANv2, by adding two components: 1) a multiplane image style generator branch that produces a set of alpha maps conditioned on their depth; 2) a pose-conditioned discriminator that evaluates the realism and view-consistency of the generated MPIs. The paper also introduces a novel loss function that encourages the generator to produce smooth and continuous MPIs. The paper evaluates the proposed method on three high-resolution datasets and shows that it can generate high-quality and view-consistent MPIs with less memory and computation than 3D GANs.

## Main Contributions

According to the paper at , the main contributions are:

- A simple and effective method to make a 2D GAN 3D-aware by generating MPIs that can be rendered from different viewpoints.
- A novel loss function that encourages the generator to produce smooth and continuous MPIs that are view-consistent and realistic.
- A demonstration that the proposed method can generate high-quality and view-consistent MPIs with less memory and computation than 3D GANs on three challenging and common high-resolution datasets.

## Method Summary

[1]: https://arxiv.org/abs/2207.10642v1 "Generative Multiplane Images: Making a 2D GAN 3D-Aware"
[2]: https://arxiv.org/pdf/2207.10642v1 "arXiv.org"
[3]: http://export.arxiv.org/abs/2302.10642v1 "[2302.10642v1] Dynamical environments of (486958) Arrokoth: prior ..."

Here is a summary of the method section of the paper at [^1^][1]:

- The paper modifies a classical 2D GAN, i.e., StyleGANv2, by adding two components: a **multiplane image style generator branch** and a **pose-conditioned discriminator**.
- The multiplane image style generator branch produces a set of alpha maps conditioned on their depth, which are then blended with the RGB images generated by the original StyleGANv2 generator to form MPIs. The number of alpha maps can be dynamically adjusted and can differ between training and inference.
- The pose-conditioned discriminator evaluates the realism and view-consistency of the generated MPIs by comparing them with real images rendered from different viewpoints. The discriminator also receives the pose information as an additional input.
- The paper also introduces a novel loss function that encourages the generator to produce smooth and continuous MPIs that are view-consistent and realistic. The loss function consists of four terms: 1) an adversarial loss that measures the realism of the generated MPIs; 2) a perceptual loss that measures the similarity between the generated and real images in feature space; 3) a depth consistency loss that measures the smoothness and continuity of the alpha maps; 4) a view consistency loss that measures the consistency between the generated MPIs rendered from different viewpoints.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Define the StyleGANv2 generator G and discriminator D
G = StyleGANv2()
D = StyleGANv2_D()

# Define the multiplane image style generator branch M
M = MultiplaneImageStyleGenerator()

# Define the pose-conditioned discriminator P
P = PoseConditionedDiscriminator()

# Define the loss function L
L = AdversarialLoss() + PerceptualLoss() + DepthConsistencyLoss() + ViewConsistencyLoss()

# Define the optimizer O
O = Adam()

# Define the number of alpha maps N
N = 8

# Define the training data X and the poses Y
X = RealImages()
Y = Poses()

# Train the model
for epoch in range(EPOCHS):
  # Sample a batch of real images x and poses y
  x, y = sample_batch(X, Y)
  
  # Generate a batch of RGB images z and alpha maps a using G and M
  z, a = G(x), M(x)
  
  # Generate a batch of MPIs m by blending z and a
  m = blend(z, a)
  
  # Render a batch of images r from m using different poses y'
  y' = sample_poses(Y)
  r = render(m, y')
  
  # Compute the discriminator outputs for x and r using D and P
  d_x = D(x) + P(x, y)
  d_r = D(r) + P(r, y')
  
  # Compute the generator outputs for z and a using D and P
  g_z = D(z) + P(z, y)
  g_a = D(a) + P(a, y)
  
  # Compute the loss value for the generator and the discriminator
  loss_g = L(g_z, g_a, d_x, d_r, x, r, a)
  loss_d = L(d_x, d_r, g_z, g_a)
  
  # Update the parameters of G, M, D and P using O
  O.step(loss_g, G.parameters() + M.parameters())
  O.step(loss_d, D.parameters() + P.parameters())
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2

# Define the StyleGANv2 generator G and discriminator D
# Use the official implementation from https://github.com/NVlabs/stylegan2-ada-pytorch
G = StyleGANv2()
D = StyleGANv2_D()

# Define the multiplane image style generator branch M
# Use a convolutional neural network with residual blocks and skip connections
class MultiplaneImageStyleGenerator(torch.nn.Module):
  def __init__(self, num_alpha_maps):
    super().__init__()
    self.num_alpha_maps = num_alpha_maps
    self.conv1 = torch.nn.Conv2d(3, 64, 3, 1, 1)
    self.res1 = ResBlock(64)
    self.res2 = ResBlock(64)
    self.res3 = ResBlock(64)
    self.res4 = ResBlock(64)
    self.conv2 = torch.nn.Conv2d(64, num_alpha_maps, 3, 1, 1)
    self.sigmoid = torch.nn.Sigmoid()
  
  def forward(self, x):
    # x is a batch of RGB images of shape (B, 3, H, W)
    # returns a batch of alpha maps of shape (B, N, H, W)
    x = self.conv1(x)
    x = self.res1(x)
    x = self.res2(x)
    x = self.res3(x)
    x = self.res4(x)
    x = self.conv2(x)
    x = self.sigmoid(x)
    return x

# Define the pose-conditioned discriminator P
# Use a convolutional neural network with residual blocks and global average pooling
class PoseConditionedDiscriminator(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = torch.nn.Conv2d(3, 64, 3, 1, 1)
    self.res1 = ResBlock(64)
    self.res2 = ResBlock(64)
    self.res3 = ResBlock(64)
    self.res4 = ResBlock(64)
    self.gap = torch.nn.AdaptiveAvgPool2d((1, 1))
    self.fc1 = torch.nn.Linear(64 + 6, 128) # 6 is the pose dimension (yaw and pitch angles in radians)
    self.fc2 = torch.nn.Linear(128, 1)

  def forward(self, x, y):
    # x is a batch of images of shape (B, 3, H, W)
    # y is a batch of poses of shape (B, 6)
    # returns a batch of scores of shape (B,)
    x = self.conv1(x)
    x = self.res1(x)
    x = self.res2(x)
    x = self.res3(x)
    x = self.res4(x)
    x = self.gap(x) # shape (B, 64, 1, 1)
    x = torch.flatten(x) # shape (B, 64)
    x = torch.cat([x, y], dim=1) # shape (B, 70)
    x = self.fc1(x) # shape (B, 128)
    x = torch.relu(x) 
    x = self.fc2(x) # shape (B, 1)
    x = torch.flatten(x) # shape (B,)
    return x

# Define the loss function L
# Use the hinge loss for the adversarial term and the LPIPS loss for the perceptual term
# Use the L2 loss for the depth consistency term and the L1 loss for the view consistency term
def L(g_z, g_a, d_x, d_r, x, r, a):
  # g_z and g_a are the generator outputs for RGB images and alpha maps
  # d_x and d_r are the discriminator outputs for real images and rendered images
  # x and r are the real images and rendered images
  # a are the alpha maps

  # Compute the adversarial loss for the generator and the discriminator
  adv_loss_g_z = -torch.mean(d_z) # hinge loss for RGB images
  adv_loss_g_a = -torch.mean(d_a) # hinge loss for alpha maps
  adv_loss_d_x = -torch.mean(torch.minimum(d_x - 1.0 ,0.0)) # hinge loss for real images
  adv_loss_d_r = -torch.mean(torch.minimum(-d_r - 1.0, 0.0)) # hinge loss for rendered images

  # Compute the perceptual loss for the generator
  # Use the LPIPS loss from https://github.com/richzhang/PerceptualSimilarity
  lpips = LPIPS()
  perc_loss_g_z = lpips(g_z, x) # LPIPS loss for RGB images
  perc_loss_g_a = lpips(g_a, a) # LPIPS loss for alpha maps

  # Compute the depth consistency loss for the generator
  # Use the L2 loss for the smoothness and continuity of the alpha maps
  depth_loss_g_a = torch.mean(torch.square(torch.gradient(g_a))) # L2 loss for alpha maps

  # Compute the view consistency loss for the generator
  # Use the L1 loss for the consistency between the rendered images and the real images
  view_loss_g_z = torch.mean(torch.abs(r - x)) # L1 loss for RGB images

  # Combine the four terms with weights
  lambda_adv = 1.0 # weight for the adversarial loss
  lambda_perc = 1.0 # weight for the perceptual loss
  lambda_depth = 1.0 # weight for the depth consistency loss
  lambda_view = 1.0 # weight for the view consistency loss

  loss_g = lambda_adv * (adv_loss_g_z + adv_loss_g_a) + lambda_perc * (perc_loss_g_z + perc_loss_g_a) + lambda_depth * depth_loss_g_a + lambda_view * view_loss_g_z
  loss_d = lambda_adv * (adv_loss_d_x + adv_loss_d_r)

  return loss_g, loss_d

# Define the optimizer O
# Use the Adam optimizer with learning rate of 0.0002 and beta1 of 0.5
O = torch.optim.Adam(lr=0.0002, betas=(0.5, 0.999))

# Define the number of alpha maps N
N = 8

# Define the training data X and the poses Y
# Use the FFHQ dataset from https://github.com/NVlabs/ffhq-dataset
# Use a pose estimator to extract yaw and pitch angles from each image
X = FFHQ()
Y = PoseEstimator(X)

# Define a function to blend RGB images and alpha maps to form MPIs
def blend(z, a):
  # z is a batch of RGB images of shape (B, 3, H, W)
  # a is a batch of alpha maps of shape (B, N, H, W)
  # returns a batch of MPIs of shape (B, N, H, W, C)
  
  # Normalize z to [0,1] range
  z = (z + 1) / 2
  
  # Expand z to match a in dimension N
  z = torch.unsqueeze(z, dim=1) # shape (B, 1, H, W, C)
  z = torch.repeat(z, (1, N, 1, 1)) # shape (B, N, H, W, C)
  
  # Expand a to match z in dimension C
  a = torch.unsqueeze(a, dim=-1) # shape (B, N, H, W, 1)
  
  # Multiply z and a element-wise to form MPIs
  m = z * a # shape (B, N, H, W, C)
  
  return m

# Define a function to render MPIs from different poses using homography matrices
def render(m, y):
  # m is a batch of MPIs of shape (B, N, H, W, C)
  # y is a batch of poses of shape (B, K) where K is the number of poses to render from
  # returns a batch of rendered images of shape (B * K , H , W , C)

  # Compute the homography matrices H for each pose in y using intrinsic and extrinsic parameters
  H = Homography(y) # shape (B , K , N ,3 ,3)

   # Warp each alpha map in m using H to form warped MPIs w
   w = [] 
   for i in range(B):
     for j in range(K):
       for k in range(N):
         w_ijk = cv2.warpPerspective(m[i][k], H[i][j][k], (W,H)) 
         w.append(w_ijk) 
   w = torch.stack(w) # shape (B * K * N , H , W , C)

   # Composite each warped MPI w to form rendered images r using alpha blending
   r = []
   for i in range(B * K):
     r_i = torch.zeros((H,W,C))
     for j in range(N):
       r_i = r_i + w[i*N+j]