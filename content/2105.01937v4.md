---
title: 2105.01937v4 FLEX  Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction
date: 2021-05-02
---

# [FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion Reconstruction](http://arxiv.org/abs/2105.01937v4)

authors: Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/2105.01937.pdf "arXiv:2105.01937v4 [cs.CV] 21 Oct 2022"
[2]: https://www.researchgate.net/publication/365009380_FLEX_Extrinsic_Parameters-free_Multi-view_3D_Human_Motion_Reconstruction "(PDF) FLEX: Extrinsic Parameters-free Multi-view 3D Human Motion ..."
[3]: https://arxiv.org/pdf/2105.01937v4 "arXiv.org e-Print archive"

Here is a summary of the paper:

- **What**: The paper introduces **FLEX**, an end-to-end extrinsic parameter-free multi-view model for 3D human motion reconstruction from multiple video streams[^1^][1] [^2^][2].
- **Why**: The paper aims to overcome the limitations of existing multi-view algorithms that strongly depend on camera parameters, particularly on relative transformations between the cameras, which are hard to obtain in dynamic and uncontrolled settings[^1^][1] [^2^][2].
- **How**: The paper proposes to learn 3D rotations and bone lengths rather than locations, as they are invariant to the camera position and can be predicted as common values for all camera views. The paper also introduces a novel multi-view fusion layer that learns fused deep features from multiple views. The paper evaluates the model on three public datasets and on synthetic video streams captured by dynamic cameras, and compares it to state-of-the-art methods that are not extrinsic parameter-free[^1^][1] [^2^][2].

## Main Contributions

The paper claims to make the following contributions:

- It presents the first end-to-end extrinsic parameter-free multi-view model for 3D human motion reconstruction.
- It proposes to learn 3D rotations and bone lengths instead of locations, which are invariant to the camera position and can be shared across views.
- It introduces a novel multi-view fusion layer that learns fused deep features from multiple views without requiring camera parameters.
- It demonstrates quantitative and qualitative results on three public datasets and on synthetic video streams captured by dynamic cameras, and shows that it outperforms state-of-the-art methods that are not extrinsic parameter-free by a large margin.

## Method Summary

The method section of the paper describes the proposed model, FLEX, in detail. The model consists of four main components:

- A **multi-view encoder** that takes multiple video streams as input and extracts 2D joint heatmaps and deep features for each view using a pre-trained pose estimation network.
- A **multi-view fusion layer** that fuses the deep features from multiple views into a single feature vector without requiring camera parameters. The fusion layer uses a learnable attention mechanism to weigh the contribution of each view based on its relevance and quality.
- A **3D rotation decoder** that predicts 3D joint rotations for each frame using the fused feature vector and a recurrent neural network. The decoder outputs quaternions that represent the relative orientation of each joint with respect to its parent joint in the kinematic tree.
- A **bone length decoder** that predicts bone lengths for each frame using the fused feature vector and a fully connected network. The decoder outputs a vector of bone lengths that are normalized by the height of the person.

The model is trained end-to-end using a combination of loss functions that measure the accuracy of the 3D rotations, bone lengths, and 2D projections. The model also uses a temporal smoothness loss to enforce temporal coherence of the motion.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: multiple video streams V = {V_1, ..., V_n}
# Output: 3D joint rotations R and bone lengths L for each frame

# Initialize the pose estimation network P
# Initialize the multi-view encoder E
# Initialize the multi-view fusion layer F
# Initialize the 3D rotation decoder D_r
# Initialize the bone length decoder D_l

# For each frame t in the video streams:
  # For each view i in V:
    # Extract 2D joint heatmaps H_i and deep features X_i using P and E
  # Fuse the deep features X = {X_1, ..., X_n} into a single feature vector Z using F
  # Predict 3D joint rotations R_t using Z and D_r
  # Predict bone lengths L_t using Z and D_l
  # Compute the loss functions L_rot, L_len, L_proj, and L_smooth using R_t, L_t, H, and R_t-1
  # Update the model parameters using gradient descent
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: multiple video streams V = {V_1, ..., V_n}
# Output: 3D joint rotations R and bone lengths L for each frame

# Define the number of views n, the number of joints m, and the number of frames T
# Define the kinematic tree K that specifies the parent-child relationship of the joints
# Define the root joint r and the global coordinate system G
# Define the camera parameters C = {C_1, ..., C_n} for each view (optional)

# Initialize the pose estimation network P (e.g., HRNet [54])
# Initialize the multi-view encoder E as a convolutional network that maps an image to a feature map
# Initialize the multi-view fusion layer F as a fully connected network that maps n feature vectors to one feature vector
# Initialize the 3D rotation decoder D_r as a bidirectional LSTM network that maps a feature vector to m quaternions
# Initialize the bone length decoder D_l as a fully connected network that maps a feature vector to m bone lengths

# For each frame t in the video streams:
  # For each view i in V:
    # Crop and resize the image I_i,t around the person of interest using a bounding box
    # Extract 2D joint heatmaps H_i,t and deep features X_i,t using P and E
    # Flatten and concatenate X_i,t into a 1D vector x_i,t
  # Fuse the deep features x = {x_1,t, ..., x_n,t} into a single feature vector z_t using F
  # Predict 3D joint rotations r_t = {r_1,t, ..., r_m,t} using z_t and D_r, where r_j,t is a quaternion for joint j at frame t
  # Predict bone lengths l_t = {l_1,t, ..., l_m,t} using z_t and D_l, where l_j,t is a bone length for joint j at frame t
  # Normalize l_t by dividing each element by the sum of all elements and multiplying by the estimated height of the person
  # Convert r_t to rotation matrices R_t = {R_1,t, ..., R_m,t} using quaternion algebra
  # Compute the global joint rotations Q_t = {Q_1,t, ..., Q_m,t} by multiplying R_j,t with Q_p(j),t, where p(j) is the parent of joint j in K
  # Compute the global joint locations J_t = {J_1,t, ..., J_m,t} by adding l_j,t * Q_j,t[:,2] to J_p(j),t, where Q_j,t[:,2] is the third column of Q_j,t
  # Transform J_t to the global coordinate system G using C if available (e.g., using PnP algorithm [18])
  # Compute the loss functions L_rot, L_len, L_proj, and L_smooth using R_t, l_t, H, and R_t-1 as follows:
    # L_rot = mean squared error between R_t and the ground truth rotations R_gt,t
    # L_len = mean squared error between l_t and the ground truth bone lengths l_gt,t
    # L_proj = mean squared error between the projected 2D joint locations P_t = {P_1,t, ..., P_n,t} and the ground truth 2D joint locations P_gt,t for each view i, where P_i,t is obtained by applying C_i to J_t
    # L_smooth = mean squared error between R_t and R_t-1
  # Update the model parameters using gradient descent with Adam optimizer [28]
```