---
title: 2208.05516v4 Quality Not Quantity  On the Interaction between Dataset Design and Robustness of CLIP
date: 2022-08-06
---

# [Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP](http://arxiv.org/abs/2208.05516v4)

authors: Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, Ludwig Schmidt


## What, Why and How

[1]: https://arxiv.org/pdf/2208.05516v4 "Quality Not Quantity: On the Interaction between Dataset Design and ..."
[2]: https://arxiv.org/abs/2208.05516 "Quality Not Quantity: On the Interaction between Dataset Design and ..."
[3]: http://export.arxiv.org/abs/2208.05516v4 "[2208.05516v4] Quality Not Quantity: On the Interaction between Dataset ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper investigates how different web-crawled data sources affect the robustness of CLIP, an image-text model that can perform zero-shot inference on various tasks.
- **Why**: The paper aims to understand the dataset design process for image-text models, which is often obscure and undocumented, and to provide insights for building better pre-training datasets for robust generalization.
- **How**: The paper introduces a testbed of six publicly available data sources (YFCC, LAION, Conceptual Captions, WIT, RedCaps, Shutterstock) and compares their performance on CLIP across different distribution shifts. The paper also proposes a theoretical model to explain some of the empirical findings and the success of a CLIP-based data filtering technique.

## Main Contributions

[1]: https://arxiv.org/pdf/2208.05516v4 "Quality Not Quantity: On the Interaction between Dataset Design and ..."
[2]: https://arxiv.org/abs/2208.05516 "Quality Not Quantity: On the Interaction between Dataset Design and ..."
[3]: http://export.arxiv.org/abs/2208.05516v4 "[2208.05516v4] Quality Not Quantity: On the Interaction between Dataset ..."

According to the paper[^1^][1], the main contributions are:

- Introducing a testbed of six publicly available web-crawled data sources for image-text pre-training and evaluating their robustness on CLIP across different distribution shifts.
- Systematically studying the interactions between these data sources and finding that combining multiple sources does not necessarily yield better models, but rather dilutes the robustness of the best individual data source.
- Providing theoretical insights from a simple setting, where combining the training data also results in diluted robustness, and explaining the success of a CLIP-based data filtering technique recently employed in the LAION dataset.

## Method Summary

[1]: https://arxiv.org/pdf/2208.05516v4 "Quality Not Quantity: On the Interaction between Dataset Design and ..."
[2]: https://arxiv.org/abs/2208.05516 "Quality Not Quantity: On the Interaction between Dataset Design and ..."
[3]: http://export.arxiv.org/abs/2208.05516v4 "[2208.05516v4] Quality Not Quantity: On the Interaction between Dataset ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses CLIP  as the base model and fine-tunes it on six different web-crawled data sources: YFCC , LAION [^2^][3], Conceptual Captions , WIT , RedCaps , and Shutterstock [^3^][2]. These data sources vary in size, quality, diversity, and domain.
- The paper evaluates the fine-tuned models on four different distribution shifts: natural distribution shift (ImageNet-V2 ), synthetic distribution shift (ImageNet-C ), task distribution shift (ImageNet-R ), and domain distribution shift (COCO ).
- The paper measures the performance of the models using top-1 accuracy for natural and synthetic shifts, and mean average precision (mAP) for task and domain shifts. The paper also reports the zero-shot performance of CLIP on these shifts using the original pre-training data.
- The paper analyzes the interactions between the data sources by fine-tuning CLIP on various combinations of them and comparing their performance with the individual data sources. The paper also studies the effect of data size and data filtering on the robustness of CLIP.
- The paper proposes a theoretical model to explain some of the empirical findings, such as why combining data sources can dilute robustness and why CLIP-based data filtering can improve robustness. The theoretical model considers a simple setting where the data sources are generated from Gaussian mixtures with different means and variances. The paper derives upper bounds on the expected risk of CLIP under different distribution shifts and shows how they depend on the parameters of the data sources.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define CLIP as the base model
clip = CLIP()

# Define the data sources
data_sources = [YFCC, LAION, Conceptual_Captions, WIT, RedCaps, Shutterstock]

# Define the distribution shifts
distribution_shifts = [ImageNet_V2, ImageNet_C, ImageNet_R, COCO]

# Define the performance metrics
metrics = [top_1_accuracy, mAP]

# Fine-tune CLIP on each data source and evaluate on each distribution shift
for data_source in data_sources:
  clip_fine_tuned = fine_tune(clip, data_source)
  for distribution_shift in distribution_shifts:
    performance = evaluate(clip_fine_tuned, distribution_shift, metrics)
    report(performance)

# Fine-tune CLIP on combinations of data sources and evaluate on each distribution shift
for combination in combinations(data_sources):
  clip_fine_tuned = fine_tune(clip, combination)
  for distribution_shift in distribution_shifts:
    performance = evaluate(clip_fine_tuned, distribution_shift, metrics)
    report(performance)

# Compare the performance of fine-tuned models with zero-shot CLIP
for distribution_shift in distribution_shifts:
  performance = evaluate(clip, distribution_shift, metrics)
  report(performance)

# Analyze the effect of data size and data filtering on the robustness of CLIP
for data_source in data_sources:
  clip_fine_tuned = fine_tune(clip, data_source)
  clip_filtered = filter_data(clip_fine_tuned, data_source)
  clip_fine_tuned_filtered = fine_tune(clip_filtered, data_source)
  for distribution_shift in distribution_shifts:
    performance = evaluate(clip_fine_tuned_filtered, distribution_shift, metrics)
    report(performance)

# Propose a theoretical model to explain the empirical findings
model = Gaussian_Mixture_Model(data_sources)
derive_upper_bounds(model, clip, distribution_shifts)
explain_robustness(model, clip)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np
import scipy.stats

# Define CLIP as the base model
clip_model, clip_preprocess = clip.load("ViT-B/32", device="cuda")

# Define the data sources
data_sources = ["YFCC", "LAION", "Conceptual_Captions", "WIT", "RedCaps", "Shutterstock"]

# Define the distribution shifts
distribution_shifts = ["ImageNet_V2", "ImageNet_C", "ImageNet_R", "COCO"]

# Define the performance metrics
def top_1_accuracy(logits, labels):
  # Compute the top-1 accuracy given logits and labels
  predictions = torch.argmax(logits, dim=-1)
  correct = torch.eq(predictions, labels).sum().item()
  total = labels.size(0)
  return correct / total

def mAP(logits, labels):
  # Compute the mean average precision given logits and labels
  scores = torch.softmax(logits, dim=-1)
  aps = []
  for i in range(logits.size(-1)):
    ap = average_precision_score(labels[:, i], scores[:, i])
    aps.append(ap)
  return np.mean(aps)

# Fine-tune CLIP on each data source and evaluate on each distribution shift
for data_source in data_sources:
  # Load the data source as a PyTorch dataset
  data_source_dataset = load_dataset(data_source)

  # Define the optimizer and the loss function for fine-tuning
  optimizer = torch.optim.Adam(clip_model.parameters(), lr=3e-4)
  loss_fn = torch.nn.CrossEntropyLoss()

  # Fine-tune CLIP for a fixed number of epochs
  epochs = 10
  for epoch in range(epochs):
    for images, texts, labels in data_source_dataset:
      # Preprocess the images and texts
      images = clip_preprocess(images).to("cuda")
      texts = clip.tokenize(texts).to("cuda")

      # Forward pass through CLIP
      image_features, text_features = clip_model(images, texts)

      # Compute the logits and the loss
      logits_per_image = image_features @ text_features.t()
      logits_per_text = logits_per_image.t()
      loss = loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)

      # Backward pass and update the parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

  # Save the fine-tuned model
  clip_fine_tuned = clip_model

  for distribution_shift in distribution_shifts:
    # Load the distribution shift as a PyTorch dataset
    distribution_shift_dataset = load_dataset(distribution_shift)

    # Evaluate the fine-tuned model on the distribution shift
    performance = []
    for images, texts, labels in distribution_shift_dataset:
      # Preprocess the images and texts
      images = clip_preprocess(images).to("cuda")
      texts = clip.tokenize(texts).to("cuda")

      # Forward pass through CLIP
      image_features, text_features = clip_fine_tuned(images, texts)

      # Compute the logits and the metrics
      logits_per_image = image_features @ text_features.t()
      accuracy = top_1_accuracy(logits_per_image, labels)
      map_ = mAP(logits_per_image, labels)
      performance.append((accuracy, map_))

    # Report the average performance across the distribution shift
    performance = np.mean(performance, axis=0)
    report(performance)

# Fine-tune CLIP on combinations of data sources and evaluate on each distribution shift
for combination in combinations(data_sources):
  # Concatenate the data sources as a PyTorch dataset
  combination_dataset = concatenate_datasets(combination)

  # Define the optimizer and the loss function for fine-tuning
  optimizer = torch.optim.Adam(clip_model.parameters(), lr=3e-4)
  loss_fn = torch.nn.CrossEntropyLoss()

  # Fine-tune CLIP for a fixed number of epochs
  epochs = 10
  for epoch in range(epochs):
    for images, texts, labels in combination_dataset:
      # Preprocess the images and texts
      images = clip_preprocess(images).to("cuda")
      texts = clip.tokenize(texts).to("cuda")

      # Forward pass through CLIP
      image_features, text_features = clip_model(images, texts)

      # Compute the logits and the loss
      logits_per_image = image_features @ text_features.t()
      logits_per_text = logits_per_image.t()
      loss = loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)

      # Backward pass and update the parameters
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

  # Save the fine-tuned model
  clip_fine_tuned = clip_model

  for distribution_shift in distribution_shifts:
    # Load the distribution shift as a PyTorch dataset
    distribution_shift_dataset = load_dataset(distribution_shift)

    # Evaluate the fine-tuned model on the distribution shift
    performance = []
    for images, texts, labels in distribution_shift_dataset:
      # Preprocess the images and texts
      images = clip_preprocess(images).to("cuda")
      texts = clip.tokenize(texts).to("cuda")

      # Forward pass through CLIP
      image_features, text_features = clip_fine_tuned(images, texts)

      # Compute the logits and the metrics
      logits_per_image = image_features @ text_features.t()
      accuracy = top_1_accuracy(logits_per_image, labels)
      map_ = mAP(logits_per_image, labels)
      performance.append((accuracy, map_))

    # Report the average performance across the distribution shift
    performance = np.mean(performance, axis=0)
    report(performance)

# Compare the performance of fine-tuned models with zero-shot CLIP
for distribution_shift in distribution_shifts:
  # Load the distribution shift as a PyTorch dataset
  distribution_shift_dataset = load_dataset(distribution_shift)

  # Evaluate the zero-shot CLIP on the distribution shift
  performance = []
  for images, texts, labels in distribution_shift_dataset:
    # Preprocess the images and texts
    images = clip_preprocess(images).to("cuda")
    texts = clip.tokenize(texts).to("cuda")

    # Forward pass through CLIP
    image_features, text_features = clip_model(images, texts)

    # Compute the logits and the metrics
    logits_per_image = image_features @ text_features.t()
    accuracy = top_1_accuracy(logits_per_image, labels)
    map_ = mAP(logits_per_image, labels)
    performance.append((accuracy, map_))

  # Report the average performance across the distribution shift
  performance = np.mean(performance, axis=0)
  report(performance)

# Analyze the effect of data size and data filtering on the robustness of CLIP
for data_source in data_sources:
  # Load the data source as a PyTorch dataset
  data_source_dataset = load_dataset(data_source)

  # Fine-tune CLIP on the data source
  optimizer = torch.optim.Adam(clip_model.parameters(), lr=3e-4)
  loss_fn = torch.nn.CrossEntropyLoss()
  epochs = 10
  for epoch in range(epochs):
    for images, texts, labels in data_source_dataset:
      images = clip_preprocess(images).to("cuda")
      texts = clip.tokenize(texts).to("cuda")
      image_features, text_features = clip_model(images, texts)
      logits_per_image = image_features @ text_features.t()
      logits_per_text = logits_per_image.t()
      loss = loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

  # Save the fine-tuned model
  clip_fine_tuned = clip_model

  # Filter the data source using CLIP as a classifier
  clip_filtered_dataset = []
  for images, texts, labels in data_source_dataset:
    images = clip_preprocess(images).to("cuda")
    texts = clip.tokenize(texts).to("cuda")
    image_features, text_features = clip_fine_tuned(images, texts)
    logits_per_image = image_features @ text_features.t()
    predictions = torch.argmax(logits_per_image, dim=-1)
    correct_indices = torch.eq(predictions, labels).nonzero(as_tuple=True)[0]
    filtered_images = images[correct_indices]
    filtered_texts = texts[correct_indices]
    filtered_labels = labels[correct_indices]
    clip_filtered_dataset.append((filtered_images, filtered_texts, filtered_labels))

  # Fine-tune CLIP on the filtered data source
  optimizer = torch.optim.Adam(clip_model.parameters(), lr=3e-4)
  loss_fn = torch.nn.CrossEntropyLoss()
  epochs = 10
  for epoch in range(epochs):
    for images, texts, labels in clip_filtered_dataset:
      images = clip_preprocess(images).to("cuda")
      texts = clip.tokenize(texts).to("cuda")
      image_features, text_features = clip_model(images, texts)
      logits_per_image = image_features @ text_features.t()
      logits_per_text = logits_per_image.t()
      loss = loss_fn(logits_per_image, labels) + loss_fn(logits_per_text, labels)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

  # Save the fine-tuned model on