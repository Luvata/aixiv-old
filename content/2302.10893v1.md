---
title: 2302.10893v1 Fair Diffusion  Instructing Text-to-Image Generation Models on Fairness
date: 2023-02-11
---

# [Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness](http://arxiv.org/abs/2302.10893v1)

authors: Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni, Kristian Kersting


## What, Why and How

[1]: https://arxiv.org/pdf/2302.10893v1 "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness"
[2]: https://arxiv.org/abs/2302.10893v1 "[2302.10893v1] Fair Diffusion: Instructing Text-to-Image Generation ..."
[3]: http://export.arxiv.org/abs/1912.10893v1 "[1912.10893v1] Self-consistent range-separated density-functional ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a novel strategy called **Fair Diffusion** to attenuate biases in generative text-to-image models after deployment.
- **Why**: The paper argues that generative text-to-image models are highly data-driven and may suffer from degenerated and biased human behavior, such as producing images that lack diversity or fairness for certain identity groups.
- **How**: The paper demonstrates shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. The paper uses **Diffusion Models** as the base generative model and introduces a **fairness loss** that penalizes the deviation from the desired proportions. The paper evaluates the proposed method on several datasets and tasks and shows that it can produce fairer and more diverse images without sacrificing quality or requiring additional training.

## Main Contributions

[1]: https://arxiv.org/pdf/2302.10893v1 "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness"
[2]: https://arxiv.org/abs/2302.10893v1 "[2302.10893v1] Fair Diffusion: Instructing Text-to-Image Generation ..."
[3]: http://export.arxiv.org/abs/1912.10893v1 "[1912.10893v1] Self-consistent range-separated density-functional ..."

According to the paper[^1^][1], the main contributions are:

- The paper introduces **Fair Diffusion**, a novel strategy to instruct generative text-to-image models on fairness after deployment, without requiring additional training or data filtering.
- The paper proposes a **fairness loss** that penalizes the deviation from the desired proportions of identity groups in the generated images, based on human instructions.
- The paper demonstrates the effectiveness of **Fair Diffusion** on several datasets and tasks, such as generating fairer and more diverse images of occupations, animals, and faces.
- The paper provides a comprehensive analysis of the impact of **Fair Diffusion** on the quality, diversity, and fairness of the generated images, using both quantitative and qualitative measures.

## Method Summary

[1]: https://arxiv.org/pdf/2302.10893v1 "Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness"
[2]: https://arxiv.org/abs/2302.10893v1 "[2302.10893v1] Fair Diffusion: Instructing Text-to-Image Generation ..."
[3]: http://export.arxiv.org/abs/1912.10893v1 "[1912.10893v1] Self-consistent range-separated density-functional ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses **Diffusion Models** (DM) as the base generative model for text-to-image synthesis. DM are a class of generative models that learn to transform a noise distribution into a data distribution by applying a series of stochastic diffusion steps.
- The paper introduces **Fair Diffusion** (FD), a novel strategy to instruct DM on fairness after deployment. FD allows the user to specify the desired proportions of identity groups in the generated images, such as gender, race, or species, using natural language instructions.
- The paper proposes a **fairness loss** that penalizes the deviation from the desired proportions in the generated images. The fairness loss is computed by applying a classifier to the generated images and comparing the predicted labels with the target labels derived from the user instructions.
- The paper optimizes the fairness loss using gradient descent on the latent variables of DM, while keeping the model parameters fixed. This allows FD to adjust the latent variables to produce fairer and more diverse images without requiring additional training or data filtering.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the base diffusion model
DM = DiffusionModel()

# Define the fairness classifier
FC = FairnessClassifier()

# Define the user instructions
UI = "Generate images of firefighters with equal proportions of male and female"

# Parse the user instructions to get the target labels
TL = parse_user_instructions(UI)

# Sample latent variables from the noise distribution
Z = sample_noise()

# Generate images from the latent variables using DM
X = DM.generate(Z)

# Predict the labels of the generated images using FC
PL = FC.predict(X)

# Compute the fairness loss by comparing PL and TL
FL = compute_fairness_loss(PL, TL)

# Optimize the fairness loss using gradient descent on Z
Z = gradient_descent(FL, Z)

# Generate fairer and more diverse images from the updated Z using DM
X = DM.generate(Z)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import nltk

# Define the hyperparameters
batch_size = 64 # number of images to generate per batch
num_steps = 1000 # number of diffusion steps
num_classes = 2 # number of identity groups
lr = 0.01 # learning rate for gradient descent
beta1 = 0.9 # beta1 parameter for Adam optimizer
beta2 = 0.999 # beta2 parameter for Adam optimizer

# Define the base diffusion model
DM = DiffusionModel() # use any existing implementation of diffusion models

# Define the fairness classifier
FC = FairnessClassifier() # use any existing implementation of image classifiers

# Load the pretrained weights for DM and FC
DM.load_state_dict(torch.load("dm.pth"))
FC.load_state_dict(torch.load("fc.pth"))

# Define the user instructions
UI = "Generate images of firefighters with equal proportions of male and female"

# Parse the user instructions to get the target labels
TL = np.zeros((batch_size, num_classes)) # initialize the target labels as a zero matrix
tokens = nltk.word_tokenize(UI) # tokenize the user instructions
for i, token in enumerate(tokens): # loop over the tokens
  if token == "equal": # if the token is "equal"
    TL[:, i+1] = 0.5 # set the target label for the next identity group to 0.5
TL = torch.tensor(TL) # convert the target labels to a torch tensor

# Sample latent variables from the noise distribution
Z = torch.randn(batch_size, DM.latent_dim) # sample a batch of random vectors

# Generate images from the latent variables using DM
X = DM.generate(Z) # apply the reverse diffusion process to get the images

# Predict the labels of the generated images using FC
PL = FC.predict(X) # apply the classifier to get the predicted labels

# Compute the fairness loss by comparing PL and TL
FL = torch.nn.BCELoss()(PL, TL) # use binary cross entropy loss as the fairness loss

# Optimize the fairness loss using gradient descent on Z
optimizer = torch.optim.Adam([Z], lr=lr, betas=(beta1, beta2)) # use Adam optimizer for Z
optimizer.zero_grad() # reset the gradients
FL.backward() # compute the gradients of FL with respect to Z
optimizer.step() # update Z using the gradients

# Generate fairer and more diverse images from the updated Z using DM
X = DM.generate(Z) # apply the reverse diffusion process to get the images

# Display or save the generated images
torchvision.utils.save_image(X, "fair_images.png") # save the images as a png file
```