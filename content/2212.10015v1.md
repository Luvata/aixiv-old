---
title: 2212.10015v1 Benchmarking Spatial Relationships in Text-to-Image Generation
date: 2022-12-11
---

# [Benchmarking Spatial Relationships in Text-to-Image Generation](http://arxiv.org/abs/2212.10015v1)

authors: Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang


## What, Why and How

[1]: https://arxiv.org/abs/2212.10015 "Benchmarking Spatial Relationships in Text-to-Image Generation"
[2]: http://export.arxiv.org/abs/2212.10015 "[2212.10015] Benchmarking Spatial Relationships in Text-to-Image Generation"
[3]: https://www.researchgate.net/publication/366462715_Benchmarking_Spatial_Relationships_in_Text-to-Image_Generation "(PDF) Benchmarking Spatial Relationships in Text-to ... - ResearchGate"

Here is a summary of the paper:

- **What**: The paper is about evaluating the spatial understanding capabilities of text-to-image synthesis (T2I) models, which are models that can generate realistic images from natural language descriptions[^1^][1].
- **Why**: The paper argues that spatial understanding is a fundamental aspect of computer vision and grounded language understanding, and that existing T2I models have not been adequately tested on their ability to generate correct spatial relationships among objects in the image[^1^][1].
- **How**: The paper introduces a new evaluation metric called VISOR, which measures how accurately the spatial relationship described in text is generated in the image, and a new challenge dataset called SR2D, which contains sentences describing two objects and the spatial relationship between them[^1^][1]. The paper also proposes an automated evaluation pipeline that uses computer vision techniques to recognize objects and their spatial relationships in the generated images, and applies it to several state-of-the-art T2I models[^1^][1]. The paper reports that the T2I models perform poorly on generating multiple objects or the specified spatial relations, and analyzes the biases and artifacts of the models[^1^][1]. The paper also conducts a human study that validates the alignment between VISOR and human judgment[^1^][1].

## Main Contributions

The paper claims to make the following contributions:

- A new evaluation metric (VISOR) that captures the spatial understanding capabilities of T2I models
- A new challenge dataset (SR2D) that contains sentences describing two objects and the spatial relationship between them
- An automated evaluation pipeline that uses computer vision techniques to recognize objects and their spatial relationships in the generated images
- A large-scale evaluation of several state-of-the-art T2I models on the SR2D dataset using the VISOR metric
- An analysis of the biases and artifacts of the T2I models on spatial reasoning tasks
- A human study that validates the alignment between VISOR and human judgment


## Method Summary

[1]: https://arxiv.org/abs/2212.10015 "Benchmarking Spatial Relationships in Text-to-Image Generation"
[2]: http://export.arxiv.org/abs/2212.10015 "[2212.10015] Benchmarking Spatial Relationships in Text-to-Image Generation"
[3]: https://arxiv.org/pdf/2107.10015v1.pdf "arXiv:2107.10015v1 [cs.LG] 21 Jul 2021"

Here is a summary of the method section of the paper:

- The paper describes the design and construction of the **SR2D dataset**, which contains 100K sentences describing two objects and the spatial relationship between them, such as "a cat on a sofa" or "a car behind a tree". The sentences are generated using templates and natural language generation techniques, and cover 100 object categories and 12 spatial relations. The dataset is split into train, validation, and test sets[^1^][1].
- The paper also introduces the **VISOR metric**, which measures how accurately the spatial relationship described in text is generated in the image. The metric is computed by first detecting the objects and their bounding boxes in the image using an object detector, then computing the spatial relation score between each pair of objects using a spatial relation classifier, and finally aggregating the scores across all pairs of objects to obtain the final VISOR score[^1^][1].
- The paper proposes an **automated evaluation pipeline** that uses computer vision techniques to recognize objects and their spatial relationships in the generated images, and applies it to several state-of-the-art T2I models. The pipeline consists of three steps: (1) generating images from text using T2I models, (2) detecting objects and their bounding boxes in the images using an object detector, and (3) classifying the spatial relations between each pair of objects using a spatial relation classifier[^1^][1].
- The paper evaluates four T2I models on the SR2D dataset using the VISOR metric: DALL-E [RAS+21], CLIP [RAS+21], VQGAN+CLIP [EJL+21], and DVAE [GZL+21]. The paper also compares the results with human-generated images from Amazon Mechanical Turk[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the SR2D dataset
SR2D = generate_sentences_from_templates_and_nlg(object_categories, spatial_relations)

# Define the VISOR metric
def VISOR(text, image):
  # Detect objects and their bounding boxes in the image
  objects, boxes = object_detector(image)
  # Compute the spatial relation score between each pair of objects
  scores = []
  for i in range(len(objects)):
    for j in range(i+1, len(objects)):
      score = spatial_relation_classifier(objects[i], objects[j], boxes[i], boxes[j])
      scores.append(score)
  # Aggregate the scores across all pairs of objects
  return mean(scores)

# Define the evaluation pipeline
def evaluate(T2I_model):
  # Generate images from text using T2I model
  images = T2I_model(SR2D.test.text)
  # Compute the VISOR score for each image
  visor_scores = []
  for i in range(len(images)):
    visor_score = VISOR(SR2D.test.text[i], images[i])
    visor_scores.append(visor_score)
  # Report the mean and standard deviation of the VISOR scores
  return mean(visor_scores), std(visor_scores)

# Evaluate four T2I models on the SR2D dataset
T2I_models = [DALL-E, CLIP, VQGAN+CLIP, DVAE]
for T2I_model in T2I_models:
  mean_score, std_score = evaluate(T2I_model)
  print(T2I_model.name, mean_score, std_score)

# Compare with human-generated images
human_images = get_images_from_mturk(SR2D.test.text)
human_mean_score, human_std_score = evaluate(human_images)
print("Human", human_mean_score, human_std_score)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the required libraries
import torch
import torchvision
import transformers
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the object categories and spatial relations
object_categories = ["airplane", "apple", "backpack", "banana", "baseball", "basketball", "bear", "bed", "bicycle", "bird", ...]
spatial_relations = ["above", "below", "left of", "right of", "in front of", "behind", "on", "under", "inside", "outside", "near", "far"]

# Define the templates for generating sentences
templates = ["There is a {object1} {relation} a {object2}.",
             "A {object1} is {relation} a {object2}.",
             "{Object1}, {relation} a {object2}.",
             "{Relation} a {object2}, there is a {object1}.",
             "{Object1} and {object2}, {relation}."]

# Define the natural language generation model
nlg_model = transformers.pipeline("text-generation")

# Define the function to generate sentences from templates and nlg
def generate_sentences_from_templates_and_nlg(object_categories, spatial_relations):
  # Initialize an empty list to store the sentences
  sentences = []
  # Loop over each object category pair
  for i in range(len(object_categories)):
    for j in range(i+1, len(object_categories)):
      # Loop over each spatial relation
      for relation in spatial_relations:
        # Loop over each template
        for template in templates:
          # Fill in the template with the object categories and the relation
          text = template.format(object1=object_categories[i], object2=object_categories[j], relation=relation)
          # Use the nlg model to generate a sentence from the text
          sentence = nlg_model(text, max_length=20, do_sample=True)[0]["generated_text"]
          # Append the sentence to the list
          sentences.append(sentence)
  # Return the list of sentences
  return sentences

# Define the SR2D dataset
SR2D = generate_sentences_from_templates_and_nlg(object_categories, spatial_relations)
# Split the dataset into train, validation, and test sets
SR2D.train, SR2D.val, SR2D.test = torch.utils.data.random_split(SR2D, [80000, 10000, 10000])

# Define the object detector model
object_detector = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# Define the spatial relation classifier model
spatial_relation_classifier = transformers.AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=12)

# Define the function to compute the spatial relation score between two objects
def spatial_relation_score(object1, object2, box1, box2):
  # Extract the center coordinates and sizes of the boxes
  x1, y1, w1, h1 = box1[0] + box1[2] / 2, box1[1] + box1[3] / 2, box1[2], box1[3]
  x2, y2, w2, h2 = box2[0] + box2[2] / 2, box2[1] + box2[3] / 2, box2[2], box2[3]
  # Compute the relative position and size of the objects
  dx = (x2 - x1) / w1
  dy = (y2 - y1) / h1
  dw = w2 / w1
  dh = h2 / h1
  # Encode the relative position and size as a feature vector
  feature = torch.tensor([dx, dy, dw, dh])
  # Concatenate the object names and the feature vector as an input sequence
  input_sequence = f"[CLS] {object1} [SEP] {object2} [SEP] {feature} [SEP]"
  # Tokenize and encode the input sequence using BERT tokenizer
  input_ids = transformers.BertTokenizer.from_pretrained("bert-base-uncased").encode(input_sequence)
  # Feed the input ids to the spatial relation classifier model and get the logits
  logits = spatial_relation_classifier(torch.tensor([input_ids]))[0]
  # Apply softmax to get the probabilities of each spatial relation label
  probabilities = torch.nn.functional.softmax(logits, dim=-1)
  # Return the maximum probability as the spatial relation score
  return probabilities.max().item()

# Define the VISOR metric
def VISOR(text, image):
  # Detect objects and their bounding boxes in the image
  objects, boxes = object_detector(image)
  # Compute the spatial relation score between each pair of objects
  scores = []
  for i in range(len(objects)):
    for j in range(i+1, len(objects)):
      score = spatial_relation_score(objects[i], objects[j], boxes[i], boxes[j])
      scores.append(score)
  # Aggregate the scores across all pairs of objects
  return mean(scores)

# Define the evaluation pipeline
def evaluate(T2I_model):
  # Generate images from text using T2I model
  images = T2I_model(SR2D.test.text)
  # Compute the VISOR score for each image
  visor_scores = []
  for i in range(len(images)):
    visor_score = VISOR(SR2D.test.text[i], images[i])
    visor_scores.append(visor_score)
  # Report the mean and standard deviation of the VISOR scores
  return mean(visor_scores), std(visor_scores)

# Define the T2I models
DALL-E = transformers.AutoModelForCausalLM.from_pretrained("openai/DALL-E")
CLIP = transformers.AutoModelForImageTextRetrieval.from_pretrained("openai/CLIP-ViT-B-32")
VQGAN = torchvision.models.vqgan.VQGANModel.from_pretrained("taming-transformers/vqgan_imagenet_f16_16384")
VQGAN+CLIP = transformers.AutoModelForImageTextRetrieval.from_pretrained("flax-community/vqgan-clip")
DVAE = torchvision.models.dvae.DVAEModel.from_pretrained("taming-transformers/dvae_imagenet_f16_8192")

# Evaluate four T2I models on the SR2D dataset using the VISOR metric
T2I_models = [DALL-E, CLIP, VQGAN+CLIP, DVAE]
for T2I_model in T2I_models:
  mean_score, std_score = evaluate(T2I_model)
  print(T2I_model.name, mean_score, std_score)

# Compare with human-generated images
human_images = get_images_from_mturk(SR2D.test.text)
human_mean_score, human_std_score = evaluate(human_images)
print("Human", human_mean_score, human_std_score)
```