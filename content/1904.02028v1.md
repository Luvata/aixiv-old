---
title: 1904.02028v1 CAM-Convs  Camera-Aware Multi-Scale Convolutions for Single-View Depth
date: 2019-04-03
---

# [CAM-Convs: Camera-Aware Multi-Scale Convolutions for Single-View Depth](http://arxiv.org/abs/1904.02028v1)

authors: Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas Brox, Javier Civera


## What, Why and How

[1]: https://arxiv.org/abs/1904.02028v1 "[1904.02028v1] CAM-Convs: Camera-Aware Multi-Scale Convolutions for ..."
[2]: https://arxiv.org/pdf/1904.02028v1.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export1.library.cornell.edu/abs/1904.02028v1 "[1904.02028v1] CAM-Convs: Camera-Aware Multi-Scale Convolutions for ..."

Here is a summary of the paper:

- **What**: The paper proposes a new type of convolution that can take the camera parameters into account, thus allowing neural networks to learn calibration-aware patterns for single-view depth estimation[^1^][1].
- **Why**: The paper addresses the problem that a network trained on images from one camera does not generalize to images taken with a different camera model, which requires collecting an entirely new training dataset[^1^][1].
- **How**: The paper introduces **CAM-Convs**, which are camera-aware multi-scale convolutions that use the camera intrinsics and extrinsics to warp the input features to a canonical view before applying standard convolutions[^1^][1]. The paper also proposes a novel loss function that incorporates the camera parameters and enforces geometric consistency between predicted depth maps and ground truth depth maps[^1^][1]. The paper evaluates the proposed method on several datasets and shows that it outperforms the state of the art when the train and test images are acquired with different cameras[^1^][1].

## Main Contributions

The main contributions of this paper are:

- A new type of convolution that can take the camera parameters into account, thus allowing neural networks to learn calibration-aware patterns for single-view depth estimation.
- A novel loss function that incorporates the camera parameters and enforces geometric consistency between predicted depth maps and ground truth depth maps.
- An extensive evaluation of the proposed method on several datasets and a comparison with the state of the art methods.

## Method Summary

[1]: https://arxiv.org/abs/1904.02028v1 "[1904.02028v1] CAM-Convs: Camera-Aware Multi-Scale Convolutions for ..."
[2]: https://arxiv.org/pdf/1904.02028v1.pdf "arXiv.org e-Print archive"
[3]: https://arxiv-export1.library.cornell.edu/abs/1904.02028v1 "[1904.02028v1] CAM-Convs: Camera-Aware Multi-Scale Convolutions for ..."

Here is a summary of the method section of the paper:

- The paper proposes a new type of convolution called **CAM-Convs**, which are camera-aware multi-scale convolutions that use the camera intrinsics and extrinsics to warp the input features to a canonical view before applying standard convolutions[^1^][1].
- The paper also proposes a novel loss function called **CAM-Loss**, which incorporates the camera parameters and enforces geometric consistency between predicted depth maps and ground truth depth maps[^1^][1].
- The paper describes the network architecture and the implementation details of the proposed method, such as the input resolution, the number of scales, the feature extraction backbone, and the optimization strategy[^1^][1].
- The paper also provides some ablation studies and qualitative results to analyze the effect of different components of the proposed method[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an RGB image I and its camera parameters K and R
# Output: a depth map D

# Define the network architecture with a feature extraction backbone and a depth prediction head
network = Network(backbone, head)

# Define the CAM-Convs with different kernel sizes and dilation rates
cam_convs = [CAM_Conv(k, d) for k, d in zip(kernel_sizes, dilation_rates)]

# Define the CAM-Loss with a scale-invariant term and a geometric consistency term
cam_loss = CAM_Loss()

# Forward pass: extract features from the input image and warp them to a canonical view using the camera parameters
features = network.backbone(I)
warped_features = [cam_conv.warp(features, K, R) for cam_conv in cam_convs]

# Forward pass: apply standard convolutions on the warped features and fuse them to predict the depth map
depth_features = [cam_conv.conv(warped_features[i]) for i, cam_conv in enumerate(cam_convs)]
fused_features = fuse(depth_features)
D = network.head(fused_features)

# Backward pass: compute the CAM-Loss between the predicted depth map and the ground truth depth map
loss = cam_loss(D, D_gt, K, R)

# Backward pass: update the network parameters using gradient descent
network.update(loss)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: an RGB image I and its camera parameters K and R
# Output: a depth map D

# Define the network architecture with a feature extraction backbone and a depth prediction head
# The backbone can be any CNN model such as ResNet or DenseNet
# The head can be a series of deconvolution layers that upsample the features to the input resolution
network = Network(backbone, head)

# Define the CAM-Convs with different kernel sizes and dilation rates
# Each CAM-Conv consists of two functions: warp and conv
# The warp function takes the input features and the camera parameters and warps them to a canonical view
# The conv function takes the warped features and applies a standard convolution with a given kernel size and dilation rate
cam_convs = [CAM_Conv(k, d) for k, d in zip(kernel_sizes, dilation_rates)]

# Define the CAM-Loss with a scale-invariant term and a geometric consistency term
# The scale-invariant term measures the difference between the predicted and ground truth depth maps in log space
# The geometric consistency term measures the difference between the predicted depth maps and the ground truth depth maps after projecting them to 3D space using the camera parameters
cam_loss = CAM_Loss()

# Forward pass: extract features from the input image and warp them to a canonical view using the camera parameters
features = network.backbone(I) # features is a tensor of shape [batch_size, channels, height, width]
warped_features = [] # warped_features is a list of tensors of shape [batch_size, channels, height, width]
for cam_conv in cam_convs:
  # Compute the pixel coordinates in the canonical view for each pixel in the input view
  # This can be done by multiplying the inverse of the camera intrinsics matrix K with the pixel coordinates in the input view
  # Then multiplying the camera extrinsics matrix R with the resulting coordinates
  # Then dividing by the last coordinate to get homogeneous coordinates
  pixel_coords = get_pixel_coords(features) # pixel_coords is a tensor of shape [batch_size, 3, height * width]
  canonical_coords = K.inverse() @ pixel_coords # canonical_coords is a tensor of shape [batch_size, 3, height * width]
  canonical_coords = R @ canonical_coords # canonical_coords is a tensor of shape [batch_size, 3, height * width]
  canonical_coords = canonical_coords / canonical_coords[:, -1:, :] # canonical_coords is a tensor of shape [batch_size, 3, height * width]

  # Warp the input features to the canonical view using bilinear interpolation
  # This can be done by using the canonical coordinates as the sampling grid for the input features
  warped_feature = cam_conv.warp(features, canonical_coords) # warped_feature is a tensor of shape [batch_size, channels, height, width]
  warped_features.append(warped_feature)

# Forward pass: apply standard convolutions on the warped features and fuse them to predict the depth map
depth_features = [] # depth_features is a list of tensors of shape [batch_size, channels, height, width]
for i, cam_conv in enumerate(cam_convs):
  # Apply a standard convolution on the warped features with a given kernel size and dilation rate
  depth_feature = cam_conv.conv(warped_features[i]) # depth_feature is a tensor of shape [batch_size, channels, height, width]
  depth_features.append(depth_feature)

# Fuse the depth features from different scales using element-wise addition or concatenation
fused_features = fuse(depth_features) # fused_features is a tensor of shape [batch_size, channels, height, width]

# Predict the depth map from the fused features using the network head
D = network.head(fused_features) # D is a tensor of shape [batch_size, 1, height, width]

# Backward pass: compute the CAM-Loss between the predicted depth map and the ground truth depth map
loss = cam_loss(D, D_gt, K, R) # loss is a scalar

# Backward pass: update the network parameters using gradient descent
network.update(loss)
```