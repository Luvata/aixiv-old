---
title: 2205.03146v3 CLIP-CLOP  CLIP-Guided Collage and Photomontage
date: 2022-05-04
---

# [CLIP-CLOP: CLIP-Guided Collage and Photomontage](http://arxiv.org/abs/2205.03146v3)

authors: Piotr Mirowski, Dylan Banarse, Mateusz Malinowski, Simon Osindero, Chrisantha Fernando


## What, Why and How

[1]: https://arxiv.org/abs/2205.03146v3 "[2205.03146v3] CLIP-CLOP: CLIP-Guided Collage and Photomontage - arXiv.org"
[2]: https://arxiv.org/pdf/2205.03146v3.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2205.03146v3 "[2205.03146v3] CLIP-CLOP: CLIP-Guided Collage and Photomontage"

Here is a summary of the paper:

- **What**: The paper presents a gradient-based generator called CLIP-CLOP that produces collages and photomontages from image patches and text prompts, using the CLIP dual image-and-text encoder as a guidance.
- **Why**: The paper aims to explore the aesthetic potentials of high-resolution collages, and to provide an artistic tool that allows human artists to reclaim some control of the generation process and achieve greater creative freedom.
- **How**: The paper describes the CLIP-CLOP generator, which consists of three main components: a patch library, a patch placement algorithm, and a patch blending algorithm. The patch library contains image patches that are extracted from source images using a segmentation model. The patch placement algorithm optimizes the positions and scales of the patches to maximize the CLIP score between the collage and the text prompt. The patch blending algorithm smooths the boundaries between the patches using a Poisson blending technique. The paper also demonstrates some examples of collages and photomontages generated by CLIP-CLOP with different prompts and patch libraries.

## Main Contributions

[1]: https://arxiv.org/abs/2205.03146v3 "[2205.03146v3] CLIP-CLOP: CLIP-Guided Collage and Photomontage - arXiv.org"
[2]: https://arxiv.org/pdf/2205.03146v3.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2205.03146v3 "[2205.03146v3] CLIP-CLOP: CLIP-Guided Collage and Photomontage"

The paper claims the following contributions:

- **A novel gradient-based generator for collages and photomontages** that uses the CLIP dual image-and-text encoder as a guidance and allows human artists to curate libraries of image patches and to describe (with prompts) the whole image composition.
- **A patch placement algorithm** that optimizes the positions and scales of the patches to maximize the CLIP score between the collage and the text prompt, with the option to manually adjust the patches' positions during generation.
- **A patch blending algorithm** that smooths the boundaries between the patches using a Poisson blending technique, which preserves the gradients and textures of the patches.
- **An open-source Google Colab** that implements the CLIP-CLOP generator and provides an artistic tool for users to create their own collages and photomontages with different prompts and patch libraries.
- **An exploration of the aesthetic potentials of high-resolution collages** generated by CLIP-CLOP with different prompts and patch libraries, and a comparison with other state-of-the-art generators.

## Method Summary

[1]: https://arxiv.org/abs/2205.03146v3 "[2205.03146v3] CLIP-CLOP: CLIP-Guided Collage and Photomontage - arXiv.org"
[2]: https://arxiv.org/pdf/2205.03146v3.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2205.03146v3 "[2205.03146v3] CLIP-CLOP: CLIP-Guided Collage and Photomontage"

Here is a summary of the method section of the paper:

- The paper describes the CLIP-CLOP generator, which consists of three main components: a patch library, a patch placement algorithm, and a patch blending algorithm.
- The patch library contains image patches that are extracted from source images using a segmentation model based on DeepLabV3+[^1^][1]. The patches are resized to have a fixed height of 256 pixels and stored in a folder. The user can curate the patch library by adding or removing patches manually.
- The patch placement algorithm optimizes the positions and scales of the patches to maximize the CLIP score between the collage and the text prompt. The CLIP score is computed by feeding the collage image and the text prompt to the CLIP dual image-and-text encoder[^2^][2] and taking the dot product of their embeddings. The optimization is done using gradient ascent with Adam optimizer[^3^][3]. The user can also manually adjust the patches' positions and scales during the optimization by dragging and resizing them on the screen.
- The patch blending algorithm smooths the boundaries between the patches using a Poisson blending technique, which preserves the gradients and textures of the patches. The Poisson blending is done by solving a sparse linear system that minimizes the difference between the collage gradients and the patch gradients in the overlapping regions. The paper uses an iterative conjugate gradient solver to solve the linear system efficiently.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a text prompt and a patch library
# Output: a collage image

# Initialize the collage image with a random background
collage = random_background()

# Extract patches from the patch library using a segmentation model
patches = extract_patches(patch_library)

# Initialize the patches' positions and scales randomly
positions = random_positions(patches)
scales = random_scales(patches)

# Optimize the patches' positions and scales using gradient ascent
while not converged:
  # Compute the CLIP score between the collage and the text prompt
  score = clip_score(collage, text_prompt)
  
  # Compute the gradients of the score with respect to the positions and scales
  gradients = compute_gradients(score, positions, scales)
  
  # Update the positions and scales using Adam optimizer
  positions, scales = adam_update(positions, scales, gradients)
  
  # Allow the user to manually adjust the positions and scales by dragging and resizing the patches on the screen
  positions, scales = user_adjust(positions, scales)
  
  # Update the collage image by placing and scaling the patches according to the optimized positions and scales
  collage = update_collage(patches, positions, scales)

# Smooth the boundaries between the patches using Poisson blending
collage = poisson_blending(collage, patches)

# Return the collage image
return collage
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing
import clip # for CLIP model
import cv2 # for Poisson blending
import scipy # for sparse linear solver

# Define some constants
HEIGHT = 256 # the fixed height of the patches
WIDTH = 256 # the fixed width of the collage
LR = 0.1 # the learning rate for Adam optimizer
EPS = 1e-8 # the epsilon value for Adam optimizer
BETA1 = 0.9 # the beta1 value for Adam optimizer
BETA2 = 0.999 # the beta2 value for Adam optimizer
MAX_ITER = 100 # the maximum number of iterations for gradient ascent
TOL = 1e-3 # the tolerance value for convergence check

# Load the CLIP model and tokenizer
model, tokenizer = clip.load("ViT-B/32")

# Load the segmentation model based on DeepLabV3+
segmentation_model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)

# Input: a text prompt and a patch library
# Output: a collage image

# Initialize the collage image with a random background
collage = torch.rand(3, WIDTH, WIDTH)

# Extract patches from the patch library using a segmentation model
def extract_patches(patch_library):
  patches = []
  for image in patch_library:
    # Resize the image to have a fixed height of HEIGHT pixels
    image = torchvision.transforms.Resize(HEIGHT)(image)
    
    # Segment the image using the segmentation model
    output = segmentation_model(image)
    
    # Get the semantic labels of each pixel
    labels = output["out"].argmax(dim=0)
    
    # Find the unique labels in the image
    unique_labels = torch.unique(labels)
    
    # For each unique label, extract the corresponding patch and append it to the patches list
    for label in unique_labels:
      # Create a mask for the label
      mask = (labels == label).float()
      
      # Crop the image and the mask to the bounding box of the label
      x_min, y_min, x_max, y_max = torchvision.ops.box_convert(torchvision.ops.box_iou(mask), "xyxy", "cxcywh")
      patch = torchvision.transforms.functional.crop(image, x_min, y_min, x_max - x_min, y_max - y_min)
      mask = torchvision.transforms.functional.crop(mask, x_min, y_min, x_max - x_min, y_max - y_min)
      
      # Resize the patch and the mask to have a fixed height of HEIGHT pixels
      patch = torchvision.transforms.Resize(HEIGHT)(patch)
      mask = torchvision.transforms.Resize(HEIGHT)(mask)
      
      # Append the patch and the mask to the patches list as a tuple
      patches.append((patch, mask))
  
  return patches

patches = extract_patches(patch_library)

# Initialize the patches' positions and scales randomly
def random_positions(patches):
  positions = []
  for patch in patches:
    # Generate a random x and y coordinate within the collage boundaries
    x = torch.randint(0, WIDTH - HEIGHT)(patch.shape[2])
    y = torch.randint(0, WIDTH - HEIGHT)(patch.shape[1])
    
    # Append the position to the positions list as a tuple
    positions.append((x, y))
  
  return positions

def random_scales(patches):
  scales = []
  for patch in patches:
    # Generate a random scale factor between 0.5 and 2.0
    scale = torch.rand(1) * 1.5 + 0.5
    
    # Append the scale to the scales list as a scalar
    scales.append(scale)
  
  return scales

positions = random_positions(patches)
scales = random_scales(patches)

# Optimize the patches' positions and scales using gradient ascent
def clip_score(collage, text_prompt):
  # Encode the collage image and the text prompt using the CLIP model and tokenizer
  collage_encoding = model.encode_image(collage.unsqueeze(0))
  text_encoding = model.encode_text(tokenizer(text_prompt, return_tensors="pt"))
  
  # Compute the dot product of their embeddings as the CLIP score
  score = torch.dot(collage_encoding[0], text_encoding[0])
  
  return score

def compute_gradients(score, positions, scales):
  # Compute the gradients of the score with respect to the positions and scales using PyTorch autograd
  gradients = torch.autograd.grad(score, [positions, scales], retain_graph=True)
  
  return gradients

def adam_update(positions, scales, gradients):
  # Initialize the Adam optimizer state
  state = {"t": 0, "m": 0, "v": 0}
  
  # Update the optimizer state
  state["t"] += 1
  state["m"] = BETA1 * state["m"] + (1 - BETA1) * gradients
  state["v"] = BETA2 * state["v"] + (1 - BETA2) * gradients ** 2
  m_hat = state["m"] / (1 - BETA1 ** state["t"])
  v_hat = state["v"] / (1 - BETA2 ** state["t"])
  
  # Update the positions and scales using Adam optimizer
  positions = positions + LR * m_hat[0] / (torch.sqrt(v_hat[0]) + EPS)
  scales = scales + LR * m_hat[1] / (torch.sqrt(v_hat[1]) + EPS)
  
  return positions, scales

def user_adjust(positions, scales):
  # Allow the user to manually adjust the positions and scales by dragging and resizing the patches on the screen
  # This can be implemented using a graphical user interface (GUI) library such as PyGame or Tkinter
  # For simplicity, we assume that the user does not make any adjustments and return the positions and scales unchanged
  return positions, scales

def update_collage(patches, positions, scales):
  # Update the collage image by placing and scaling the patches according to the optimized positions and scales
  collage = torch.zeros(3, WIDTH, WIDTH)
  for i in range(len(patches)):
    # Get the patch and the mask from the patches list
    patch, mask = patches[i]
    
    # Get the position and the scale from the positions and scales lists
    x, y = positions[i]
    scale = scales[i]
    
    # Resize the patch and the mask according to the scale factor
    patch = torchvision.transforms.Resize(int(HEIGHT * scale))(patch)
    mask = torchvision.transforms.Resize(int(HEIGHT * scale))(mask)
    
    # Place the patch and the mask on the collage image according to the position coordinates
    collage[:, x:x+patch.shape[2], y:y+patch.shape[1]] = patch * mask + collage[:, x:x+patch.shape[2], y:y+patch.shape[1]] * (1 - mask)
  
  return collage

# Initialize a variable to store the previous score for convergence check
prev_score = -float("inf")

# Run gradient ascent for a maximum number of iterations or until convergence
for iter in range(MAX_ITER):
  # Compute the CLIP score between the collage and the text prompt
  score = clip_score(collage, text_prompt)
  
  # Compute the gradients of the score with respect to the positions and scales
  gradients = compute_gradients(score, positions, scales)
  
  # Update the positions and scales using Adam optimizer
  positions, scales = adam_update(positions, scales, gradients)
  
  # Allow the user to manually adjust the positions and scales by dragging and resizing the patches on the screen
  positions, scales = user_adjust(positions, scales)
  
  # Update the collage image by placing and scaling the patches according to the optimized positions and scales
  collage = update_collage(patches, positions, scales)
  
  # Print the current iteration and score
  print(f"Iteration {iter}: Score {score}")
  
  # Check if the score has improved by more than a tolerance value since the previous iteration
  if score - prev_score > TOL:
    # Update the previous score with the current score
    prev_score = score
  else:
    # Break out of the loop as convergence is reached
    break

# Smooth the boundaries between the patches using Poisson blending
def poisson_blending(collage, patches):
  # Convert the collage image from PyTorch tensor to OpenCV matrix format
  collage = collage.permute(1,2,0).numpy()
  
  # Initialize a mask image with zeros
  mask = np.zeros((WIDTH, WIDTH), dtype=np.uint8)
  
  # For each patch in the patches list, add its mask to the mask image
  for i in range(len(patches)):
    # Get the patch and the mask from the patches list
    patch, mask = patches[i]
    
    # Convert them from PyTorch tensor to OpenCV matrix format
    patch = patch.permute(1,2,0).numpy()
    mask = mask.numpy()
    
    # Get their position and scale from the positions and scales lists
    x, y = positions[i]
    scale = scales[i]
    
    # Resize them according to their scale factor
    patch = cv2.resize(patch, (int(HEIGHT * scale), int