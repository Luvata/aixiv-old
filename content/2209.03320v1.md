---
title: 2209.03320v1 What does a platypus look like? Generating customized prompts for zero-shot image classification
date: 2022-09-04
---

# [What does a platypus look like? Generating customized prompts for zero-shot image classification](http://arxiv.org/abs/2209.03320v1)

authors: Sarah Pratt, Rosanne Liu, Ali Farhadi


## What, Why and How

[1]: https://arxiv.org/abs/2209.03320 "[2209.03320] What does a platypus look like? Generating customized ..."
[2]: https://arxiv.org/pdf/2209.03320v1.pdf "Generatingcustomizedpromptsforzero-shotimageclassiﬁcation - arXiv.org"
[3]: http://export.arxiv.org/abs/2303.03320v1 "[2303.03320v1] Learning to Backdoor Federated Learning"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a method to generate customized prompts for zero-shot image classification using open vocabulary models and large language models (LLMs).
- **Why**: The paper aims to improve the accuracy of open vocabulary models, which can classify images among any arbitrary set of categories specified with natural language during inference, by generating more descriptive and customized prompts for each category.
- **How**: The paper leverages the knowledge contained in LLMs to generate many descriptive sentences that are customized for each object category, and uses them as prompts for the open vocabulary models. The paper evaluates the method on several zero-shot image classification benchmarks, including ImageNet, and shows that it outperforms the standard method of using hand-written templates.

## Main Contributions

[1]: https://arxiv.org/abs/2209.03320 "[2209.03320] What does a platypus look like? Generating customized ..."
[2]: https://arxiv.org/pdf/2209.03320v1.pdf "Generatingcustomizedpromptsforzero-shotimageclassiﬁcation - arXiv.org"
[3]: http://export.arxiv.org/abs/2303.03320v1 "[2303.03320v1] Learning to Backdoor Federated Learning"

The paper at [^1^][1] lists the following contributions:

- **A simple and general method to generate customized prompts for zero-shot image classification using LLMs and open vocabulary models, called CuPL.**
- **An empirical evaluation of CuPL on several zero-shot image classification benchmarks, showing that it outperforms the standard method of using hand-written templates.**
- **An analysis of the properties and limitations of CuPL, as well as some potential applications and extensions.**

## Method Summary

[1]: https://arxiv.org/abs/2209.03320 "[2209.03320] What does a platypus look like? Generating customized ..."
[2]: https://arxiv.org/pdf/2209.03320v1.pdf "Generatingcustomizedpromptsforzero-shotimageclassiﬁcation - arXiv.org"
[3]: http://export.arxiv.org/abs/2303.03320v1 "[2303.03320v1] Learning to Backdoor Federated Learning"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper describes the standard method of zero-shot open vocabulary image classification, which uses an image encoder and a text encoder to compute the similarity between an image and a prompt. The prompt is a natural language sentence that describes an image category, such as "a photo of a platypus".
- The paper introduces CuPL, which stands for Customized Prompts via Language models. CuPL uses a large language model (LLM) such as GPT-3 to generate multiple descriptive sentences for each image category, such as "a platypus looks like a beaver with a duck's bill". CuPL then uses these sentences as prompts for the open vocabulary model, and selects the best one based on the similarity score.
- The paper explains how CuPL leverages the knowledge and diversity of LLMs to generate customized prompts that are more informative and distinctive than the standard ones. CuPL also avoids generating prompts that are too long, too vague, or too specific, by using a length penalty and a frequency filter.
- The paper details the implementation and evaluation of CuPL on several zero-shot image classification benchmarks, including ImageNet, CIFAR-10, CIFAR-100, and CUB-200. CuPL uses CLIP as the open vocabulary model and GPT-3 as the LLM. CuPL outperforms the standard method of using hand-written templates on all benchmarks, especially on fine-grained categories.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper at :

```python
# Input: a set of image categories C, an open vocabulary model M, a large language model L
# Output: a set of prompts P for each category in C

# Initialize an empty set of prompts P
P = {}

# For each category c in C
for c in C:

  # Generate k sentences S for c using L
  S = L.generate_sentences(c, k)

  # Filter out sentences that are too long, too vague, or too specific
  S = filter_sentences(S)

  # Initialize an empty list of scores Q
  Q = []

  # For each sentence s in S
  for s in S:

    # Compute the similarity score between s and M using the text encoder
    q = M.text_encoder(s)

    # Append q to Q
    Q.append(q)

  # Find the index of the highest score in Q
  i = argmax(Q)

  # Select the corresponding sentence in S as the prompt for c
  p = S[i]

  # Add p to P with the key c
  P[c] = p

# Return P
return P
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper at :

```python
# Input: a set of image categories C, an open vocabulary model M, a large language model L
# Output: a set of prompts P for each category in C

# Import the necessary libraries
import torch
import transformers
import clip

# Load the open vocabulary model M and the large language model L
M = clip.load("ViT-B/32", device="cuda")
L = transformers.AutoModelForCausalLM.from_pretrained("gpt3-large")

# Initialize an empty set of prompts P
P = {}

# Define the length penalty function
def length_penalty(sentence):
  # Return a penalty score based on the length of the sentence
  # The longer the sentence, the higher the penalty
  return len(sentence) * 0.01

# Define the frequency filter function
def frequency_filter(sentence):
  # Return a boolean value indicating whether the sentence contains any common words
  # The common words are defined as the top 1000 most frequent words in English
  common_words = ["the", "of", "and", "to", "a", "in", "for", "is", "on", "that", ...]
  for word in sentence.split():
    if word.lower() in common_words:
      return True
  return False

# Define the filter sentences function
def filter_sentences(sentences):
  # Return a filtered list of sentences that are not too long, too vague, or too specific
  filtered_sentences = []
  for sentence in sentences:
    if len(sentence) < 20 and not frequency_filter(sentence):
      filtered_sentences.append(sentence)
  return filtered_sentences

# For each category c in C
for c in C:

  # Generate k sentences S for c using L
  # Use the prefix "What does a" and the suffix "look like?" to query L
  S = L.generate("What does a " + c + " look like?", k)

  # Filter out sentences that are too long, too vague, or too specific
  S = filter_sentences(S)

  # Initialize an empty list of scores Q
  Q = []

  # For each sentence s in S
  for s in S:

    # Compute the similarity score between s and M using the text encoder
    # Use the cosine similarity function from torch.nn.functional
    q = torch.nn.functional.cosine_similarity(M.text_encoder(s), M.text_encoder(c))

    # Subtract the length penalty from q
    q = q - length_penalty(s)

    # Append q to Q
    Q.append(q)

  # Find the index of the highest score in Q
  i = torch.argmax(Q)

  # Select the corresponding sentence in S as the prompt for c
  p = S[i]

  # Add p to P with the key c
  P[c] = p

# Return P
return P
```