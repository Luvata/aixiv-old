---
title: 2306.00980v2 SnapFusion  Text-to-Image Diffusion Model on Mobile Devices within Two Seconds
date: 2023-06-01
---

# [SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds](http://arxiv.org/abs/2306.00980v2)

authors: Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, Jian Ren


## What, Why and How

[1]: https://arxiv.org/abs/2306.00980 "[2306.00980] SnapFusion: Text-to-Image Diffusion Model on Mobile ..."
[2]: http://export.arxiv.org/abs/2306.00980 "[2306.00980] SnapFusion: Text-to-Image Diffusion Model on Mobile ..."
[3]: https://arxiv.org/pdf/2306.00989v1.pdf "Abstract arXiv:2306.00989v1 [cs.CV] 1 Jun 2023"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a text-to-image diffusion model that can run on mobile devices within two seconds and generate realistic images from natural language descriptions.
- **Why**: The paper aims to overcome the challenges of existing text-to-image diffusion models, which are large, complex, slow, and require high-end GPUs or cloud-based inference. The paper also aims to democratize content creation by bringing powerful text-to-image diffusion models to the hands of users.
- **How**: The paper introduces an efficient network architecture and improves step distillation. Specifically, the paper proposes an efficient UNet by identifying the redundancy of the original model and reducing the computation of the image decoder via data distillation. Further, the paper enhances the step distillation by exploring training strategies and introducing regularization from classifier-free guidance. The paper evaluates the model on MS-COCO and shows that it achieves better FID and CLIP scores than Stable Diffusion v1.5 with fewer denoising steps.


## Main Contributions

According to the paper, the main contributions are:

- The first text-to-image diffusion model that can run on mobile devices within two seconds.
- An efficient UNet architecture that reduces the redundancy and computation of the image decoder.
- An improved step distillation method that explores training strategies and introduces classifier-free guidance.
- A comprehensive evaluation on MS-COCO that demonstrates the superiority of the model over existing methods in terms of quality and efficiency.


## Method Summary

Here is a summary of the method section of the paper:

- The paper adopts the text-to-image diffusion framework proposed by Li et al. (2022a), which consists of a text encoder, an image encoder, and an image decoder. The text encoder encodes the natural language description into a latent vector, which is then concatenated with a random noise vector to form the input of the image decoder. The image decoder generates an image from the input vector using a UNet architecture. The image encoder encodes the generated image into another latent vector, which is used to compute the loss function with the text vector and the noise vector.
- The paper introduces two main improvements to the text-to-image diffusion framework: efficient network architecture and improved step distillation. For the efficient network architecture, the paper proposes to reduce the redundancy and computation of the image decoder by removing some convolutional layers and applying data distillation. Data distillation is a technique that uses a teacher model to generate pseudo labels for unlabeled data, which are then used to train a student model. The paper uses Stable Diffusion v1.5 as the teacher model and trains an efficient UNet as the student model using MS-COCO images as unlabeled data. For the improved step distillation, the paper proposes to explore different training strategies and introduce classifier-free guidance. Step distillation is a technique that trains a diffusion model with fewer denoising steps by mimicking the outputs of a model with more steps. The paper explores different ways to select the steps for training, such as uniform sampling, exponential sampling, and adaptive sampling. The paper also introduces classifier-free guidance, which is a regularization term that encourages the model to generate images that are consistent with the text description without using an external classifier. The paper uses CLIP as a metric to measure the semantic similarity between the text and the image.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the text encoder, image encoder, and image decoder
text_encoder = TextEncoder()
image_encoder = ImageEncoder()
image_decoder = EfficientUNet()

# Define the loss function
def loss_function(text_vector, noise_vector, image_vector):
  # Compute the KL divergence between the noise vector and a standard normal distribution
  kl_loss = KL_divergence(noise_vector, N(0, 1))
  # Compute the cosine similarity between the text vector and the image vector
  cos_sim = cosine_similarity(text_vector, image_vector)
  # Return the weighted sum of the kl_loss and the negative cos_sim
  return alpha * kl_loss - beta * cos_sim

# Define the classifier-free guidance
def classifier_free_guidance(text, image):
  # Use CLIP to compute the logit score of the text-image pair
  logit = CLIP(text, image)
  # Return the negative logit as the guidance
  return -logit

# Train the model with data distillation and step distillation
for epoch in range(num_epochs):
  # Sample a batch of text descriptions and images from MS-COCO
  text_batch, image_batch = sample_batch(MS-COCO)
  # Encode the text descriptions into latent vectors
  text_vector_batch = text_encoder(text_batch)
  # Sample a batch of random noise vectors
  noise_vector_batch = sample_noise_batch()
  # Concatenate the text vectors and the noise vectors
  input_vector_batch = concatenate(text_vector_batch, noise_vector_batch)
  # Generate images from the input vectors using the image decoder
  generated_image_batch = image_decoder(input_vector_batch)
  # Encode the generated images into latent vectors using the image encoder
  generated_image_vector_batch = image_encoder(generated_image_batch)
  # Compute the loss function for the generated images
  loss_generated = loss_function(text_vector_batch, noise_vector_batch, generated_image_vector_batch)
  # Compute the classifier-free guidance for the generated images
  guidance_generated = classifier_free_guidance(text_batch, generated_image_batch)
  # Use Stable Diffusion v1.5 as a teacher model to generate pseudo labels for the MS-COCO images
  pseudo_label_batch = StableDiffusion(image_batch)
  # Encode the pseudo labels into latent vectors using the image encoder
  pseudo_label_vector_batch = image_encoder(pseudo_label_batch)
  # Compute the loss function for the pseudo labels
  loss_pseudo = loss_function(text_vector_batch, noise_vector_batch, pseudo_label_vector_batch)
  # Compute the classifier-free guidance for the pseudo labels
  guidance_pseudo = classifier_free_guidance(text_batch, pseudo_label_batch)
  # Select a subset of denoising steps for step distillation using adaptive sampling
  steps = adaptive_sampling()
  # For each selected step, generate an intermediate image using Stable Diffusion v1.5 and mimic its output using EfficientUNet
  for step in steps:
    # Generate an intermediate image using Stable Diffusion v1.5 at the given step
    intermediate_image = StableDiffusion(image_batch, step)
    # Encode the intermediate image into a latent vector using the image encoder
    intermediate_image_vector = image_encoder(intermediate_image)
    # Generate an intermediate image using EfficientUNet at the same step
    mimicked_image = EfficientUNet(input_vector_batch, step)
    # Encode the mimicked image into a latent vector using the image encoder
    mimicked_image_vector = image_encoder(mimicked_image)
    # Compute the L2 distance between the intermediate image vector and the mimicked image vector as the step distillation loss
    loss_step_distillation += L2_distance(intermediate_image_vector, mimicked_image_vector)
  
  # Compute the total loss as a weighted sum of all losses and guidances
  total_loss = gamma * (loss_generated + loss_pseudo) + delta * (guidance_generated + guidance_pseudo) + epsilon * loss_step_distillation
  
  # Update the model parameters using gradient descent
  update_parameters(total_loss)

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import clip
import numpy as np

# Define the hyperparameters
num_epochs = 100 # Number of training epochs
batch_size = 32 # Batch size
image_size = 256 # Image size
text_dim = 512 # Text embedding dimension
noise_dim = 512 # Noise vector dimension
hidden_dim = 256 # Hidden layer dimension
num_channels = 3 # Number of image channels
num_steps = 8 # Number of denoising steps for EfficientUNet
alpha = 0.1 # Weight for KL loss
beta = 0.9 # Weight for cosine similarity
gamma = 0.5 # Weight for loss function
delta = 0.5 # Weight for classifier-free guidance
epsilon = 0.01 # Weight for step distillation loss
learning_rate = 0.001 # Learning rate

# Define the text encoder as a pre-trained CLIP model
text_encoder = clip.load("ViT-B/32", jit=False)[0]
text_encoder.eval() # Set the text encoder to evaluation mode

# Define the image encoder as a convolutional neural network with residual blocks
class ImageEncoder(torch.nn.Module):
  def __init__(self, num_channels, hidden_dim, text_dim):
    super(ImageEncoder, self).__init__()
    # Define the convolutional layers with ReLU activation and batch normalization
    self.conv1 = torch.nn.Sequential(
      torch.nn.Conv2d(num_channels, hidden_dim, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim)
    )
    self.conv2 = torch.nn.Sequential(
      torch.nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim * 2)
    )
    self.conv3 = torch.nn.Sequential(
      torch.nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim * 4)
    )
    self.conv4 = torch.nn.Sequential(
      torch.nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim * 8)
    )
    self.conv5 = torch.nn.Sequential(
      torch.nn.Conv2d(hidden_dim * 8, hidden_dim * 16, kernel_size=3, stride=2, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim * 16)
    )
    # Define the residual blocks with skip connections
    self.res1 = ResBlock(hidden_dim * 16)
    self.res2 = ResBlock(hidden_dim * 16)
    self.res3 = ResBlock(hidden_dim * 16)
    self.res4 = ResBlock(hidden_dim * 16)
    # Define the linear layer to project the image vector to the text dimension
    self.linear = torch.nn.Linear(hidden_dim * 16, text_dim)

  def forward(self, x):
    # Apply the convolutional layers to the input image x
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.conv4(x)
    x = self.conv5(x)
    # Apply the residual blocks to the output of the last convolutional layer
    x = self.res1(x)
    x = self.res2(x)
    x = self.res3(x)
    x = self.res4(x)
    # Flatten the output of the last residual block and apply the linear layer
    x = x.view(x.size(0), -1)
    x = self.linear(x)
    return x

# Define the residual block as a sub-module
class ResBlock(torch.nn.Module):
  def __init__(self, hidden_dim):
    super(ResBlock, self).__init__()
    # Define two convolutional layers with ReLU activation and batch normalization
    self.conv1 = torch.nn.Sequential(
      torch.nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim)
    )
    self.conv2 = torch.nn.Sequential(
      torch.nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1),
      torch.nn.ReLU(),
      torch.nn.BatchNorm2d(hidden_dim)
    )

  def forward(self, x):
    # Save the input x as a skip connection
    skip = x
    # Apply the convolutional layers to the input x
    x = self.conv1(x)
    x = self.conv2(x)
    # Add the skip connection to the output of the second convolutional layer
    x = x + skip
    return x

# Define the image decoder as an efficient UNet with fewer convolutional layers and data distillation
class EfficientUNet(torch.nn.Module):
  def __init__(self, text_dim, noise_dim, hidden_dim, num_channels, num_steps):
    super(EfficientUNet, self).__init__()
    # Define the input dimension as the sum of the text dimension and the noise dimension
    input_dim = text_dim + noise_dim
    # Define the convolutional layers for the encoder part of the UNet
    self.enc1 = torch.nn.Conv2d(input_dim, hidden_dim, kernel_size=3, stride=1, padding=1)
    self.enc2 = torch.nn.Conv2d(hidden_dim, hidden_dim * 2, kernel_size=4, stride=2, padding=1)
    self.enc3 = torch.nn.Conv2d(hidden_dim * 2, hidden_dim * 4, kernel_size=4, stride=2, padding=1)
    self.enc4 = torch.nn.Conv2d(hidden_dim * 4, hidden_dim * 8, kernel_size=4, stride=2, padding=1)
    self.enc5 = torch.nn.Conv2d(hidden_dim * 8, hidden_dim * 16, kernel_size=4, stride=2, padding=1)
    # Define the convolutional layers for the decoder part of the UNet
    self.dec5 = torch.nn.ConvTranspose2d(hidden_dim * 16, hidden_dim * 8, kernel_size=4, stride=2, padding=1)
    self.dec4 = torch.nn.ConvTranspose2d(hidden_dim * 16, hidden_dim * 4, kernel_size=4, stride=2, padding=1)
    self.dec3 = torch.nn.ConvTranspose2d(hidden_dim * 8, hidden_dim * 2, kernel_size=4, stride=2, padding=1)
    self.dec2 = torch.nn.ConvTranspose2d(hidden_dim * 4, hidden_dim * 1, kernel_size=4, stride=2, padding=1)
    self.dec1 = torch.nn.ConvTranspose2d(hidden_dim * 2, num_channels, kernel_size=3, stride=1, padding=1)
    # Define the output layer as a sigmoid function
    self.output = torch.nn.Sigmoid()
    # Define the number of denoising steps
    self.num_steps = num_steps

  def forward(self, x):
    # Initialize a list to store the intermediate outputs for step distillation
    outputs = []
    # Apply the encoder part of the UNet to the input vector x
    e1 = self.enc1(x)
    e2 = self.enc2(e1)
    e3 = self.enc3(e2)
    e4 = self.enc4(e3)
    e5 = self.enc5(e4)
    # Apply the decoder part of the UNet to the output of the encoder part
    d5 = self.dec5(e5)
    d5 = torch.cat([d5,e4], dim=1) # Concatenate with the skip connection from e4
    d4 = self.dec4(d5)
    d4 = torch.cat([d4,e3], dim=1) # Concatenate with the skip connection from e3
    d3 = self.dec3(d4)
    d3 = torch.cat([d3,e2], dim=1) # Concatenate with the skip connection from e2
    d2 = self.dec2(d3)
    d2 = torch.cat([d2,e1], dim=1) # Concatenate with the skip connection from e1
    d1 = self.dec1(d2)