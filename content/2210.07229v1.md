---
title: 2210.07229v1 Mass-Editing Memory in a Transformer
date: 2022-10-08
---

# [Mass-Editing Memory in a Transformer](http://arxiv.org/abs/2210.07229v1)

authors: Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau


## What, Why and How

[1]: https://arxiv.org/abs/2210.07229 "[2210.07229] Mass-Editing Memory in a Transformer - arXiv.org"
[2]: https://arxiv.org/pdf/2210.07229v1 "M -E M TRANSFORMER - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.07229v1 "[2203.07229v1] Physico-chemical properties extraction from the ..."

Here is a summary of the paper in terms of What, Why, and How:

- **What**: The paper proposes a method called MEMIT (Mass-Editing Memory in a Transformer) that can directly update a large language model with many new memories (such as facts or associations) by modifying its weights.
- **Why**: The paper aims to address the limitations of existing knowledge-editing methods that are mostly restricted to updating single or few memories and fail to scale up to thousands of memories. The paper also argues that updating a model with many memories can improve its factual knowledge, generalization, specificity, and fluency.
- **How**: The paper develops a rank-one editing framework that can efficiently compute the optimal weight updates for multiple memories simultaneously. The paper also introduces a memory-editing loss function that balances between preserving the original model behavior and enforcing the new memories. The paper evaluates MEMIT on two large language models (GPT-J and GPT-NeoX) and shows that it can successfully edit up to 10,000 memories while outperforming baselines on various metrics.

## Main Contributions

[1]: https://arxiv.org/abs/2210.07229 "[2210.07229] Mass-Editing Memory in a Transformer - arXiv.org"
[2]: https://arxiv.org/pdf/2210.07229v1 "M -E M TRANSFORMER - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.07229v1 "[2203.07229v1] Physico-chemical properties extraction from the ..."

According to the paper, the main contributions are:

- **A novel method for mass-editing memory in a transformer**: The paper introduces MEMIT, a rank-one editing framework that can efficiently compute the optimal weight updates for multiple memories simultaneously, without requiring re-training or fine-tuning the model.
- **A memory-editing loss function that balances between preservation and enforcement**: The paper proposes a loss function that measures the trade-off between preserving the original model behavior and enforcing the new memories, and shows how to optimize it using gradient descent.
- **A large-scale evaluation on two state-of-the-art language models**: The paper evaluates MEMIT on GPT-J (6B) and GPT-NeoX (20B), two large autoregressive language models, and shows that it can successfully edit up to 10,000 memories while outperforming baselines on various metrics such as factual accuracy, generalization, specificity, and fluency.

## Method Summary

[1]: https://arxiv.org/abs/2210.07229 "[2210.07229] Mass-Editing Memory in a Transformer - arXiv.org"
[2]: https://arxiv.org/pdf/2210.07229v1 "M -E M TRANSFORMER - arXiv.org"
[3]: http://export.arxiv.org/abs/2203.07229v1 "[2203.07229v1] Physico-chemical properties extraction from the ..."

Here is a summary of the method section of the paper:

- **Rank-one editing framework**: The paper develops a framework that can edit multiple memories in a transformer by adding rank-one matrices to its weights. The paper shows how to compute the optimal rank-one updates for each memory using singular value decomposition (SVD) and how to combine them using a weighted sum. The paper also proves that the rank-one updates are orthogonal to each other and preserve the original model behavior as much as possible.
- **Memory-editing loss function**: The paper proposes a loss function that measures the trade-off between preserving the original model behavior and enforcing the new memories. The loss function consists of two terms: a preservation term that minimizes the distance between the original and edited model outputs on a large corpus, and an enforcement term that maximizes the likelihood of the new memories on a small set of queries. The paper shows how to optimize the loss function using gradient descent and how to choose the optimal trade-off parameter using cross-validation.
- **Implementation details**: The paper provides details on how to implement MEMIT on large language models such as GPT-J and GPT-NeoX. The paper describes how to select the memories to edit, how to construct the queries and answers for each memory, how to sample the corpus for the preservation term, how to parallelize the computation of the rank-one updates, and how to tune the hyperparameters of MEMIT.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a large language model M, a set of memories D = {(s_i, r_i, o_i)}, a corpus C, a trade-off parameter lambda
# Output: an edited model M'

# Initialize the edited model M' as a copy of M
M' = M.copy()

# For each memory in D
for (s, r, o) in D:

  # Construct a query q and an answer a for the memory
  q = s + " " + r + " ?"
  a = o

  # Compute the optimal rank-one update U for the memory using SVD
  U = compute_rank_one_update(M, q, a)

  # Add the rank-one update to the edited model weights with a weight alpha
  M' = M' + alpha * U

# Optimize the trade-off parameter lambda using cross-validation
lambda = optimize_trade_off(M, M', D, C)

# Compute the memory-editing loss L on the corpus C and the memories D
L = compute_memory_editing_loss(M, M', D, C, lambda)

# Minimize the memory-editing loss using gradient descent
M' = gradient_descent(M', L)

# Return the edited model
return M'
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a large language model M, a set of memories D = {(s_i, r_i, o_i)}, a corpus C, a trade-off parameter lambda
# Output: an edited model M'

# Define some constants
K = number of transformer layers
H = number of attention heads
D = dimension of hidden states
E = dimension of embeddings
N = number of memories to edit
B = batch size for preservation term
T = number of gradient descent steps
LR = learning rate for gradient descent

# Initialize the edited model M' as a copy of M
M' = M.copy()

# Initialize the rank-one updates U_k for each layer k as zero matrices
U_k = np.zeros((D, D)) for k in range(K)

# Initialize the weights alpha_i for each memory i as random scalars
alpha_i = np.random.rand() for i in range(N)

# For each memory i in D
for i in range(N):

  # Extract the subject s_i, relation r_i, and object o_i from the memory
  (s_i, r_i, o_i) = D[i]

  # Construct a query q_i and an answer a_i for the memory
  q_i = s_i + " " + r_i + " ?"
  a_i = o_i

  # Encode the query and answer using the model's tokenizer
  x_i = tokenizer.encode(q_i)
  y_i = tokenizer.encode(a_i)

  # Compute the model output z_i on the query using M
  z_i = M(x_i)

  # Compute the error e_i between the model output and the answer
  e_i = z_i - y_i

  # For each layer k in reverse order
  for k in range(K-1, -1, -1):

    # Extract the attention matrix A_k and the value matrix V_k from layer k
    A_k = M.layers[k].attention(x_i)
    V_k = M.layers[k].value(x_i)

    # Compute the optimal rank-one update U_k,i for layer k and memory i using SVD
    U_k,i = compute_rank_one_update(A_k, V_k, e_i)

    # Add the rank-one update to the existing rank-one updates for layer k with weight alpha_i
    U_k = U_k + alpha_i * U_k,i

    # Propagate the error e_i to the previous layer using the transpose of U_k,i
    e_i = e_i @ U_k,i.T

# Add the rank-one updates U_k to the edited model weights W_k for each layer k
for k in range(K):
  W_k = M'.layers[k].weight()
  W_k = W_k + U_k
  M'.layers[k].set_weight(W_k)

# Optimize the trade-off parameter lambda using cross-validation on a validation set V
lambda = optimize_trade_off(M, M', D, V)

# Compute the preservation term P on a random batch of sentences from C using M and M'
x_batch = sample_batch(C, B)
z_batch = M(x_batch)
z'_batch = M'(x_batch)
P = mean_squared_error(z_batch, z'_batch)

# Compute the enforcement term E on the memories D using M'
E = 0
for i in range(N):
  (s_i, r_i, o_i) = D[i]
  q_i = s_i + " " + r_i + " ?"
  a_i = o_i
  x_i = tokenizer.encode(q_i)
  y_i = tokenizer.encode(a_i)
  z'_i = M'(x_i)
  E += cross_entropy(y_i, z'_i)
E /= N

# Compute the memory-editing loss L as a weighted sum of P and E with lambda
L = P + lambda * E

# Minimize the memory-editing loss using gradient descent for T steps with learning rate LR
for t in range(T):
  grad_L = compute_gradient(L, M')
  M' = M' - LR * grad_L

# Return the edited model
return M'
```