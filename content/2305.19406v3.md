---
title: 2305.19406v3 PaintSeg  Training-free Segmentation via Painting
date: 2023-05-20
---

# [PaintSeg: Training-free Segmentation via Painting](http://arxiv.org/abs/2305.19406v3)

authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, Bhiksha Raj


## What, Why and How

[1]: https://arxiv.org/pdf/2305.19406v3.pdf "arXiv:2305.19406v3 [cs.CV] 4 Jun 2023"
[2]: https://arxiv.org/abs/2305.19406 "[2305.19406] PaintSeg: Training-free Segmentation via Painting - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2305.19406v3 "[2305.19406v3] PaintSeg: Training-free Segmentation via Painting"

Here is a summary of the paper:

- **What**: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training[^1^][1] [^2^][2] [^3^][3].
- **Why**: The paper aims to leverage off-the-shelf generative models to create a contrast between the original image and a painted image in which a masked area is painted, and use this contrast to segment objects without supervision or training[^1^][1] [^2^][2] [^3^][3].
- **How**: The paper proposes an adversarial masked contrastive painting (AMCP) process, which alternates between inpainting and outpainting steps. Inpainting masks the foreground and fills in the background, while outpainting masks the background and recovers the missing part of the foreground object. The paper also shows that PaintSeg can work with various prompts, such as coarse masks, boxes, scribbles, and points[^1^][1] [^2^][2] [^3^][3].


## Main Contributions

[1]: https://arxiv.org/pdf/2305.19406v3.pdf "arXiv:2305.19406v3 [cs.CV] 4 Jun 2023"
[2]: https://arxiv.org/abs/2305.19406 "[2305.19406] PaintSeg: Training-free Segmentation via Painting - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2305.19406v3 "[2305.19406v3] PaintSeg: Training-free Segmentation via Painting"

According to the paper, the main contributions are:

- **A novel unsupervised segmentation method** that does not require any training or supervision, and can segment objects in images using off-the-shelf generative models[^1^][1] [^2^][2] [^3^][3].
- **An adversarial masked contrastive painting (AMCP) process** that alternates between inpainting and outpainting steps to create a contrast between the original image and a painted image, and uses this contrast to refine the segmentation mask[^1^][1] [^2^][2] [^3^][3].
- **A flexible and robust framework** that can work with various prompts, such as coarse masks, boxes, scribbles, and points, and can handle complex scenes with multiple objects and occlusions[^1^][1] [^2^][2] [^3^][3].
- **Extensive experiments and comparisons** that demonstrate the effectiveness and superiority of PaintSeg over existing methods in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks[^1^][1] [^2^][2] [^3^][3].


## Method Summary

[1]: https://arxiv.org/pdf/2305.19406v3.pdf "arXiv:2305.19406v3 [cs.CV] 4 Jun 2023"
[2]: https://arxiv.org/abs/2305.19406 "[2305.19406] PaintSeg: Training-free Segmentation via Painting - arXiv.org"
[3]: https://lib-arxiv-017.serverfarm.cornell.edu/abs/2305.19406v3 "[2305.19406v3] PaintSeg: Training-free Segmentation via Painting"

Here is a summary of the method section of the paper:

- **Overview**: The paper proposes a method called PaintSeg, which uses off-the-shelf generative models to segment objects in images without any training or supervision[^1^][1] [^2^][2] [^3^][3]. The method consists of two main components: an adversarial masked contrastive painting (AMCP) process and a segmentation refinement module[^1^][1].
- **AMCP process**: The AMCP process creates a contrast between the original image and a painted image in which a masked area is painted using generative models. The process alternates between inpainting and outpainting steps. Inpainting masks the foreground object and fills in the background using a denoising diffusion probabilistic model (DDPM), while outpainting masks the background and recovers the missing part of the foreground object using a text-to-image synthesis model (T2I). The process also uses an adversarial discriminator to ensure the realism of the painted images and to provide feedback for mask refinement[^1^][1].
- **Segmentation refinement module**: The segmentation refinement module uses the contrast between the original image and the painted image to refine the segmentation mask. The module consists of two sub-modules: a contrastive feature extractor and a mask refinement network. The contrastive feature extractor extracts features from both images and computes their cosine similarity. The mask refinement network takes the similarity map and the initial mask as inputs and outputs a refined mask[^1^][1].
- **Prompt adaptation**: The paper also shows how PaintSeg can work with various prompts, such as coarse masks, boxes, scribbles, and points. The paper introduces a prompt adaptation module that converts different types of prompts into coarse masks that can be used by PaintSeg[^1^][1].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: an image x and a prompt p
# Output: a segmentation mask m

# Convert the prompt p into a coarse mask m0 using the prompt adaptation module
m0 = prompt_adaptation(x, p)

# Initialize the painted image y as x
y = x

# Repeat for a fixed number of iterations
for i in range(num_iterations):

  # Perform inpainting on y using DDPM and m0
  y_inpaint = inpaint(y, m0)

  # Perform outpainting on y using T2I and m0
  y_outpaint = outpaint(y, m0)

  # Use an adversarial discriminator to compute the realism scores of y_inpaint and y_outpaint
  score_inpaint = discriminator(y_inpaint)
  score_outpaint = discriminator(y_outpaint)

  # Update the mask m0 based on the scores and a threshold
  m0 = update_mask(m0, score_inpaint, score_outpaint, threshold)

  # Update the painted image y as a weighted combination of y_inpaint and y_outpaint
  y = alpha * y_inpaint + (1 - alpha) * y_outpaint

# Extract features from x and y using a contrastive feature extractor
f_x = feature_extractor(x)
f_y = feature_extractor(y)

# Compute the cosine similarity between f_x and f_y
s = cosine_similarity(f_x, f_y)

# Refine the mask m0 using a mask refinement network with s as input
m = mask_refinement(s, m0)

# Return the final segmentation mask m
return m
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the required libraries
import torch
import torchvision
import numpy as np
import cv2

# Load the pre-trained DDPM model
ddpm = torch.hub.load('openai/DALL-E', 'ddpm')

# Load the pre-trained T2I model
t2i = torch.hub.load('openai/DALL-E', 't2i')

# Load the pre-trained discriminator model
discriminator = torch.hub.load('openai/DALL-E', 'discriminator')

# Load the pre-trained feature extractor model
feature_extractor = torch.hub.load('openai/DALL-E', 'feature_extractor')

# Load the pre-trained mask refinement network model
mask_refinement = torch.hub.load('openai/DALL-E', 'mask_refinement')

# Define the prompt adaptation module
def prompt_adaptation(x, p):
  # Input: an image x and a prompt p
  # Output: a coarse mask m0

  # Convert the image x to grayscale and resize it to 256x256
  x_gray = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)
  x_gray = cv2.resize(x_gray, (256, 256))

  # Initialize the coarse mask m0 as a zero matrix of size 256x256
  m0 = np.zeros((256, 256))

  # Check the type of the prompt p
  if p is a mask:
    # Resize the mask p to 256x256 and binarize it with a threshold of 0.5
    p = cv2.resize(p, (256, 256))
    p = (p > 0.5).astype(np.uint8)

    # Copy the mask p to m0
    m0 = p

  elif p is a box:
    # Get the coordinates of the box p as (x1, y1, x2, y2)
    x1, y1, x2, y2 = p

    # Resize the coordinates according to the image size
    x1 = int(x1 * 256 / x.shape[1])
    y1 = int(y1 * 256 / x.shape[0])
    x2 = int(x2 * 256 / x.shape[1])
    y2 = int(y2 * 256 / x.shape[0])

    # Draw a white rectangle on m0 with the resized coordinates
    cv2.rectangle(m0, (x1, y1), (x2, y2), (255, 255, 255), -1)

  elif p is a scribble:
    # Resize the scribble p to 256x256 and binarize it with a threshold of 0.5
    p = cv2.resize(p, (256, 256))
    p = (p > 0.5).astype(np.uint8)

    # Find the contours of the scribble p using OpenCV
    contours, _ = cv2.findContours(p, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)

    # Draw a white filled contour on m0 for each contour in contours
    for contour in contours:
      cv2.drawContours(m0, [contour], -1, (255, 255, 255), -1)

  elif p is a point:
    # Get the coordinates of the point p as (x, y)
    x, y = p

    # Resize the coordinates according to the image size
    x = int(x * 256 / x.shape[1])
    y = int(y * 256 / x.shape[0])

    # Draw a white circle on m0 with the resized coordinates and a radius of 10 pixels
    cv2.circle(m0, (x, y), 10, (255, 255, 255), -1)

  else:
    # Raise an error if the prompt type is not supported
    raise ValueError("Unsupported prompt type")

  # Return the coarse mask m0 as a torch tensor
  return torch.from_numpy(m0)

# Define the inpainting function using DDPM model
def inpaint(y, m0):
  # Input: an image y and a mask m0
  # Output: an inpainted image y_inpaint

  # Convert the image y and the mask m0 to torch tensors and normalize them to [0, 1]
  y = torchvision.transforms.ToTensor()(y)
  m0 = torchvision.transforms.ToTensor()(m0) / 255.0

  # Invert the mask m0 to get m1
  m1 = 1 - m0

  # Apply the mask m1 to the image y to get y_masked
  y_masked = y * m1

  # Generate a random text caption for the inpainting task
  caption = "a photo of " + np.random.choice(["a cat", "a dog", "a bird", "a flower", "a car"])

  # Encode the caption using the T2I model
  z = t2i.encode_text(caption)

  # Inpaint the image y_masked using the DDPM model with the encoded caption z as condition
  y_inpaint = ddpm.inpaint(y_masked, z)

  # Return the inpainted image y_inpaint
  return y_inpaint

# Define the outpainting function using T2I model
def outpaint(y, m0):
  # Input: an image y and a mask m0
  # Output: an outpainted image y_outpaint

  # Convert the image y and the mask m0 to torch tensors and normalize them to [0, 1]
  y = torchvision.transforms.ToTensor()(y)
  m0 = torchvision.transforms.ToTensor()(m0) / 255.0

  # Apply the mask m0 to the image y to get y_masked
  y_masked = y * m0

  # Generate a random text caption for the outpainting task
  caption = "a photo of " + np.random.choice(["a cat", "a dog", "a bird", "a flower", "a car"])

  # Encode the caption using the T2I model
  z = t2i.encode_text(caption)

  # Outpaint the image y_masked using the T2I model with the encoded caption z as condition
  y_outpaint = t2i.outpaint(y_masked, z)

  # Return the outpainted image y_outpaint
  return y_outpaint

# Define the update_mask function using the discriminator model
def update_mask(m0, score_inpaint, score_outpaint, threshold):
  # Input: a mask m0, a score for inpainted image score_inpaint, a score for outpainted image score_outpaint, and a threshold value
  # Output: an updated mask m1

  # Convert the mask m0 to a numpy array and normalize it to [0, 1]
  m0 = m0.numpy() / 255.0

  # Compute the difference between score_inpaint and score_outpaint
  diff = score_inpaint - score_outpaint

  # Find the pixels where diff is greater than the threshold
  idx = np.where(diff > threshold)

  # Set those pixels to white in m0
  m0[idx] = 1.0

  # Return the updated mask m1 as a torch tensor
  return torch.from_numpy(m0)

# Define the cosine_similarity function using torch.nn.functional module
def cosine_similarity(f_x, f_y):
  # Input: two feature tensors f_x and f_y
  # Output: a similarity tensor s

  # Compute the cosine similarity between f_x and f_y along the channel dimension
  s = torch.nn.functional.cosine_similarity(f_x, f_y, dim=1)

  # Return the similarity tensor s
  return s

# Define the main PaintSeg function
def PaintSeg(x, p):
  
```