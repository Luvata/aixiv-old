---
title: 2306.09344v2 DreamSim  Learning New Dimensions of Human Visual Similarity using Synthetic Data
date: 2023-06-10
---

# [DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data](http://arxiv.org/abs/2306.09344v2)

authors: Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, Phillip Isola


## What, Why and How

[1]: https://arxiv.org/abs/2306.09344 "[2306.09344] DreamSim: Learning New Dimensions of Human Visual ..."
[2]: https://arxiv.org/pdf/2006.09344v2.pdf "arXiv:2006.09344v2 [math.CA] 28 Jun 2020"
[3]: https://arxiv.org/pdf/2303.09344v2.pdf "arXiv:2303.09344v2 [physics.ins-det] 9 Jun 2023"

Here is a summary of the paper:

```
What: The paper develops a perceptual metric that assesses images holistically, called DreamSim, by using synthetic data and human similarity judgments.

Why: The paper aims to overcome the limitations of current perceptual similarity metrics that operate at the level of pixels and patches, and fail to capture mid-level similarities and differences in image layout, object pose, and semantic content.

How: The paper collects a new dataset of human similarity judgments over image pairs that are alike in diverse ways, using recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. The paper then trains a neural network model, DreamSim, to predict the human similarity scores, and evaluates it on various tasks such as retrieval and reconstruction. The paper also analyzes how the metric is affected by different visual attributes, and compares it with prior learned metrics and recent large vision models.
```

## Main Contributions

[1]: https://arxiv.org/abs/2306.09344 "[2306.09344] DreamSim: Learning New Dimensions of Human Visual ..."
[2]: https://arxiv.org/pdf/2006.09344v2.pdf "arXiv:2006.09344v2 [math.CA] 28 Jun 2020"
[3]: https://arxiv.org/pdf/2303.09344v2.pdf "arXiv:2303.09344v2 [physics.ins-det] 9 Jun 2023"

The paper claims the following contributions:

- A new dataset of human similarity judgments over image pairs that are alike in diverse ways, using synthetic data generated by text-to-image models.
- A new perceptual metric, DreamSim, that predicts human similarity scores and assesses images holistically, taking into account various visual attributes such as foreground objects, semantic content, color and layout.
- An evaluation of DreamSim on various tasks such as retrieval and reconstruction, showing that it outperforms both prior learned metrics and recent large vision models.
- An analysis of how DreamSim is affected by different visual attributes, and how it generalizes to real images despite being trained on synthetic data.

## Method Summary

[1]: https://arxiv.org/abs/2306.09344 "[2306.09344] DreamSim: Learning New Dimensions of Human Visual ..."
[2]: https://arxiv.org/pdf/2006.09344v2.pdf "arXiv:2006.09344v2 [math.CA] 28 Jun 2020"
[3]: https://arxiv.org/pdf/2303.09344v2.pdf "arXiv:2303.09344v2 [physics.ins-det] 9 Jun 2023"

Here is a summary of the method section:

```
The method section consists of four subsections: Data Collection, Model Architecture, Training and Evaluation, and Analysis.

Data Collection: The authors describe how they use recent text-to-image models, such as DALL-E and CLIP, to generate synthetic image pairs that are perturbed along various dimensions, such as color, texture, layout, pose, and content. They then collect human similarity judgments over these pairs using Amazon Mechanical Turk, and obtain a dataset of 10K image pairs with similarity scores ranging from 0 to 1.

Model Architecture: The authors introduce DreamSim, a neural network model that takes two images as input and outputs a similarity score between 0 and 1. The model consists of three components: an encoder that extracts features from each image using a pretrained CLIP model, a comparator that computes the cosine similarity between the features, and a regressor that maps the cosine similarity to a human-like similarity score using a learned function.

Training and Evaluation: The authors describe how they train DreamSim on their synthetic dataset using mean squared error as the loss function. They then evaluate DreamSim on various tasks such as retrieval and reconstruction, where they compare the performance of DreamSim with other perceptual metrics such as LPIPS, SSIM, and CLIP. They also compare DreamSim with recent large vision models such as DALL-E and VQGAN on these tasks.

Analysis: The authors analyze how DreamSim is affected by different visual attributes, such as foreground objects, semantic content, color and layout. They use ablation studies and visualization techniques to show how DreamSim assigns different weights to these attributes depending on the task and the image pair. They also show how DreamSim generalizes to real images despite being trained on synthetic data, and how it captures human-like similarities that are not captured by other metrics or models.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2306.09344 "[2306.09344] DreamSim: Learning New Dimensions of Human Visual ..."
[2]: https://arxiv.org/pdf/2006.09344v2.pdf "arXiv:2006.09344v2 [math.CA] 28 Jun 2020"
[3]: https://arxiv.org/pdf/2303.09344v2.pdf "arXiv:2303.09344v2 [physics.ins-det] 9 Jun 2023"

Here is a high-level pseudo code for the paper:

```
# Data Collection
for each caption in a set of captions:
  generate two images using text-to-image models (e.g. DALL-E, CLIP)
  perturb one image along a random dimension (e.g. color, texture, layout, pose, content)
  show the image pair to human annotators on Amazon Mechanical Turk
  collect similarity scores between 0 and 1 from the annotators
  store the image pair and the similarity score in a dataset

# Model Architecture
define DreamSim as a neural network model with three components:
  an encoder that takes an image as input and outputs a feature vector using a pretrained CLIP model
  a comparator that takes two feature vectors as input and outputs a cosine similarity between them
  a regressor that takes a cosine similarity as input and outputs a human-like similarity score using a learned function

# Training and Evaluation
split the dataset into train, validation, and test sets
train DreamSim on the train set using mean squared error as the loss function
evaluate DreamSim on the validation and test sets using various tasks such as retrieval and reconstruction
compare DreamSim with other perceptual metrics (e.g. LPIPS, SSIM, CLIP) and large vision models (e.g. DALL-E, VQGAN) on these tasks

# Analysis
analyze how DreamSim is affected by different visual attributes (e.g. foreground objects, semantic content, color, layout)
use ablation studies and visualization techniques to show how DreamSim assigns different weights to these attributes depending on the task and the image pair
show how DreamSim generalizes to real images despite being trained on synthetic data
show how DreamSim captures human-like similarities that are not captured by other metrics or models
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2306.09344 "[2306.09344] DreamSim: Learning New Dimensions of Human Visual ..."
[2]: https://arxiv.org/pdf/2006.09344v2.pdf "arXiv:2006.09344v2 [math.CA] 28 Jun 2020"
[3]: https://arxiv.org/pdf/2303.09344v2.pdf "arXiv:2303.09344v2 [physics.ins-det] 9 Jun 2023"

Here is a detailed pseudo code to implement the paper:

```
# Data Collection
import text_to_image_models # e.g. DALL-E, CLIP
import mechanical_turk_api # e.g. boto3
import numpy as np

captions = load_captions() # a set of captions for generating images
dataset = [] # a list of tuples of (image1, image2, similarity_score)
perturbations = ["color", "texture", "layout", "pose", "content"] # a list of possible perturbation dimensions

for caption in captions:
  image1 = text_to_image_models.generate_image(caption) # generate an image from the caption
  perturbation = np.random.choice(perturbations) # choose a random perturbation dimension
  image2 = text_to_image_models.perturb_image(image1, perturbation) # perturb the image along that dimension
  similarity_score = mechanical_turk_api.get_similarity_score(image1, image2) # get the human similarity score from Amazon Mechanical Turk
  dataset.append((image1, image2, similarity_score)) # add the image pair and the score to the dataset

save_dataset(dataset) # save the dataset for later use

# Model Architecture
import torch
import torch.nn as nn
import clip_model # e.g. https://github.com/openai/CLIP

class DreamSim(nn.Module):
  def __init__(self):
    super(DreamSim, self).__init__()
    self.encoder = clip_model.load_model() # load a pretrained CLIP model as the encoder
    self.comparator = nn.CosineSimilarity(dim=1) # use cosine similarity as the comparator
    self.regressor = nn.Sequential( # use a simple neural network as the regressor
      nn.Linear(1, 64),
      nn.ReLU(),
      nn.Linear(64, 1),
      nn.Sigmoid()
    )

  def forward(self, image1, image2):
    feature1 = self.encoder(image1) # encode the first image into a feature vector
    feature2 = self.encoder(image2) # encode the second image into a feature vector
    cosine_similarity = self.comparator(feature1, feature2) # compute the cosine similarity between the features
    similarity_score = self.regressor(cosine_similarity) # map the cosine similarity to a human-like similarity score
    return similarity_score

# Training and Evaluation
import torch.optim as optim

dreamsim = DreamSim() # create an instance of DreamSim model
criterion = nn.MSELoss() # use mean squared error as the loss function
optimizer = optim.Adam(dreamsim.parameters(), lr=0.001) # use Adam as the optimizer

train_set, val_set, test_set = split_dataset(dataset) # split the dataset into train, validation, and test sets

def train_epoch(dreamsim, train_set, criterion, optimizer): # define a function to train one epoch
  dreamsim.train() # set the model to training mode
  train_loss = 0.0 # initialize the train loss to zero
  for image1, image2, similarity_score in train_set: # iterate over the train set
    optimizer.zero_grad() # zero the gradients
    output = dreamsim(image1, image2) # get the model output for the image pair
    loss = criterion(output, similarity_score) # compute the loss between the output and the target score
    loss.backward() # perform backpropagation
    optimizer.step() # update the model parameters
    train_loss += loss.item() # accumulate the train loss
  train_loss /= len(train_set) # compute the average train loss
  return train_loss

def evaluate(dreamsim, val_set, criterion): # define a function to evaluate on the validation set
  dreamsim.eval() # set the model to evaluation mode
  val_loss = 0.0 # initialize the validation loss to zero
  with torch.no_grad(): # disable gradient computation
    for image1, image2, similarity_score in val_set: # iterate over the validation set
      output = dreamsim(image1, image2) # get the model output for the image pair
      loss = criterion(output, similarity_score) # compute the loss between the output and the target score
      val_loss += loss.item() # accumulate the validation loss
  val_loss /= len(val_set) # compute the average validation loss
  return val_loss

num_epochs = 10 # specify the number of epochs to train

for epoch in range(num_epochs): # iterate over epochs
  train_loss = train_epoch(dreamsim, train_set, criterion, optimizer) # train one epoch and get the train loss
  val_loss = evaluate(dreamsim, val_set, criterion) # evaluate on the validation set and get the validation loss
  print(f"Epoch {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}") # print the losses

test_loss = evaluate(dreamsim, test_set, criterion) # evaluate on the test set and get the test loss
print(f"Test Loss: {test_loss}") # print the test loss

# Analysis
import matplotlib.pyplot as plt

def ablate(dreamsim, test_set, perturbation): # define a function to ablate the model on a specific perturbation dimension
  dreamsim.eval() # set the model to evaluation mode
  scores = [] # a list of similarity scores for the image pairs with the perturbation
  with torch.no_grad(): # disable gradient computation
    for image1, image2, similarity_score in test_set: # iterate over the test set
      if perturbation == get_perturbation(image1, image2): # check if the image pair has the perturbation
        output = dreamsim(image1, image2) # get the model output for the image pair
        scores.append(output.item()) # add the output to the scores list
  return scores

def visualize(dreamsim, test_set): # define a function to visualize the model output for some image pairs
  dreamsim.eval() # set the model to evaluation mode
  samples = np.random.choice(test_set, size=10) # randomly sample 10 image pairs from the test set
  with torch.no_grad(): # disable gradient computation
    for image1, image2, similarity_score in samples: # iterate over the samples
      output = dreamsim(image1, image2) # get the model output for the image pair
      perturbation = get_perturbation(image1, image2) # get the perturbation dimension for the image pair
      plt.figure() # create a new figure
      plt.subplot(1, 2, 1) # create a subplot for the first image
      plt.imshow(image1) # show the first image
      plt.title("Image 1") # add a title
      plt.subplot(1, 2, 2) # create a subplot for the second image
      plt.imshow(image2) # show the second image
      plt.title("Image 2") # add a title
      plt.suptitle(f"Perturbation: {perturbation}, Human Score: {similarity_score}, DreamSim Score: {output}") # add a super title with the perturbation and the scores
      plt.show() # show the figure

perturbations = ["color", "texture", "layout", "pose", "content"] # a list of possible perturbation dimensions

for perturbation in perturbations: # iterate over perturbations
  scores = ablate(dreamsim, test_set, perturbation) # ablate the model on that perturbation and get the scores
  print(f"Average score for {perturbation}: {np.mean(scores)}") # print the average score for that perturbation

visualize(dreamsim, test_set) # visualize some examples of model output

```