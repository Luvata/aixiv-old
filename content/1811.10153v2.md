---
title: 1811.10153v2 Spatially Controllable Image Synthesis with Internal Representation Collaging
date: 2018-11-11
---

# [Spatially Controllable Image Synthesis with Internal Representation Collaging](http://arxiv.org/abs/1811.10153v2)

authors: Ryohei Suzuki, Masanori Koyama, Takeru Miyato, Taizan Yonetsuji, Huachun Zhu


## What, Why and How

[1]: https://arxiv.org/abs/1811.10153v2 "[1811.10153v2] Spatially Controllable Image Synthesis with Internal ..."
[2]: https://arxiv.org/pdf/1811.10153 "arXiv:1811.10153v2 [cs.CV] 9 Apr 2019"
[3]: http://export.arxiv.org/abs/2111.10153v2 "[2111.10153v2] Blockchain for Genomics: A Systematic Literature Review"

Here is a summary of the paper:

- **What**: The paper presents a novel CNN-based image editing strategy that allows the user to change the semantic information of an image over an arbitrary region by manipulating the feature-space representation of the image in a trained GAN model. The paper introduces two variants of the strategy: spatial conditional batch normalization (sCBN) and feature-blending.
- **Why**: The paper aims to address the challenge of regulating the GANs’ output at the user’s will, which is important for various image editing applications such as image colorization, inpainting, domain translation, style transfer, and object transfiguration.
- **How**: The paper proposes sCBN, a type of conditional batch normalization with user-specifiable spatial weight maps, and feature-blending, a method of directly modifying the intermediate features. The paper demonstrates the power of the methods through experiments on various types of GANs trained on different datasets. The paper also shows that the methods can be used to edit both artificial images and real images.

The summary is based on the information from [^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/1811.10153v2 "[1811.10153v2] Spatially Controllable Image Synthesis with Internal ..."
[2]: https://info.arxiv.org/help/submit/index.html "Submission Overview - arXiv info"
[3]: https://arxiv.org/pdf/1811.10153v2.pdf "arXiv.org e-Print archive"

The paper claims the following contributions:

- **sCBN**: A novel type of conditional batch normalization with user-specifiable spatial weight maps that can change the label of the user-specified parts of an image to the user-specified target labels by specifying an appropriate label map.
- **Feature-blending**: A method of directly modifying the intermediate features that can blend the features of a reference image into a base image over an arbitrary region.
- **Experiments**: A comprehensive evaluation of the proposed methods on various types of GANs trained on different datasets, showing that the methods can produce realistic and diverse images with fine-grained control over the semantic information.
- **Real image editing**: A demonstration of the applicability of the proposed methods to real image editing tasks such as face editing and object transfiguration.

The contributions are based on the information from [^1^][1].

## Method Summary

[1]: https://arxiv.org/abs/1811.10153v2 "[1811.10153v2] Spatially Controllable Image Synthesis with Internal ..."
[2]: https://arxiv.org/pdf/1811.10153v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section of the paper:

- **sCBN**: The paper proposes a novel type of conditional batch normalization that can change the label of the user-specified parts of an image to the user-specified target labels by specifying an appropriate label map. The paper defines a spatial weight map that controls the strength of the conditional information at each pixel location. The paper modifies the conditional batch normalization layer to take the spatial weight map as an additional input and apply it to the learned affine transformation parameters. The paper shows that sCBN can be used to edit both artificial images generated by GANs and real images encoded by an encoder network.
- **Feature-blending**: The paper proposes a method of directly modifying the intermediate features of a base image by blending them with the features of a reference image over an arbitrary region. The paper defines a blending mask that controls the degree of blending at each pixel location. The paper modifies the feature maps of the base image by element-wise multiplying them with the blending mask and adding them with the element-wise product of the feature maps of the reference image and the inverse blending mask. The paper shows that feature-blending can be used to edit both artificial images generated by GANs and real images encoded by an encoder network.

The summary is based on the information from [^1^][1] and [^2^][2].


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a base image x, a reference image y, a label map l, a blending mask m
# Output: an edited image z

# Define a GAN model with conditional normalization layers
GAN = GAN_model()

# Define an encoder network to encode real images into feature maps
Encoder = Encoder_network()

# If x and y are artificial images, use the GAN generator to obtain their feature maps
if x and y are artificial:
  fx = GAN.generator(x)
  fy = GAN.generator(y)

# If x and y are real images, use the encoder network to obtain their feature maps
if x and y are real:
  fx = Encoder(x)
  fy = Encoder(y)

# Apply sCBN to fx with the label map l to obtain fxc
fxc = sCBN(fx, l)

# Apply feature-blending to fxc and fy with the blending mask m to obtain fxy
fxy = feature_blending(fxc, fy, m)

# Use the GAN generator to obtain the edited image z from fxy
z = GAN.generator(fxy)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a base image x, a reference image y, a label map l, a blending mask m
# Output: an edited image z

# Define a GAN model with conditional normalization layers
GAN = GAN_model()

# Define an encoder network to encode real images into feature maps
Encoder = Encoder_network()

# Define the sCBN function
def sCBN(feature_map, label_map):
  # Get the shape of the feature map
  height, width, channels = feature_map.shape

  # Get the number of labels
  num_labels = label_map.max() + 1

  # Initialize the spatial weight map with zeros
  weight_map = zeros(height, width, num_labels)

  # For each pixel location in the label map
  for i in range(height):
    for j in range(width):
      # Get the label at that location
      label = label_map[i][j]

      # Set the corresponding entry in the weight map to one
      weight_map[i][j][label] = 1

  # For each conditional normalization layer in the GAN model
  for layer in GAN.layers:
    if layer is conditional normalization:
      # Get the learned affine transformation parameters (gamma and beta)
      gamma = layer.gamma
      beta = layer.beta

      # Apply the spatial weight map to the parameters
      gamma = gamma * weight_map
      beta = beta * weight_map

      # Apply the modified conditional normalization to the feature map
      feature_map = layer(feature_map, gamma, beta)

  # Return the modified feature map
  return feature_map

# Define the feature-blending function
def feature_blending(base_feature_map, reference_feature_map, blending_mask):
  # Element-wise multiply the base feature map with the blending mask
  base_feature_map = base_feature_map * blending_mask

  # Element-wise multiply the reference feature map with the inverse blending mask
  reference_feature_map = reference_feature_map * (1 - blending_mask)

  # Element-wise add the two feature maps
  blended_feature_map = base_feature_map + reference_feature_map

  # Return the blended feature map
  return blended_feature_map

# If x and y are artificial images, use the GAN generator to obtain their feature maps
if x and y are artificial:
  fx = GAN.generator(x)
  fy = GAN.generator(y)

# If x and y are real images, use the encoder network to obtain their feature maps
if x and y are real:
  fx = Encoder(x)
  fy = Encoder(y)

# Apply sCBN to fx with the label map l to obtain fxc
fxc = sCBN(fx, l)

# Apply feature-blending to fxc and fy with the blending mask m to obtain fxy
fxy = feature_blending(fxc, fy, m)

# Use the GAN generator to obtain the edited image z from fxy
z = GAN.generator(fxy)
```