---
title: 2102.05918v2 Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision
date: 2021-02-06
---

# [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](http://arxiv.org/abs/2102.05918v2)

authors: Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig


## What, Why and How

[1]: https://arxiv.org/pdf/2102.05918v2.pdf "Abstract arXiv:2102.05918v2 [cs.CV] 11 Jun 2021"
[2]: https://arxiv.org/abs/2102.05918 "[2102.05918] Scaling Up Visual and Vision-Language Representation ..."
[3]: https://arxiv.org/abs/2201.05918 "Recursive Least Squares Advantage Actor-Critic Algorithms"

Here is a summary of the paper:

```
What: The paper proposes a method to scale up visual and vision-language representation learning using a noisy dataset of over one billion image alt-text pairs.

Why: The paper aims to overcome the limitations of existing pre-training datasets that are expensive or require expert knowledge, and to leverage the abundant and diverse data available on the web.

How: The paper uses a simple dual-encoder architecture that learns to align visual and language representations of the image and text pairs using a contrastive loss. The paper shows that the scale of the corpus can make up for its noise and leads to state-of-the-art representations for various downstream tasks.
```

## Main Contributions

[1]: https://arxiv.org/pdf/2102.05918v2.pdf "Abstract arXiv:2102.05918v2 [cs.CV] 11 Jun 2021"
[2]: https://arxiv.org/abs/2102.05918 "[2102.05918] Scaling Up Visual and Vision-Language Representation ..."
[3]: https://arxiv.org/abs/2201.05918 "Recursive Least Squares Advantage Actor-Critic Algorithms"

Here are some of the contributions of the paper:

- The paper introduces a large-scale noisy dataset of over one billion image alt-text pairs for visual and vision-language representation learning.
- The paper proposes a simple dual-encoder architecture that learns to align visual and language representations using a contrastive loss.
- The paper demonstrates that the scale of the dataset can compensate for its noise and achieve state-of-the-art results on various downstream tasks such as image classification, image-text retrieval, and cross-modality search.

## Method Summary

[1]: https://arxiv.org/pdf/2102.05918v2.pdf "Abstract arXiv:2102.05918v2 [cs.CV] 11 Jun 2021"
[2]: https://arxiv.org/abs/2102.05918 "[2102.05918] Scaling Up Visual and Vision-Language Representation ..."
[3]: https://arxiv.org/abs/2201.05918 "Recursive Least Squares Advantage Actor-Critic Algorithms"

Here is a summary of the method section of the paper:

```
The paper uses a dual-encoder architecture that consists of a vision encoder and a language encoder. The vision encoder is a ResNet-50 model that extracts features from images. The language encoder is a Transformer model that encodes text into embeddings. The paper uses a contrastive loss function that maximizes the similarity between the image and text pairs from the same source, and minimizes the similarity between the pairs from different sources. The paper also applies several techniques to improve the training efficiency and robustness, such as hard negative mining, gradient clipping, and label smoothing.
```

## Pseudo Code - High level

[1]: https://arxiv.org/pdf/2102.05918v2.pdf "Abstract arXiv:2102.05918v2 [cs.CV] 11 Jun 2021"
[2]: https://arxiv.org/abs/2102.05918 "[2102.05918] Scaling Up Visual and Vision-Language Representation ..."
[3]: https://arxiv.org/abs/2201.05918 "Recursive Least Squares Advantage Actor-Critic Algorithms"

Here is a high-level pseudo code for the paper:

```
# Initialize vision encoder, language encoder, and contrastive loss function
vision_encoder = ResNet-50()
language_encoder = Transformer()
contrastive_loss = NT-Xent()

# Loop over the image-text pairs from the noisy dataset
for image, text in noisy_dataset:

  # Extract image features and text embeddings
  image_features = vision_encoder(image)
  text_embeddings = language_encoder(text)

  # Compute the contrastive loss between the image and text pairs
  loss = contrastive_loss(image_features, text_embeddings)

  # Update the parameters of the vision encoder and language encoder
  update_parameters(vision_encoder, language_encoder, loss)

# Save the trained vision and language representations
save_representations(vision_encoder, language_encoder)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/pdf/2102.05918v2.pdf "Abstract arXiv:2102.05918v2 [cs.CV] 11 Jun 2021"
[2]: https://arxiv.org/abs/2102.05918 "[2102.05918] Scaling Up Visual and Vision-Language Representation ..."
[3]: https://arxiv.org/abs/2201.05918 "Recursive Least Squares Advantage Actor-Critic Algorithms"

Here is a detailed pseudo code to implement the paper:

```
# Import the required libraries
import torch
import torchvision
import transformers
import numpy as np

# Define the hyperparameters
batch_size = 256 # The size of each mini-batch
image_size = 224 # The size of each input image
text_length = 64 # The maximum length of each input text
hidden_size = 512 # The size of the hidden layer in the vision encoder and language encoder
temperature = 0.07 # The temperature parameter for the contrastive loss function
margin = 0.2 # The margin parameter for the hard negative mining technique
clip_value = 10.0 # The value for gradient clipping
smooth_value = 0.1 # The value for label smoothing
learning_rate = 1e-4 # The learning rate for the optimizer
num_epochs = 100 # The number of epochs to train

# Load the noisy dataset of image-text pairs
noisy_dataset = load_noisy_dataset()

# Initialize the vision encoder, language encoder, and contrastive loss function
vision_encoder = torchvision.models.resnet50(pretrained=True)
vision_encoder.fc = torch.nn.Linear(vision_encoder.fc.in_features, hidden_size)
language_encoder = transformers.BertModel.from_pretrained('bert-base-uncased')
language_encoder.pooler.dense = torch.nn.Linear(language_encoder.pooler.dense.in_features, hidden_size)
contrastive_loss = NT_Xent(temperature)

# Initialize the optimizer and the learning rate scheduler
optimizer = torch.optim.Adam(params=list(vision_encoder.parameters()) + list(language_encoder.parameters()), lr=learning_rate)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# Define a function to compute the cosine similarity between two tensors
def cosine_similarity(x, y):
  return torch.matmul(x, y.t()) / (torch.norm(x, dim=1, keepdim=True) * torch.norm(y, dim=1, keepdim=True).t())

# Define a function to perform hard negative mining on a batch of image-text pairs
def hard_negative_mining(sim_matrix):
  # sim_matrix: a tensor of shape [batch_size, batch_size] containing the cosine similarity between each pair of image and text embeddings
  # returns: a tensor of shape [batch_size] containing the indices of the hardest negatives for each positive pair

  # Mask out the diagonal elements (the positive pairs) and the lower triangular elements (to avoid duplicates)
  mask = torch.eye(batch_size).bool() | torch.tril(torch.ones(batch_size, batch_size)).bool()
  sim_matrix.masked_fill_(mask, -np.inf)

  # Find the maximum similarity along each row (the hardest negative for each image) and each column (the hardest negative for each text)
  hard_negatives_image, _ = sim_matrix.max(dim=1)
  hard_negatives_text, _ = sim_matrix.max(dim=0)

  # Concatenate the two tensors and return
  return torch.cat([hard_negatives_image, hard_negatives_text])

# Define a function to apply label smoothing on a batch of labels
def label_smoothing(labels):
  # labels: a tensor of shape [batch_size] containing the ground truth labels (0 or 1) for each pair of image and text embeddings
  # returns: a tensor of shape [batch_size] containing the smoothed labels

  # Replace each label with a weighted average of itself and its opposite value
  return (1 - smooth_value) * labels + smooth_value * (1 - labels)

# Loop over the number of epochs
for epoch in range(num_epochs):

  # Loop over the mini-batches of image-text pairs
  for image_batch, text_batch in noisy_dataset.batch(batch_size):

    # Resize and normalize the images
    image_batch = torchvision.transforms.Resize(image_size)(image_batch)
    image_batch = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image_batch)

    # Tokenize and pad the texts
    text_batch = transformers.BertTokenizer.from_pretrained('bert-base-uncased').batch_encode_plus(text_batch, padding='max_length', max_length=text_length)

    # Extract image features and text embeddings using the vision encoder and language encoder
    image_features = vision_encoder(image_batch)
    text_embeddings = language_encoder(**text_batch).pooler_output

    # Compute the cosine similarity matrix between the image features and text embeddings
    sim_matrix = cosine_similarity(image_features, text_embeddings)

    # Perform hard negative mining to find the hardest negatives for each positive pair
    hard_negatives = hard_negative_mining(sim_matrix)

    # Compute the contrastive loss between the positive pairs and the hardest negatives
    loss = contrastive_loss(sim_matrix.diag(), hard_negatives)

    # Update the parameters of the vision encoder and language encoder using gradient descent
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(list(vision_encoder.parameters()) + list(language_encoder.parameters()), clip_value)
    optimizer.step()

    # Update the learning rate using the scheduler
    scheduler.step()

  # Print the epoch number and the loss value
  print(f'Epoch {epoch + 1}, Loss: {loss.item()}')

# Save the trained vision and language representations
save_representations(vision_encoder, language_encoder)
```