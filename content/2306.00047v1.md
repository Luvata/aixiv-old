---
title: 2306.00047v1 Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-empowered Learning
date: 2023-06-01
---

# [Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-empowered Learning](http://arxiv.org/abs/2306.00047v1)

authors: Ruining Deng, Yanwei Li, Peize Li, Jiacheng Wang, Lucas W. Remedios, Saydolimkhon Agzamkhodjaev, Zuhayr Asad, Quan Liu, Can Cui, Yucheng Tang, Haichun Yang, Yuankai Huo


## What, Why and How

[1]: https://arxiv.org/abs/2306.00047 "[2306.00047] Democratizing Pathological Image Segmentation with Lay ..."
[2]: https://arxiv.org/pdf/2306.00047 "Saydolimkhon Agzamkhodjaev 1. Vanderbilt University ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00047 "[2306.00047] Democratizing Pathological Image Segmentation with Lay ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a molecular-empowered learning scheme for multi-class cell segmentation in high-resolution Giga-pixel whole slide images (WSI) using partial labels from lay annotators (annotators without medical domain knowledge).
- **Why**: The paper aims to democratize pathological AI deployment by reducing the reliance on labor-intensive and error-prone pixel-wise manual annotation from experienced domain experts (e.g., pathologists).
- **How**: The paper integrates Giga-pixel level molecular-morphology cross-modality registration, molecular-informed annotation, and molecular-oriented segmentation model, so as to achieve significantly superior performance via 3 lay annotators as compared with 2 experienced pathologists. The paper also proposes a deep corrective learning method to further improve the segmentation performance using partially annotated noisy data.

## Main Contributions

The contributions of this paper are:

- It proposes a molecular-empowered learning scheme for multi-class cell segmentation using partial labels from lay annotators.
- It integrates Giga-pixel level molecular-morphology cross-modality registration, molecular-informed annotation, and molecular-oriented segmentation model, so as to achieve significantly superior performance via 3 lay annotators as compared with 2 experienced pathologists.
- It proposes a deep corrective learning method to further improve the segmentation performance using partially annotated noisy data.
- It democratizes the development of a pathological segmentation deep model to the lay annotator level, which consequently scales up the learning process similar to a non-medical computer vision task.
- It provides the official implementation and cell annotations publicly available at https://github.com/hrlblab/MolecularEL.

## Method Summary

[1]: https://arxiv.org/abs/2306.00047 "[2306.00047] Democratizing Pathological Image Segmentation with Lay ..."
[2]: https://arxiv.org/pdf/2306.00047 "Saydolimkhon Agzamkhodjaev 1. Vanderbilt University ... - arXiv.org"
[3]: http://export.arxiv.org/abs/2306.00047 "[2306.00047] Democratizing Pathological Image Segmentation with Lay ..."

Here is a summary of the method section of the paper[^1^][1]:

- The method section consists of four subsections: molecular-morphology cross-modality registration, molecular-informed annotation, molecular-oriented segmentation model, and deep corrective learning.
- The molecular-morphology cross-modality registration subsection describes how to align the molecular image (fluorescence immunohistochemistry) and the anatomical image (hematoxylin and eosin) at the Giga-pixel level using a multi-scale affine transformation and a thin-plate spline transformation.
- The molecular-informed annotation subsection explains how to use the molecular image as a reference to guide the lay annotators to label the cell types in the anatomical image. The annotators only need to label a small fraction of cells in each WSI patch.
- The molecular-oriented segmentation model subsection presents how to train a deep neural network to segment the cell types in the anatomical image using the molecular-informed annotations. The network consists of an encoder-decoder architecture with a residual attention module and a multi-class dice loss function.
- The deep corrective learning subsection proposes how to improve the segmentation performance by correcting the noisy labels from the lay annotators. The method uses a self-training strategy with a confidence threshold and a label smoothing technique.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: molecular image M and anatomical image A of the same tissue sample
# Output: segmented cell types S in A

# Step 1: Molecular-morphology cross-modality registration
# Align M and A using a multi-scale affine transformation and a thin-plate spline transformation
M_aligned, A_aligned = register(M, A)

# Step 2: Molecular-informed annotation
# Use M_aligned as a reference to guide lay annotators to label cell types in A_aligned
# Only label a small fraction of cells in each WSI patch
L = annotate(M_aligned, A_aligned)

# Step 3: Molecular-oriented segmentation model
# Train a deep neural network to segment cell types in A_aligned using L
# Use an encoder-decoder architecture with a residual attention module and a multi-class dice loss function
model = train(L, A_aligned)

# Step 4: Deep corrective learning
# Improve the segmentation performance by correcting the noisy labels from L
# Use a self-training strategy with a confidence threshold and a label smoothing technique
model = correct(model, L, A_aligned)

# Step 5: Segmentation inference
# Use the trained model to segment cell types in A_aligned
S = model.predict(A_aligned)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import numpy as np
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T

# Define constants
NUM_CLASSES = 4 # number of cell types
PATCH_SIZE = 256 # size of WSI patch
CONF_THRES = 0.9 # confidence threshold for self-training
SMOOTH_THRES = 0.1 # smoothing threshold for label smoothing
EPOCHS = 100 # number of training epochs

# Define helper functions
def register(M, A):
  # Align M and A using a multi-scale affine transformation and a thin-plate spline transformation
  # Input: molecular image M and anatomical image A of the same tissue sample
  # Output: aligned molecular image M_aligned and aligned anatomical image A_aligned

  # Convert M and A to grayscale
  M_gray = cv2.cvtColor(M, cv2.COLOR_BGR2GRAY)
  A_gray = cv2.cvtColor(A, cv2.COLOR_BGR2GRAY)

  # Downsample M and A to reduce computation cost
  M_small = cv2.resize(M_gray, (1024, 1024))
  A_small = cv2.resize(A_gray, (1024, 1024))

  # Detect keypoints and descriptors in M_small and A_small using SIFT
  sift = cv2.SIFT_create()
  kp_M, des_M = sift.detectAndCompute(M_small, None)
  kp_A, des_A = sift.detectAndCompute(A_small, None)

  # Match descriptors using FLANN
  flann = cv2.FlannBasedMatcher()
  matches = flann.knnMatch(des_M, des_A, k=2)

  # Filter matches using Lowe's ratio test
  good_matches = []
  for m, n in matches:
    if m.distance < 0.7 * n.distance:
      good_matches.append(m)

  # Extract source and destination points from good matches
  src_pts = np.float32([kp_M[m.queryIdx].pt for m in good_matches]).reshape(-1,1,2)
  dst_pts = np.float32([kp_A[m.trainIdx].pt for m in good_matches]).reshape(-1,1,2)

  # Find the affine transformation matrix H using RANSAC
  H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC)

  # Apply the affine transformation to M_small to align it with A_small
  M_small_aligned = cv2.warpPerspective(M_small, H, (1024, 1024))

  # Upsample M_small_aligned and A_small to the original size
  M_aligned = cv2.resize(M_small_aligned, (M.shape[1], M.shape[0]))
  A_aligned = cv2.resize(A_small, (A.shape[1], A.shape[0]))

  # Find the landmarks in M_aligned and A_aligned using Harris corner detector
  lm_M = cv2.cornerHarris(M_aligned, blockSize=5, ksize=3, k=0.04)
  lm_A = cv2.cornerHarris(A_aligned, blockSize=5, ksize=3, k=0.04)

  # Extract the coordinates of the landmarks above a threshold
  lm_M_coords = np.argwhere(lm_M > lm_M.max() * 0.01)
  lm_A_coords = np.argwhere(lm_A > lm_A.max() * 0.01)

  # Find the thin-plate spline transformation T using the landmarks
  T = cv2.createThinPlateSplineShapeTransformer()