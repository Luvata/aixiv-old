---
title: 2009.13856v1 Neural Alignment for Face De-pixelization
date: 2020-09-14
---

# [Neural Alignment for Face De-pixelization](http://arxiv.org/abs/2009.13856v1)

authors: Maayan Shuvi, Noa Fish, Kfir Aberman, Ariel Shamir, Daniel Cohen-Or


## What, Why and How

[1]: https://arxiv.org/pdf/2009.13856v1.pdf "Neural Alignment for Face De-pixelization - arXiv.org"
[2]: https://arxiv.org/abs/2009.13856v1 "[2009.13856v1] Neural Alignment for Face De-pixelization - arXiv.org"
[3]: https://www.researchgate.net/publication/355664544_Interplanetary_Dust_as_a_Foreground_for_the_LiteBIRD_CMB_Satellite_Mission/fulltext/6178e4a1eef53e51e1f0e037/Interplanetary-Dust-as-a-Foreground-for-the-LiteBIRD-CMB-Satellite-Mission.pdf "arXiv:2110.13856v1 [astro-ph.CO] 26 Oct 2021 - ResearchGate"

Here is a summary of the paper at [^1^][1] in terms of What, Why, and How:

- **What**: The paper presents a method to reconstruct a high-resolution video from a face-video, where the identity of a person is obscured by pixelization.
- **Why**: The paper aims to show that pixelization is not a reliable method to hide a person's identity in a video, and that neural networks can exploit the temporal and spatial information in pixelated frames to reconstruct a fairly good approximation of the original video.
- **How**: The paper employs a spatial transformation component that learns the alignment between the pixelated frames. Each frame, supported by its aligned surrounding frames, is first encoded, then decoded to a higher resolution. Reconstruction and perceptual losses promote adherence to the ground-truth, and an adversarial loss assists in maintaining domain faithfulness. There is no need for explicit temporal coherency loss as it is maintained implicitly by the alignment of neighboring frames and reconstruction.

## Main Contributions

[1]: https://arxiv.org/pdf/2009.13856v1.pdf "Neural Alignment for Face De-pixelization - arXiv.org"
[2]: https://arxiv.org/abs/2009.13856v1 "[2009.13856v1] Neural Alignment for Face De-pixelization - arXiv.org"
[3]: https://www.researchgate.net/publication/355664544_Interplanetary_Dust_as_a_Foreground_for_the_LiteBIRD_CMB_Satellite_Mission/fulltext/6178e4a1eef53e51e1f0e037/Interplanetary-Dust-as-a-Foreground-for-the-LiteBIRD-CMB-Satellite-Mission.pdf "arXiv:2110.13856v1 [astro-ph.CO] 26 Oct 2021 - ResearchGate"

According to the paper at [^1^][1], the main contributions are:

- **A simple and effective method** to reconstruct a high-resolution video from a face-video, where the identity of a person is obscured by pixelization.
- **A spatial transformation component** that learns the alignment between the pixelated frames, and exploits the temporal and spatial information in pixelated frames to reconstruct a fairly good approximation of the original video.
- **A demonstration** that pixelization is not a reliable method to hide a person's identity in a video, and that neural networks can compromise anonymity by synthesizing high-quality face reconstructions.

## Method Summary

[1]: https://arxiv.org/pdf/2009.13856v1.pdf "Neural Alignment for Face De-pixelization - arXiv.org"
[2]: https://arxiv.org/abs/2009.13856v1 "[2009.13856v1] Neural Alignment for Face De-pixelization - arXiv.org"
[3]: https://www.researchgate.net/publication/355664544_Interplanetary_Dust_as_a_Foreground_for_the_LiteBIRD_CMB_Satellite_Mission/fulltext/6178e4a1eef53e51e1f0e037/Interplanetary-Dust-as-a-Foreground-for-the-LiteBIRD-CMB-Satellite-Mission.pdf "arXiv:2110.13856v1 [astro-ph.CO] 26 Oct 2021 - ResearchGate"

Here is a summary of the method section of the paper at [^1^][1]:

- The paper proposes a **neural alignment and reconstruction (NAR) framework** that consists of three main components: an **encoder**, a **decoder**, and a **spatial transformer network (STN)**.
- The encoder takes a pixelated frame and its surrounding frames as input, and produces a latent representation for each frame. The decoder takes the latent representation of the target frame and generates a high-resolution output. The STN takes the latent representations of the surrounding frames and aligns them with the target frame using learned affine transformations.
- The paper uses a combination of losses to train the NAR framework: a **reconstruction loss** that measures the pixel-wise difference between the output and the ground-truth, a **perceptual loss** that measures the feature-wise difference between the output and the ground-truth using a pre-trained VGG network, and an **adversarial loss** that measures the realism of the output using a discriminator network.
- The paper evaluates the NAR framework on two datasets: VoxCeleb2 and YouTube Faces. The paper compares the NAR framework with several baselines, including bicubic interpolation, SRGAN, ESRGAN, and FSRNet. The paper uses both quantitative and qualitative metrics to measure the performance of the NAR framework and the baselines.

## Pseudo Code - High level

Here is the high-level pseudo code for the paper:

```python
# Define the encoder network
encoder = Encoder()

# Define the decoder network
decoder = Decoder()

# Define the spatial transformer network
stn = STN()

# Define the discriminator network
discriminator = Discriminator()

# Define the reconstruction loss function
reconstruction_loss = L1Loss()

# Define the perceptual loss function
perceptual_loss = VGGPerceptualLoss()

# Define the adversarial loss function
adversarial_loss = HingeLoss()

# Define the optimizer for the generator (encoder + decoder + stn)
generator_optimizer = Adam(encoder.parameters() + decoder.parameters() + stn.parameters())

# Define the optimizer for the discriminator
discriminator_optimizer = Adam(discriminator.parameters())

# Loop over the epochs
for epoch in range(num_epochs):

  # Loop over the batches
  for batch in dataloader:

    # Get the pixelated frames and the ground-truth frames
    pixelated_frames, ground_truth_frames = batch

    # Get the target frame and the surrounding frames
    target_frame = pixelated_frames[:, 0, :, :, :]
    surrounding_frames = pixelated_frames[:, 1:, :, :, :]

    # Encode the target frame and the surrounding frames
    target_latent = encoder(target_frame)
    surrounding_latents = encoder(surrounding_frames)

    # Align the surrounding latents with the target latent using STN
    aligned_latents = stn(surrounding_latents, target_latent)

    # Concatenate the target latent and the aligned latents
    concatenated_latent = torch.cat([target_latent, aligned_latents], dim=1)

    # Decode the concatenated latent to get the output frame
    output_frame = decoder(concatenated_latent)

    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(output_frame, ground_truth_frame)

    # Compute the perceptual loss
    per_loss = perceptual_loss(output_frame, ground_truth_frame)

    # Compute the adversarial loss for the generator
    gen_adv_loss = adversarial_loss(discriminator(output_frame), True)

    # Compute the total generator loss
    gen_loss = rec_loss + per_loss + gen_adv_loss

    # Update the generator parameters
    generator_optimizer.zero_grad()
    gen_loss.backward()
    generator_optimizer.step()

    # Compute the adversarial loss for the discriminator
    real_adv_loss = adversarial_loss(discriminator(ground_truth_frame), True)
    fake_adv_loss = adversarial_loss(discriminator(output_frame.detach()), False)

    # Compute the total discriminator loss
    dis_loss = real_adv_loss + fake_adv_loss

    # Update the discriminator parameters
    discriminator_optimizer.zero_grad()
    dis_loss.backward()
    discriminator_optimizer.step()
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement the paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models

# Define the encoder network
class Encoder(nn.Module):

  def __init__(self):
    super(Encoder, self).__init__()

    # Define the convolutional layers
    self.conv1 = nn.Conv2d(3, 64, 3, 1, 1)
    self.conv2 = nn.Conv2d(64, 128, 3, 2, 1)
    self.conv3 = nn.Conv2d(128, 256, 3, 2, 1)
    self.conv4 = nn.Conv2d(256, 512, 3, 2, 1)

    # Define the batch normalization layers
    self.bn1 = nn.BatchNorm2d(64)
    self.bn2 = nn.BatchNorm2d(128)
    self.bn3 = nn.BatchNorm2d(256)
    self.bn4 = nn.BatchNorm2d(512)

    # Define the leaky ReLU activation function
    self.lrelu = nn.LeakyReLU(0.2)

  def forward(self, x):
    # Apply the convolutional layers with batch normalization and leaky ReLU
    x = self.lrelu(self.bn1(self.conv1(x)))
    x = self.lrelu(self.bn2(self.conv2(x)))
    x = self.lrelu(self.bn3(self.conv3(x)))
    x = self.lrelu(self.bn4(self.conv4(x)))

    # Return the latent representation
    return x

# Define the decoder network
class Decoder(nn.Module):

  def __init__(self):
    super(Decoder, self).__init__()

    # Define the deconvolutional layers
    self.deconv1 = nn.ConvTranspose2d(512 * (num_surrounding_frames + 1), 256, 4, 2, 1)
    self.deconv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)
    self.deconv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)
    self.deconv4 = nn.ConvTranspose2d(64, 3, 3, 1, 1)

    # Define the batch normalization layers
    self.bn1 = nn.BatchNorm2d(256)
    self.bn2 = nn.BatchNorm2d(128)
    self.bn3 = nn.BatchNorm2d(64)

    # Define the ReLU activation function
    self.relu = nn.ReLU()

    # Define the tanh activation function
    self.tanh = nn.Tanh()

  def forward(self, x):
    # Apply the deconvolutional layers with batch normalization and ReLU
    x = self.relu(self.bn1(self.deconv1(x)))
    x = self.relu(self.bn2(self.deconv2(x)))
    x = self.relu(self.bn3(self.deconv3(x)))

    # Apply the final deconvolutional layer with tanh
    x = self.tanh(self.deconv4(x))

    # Return the output frame
    return x

# Define the spatial transformer network
class STN(nn.Module):

  def __init__(self):
    super(STN, self).__init__()

    # Define the fully connected layers to predict the affine transformation parameters
    self.fc1 = nn.Linear(512 * image_height * image_width * num_surrounding_frames * 2 , num_surrounding_frames * num_affine_params)

  def forward(self, surrounding_latents , target_latent):
    
     # Flatten the surrounding latents and concatenate them with the target latent
     surrounding_latents_flat = surrounding_latents.view(-1 , num_surrounding_frames , image_height * image_width * num_channels)
     target_latent_flat = target_latent.view(-1 , image_height * image_width * num_channels)
     concatenated_latent_flat = torch.cat([surrounding_latents_flat , target_latent_flat] , dim= -1)

     # Apply the fully connected layer to get the affine transformation parameters for each surrounding latent
     theta_flat = self.fc1(concatenated_latent_flat)

     # Reshape the affine transformation parameters to a matrix form
     theta_matrix = theta_flat.view(-1 , num_surrounding_frames , num_affine_params // num_affine_rows , num_affine_rows)

     # Apply the affine transformation to each surrounding latent using a grid sampler
     grid = F.affine_grid(theta_matrix , surrounding_latents.size())
     aligned_latents = F.grid_sample(surrounding_latents , grid)

     # Return the aligned latents
     return aligned_latents

# Define the discriminator network
class Discriminator(nn.Module):

  def __init__(self):
    super(Discriminator, self).__init__()

    # Define the convolutional layers
    self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)
    self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)
    self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)
    self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)
    self.conv5 = nn.Conv2d(512, 1, 4, 1, 0)

    # Define the batch normalization layers
    self.bn1 = nn.BatchNorm2d(64)
    self.bn2 = nn.BatchNorm2d(128)
    self.bn3 = nn.BatchNorm2d(256)
    self.bn4 = nn.BatchNorm2d(512)

    # Define the leaky ReLU activation function
    self.lrelu = nn.LeakyReLU(0.2)

  def forward(self, x):
    # Apply the convolutional layers with batch normalization and leaky ReLU
    x = self.lrelu(self.bn1(self.conv1(x)))
    x = self.lrelu(self.bn2(self.conv2(x)))
    x = self.lrelu(self.bn3(self.conv3(x)))
    x = self.lrelu(self.bn4(self.conv4(x)))

    # Apply the final convolutional layer
    x = self.conv5(x)

    # Return the output
    return x

# Define the reconstruction loss function
reconstruction_loss = L1Loss()

# Define the perceptual loss function
perceptual_loss = VGGPerceptualLoss()

# Define the adversarial loss function
adversarial_loss = HingeLoss()

# Define the optimizer for the generator (encoder + decoder + stn)
generator_optimizer = Adam(encoder.parameters() + decoder.parameters() + stn.parameters())

# Define the optimizer for the discriminator
discriminator_optimizer = Adam(discriminator.parameters())

# Loop over the epochs
for epoch in range(num_epochs):

  # Loop over the batches
  for batch in dataloader:

    # Get the pixelated frames and the ground-truth frames
    pixelated_frames, ground_truth_frames = batch

    # Get the target frame and the surrounding frames
    target_frame = pixelated_frames[:, 0, :, :, :]
    surrounding_frames = pixelated_frames[:, 1:, :, :, :]

    # Encode the target frame and the surrounding frames
    target_latent = encoder(target_frame)
    surrounding_latents = encoder(surrounding_frames)

    # Align the surrounding latents with the target latent using STN
    aligned_latents = stn(surrounding_latents, target_latent)

    # Concatenate the target latent and the aligned latents
    concatenated_latent = torch.cat([target_latent, aligned_latents], dim=1)

    # Decode the concatenated latent to get the output frame
    output_frame = decoder(concatenated_latent)

    # Compute the reconstruction loss
    rec_loss = reconstruction_loss(output_frame, ground_truth_frame)

    # Compute the perceptual loss
    per_loss = perceptual_loss(output_frame, ground_truth_frame)

    # Compute the adversarial loss for the generator
    gen_adv_loss = adversarial_loss(discriminator(output_frame), True)

    # Compute the total generator loss
    gen_loss = rec_loss + per_loss + gen_adv_loss

    # Update the generator parameters
    generator_optimizer.zero_grad()
    gen_loss.backward()
    generator_optimizer.step()

    # Compute the adversarial loss for the discriminator
    real_adv_loss = adversarial_loss(discriminator(ground_truth_frame), True)
    fake_adv_loss = adversarial_loss(discriminator(output_frame.detach()), False)

    # Compute the total discriminator loss
    dis_loss = real_adv_loss + fake_adv_loss

    # Update the discriminator parameters
    discriminator_optimizer.zero_grad()
    dis_loss.backward()
    discriminator_optimizer.step()
```