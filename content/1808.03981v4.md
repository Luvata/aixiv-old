---
title: 1808.03981v4 SAGNet Structure-aware Generative Network for 3D-Shape Modeling
date: 2018-08-04
---

# [SAGNet:Structure-aware Generative Network for 3D-Shape Modeling](http://arxiv.org/abs/1808.03981v4)

authors: Zhijie Wu, Xiang Wang, Di Lin, Dani Lischinski, Daniel Cohen-Or, Hui Huang


## What, Why and How

[1]: https://arxiv.org/pdf/1808.03981v4 "SAGNet: Structure-aware Generative Network for 3D-Shape Modeling"
[2]: https://arxiv.org/abs/1808.03981 "SAGNet:Structure-aware Generative Network for 3D-Shape Modeling"
[3]: https://arxiv-export2.library.cornell.edu/abs/1511.03981 "[1511.03981] Disconnected, fragmented, or united? A trans-disciplinary ..."

Here is a summary of the paper at [^1^][1]:

- **What**: The paper presents SAGNet, a structure-aware generative model for 3D shapes. SAGNet can learn and embed the geometry and structure of 3D models in a latent space, and generate new models by controlling the geometry and structure features separately.
- **Why**: The paper aims to address the challenge of generating realistic and diverse 3D shapes that preserve both geometric and structural similarity to existing models. Existing methods either focus on geometry or structure, but not both, or do not allow separate control over the two aspects.
- **How**: The paper proposes an autoencoder network that consists of two branches: one for geometry and one for structure. The network takes segmented 3D shapes as input and outputs reconstructed shapes. The encoder intertwines the geometry and structure features into a single latent code, while the decoder disentangles the features and reconstructs the geometry and structure of the 3D model. The key idea is that during the analysis, the two branches exchange information between them, thereby learning the dependencies between structure and geometry and encoding two augmented features, which are then fused into a single latent code. This explicit intertwining of information enables separately controlling the geometry and the structure of the generated models. The paper evaluates the performance of SAGNet and conducts an ablation study. It also shows a variety of quality results generated by SAGNet.

## Main Contributions

The paper claims the following contributions:

- A novel structure-aware generative model for 3D shapes that jointly learns and embeds the geometry and structure of 3D models in a latent space.
- A novel autoencoder network that intertwines and disentangles the geometry and structure features using information exchange between two branches.
- A novel method to separately control the geometry and structure of the generated models by manipulating the latent code.
- A comprehensive evaluation and ablation study of SAGNet and a comparison with existing methods.
- A demonstration of various applications of SAGNet, such as shape interpolation, shape completion, shape editing, and shape synthesis.

## Method Summary

The method section of the paper describes the details of SAGNet, which consists of an encoder and a decoder. The encoder takes a segmented 3D shape as input and outputs a latent code that encodes both geometry and structure features. The decoder takes the latent code as input and outputs a reconstructed 3D shape that preserves the geometry and structure of the input shape. The encoder and decoder have two branches: one for geometry and one for structure. The geometry branch operates on voxel grids that represent the shape parts, while the structure branch operates on adjacency matrices that represent the pairwise relationships between the parts. The encoder uses 3D convolutional layers to extract geometry features and fully-connected layers to extract structure features. The decoder uses transposed 3D convolutional layers to reconstruct geometry features and fully-connected layers to reconstruct structure features. The key idea of SAGNet is that during the encoding process, the two branches exchange information between them, thereby learning the dependencies between structure and geometry and encoding two augmented features, which are then fused into a single latent code. This explicit intertwining of information enables separately controlling the geometry and the structure of the generated models by manipulating the latent code. The paper also describes the loss functions used to train SAGNet, which include reconstruction loss, KL-divergence loss, cycle-consistency loss, and part-alignment loss. The paper also provides implementation details and hyperparameters of SAGNet.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the encoder network
def encoder(input_shape):
  # Split the input shape into voxel grids and adjacency matrices
  voxel_grids, adjacency_matrices = split(input_shape)
  # Extract geometry features using 3D convolutional layers
  geometry_features = conv3d(voxel_grids)
  # Extract structure features using fully-connected layers
  structure_features = fc(adjacency_matrices)
  # Exchange information between geometry and structure branches
  geometry_features, structure_features = exchange(geometry_features, structure_features)
  # Fuse the augmented features into a single latent code
  latent_code = fuse(geometry_features, structure_features)
  return latent_code

# Define the decoder network
def decoder(latent_code):
  # Split the latent code into geometry and structure features
  geometry_features, structure_features = split(latent_code)
  # Reconstruct geometry features using transposed 3D convolutional layers
  voxel_grids = deconv3d(geometry_features)
  # Reconstruct structure features using fully-connected layers
  adjacency_matrices = fc(structure_features)
  # Merge the voxel grids and adjacency matrices into a reconstructed shape
  output_shape = merge(voxel_grids, adjacency_matrices)
  return output_shape

# Define the loss functions
def reconstruction_loss(input_shape, output_shape):
  # Compute the voxel-wise L2 loss between input and output shapes
  voxel_loss = l2_loss(input_shape.voxel_grids, output_shape.voxel_grids)
  # Compute the element-wise cross-entropy loss between input and output adjacency matrices
  adjacency_loss = cross_entropy_loss(input_shape.adjacency_matrices, output_shape.adjacency_matrices)
  # Combine the voxel loss and adjacency loss with a weight factor
  return voxel_loss + weight * adjacency_loss

def kl_divergence_loss(latent_code):
  # Compute the KL-divergence loss between the latent code and a standard normal distribution
  return kl_loss(latent_code, normal_distribution)

def cycle_consistency_loss(input_shape):
  # Encode the input shape into a latent code
  latent_code = encoder(input_shape)
  # Decode the latent code into a reconstructed shape
  output_shape = decoder(latent_code)
  # Encode the reconstructed shape into another latent code
  latent_code_prime = encoder(output_shape)
  # Compute the L2 loss between the original and reconstructed latent codes
  return l2_loss(latent_code, latent_code_prime)

def part_alignment_loss(output_shape):
  # Compute the part centroids of the output shape
  part_centroids = compute_centroids(output_shape.voxel_grids)
  # Compute the pairwise distances between part centroids
  pairwise_distances = compute_distances(part_centroids)
  # Compute the L2 loss between the pairwise distances and a predefined distance matrix
  return l2_loss(pairwise_distances, distance_matrix)

# Define the total loss function as a weighted sum of individual losses
def total_loss(input_shape, output_shape, latent_code):
  return reconstruction_loss(input_shape, output_shape) + beta * kl_divergence_loss(latent_code) + gamma * cycle_consistency_loss(input_shape) + delta * part_alignment_loss(output_shape)

# Train SAGNet using gradient descent to minimize the total loss function
for epoch in epochs:
  for batch in batches:
    input_shapes = get_input_shapes(batch)
    latent_codes = encoder(input_shapes)
    output_shapes = decoder(latent_codes)
    loss = total_loss(input_shapes, output_shapes, latent_codes)
    update_parameters(loss)

# Generate new shapes by sampling latent codes from a normal distribution and decoding them
for sample in samples:
  latent_code = sample_from(normal_distribution)
  output_shape = decoder(latent_code)
```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# Define the hyperparameters
batch_size = 32 # The number of shapes in each batch
latent_dim = 256 # The dimension of the latent code
voxel_dim = 32 # The dimension of the voxel grid for each part
part_num = 4 # The number of parts for each shape
weight = 0.1 # The weight factor for the adjacency loss
beta = 0.01 # The weight factor for the KL-divergence loss
gamma = 0.1 # The weight factor for the cycle-consistency loss
delta = 0.01 # The weight factor for the part-alignment loss
distance_matrix = np.array([[0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 0, 1], [1, 1, 1, 0]]) # The predefined distance matrix for part centroids
learning_rate = 0.001 # The learning rate for gradient descent
epochs = 100 # The number of epochs for training

# Define the encoder network
class Encoder(nn.Module):
  def __init__(self):
    super(Encoder, self).__init__()
    # Define the geometry branch
    self.conv1_g = nn.Conv3d(part_num, 64, kernel_size=4, stride=2, padding=1) # A 3D convolutional layer with 64 filters and a kernel size of 4x4x4
    self.conv2_g = nn.Conv3d(64, 128, kernel_size=4, stride=2, padding=1) # A 3D convolutional layer with 128 filters and a kernel size of 4x4x4
    self.conv3_g = nn.Conv3d(128, 256, kernel_size=4, stride=2, padding=1) # A 3D convolutional layer with 256 filters and a kernel size of 4x4x4
    self.fc_g = nn.Linear(256 * (voxel_dim // 8) ** 3, latent_dim // 2) # A fully-connected layer with latent_dim // 2 output units
    # Define the structure branch
    self.fc1_s = nn.Linear(part_num ** 2, latent_dim // 2) # A fully-connected layer with latent_dim // 2 output units
    self.fc2_s = nn.Linear(latent_dim // 2, latent_dim // 2) # A fully-connected layer with latent_dim // 2 output units

    # Define the information exchange layers
    self.fc_g_to_s = nn.Linear(latent_dim // 2, latent_dim // 2) # A fully-connected layer to map geometry features to structure features
    self.fc_s_to_g = nn.Linear(latent_dim // 2, latent_dim // 2) # A fully-connected layer to map structure features to geometry features

    # Define the feature fusion layer
    self.fc_fuse = nn.Linear(latent_dim, latent_dim) # A fully-connected layer to fuse geometry and structure features

    # Define the batch normalization and activation layers
    self.bn1_g = nn.BatchNorm3d(64) # A batch normalization layer for the geometry branch
    self.bn2_g = nn.BatchNorm3d(128) # A batch normalization layer for the geometry branch
    self.bn3_g = nn.BatchNorm3d(256) # A batch normalization layer for the geometry branch
    self.bn_g = nn.BatchNorm1d(latent_dim // 2) # A batch normalization layer for the geometry branch

    self.bn1_s = nn.BatchNorm1d(latent_dim // 2) # A batch normalization layer for the structure branch
    self.bn2_s = nn.BatchNorm1d(latent_dim // 2) # A batch normalization layer for the structure branch

    self.relu = nn.ReLU() # A ReLU activation layer

  
  def forward(self, input_shape):
    # Split the input shape into voxel grids and adjacency matrices
    voxel_grids = input_shape[:, :part_num] # Shape: (batch_size, part_num, voxel_dim, voxel_dim, voxel_dim)
    adjacency_matrices = input_shape[:, part_num:] # Shape: (batch_size, part_num ** 2)

    # Extract geometry features using 3D convolutional layers and a fully-connected layer
    x_g = self.conv1_g(voxel_grids) # Shape: (batch_size, 64, voxel_dim // 2, voxel_dim // 2, voxel_dim // 2)
    x_g = self.bn1_g(x_g) # Shape: (batch_size, 64, voxel_dim // 2, voxel_dim // 2, voxel_dim // 2)
    x_g = self.relu(x_g) # Shape: (batch_size, 64, voxel_dim // 2, voxel_dim // 2, voxel_dim // 2)
    x_g = self.conv2_g(x_g) # Shape: (batch_size, 128, voxel_dim // 4, voxel_dim // 4, voxel_dim // 4)
    x_g = self.bn2_g(x_g) # Shape: (batch_size, 128, voxel_dim // 4, voxel_dim // 4, voxel_dim // 4)
    x_g = self.relu(x_g) # Shape: (batch_size, 128, voxel_dim // 4, voxel_dim // 4, voxel_dim // 4)
    x_g = self.conv3_g(x_g) # Shape: (batch_size, 256, voxel_dim // 8, voxel_dim // 8, voxel_dim // 8)
    x_g = self.bn3_g(x_g) # Shape: (batch_size, 256, voxel_dim // 8, voxel_dim // 8, voxel_dim // 8)
    x_g = self.relu(x_g) # Shape: (batch_size, 256, voxel_dim // 8, voxel_dim // 8, voxel_dim // 8)
    x_g = x_g.view(batch_size, -1) # Shape: (batch_size, 256 * (voxel_dim // 8) ** 3)
    x_g = self.fc_g(x_g) # Shape: (batch_size, latent_dim // 2)
    x_g = self.bn_g(x_g) # Shape: (batch_size, latent_dim // 2)
    x_g = self.relu(x_g) # Shape: (batch_size, latent_dim // 2)

    # Extract structure features using fully-connected layers
    x_s = self.fc1_s(adjacency_matrices) # Shape: (batch_size, latent_dim // 2)
    x_s = self.bn1_s(x_s) # Shape: (batch_size, latent_dim // 2)
    x_s = self.relu(x_s) # Shape: (batch_size, latent_dim // 2)
    x_s = self.fc2_s(x_s) # Shape: (batch_size, latent_dim // 2)
    x_s = self.bn2_s(x_s) # Shape: (batch_size, latent_dim // 2)
    x_s = self.relu(x_s) # Shape: (batch_size, latent_dim // 2)

    # Exchange information between geometry and structure branches
    y_s = x_s + self.fc_g_to_s(x_g) # Shape: (batch_size, latent_dim // 2)
    y_s = self.relu(y_s) # Shape: (batch_size, latent_dim // 2)
    y_g = x_g + self.fc_s_to_g(x_s) # Shape: (batch_size, latent_dim // 2)
    y_g = self.relu(y_g) # Shape: (batch_size, latent_dim // 2)

    # Fuse the augmented features into a single latent code
    z = torch.cat([y_g, y_s], dim=1) # Shape: (batch_size, latent_dim)
    z = self.fc_fuse(z) # Shape: (batch_size, latent_dim)

    return z

# Define the decoder network
class Decoder(nn.Module):
  
```