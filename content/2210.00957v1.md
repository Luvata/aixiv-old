---
title: 2210.00957v1 UnGANable  Defending Against GAN-based Face Manipulation
date: 2022-10-01
---

# [UnGANable: Defending Against GAN-based Face Manipulation](http://arxiv.org/abs/2210.00957v1)

authors: Zheng Li, Ning Yu, Ahmed Salem, Michael Backes, Mario Fritz, Yang Zhang


## What, Why and How

[1]: https://arxiv.org/abs/2210.00957 "UnGANable: Defending Against GAN-based Face Manipulation"
[2]: https://arxiv.org/pdf/2210.00957.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2301.00957v1 "[2301.00957v1] Meta-learning generalizable dynamics from trajectories"

Here is a summary of the paper:

- **What**: The paper proposes UnGANable, a defense system against GAN-inversion-based face manipulation, which is a technique to modify facial attributes in an image using GANs.
- **Why**: The paper aims to protect individuals' facial images from being misused by malicious manipulators who can generate deepfakes for misinformation or other harmful purposes.
- **How**: The paper's core technique is to search for alternative images (called cloaked images) around the original images (called target images) in image space. These cloaked images can jeopardize the GAN inversion process, which is an essential step for face manipulation. The paper considers two state-of-the-art inversion techniques and designs five different defenses under five scenarios depending on the defender's background knowledge. The paper evaluates the effectiveness and utility of UnGANable on four popular GAN models trained on two benchmark face datasets and compares it with multiple baseline methods. The paper also investigates four adaptive adversaries to bypass UnGANable and shows that some of them are slightly effective.

This summary is based on the information from [^1^][1] and [^2^][2].

## Main Contributions

[1]: https://arxiv.org/abs/2210.00957 "UnGANable: Defending Against GAN-based Face Manipulation"
[2]: https://arxiv.org/pdf/2210.00957.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2301.00957v1 "[2301.00957v1] Meta-learning generalizable dynamics from trajectories"

According to [^1^][1] and [^2^][2], the contributions of this paper are:

- The paper proposes the **first defense system** against GAN-inversion-based face manipulation, which is a novel and emerging threat of deepfakes.
- The paper introduces a **core technique** of searching for cloaked images around the target images in image space, which can effectively disrupt the GAN inversion process without compromising the utility of the target images.
- The paper considers **two state-of-the-art inversion techniques** and designs **five different defenses** under five scenarios depending on the defender's background knowledge, covering a comprehensive range of possible situations.
- The paper conducts **extensive experiments** on four popular GAN models trained on two benchmark face datasets and shows that UnGANable achieves remarkable effectiveness and utility performance, and outperforms multiple baseline methods.
- The paper investigates **four adaptive adversaries** to bypass UnGANable and shows that some of them are slightly effective, providing insights for future research directions.

## Method Summary

[1]: https://arxiv.org/abs/2210.00957 "UnGANable: Defending Against GAN-based Face Manipulation"
[2]: https://arxiv.org/pdf/2210.00957.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/2301.00957v1 "[2301.00957v1] Meta-learning generalizable dynamics from trajectories"

Here is a summary of the method section of the paper:

- The paper first introduces the **preliminaries** of GANs, GAN inversion, and face manipulation, and defines the **threat model** and the **evaluation metrics** for the defense system.
- The paper then presents the **core technique** of UnGANable, which is to search for cloaked images around the target images in image space. The paper explains how to measure the **distance** between the cloaked image and the target image, and how to ensure the **utility** of the cloaked image for benign applications.
- The paper then describes **five different defenses** under five scenarios depending on the defender's background knowledge. The paper explains how to choose the **search direction**, the **search step size**, and the **search termination condition** for each defense. The paper also provides a **theoretical analysis** of the effectiveness and utility guarantees of each defense.
- The paper then discusses **four adaptive adversaries** who can try to bypass UnGANable by using different strategies, such as using a different GAN model, using a different inversion technique, using multiple inversion attempts, or using a pre-trained classifier. The paper explains how UnGANable can be adapted to counter these adversaries.

This summary is based on the information from [^1^][2].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: a target image x, a GAN model G, an inversion technique I, a defense scenario S
# Output: a cloaked image x_c

# Step 1: Initialize the search direction d, the search step size s, and the search termination condition c according to S
d = get_search_direction(x, G, I, S)
s = get_search_step_size(x, G, I, S)
c = get_search_termination_condition(x, G, I, S)

# Step 2: Search for a cloaked image x_c around x in image space
x_c = x # initialize the cloaked image as the target image
while not c(x_c): # loop until the termination condition is met
  x_c = x_c + s * d # update the cloaked image by taking a step along the search direction
  x_c = clip(x_c) # clip the cloaked image to ensure it is in the valid range
  d = update_search_direction(x_c, G, I, S) # update the search direction if needed

# Step 3: Return the cloaked image x_c
return x_c
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: a target image x, a GAN model G, an inversion technique I, a defense scenario S
# Output: a cloaked image x_c

# Step 1: Initialize the search direction d, the search step size s, and the search termination condition c according to S
if S == "Scenario 1": # defender knows nothing
  d = random_normal_vector() # use a random normal vector as the search direction
  s = 0.01 # use a small constant step size
  c = lambda x_c: distance(x_c, x) > epsilon # terminate when the distance between x_c and x exceeds a threshold epsilon
elif S == "Scenario 2": # defender knows G
  d = gradient(x, G) # use the gradient of G with respect to x as the search direction
  s = 0.01 # use a small constant step size
  c = lambda x_c: distance(x_c, x) > epsilon # terminate when the distance between x_c and x exceeds a threshold epsilon
elif S == "Scenario 3": # defender knows G and I
  d = gradient(x, G) # use the gradient of G with respect to x as the search direction
  s = adaptive_step_size(x, G, I) # use an adaptive step size based on the inversion loss of I on x
  c = lambda x_c: inversion_loss(x_c, G, I) > delta # terminate when the inversion loss of I on x_c exceeds a threshold delta
elif S == "Scenario 4": # defender knows G and I and has access to multiple images from the same person
  d = average_gradient(X, G) # use the average gradient of G with respect to a set of images X from the same person as x as the search direction
  s = adaptive_step_size(x, G, I) # use an adaptive step size based on the inversion loss of I on x
  c = lambda x_c: inversion_loss(x_c, G, I) > delta # terminate when the inversion loss of I on x_c exceeds a threshold delta
elif S == "Scenario 5": # defender knows G and I and has access to multiple images from different people with similar attributes as x
  d = average_gradient(X, G) # use the average gradient of G with respect to a set of images X from different people with similar attributes as x as the search direction
  s = adaptive_step_size(x, G, I) # use an adaptive step size based on the inversion loss of I on x
  c = lambda x_c: inversion_loss(x_c, G, I) > delta # terminate when the inversion loss of I on x_c exceeds a threshold delta

# Step 2: Search for a cloaked image x_c around x in image space
x_c = x # initialize the cloaked image as the target image
while not c(x_c): # loop until the termination condition is met
  x_c = x_c + s * d # update the cloaked image by taking a step along the search direction
  x_c = clip(x_c) # clip the cloaked image to ensure it is in the valid range [0,1]
  if S == "Scenario 3" or S == "Scenario 4" or S == "Scenario 5": # update the search direction if needed
    d = update_search_direction(x_c, G, I, S)

# Step 3: Return the cloaked image x_c
return x_c

# Helper functions

def distance(x1, x2):
  # compute the L2 distance between two images x1 and x2
  return sqrt(sum((x1 - x2)**2))

def gradient(x, G):
  # compute the gradient of G with respect to an image x using backpropagation
  z = encode(x, G) # encode x into a latent code z using G's encoder
  y = decode(z, G) # decode z into an image y using G's decoder
  L = reconstruction_loss(y, x) # compute the reconstruction loss between y and x using MSE or LPIPS
  return backprop(L, x) # compute the gradient of L with respect to x using backpropagation

def average_gradient(X, G):
  # compute the average gradient of G with respect to a set of images X using backpropagation
  Z = encode(X, G) # encode X into a set of latent codes Z using G's encoder
  Y = decode(Z, G) # decode Z into a set of images Y using G's decoder
  L = reconstruction_loss(Y, X) # compute the reconstruction loss between Y and X using MSE or LPIPS
  return backprop(L, X) / len(X) # compute the average gradient of L with respect to X using backpropagation

def adaptive_step_size(x, G, I):
  # compute an adaptive step size based on the inversion loss of I on x
  z = invert(x, G, I) # invert x into a latent code z using I
  y = decode(z, G) # decode z into an image y using G's decoder
  L = inversion_loss(y, x) # compute the inversion loss between y and x using MSE or LPIPS
  return alpha / sqrt(L + beta) # compute the adaptive step size using a formula with two hyperparameters alpha and beta

def update_search_direction(x_c, G, I, S):
  # update the search direction based on the current cloaked image x_c, the GAN model G, the inversion technique I, and the defense scenario S
  if S == "Scenario 3": # use the gradient of I with respect to x_c as the search direction
    return gradient(x_c, I)
  elif S == "Scenario 4" or S == "Scenario 5": # use the average gradient of I with respect to a set of images X_c that are cloaked versions of X as the search direction
    X_c = cloak(X, G, I, S) # cloak a set of images X from the same person or different people with similar attributes as x_c using the same defense scenario S
    return average_gradient(X_c, I)
  else: # no need to update the search direction
    return None

def clip(x):
  # clip an image x to ensure it is in the valid range [0,1]
  return min(max(x, 0), 1)

def encode(x, G):
  # encode an image x into a latent code z using G's encoder
  return G.encoder(x)

def decode(z, G):
  # decode a latent code z into an image y using G's decoder
  return G.decoder(z)

def invert(x, G, I):
  # invert an image x into a latent code z using an inversion technique I
  if I == "optimization-based inversion": # use an optimization-based method such as ZO-Adam or GD to minimize the reconstruction loss between x and G(z)
    z = random_normal_vector() # initialize z as a random normal vector
    for t in range(T): # loop for T iterations
      y = decode(z, G) # decode z into an image y using G's decoder
      L = reconstruction_loss(y, x) # compute the reconstruction loss between y and x using MSE or LPIPS
      g = gradient(L, z) # compute the gradient of L with respect to z using backpropagation
      z = z - eta * g # update z by taking a step along the negative gradient direction with a learning rate eta
    return z
  elif I == "hybrid inversion": # use a hybrid method that combines optimization-based and encoder-based methods such as ReStyle or pSp to minimize the reconstruction loss between x and G(E(x)+z)
    z = random_normal_vector() # initialize z as a random normal vector
    for t in range(T): # loop for T iterations
      w = encode(x, G) + z # compute w as the sum of E(x) and z
      y = decode(w, G) # decode w into an image y using G's decoder
      L = reconstruction_loss(y, x) # compute the reconstruction loss between y and x using MSE or LPIPS
      g = gradient(L, z) # compute the gradient of L with respect to z using backpropagation
      z = z - eta * g # update z by taking a step along the negative gradient direction with a learning rate eta
    return w

def reconstruction_loss(y, x):
  # compute the reconstruction loss between two images y and x using MSE or LPIPS
  if use_MSE: # use mean squared error as the loss function
    return mean((y - x)**2)
  elif use_LPIPS: # use learned perceptual image patch similarity as the loss function
    return LPIPS(y, x)

def inversion_loss(y, x):
  # compute the inversion loss between two images y and x using MSE or LPIPS
  return reconstruction_loss(y, x)

def cloak(X, G, I, S):
  # cloak a set of images X using UnGANable with a given GAN model G, an inversion technique I, and a defense scenario S
  X_c = [] # initialize an empty list for cloaked images
  for x in X: # loop over each image in X

```