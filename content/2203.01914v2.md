---
title: 2203.01914v2 Playable Environments  Video Manipulation in Space and Time
date: 2022-03-02
---

# [Playable Environments: Video Manipulation in Space and Time](http://arxiv.org/abs/2203.01914v2)

authors: Willi Menapace, Stéphane Lathuilière, Aliaksandr Siarohin, Christian Theobalt, Sergey Tulyakov, Vladislav Golyanik, Elisa Ricci


## What, Why and How

[1]: https://arxiv.org/abs/2203.01914v2 "Playable Environments: Video Manipulation in Space and Time"
[2]: https://arxiv.org/pdf/2203.01914v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.01914 "[2203.01914] Playable Environments: Video Manipulation in Space and Time"

Here is a summary of the paper:

- **What**: The paper presents a new representation for interactive video generation and manipulation in space and time, called Playable Environments[^1^][1].
- **Why**: The paper aims to enable creative applications that are not attainable by prior video synthesis works, such as playable 3D video generation, stylization and manipulation[^1^][1].
- **How**: The paper proposes a novel framework that allows the user to move objects in 3D while generating a video by providing a sequence of desired actions, with a single image at inference time[^1^][1]. The framework consists of three main components: an environment state module that builds a 3D representation of the scene for each frame, an action module that manipulates the environment state according to the user input, and a volumetric rendering module that decodes the environment state back to the image space[^1^][1]. The paper also introduces two large scale video datasets with significant camera movements to evaluate the method[^1^][1].

## Main Contributions

[1]: https://arxiv.org/abs/2203.01914v2 "Playable Environments: Video Manipulation in Space and Time"
[2]: https://arxiv.org/pdf/2203.01914v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.01914 "[2203.01914] Playable Environments: Video Manipulation in Space and Time"

The paper claims the following contributions[^1^][1]:

- A new representation for interactive video generation and manipulation in space and time, called Playable Environments, that allows the user to move objects in 3D while generating a video by providing a sequence of desired actions, with a single image at inference time.
- A novel framework that consists of three main components: an environment state module that builds a 3D representation of the scene for each frame, an action module that manipulates the environment state according to the user input, and a volumetric rendering module that decodes the environment state back to the image space.
- An extension of neural radiance fields with style-based modulation to support diverse appearances of objects.
- An unsupervised learning approach that trains on a collection of various monocular videos requiring only the estimated camera parameters and 2D object locations.
- Two large scale video datasets with significant camera movements to evaluate the method.
- Several creative applications enabled by playable environments, such as playable 3D video generation, stylization and manipulation.

## Method Summary

[1]: https://arxiv.org/abs/2203.01914v2 "Playable Environments: Video Manipulation in Space and Time"
[2]: https://arxiv.org/pdf/2203.01914v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.01914 "[2203.01914] Playable Environments: Video Manipulation in Space and Time"

Here is a summary of the method section of the paper:

- The paper proposes a novel framework that consists of three main components: an environment state module, an action module, and a volumetric rendering module[^1^][1].
- The environment state module builds a 3D representation of the scene for each frame, using a neural radiance field (NeRF) that encodes the color and density of each point in space[^1^][1]. The NeRF is extended with style-based modulation to support diverse appearances of objects[^1^][1].
- The action module manipulates the environment state according to the user input, which consists of a sequence of desired actions for each object in the scene[^1^][1]. The actions are learnt in an unsupervised manner from the video data, using a variational autoencoder (VAE) that encodes the motion of each object as a latent vector[^1^][1]. The action module also controls the camera pose and focal length to get the desired viewpoint[^1^][1].
- The volumetric rendering module decodes the environment state back to the image space, using a differentiable ray marching algorithm that integrates the color and density along each ray[^1^][1]. The module also applies a post-processing step to refine the output image and add realistic effects such as motion blur and depth of field[^1^][1].
- The paper trains the framework on a collection of various monocular videos requiring only the estimated camera parameters and 2D object locations[^1^][1]. The paper introduces two large scale video datasets with significant camera movements to evaluate the method: Playable Cars and Playable Humans[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the environment state module
def environment_state_module(image, camera_pose, object_locations):
  # Encode the image into a style vector using a style encoder
  style = style_encoder(image)
  # Initialize a neural radiance field (NeRF) with style-based modulation
  nerf = NeRF(style)
  # For each object in the scene, estimate its 3D bounding box and occupancy mask using a mask encoder
  boxes = []
  masks = []
  for location in object_locations:
    box, mask = mask_encoder(location)
    boxes.append(box)
    masks.append(mask)
  # Return the nerf, the boxes and the masks as the environment state
  return nerf, boxes, masks

# Define the action module
def action_module(environment_state, user_input):
  # Unpack the environment state into nerf, boxes and masks
  nerf, boxes, masks = environment_state
  # For each object in the scene, get its desired action from the user input and encode it into a latent vector using a variational autoencoder (VAE)
  actions = []
  for i in range(len(boxes)):
    action = user_input[i]
    latent = VAE(action)
    actions.append(latent)
  # For each object in the scene, apply its action to its bounding box and occupancy mask using a spatial transformer network (STN)
  new_boxes = []
  new_masks = []
  for i in range(len(boxes)):
    box = boxes[i]
    mask = masks[i]
    latent = actions[i]
    new_box, new_mask = STN(box, mask, latent)
    new_boxes.append(new_box)
    new_masks.append(new_mask)
  # Get the desired camera pose and focal length from the user input
  camera_pose, focal_length = user_input[-1]
  # Return the updated environment state with the new boxes and masks, and the camera parameters
  return nerf, new_boxes, new_masks, camera_pose, focal_length

# Define the volumetric rendering module
def volumetric_rendering_module(environment_state):
  # Unpack the environment state into nerf, boxes, masks and camera parameters
  nerf, boxes, masks, camera_pose, focal_length = environment_state
  # For each pixel in the output image, cast a ray from the camera origin through the pixel center
  rays = cast_rays(camera_pose, focal_length)
  # For each ray, sample points along it and query the nerf for their color and density
  colors = []
  densities = []
  for ray in rays:
    points = sample_points(ray)
    color, density = nerf(points)
    colors.append(color)
    densities.append(density)
  # For each ray, integrate the color and density along it using a differentiable ray marching algorithm
  images = []
  for i in range(len(rays)):
    ray = rays[i]
    color = colors[i]
    density = densities[i]
    image = ray_marching(ray, color, density)
    images.append(image)
  # Combine the images into a single output image
  output_image = combine_images(images)
  # Apply a post-processing step to refine the output image and add realistic effects such as motion blur and depth of field
  output_image = post_process(output_image)
  # Return the output image
  return output_image

# Define the main function that takes an input image and user input and generates an output video
def main(image, user_input):
  # Estimate the camera pose and object locations from the input image using a pre-trained network
  camera_pose, object_locations = estimate_camera_and_objects(image)
  # Build the initial environment state from the input image, camera pose and object locations using the environment state module
  environment_state = environment_state_module(image, camera_pose, object_locations)
  # Initialize an empty list to store the output frames
  output_frames = []
  # For each frame in the output video:
  for t in range(video_length):
    # Get the user input for the current frame
    current_user_input = user_input[t]
    # Manipulate the environment state according to the user input using the action module
    environment_state = action_module(environment_state, current_user_input)
    # Generate an output image from the environment state using the volumetric rendering module
    output_image = volumetric_rendering_module(environment_state)
    # Append the output image to the output frames list
    output_frames.append(output_image)
  # Return the output frames as an output video
  return output_frames

```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch # for tensor operations
import torchvision # for image processing
import numpy as np # for numerical operations
import cv2 # for video processing
import matplotlib.pyplot as plt # for visualization

# Define some hyperparameters
image_size = 256 # the size of the input and output images
video_length = 10 # the number of frames in the output video
num_samples = 64 # the number of points sampled along each ray
num_latents = 16 # the dimension of the action latent vector
num_styles = 64 # the dimension of the style vector
num_layers = 8 # the number of layers in the NeRF and VAE networks
num_channels = 256 # the number of channels in the NeRF and VAE networks
learning_rate = 1e-4 # the learning rate for training
batch_size = 32 # the batch size for training
num_epochs = 100 # the number of epochs for training

# Define a function to load a video from a file and extract its frames, camera parameters and object locations
def load_video(video_file):
  # Open the video file using cv2.VideoCapture
  video = cv2.VideoCapture(video_file)
  # Initialize an empty list to store the frames
  frames = []
  # Initialize an empty list to store the camera parameters
  camera_params = []
  # Initialize an empty list to store the object locations
  object_locations = []
  # While the video is not finished:
  while video.isOpened():
    # Read a frame from the video using video.read()
    ret, frame = video.read()
    # If the frame is valid:
    if ret:
      # Resize the frame to image_size x image_size using cv2.resize()
      frame = cv2.resize(frame, (image_size, image_size))
      # Convert the frame from BGR to RGB format using cv2.cvtColor()
      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
      # Normalize the frame to [0, 1] range by dividing by 255.0
      frame = frame / 255.0
      # Append the frame to the frames list
      frames.append(frame)
      # Estimate the camera pose and focal length from the frame using a pre-trained network (e.g. COLMAP)
      camera_pose, focal_length = estimate_camera(frame)
      # Append the camera pose and focal length to the camera_params list as a tuple
      camera_params.append((camera_pose, focal_length))
      # Estimate the object locations from the frame using a pre-trained network (e.g. Mask R-CNN)
      object_locations.append(estimate_objects(frame))
    # Else:
    else:
      # Break the loop
      break
  # Release the video using video.release()
  video.release()
  # Convert the frames list to a numpy array of shape (video_length, image_size, image_size, 3)
  frames = np.array(frames)
  # Convert the camera_params list to a numpy array of shape (video_length, 7) where each row contains (x, y, z, qw, qx, qy, qz, f) 
  camera_params = np.array(camera_params)
  # Convert the object_locations list to a numpy array of shape (video_length, num_objects, 4) where each row contains (x1, y1, x2, y2) 
  object_locations = np.array(object_locations)
  # Return the frames, camera_params and object_locations as a tuple
  return frames, camera_params, object_locations

# Define a function to cast rays from a camera origin through pixel centers
def cast_rays(camera_pose, focal_length):
  # Unpack the camera pose into translation and rotation as tensors
  translation = torch.tensor(camera_pose[:3])
  rotation = torch.tensor(camera_pose[3:])
  # Compute the camera origin as -rotation * translation using torch.matmul()
  origin = -torch.matmul(rotation, translation)
  # Compute the camera direction as rotation * [0,0,-1] using torch.matmul()
  direction = torch.matmul(rotation, torch.tensor([0.0,0.0,-1.0]))
  # Compute the camera right vector as rotation * [1,0,0] using torch.matmul()
  right = torch.matmul(rotation, torch.tensor([1.0,0.0,0.0]))
  # Compute the camera up vector as rotation * [0,-1,0] using torch.matmul()
  up = torch.matmul(rotation, torch.tensor([0.0,-1.0,0.0]))
  # Compute the camera plane center as origin + direction * focal_length using torch.add()
  plane_center = torch.add(origin, torch.mul(direction, focal_length))
  # Initialize an empty list to store the rays
  rays = []
  # For each pixel in the output image:
  for i in range(image_size):
    for j in range(image_size):
      # Compute the pixel center coordinates as (i - image_size / 2, image_size / 2 - j) using torch.tensor()
      pixel_center = torch.tensor([i - image_size / 2, image_size / 2 - j])
      # Compute the pixel position on the camera plane as plane_center + right * pixel_center[0] + up * pixel_center[1] using torch.add()
      pixel_position = torch.add(plane_center, torch.add(torch.mul(right, pixel_center[0]), torch.mul(up, pixel_center[1])))
      # Compute the ray direction as pixel_position - origin using torch.sub()
      ray_direction = torch.sub(pixel_position, origin)
      # Normalize the ray direction using torch.nn.functional.normalize()
      ray_direction = torch.nn.functional.normalize(ray_direction)
      # Append the ray origin and direction as a tuple to the rays list
      rays.append((origin, ray_direction))
  # Return the rays list
  return rays

# Define a function to sample points along a ray
def sample_points(ray):
  # Unpack the ray into origin and direction as tensors
  origin, direction = ray
  # Compute the near and far distances as 0.0 and 10.0 using torch.tensor()
  near, far = torch.tensor(0.0), torch.tensor(10.0)
  # Compute the depths as a tensor of shape (num_samples,) with values linearly spaced between near and far using torch.linspace()
  depths = torch.linspace(near, far, num_samples)
  # Compute the points as a tensor of shape (num_samples, 3) with values origin + direction * depth for each depth using torch.add() and torch.mul()
  points = torch.add(origin, torch.mul(direction, depths.unsqueeze(1)))
  # Return the points tensor
  return points

# Define a function to combine images into a single output image
def combine_images(images):
  # Convert the images list to a tensor of shape (image_size * image_size, image_size, image_size, 3) using torch.stack()
  images = torch.stack(images)
  # Reshape the images tensor to (image_size, image_size * image_size, image_size, 3) using torch.reshape()
  images = torch.reshape(images, (image_size, image_size * image_size, image_size, 3))
  # Permute the images tensor to (image_size * image_size, image_size, 3) using torch.permute()
  images = torch.permute(images, (1,0,2))
  # Reshape the images tensor to (image_size * image_size * image_size, 3) using torch.reshape()
  images = torch.reshape(images, (image_size * image_size * image_size, 3))
  # Return the images tensor
  return images

# Define a function to post-process the output image and add realistic effects such as motion blur and depth of field
def post_process(image):
  # Apply a Gaussian blur filter to the image using cv2.GaussianBlur()
  image = cv2.GaussianBlur(image, (5,5), sigmaX=1.0)
  # Apply a depth of field effect to the image using cv2.filter2D() with a circular kernel
  kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))
  kernel = kernel / np.sum(kernel)
  image = cv2.filter2D(image,-1,kernel)
  # Return the post-processed image
  return image

# Define a class for the style encoder network
class StyleEncoder(torch.nn.Module):
  
  # Define the constructor
  def __init__(self):
    # Call the parent constructor
    super(StyleEncoder,self).__init__()
    # Define a convolutional layer with input channels=3, output channels=num_channels and kernel size=3
    self.conv1 = torch.nn.Conv2d(3,num_channels,kernel_size=3,padding=1)
    # Define a batch normalization layer with num_channels features
    self.bn1 = torch.nn.BatchNorm2d(num_channels)
    # Define a ReLU activation layer
    self.relu1 = torch.nn.ReLU()
    # Define a max pooling layer with kernel size=2 and stride=2
    self.pool1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)
    # Define another convolutional layer with input channels=num_channels and output channels=num_channels*2 and kernel size=3
    self.conv2 = torch.nn.Conv2d(num_channels,num_channels*2,kernel_size=3,p