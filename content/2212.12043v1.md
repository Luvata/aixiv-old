---
title: 2212.12043v1 When are Lemons Purple? The Concept Association Bias of CLIP
date: 2022-12-13
---

# [When are Lemons Purple? The Concept Association Bias of CLIP](http://arxiv.org/abs/2212.12043v1)

authors: Yutaro Yamada, Yingtian Tang, Ilker Yildirim


## What, Why and How

[1]: https://arxiv.org/pdf/2212.12043v1 "When are Lemons Purple? The Concept Association Bias of CLIP - arXiv.org"
[2]: https://arxiv.org/abs/2212.12043 "When are Lemons Purple? The Concept Association Bias of CLIP"
[3]: http://export.arxiv.org/abs/2102.12043v1 "[2102.12043v1] Instance Independence of Single Layer Quantum ..."

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper investigates the phenomenon of Concept Association Bias (CAB) of CLIP, a large-scale vision-language model that can perform zero-shot image classification and image-to-text retrieval. CAB is when CLIP predicts an incorrect answer based on the association between two concepts in an image and a text prompt, such as predicting that a lemon is purple if there is an eggplant in the same image.
- **Why**: The paper aims to understand why CLIP's zero-shot performance does not generalize well to tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA). The paper hypothesizes that CAB is a potential cause of this difficulty and that it reflects the limitations of CLIP's cross-modal contrastive learning objective.
- **How**: The paper demonstrates CAB by showing that CLIP's zero-shot classification accuracy drops significantly when there is a strong concept association between an object and an attribute, such as color or shape. The paper also shows that CAB is mitigated when CLIP is fine-tuned on VQA with an additional Transformer layer on top of its original architecture. The paper finds that the strength of CAB in a model predicts how well it performs on VQA.

## Main Contributions

[1]: https://arxiv.org/pdf/2212.12043v1 "When are Lemons Purple? The Concept Association Bias of CLIP - arXiv.org"
[2]: https://arxiv.org/abs/2212.12043 "When are Lemons Purple? The Concept Association Bias of CLIP"
[3]: http://export.arxiv.org/abs/2102.12043v1 "[2102.12043v1] Instance Independence of Single Layer Quantum ..."

The paper[^1^][1] claims the following contributions:

- It introduces the concept of Concept Association Bias (CAB) of CLIP and provides empirical evidence for its existence and impact on zero-shot image classification and VQA tasks.
- It proposes a method to measure the strength of CAB in a model by using a synthetic dataset of images with two concepts and text prompts with one concept.
- It shows that CAB can be reduced by fine-tuning CLIP on VQA with an additional Transformer layer and that the strength of CAB correlates with the VQA performance across different models.

## Method Summary

[1]: https://arxiv.org/pdf/2212.12043v1 "When are Lemons Purple? The Concept Association Bias of CLIP - arXiv.org"
[2]: https://arxiv.org/abs/2212.12043 "When are Lemons Purple? The Concept Association Bias of CLIP"
[3]: http://export.arxiv.org/abs/2102.12043v1 "[2102.12043v1] Instance Independence of Single Layer Quantum ..."

Here is a summary of the method section of the paper[^1^][1]:

- The paper uses CLIP-RN50x4, a variant of CLIP with a ResNet-50x4 image encoder and a 12-layer Transformer text encoder, as the base model for all experiments.
- The paper constructs a synthetic dataset of images with two concepts (an object and an attribute) and text prompts with one concept (either the object or the attribute). The paper uses 10 objects and 10 attributes that have strong associations (e.g. lemon-yellow) or weak associations (e.g. lemon-square) according to human judgments. The paper generates images by cropping and pasting objects and attributes from ImageNet and COCO datasets. The paper generates text prompts by using templates such as "In this picture, the color of the [object] is [mask]" or "In this picture, the shape of the [attribute] is [mask]", where [mask] is one of the possible values for the concept.
- The paper measures the strength of CAB in CLIP by computing the accuracy of zero-shot classification on the synthetic dataset using different text prompts. The paper compares the accuracy of CLIP with a random baseline and a human baseline obtained from Amazon Mechanical Turk. The paper also analyzes the effect of image size and text length on CAB.
- The paper fine-tunes CLIP on VQA-v2 dataset with an additional Transformer layer that takes the concatenated image and text embeddings as input and outputs a final embedding for classification. The paper uses different learning rates and dropout rates for different components of CLIP. The paper evaluates the fine-tuned models on VQA-v2 test set and compares them with CLIP's zero-shot performance and other state-of-the-art models.
- The paper measures the strength of CAB in the fine-tuned models by using the same synthetic dataset and text prompts as before. The paper plots the correlation between CAB strength and VQA performance across different models and shows that they are negatively correlated.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Load CLIP-RN50x4 model
clip = load_model("CLIP-RN50x4")

# Generate synthetic dataset of images and text prompts
images, texts = generate_synthetic_dataset(objects, attributes)

# Measure CAB strength of CLIP on synthetic dataset
cab_score = measure_cab(clip, images, texts)

# Fine-tune CLIP on VQA-v2 dataset with an additional Transformer layer
clip_vqa = fine_tune(clip, vqa_data, transformer_layer)

# Evaluate CLIP-VQA on VQA-v2 test set
vqa_score = evaluate(clip_vqa, vqa_test)

# Measure CAB strength of CLIP-VQA on synthetic dataset
cab_score_vqa = measure_cab(clip_vqa, images, texts)

# Plot the correlation between CAB strength and VQA performance
plot_correlation(cab_scores, vqa_scores)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import libraries
import torch
import torchvision
import clip
import numpy as np
import PIL
import json

# Load CLIP-RN50x4 model
clip_model, preprocess = clip.load("RN50x4", device="cuda")

# Define constants
NUM_OBJECTS = 10 # number of objects to use
NUM_ATTRIBUTES = 10 # number of attributes to use
NUM_VALUES = 5 # number of values for each concept
IMAGE_SIZE = 224 # size of the image input for CLIP
TEXT_LENGTH = 32 # length of the text input for CLIP
BATCH_SIZE = 32 # batch size for training and evaluation
NUM_EPOCHS = 10 # number of epochs for fine-tuning
LEARNING_RATE = 1e-4 # learning rate for fine-tuning
DROPOUT_RATE = 0.1 # dropout rate for fine-tuning

# Define objects and attributes with strong and weak associations
objects = ["lemon", "apple", "banana", "carrot", "tomato", "book", "pen", "clock", "shoe", "hat"]
attributes = ["color", "shape", "size", "texture", "taste", "weight", "material", "sound", "smell", "function"]
strong_values = {
    "color": ["yellow", "red", "green", "orange", "purple"],
    "shape": ["round", "oval", "long", "square", "star"],
    "size": ["small", "medium", "large", "tiny", "huge"],
    "texture": ["smooth", "rough", "soft", "hard", "fuzzy"],
    "taste": ["sour", "sweet", "bitter", "salty", "spicy"],
    ...
}
weak_values = {
    "color": ["blue", "pink", "black", "white", "brown"],
    "shape": ["triangle", "rectangle", "pentagon", "hexagon", "octagon"],
    ...
}

# Define text templates for different concepts
text_templates = {
    "[object]": [
        "[object] is [mask]",
        "[mask] is an [object]",
        "[object] has [mask] property",
        "[mask] is a property of [object]",
        "[object] belongs to [mask] category",
        "[mask] is a category that [object] belongs to"
    ],
    "[attribute]": [
        "[attribute] of [mask] is [value]",
        "[value] is the [attribute] of [mask]",
        "[mask] has [value] [attribute]",
        "[value] is the [attribute] that [mask] has",
        "[attribute] is a feature of [mask]",
        "[value] is a feature that [mask] has"
    ]
}

# Define a function to generate synthetic images by cropping and pasting objects and attributes from ImageNet and COCO datasets
def generate_synthetic_images(objects, attributes, strong_values, weak_values):
    # Load ImageNet and COCO datasets
    imagenet = torchvision.datasets.ImageNet(root="data/imagenet")
    coco = torchvision.datasets.CocoDetection(root="data/coco")

    # Initialize an empty list to store synthetic images
    synthetic_images = []

    # Loop over each object and attribute pair
    for object in objects:
        for attribute in attributes:
            # Get the strong and weak values for the attribute
            strong_value_list = strong_values[attribute]
            weak_value_list = weak_values[attribute]

            # Get the images that contain the object from ImageNet or COCO
            object_images = get_images(object, imagenet, coco)

            # Loop over each strong value
            for strong_value in strong_value_list:
                # Get the images that contain the attribute with the strong value from ImageNet or COCO
                attribute_images = get_images(attribute + ":" + strong_value, imagenet, coco)

                # Randomly select one object image and one attribute image
                object_image = random.choice(object_images)
                attribute_image = random.choice(attribute_images)

                # Crop the object and the attribute from their images using bounding boxes or masks
                object_crop = crop_object(object_image, object)
                attribute_crop = crop_attribute(attribute_image, attribute)

                # Paste the object and the attribute on a white background with some random offset and rotation
                synthetic_image = paste_object_attribute(object_crop, attribute_crop)

                # Resize the synthetic image to match the input size of CLIP
                synthetic_image = resize_image(synthetic_image, IMAGE_SIZE)

                # Append the synthetic image to the list
                synthetic_images.append(synthetic_image)

            # Loop over each weak value
            for weak_value in weak_value_list:
                # Get the images that contain the attribute with the weak value from ImageNet or COCO
                attribute_images = get_images(attribute + ":" + weak_value, imagenet, coco)

                # Randomly select one object image and one attribute image
                object_image = random.choice(object_images)
                attribute_image = random.choice(attribute_images)

                # Crop the object and the attribute from their images using bounding boxes or masks
                object_crop = crop_object(object_image, object)
                attribute_crop = crop_attribute(attribute_image, attribute)

                # Paste the object and the attribute on a white background with some random offset and rotation
                synthetic_image = paste_object_attribute(object_crop, attribute_crop)

                # Resize the synthetic image to match the input size of CLIP
                synthetic_image = resize_image(synthetic_image, IMAGE_SIZE)

                # Append the synthetic image to the list
                synthetic_images.append(synthetic_image)

    # Return the list of synthetic images
    return synthetic_images

# Define a function to generate text prompts by using templates and filling in the concepts and values
def generate_text_prompts(objects, attributes, strong_values, weak_values, text_templates):
    # Initialize an empty list to store text prompts
    text_prompts = []

    # Loop over each object and attribute pair
    for object in objects:
        for attribute in attributes:
            # Get the strong and weak values for the attribute
            strong_value_list = strong_values[attribute]
            weak_value_list = weak_values[attribute]

            # Loop over each strong value
            for strong_value in strong_value_list:
                # Randomly select a text template for the object or the attribute
                text_template = random.choice(text_templates["[object]"] + text_templates["[attribute]"])

                # Fill in the template with the object, attribute, and value
                text_prompt = text_template.replace("[object]", object).replace("[attribute]", attribute).replace("[value]", strong_value).replace("[mask]", "[MASK]")

                # Append the text prompt to the list
                text_prompts.append(text_prompt)

            # Loop over each weak value
            for weak_value in weak_value_list:
                # Randomly select a text template for the object or the attribute
                text_template = random.choice(text_templates["[object]"] + text_templates["[attribute]"])

                # Fill in the template with the object, attribute, and value
                text_prompt = text_template.replace("[object]", object).replace("[attribute]", attribute).replace("[value]", weak_value).replace("[mask]", "[MASK]")

                # Append the text prompt to the list
                text_prompts.append(text_prompt)

    # Return the list of text prompts
    return text_prompts

# Define a function to measure CAB strength of a model on synthetic dataset by computing zero-shot classification accuracy using different text prompts
def measure_cab(model, images, texts):
    # Initialize an empty list to store CAB scores for each concept pair
    cab_scores = []

    # Loop over each image and text pair
    for image, text in zip(images, texts):
        # Preprocess the image and encode it with CLIP's image encoder
        image_tensor = preprocess(image).unsqueeze(0).to("cuda")
        image_features = model.encode_image(image_tensor)

        # Tokenize the text and encode it with CLIP's text encoder
        text_tokens = clip.tokenize([text]).to("cuda")
        text_features = model.encode_text(text_tokens)

        # Compute the logits by taking the dot product of image and text features
        logits = torch.matmul(text_features, image_features.T)

        # Get the index of the masked token in the text tokens
        mask_index = (text_tokens == clip._tokenizer.encoder["[MASK]"]).nonzero(as_tuple=True)[1].item()

        # Get the probabilities of different values for the masked token by applying softmax on the logits
        probs = torch.softmax(logits[:, mask_index], dim=-1)

        # Get the predicted value by taking the argmax of the probabilities
        pred_index = probs.argmax(dim=-1).item()
        pred_value = clip._tokenizer.decode([pred_index])

        # Get the true value by extracting it from the text prompt
        true_value = text.split("[MASK]")[1].strip()

        # Compare the predicted value and the true value and compute a binary accuracy score
        accuracy = int(pred_value == true_value)

        # Append the accuracy score to the list of CAB scores
        cab_scores.append(accuracy)

    # Compute the average CAB score across all concept pairs
    cab_score = np.mean(cab_scores)

    # Return the average CAB score
    return cab_score

# Define a function to fine-tune CLIP on VQA-v2 dataset