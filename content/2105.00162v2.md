---
title: 2105.00162v2 Generative Art Using Neural Visual Grammars and Dual Encoders
date: 2021-05-01
---

# [Generative Art Using Neural Visual Grammars and Dual Encoders](http://arxiv.org/abs/2105.00162v2)

authors: Chrisantha Fernando, S. M. Ali Eslami, Jean-Baptiste Alayrac, Piotr Mirowski, Dylan Banarse, Simon Osindero


## What, Why and How

[1]: https://arxiv.org/abs/2105.00162 "Generative Art Using Neural Visual Grammars and Dual Encoders"
[2]: https://arxiv.org/pdf/2105.00162v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.00162v2 "[2203.00162v2] Do Transformers use variable binding?"

Here is a summary of the paper:

- **What**: The paper describes a novel algorithm for producing generative art that takes a text string as input and outputs an image that interprets that string.
- **Why**: The paper aims to understand some of the processes of art making by automating them partially and analyzing which aspects of the artistic process are delegated to the algorithm and which remain the responsibility of the artist.
- **How**: The paper uses a hierarchical neural Lindenmeyer system to evolve images and an image text dual encoder trained on billions of images and their associated text from the internet to evaluate them. The paper also presents some examples of generated images and discusses their artistic qualities and limitations.

## Main Contributions

[1]: https://arxiv.org/abs/2105.00162 "Generative Art Using Neural Visual Grammars and Dual Encoders"
[2]: https://arxiv.org/pdf/2105.00162v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.00162v2 "[2203.00162v2] Do Transformers use variable binding?"

According to the paper[^1^][1], some of the contributions are:

- **A novel algorithm for generating art from text**: The paper proposes a method that combines a neural visual grammar with a dual encoder to produce images that match a given text input.
- **A demonstration of the artistic potential and limitations of the algorithm**: The paper presents several examples of generated images and discusses their aesthetic qualities, diversity, and coherence, as well as the challenges and failures of the algorithm.
- **An analysis of the artistic process and its automation**: The paper reflects on the role of the algorithm and the user in the artistic process and how they interact and influence each other. The paper also explores some ethical and philosophical implications of automating art making.

## Method Summary

[1]: https://arxiv.org/abs/2105.00162 "Generative Art Using Neural Visual Grammars and Dual Encoders"
[2]: https://arxiv.org/pdf/2105.00162v2.pdf "arXiv.org"
[3]: http://export.arxiv.org/abs/2203.00162v2 "[2203.00162v2] Do Transformers use variable binding?"

Here is a summary of the method section of the paper:

- **Neural Visual Grammar**: The paper uses a hierarchical neural Lindenmeyer system (NLS) to generate images from text. A NLS is a grammar that defines rules for transforming symbols into graphical primitives. The paper extends the NLS with neural networks that learn the parameters of the rules from data. The paper also introduces a novel way of encoding the NLS rules as a tree structure that can be manipulated by genetic operators.
- **Image Text Dual Encoder**: The paper uses a dual encoder model that is trained on billions of image-text pairs from the internet. The dual encoder maps both images and text into a common embedding space where their similarity can be measured. The paper uses the dual encoder as a fitness function to guide the evolution of the NLS images towards matching the input text.
- **Evolutionary Algorithm**: The paper uses a genetic algorithm to evolve the NLS images. The algorithm starts with a population of random NLS trees and applies crossover and mutation operators to generate new trees. The algorithm evaluates the fitness of each tree by rendering its image and computing its similarity to the input text using the dual encoder. The algorithm selects the best trees for reproduction and discards the worst ones. The algorithm repeats this process until a termination criterion is met.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the NLS grammar and the neural networks for the rule parameters
grammar = NLS_Grammar()
networks = Neural_Networks()

# Load the dual encoder model trained on image-text pairs
dual_encoder = Dual_Encoder()

# Initialize a population of random NLS trees
population = [NLS_Tree(grammar, networks) for i in range(population_size)]

# Get the input text from the user
input_text = get_input()

# Repeat until termination criterion is met
while not terminated():

  # Render the images for each tree in the population
  images = [tree.render() for tree in population]

  # Compute the fitness of each image using the dual encoder
  fitness = [dual_encoder.similarity(image, input_text) for image in images]

  # Select the best trees for reproduction
  parents = select(population, fitness)

  # Apply crossover and mutation operators to generate new trees
  offspring = crossover_and_mutation(parents, grammar, networks)

  # Replace the worst trees with the new ones
  population = replace(population, offspring, fitness)

# Return the best image and its corresponding tree
best_image, best_tree = get_best(population, fitness)
return best_image, best_tree
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Define the NLS grammar and the neural networks for the rule parameters
grammar = NLS_Grammar()
# The grammar consists of a set of symbols and a set of rules
# Each symbol represents a graphical primitive such as a line, a circle, or a color
# Each rule defines how to replace a symbol with a sequence of symbols
# For example, the rule F -> F+F-F means to replace F with F+F-F
# The grammar also has an axiom, which is the initial symbol to start the generation
# For example, the axiom could be F

networks = Neural_Networks()
# The networks consist of a set of neural networks that learn the parameters of the rules
# Each network takes as input a text string and outputs a vector of parameters
# For example, the network for the rule F -> F+F-F could output the angles and lengths of the lines
# The networks are trained on a dataset of images and their corresponding NLS trees

# Load the dual encoder model trained on image-text pairs
dual_encoder = Dual_Encoder()
# The dual encoder consists of two encoders: one for images and one for text
# Each encoder maps its input to a vector in a common embedding space
# The dual encoder is trained on a large dataset of image-text pairs from the internet
# The dual encoder learns to maximize the similarity between matching pairs and minimize it between non-matching pairs
# The similarity is computed as the cosine distance between the vectors

# Define a class for NLS trees
class NLS_Tree:

  # Initialize a tree with a grammar, networks, and an optional symbol sequence
  def __init__(self, grammar, networks, symbols=None):
    self.grammar = grammar # The grammar to use for generation
    self.networks = networks # The networks to use for parameters
    self.symbols = symbols # The sequence of symbols in the tree
    if self.symbols is None: # If no symbols are given, use the axiom as the root symbol
      self.symbols = [self.grammar.axiom]

  # Render the image for the tree
  def render(self):
    # Initialize an empty canvas
    canvas = Canvas()
    # Initialize a stack to store the state of the turtle
    stack = Stack()
    # Initialize a turtle to draw on the canvas
    turtle = Turtle()
    # For each symbol in the tree
    for symbol in self.symbols:
      # If the symbol is a graphical primitive
      if symbol.is_primitive():
        # Get the parameters for the symbol from the corresponding network
        parameters = self.networks.get_parameters(symbol)
        # Draw the symbol on the canvas using the turtle and the parameters
        turtle.draw(symbol, parameters)
      # If the symbol is a push operation
      elif symbol.is_push():
        # Push the current state of the turtle to the stack
        stack.push(turtle.state)
      # If the symbol is a pop operation
      elif symbol.is_pop():
        # Pop the last state of the turtle from the stack
        turtle.state = stack.pop()
      # Otherwise, ignore the symbol
      else:
        pass
    # Return the canvas as an image
    return canvas.to_image()

  # Apply crossover operator with another tree
  def crossover(self, other):
    # Choose a random point in both trees to swap their symbols
    point1 = random.randint(0, len(self.symbols) - 1)
    point2 = random.randint(0, len(other.symbols) - 1)
    # Swap the symbols at those points and create new trees from them
    new_symbols1 = self.symbols[:point1] + other.symbols[point2:]
    new_symbols2 = other.symbols[:point2] + self.symbols[point1:]
    new_tree1 = NLS_Tree(self.grammar, self.networks, new_symbols1)
    new_tree2 = NLS_Tree(other.grammar, other.networks, new_symbols2)
    # Return the new trees as offspring
    return new_tree1, new_tree2

  # Apply mutation operator to the tree
  def mutate(self):
    # Choose a random point in the tree to mutate its symbol
    point = random.randint(0, len(self.symbols) - 1)
    old_symbol = self.symbols[point]
    # Choose a random rule from the grammar that has the same left-hand side as the old symbol
    rule = random.choice(self.grammar.get_rules(old_symbol))
    # Replace the old symbol with the right-hand side of the rule and create a new tree from it
    new_symbols = self.symbols[:point] + rule.right + self.symbols[point+1:]
    new_tree = NLS_Tree(self.grammar, self.networks, new_symbols)
    # Return the new tree as offspring
    return new_tree

# Initialize a population of random NLS trees
population = [NLS_Tree(grammar, networks) for i in range(population_size)]

# Get the input text from the user
input_text = get_input()

# Repeat until termination criterion is met
while not terminated():

  # Render the images for each tree in the population
  images = [tree.render() for tree in population]

  # Compute the fitness of each image using the dual encoder
  fitness = [dual_encoder.similarity(image, input_text) for image in images]

  # Select the best trees for reproduction
  parents = select(population, fitness)
  # The selection function can use any method such as roulette wheel, tournament, or rank-based selection

  # Apply crossover and mutation operators to generate new trees
  offspring = []
  # For each pair of parents
  for parent1, parent2 in zip(parents[::2], parents[1::2]):
    # Apply crossover with some probability and get two offspring
    if random.random() < crossover_rate:
      child1, child2 = parent1.crossover(parent2)
    else:
      child1, child2 = parent1, parent2
    # Apply mutation to each offspring with some probability and get two offspring
    if random.random() < mutation_rate:
      child1 = child1.mutate()
    if random.random() < mutation_rate:
      child2 = child2.mutate()
    # Add the offspring to the list
    offspring.append(child1)
    offspring.append(child2)

  # Replace the worst trees with the new ones
  population = replace(population, offspring, fitness)
  # The replace function can use any method such as elitism, truncation, or steady-state replacement

# Return the best image and its corresponding tree
best_image, best_tree = get_best(population, fitness)
return best_image, best_tree
```