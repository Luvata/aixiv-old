---
title: 2205.10747v4 Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners
date: 2022-05-11
---

# [Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners](http://arxiv.org/abs/2205.10747v4)

authors: Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, Shih-Fu Chang, Mohit Bansal, Heng Ji


## What, Why and How

[1]: https://arxiv.org/abs/2205.10747v4 "[2205.10747v4] Language Models with Image Descriptors are Strong Few ..."
[2]: https://arxiv.org/pdf/2205.10747v4.pdf "Abstract - arXiv.org"
[3]: https://info.arxiv.org/about/index.html "About arXiv - arXiv info"

Here is a summary of the paper:

- **What**: The paper proposes VidIL, a few-shot video-language learner that uses image-language models and language models to generate text from videos for various tasks, such as captioning, question answering, and future prediction.
- **Why**: The paper aims to build flexible video-language models that can generalize to unseen tasks with limited annotations, without pretraining or finetuning on any video datasets. The paper argues that existing video-language models are either encoder-only or rely heavily on finetuning, and cannot handle generative tasks in a few-shot setting.
- **How**: The paper uses image-language models to translate the video content into frame captions, object, attribute, and event phrases, and composes them into a temporal-aware template. The paper then instructs a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content. The paper evaluates the model on four video-language tasks: video captioning, video question answering, video caption retrieval, and video future event prediction. The paper shows that the model outperforms state-of-the-art supervised models on video future event prediction, and achieves competitive results on the other tasks.[^1^][1] [^2^][2]

## Main Contributions

According to the paper, the contributions are:

- The paper proposes VidIL, a novel few-shot video-language learner that leverages image-language models and language models to generate text from videos for various tasks, without pretraining or finetuning on any video datasets.
- The paper introduces a temporal-aware template composition method that combines frame captions, object, attribute, and event phrases into a structured input for the language model.
- The paper demonstrates the flexibility and effectiveness of VidIL on four video-language tasks: video captioning, video question answering, video caption retrieval, and video future event prediction. The paper shows that VidIL outperforms state-of-the-art supervised models on video future event prediction, and achieves competitive results on the other tasks.

## Method Summary

Here is a summary of the method section of the paper:

- The paper presents VidIL, a few-shot video-language learner that consists of two components: an image-language model and a language model.
- The image-language model is used to extract visual features and generate text descriptors from each video frame. The paper uses CLIP [47] as the image-language model, which is pretrained on a large-scale image-text dataset. The paper uses four types of text descriptors: frame captions, object phrases, attribute phrases, and event phrases. The paper generates frame captions by using the image-language model to rank a set of candidate captions from a large corpus. The paper generates object, attribute, and event phrases by using the image-language model to classify the frames into predefined categories.
- The language model is used to generate the final text output from the text descriptors. The paper uses InstructGPT [40] as the language model, which is pretrained on a large-scale text corpus and can be instructed by natural language prompts. The paper uses a temporal-aware template composition method to combine the text descriptors into a structured input for the language model. The paper uses different templates for different tasks, such as captioning, question answering, and future prediction. The paper also provides a few in-context examples in the prompt to guide the language model to generate the desired output.


## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Define the image-language model and the language model
image_language_model = CLIP()
language_model = InstructGPT()

# Define the text descriptor types and the candidate caption corpus
text_descriptor_types = ["frame_caption", "object_phrase", "attribute_phrase", "event_phrase"]
candidate_caption_corpus = load_corpus()

# Define the task-specific templates and prompts
templates = {
  "captioning": "[CLS] [frame_caption] [SEP] [object_phrase] [SEP] [attribute_phrase] [SEP] [event_phrase] [SEP]",
  "question_answering": "[CLS] [question] [SEP] [frame_caption] [SEP] [object_phrase] [SEP] [attribute_phrase] [SEP] [event_phrase] [SEP]",
  "future_prediction": "[CLS] [frame_caption] [SEP] [object_phrase] [SEP] [attribute_phrase] [SEP] [event_phrase] [SEP]"
}
prompts = {
  "captioning": "Given a video, generate a caption that describes its content. For example:\n\nVideo: A man is playing guitar in a park.\nCaption: A man is enjoying music in nature.\n\nVideo: A dog is chasing a ball in a backyard.\nCaption: A dog is having fun with its toy.\n\nVideo: {video}\nCaption: {caption}",
  "question_answering": "Given a video and a question, generate an answer that is relevant and accurate. For example:\n\nVideo: A woman is baking a cake in a kitchen.\nQuestion: What is the color of the cake?\nAnswer: The cake is brown.\n\nVideo: A boy is riding a bike on a street.\nQuestion: How many wheels does the bike have?\nAnswer: The bike has two wheels.\n\nVideo: {video}\nQuestion: {question}\nAnswer: {answer}",
  "future_prediction": "Given a video, generate a sentence that predicts what will happen next. For example:\n\nVideo: A car is approaching a red traffic light.\nPrediction: The car will stop at the traffic light.\n\nVideo: A girl is swinging on a swing.\nPrediction: The girl will jump off the swing.\n\nVideo: {video}\nPrediction: {prediction}"
}

# For each video in the test set
for video in test_set:
  # Initialize an empty dictionary to store the text descriptors
  text_descriptors = {}

  # For each frame in the video
  for frame in video:
    # Extract the visual features using the image-language model
    visual_features = image_language_model.encode_image(frame)

    # For each text descriptor type
    for text_descriptor_type in text_descriptor_types:
      # If the text descriptor type is frame caption
      if text_descriptor_type == "frame_caption":
        # Generate a frame caption by ranking the candidate captions using the image-language model
        frame_caption = rank_captions(visual_features, candidate_caption_corpus, image_language_model)
      # Else
      else:
        # Generate a text descriptor by classifying the frame into a predefined category using the image-language model
        text_descriptor = classify_frame(visual_features, text_descriptor_type, image_language_model)
      
      # Append the text descriptor to the dictionary
      text_descriptors[text_descriptor_type].append(text_descriptor)
  
  # Compose the text descriptors into a temporal-aware template according to the task
  template_input = compose_template(text_descriptors, templates[task])

  # Generate the prompt by filling in the template input and adding a few in-context examples
  prompt = fill_prompt(template_input, prompts[task])

  # Generate the output by instructing the language model with the prompt
  output = language_model.generate(prompt)

  # Print the output
  print(output)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the required libraries
import torch
import clip
import transformers

# Load the image-language model and the language model
image_language_model = clip.load("ViT-B/32", device="cuda")
language_model = transformers.AutoModelForCausalLM.from_pretrained("instructgpt")

# Load the tokenizer for the image-language model and the language model
image_language_tokenizer = clip.tokenize
language_model_tokenizer = transformers.AutoTokenizer.from_pretrained("instructgpt")

# Define the text descriptor types and the candidate caption corpus
text_descriptor_types = ["frame_caption", "object_phrase", "attribute_phrase", "event_phrase"]
candidate_caption_corpus = load_corpus()

# Define the object, attribute, and event categories and their corresponding labels
object_categories = ["person", "animal", "vehicle", "plant", "food", "instrument", "sport", "toy"]
object_labels = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7])
attribute_categories = ["color", "size", "shape", "texture", "emotion", "action", "location", "time"]
attribute_labels = torch.tensor([8, 9, 10, 11, 12, 13, 14, 15])
event_categories = ["birth", "death", "marriage", "graduation", "travel", "celebration", "competition", "performance"]
event_labels = torch.tensor([16, 17, 18, 19, 20, 21, 22, 23])

# Define the task-specific templates and prompts
templates = {
  "captioning": "[CLS] [frame_caption] [SEP] [object_phrase] [SEP] [attribute_phrase] [SEP] [event_phrase] [SEP]",
  "question_answering": "[CLS] [question] [SEP] [frame_caption] [SEP] [object_phrase] [SEP] [attribute_phrase] [SEP] [event_phrase] [SEP]",
  "future_prediction": "[CLS] [frame_caption] [SEP] [object_phrase] [SEP] [attribute_phrase] [SEP] [event_phrase] [SEP]"
}
prompts = {
  "captioning": "Given a video, generate a caption that describes its content. For example:\n\nVideo: A man is playing guitar in a park.\nCaption: A man is enjoying music in nature.\n\nVideo: A dog is chasing a ball in a backyard.\nCaption: A dog is having fun with its toy.\n\nVideo: {video}\nCaption: {caption}",
  "question_answering": "Given a video and a question, generate an answer that is relevant and accurate. For example:\n\nVideo: A woman is baking a cake in a kitchen.\nQuestion: What is the color of the cake?\nAnswer: The cake is brown.\n\nVideo: A boy is riding a bike on a street.\nQuestion: How many wheels does the bike have?\nAnswer: The bike has two wheels.\n\nVideo: {video}\nQuestion: {question}\nAnswer: {answer}",
  "future_prediction": "Given a video, generate a sentence that predicts what will happen next. For example:\n\nVideo: A car is approaching a red traffic light.\nPrediction: The car will stop at the traffic light.\n\nVideo: A girl is swinging on a swing.\nPrediction: The girl will jump off the swing.\n\nVideo: {video}\nPrediction: {prediction}"
}

# Define a function to rank captions using the image-language model
def rank_captions(visual_features, candidate_captions, image_language_model):
  # Tokenize the candidate captions using the image-language tokenizer
  candidate_tokens = image_language_tokenizer(candidate_captions)

  # Encode the candidate tokens using the image-language model
  candidate_features = image_language_model.encode_text(candidate_tokens)

  # Compute the cosine similarity between the visual features and the candidate features
  similarity = torch.cosine_similarity(visual_features.unsqueeze(0), candidate_features)

  # Find the index of the most similar candidate caption
  best_index = torch.argmax(similarity)

  # Return the best candidate caption
  return candidate_captions[best_index]

# Define a function to classify frames using the image-language model
def classify_frame(visual_features, text_descriptor_type, image_language_model):
  # If the text descriptor type is object phrase
  if text_descriptor_type == "object_phrase":
    # Tokenize the object categories using the image-language tokenizer
    category_tokens = image_language_tokenizer(object_categories)

    # Encode the category tokens using the image-language model
    category_features = image_language_model.encode_text(category_tokens)

    # Compute the cosine similarity between the visual features and the category features
    similarity = torch.cosine_similarity(visual_features.unsqueeze(0), category_features)

    # Find the index of the most similar category
    best_index = torch.argmax(similarity)

    # Return the corresponding object phrase
    return "a " + object_categories[best_index]
  # Else if the text descriptor type is attribute phrase
  elif text_descriptor_type == "attribute_phrase":
    # Tokenize the attribute categories using the image-language tokenizer
    category_tokens = image_language_tokenizer(attribute_categories)

    # Encode the category tokens using the image-language model
    category_features = image_language_model.encode_text(category_tokens)

    # Compute the cosine similarity between the visual features and the category features
    similarity = torch.cosine_similarity(visual_features.unsqueeze(0), category_features)

    # Find the index of the most similar category
    best_index = torch.argmax(similarity)

    # Return the corresponding attribute phrase
    return "is " + attribute_categories[best_index]
  # Else if the text descriptor type is event phrase
  elif text_descriptor_type == "event_phrase":
    # Tokenize the event categories using the image-language tokenizer
    category_tokens = image_language_tokenizer(event_categories)

    # Encode the category tokens using the image-language model
    category_features = image_language_model.encode_text(category_tokens)

    # Compute the cosine similarity between the visual features and the category features
    similarity = torch.cosine_similarity(visual_features.unsqueeze(0), category_features)

    # Find the index of the most similar category
    best_index = torch.argmax(similarity)

    # Return the corresponding event phrase
    return "will " + event_categories[best_index]

# Define a function to compose text descriptors into a temporal-aware template
def compose_template(text_descriptors, template):
  # Initialize an empty string to store the template input
  template_input = ""

  # For each text descriptor type in the template
  for text_descriptor_type in template.split():
    # If the text descriptor type is a special token, such as [CLS] or [SEP]
    if text_descriptor_type.startswith("[") and text_descriptor_type.endswith("]"):
      # Append it to the template input
      template_input += text_descriptor_type + " "
    # Else
    else:
      # Join all the text descriptors of that type with commas and append them to the template input
      template_input += ", ".join(text_descriptors[text_descriptor_type]) + " "

  # Return the template input
  return template_input

# Define a function to fill in the prompt with the template input and a few in-context examples
def fill_prompt(template_input, prompt):
  # Replace the placeholder for video with the template input in the prompt
  prompt = prompt.replace("{video}", template_input)

  # Replace the placeholder for caption, answer, or prediction with a blank line in the prompt
  prompt = prompt.replace("{caption}", "\n")
  prompt = prompt.replace("{answer}", "\n")
  prompt = prompt.replace("{prediction}", "\n")

  # Return the prompt
  return prompt

# For each video in the test set
for video in test_set:
  # Initialize an empty dictionary to store the text descriptors
  text_descriptors = {}

  # For each text descriptor type, initialize an empty list in the dictionary
  for text_descriptor_type in text_descriptor_types:
    text_descriptors[text_descriptor_type] = []

  # For each frame in the video
  for frame in video:
    # Convert the frame to a tensor and normalize it
    frame_tensor = torch.from_numpy(frame).permute(2,0,1).unsqueeze(0).to("cuda")
    frame_tensor = frame_tensor / 255

    # Extract the visual features using the image-language model
    visual_features = image_language_model.encode_image(frame_tensor)

    # For each text descriptor type
    for text_descriptor_type in text_descriptor_types:
      # If the text descriptor type is frame caption
      if text_descriptor_type == "frame_caption":
        # Generate a frame caption by ranking candidate captions using the image-language model
        frame_caption = rank_captions(visual_features, candidate_caption_corpus, image_language_model)
      # Else
      else:
        # Generate a text descriptor by classifying frames using the image-language model
        text_descriptor = classify_frame(visual_features, text_descriptor_type, image_language_model)
      
      # Append the text descriptor to the dictionary
      text_descriptors[text_descriptor_type].append(text_descriptor)
  
  # Compose the text descriptors into a temporal-aware template according to the task
  template_input = compose_template(text_descriptors, templates[task])

  # Generate a prompt by filling in with a few in-context examples and