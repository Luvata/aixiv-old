---
title: 2202.06687v1 Domain Adaptation via Prompt Learning
date: 2022-02-07
---

# [Domain Adaptation via Prompt Learning](http://arxiv.org/abs/2202.06687v1)

authors: Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, Gao Huang


## What, Why and How

[1]: https://arxiv.org/abs/2202.06687v1 "[2202.06687v1] Domain Adaptation via Prompt Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2202.06687v1 "Domain Adaptation via Prompt Learning - arXiv.org"
[3]: https://arxiv.org/abs/2302.06687 "The relation between morphology, star formation history, and ..."

Here is a summary of the paper:

```
What: The paper introduces a novel prompt learning paradigm for unsupervised domain adaptation (UDA), named Domain Adaptation via Prompt Learning (DAPL).

Why: The paper aims to address the limitations of conventional UDA methods that rely on feature alignment, which could distort the semantic feature structures and lose class discriminability.

How: The paper proposes to embed domain information into prompts, which are natural language representations that are used to perform classification. The prompts are dynamically adapted according to each domain, and are optimized using a pre-trained vision-language model.
```

## Main Contributions

[1]: https://arxiv.org/abs/2202.06687v1 "[2202.06687v1] Domain Adaptation via Prompt Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2202.06687v1 "Domain Adaptation via Prompt Learning - arXiv.org"
[3]: https://arxiv.org/abs/2302.06687 "The relation between morphology, star formation history, and ..."

Here are some of the contributions of the paper:

```
- The paper proposes a novel prompt learning paradigm for UDA, which embeds domain information into natural language prompts and dynamically adapts them according to each domain.
- The paper leverages pre-trained vision-language models and optimizes only very few parameters, making the approach efficient and easy to implement.
- The paper demonstrates that the proposed approach outperforms previous methods on several cross-domain benchmarks, such as Office-31, Office-Home, and VisDA-2017.
```

## Method Summary

[1]: https://arxiv.org/abs/2202.06687v1 "[2202.06687v1] Domain Adaptation via Prompt Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2202.06687v1 "Domain Adaptation via Prompt Learning - arXiv.org"
[3]: https://arxiv.org/abs/2302.06687 "The relation between morphology, star formation history, and ..."

Here is a summary of the method section:

```
The method section consists of three subsections: prompt tuning, domain-aware prompt learning, and implementation details.

- Prompt tuning: The authors describe how they use a pre-trained vision-language model (CLIP) to encode images and prompts into a common feature space, and how they optimize the prompts for each class using gradient descent.
- Domain-aware prompt learning: The authors explain how they embed domain information into the prompts by adding a domain-specific token to the class name, and how they learn a prompt for each domain using a domain classifier and a consistency loss.
- Implementation details: The authors provide the details of the datasets, the model architecture, the hyperparameters, and the evaluation metrics used in their experiments.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2202.06687v1 "[2202.06687v1] Domain Adaptation via Prompt Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2202.06687v1 "Domain Adaptation via Prompt Learning - arXiv.org"
[3]: https://arxiv.org/abs/2302.06687 "The relation between morphology, star formation history, and ..."

Here is a high-level pseudo code for the paper:

```
# Load a pre-trained vision-language model (CLIP)
model = load_clip()

# Initialize prompts for each class and domain
prompts = init_prompts()

# Loop over epochs
for epoch in epochs:

  # Loop over batches of source and target images
  for source_images, target_images in batches:

    # Encode images and prompts into a common feature space
    source_features = model.encode_image(source_images)
    target_features = model.encode_image(target_images)
    prompt_features = model.encode_text(prompts)

    # Compute classification loss on source images using prompts
    source_loss = compute_classification_loss(source_features, prompt_features)

    # Compute domain classifier loss on source and target images
    domain_loss = compute_domain_loss(source_features, target_features)

    # Compute consistency loss on target images using prompts
    consistency_loss = compute_consistency_loss(target_features, prompt_features)

    # Update prompts using gradient descent
    prompts = update_prompts(source_loss, domain_loss, consistency_loss)

# Evaluate the model on the target domain using prompts
target_accuracy = evaluate_model(target_images, prompts)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2202.06687v1 "[2202.06687v1] Domain Adaptation via Prompt Learning - arXiv.org"
[2]: https://arxiv.org/pdf/2202.06687v1 "Domain Adaptation via Prompt Learning - arXiv.org"
[3]: https://arxiv.org/abs/2302.06687 "The relation between morphology, star formation history, and ..."

Here is a detailed pseudo code to implement the paper:

```
# Import libraries
import torch
import clip
import torch.nn as nn
import torch.optim as optim

# Load a pre-trained vision-language model (CLIP)
model = clip.load("ViT-B/32", jit=False)

# Initialize prompts for each class and domain
num_classes = 31 # number of classes in Office-31 dataset
num_domains = 3 # number of domains in Office-31 dataset
prompt_length = 2 # length of each prompt
prompt_tokens = torch.randint(0, model.vocab_size, (num_classes, num_domains, prompt_length)) # random tokens
prompt_tokens.requires_grad = True # enable gradient computation
prompts = model.decode(prompt_tokens) # decode tokens into text

# Define hyperparameters
lr = 1e-4 # learning rate
alpha = 0.1 # weight for domain classifier loss
beta = 0.1 # weight for consistency loss
gamma = 0.1 # temperature for softmax

# Define loss functions
classification_loss = nn.CrossEntropyLoss() # cross entropy loss for classification
domain_loss = nn.BCEWithLogitsLoss() # binary cross entropy loss for domain classifier
consistency_loss = nn.KLDivLoss(reduction="batchmean") # KL divergence loss for consistency

# Define optimizer
optimizer = optim.Adam([prompt_tokens], lr=lr) # Adam optimizer for prompt tokens

# Define domain classifier
domain_classifier = nn.Linear(model.visual.output_dim, 1) # linear layer for domain classifier

# Loop over epochs
for epoch in range(epochs):

  # Loop over batches of source and target images and labels
  for source_images, source_labels, target_images in dataloader:

    # Encode images and prompts into a common feature space
    source_features = model.encode_image(source_images) # encode source images
    target_features = model.encode_image(target_images) # encode target images
    prompt_features = model.encode_text(prompts) # encode prompts

    # Compute classification loss on source images using prompts
    source_logits = source_features @ prompt_features.t() # compute logits by dot product
    source_loss = classification_loss(source_logits, source_labels) # compute classification loss

    # Compute domain classifier loss on source and target images
    source_domain_logits = domain_classifier(source_features) # compute domain logits for source images
    target_domain_logits = domain_classifier(target_features) # compute domain logits for target images
    source_domain_labels = torch.ones(source_images.size(0), 1) # generate domain labels for source images (1 for source)
    target_domain_labels = torch.zeros(target_images.size(0), 1) # generate domain labels for target images (0 for target)
    domain_loss = domain_loss(source_domain_logits, source_domain_labels) + domain_loss(target_domain_logits, target_domain_labels) # compute domain loss

    # Compute consistency loss on target images using prompts
    target_probs = torch.softmax(target_logits / gamma, dim=-1) # compute softmax probabilities for target logits with temperature scaling
    target_probs_augmented = torch.softmax(target_logits / (gamma * 2), dim=-1) # compute softmax probabilities for target logits with higher temperature scaling (data augmentation)
    consistency_loss = consistency_loss(torch.log(target_probs_augmented), target_probs) # compute consistency loss

    # Update prompts using gradient descent
    total_loss = source_loss + alpha * domain_loss + beta * consistency_loss # compute total loss
    optimizer.zero_grad() # reset gradients
    total_loss.backward() # compute gradients
    optimizer.step() # update prompt tokens

# Evaluate the model on the target domain using prompts
target_accuracy = 0.0 # initialize target accuracy
for target_images, target_labels in test_dataloader: # loop over batches of target images and labels

  # Encode images and prompts into a common feature space
  target_features = model.encode_image(target_images) # encode target images
  prompt_features = model.encode_text(prompts) # encode prompts

  # Compute predictions on target images using prompts
  target_logits = target_features @ prompt_features.t() # compute logits by dot product
  target_preds = torch.argmax(target_logits, dim=-1) # compute predictions by argmax

  # Compute accuracy on target images using predictions and labels
  target_accuracy += torch.sum(target_preds == target_labels).item() / len(target_labels) # update accuracy

target_accuracy /= len(test_dataloader) # average accuracy over batches

```