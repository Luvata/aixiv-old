---
title: 1812.04948v3 A Style-Based Generator Architecture for Generative Adversarial Networks
date: 2018-12-05
---

# [A Style-Based Generator Architecture for Generative Adversarial Networks](http://arxiv.org/abs/1812.04948v3)

authors: Tero Karras, Samuli Laine, Timo Aila


## What, Why and How

[1]: https://arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."
[2]: https://arxiv.org/pdf/1812.04948v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."

Here is a summary of the paper:

```
What: The paper proposes a new generator architecture for generative adversarial networks (GANs) that is based on style transfer.

Why: The paper aims to improve the quality, diversity, and controllability of the generated images by GANs, and to disentangle the latent factors of variation.

How: The paper introduces a style-based generator that maps latent codes to intermediate style parameters that control the adaptive instance normalization (AdaIN) layers in the generator. The paper also proposes two new methods to measure the interpolation quality and disentanglement of GANs, and presents a new dataset of human faces.
```

## Main Contributions

[1]: https://arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."
[2]: https://arxiv.org/pdf/1812.04948v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."

The paper claims to make the following contributions:

- A new style-based generator architecture for GANs that separates high-level attributes and stochastic variation in the generated images.
- Two new methods to measure the interpolation quality and disentanglement of GANs, based on perceptual path length and linear separability.
- A new dataset of human faces with high quality and diversity, called Flickr-Faces-HQ (FFHQ).

## Method Summary

[1]: https://arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."
[2]: https://arxiv.org/pdf/1812.04948v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."

Here is a summary of the method section of the paper:

```
The method section consists of three subsections: style-based generator, measuring interpolation quality, and measuring disentanglement.

Style-based generator: The authors propose a new generator architecture that replaces the traditional input layer with a learned constant input, and introduces intermediate latent vectors that are mapped to style parameters that control the AdaIN layers in the generator. The style parameters modulate the feature maps at different resolutions, allowing for scale-specific control of the synthesis. The authors also introduce noise inputs that are added to the feature maps after the AdaIN layers, creating stochastic variation in the generated images. The authors use progressive growing and minibatch standard deviation to train the generator.

Measuring interpolation quality: The authors propose a new metric called perceptual path length (PPL) that measures how smooth and realistic the interpolations between latent codes are. The PPL is computed by sampling pairs of latent codes, interpolating them linearly in W space, and measuring the perceptual distance between adjacent images using a pretrained VGG network. The PPL is inversely proportional to the interpolation quality.

Measuring disentanglement: The authors propose a new metric called linear separability (LS) that measures how well a classifier can distinguish between different classes of images generated by varying one latent dimension while keeping others fixed. The LS is computed by sampling latent codes, perturbing them along one dimension, generating images, and training a linear classifier to predict the perturbation magnitude. The LS is directly proportional to the disentanglement.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."
[2]: https://arxiv.org/pdf/1812.04948v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."

Here is a high-level pseudo code for the paper:

```
# Define the style-based generator
def generator(z):
  # Map the latent code z to an intermediate latent space W
  w = mapping_network(z)
  # Initialize the constant input
  x = constant_input
  # Loop over the resolution levels
  for i in range(min_resolution, max_resolution):
    # Apply noise to the input
    x = x + noise[i]
    # Apply convolution to the input
    x = conv(x)
    # Apply adaptive instance normalization (AdaIN) to the input using the style parameters
    x = adain(x, style[i])
    # Apply activation function to the input
    x = activation(x)
  # Return the output image
  return x

# Define the discriminator
def discriminator(x):
  # Loop over the resolution levels in reverse order
  for i in range(max_resolution, min_resolution, -1):
    # Apply minibatch standard deviation to the input
    x = minibatch_stddev(x)
    # Apply convolution to the input
    x = conv(x)
    # Apply activation function to the input
    x = activation(x)
    # Downsample the input
    x = downsample(x)
  # Flatten the input
  x = flatten(x)
  # Apply a linear layer to the input
  x = linear(x)
  # Return the output score
  return x

# Define the perceptual path length (PPL) metric
def ppl(generator):
  # Initialize the PPL score
  ppl = 0
  # Sample pairs of latent codes from a normal distribution
  z1, z2 = sample_normal(n_pairs)
  # Interpolate them linearly in W space
  w1, w2 = mapping_network(z1), mapping_network(z2)
  epsilon = sample_uniform(n_steps)
  w = (1 - epsilon) * w1 + epsilon * w2
  # Generate images from the interpolated codes
  x = generator(w)
  # Compute the perceptual distance between adjacent images using a pretrained VGG network
  d = perceptual_distance(vgg(x[:-1]), vgg(x[1:]))
  # Compute the PPL score as the squared distance normalized by epsilon squared
  ppl = mean(d^2 / epsilon^2)
  # Return the PPL score
  return ppl

# Define the linear separability (LS) metric
def ls(generator):
  # Initialize the LS score
  ls = 0
  # Loop over the latent dimensions
  for i in range(latent_dim):
    # Sample latent codes from a normal distribution
    z = sample_normal(n_samples)
    # Perturb them along one dimension by a fixed magnitude
    z_perturbed = z.clone()
    z_perturbed[:, i] += perturbation_magnitude
    # Generate images from the original and perturbed codes
    x = generator(z)
    x_perturbed = generator(z_perturbed)
    # Concatenate the images and labels
    X = concat(x, x_perturbed)
    y = concat(zeros(n_samples), ones(n_samples))
    # Train a linear classifier to predict the labels from the images
    classifier = train_linear_classifier(X, y)
    # Compute the LS score as the accuracy of the classifier
    ls += accuracy(classifier(X), y)
  # Return the average LS score over all dimensions
  return ls / latent_dim

# Train a style-based GAN on a dataset of human faces using progressive growing and hinge loss

# Initialize the generator and discriminator networks
generator = generator()
discriminator = discriminator()

# Initialize the optimizer and learning rate scheduler for both networks
optimizer_g = optimizer(generator.parameters())
optimizer_d = optimizer(discriminator.parameters())
scheduler_g = scheduler(optimizer_g)
scheduler_d = scheduler(optimizer_d)

# Initialize the resolution level and number of training iterations per level
resolution_level = min_resolution_level
n_iter = n_iter_per_level

# Loop over the training iterations
for iter in range(max_iter):

  # Update the resolution level and number of iterations per level based on a predefined schedule
  resolution_level, n_iter = update_schedule(iter)

  # Sample a batch of latent codes from a normal distribution and generate fake images using the generator network at the current resolution level 
  z_fake = sample_normal(batch_size)
  x_fake = generator(z_fake, resolution_level)

  # Sample a batch of real images from the dataset at the current resolution level 
  x_real = sample_data(batch_size, resolution_level)

  # Compute the discriminator scores for both fake and real images 
  d_fake = discriminator(x_fake, resolution_level)
  d_real = discriminator(x_real, resolution_level)

  # Compute the hinge loss for both networks 
  loss_g = -mean(d_fake)
  loss_d = mean(max(0, 1 - d_real)) + mean(max(0, 1 + d_fake))

  # Update the generator network parameters by backpropagating the loss and applying the optimizer 
  optimizer_g.zero_grad()
  loss_g.backward()
  optimizer_g.step()

  # Update the discriminator network parameters by backpropagating the loss and applying the optimizer 
  optimizer_d.zero_grad()
  loss_d.backward()
  optimizer_d.step()

  # Update the learning rates for both networks using the scheduler 
  scheduler_g.step()
  scheduler_d.step()

  # Evaluate the generator network using the PPL and LS metrics every n_eval iterations 
  if iter % n_eval == 0:
    ppl_score = ppl(generator)
    ls_score = ls(generator)
    print("Iteration:", iter, "PPL:", ppl_score, "LS:", ls_score)

# Save the trained generator network
save_model(generator)
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."
[2]: https://arxiv.org/pdf/1812.04948v3.pdf "arXiv.org e-Print archive"
[3]: http://export.arxiv.org/abs/1812.04948 "[1812.04948] A Style-Based Generator Architecture for Generative ..."

Here is a detailed pseudo code to implement the paper:

```
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import math

# Define some hyperparameters
latent_dim = 512 # The dimension of the latent space Z and W
n_layers = 18 # The number of convolutional layers in the generator
n_channels = [512, 512, 512, 512, 256, 128, 64, 32, 16] # The number of output channels for each convolutional layer in the generator
n_filters = 3 # The size of the convolutional filters in the generator
min_resolution = 4 # The minimum resolution level for the generator and discriminator
max_resolution = 1024 # The maximum resolution level for the generator and discriminator
n_iter_per_level = 600000 # The number of training iterations per resolution level
max_iter = n_layers * n_iter_per_level # The maximum number of training iterations
batch_size = 32 # The batch size for training
lr = 0.001 # The initial learning rate for both networks
beta1 = 0 # The beta1 parameter for Adam optimizer
beta2 = 0.99 # The beta2 parameter for Adam optimizer
gamma = 0.1 # The gamma parameter for exponential learning rate decay
lambda_ppl = 2 # The weight for perceptual path length regularization
n_pairs = 10 # The number of pairs of latent codes to sample for PPL computation
n_steps = 10 # The number of interpolation steps for PPL computation
perturbation_magnitude = 0.5 # The magnitude of perturbation along one latent dimension for LS computation
n_samples = 1000 # The number of samples to generate for LS computation
n_eval = 10000 # The frequency of evaluation in terms of iterations

# Define the mapping network that maps a latent code z to an intermediate latent space W
class MappingNetwork(nn.Module):
  def __init__(self, latent_dim, n_layers):
    super(MappingNetwork, self).__init__()
    # Initialize a list of linear layers with leaky ReLU activation and pixel norm normalization
    self.layers = nn.ModuleList()
    for i in range(n_layers):
      self.layers.append(nn.Linear(latent_dim, latent_dim))
      self.layers.append(nn.LeakyReLU(0.2))
      self.layers.append(PixelNorm())
  
  def forward(self, z):
    # Loop over the layers and apply them sequentially to the input z
    w = z
    for layer in self.layers:
      w = layer(w)
    # Return the output w
    return w

# Define the pixel norm layer that normalizes each feature vector to unit length 
class PixelNorm(nn.Module):
  def __init__(self):
    super(PixelNorm, self).__init__()
  
  def forward(self, x):
    # Compute the squared L2 norm of each feature vector along the channel dimension 
    norm = torch.sum(x**2, dim=1, keepdim=True)
    # Divide each feature vector by its norm 
    x = x / torch.sqrt(norm + 1e-8)
    # Return the normalized output x 
    return x

# Define the adaptive instance normalization (AdaIN) layer that applies style modulation and demodulation 
class AdaIN(nn.Module):
  def __init__(self, n_channels):
    super(AdaIN, self).__init__()
    # Initialize two linear layers that map a style parameter to scale and bias factors 
    self.scale_layer = nn.Linear(latent_dim, n_channels)
    self.bias_layer = nn.Linear(latent_dim, n_channels)
  
  def forward(self, x, w):
    # Compute the mean and standard deviation of each feature map along the spatial dimensions 
    mean = torch.mean(x, dim=[2,3], keepdim=True)
    std = torch.std(x, dim=[2,3], keepdim=True)
    # Normalize each feature map by subtracting its mean and dividing by its standard deviation 
    x = (x - mean) / (std + 1e-8)
    # Compute the scale and bias factors from the style parameter w using the linear layers 
    scale = self.scale_layer(w).unsqueeze(2).unsqueeze(3)
    bias = self.bias_layer(w).unsqueeze(2).unsqueeze(3)
    # Modulate each feature map by multiplying it by its scale factor and adding its bias factor 
    x = x * scale + bias
    # Demodulate each feature map by dividing it by its L2 norm 
    norm = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True) + 1e-8)
    x = x / norm
    # Return the output x 
    return x

# Define the style-based generator that maps a latent code z to an image x
class StyleGenerator(nn.Module):
  def __init__(self, latent_dim, n_layers, n_channels, n_filters):
    super(StyleGenerator, self).__init__()
    # Initialize the mapping network that maps z to w
    self.mapping_network = MappingNetwork(latent_dim, n_layers)
    # Initialize the constant input that is learned during training
    self.constant_input = nn.Parameter(torch.randn(1, n_channels[0], 4, 4))
    # Initialize a list of convolutional layers with noise inputs and AdaIN layers
    self.layers = nn.ModuleList()
    for i in range(n_layers):
      # If the resolution level is not the minimum, add an upsampling layer
      if i > 0:
        self.layers.append(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False))
      # Add a convolutional layer with the corresponding number of input and output channels and filter size
      self.layers.append(nn.Conv2d(n_channels[i], n_channels[i+1], n_filters, padding=n_filters//2))
      # Add a noise input layer that adds random noise to the feature maps
      self.layers.append(NoiseInput(n_channels[i+1]))
      # Add an activation function layer
      self.layers.append(nn.LeakyReLU(0.2))
      # Add an AdaIN layer that modulates the feature maps using the style parameters
      self.layers.append(AdaIN(n_channels[i+1]))
  
  def forward(self, z, resolution_level):
    # Map the latent code z to the intermediate latent space W using the mapping network
    w = self.mapping_network(z)
    # Repeat the constant input to match the batch size
    x = self.constant_input.repeat(z.size(0), 1, 1, 1)
    # Loop over the layers until the current resolution level is reached
    for i in range(resolution_level + 1):
      # Apply the layer to the input x using the corresponding style parameter w
      x = self.layers[i](x, w[:, i])
    # Return the output image x
    return x

# Define the noise input layer that adds random noise to the feature maps 
class NoiseInput(nn.Module):
  def __init__(self, n_channels):
    super(NoiseInput, self).__init__()
    # Initialize a noise strength parameter that is learned during training 
    self.noise_strength = nn.Parameter(torch.zeros(1))
  
  def forward(self, x):
    # Sample random noise from a normal distribution with the same size as the input x 
    noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device)
    # Add the noise to the input x scaled by the noise strength parameter 
    x = x + self.noise_strength * noise
    # Return the output x 
    return x

# Define the minibatch standard deviation layer that computes the standard deviation of each feature map across the batch dimension and appends it as an extra channel 
class MinibatchStddev(nn.Module):
  def __init__(self):
    super(MinibatchStddev, self).__init__()
  
  def forward(self, x):
    # Compute the mean and standard deviation of each feature map across the batch dimension 
    mean = torch.mean(x, dim=0, keepdim=True)
    std = torch.std(x - mean, dim=0, keepdim=True)
    # Compute the mean of the standard deviation across all feature maps 
    std_mean = torch.mean(std)
    # Append the mean of the standard deviation as an extra channel to the input x 
    std_mean = std_mean.expand(x.size(0), 1, x.size(2), x.size(3))
    x = torch.cat([x, std_mean], dim=1)
    # Return the output x 
    return x

# Define the discriminator that maps an image x to a score s
class Discriminator(nn.Module):
  def __init__(self, n_layers, n_channels, n_filters):
    super(Discriminator, self).__init__()
    # Initialize a list of convolutional layers with minibatch standard deviation and activation function layers 
    self.layers = nn.ModuleList()
    for i in range(n_layers):
      # If the resolution level is not the maximum, add a minibatch standard deviation layer 
      if i == 0:
        self.layers.append(MinibatchStddev())
      # Add a convolutional layer with the corresponding number of input and output channels and filter size 
      self.layers.append(nn.Conv2d(n_channels[i] + (i == 0), n_channels[i-1], n_filters, padding=n_filters//2))
      # Add an activation function