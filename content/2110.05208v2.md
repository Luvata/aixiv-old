---
title: 2110.05208v2 Supervision Exists Everywhere  A Data Efficient Contrastive Language-Image Pre-training Paradigm
date: 2021-10-06
---

# [Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm](http://arxiv.org/abs/2110.05208v2)

authors: Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, Junjie Yan


## What, Why and How

[1]: https://arxiv.org/abs/2110.05208 "[2110.05208] Supervision Exists Everywhere: A Data Efficient ..."
[2]: https://arxiv.org/pdf/2110.05208.pdf "arXiv:2110.05208v2 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2110.05208v2.pdf "arXiv.org e-Print archive"

Here is a summary of the paper:

```
What: The paper proposes a data efficient contrastive language-image pre-training paradigm called DeCLIP, which utilizes multiple types of supervision among the image-text pairs to learn generic visual features.

Why: The paper aims to address the limitation of existing contrastive language-image pre-training methods, such as CLIP and ALIGN, which require a large amount of image-text pairs for pre-training and are not scalable.

How: The paper introduces three types of supervision that can be exploited from the image-text pairs: self-supervision within each modality, multi-view supervision across modalities, and nearest-neighbor supervision from other similar pairs. The paper also proposes a novel loss function that combines these supervision signals and balances their contributions. The paper evaluates DeCLIP on zero-shot recognition and downstream tasks, and shows that it outperforms CLIP and ALIGN with much less data.
```

## Main Contributions

[1]: https://arxiv.org/abs/2110.05208 "[2110.05208] Supervision Exists Everywhere: A Data Efficient ..."
[2]: https://arxiv.org/pdf/2110.05208.pdf "arXiv:2110.05208v2 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2110.05208v2.pdf "arXiv.org e-Print archive"

The paper claims the following contributions:

- A novel data efficient contrastive language-image pre-training paradigm, DeCLIP, which leverages multiple types of supervision among the image-text pairs.
- A novel loss function that combines and balances the different supervision signals, and a simple yet effective data sampling strategy that ensures data diversity and quality.
- Extensive experiments on zero-shot recognition and downstream tasks, showing that DeCLIP outperforms existing methods with much less data and achieves state-of-the-art results on some benchmarks.

## Method Summary

[1]: https://arxiv.org/abs/2110.05208 "[2110.05208] Supervision Exists Everywhere: A Data Efficient ..."
[2]: https://arxiv.org/pdf/2110.05208.pdf "arXiv:2110.05208v2 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2110.05208v2.pdf "arXiv.org e-Print archive"

Here is a summary of the method section:

```
The method section consists of three subsections: (1) Preliminaries, (2) DeCLIP Framework, and (3) Implementation Details.

(1) Preliminaries: This subsection reviews the background of contrastive language-image pre-training, such as CLIP and ALIGN, and introduces the notation and problem formulation.

(2) DeCLIP Framework: This subsection presents the main components of DeCLIP, including the three types of supervision (self-supervision, multi-view supervision, and nearest-neighbor supervision), the loss function that combines them, and the data sampling strategy that ensures data diversity and quality.

(3) Implementation Details: This subsection describes the details of the model architecture, the pre-training dataset, the optimization algorithm, and the hyperparameters used in DeCLIP.
```

## Pseudo Code - High level

[1]: https://arxiv.org/abs/2110.05208 "[2110.05208] Supervision Exists Everywhere: A Data Efficient ..."
[2]: https://arxiv.org/pdf/2110.05208.pdf "arXiv:2110.05208v2 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2110.05208v2.pdf "arXiv.org e-Print archive"

Here is the high-level pseudo code for the paper:

```
# Define the model architecture
model = EncoderDecoder(image_encoder, text_encoder, image_decoder, text_decoder)

# Define the loss function
loss = ContrastiveLoss() + SelfSupervisionLoss() + MultiViewLoss() + NearestNeighborLoss()

# Define the optimizer
optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

# Define the data sampler
sampler = DataSampler(dataset, batch_size, num_negatives, num_neighbors)

# Pre-train the model
for epoch in range(num_epochs):
  for batch in sampler:
    # Get the image-text pairs and their variants
    images, texts, images_aug, texts_aug, images_nn, texts_nn = batch
    
    # Forward pass
    image_embeds = model.image_encoder(images)
    text_embeds = model.text_encoder(texts)
    image_recons = model.image_decoder(image_embeds)
    text_recons = model.text_decoder(text_embeds)
    image_embeds_aug = model.image_encoder(images_aug)
    text_embeds_aug = model.text_encoder(texts_aug)
    image_embeds_nn = model.image_encoder(images_nn)
    text_embeds_nn = model.text_encoder(texts_nn)
    
    # Compute the loss
    total_loss = loss(image_embeds, text_embeds, image_recons, text_recons, image_embeds_aug, text_embeds_aug, image_embeds_nn, text_embeds_nn)
    
    # Backward pass and update
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
```

## Pseudo Code - Detail

[1]: https://arxiv.org/abs/2110.05208 "[2110.05208] Supervision Exists Everywhere: A Data Efficient ..."
[2]: https://arxiv.org/pdf/2110.05208.pdf "arXiv:2110.05208v2 [cs.CV] 14 Mar 2022"
[3]: https://arxiv.org/pdf/2110.05208v2.pdf "arXiv.org e-Print archive"

Here is the detailed pseudo code for the paper:

```
# Define the model architecture
# Use a ResNet-based encoder for images and a Transformer-based encoder for texts
# Use a PixelCNN-based decoder for images and a Transformer-based decoder for texts
# Use a cosine similarity layer to measure the similarity between image and text embeddings
model = EncoderDecoder(image_encoder, text_encoder, image_decoder, text_decoder, similarity_layer)

# Define the loss function
# Use a temperature-scaled cross entropy loss for contrastive learning
# Use a mean squared error loss for self-supervision within each modality
# Use a multi-view consistency loss for multi-view supervision across modalities
# Use a nearest-neighbor consistency loss for nearest-neighbor supervision from other similar pairs
# Use a weighted sum of these losses as the total loss
loss = ContrastiveLoss() + SelfSupervisionLoss() + MultiViewLoss() + NearestNeighborLoss()
total_loss = alpha * ContrastiveLoss() + beta * SelfSupervisionLoss() + gamma * MultiViewLoss() + delta * NearestNeighborLoss()

# Define the optimizer
# Use AdamW with linear warmup and decay
optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, total_steps=total_steps)

# Define the data sampler
# Use a data sampler that randomly samples image-text pairs from the dataset
# For each pair, generate multiple variants by applying data augmentation techniques such as cropping, flipping, masking, etc.
# For each pair, also find its nearest neighbors in the dataset based on image or text similarity
sampler = DataSampler(dataset, batch_size, num_negatives, num_neighbors)

# Pre-train the model
for epoch in range(num_epochs):
  for batch in sampler:
    # Get the image-text pairs and their variants
    # images: original images
    # texts: original texts
    # images_aug: augmented images (e.g., cropped, flipped, masked, etc.)
    # texts_aug: augmented texts (e.g., masked, shuffled, replaced, etc.)
    # images_nn: nearest neighbor images based on image similarity
    # texts_nn: nearest neighbor texts based on text similarity
    images, texts, images_aug, texts_aug, images_nn, texts_nn = batch
    
    # Forward pass
    # Compute the image and text embeddings for each variant
    # Compute the image and text reconstructions for each variant
    # Compute the similarity scores between image and text embeddings for each variant
    image_embeds = model.image_encoder(images)
    text_embeds = model.text_encoder(texts)
    image_recons = model.image_decoder(image_embeds)
    text_recons = model.text_decoder(text_embeds)
    image_embeds_aug = model.image_encoder(images_aug)
    text_embeds_aug = model.text_encoder(texts_aug)
    image_recons_aug = model.image_decoder(image_embeds_aug)
    text_recons_aug = model.text_decoder(text_embeds_aug)
    image_embeds_nn = model.image_encoder(images_nn)
    text_embeds_nn = model.text_encoder(texts_nn)
    image_recons_nn = model.image_decoder(image_embeds_nn)
    text_recons_nn = model.text_decoder(text_embeds_nn)
    sim_scores = model.similarity_layer(image_embeds, text_embeds)
    sim_scores_aug = model.similarity_layer(image_embeds_aug, text_embeds_aug)
    sim_scores_nn = model.similarity_layer(image_embeds_nn, text_embeds_nn)
    
    # Compute the loss
    # Compute the contrastive loss based on the similarity scores and the ground truth labels (1 for matched pairs and 0 for non-matched pairs)
    # Compute the self-supervision loss based on the reconstruction errors of images and texts
    # Compute the multi-view loss based on the consistency of embeddings across different variants of the same pair
    # Compute the nearest-neighbor loss based on the consistency of embeddings across similar pairs in the dataset
    # Compute the total loss as a weighted sum of these losses
    contrastive_loss = ContrastiveLoss(sim_scores, labels)
    self_supervision_loss = SelfSupervisionLoss(image_recons, images) + SelfSupervisionLoss(text_recons, texts)
    multi_view_loss = MultiViewLoss(image_embeds, image_embeds_aug) + MultiViewLoss(text_embeds, text_embeds_aug)
    nearest_neighbor_loss = NearestNeighborLoss(image_embeds, image_embeds_nn) + NearestNeighborLoss(text_embeds, text_embeds_nn)
    total_loss = alpha * contrastive_loss + beta * self_supervision_loss + gamma * multi_view_loss + delta * nearest_neighbor_loss
    
    # Backward pass and update
    # Zero out the gradients
    # Compute the gradients of the total loss with respect to the model parameters
    # Clip the gradients to avoid exploding gradients
    # Update the model parameters using the optimizer
    # Update the learning rate using the scheduler
    optimizer.zero_grad()
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
    optimizer.step()
    scheduler.step()
```