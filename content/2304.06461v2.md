---
title: 2304.06461v2 Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning
date: 2023-04-07
---

# [Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning](http://arxiv.org/abs/2304.06461v2)

authors: Kaiyou Song, Jin Xie, Shan Zhang, Zimeng Luo


## What, Why and How

[1]: https://arxiv.org/pdf/2304.06461v2.pdf "{songkaiyou, xiejin, zhangshan, luozimeng}@megvii.com arXiv:2304 ..."
[2]: https://arxiv.org/abs/2304.06461 "[2304.06461] Multi-Mode Online Knowledge Distillation for Self ..."
[3]: http://arxiv-export2.library.cornell.edu/abs/2304.06461 "[2304.06461] Multi-Mode Online Knowledge Distillation for Self ..."

Here is a summary of the paper:

- **What**: The paper proposes a Multi-mode Online Knowledge Distillation method (MOKD) to boost self-supervised visual representation learning[^1^][1] [^2^][2] [^3^][3].
- **Why**: The paper aims to address the limitations of existing methods that combine self-supervised learning (SSL) with knowledge distillation (SSL-KD), which transfer knowledge from a static pre-trained teacher to a student[^1^][1].
- **How**: The paper introduces two distillation modes: self-distillation and cross-distillation. In self-distillation, each model performs SSL independently. In cross-distillation, two different models interact with each other through a cross-attention feature search strategy that enhances the semantic feature alignment between them[^1^][1]. The paper evaluates MOKD on different backbones and datasets and shows that it outperforms existing SSL-KD methods for both the student and teacher models[^1^][1].

## Main Contributions

The paper claims the following contributions:

- It proposes a novel Multi-mode Online Knowledge Distillation method (MOKD) that enables two different models to learn collaboratively in a self-supervised manner.
- It introduces a cross-attention feature search strategy that improves the semantic feature alignment between different models in cross-distillation mode.
- It demonstrates the effectiveness of MOKD on various backbones and datasets and shows that it surpasses existing SSL-KD methods for both the student and teacher models.

## Method Summary

[1]: https://arxiv.org/pdf/2304.06461v2.pdf "{songkaiyou, xiejin, zhangshan, luozimeng}@megvii.com arXiv:2304 ..."
[2]: https://arxiv.org/abs/2304.06461 "[2304.06461] Multi-Mode Online Knowledge Distillation for Self ..."
[3]: http://arxiv-export2.library.cornell.edu/abs/2304.06461 "[2304.06461] Multi-Mode Online Knowledge Distillation for Self ..."

Here is a summary of the method section of the paper:

- The paper proposes a Multi-mode Online Knowledge Distillation method (MOKD) that consists of two distillation modes: self-distillation and cross-distillation[^1^][1].
- In self-distillation mode, each model performs self-supervised learning independently using contrastive learning with a memory bank[^1^][1].
- In cross-distillation mode, two different models interact with each other through a cross-attention feature search strategy that enhances the semantic feature alignment between them[^1^][1].
- The cross-attention feature search strategy consists of three steps: feature extraction, feature matching, and feature alignment[^1^][1].
- Feature extraction uses a convolutional layer to extract features from the input images[^1^][1].
- Feature matching uses a cosine similarity matrix to find the most similar features between different models[^1^][1].
- Feature alignment uses a weighted sum of the matched features to generate new features for each model[^1^][1].
- The paper uses an exponential-moving-average (EMA) mechanism to update the model parameters and the memory bank in both modes[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Initialize two models M1 and M2 with different architectures
M1 = Model1()
M2 = Model2()

# Initialize two memory banks B1 and B2 with size K
B1 = MemoryBank(K)
B2 = MemoryBank(K)

# Initialize two EMA updaters U1 and U2 with decay rate alpha
U1 = EMAUpdater(alpha)
U2 = EMAUpdater(alpha)

# Define the loss functions L1 and L2 for contrastive learning
L1 = ContrastiveLoss()
L2 = ContrastiveLoss()

# Define the hyperparameters lambda and beta for cross-distillation
lambda = 0.5
beta = 0.01

# Define the feature extraction layer F
F = ConvLayer()

# Define the feature alignment layer A
A = AlignmentLayer()

# For each iteration
for i in range(iterations):

  # Sample a mini-batch of images X
  X = sample_batch()

  # Apply random augmentations to X and get two views X1 and X2
  X1, X2 = augment(X)

  # Perform self-distillation for M1
  Z1_1, Z2_1 = M1(X1), M1(X2) # Forward pass
  L_self_1 = L1(Z1_1, Z2_1, B1) # Contrastive loss
  L_self_1.backward() # Backward pass
  U1.update(M1) # Update model parameters with EMA
  U1.update(B1) # Update memory bank with EMA

  # Perform self-distillation for M2
  Z1_2, Z2_2 = M2(X1), M2(X2) # Forward pass
  L_self_2 = L2(Z1_2, Z2_2, B2) # Contrastive loss
  L_self_2.backward() # Backward pass
  U2.update(M2) # Update model parameters with EMA
  U2.update(B2) # Update memory bank with EMA

  # Perform cross-distillation between M1 and M2
  F1_1, F2_1 = F(Z1_1), F(Z2_1) # Feature extraction for M1
  F1_2, F2_2 = F(Z1_2), F(Z2_2) # Feature extraction for M2
  S11, S12, S21, S22 = feature_matching(F1_1, F2_1, F1_2, F2_2) # Feature matching between models
  A11, A12, A21, A22 = A(F11, F12, F21, F22, S11, S12, S21, S22) # Feature alignment between models
  L_cross_11 = L1(A11 + lambda * Z11, A12 + lambda * Z12, B11) # Cross-distillation loss for M11
  L_cross_12 = L1(A21 + lambda * Z21, A22 + lambda * Z22, B12) # Cross-distillation loss for M12
  L_cross_21 = L2(A21 + lambda * Z21, A22 + lambda * Z22, B21) # Cross-distillation loss for M21
  L_cross_22 = L2(A11 + lambda * Z11, A12 + lambda * Z12, B22) # Cross-distillation loss for M22
  L_cross = beta * (L_cross_11 + L_cross_12 + L_cross_21 + L_cross_22) # Total cross-distillation loss
  L_cross.backward() # Backward pass

```


## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torchvision
import numpy as np

# Define the model architectures for M1 and M2
# For example, using ResNet-50 and ResNet-18
M1 = torchvision.models.resnet50(pretrained=False)
M2 = torchvision.models.resnet18(pretrained=False)

# Define the output dimension for the projection head
D = 128

# Add a projection head to each model
M1.fc = torch.nn.Linear(M1.fc.in_features, D)
M2.fc = torch.nn.Linear(M2.fc.in_features, D)

# Define the temperature parameter for contrastive learning
tau = 0.07

# Define the loss functions L1 and L2 for contrastive learning
# Using InfoNCE loss with memory bank
L1 = InfoNCELoss(tau)
L2 = InfoNCELoss(tau)

# Define the memory bank size K
K = 65536

# Initialize two memory banks B1 and B2 with size K and dimension D
B1 = torch.randn(K, D)
B2 = torch.randn(K, D)

# Normalize the memory banks
B1 = torch.nn.functional.normalize(B1, dim=1)
B2 = torch.nn.functional.normalize(B2, dim=1)

# Define the decay rate alpha for EMA
alpha = 0.999

# Initialize two EMA updaters U1 and U2 with decay rate alpha
U1 = EMAUpdater(alpha)
U2 = EMAUpdater(alpha)

# Define the hyperparameters lambda and beta for cross-distillation
lambda = 0.5
beta = 0.01

# Define the feature extraction layer F
# Using a 3x3 convolutional layer with stride 1 and padding 1
F = torch.nn.Conv2d(D, D, kernel_size=3, stride=1, padding=1)

# Define the feature alignment layer A
# Using a linear layer with input and output dimension D
A = torch.nn.Linear(D, D)

# Define the optimizer and learning rate scheduler
# For example, using SGD with momentum and cosine annealing
optimizer = torch.optim.SGD([M1.parameters(), M2.parameters()], lr=0.03, momentum=0.9, weight_decay=0.0001)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# Define the number of iterations and batch size
iterations = 200000
batch_size = 256

# Define the data loader for unlabeled images
# For example, using ImageNet dataset with random cropping and flipping
dataset = torchvision.datasets.ImageFolder(root='imagenet/train', transform=torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(224),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
]))
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)

# Move the models and memory banks to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
M1.to(device)
M2.to(device)
B1.to(device)
B2.to(device)

# For each iteration
for i in range(iterations):

  # Sample a mini-batch of images X and labels Y from the data loader
  X, Y = next(iter(dataloader))

  # Move the images and labels to GPU if available
  X.to(device)
  Y.to(device)

  # Apply random augmentations to X and get two views X1 and X2
  # For example, using color jittering and Gaussian blur
  transform = torchvision.transforms.Compose([
    torchvision.transforms.ColorJitter(0.4, 0.4, 0.4),
    torchvision.transforms.GaussianBlur(23),
    torchvision.transforms.ToTensor()
  ])
  X1 = transform(X)
  X2 = transform(X)

  # Perform self-distillation for M1

  # Set M1 to train mode and M2 to eval mode
  M1.train()
  M2.eval()

  # Forward pass for M1 with both views X1 and X2 
  Z1_1 = M1(X1) # Shape: (batch_size, D)
  Z2_1 = M1(X2) # Shape: (batch_size, D)

  # Normalize the output features of M1 
  Z1_1 = torch.nn.functional.normalize(Z1_1, dim=1)
  Z2_1 = torch.nn.functional.normalize(Z2_1, dim=1)

  # Compute the contrastive loss for M1 using L1 and B1
  L_self_1 = L1(Z1_1, Z2_1, B1, Y)

  # Backward pass for M1
  optimizer.zero_grad()
  L_self_1.backward()

  # Update the model parameters of M1 with SGD
  optimizer.step()

  # Update the model parameters and memory bank of M1 with EMA
  U1.update(M1)
  U1.update(B1)

  # Perform self-distillation for M2

  # Set M2 to train mode and M1 to eval mode
  M2.train()
  M1.eval()

  # Forward pass for M2 with both views X1 and X2 
  Z1_2 = M2(X1) # Shape: (batch_size, D)
  Z2_2 = M2(X2) # Shape: (batch_size, D)

  # Normalize the output features of M2 
  Z1_2 = torch.nn.functional.normalize(Z1_2, dim=1)
  Z2_2 = torch.nn.functional.normalize(Z2_2, dim=1)

  # Compute the contrastive loss for M2 using L2 and B2
  L_self_2 = L2(Z1_2, Z2_2, B2, Y)

  # Backward pass for M2
  optimizer.zero_grad()
  L_self_2.backward()

  # Update the model parameters of M2 with SGD
  optimizer.step()

  # Update the model parameters and memory bank of M2 with EMA
  U2.update(M2)
  U2.update(B2)

  # Perform cross-distillation between M1 and M2

  # Set both models to eval mode
  M1.eval()
  M2.eval()

  # Extract features from the output features of both models using F
  F1_1 = F(Z1_1) # Shape: (batch_size, D, H, W)
  F2_1 = F(Z2_1) # Shape: (batch_size, D, H, W)
  F1_2 = F(Z1_2) # Shape: (batch_size, D, H, W)
  F2_2 = F(Z2_2) # Shape: (batch_size, D, H, W)

  # Perform feature matching between different models using cosine similarity
  S11 = torch.einsum('bchw,bchw->bc', F1_1, F1_2) / D # Shape: (batch_size, H*W)
  S12 = torch.einsum('bchw,bchw->bc', F2_1, F2_2) / D # Shape: (batch_size, H*W)
  S21 = torch.einsum('bchw,bchw->bc', F1_2, F1_1) / D # Shape: (batch_size, H*W)
  S22 = torch.einsum('bchw,bchw->bc', F2_2, F2_1) / D # Shape: (batch_size, H*W)

  # Perform feature alignment between different models using A and weighted sum
  A11 = A(torch.sum(F1_1 * S11.unsqueeze(3).unsqueeze(4), dim=(0, -3))) + lambda * Z11 # Shape: (D,)
  A12 = A(torch.sum(F12 * S12.unsqueeze(3).unsqueeze(4), dim=(0, -3))) + lambda * Z12 # Shape: (D,)
  A21 = A(torch.sum(F21 * S21.unsqueeze(3).unsqueeze(4), dim=(0, -3))) + lambda * Z21 # Shape: (D,)
  A22 = A(torch.sum(F22 * S22.unsqueeze(3).unsqueeze(4), dim=(0, -3))) + lambda * Z22 # Shape: (D,)
  
```