---
title: 2112.10098v1 Initiative Defense against Facial Manipulation
date: 2021-12-11
---

# [Initiative Defense against Facial Manipulation](http://arxiv.org/abs/2112.10098v1)

authors: Qidong Huang, Jie Zhang, Wenbo Zhou, WeimingZhang, Nenghai Yu


## What, Why and How

[1]: https://arxiv.org/pdf/2112.10098v1 "arXiv:2112.10098v1 [cs.CV] 19 Dec 2021"
[2]: https://arxiv.org/abs/2112.10098 "[2112.10098] Initiative Defense against Facial Manipulation - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.10098v1 "[2112.10098v1] Initiative Defense against Facial Manipulation"

Here is a summary of the paper:

- **What**: The paper proposes a novel framework of initiative defense against facial manipulation, which is to actively inject imperceptible venom into target facial data before manipulation, so as to degrade the performance of the malicious manipulation models.
- **Why**: The paper aims to address the limitation of existing detection-based methods, which can only work for ex-post forensics but cannot prevent the engendering of malicious behavior. The paper also hopes to shed some light on initiative countermeasures against more adversarial scenarios.
- **How**: The paper first imitates the target manipulation model with a surrogate model, and then devises a poison perturbation generator to obtain the desired venom. An alternating training strategy is further leveraged to train both the surrogate model and the perturbation generator. Two typical facial manipulation tasks: face attribute editing and face reenactment, are considered in the initiative defense framework. Extensive experiments demonstrate the effectiveness and robustness of the framework in different settings.

The summary is based on the abstract[^1^][1] and introduction[^1^][1] of the paper.

## Main Contributions

[1]: https://arxiv.org/pdf/2112.10098v1 "arXiv:2112.10098v1 [cs.CV] 19 Dec 2021"
[2]: https://arxiv.org/abs/2112.10098 "[2112.10098] Initiative Defense against Facial Manipulation - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.10098v1 "[2112.10098v1] Initiative Defense against Facial Manipulation"

According to the paper[^1^][1], the main contributions are:

- **The first initiative defense framework against facial manipulation**, which can actively inject imperceptible venom into target facial data before manipulation, so as to degrade the performance of the malicious manipulation models.
- **A novel poison perturbation generator**, which can generate venom that is visually consistent to the clean data and can significantly reduce the quality of the manipulated results.
- **An alternating training strategy**, which can train both the surrogate model and the perturbation generator in an adversarial manner, and can adapt to different facial manipulation tasks and models.
- **Extensive experiments** on two typical facial manipulation tasks: face attribute editing and face reenactment, which demonstrate the effectiveness and robustness of the proposed framework in different settings.

## Method Summary

[1]: https://arxiv.org/pdf/2112.10098v1 "arXiv:2112.10098v1 [cs.CV] 19 Dec 2021"
[2]: https://arxiv.org/abs/2112.10098 "[2112.10098] Initiative Defense against Facial Manipulation - arXiv.org"
[3]: http://export.arxiv.org/abs/2112.10098v1 "[2112.10098v1] Initiative Defense against Facial Manipulation"

Here is a summary of the method section of the paper:

- **The method section** describes the proposed framework of initiative defense against facial manipulation, which consists of three main components: a surrogate model, a poison perturbation generator, and an alternating training strategy.
- **The surrogate model** is used to imitate the target manipulation model controlled by the malicious user, and to provide feedback to the perturbation generator. The surrogate model can be any facial manipulation model that can be trained on public datasets, such as StarGAN[^1^][1] or Face2Face[^1^][1].
- **The poison perturbation generator** is used to generate venom that can be added to the target facial data before manipulation, so as to degrade the performance of the target model. The perturbation generator is a conditional GAN that takes the clean facial data and the desired attribute as inputs, and outputs the infected facial data with imperceptible venom. The perturbation generator is trained to minimize a loss function that balances between visual consistency and manipulation degradation[^1^][1].
- **The alternating training strategy** is used to train both the surrogate model and the perturbation generator in an adversarial manner, and to adapt to different facial manipulation tasks and models. The training strategy alternates between two phases: imitation phase and infection phase. In the imitation phase, the surrogate model is trained to imitate the target model on clean facial data. In the infection phase, the perturbation generator is trained to generate venom that can fool the surrogate model on infected facial data[^1^][1].

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Initialize the surrogate model M and the perturbation generator G
M = SurrogateModel()
G = PerturbationGenerator()

# Initialize the clean facial data D and the desired attribute A
D = CleanFacialData()
A = DesiredAttribute()

# Initialize the imitation loss L_imit and the infection loss L_infect
L_imit = ImitationLoss()
L_infect = InfectionLoss()

# Repeat until convergence
while not converged:

  # Imitation phase: train M to imitate the target model on D
  for i in range(imitation_steps):
    # Sample a batch of clean facial data and desired attribute
    d, a = sample(D, A)
    # Generate manipulated facial data using M
    m = M(d, a)
    # Calculate the imitation loss using the target model
    l_imit = L_imit(m, target_model(d, a))
    # Update M to minimize l_imit
    M.update(l_imit)

  # Infection phase: train G to generate venom that can fool M on D
  for j in range(infection_steps):
    # Sample a batch of clean facial data and desired attribute
    d, a = sample(D, A)
    # Generate infected facial data using G
    i = G(d, a)
    # Generate manipulated facial data using M
    m = M(i, a)
    # Calculate the infection loss using the clean facial data and the target model
    l_infect = L_infect(i, d, m, target_model(i, a))
    # Update G to minimize l_infect
    G.update(l_infect)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Import the necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms

# Define the hyperparameters
batch_size = 16 # The size of each batch of facial data
imitation_steps = 10 # The number of steps to train the surrogate model in each iteration
infection_steps = 10 # The number of steps to train the perturbation generator in each iteration
lambda_1 = 0.01 # The weight of the L1 loss in the infection loss
lambda_2 = 0.01 # The weight of the perceptual loss in the infection loss
lambda_3 = 0.01 # The weight of the adversarial loss in the infection loss

# Define the surrogate model M as a StarGAN model (Choi et al., 2018)
M = StarGAN()

# Define the perturbation generator G as a conditional GAN model (Mirza and Osindero, 2014)
G = ConditionalGAN()

# Define the target model as a Face2Face model (Thies et al., 2016)
target_model = Face2Face()

# Define the imitation loss L_imit as a mean squared error loss
L_imit = nn.MSELoss()

# Define the infection loss L_infect as a combination of L1, perceptual and adversarial losses
L_infect = lambda_1 * nn.L1Loss() + lambda_2 * PerceptualLoss() + lambda_3 * AdversarialLoss()

# Define the optimizers for M and G
optimizer_M = optim.Adam(M.parameters(), lr=0.0001, betas=(0.5, 0.999))
optimizer_G = optim.Adam(G.parameters(), lr=0.0001, betas=(0.5, 0.999))

# Load the clean facial data D and the desired attribute A from a public dataset (e.g. CelebA)
D = torchvision.datasets.CelebA(root='./data', split='train', transform=transforms.ToTensor(), download=True)
A = D.attr # The attribute labels of D
D = D.data # The image data of D

# Create data loaders for D and A
loader_D = torch.utils.data.DataLoader(D, batch_size=batch_size, shuffle=True)
loader_A = torch.utils.data.DataLoader(A, batch_size=batch_size, shuffle=True)

# Repeat until convergence
while not converged:

  # Imitation phase: train M to imitate the target model on D
  for i in range(imitation_steps):
    # Sample a batch of clean facial data and desired attribute
    d, a = next(iter(loader_D)), next(iter(loader_A))
    # Generate manipulated facial data using M
    m = M(d, a)
    # Calculate the imitation loss using the target model
    l_imit = L_imit(m, target_model(d, a))
    # Update M to minimize l_imit
    optimizer_M.zero_grad()
    l_imit.backward()
    optimizer_M.step()

  # Infection phase: train G to generate venom that can fool M on D
  for j in range(infection_steps):
    # Sample a batch of clean facial data and desired attribute
    d, a = next(iter(loader_D)), next(iter(loader_A))
    # Generate infected facial data using G
    i = G(d, a)
    # Generate manipulated facial data using M
    m = M(i, a)
    # Calculate the infection loss using the clean facial data and the target model
    l_infect = L_infect(i, d, m, target_model(i, a))
    # Update G to minimize l_infect
    optimizer_G.zero_grad()
    l_infect.backward()
    optimizer_G.step()
```