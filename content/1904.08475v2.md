---
title: 1904.08475v2 Image Resizing by Reconstruction from Deep Features
date: 2019-04-09
---

# [Image Resizing by Reconstruction from Deep Features](http://arxiv.org/abs/1904.08475v2)

authors: Moab Arar, Dov Danon, Daniel Cohen-Or, Ariel Shamir


## What, Why and How

[1]: https://arxiv.org/abs/1904.08475v2 "Image Resizing by Reconstruction from Deep Features"
[2]: https://arxiv.org/pdf/1904.08475v2.pdf "Abstract arXiv:1904.08475v2 [cs.CV] 22 Jun 2021"
[3]: http://export.arxiv.org/abs/2211.08475v2 "[2211.08475v2] AutoDRIVE -- Technical Report"

Here is a summary of the paper[^1^][1] in terms of What, Why, and How:

- **What**: The paper proposes a novel method for image resizing that works in feature space rather than pixel space. It uses a pre-trained classification network to extract deep features from the input image, adjusts the size of the feature maps according to the desired output shape, and reconstructs the resized image using a neural-network based optimization.
- **Why**: The paper aims to address the limitations of traditional image resizing methods that use various saliency measures to preserve important content. These methods often disregard high-level semantic information and introduce artifacts that reduce the naturalness of the resulting image. The paper claims that by working in feature space, it can leverage the hierarchical encoding and the high-level discriminative power of the deep network, and thus maintain the aspect ratio of semantic objects and regions and diminish the artifacts.
- **How**: The paper describes the main steps of its method as follows: (1) Given an input image and a target shape, it feeds the image to a pre-trained classification network (VGG-19) and extracts feature maps from different layers. (2) It applies a resizing operator (such as linear scaling or seam carving) to each feature map, while protecting important semantic regions using a mask generated from the deepest layer. (3) It reconstructs the resized image by minimizing a loss function that consists of three terms: a reconstruction term that measures the difference between the resized feature maps and the feature maps of the output image, a perceptual term that measures the difference between high-level features of the input and output images, and a regularization term that encourages smoothness and naturalness of the output image. The reconstruction is done using an iterative optimization algorithm (L-BFGS) that updates the output image pixel values at each iteration.

## Main Contributions

[1]: https://arxiv.org/abs/1904.08475v2 "Image Resizing by Reconstruction from Deep Features"
[2]: https://arxiv.org/pdf/1904.08475v2.pdf "Abstract arXiv:1904.08475v2 [cs.CV] 22 Jun 2021"
[3]: http://export.arxiv.org/abs/2211.08475v2 "[2211.08475v2] AutoDRIVE -- Technical Report"

According to the paper[^1^][1], the main contributions are:

- **A novel image resizing method that works in feature space rather than pixel space**, using a pre-trained classification network and a neural-network based optimization.
- **A feature map resizing operator that preserves important semantic regions** using a mask generated from the deepest layer of the network.
- **A reconstruction loss function that combines reconstruction, perceptual, and regularization terms** to minimize the difference between the resized and output feature maps, the high-level features of the input and output images, and the smoothness and naturalness of the output image, respectively.
- **An extensive evaluation on benchmarks and challenging images**, showing that the proposed method outperforms existing methods in terms of semantic preservation and artifact reduction.

## Method Summary

[1]: https://arxiv.org/abs/1904.08475v2 "Image Resizing by Reconstruction from Deep Features"
[2]: https://arxiv.org/pdf/1904.08475v2.pdf "Abstract arXiv:1904.08475v2 [cs.CV] 22 Jun 2021"
[3]: http://export.arxiv.org/abs/2211.08475v2 "[2211.08475v2] AutoDRIVE -- Technical Report"

Here is a summary of the method section of the paper[^1^][1]:

- The method consists of two main steps: feature map resizing and image reconstruction.
- Feature map resizing: Given an input image and a target shape, the method feeds the image to a pre-trained classification network (VGG-19) and extracts feature maps from different layers. It applies a resizing operator (such as linear scaling or seam carving) to each feature map, while protecting important semantic regions using a mask generated from the deepest layer of the network. The mask is computed by thresholding the activation values of the feature maps and applying morphological operations to remove noise and fill holes. The resized feature maps are then concatenated and fed to the next step.
- Image reconstruction: Given the resized feature maps, the method reconstructs the output image by minimizing a loss function that consists of three terms: a reconstruction term, a perceptual term, and a regularization term. The reconstruction term measures the difference between the resized feature maps and the feature maps of the output image, extracted from the same network. The perceptual term measures the difference between high-level features of the input and output images, extracted from another pre-trained network (VGG-16). The regularization term encourages smoothness and naturalness of the output image by penalizing high gradients and deviations from natural image statistics. The loss function is minimized using an iterative optimization algorithm (L-BFGS) that updates the output image pixel values at each iteration. The initial output image is set to be either a linearly scaled version of the input image or a random noise image.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```python
# Input: image I, target shape (w,h)
# Output: resized image R

# Load pre-trained networks VGG-19 and VGG-16
vgg19 = load_model("vgg19")
vgg16 = load_model("vgg16")

# Extract feature maps from different layers of VGG-19
F = vgg19.extract_features(I)

# Generate a mask from the deepest layer of VGG-19
M = threshold(F[-1])
M = morphological_operations(M)

# Apply a resizing operator to each feature map, while protecting the mask regions
F_resized = []
for f in F:
  f_resized = resize(f, (w,h), mask=M)
  F_resized.append(f_resized)

# Concatenate the resized feature maps
F_concat = concatenate(F_resized)

# Define the loss function
def loss(R):
  # Compute the reconstruction term
  R_features = vgg19.extract_features(R)
  L_rec = mean_squared_error(F_concat, R_features)

  # Compute the perceptual term
  I_features = vgg16.extract_features(I)
  R_features = vgg16.extract_features(R)
  L_per = mean_squared_error(I_features, R_features)

  # Compute the regularization term
  L_reg = gradient_penalty(R) + deviation_penalty(R)

  # Return the weighted sum of the terms
  return alpha * L_rec + beta * L_per + gamma * L_reg

# Initialize the output image to be a linearly scaled version of the input image or a random noise image
R = linear_scale(I, (w,h)) # or R = random_noise((w,h))

# Minimize the loss function using L-BFGS algorithm
R = optimize(loss, R)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```python
# Input: image I, target shape (w,h)
# Output: resized image R

# Load pre-trained networks VGG-19 and VGG-16
vgg19 = load_model("vgg19")
vgg16 = load_model("vgg16")

# Extract feature maps from different layers of VGG-19
# F is a list of tensors of shape (c_i, h_i, w_i), where c_i is the number of channels, h_i and w_i are the height and width of the feature map at layer i
F = vgg19.extract_features(I)

# Generate a mask from the deepest layer of VGG-19
# M is a tensor of shape (c_n, h_n, w_n), where c_n is the number of channels, h_n and w_n are the height and width of the feature map at the last layer n
M = F[-1]

# Threshold the activation values of M to get a binary mask
# M is a tensor of shape (c_n, h_n, w_n) with values 0 or 1
M = threshold(M, t=0.5)

# Apply morphological operations to remove noise and fill holes in M
# M is a tensor of shape (c_n, h_n, w_n) with values 0 or 1
M = opening(M, kernel_size=3) # erosion followed by dilation
M = closing(M, kernel_size=3) # dilation followed by erosion

# Apply a resizing operator to each feature map, while protecting the mask regions
# F_resized is a list of tensors of shape (c_i, h, w), where c_i is the number of channels, h and w are the target height and width
F_resized = []
for f in F:
  # Resize the mask M to match the shape of f
  # M_f is a tensor of shape (c_i, h_i, w_i) with values 0 or 1
  M_f = resize(M, (h_i, w_i))

  # Apply a resizing operator (such as linear scaling or seam carving) to f, while protecting the regions where M_f is 1
  # f_resized is a tensor of shape (c_i, h, w)
  f_resized = resize(f, (h,w), mask=M_f)

  # Append f_resized to F_resized
  F_resized.append(f_resized)

# Concatenate the resized feature maps along the channel dimension
# F_concat is a tensor of shape (C, h, w), where C is the sum of c_i for all i
F_concat = concatenate(F_resized)

# Define the loss function
def loss(R):
  # Compute the reconstruction term
  # R_features is a list of tensors of shape (c_i, h, w), where c_i is the number of channels
  R_features = vgg19.extract_features(R)

  # Concatenate R_features along the channel dimension
  # R_concat is a tensor of shape (C, h, w), where C is the sum of c_i for all i
  R_concat = concatenate(R_features)

  # Compute the mean squared error between F_concat and R_concat
  # L_rec is a scalar value
  L_rec = mean_squared_error(F_concat, R_concat)

  # Compute the perceptual term
  # I_features and R_features are lists of tensors of shape (c_j, h_j', w_j'), where c_j is the number of channels at layer j in VGG-16,
  # h_j' and w_j' are the height and width of the feature map at layer j in VGG-16 after resizing to match I or R
  I_features = vgg16.extract_features(resize(I,(h,w)))
  R_features = vgg16.extract_features(R)

  # Compute the mean squared error between I_features and R_features for each layer j in VGG-16 and sum them up with weights w_j
  # L_per is a scalar value
  L_per = 0
  for j in range(len(I_features)):
    L_per += w_j * mean_squared_error(I_features[j], R_features[j])

  # Compute the regularization term
  # L_reg is a scalar value

  # Compute the gradient penalty by taking the gradient magnitude of R and applying an L2 norm
  # G_R is a tensor of shape (h,w) with values >=0
  G_R = gradient_magnitude(R)

  # Compute the mean value of G_R and multiply it by a weight lambda_g
  # L_g is a scalar value
  L_g = lambda_g * mean(G_R)

  # Compute the deviation penalty by taking the difference between the mean and standard deviation of R and the mean and standard deviation of natural images
  # D_R is a scalar value >=0
  D_R = abs(mean(R) - mean_nat) + abs(std(R) - std_nat)

  # Multiply D_R by a weight lambda_d
  # L_d is a scalar value
  L_d = lambda_d * D_R

  # Sum up L_g and L_d to get L_reg
  L_reg = L_g + L_d

  # Return the weighted sum of the terms
  return alpha * L_rec + beta * L_per + gamma * L_reg

# Initialize the output image to be a linearly scaled version of the input image or a random noise image
# R is a tensor of shape (h,w,3) with values in [0,1]
R = linear_scale(I, (h,w)) # or R = random_noise((h,w,3))

# Minimize the loss function using L-BFGS algorithm
# R is a tensor of shape (h,w,3) with values in [0,1]
R = optimize(loss, R)
```