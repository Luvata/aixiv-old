---
title: 2207.05300v1 SD-GAN  Semantic Decomposition for Face Image Synthesis with Discrete Attribute
date: 2022-07-06
---

# [SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute](http://arxiv.org/abs/2207.05300v1)

authors: Zhou Kangneng, Zhu Xiaobin, Gao Daiheng, Lee Kai, Li Xinjie, Yin Xu-Cheng


## What, Why and How

[1]: https://arxiv.org/abs/2207.05300v1 "[2207.05300v1] SD-GAN: Semantic Decomposition for Face Image Synthesis ..."
[2]: https://arxiv.org/abs/2207.05300 "[2207.05300] SD-GAN: Semantic Decomposition for Face Image Synthesis ..."
[3]: http://export.arxiv.org/abs/2207.05300v1 "[2207.05300v1] SD-GAN: Semantic Decomposition for Face Image Synthesis ..."

Here is a summary of the paper:

- **What**: The paper proposes a novel framework for face image synthesis with discrete attributes (such as face mask and eyeglasses) using generative adversarial networks (GANs). The framework is called SD-GAN, which stands for Semantic Decomposition GAN.
- **Why**: The paper aims to address the challenge of facial discrete attribute synthesis, which is less explored than continuous attribute synthesis (such as age, pose and emotion). Existing methods may produce inaccurate results when applied to discrete attributes, due to the lack of semantic understanding and 3D awareness.
- **How**: The paper introduces two key components in SD-GAN: the semantic prior basis and the offset latent representation. The semantic prior basis provides an initial direction for manipulating the face representation in the latent space, while the offset latent representation adjusts the prior basis using a 3D-aware semantic fusion network. The fusion network also integrates 3D embedding for better identity preservation and discrete attribute synthesis. The paper also constructs a new dataset called MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) to evaluate the performance of SD-GAN. The paper claims that SD-GAN can generate photo-realistic face images with discrete attributes and outperforms state-of-the-art methods[^1^][1] [^2^][2].

## Main Contributions

According to the paper, the main contributions are:

- A novel framework for facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN, which explicitly decomposes the discrete attribute representation into two components: the semantic prior basis and the offset latent representation.
- A 3D-aware semantic fusion network that integrates 3D embedding for better identity preservation and discrete attribute synthesis. The fusion network also adjusts the prior basis using the offset latent representation.
- A large and valuable dataset MEGN for facial discrete attribute synthesis, which contains face images with mask and eyeglasses attributes crawled from Google and Naver.
- Extensive qualitative and quantitative experiments that demonstrate the state-of-the-art performance of SD-GAN on various facial discrete attributes.

## Method Summary

[1]: https://arxiv.org/abs/2207.05300v1 "[2207.05300v1] SD-GAN: Semantic Decomposition for Face ... - arXiv.org"
[2]: https://arxiv.org/pdf/2105.05300v1.pdf "arXiv:2105.05300v1 [cs.CV] 11 May 2021"
[3]: http://export.arxiv.org/abs/2207.05300v1 "[2207.05300v1] SD-GAN: Semantic Decomposition for Face Image Synthesis ..."

Here is a summary of the method section of the paper:

- The paper describes the proposed framework SD-GAN, which consists of three main modules: the generator, the discriminator and the 3D-aware semantic fusion network.
- The generator takes a latent code and a discrete attribute label as inputs and outputs a face image with the desired attribute. The generator is based on StyleGAN2 [^1^][1] [^2^][2], which uses adaptive instance normalization (AdaIN) to inject style information into the latent code.
- The discriminator takes a face image as input and outputs a score indicating how realistic the image is and a prediction of the discrete attribute label. The discriminator is based on StyleGAN2 [^1^][1] [^2^][2], which uses multi-scale gradient (MSG) to improve the stability of training.
- The 3D-aware semantic fusion network takes a face image and a discrete attribute label as inputs and outputs an offset latent representation that adjusts the semantic prior basis. The semantic prior basis is a fixed vector that represents the initial direction for manipulating the face representation in the latent space. The offset latent representation is learned by fusing 3D embedding and semantic embedding. The 3D embedding is obtained by projecting the face image onto a 3D face model using PRNet [^1^][1] [^2^][2]. The semantic embedding is obtained by encoding the discrete attribute label using an embedding layer.
- The paper also introduces a new dataset MEGN, which contains 10,000 face images with mask and eyeglasses attributes crawled from Google and Naver. The paper uses MEGN to evaluate SD-GAN on various facial discrete attributes, such as mask, eyeglasses, sunglasses, beard and mustache.

## Pseudo Code - High level

Here is the high-level pseudo code for this paper:

```
# Define the generator G, the discriminator D and the 3D-aware semantic fusion network F
G = StyleGAN2()
D = StyleGAN2()
F = SemanticFusionNetwork()

# Define the semantic prior basis B for each discrete attribute
B = {mask: b_mask, eyeglasses: b_eyeglasses, sunglasses: b_sunglasses, beard: b_beard, mustache: b_mustache}

# Define the loss functions
L_adv = adversarial_loss()
L_att = attribute_loss()
L_id = identity_loss()
L_rec = reconstruction_loss()

# Train SD-GAN
for each batch of face images X and discrete attribute labels Y:
  # Generate latent codes Z from a normal distribution
  Z = sample_normal_distribution()
  
  # Generate face images with desired attributes using G and B
  X_hat = G(Z + B[Y])
  
  # Compute the discriminator outputs for real and fake images
  D_real, Y_real = D(X)
  D_fake, Y_fake = D(X_hat)
  
  # Compute the adversarial loss for G and D
  L_adv_G = L_adv(D_fake, 1)
  L_adv_D = L_adv(D_real, 1) + L_adv(D_fake, 0)
  
  # Compute the attribute loss for G and D
  L_att_G = L_att(Y_fake, Y)
  L_att_D = L_att(Y_real, Y) + L_att(Y_fake, Y)
  
  # Compute the identity loss for G
  L_id_G = L_id(X_hat, X)
  
  # Compute the offset latent representation using F
  O = F(X, Y)
  
  # Generate face images with reconstructed attributes using G and O
  X_rec = G(Z + O)
  
  # Compute the reconstruction loss for F
  L_rec_F = L_rec(X_rec, X)
  
  # Update the parameters of G, D and F using gradient descent
  update_parameters(G, L_adv_G + L_att_G + L_id_G)
  update_parameters(D, L_adv_D + L_att_D)
  update_parameters(F, L_rec_F)
```

## Pseudo Code - Detail

Here is the detailed pseudo code to implement this paper:

```
# Import the necessary libraries
import torch
import torchvision
import numpy as np
import cv2
import PRNet

# Define the hyperparameters
batch_size = 16
latent_dim = 512
num_attributes = 5
num_epochs = 100
learning_rate = 0.0002
beta1 = 0.5
beta2 = 0.999

# Define the generator G, the discriminator D and the 3D-aware semantic fusion network F
G = StyleGAN2(latent_dim, num_attributes)
D = StyleGAN2(num_attributes)
F = SemanticFusionNetwork(latent_dim, num_attributes)

# Define the semantic prior basis B for each discrete attribute
B = torch.randn(num_attributes, latent_dim)
B = B / torch.norm(B, dim=1, keepdim=True)

# Define the loss functions
L_adv = torch.nn.BCEWithLogitsLoss()
L_att = torch.nn.CrossEntropyLoss()
L_id = torch.nn.CosineSimilarity()
L_rec = torch.nn.L1Loss()

# Define the optimizers
optimizer_G = torch.optim.Adam(G.parameters(), lr=learning_rate, betas=(beta1, beta2))
optimizer_D = torch.optim.Adam(D.parameters(), lr=learning_rate, betas=(beta1, beta2))
optimizer_F = torch.optim.Adam(F.parameters(), lr=learning_rate, betas=(beta1, beta2))

# Load the MEGN dataset
dataset = MEGNDataset()
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Train SD-GAN
for epoch in range(num_epochs):
  for i, (X, Y) in enumerate(dataloader):
    # Move the data to the device (CPU or GPU)
    X = X.to(device)
    Y = Y.to(device)
    
    # Generate latent codes Z from a normal distribution
    Z = torch.randn(batch_size, latent_dim).to(device)
    
    # Generate face images with desired attributes using G and B
    X_hat = G(Z + B[Y])
    
    # Compute the discriminator outputs for real and fake images
    D_real, Y_real = D(X)
    D_fake, Y_fake = D(X_hat)
    
    # Compute the adversarial loss for G and D
    L_adv_G = L_adv(D_fake, torch.ones(batch_size).to(device))
    L_adv_D = L_adv(D_real, torch.ones(batch_size).to(device)) + L_adv(D_fake, torch.zeros(batch_size).to(device))
    
    # Compute the attribute loss for G and D
    L_att_G = L_att(Y_fake, Y)
    L_att_D = L_att(Y_real, Y) + L_att(Y_fake, Y)
    
    # Compute the identity loss for G
    L_id_G = -torch.mean(L_id(X_hat, X))
    
    # Compute the offset latent representation using F
    O = F(X, Y)
    
    # Generate face images with reconstructed attributes using G and O
    X_rec = G(Z + O)
    
    # Compute the reconstruction loss for F
    L_rec_F = L_rec(X_rec, X)
    
    # Update the parameters of G, D and F using gradient descent
    optimizer_G.zero_grad()
    (L_adv_G + L_att_G + L_id_G).backward()
    optimizer_G.step()
    
    optimizer_D.zero_grad()
    (L_adv_D + L_att_D).backward()
    optimizer_D.step()
    
    optimizer_F.zero_grad()
    L_rec_F.backward()
    optimizer_F.step()
    
    # Print the losses and save the generated images periodically
    if i % 100 == 0:
      print(f"Epoch {epoch}, Batch {i}, L_adv_G: {L_adv_G.item()}, L_adv_D: {L_adv_D.item()}, L_att_G: {L_att_G.item()}, L_att_D: {L_att_D.item()}, L_id_G: {L_id_G.item()}, L_rec_F: {L_rec_F.item()}")
      torchvision.utils.save_image(X_hat, f"X_hat_{epoch}_{i}.png", normalize=True)
      torchvision.utils.save_image(X_rec, f"X_rec_{epoch}_{i}.png", normalize=True)
```